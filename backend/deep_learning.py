# Standard library imports
import os
import json
import logging
import argparse
import platform
import shutil
import traceback
import time
import select
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Tuple, Union, Any, Callable
from enum import Enum, auto
from copy import deepcopy
from collections import defaultdict, OrderedDict, deque
from functools import wraps, partial
from contextlib import nullcontext
import threading
import subprocess
import hashlib
import pickle
import gc
import re
import uuid
import tempfile
import webbrowser
import tracemalloc
from tqdm import tqdm
from alive_progress import alive_bar
import msvcrt
import math
import importlib.util

# Scientific computing and data manipulation
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, KFold, StratifiedKFold
from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_classification, make_blobs
from sklearn.covariance import EllipticEnvelope
from sklearn.mixture import GaussianMixture

# PyTorch ecosystem
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, Dataset, WeightedRandomSampler, DistributedSampler
from torch.utils.data.dataloader import default_collate
from torch.utils.tensorboard import SummaryWriter
from torch.cuda.amp import GradScaler, autocast
import torchvision
import torchvision.transforms as transforms
import torch.distributed as dist
from torch.cuda.amp import autocast as cuda_autocast, GradScaler
from torch.cpu.amp import autocast as cpu_autocast

# Hyperparameter optimization
import optuna
import optuna.visualization as vis
from optuna import visualization as vis
from optuna.samplers import TPESampler, RandomSampler, CmaEsSampler, NSGAIISampler
from optuna.pruners import MedianPruner, HyperbandPruner, NopPruner, SuccessiveHalvingPruner
from optuna.storages import RDBStorage
import joblib

# Visualization and plotting
import plotly
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
from plotly.subplots import make_subplots
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns

# Rich UI components for enhanced terminal interface
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, BarColumn, TimeElapsedColumn, SpinnerColumn, track, ProgressColumn, TextColumn
from rich import box
from rich.text import Text
from rich.columns import Columns
from rich.prompt import Prompt, Confirm, IntPrompt, FloatPrompt
from rich.syntax import Syntax
from rich.tree import Tree
from rich.layout import Layout
from rich.live import Live
from rich.spinner import Spinner
from rich.rule import Rule
from rich.align import Align
from rich.padding import Padding
from rich.markup import escape
from rich.status import Status, Spinner

# Terminal styling and colors
from colorama import Fore, Back, Style, init

# Initialize colorama
init(autoreset=True)

# Initialize rich console
console = Console()

# Configuration and serialization
import yaml
try:
    from yaml import CLoader as Loader, CDumper as Dumper
except ImportError:
    from yaml import Loader, Dumper

import toml
import configparser
from dataclasses import dataclass, field, asdict
from enum import Enum, auto

# Networking and API (for future remote capabilities)
import socket
import urllib.request
import urllib.parse
import urllib.error

# Parallel processing and concurrency
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from multiprocessing import Pool, cpu_count
#import asyncio

# File handling and compression
import zipfile
import tarfile
import gzip
import lzma

# Additional utility libraries
import itertools
import random
import string
from contextlib import contextmanager, suppress, nullcontext
import weakref
from types import SimpleNamespace
import pathlib
from pynput.keyboard import Key, Listener
#import pkg_resources
import packaging.version
from packaging import version as pkg_version
from sklearn.exceptions import ConvergenceWarning
from matplotlib import MatplotlibDeprecationWarning

# Development and debugging tools
import inspect
import cProfile
import pstats

# Version checking utilities (safe, future-proof)
import sys
import warnings

# Use importlib.metadata (stdlib) or backport
try:
    from importlib.metadata import version as _get_version, PackageNotFoundError
    IMPORTLIB_METADATA_AVAILABLE = True
except ImportError:  # Python <3.8
    try:
        from importlib_metadata import version as _get_version, PackageNotFoundError  # type: ignore
        IMPORTLIB_METADATA_AVAILABLE = True
    except ImportError:
        IMPORTLIB_METADATA_AVAILABLE = False
        PackageNotFoundError = Exception  # type: ignore

# Packaging for version parsing/comparison
try:
    from packaging import version as pkg_version
    PACKAGING_AVAILABLE = True
except ImportError:
    PACKAGING_AVAILABLE = False
    # Create a dummy version class for fallback
    class DummyVersion:
        def __init__(self, version_str):
            self.version_str = str(version_str)
        
        def __str__(self):
            return self.version_str
        
        def __lt__(self, other):
            return self.version_str < str(other)
        
        def __le__(self, other):
            return self.version_str <= str(other)
        
        def __eq__(self, other):
            return self.version_str == str(other)
        
        def __ge__(self, other):
            return self.version_str >= str(other)
        
        def __gt__(self, other):
            return self.version_str > str(other)
    
    def dummy_parse(version_str):
        return DummyVersion(version_str)
    
    # Fallback Dummy version module to mimic packaging.version
    pkg_version = type('DummyVersionModule', (), {'parse': dummy_parse})()

def safe_version(package_name: str) -> str:
    """
    Safely get the version of a package.
    Uses importlib.metadata, falls back to getattr(__version__),
    and returns 'N/A' if not found.
    """
    try:
        # Special cases where direct import is safer or required
        if package_name == "sklearn":
            import sklearn
            return sklearn.__version__
        if package_name == "torch":
            import torch
            return torch.__version__
        if package_name == "numpy":
            import numpy as np
            return np.__version__
        if package_name == "pandas":
            import pandas as pd
            return pd.__version__
        if package_name == "optuna":
            import optuna
            return optuna.__version__
        if package_name == "plotly":
            import plotly
            return plotly.__version__

        # Try importlib.metadata (preferred)
        if IMPORTLIB_METADATA_AVAILABLE:
            return _get_version(package_name)
        
        # Fallback: import the module and check __version__
        module = __import__(package_name)
        return getattr(module, "__version__", "unknown")
        
    except (PackageNotFoundError, ImportError):
        return "N/A"
    except Exception:
        return "unknown"

# Alias for backward compatibility with your existing code
get_package_version = safe_version

# Model export and optimization
import onnx

# Global flag and dummy module
ONNXRUNTIME_AVAILABLE = False
ort: Any = None

def initialize_onnx_runtime() -> bool:
    """
    Safely initialize ONNX Runtime with proper error handling.
    Returns True if ONNX Runtime is available and functional.
    """
    global ONNXRUNTIME_AVAILABLE, ort
    
    try:
        import onnxruntime as _ort
        # Test basic functionality
        try:
            # Simple test to verify DLL loading works
            _ort.get_device()
            ort = _ort
            ONNXRUNTIME_AVAILABLE = True
            return True
        except Exception as dll_error:
            warnings.warn(
                f"ONNX Runtime DLL load failed: {str(dll_error)}. "
                "ONNX validation will be disabled.",
                RuntimeWarning
            )
            create_dummy_ort()
            return False
    except ImportError:
        create_dummy_ort()
        return False

def create_dummy_ort() -> None:
    """Create a dummy ONNX Runtime module for compatibility."""
    global ort
    
    class DummyORT:
        __version__ = "N/A"
        
        class InferenceSession:
            def __init__(self, *args, **kwargs):
                raise RuntimeError(
                    "ONNX Runtime not available. "
                    "Original error: DLL load failed"
                )
        
        @staticmethod
        def get_available_providers() -> list:
            return []
        
        @staticmethod
        def get_device() -> str:
            return "CPU (ONNX Runtime not available)"
    
    ort = DummyORT()

# Initialize at module level
initialize_onnx_runtime()

try:
    from torch.jit import script, trace
    TORCH_JIT_AVAILABLE = True
except ImportError:
    TORCH_JIT_AVAILABLE = False

# System monitoring and profiling
import psutil
try:
    import memory_profiler
    MEMORY_PROFILER_AVAILABLE = True
except ImportError:
    MEMORY_PROFILER_AVAILABLE = False

try:
    import nvidia_ml_py3 as nvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False

# Cryptography and security (for model protection)
try:
    from cryptography.fernet import Fernet
    from cryptography.hazmat.primitives import hashes
    from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
    import base64
    CRYPTO_AVAILABLE = True
except ImportError:
    CRYPTO_AVAILABLE = False

# Database connectivity (for advanced storage)
try:
    import sqlite3
    import sqlalchemy
    DATABASE_AVAILABLE = True
except ImportError:
    DATABASE_AVAILABLE = False

# Additional ML libraries
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.svm import OneClassSVM
    from sklearn.neighbors import LocalOutlierFactor
    SKLEARN_ANOMALY_AVAILABLE = True
except ImportError:
    SKLEARN_ANOMALY_AVAILABLE = False

# Time series analysis (for temporal anomaly detection)
try:
    import statsmodels.api as sm
    from statsmodels.tsa.arima.model import ARIMA
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False

# Advanced numerical computation
try:
    from numba import jit, cuda
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False

# Profiling tools
try:
    from line_profiler import LineProfiler
    LINE_PROFILER_AVAILABLE = True
except ImportError:
    LINE_PROFILER_AVAILABLE = False

# Availability flags for optional dependencies
OPTIONAL_DEPENDENCIES = {
    'torch_jit': TORCH_JIT_AVAILABLE,
    'onnxruntime': ONNXRUNTIME_AVAILABLE,
    'nvml': NVML_AVAILABLE,
    'crypto': CRYPTO_AVAILABLE,
    'database': DATABASE_AVAILABLE,
    'sklearn_anomaly': SKLEARN_ANOMALY_AVAILABLE,
    'statsmodels': STATSMODELS_AVAILABLE,
    'numba': NUMBA_AVAILABLE,
    'memory_profiler': MEMORY_PROFILER_AVAILABLE,
    'line_profiler': LINE_PROFILER_AVAILABLE,
    'packaging': PACKAGING_AVAILABLE,
    'importlib_metadata': IMPORTLIB_METADATA_AVAILABLE
}

# Version information - Updated to properly detect Rich
VERSION_INFO = {
    'python': sys.version.split()[0],
    'torch': safe_version('torch'),
    'numpy': safe_version('numpy'),
    'pandas': safe_version('pandas'),
    'optuna': safe_version('optuna'),
    'rich': safe_version('rich'),
    'plotly': safe_version('plotly'),
    'sklearn': safe_version('sklearn'),
    'onnx': safe_version('onnx'),
    'psutil': safe_version('psutil')
}

# Setup logging
LOG_DIR = Path("logs")
LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE_NAME = "deep_learning.log"
LOG_FILE = LOG_DIR / LOG_FILE_NAME

# Configure directories
DEFAULT_MODEL_DIR = Path("models")
DEFAULT_MODEL_DIR.mkdir(exist_ok=True)

CONFIG_DIR = Path("config")
CONFIG_DIR.mkdir(exist_ok=True)
CONFIG_FILE_NAME = "deep_learning_config.json"
CONFIG_FILE = CONFIG_DIR / CONFIG_FILE_NAME

REPORTS_DIR = Path("reports")
REPORTS_DIR.mkdir(exist_ok=True)

TB_DIR = Path("tensorboard")
TB_DIR.mkdir(exist_ok=True)

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

CACHE_DIR = Path("cache")
CACHE_DIR.mkdir(exist_ok=True)

CHECKPOINTS_DIR = Path("checkpoints") / f"checkpoints_v{VERSION_INFO['torch']}"
CHECKPOINTS_DIR.mkdir(exist_ok=True)

RESULTS_DIR = Path("results")
RESULTS_DIR.mkdir(exist_ok=True)
RESULTS_FILE_NAME = "training_results.json"
RESULTS_FILE = RESULTS_DIR / RESULTS_FILE_NAME

METRICS_DIR = Path("metrics")
METRICS_DIR.mkdir(exist_ok=True)

DATASETS_DIR = Path("datasets")
DATASETS_DIR.mkdir(exist_ok=True)

ARTIFACTS_DIR = Path("artifacts")
ARTIFACTS_DIR.mkdir(exist_ok=True)

FIGURES_DIR = Path("figures")
FIGURES_DIR.mkdir(exist_ok=True)

INFO_DIR = Path("info")
INFO_DIR.mkdir(exist_ok=True)

def configure_device(device_type='auto'):
    """
    Configure device settings based on preference and availability.
    Defaults to CPU if CUDA is requested but not available.
    """
    # Handle CUDA preference with fallback to CPU
    if device_type == 'cuda' and not torch.cuda.is_available():
        print("Warning: CUDA requested but not available. Defaulting to CPU.")
        device_type = 'cpu'
    
    # Resolve device
    if device_type == 'auto':
        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    elif device_type in ['cpu', 'cuda']:
        DEVICE = device_type
    else:
        raise ValueError("device_type must be 'auto', 'cpu', or 'cuda'")
    
    MIXED_PRECISION = False if DEVICE == 'cpu' else True
    
    return DEVICE, MIXED_PRECISION

def configure_normalization(normalization_type='batch'):
    """
    Configure normalization settings based on the specified type.
    
    Args:
        normalization_type (str): 'batch', 'layer', 'instance', 'group', 'none', or None
    
    Returns:
        tuple: (NORMALIZATION, USE_BATCH_NORM, USE_LAYER_NORM)
    """
    
    normalization_options = ['batch', 'layer', 'instance', 'group', 'none', None]
    
    if normalization_type not in normalization_options:
        raise ValueError(f"Invalid normalization_type: {normalization_type}, it must be one of {normalization_options}")
    
    # Set the normalization type
    NORMALIZATION = normalization_type
    
    # Configure the boolean flags based on normalization type
    if normalization_type == 'batch':
        USE_BATCH_NORM = True
        USE_LAYER_NORM = False
    elif normalization_type == 'layer':
        USE_BATCH_NORM = False
        USE_LAYER_NORM = True
    elif normalization_type in ['none', None]:
        USE_BATCH_NORM = False
        USE_LAYER_NORM = False
    else:  # 'instance' or 'group'
        USE_BATCH_NORM = False
        USE_LAYER_NORM = False
    
    return NORMALIZATION, USE_BATCH_NORM, USE_LAYER_NORM

# Configuration Constants
DEFAULT_BATCH_SIZE = 64
DEFAULT_EPOCHS = 10
EARLY_STOPPING_PATIENCE = 100
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-4
GRADIENT_CLIP = 1.0
GRADIENT_ACCUMULATION_STEPS = 4
#DEVICE = 'cpu' if not torch.cuda.is_available() else 'cuda'
#MIXED_PRECISION = False if not torch.cuda.is_available() else True
DEVICE, MIXED_PRECISION = configure_device('auto')

# Model Architecture Constants
DEFAULT_ENCODING_DIM = 10
HIDDEN_LAYER_SIZES = [128, 64]
DROPOUT_RATES = [0.2, 0.15]
ACTIVATION = 'leaky_relu'
ACTIVATION_PARAM = 0.2
NORMALIZATION_OPTIONS = ['batch', 'layer', 'instance', 'group', 'none', None]
#NORMALIZATION = 'batch'
#USE_BATCH_NORM = True
#USE_LAYER_NORM = False
NORMALIZATION, USE_BATCH_NORM, USE_LAYER_NORM = configure_normalization('batch')

DIVERSITY_FACTOR = 0.1
MIN_FEATURES = 5
NUM_MODELS = 3
FEATURES = 20
INPUT_DIM = 20
NORMAL_SAMPLES = 8000
ATTACK_SAMPLES = 2000
ANOMALY_FACTOR = 1.5
RANDOM_STATE = 42

# Security Constants
DEFAULT_PERCENTILE = 95
DEFAULT_ATTACK_THRESHOLD = 0.3
FALSE_NEGATIVE_COST = 2.0
SECURITY_METRICS = True

# System Constants
NUM_WORKERS = min(4, os.cpu_count() or 1)
MAX_MEMORY_PERCENT = 80
# Cache timeout for model artifacts in seconds
CACHE_TIMEOUT = 3600

# Forward declarations for classes that will be defined later
class SimpleAutoencoder:
    """Forward declaration for SimpleAutoencoder class."""
    pass

class EnhancedAutoencoder:
    """Forward declaration for EnhancedAutoencoder class."""
    pass

class AutoencoderEnsemble:
    """Forward declaration for AutoencoderEnsemble class."""
    pass

class EnhancedCollateFn:
    """Forward declaration for EnhancedCollateFn class."""
    pass

class WorkerInitializer:
    """Forward declaration for WorkerInitializer class."""
    pass

class UnicodeStreamHandler:
    # Forward declaration
    pass

def setup_safe_globals():
    """
    Register a curated list of safe, version-stable classes for torch.load
    to prevent unpickling errors across different PyTorch, NumPy, and Pandas
    versions, including fallbacks for older/newer library structures.
    """
    # Dictionary of safe classes with their full import paths
    safe_classes = {
        # PyTorch core classes
        'torch.Tensor': torch.Tensor,
        'torch.nn.Module': torch.nn.Module,
        'torch.nn.parameter.Parameter': torch.nn.parameter.Parameter,
        'torch.FloatTensor': torch.FloatTensor,
        'torch.LongTensor': torch.LongTensor,
        'torch.IntTensor': torch.IntTensor,
        'torch.DoubleTensor': torch.DoubleTensor,
        
        # PyTorch optimizers and schedulers
        'torch.optim.Optimizer': torch.optim.Optimizer,
        'torch.optim.AdamW': torch.optim.AdamW,
        'torch.optim.SGD': torch.optim.SGD,
        'torch.optim.lr_scheduler.ReduceLROnPlateau': torch.optim.lr_scheduler.ReduceLROnPlateau,
        
        # PyTorch data loading
        'torch.utils.data.Dataset': torch.utils.data.Dataset,
        'torch.utils.data.DataLoader': torch.utils.data.DataLoader,
        'torch.utils.data.TensorDataset': torch.utils.data.TensorDataset,
        
        # Path handling classes (critical for the error you're seeing)
        'pathlib.Path': pathlib.Path,
        'pathlib.WindowsPath': pathlib.WindowsPath,
        'pathlib.PosixPath': pathlib.PosixPath,
        
        # Version handling
        'torch.torch_version.TorchVersion': torch.torch_version.TorchVersion,
        
        # NumPy core classes
        'numpy.ndarray': np.ndarray,
        'numpy.float32': np.float32,
        'numpy.float64': np.float64,
        'numpy.int32': np.int32,
        'numpy.int64': np.int64,
        'numpy.dtype': np.dtype,
        'numpy.number': np.number,
        
        # Python built-ins
        'builtins.dict': dict,
        'builtins.list': list,
        'builtins.tuple': tuple,
        'builtins.set': set,
        
        'EnhancedCollateFn': EnhancedCollateFn,
        'WorkerInitializer': WorkerInitializer,
        'UnicodeStreamHandler': UnicodeStreamHandler
    }

    # Add custom model classes if they exist
    try:
        from models import SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble
        safe_classes.update({
            'models.SimpleAutoencoder': SimpleAutoencoder,
            'models.EnhancedAutoencoder': EnhancedAutoencoder,
            'models.AutoencoderEnsemble': AutoencoderEnsemble
        })
    except ImportError:
        pass

    # Special handling for PyTorch storage types
    storage_types = [
        'torch.FloatStorage',
        'torch.LongStorage',
        'torch.IntStorage',
        'torch.DoubleStorage',
    ]
    
    for stype in storage_types:
        try:
            if hasattr(torch, stype):
                safe_classes[f'torch.{stype}'] = getattr(torch, stype)
        except Exception:
            pass

    # Register all safe classes using torch.serialization.add_safe_globals
    try:
        # Convert values to list for add_safe_globals
        safe_objects = list(safe_classes.values())
        torch.serialization.add_safe_globals(safe_objects)
        
        # Additional numpy-specific reconstruction functions
        numpy_reconstructors = []
        try:
            # For numpy >= 1.20 (new structure)
            if hasattr(np, '_core') and hasattr(np._core, 'multiarray'):
                numpy_reconstructors.extend([
                    np._core.multiarray._reconstruct,
                    np._core.multiarray.scalar,
                    np._core.multiarray.array,
                ])
            # For numpy < 1.20 (old structure)
            elif hasattr(np, 'core') and hasattr(np.core, 'multiarray'):
                numpy_reconstructors.extend([
                    np.core.multiarray._reconstruct,
                    np.core.multiarray.scalar,
                    np.core.multiarray.array,
                ])
        except AttributeError:
            pass
        
        if numpy_reconstructors:
            torch.serialization.add_safe_globals(numpy_reconstructors)

        # Add PyTorch internal reconstruction functions
        torch_reconstructors = []
        try:
            torch_reconstructors.extend([
                torch._utils._rebuild_tensor_v2,
                torch._utils._rebuild_parameter,
                torch._utils._rebuild_tensor,
            ])
        except AttributeError:
            pass
        
        if torch_reconstructors:
            torch.serialization.add_safe_globals(torch_reconstructors)

    except Exception as e:
        logging.warning(f"Failed to register some safe globals: {str(e)}")

# Setup safe globals at module level
setup_safe_globals()

# Disable PyTorch's duplicate logging
torch._logging.set_logs(all=logging.ERROR)

# Initialize logger at module level
logger = logging.getLogger(__name__)

# Loading Screen and System Check Framework
class CheckLevel(Enum):
    """Enumeration representing the severity of a system check."""
    # Check must pass for the program to continue running
    CRITICAL = auto()
    
    # Check should pass for full functionality but not fatal
    IMPORTANT = auto()
    
    # Non-essential check providing useful system information
    INFORMATIONAL = auto()

class CheckResult:
    """Encapsulates the outcome of a system check with enhanced functionality."""
    
    def __init__(self, 
                 passed: bool, 
                 message: str, 
                 level: CheckLevel = CheckLevel.IMPORTANT,
                 details: Optional[Union[str, Dict[str, Any]]] = None,
                 metadata: Optional[Dict[str, Any]] = None,
                 exception: Optional[Exception] = None):
        self.passed = passed
        self.message = message
        self.level = level
        self.details = details
        self.metadata = metadata if metadata is not None else {}
        self.exception = exception

    def with_details(self, details: Union[str, Dict[str, Any]]) -> 'CheckResult':
        """Return CheckResult with additional details."""
        self.details = details
        return self
    
    def with_exception(self, exception: Exception) -> 'CheckResult':
        """Return CheckResult with an exception."""
        self.exception = exception
        return self
    
    def with_metadata(self, metadata: Dict[str, Any]) -> 'CheckResult':
        """Return CheckResult with additional metadata."""
        if self.metadata is None:
            self.metadata = {}
        self.metadata.update(metadata)
        return self
    
    def with_passed(self, passed: bool) -> 'CheckResult':
        """Update the passed status and return self."""
        self.passed = passed
        return self
    
    def with_message(self, message: str) -> 'CheckResult':
        """Update the message and return self."""
        self.message = message
        return self
    
    def with_level(self, level: CheckLevel) -> 'CheckResult':
        """Update the check level and return self."""
        self.level = level
        return self

def loading_screen(
    logger: logging.Logger,
    extended: bool = False,
    include_performance: bool = False,
    hardware_data: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Display loading screen with system checks and interactive prompts.
    
    Args:
        logger: Logger for recording system check results
        extended: Whether to run extended initialization-specific checks
        include_performance: Whether to include performance-related checks
        hardware_data: Pre-fetched hardware data (optional, for optimization)
        
    Returns:
        bool: True if all critical checks pass and user chooses to continue,
              False if critical checks fail or user chooses to quit
    """
    # Thread safety lock
    _loading_lock = threading.RLock()
    
    with _loading_lock:
        try:
            # Console safety checks
            if not hasattr(console, 'width'):
                # Safe default
                console_width = 80
            else:
                # Minimum width
                console_width = max(60, getattr(console, 'width', 80))
            
            # Terminal capability detection
            is_tty = sys.stdout.isatty()
            supports_color = is_tty and hasattr(sys.stdout, 'isatty')
            
            # Safe console clear
            try:
                if is_tty:
                    console.clear()
                else:
                    # Fallback for non-TTY
                    console.print("\n" * 3)
            except Exception:
                # Safe fallback
                console.print("\n" * 3)
            
            # Initialize timing with thread-safe approach
            start_time = time.perf_counter()
            status_messages = [
                "Running System Diagnostics...",
                "Initializing system checks...",
                "Validating environment...",
                "Executing system checks..."
            ]
            
            # Non-blocking loading animation with proper status management
            current_status = None
            try:
                # Sequential status updates to avoid context conflicts
                for i, message in enumerate(status_messages):
                    if current_status:
                        current_status.stop()
                    
                    if is_tty:
                        current_status = console.status(
                            f"[bold blue]{message}[/bold blue]" if supports_color else message,
                            spinner="dots" if is_tty else None
                        )
                        current_status.start()
                        # Progressive timing
                        time.sleep(0.3 + i * 0.1)
                    else:
                        console.print(f"- {message}")
                        # Minimal delay for non-TTY
                        time.sleep(0.1)
            finally:
                if current_status:
                    current_status.stop()
                    current_status = None
            
            # ASCII art banner with width adaptation
            banner_width = min(console_width - 8, 100)
            ascii_art = """
        ⠀⠀⠀⢠⣾⣷⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⠀⠀⣰⣿⣿⣿⣿⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⠀⢰⣿⣿⣿⣿⣿⣿⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣤⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣄⣀⣀⣤⣤⣶⣾⣿⣿⣿⡷
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠁
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠁⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠏⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠏⠀⠀⠀⠀
        ⣿⣿⣿⡇⠀⡾⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠁⠀⠀⠀⠀⠀
        ⣿⣿⣿⣧⡀⠁⣀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠉⢹⠉⠙⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣀⠀⣀⣼⣿⣿⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠋⠀⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠛⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠛⠀⠤⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⠿⣿⣿⣿⣿⣿⣿⣿⠿⠋⢃⠈⠢⡁⠒⠄⡀⠈⠁⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⠟⠁⠀⠀⠈⠉⠉⠁⠀⠀⠀⠀⠈⠆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠀⠀⠀⠀⠀⠀⠀⠀⠀
            """
            
            # Safe banner display with width adaptation
            try:
                if banner_width > 80 and supports_color:
                    console.print("\n", Panel.fit(
                        ascii_art,
                        style="bold cyan" if supports_color else "",
                        title="[bold yellow]GreyChamp | IDS[/]" if supports_color else "GreyChamp | IDS",
                        subtitle="[magenta]SYSTEM INITIALIZATION[/]" if supports_color else "SYSTEM INITIALIZATION",
                        border_style="bold blue" if supports_color else "ascii",
                        box=box.DOUBLE if supports_color else box.ASCII,
                        padding=(1, 1),
                        width=min(banner_width, console_width - 4)
                    ))
                else:
                    # Simple fallback for narrow terminals
                    console.print("\n" + "=" * min(60, banner_width))
                    console.print("    GreyChamp | IDS - SYSTEM INITIALIZATION")
                    console.print("=" * min(60, banner_width) + "\n")
            except Exception as banner_error:
                # Ultra-safe fallback
                console.print("\nGreyChamp | IDS - SYSTEM INITIALIZATION\n")
                if logger:
                    logger.debug(f"Banner display failed: {banner_error}")
            
            # Check type information display
            check_type_info = "BASIC CHECKS"
            if extended and include_performance:
                check_type_info = "EXTENDED CHECKS (with Performance)"
            elif extended:
                check_type_info = "EXTENDED CHECKS"
            
            try:
                if supports_color and banner_width > 60:
                    console.print(Panel.fit(
                        f"Running {check_type_info}\n"
                        "Please wait while we validate your system...",
                        border_style="cyan",
                        style="bold cyan",
                        padding=(0, 2),
                        width=min(banner_width, console_width - 4)
                    ))
                else:
                    console.print(f"\nRunning {check_type_info}")
                    console.print("Please wait while we validate your system...\n")
            except Exception:
                console.print(f"\nRunning {check_type_info}\n")
            
            # Thread-safe system checks execution
            console.print("Executing system checks..." if not supports_color else "[bold cyan]Executing system checks...[/bold cyan]")
            
            # Use thread-safe timing measurement
            checks_start = time.perf_counter()
            # Pass the hardware_data to run_system_checks if provided
            results = run_system_checks(logger, extended, include_performance, hardware_data)
            elapsed_time = time.perf_counter() - checks_start
            
            # Thread-safe results processing
            if "system_summary" in results and results["system_summary"].details:
                if isinstance(results["system_summary"].details, dict):
                    results["system_summary"].details["execution_time"] = f"{elapsed_time:.2f}s"
            
            # Display results with error handling
            console.print()
            try:
                display_check_results(results, logger, extended, include_performance)
            except Exception as display_error:
                console.print(f"[red]Error displaying results: {display_error}[/red]" if supports_color else f"Error displaying results: {display_error}")
                if logger:
                    logger.error(f"Failed to display check results: {display_error}")
            console.print()
            
            # Safe results analysis
            summary = results.get("system_summary")
            system_error = results.get("system_error")
            
            # Determine system status with null safety
            system_status = "UNKNOWN"
            if summary and summary.details and isinstance(summary.details, dict):
                system_status = summary.details.get('system_status', 'UNKNOWN')
            
            # Count failures by level with safety checks
            critical_failed = sum(1 for result in results.values() if result and hasattr(result, 'level') and hasattr(result, 'passed') and result.level == CheckLevel.CRITICAL and not result.passed and result != summary)
            
            important_failed = sum(1 for result in results.values() if result and hasattr(result, 'level') and hasattr(result, 'passed') and result.level == CheckLevel.IMPORTANT and not result.passed)
            
            informational_failed = sum(1 for result in results.values() if result and hasattr(result, 'level') and hasattr(result, 'passed') and result.level == CheckLevel.INFORMATIONAL and not result.passed)
            
            # Handle different scenarios with proper cleanup
            return_value = False
            
            try:
                if system_error or system_status == "CRITICAL_FAILURE" or critical_failed > 0:
                    # Critical failure - system cannot continue
                    failed_critical_checks = [
                        name.replace("_", " ").title() 
                        for name, result in results.items() 
                        if (result and hasattr(result, 'level') and hasattr(result, 'passed') and
                            result.level == CheckLevel.CRITICAL and not result.passed and 
                            name != "system_summary")
                    ]
                    
                    error_message = (
                        f"CRITICAL SYSTEM CHECKS FAILED\n\n"
                        f"The system cannot continue due to critical failures.\n"
                        f"Failed checks: {', '.join(failed_critical_checks) if failed_critical_checks else 'System error occurred'}\n\n"
                        f"Please check the logs and resolve these issues before continuing."
                    )
                    
                    try:
                        if supports_color and banner_width > 60:
                            console.print(Panel.fit(
                                f"[bold red]{error_message}[/bold red]",
                                border_style="red",
                                title="Critical Failure",
                                padding=(1, 3),
                                width=min(banner_width, console_width - 4)
                            ))
                        else:
                            console.print(f"\nCRITICAL FAILURE:\n{error_message}")
                    except Exception:
                        console.print(f"\nCRITICAL FAILURE:\n{error_message}")
                    
                    if logger:
                        logger.critical(f"Critical system checks failed - cannot continue. Failed checks: {failed_critical_checks}")
                    
                    return_value = False
                    
                elif system_status in ["DEGRADED", "LIMITED"] or important_failed > 0 or informational_failed > 0:
                    # Non-critical failures - user decision with proper input handling
                    user_choice = _handle_user_decision_safe(
                        results, system_status, important_failed, informational_failed, 
                        elapsed_time, supports_color, banner_width, console_width, logger
                    )
                    
                    if user_choice is False:
                        return_value = False
                        # Cleanup handled in _handle_user_decision_safe
                    else:
                        return_value = True
                        
                else:
                    # All checks passed - success scenario
                    return_value = _handle_success_scenario(
                        summary, elapsed_time, supports_color, banner_width, console_width, logger
                    )
            
            except Exception as scenario_error:
                console.print(f"Error handling system check results: {scenario_error}")
                if logger:
                    logger.error(f"Error in scenario handling: {scenario_error}")
                return_value = False
            
            # Safe console clear before return
            try:
                if return_value and is_tty:
                    console.clear()
            except Exception:
                # Ignore clear failures
                pass
            
            #return return_value
            return return_value, results if return_value else None
            
        except KeyboardInterrupt:
            # Thread-safe interrupt handling
            try:
                console.print(Panel.fit(
                    "INITIALIZATION INTERRUPTED\n\n"
                    "System initialization was cancelled by user.",
                    border_style="red" if supports_color else "ascii",
                    title="Interrupted",
                    padding=(1, 3)
                ) if supports_color else "\nINITIALIZATION INTERRUPTED\n\nSystem initialization was cancelled by user.\n")
            except Exception:
                console.print("\nINITIALIZATION INTERRUPTED\n\nSystem initialization was cancelled by user.\n")
            
            if logger:
                logger.warning("System initialization interrupted by user (Ctrl+C)")
            
            sys.exit(0)
            
        except Exception as e:
            # Thread-safe error handling
            error_msg = f"UNEXPECTED ERROR DURING INITIALIZATION\n\nAn unexpected error occurred: {str(e)}\nError type: {type(e).__name__}"
            
            try:
                if supports_color:
                    console.print(Panel.fit(
                        f"[bold red]{error_msg}[/bold red]",
                        border_style="red",
                        title="System Error",
                        padding=(1, 3)
                    ))
                else:
                    console.print(f"\nSYSTEM ERROR:\n{error_msg}\n")
            except Exception:
                console.print(f"\nSYSTEM ERROR:\n{error_msg}\n")
            
            if logger:
                logger.critical(f"Loading screen failed with unexpected error: {str(e)}", exc_info=True)
                logger.error(f"Error occurred during {'extended' if extended else 'basic'} system checks")
            
            return False

def _handle_user_decision_safe(results, system_status, important_failed, informational_failed, elapsed_time, supports_color, banner_width, console_width, logger):
    """Thread-safe user decision handling with proper resource cleanup."""
    listener = None
    
    try:
        # Collect failed non-critical checks safely
        failed_checks = []
        for name, result in results.items():
            if (result and hasattr(result, 'passed') and hasattr(result, 'level') and 
                hasattr(result, 'message') and not result.passed and 
                result.level in [CheckLevel.IMPORTANT, CheckLevel.INFORMATIONAL] and 
                name not in ["system_summary", "system_error"]):
                failed_checks.append({
                    'name': name.replace("_", " ").title(),
                    'level': result.level.name,
                    'message': result.message
                })
        
        # Display failed checks summary safely
        if failed_checks:
            try:
                if supports_color and banner_width > 80:
                    fail_table = Table(
                        title="[bold yellow]Failed Non-Critical Checks[/bold yellow]",
                        box=box.SIMPLE,
                        header_style="bold magenta",
                        title_justify="left",
                        show_header=True,
                        show_lines=True,
                        width=min(100, console_width - 4)
                    )
                    fail_table.add_column("Check", style="bold cyan", width=28)
                    fail_table.add_column("Issue", style="bold white", no_wrap=False)
                    fail_table.add_column("Level", justify="center", width=14)
                    
                    for check in failed_checks:
                        level_style = {
                            "IMPORTANT": "bold yellow",
                            "INFORMATIONAL": "bold blue"
                        }.get(check['level'], "white")
                        
                        fail_table.add_row(
                            check['name'],
                            check['message'],
                            Text(check['level'], style=level_style)
                        )
                    
                    console.print(fail_table)
                else:
                    # Simple fallback display
                    print(Fore.YELLOW + Style.BRIGHT + "\nFailed Non-Critical Checks:")
                    for check in failed_checks:
                        console.print(f"  - {check['name']}: {check['message']} ({check['level']})")
                
                console.print()
                
            except Exception as table_error:
                # Ultra-safe fallback
                print(Fore.YELLOW + Style.BRIGHT + "\nSome non-critical checks failed:")
                for check in failed_checks:
                    console.print(f"  {check['name']}: {check['message']}")
                console.print()
                if logger:
                    logger.debug(f"Failed checks table display error: {table_error}")
        
        # Status display with safe formatting
        status_color = "yellow" if system_status == "DEGRADED" else "blue" if system_status == "LIMITED" else "yellow"
        status_message = {
            "DEGRADED": "SYSTEM DEGRADED",
            "LIMITED": "LIMITED FUNCTIONALITY", 
        }.get(system_status, "SOME CHECKS FAILED")
        
        prompt_text = (
            f"{status_message}\n\n"
            f"System Status Details:\n"
            f"- Important failures: {important_failed}\n"
            f"- Informational failures: {informational_failed}\n"
            f"- Total execution time: {elapsed_time:.2f}s\n\n"
            f"The system can continue with reduced functionality.\n"
        )
        try:
            if supports_color and banner_width > 60:
                console.print(Panel.fit(
                    f"[bold {status_color}]{prompt_text}[/bold {status_color}]",
                    border_style=status_color,
                    title="User Decision Required",
                    padding=(1, 3),
                    width=min(banner_width, console_width - 4)
                ))
            else:
                console.print(f"\n{status_message}\n")
                console.print(prompt_text)
        except Exception:
            console.print(f"\n{status_message}\n")
            console.print(prompt_text)
        
        # Use standard input instead of pynput to avoid buffer issues
        user_choice = None
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                response = input(Fore.YELLOW + Style.BRIGHT + "\nContinue anyway? (Y/n/q): ").strip().lower()
                
                # Default to yes
                if response in ['y', 'yes', '']:
                    user_choice = True
                    break
                elif response in ['n', 'no', 'q', 'quit']:
                    user_choice = False
                    break
                else:
                    if attempt < max_attempts - 1:
                        print(Fore.YELLOW + Style.BRIGHT + "\nPlease enter 'y' for yes or 'n' for no or 'q' for quit.")
                    
            except (EOFError, KeyboardInterrupt):
                user_choice = False
                break
            except Exception as input_error:
                if logger:
                    logger.debug(f"Input error on attempt {attempt + 1}: {input_error}")
                if attempt < max_attempts - 1:
                    print(Fore.RED + Style.BRIGHT + "\nInput error, please try again.")
        
        # Default to continue if no valid choice after max attempts
        if user_choice is None:
            user_choice = True
            print(Fore.CYAN + Style.BRIGHT + "\nUsing default choice: continue")
        
        # Handle user choice with safe output
        if user_choice is False:
            try:
                cancel_message = (
                    "USER CANCELLED INITIALIZATION\n\n"
                    "You chose to quit and resolve the issues.\n"
                    "Please check the logs and fix the failed checks."
                )
                if supports_color and banner_width > 60:
                    console.print(Panel.fit(
                        f"{cancel_message}",
                        border_style="red",
                        title="CANCELLED",
                        style="bold red",
                        padding=(1, 3),
                        width=min(banner_width, console_width - 4)
                    ))
                else:
                    print(Fore.RED + Style.BRIGHT + f"\nCANCELLED:\n{cancel_message}")
            except Exception:
                print(Fore.RED + Style.BRIGHT + f"\nCANCELLED:\n{cancel_message}")
            
            if logger:
                logger.warning("User chose to quit after seeing failed checks")
                logger.info(f"Failed checks summary: {len(failed_checks)} non-critical failures")
            
            print(Fore.RED + Style.BRIGHT + "\nExiting system initialization...")
            
            # give user time to read the message
            time.sleep(2)
            sys.exit(0)
            
            return False
        
        # User chose to continue
        try:
            continue_message = (
                "CONTINUING WITH WARNINGS\n\n"
                "You chose to continue despite the warnings.\n"
                "Some functionality may be limited."
            )
            if supports_color and banner_width > 60:
                console.print(Panel.fit(
                    f"{continue_message}",
                    border_style="green",
                    title="CONTINUING",
                    style="bold green",
                    padding=(1, 2),
                    width=min(banner_width, console_width - 4)
                ))
            else:
                print(Fore.GREEN + Style.BRIGHT + f"\nCONTINUING:\n{continue_message}")
        except Exception:
            print(Fore.GREEN + Style.BRIGHT + f"\nCONTINUING:\n{continue_message}")
        
        if logger:
            logger.info("User chose to continue despite failed checks")
            logger.info(f"System status: {system_status} with {len(failed_checks)} failed checks")
        
        # give user time to read the message
        time.sleep(2)
        
        return True
        
    except Exception as decision_error:
        if logger:
            logger.error(f"Error in user decision handling: {decision_error}")
        print(Fore.RED + Style.BRIGHT + f"\nError in user input - continuing with warnings: {decision_error}")
        
        # Default to continue on error
        return True
    
    finally:
        # Clean up input buffer
        try:
            # Small delay for any pending I/O
            time.sleep(0.05)
            
            # Flush streams
            sys.stdout.flush()
            sys.stderr.flush()
            
            # Clear input buffer
            if hasattr(select, 'select') and sys.stdin.isatty():
                while select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], []):
                    line = sys.stdin.readline()
                    if not line:
                        break
            
            # Windows approach
            try:
                import msvcrt
                while msvcrt.kbhit():
                    msvcrt.getch()
            except ImportError:
                pass
            
            # Final flush
            sys.stdin.flush()
            
        except Exception as cleanup_error:
            if logger:
                logger.debug(f"Input buffer cleanup failed: {cleanup_error}")

def _handle_success_scenario(summary, elapsed_time, supports_color, banner_width, console_width, logger):
    """Handle successful system checks scenario with safe input."""
    try:
        success_details = ""
        elapsed_time_details = f"{elapsed_time:.2f}"
        if summary and summary.details and isinstance(summary.details, dict):
            total_checks = summary.details.get('total_checks', 0)
            if supports_color:
                success_details = f"[bold yellow]{total_checks}[/bold yellow]"
                elapsed_time_details = f"[bold yellow]{elapsed_time:.2f}[/bold yellow]"
            else:
                success_details = str(total_checks)
        
        success_message = (
            f"ALL SYSTEM CHECKS PASSED\n"
            f"System is fully operational and ready!\n"
            f"Completed {success_details} checks successfully.\n"
            f"Completed in {elapsed_time_details} seconds.\n\n"
            f"Continue to system? (Y/n)"
        )
        
        try:
            if supports_color and banner_width > 60:
                console.print(Panel.fit(
                    f"{success_message}",
                    border_style="green",
                    style="bold green",
                    title="SUCCESS",
                    padding=(1, 3),
                    width=min(banner_width, console_width - 4)
                ))
            else:
                print(Fore.GREEN + Style.BRIGHT + f"\nSUCCESS:\n{success_message}")
        except Exception:
            print(Fore.GREEN + Style.BRIGHT + f"\nSUCCESS:\n{success_message}")
        
        # Handle user choice with safe input
        user_choice = None
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                response = input(Fore.YELLOW + Style.BRIGHT + "\nYour choice: ").strip().lower()
                
                # Default to yes (continue)
                if response in ['y', 'yes', '']:
                    user_choice = True
                    break
                elif response in ['n', 'no', 'q', 'quit']:
                    user_choice = False
                    break
                else:
                    if attempt < max_attempts - 1:
                        print(Fore.YELLOW + Style.BRIGHT + "\nPlease enter 'y' for yes or 'n' for no.")
                    
            except (EOFError, KeyboardInterrupt):
                user_choice = False
                break
            except Exception as input_error:
                if logger:
                    logger.debug(f"Input error on attempt {attempt + 1}: {input_error}")
                if attempt < max_attempts - 1:
                    print(Fore.RED + Style.BRIGHT + "\nInput error, please try again.")
        
        # Default to continue if no valid choice after max attempts
        if user_choice is None:
            user_choice = True
            print(Fore.CYAN + Style.BRIGHT + "\nUsing default choice: continue")
        
        # Handle user choice
        if user_choice is False:
            try:
                quit_message = (
                    "USER CHOSE TO QUIT\n\n"
                    "You chose to quit despite all checks passing.\n"
                    "System initialization cancelled."
                )
                if supports_color and banner_width > 60:
                    console.print(Panel.fit(
                        f"{quit_message}",
                        border_style="red",
                        style="bold red",
                        title="QUIT",
                        padding=(1, 3),
                        width=min(banner_width, console_width - 4)
                    ))
                else:
                    print(Fore.RED + Style.BRIGHT + f"\nQUIT:\n{quit_message}")
            except Exception:
                print(Fore.RED + Style.BRIGHT + f"\nQUIT:\n{quit_message}")
            
            if logger:
                logger.debug("User chose to quit after successful system checks")
            
            print(Fore.RED + Style.BRIGHT + "\nExiting system initialization...")
            
            time.sleep(2)
            sys.exit(0)
        
        # User chose to continue
        try:
            continue_message = (
                "CONTINUING TO SYSTEM\n\n"
                "All checks passed - proceeding to main system."
            )
            
            if supports_color and banner_width > 60:
                console.print(Panel.fit(
                    f"{continue_message}",
                    border_style="green",
                    title="PROCEEDING",
                    style="bold green",
                    padding=(1, 2),
                    width=min(banner_width, console_width - 4)
                ))
            else:
                print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{continue_message}")
        except Exception:
            print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{continue_message}")
        
        if logger:
            logger.debug(f"All system checks passed successfully in {elapsed_time:.2f}s - user chose to continue")
            if summary and summary.details:
                system_status = summary.details.get('system_status', 'OPTIMAL')
                logger.info(f"System status: {system_status}")
        
        # give user time to read the message
        time.sleep(2)
        
        return True
        
    except Exception as success_error:
        if logger:
            logger.error(f"Error in success scenario: {success_error}")
        print(Fore.GREEN + Style.BRIGHT + f"\nAll checks passed - continuing...")
        return True
    
    finally:
        # Clean up input buffer
        try:
            # Small delay for any pending I/O
            time.sleep(0.05)
            
            # Flush streams
            sys.stdout.flush()
            sys.stderr.flush()
            
            # Clear input buffer
            if hasattr(select, 'select') and sys.stdin.isatty():
                while select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], []):
                    line = sys.stdin.readline()
                    if not line:
                        break
            
            # Windows approach
            try:
                while msvcrt.kbhit():
                    msvcrt.getch()
            except ImportError:
                pass
            
            # Final flush
            sys.stdin.flush()
            
        except Exception as cleanup_error:
            if logger:
                logger.debug(f"Input buffer cleanup failed: {cleanup_error}")

def run_system_checks(
    logger: logging.Logger,
    extended: bool = False,
    include_performance: bool = False,
    hardware_data: Optional[Dict[str, Any]] = None
) -> Dict[str, CheckResult]:
    """
    Run comprehensive system checks with optional extended validations.
    
    Args:
        logger: Configured logger for recording check results
        extended: Whether to include initialization-specific checks
        include_performance: Whether to include performance-related checks
        hardware_data: Pre-fetched hardware data (optional, for optimization)
        
    Returns:
        Dictionary mapping check names to their CheckResult objects
    """
    checks: Dict[str, CheckResult] = {}
    
    try:
        # Core system checks (always run)
        raw_checks = {
            # Critical checks (essential for operation)
            'python_version': check_python_version(),
            'torch_available': check_torch(),
            
            # Important checks (affects functionality but not critical)
            'package_versions': check_package_versions_wrapper(),
            'directory_access': check_directory_access_wrapper(),
            
            # Hardware resource checks - use provided hardware_data if available
            'hardware': check_hardware_wrapper() if hardware_data is None else CheckResult(
                passed=True,
                message="Hardware check completed (using cached data)",
                level=CheckLevel.INFORMATIONAL,
                details=hardware_data
            ),
            
            # Informational checks (diagnostic purposes)
            'logging_setup': check_logging_setup(),
            'seed_config': check_seed_config(hardware_data)
        }

        # Extended system checks (when requested)
        if extended:
            raw_checks.update({
                'exception_handler': check_global_exception_handler(),
                'configuration_system': check_configuration_system_wrapper()
                #'model_variants': check_model_variants_wrapper()
            })
            
            # Performance-related checks (only when explicitly requested)
            if include_performance:
                raw_checks.update({
                    'performance_monitoring': check_performance_monitoring(),
                    'memory_management': check_memory_management(),
                    'performance_baseline': check_performance_baseline()
                })
        
        # Convert all results to CheckResult objects if they aren't already
        for name, result in raw_checks.items():
            if isinstance(result, CheckResult):
                checks[name] = result
            elif isinstance(result, dict):
                # Convert dictionary result to CheckResult
                checks[name] = CheckResult(
                    passed=result.get('passed', False),
                    message=result.get('message', f"Check {name} completed"),
                    # Default level, should be overridden by specific checks
                    level=CheckLevel.INFORMATIONAL,
                    details=result.get('details', result)
                )
            else:
                # Handle unexpected result types
                checks[name] = CheckResult(
                    passed=False,
                    message=f"Invalid result type for {name}: {type(result)}",
                    level=CheckLevel.CRITICAL,
                    details={'raw_result': str(result), 'result_type': str(type(result))}
                )
        
        # Calculate overall system status
        critical_checks = [result for result in checks.values() if result.level in {CheckLevel.CRITICAL, CheckLevel.IMPORTANT}]
        overall_passed = all(result.passed for result in critical_checks)
        
        # Determine system status based on failures
        critical_failures = sum(1 for r in checks.values() if not r.passed and r.level == CheckLevel.CRITICAL)
        important_failures = sum(1 for r in checks.values() if not r.passed and r.level == CheckLevel.IMPORTANT)
        
        if critical_failures > 0:
            system_status = "CRITICAL_FAILURE"
        elif important_failures > 0:
            system_status = "DEGRADED"
        elif any(not r.passed for r in checks.values()):
            system_status = "LIMITED"
        else:
            system_status = "OPTIMAL"
        
        # Create summary
        summary_details = {
            'total_checks': len(checks),
            'passed_checks': sum(1 for r in checks.values() if r.passed),
            'failed_checks': sum(1 for r in checks.values() if not r.passed),
            'critical_failures': critical_failures,
            'important_failures': important_failures,
            'system_status': system_status,
            'check_results': {
                name: {
                    'passed': result.passed,
                    'message': result.message,
                    'level': result.level.name,
                    'details': result.details if isinstance(result.details, (str, dict)) else str(result.details)
                }
                for name, result in checks.items()
            }
        }
        
        summary_message = (
            f"{system_status}: Extended system check summary" if extended 
            else f"{system_status}: Basic system check summary"
        )
        
        checks['system_summary'] = CheckResult(
            passed=overall_passed,
            message=summary_message,
            level=CheckLevel.CRITICAL if not overall_passed else CheckLevel.INFORMATIONAL,
            details=summary_details
        )
        
        # Log critical failures immediately
        if logger:
            for name, result in checks.items():
                if not result.passed and result.level == CheckLevel.CRITICAL:
                    logger.error(
                        f"Critical check failed: {name} - {result.message}",
                        extra={'check_details': result.details}
                    )
            
            if not overall_passed:
                logger.warning(
                    f"System checks completed with failures - Status: {system_status}",
                    extra={'summary': summary_details}
                )
            else:
                logger.debug(
                    f"All system checks passed - Status: {system_status}",
                    extra={'summary': {
                        'total_checks': summary_details['total_checks'],
                        'passed_checks': summary_details['passed_checks']
                    }}
                )
        
        return checks
    
    except Exception as e:
        error_result = CheckResult(
            passed=False,
            message="System checks failed to complete",
            level=CheckLevel.CRITICAL,
            details={
                'error': str(e),
                'completed_checks': list(checks.keys()),
                'traceback': traceback.format_exc()
            }
        ).with_exception(e)
        
        checks['system_error'] = error_result
        
        if logger:
            logger.critical(
                "Fatal error during system checks",
                exc_info=True,
                extra={
                    'completed_checks': list(checks.keys()),
                    'error': str(e)
                }
            )
        
        return checks

def display_check_results(
    results: Dict[str, CheckResult],
    logger: logging.Logger,
    extended: bool = False,
    include_performance: bool = False
) -> None:
    """
    Display check results in a styled table with improved formatting that matches
    the structure of run_system_checks output.
    
    Args:
        results: Dictionary of check results from run_system_checks()
        logger: Configured logger for recording the output
        extended: Whether extended checks were included (affects display)
        include_performance: Whether performance checks were included (affects display)
    """
    try:
        # Create the report table with dynamic title
        report_type = "Extended" if extended else "Basic"
        if include_performance:
            report_type += " (Performance)"
            
        table = Table(
            title=f"\n[bold]SYSTEM DIAGNOSTICS REPORT - {report_type}[/bold]",
            box=box.ROUNDED,
            header_style="bold white",
            border_style="white",
            title_style="bold yellow",
            title_justify="left",
            show_lines=True,
            expand=True,
            width=min(120, console.width - 4)
        )
        
        # Configure columns
        table.add_column("Check", style="bold cyan", width=25)
        table.add_column("Status", width=12, justify="center")
        table.add_column("Level", width=14, justify="center")
        table.add_column("Details", style="bold", min_width=50, max_width=80)
        
        # Group by check level in priority order
        for level in [CheckLevel.CRITICAL, CheckLevel.IMPORTANT, CheckLevel.INFORMATIONAL]:
            # Filter checks for this level, excluding summary/error entries
            level_rows = [
                (name, result) for name, result in results.items() 
                if result.level == level 
                and name not in ["system_summary", "system_error"]
            ]
            
            if not level_rows:
                continue
                
            # Add section header with colored background
            level_style = {
                CheckLevel.CRITICAL: "bold white on red",
                CheckLevel.IMPORTANT: "bold white on yellow",
                CheckLevel.INFORMATIONAL: "bold white on blue"
            }[level]
            
            table.add_row(
                Text(level.name, style=level_style),
                "",
                "",
                "",
                style=level_style
            )
            
            # Add checks for this level
            for name, result in level_rows:
                # Determine status styling
                if result.passed:
                    status_style = "bold green"
                    status_text = "PASS"
                else:
                    status_style = "bold red" if level == CheckLevel.CRITICAL else "bold yellow"
                    status_text = "FAIL" if level == CheckLevel.CRITICAL else "WARN"
                
                # Format details with special handling for specific checks
                details_lines = []
                
                # Main message
                details_lines.append(f"[white]{result.message}[/white]")
                
                # Detail formatting
                if isinstance(result.details, dict):
                    # Special handling for configuration system - show basic info only
                    if name == 'configuration_system':
                        if 'sections_loaded' in result.details:
                            details_lines.append(f"Sections: [bold green]{result.details['sections_loaded']}[/bold green], Parameters: [bold green]{result.details['total_parameters']}[/bold green]")
                        if 'active_preset' in result.details:
                            details_lines.append(f"Active preset: [bold green]{result.details['active_preset'] or 'default'}[/bold green]")
                        if 'model_type' in result.details:
                            details_lines.append(f"Model type: [bold green]{result.details['model_type']}[/bold green]")
                        # Note that detailed tables were already displayed by the wrapper
                    
                    # Special handling for model variants
                    elif name == 'model_variants':
                        if 'variant_names' in result.details:
                            details_lines.append(f"Available: [bold green]{', '.join(result.details['variant_names'])}[/bold green]")
                        if 'initialization_summary' in result.details:
                            summary = result.details['initialization_summary']
                            details_lines.append(f"Success rate: [bold green]{summary['successful']}/{summary['attempted']}[/bold green]")
                    
                    # Special handling for version info
                    elif 'version_info' in result.details:
                        versions = result.details['version_info']
                        details_lines.append("Dependencies:")
                        for pkg, info in versions.items():
                            status = "[bold green]OK" if info['compatible'] else "[bold red]FAIL"
                            details_lines.append(
                                f"  {status} [bold green]{pkg}: {info['version']}[/bold green]"
                                f"(requires {info['required_version'] or 'any'})"
                            )
                    
                    # General dict handling for other checks
                    else:
                        for key, value in result.details.items():
                            if key not in ['error', 'exception', 'traceback', 'variant_status', 'section_details', 'full_details']:
                                if isinstance(value, (int, float, str, bool)):
                                    details_lines.append(f"{key}: {value}")
                
                elif result.details and isinstance(result.details, str):
                    details_lines.append(f"{result.details}")
                
                # Add exception if present
                if result.exception:
                    details_lines.append(f"[bold red]Error: {str(result.exception)}[/bold red]")
                
                details_text = "\n".join(details_lines)
                
                # Add row to table
                table.add_row(
                    Text(name.replace("_", " ").title()), 
                    Text(status_text, style=status_style),
                    Text(level.name, style="bold yellow" if level == CheckLevel.IMPORTANT else "bold blue" if level == CheckLevel.INFORMATIONAL else "bold red"),
                    details_text
                )
        
        # Add summary/error rows if present
        if "system_summary" in results:
            summary = results["system_summary"]
            summary_style = "bold white on green" if summary.passed else "bold white on red"
            checks_run = summary.details.get('total_checks', 0)
            passed_checks = summary.details.get('passed_checks', 0)
            checks_critical = summary.details.get('critical_failures', 0)
            summary_status = summary.details.get('system_status', 'UNKNOWN')
            
            table.add_row(
                Text("SUMMARY", style="bold white on yellow"),
                Text("PASS" if summary.passed else "FAIL", style=summary_style),
                "",
                Text(
                    f"{passed_checks}/{checks_run} checks passed | "
                    f"{checks_critical} critical failures | "
                    f"Status: {summary_status}",
                    style=summary_style
                )
            )
        
        if "system_error" in results:
            error = results["system_error"]
            error_details = error.details.get('error', 'Unknown error')
            checks_completed = error.details.get('completed_checks', [])
            error_message = (f"{error.message}\n"
                             f"[bold yellow]{error_details}[/bold yellow]")
            table.add_row(
                Text("FATAL ERROR", style="bold white on red"),
                Text("ERROR", style="bold white on red"),
                "",
                Text(
                    f"{error_message}\n"
                    f"Completed checks: {', '.join(checks_completed)}",
                    style="bold white on red"
                )
            )
        
        # Print the main table
        console.print(table)
        
        # Logging - suppress redundant configuration/model messages
        if logger:
            # Log only the summary
            summary = results.get("system_summary")
            if summary:
                logger.debug(
                    f"System diagnostics completed: {summary.details['passed_checks']}/"
                    f"{summary.details['total_checks']} checks passed, "
                    f"{summary.details['critical_failures']} critical failures, "
                    f"Status: {summary.details.get('system_status', 'UNKNOWN')}"
                )
            
            # Log configuration and model status briefly
            if 'configuration_system' in results:
                config_result = results['configuration_system']
                if config_result.passed and isinstance(config_result.details, dict):
                    logger.debug(f"Configuration system: {config_result.details.get('sections_loaded', 0)} sections loaded")
            
            if 'model_variants' in results:
                model_result = results['model_variants']
                if model_result.passed and isinstance(model_result.details, dict):
                    variants = model_result.details.get('variant_names', [])
                    logger.debug(f"Model variants: {len(variants)} available ({', '.join(variants)})")
            
            # Log only critical failures
            critical_failures = [
                (name, result) for name, result in results.items()
                if not result.passed and result.level == CheckLevel.CRITICAL
                and name not in ["system_summary", "system_error"]
            ]
            
            if critical_failures:
                for name, result in critical_failures:
                    logger.critical(f"Critical check failed: {name} - {result.message}")
            
            # Log any system error
            if "system_error" in results:
                error = results["system_error"]
                logger.critical(f"System checks failed: {error.details.get('error', 'Unknown error')}")
    
    except Exception as e:
        error_msg = f"Failed to display check results: {str(e)}"
        print(Fore.RED + Style.BRIGHT + f"{error_msg}")
        
        if logger:
            logger.critical(error_msg, exc_info=True)

# Individual check implementations
def check_python_version(min_version: Tuple[int, int] = (3, 8)) -> CheckResult:
    """Verify that the current Python version meets the minimum requirement."""
    try:
        # Leverage the version info from check_versions
        version_info = check_versions(include_optional=False)
        python_info = version_info.get('Python', {})
        
        if not python_info:
            return CheckResult(
                passed=False,
                message="Python version information not available",
                level=CheckLevel.CRITICAL
            )
        
        # Get version from the comprehensive check
        current_version = tuple(map(int, python_info['version'].split('.')[:2]))
        passed = current_version >= min_version
        
        message = (
            f"Python version {'meets' if passed else 'fails'} minimum requirement "
            f"({'.'.join(map(str, min_version))})"
        )
        
        base_result = CheckResult(
            passed=passed,
            message=message,
            level=CheckLevel.CRITICAL if not passed else CheckLevel.INFORMATIONAL,
            details=python_info.get('details')
        )
        
        return base_result.with_details(
            f"Current version: {python_info['version']}\n"
            f"Minimum required: {'.'.join(map(str, min_version))}\n"
            f"Status: {'Compatible' if passed else 'Incompatible'}"
        )
        
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Python version check failed",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Could not determine Python version: {str(e)}")
            .with_exception(e)
        )

def safe_version_compare(current_version: str, requirement: Optional[str]) -> bool:
    """Safely compare version against requirement."""
    if current_version in ['N/A', 'unknown', 'Available']:
        return current_version == 'Available'
    if not requirement:
        return True
    
    try:
        # Extract version number from requirement (remove >=, ==, etc.)
        req_version = requirement.replace('>=', '').replace('<=', '').replace('==', '').replace('>', '').replace('<', '').strip()
        
        if not PACKAGING_AVAILABLE:
            # Fallback comparison without packaging
            return str(current_version) >= req_version
        
        return pkg_version.parse(current_version) >= pkg_version.parse(req_version)
    except Exception:
        return False

def check_versions(include_optional: bool = True) -> Dict[str, Dict[str, Any]]:
    """
    Verify package versions with comprehensive dependency checking.
    Returns a dictionary containing version status for all dependencies.
    
    Args:
        include_optional: Whether to include optional dependencies in the check
        
    Returns:
        Dictionary with package names as keys and version info as values
    """
    version_info = {}
    
    # Core dependencies - get actual versions from VERSION_INFO
    core_deps = {
        'Python': (VERSION_INFO['python'], '>=3.7', True),
        'PyTorch': (VERSION_INFO['torch'], '>=1.8', True),
        'NumPy': (VERSION_INFO['numpy'], '>=1.19', True),
        'Pandas': (VERSION_INFO['pandas'], '>=1.2', True),
        'Scikit-learn': (VERSION_INFO['sklearn'], '>=0.24', True),
        'Optuna': (VERSION_INFO['optuna'], '>=2.8', True),
        'Rich': (VERSION_INFO['rich'], '>=10.0', True),
        'Plotly': (VERSION_INFO['plotly'], '>=5.0', False)
    }
    
    # Optional dependencies - all using safe_version now
    optional_deps = {
        'ONNX Runtime': (safe_version('onnxruntime') if OPTIONAL_DEPENDENCIES.get('onnxruntime', False) else 'N/A', '>=1.8', False),
        'ONNX': (VERSION_INFO['onnx'], '>=1.8', False),
        'NVIDIA ML': (safe_version('nvidia-ml-py') if OPTIONAL_DEPENDENCIES.get('nvml', False) else 'N/A', '>=11.0', False),
        'Torch JIT': ('Available' if OPTIONAL_DEPENDENCIES.get('torch_jit', False) else 'N/A', None, False),
        'Cryptography': (safe_version('cryptography') if OPTIONAL_DEPENDENCIES.get('crypto', False) else 'N/A', None, False),
        'Database': ('Available' if OPTIONAL_DEPENDENCIES.get('database', False) else 'N/A', None, False),
        'Sklearn Anomaly': ('Available' if OPTIONAL_DEPENDENCIES.get('sklearn_anomaly', False) else 'N/A', None, False),
        'Statsmodels': (safe_version('statsmodels') if OPTIONAL_DEPENDENCIES.get('statsmodels', False) else 'N/A', None, False),
        'Numba': (safe_version('numba') if OPTIONAL_DEPENDENCIES.get('numba', False) else 'N/A', None, False),
        'Memory Profiler': (safe_version('memory-profiler') if OPTIONAL_DEPENDENCIES.get('memory_profiler', False) else 'N/A', None, False),
        'Line Profiler': (safe_version('line-profiler') if OPTIONAL_DEPENDENCIES.get('line_profiler', False) else 'N/A', None, False),
        'Packaging': ('Available' if OPTIONAL_DEPENDENCIES.get('packaging', False) else 'N/A', None, False),
        'PSUtil': (VERSION_INFO['psutil'], '>=5.8', False)
    }
    
    # Check all dependencies
    all_deps = {**core_deps}
    if include_optional:
        all_deps.update(optional_deps)
    
    for name, (version, requirement, required) in all_deps.items():
        if not include_optional and not required:
            continue
            
        try:
            if version == 'N/A':
                status = 'MISSING'
                meets_req = False
            elif version == 'Available' or requirement is None:
                status = 'OK'
                meets_req = True
            elif version == 'unknown':
                status = 'UNKNOWN'
                meets_req = False
            else:
                meets_req = safe_version_compare(version, requirement)
                status = 'OK' if meets_req else 'WARNING'
                
        except Exception as e:
            meets_req = False
            status = 'ERROR'
            version = f"Error: {str(e)}"
        
        version_info[name] = {
            'version': version,
            'required_version': requirement,
            'status': status,
            'required': required,
            'description': get_dependency_description(name),
            'compatible': meets_req,
            'available': version not in ['N/A', 'unknown'] and not str(version).startswith('Error:')
        }
    
    return version_info

def check_torch() -> CheckResult:
    """Confirm that PyTorch is installed and operational."""
    try:
        # Use the comprehensive version check
        version_info = check_versions(include_optional=False)
        torch_info = version_info.get('PyTorch', {})
        
        if not torch_info:
            return CheckResult(
                passed=False,
                message="PyTorch version information not available",
                level=CheckLevel.CRITICAL
            )
        
        passed = torch_info.get('compatible', False)
        base_result = CheckResult(
            passed=passed,
            message=f"PyTorch is {'available and compatible' if passed else 'not properly installed or incompatible'}",
            level=CheckLevel.CRITICAL,
            details=torch_info.get('details')
        )
        
        if passed:
            return base_result.with_details(
                f"Version: {torch_info['version']}\n"
                f"Required: {torch_info['required_version'] or 'Not specified'}\n"
                f"Description: {torch_info['description']}"
            )
        return base_result.with_details(
            f"PyTorch check failed\n"
            f"Installed version: {torch_info.get('version', 'unknown')}\n"
            f"Required version: {torch_info.get('required_version', 'unknown')}"
        )
        
    except ImportError as e:
        return (
            CheckResult(
                passed=False,
                message="PyTorch is not installed",
                level=CheckLevel.CRITICAL
            )
            .with_details("PyTorch package not found in Python environment")
            .with_exception(e)
        )
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="PyTorch check failed unexpectedly",
                level=CheckLevel.CRITICAL
            )
            .with_details(str(e))
            .with_exception(e)
        )

def check_package_versions_wrapper(include_optional: bool = True) -> CheckResult:
    """
    Run comprehensive package version validation with rich output.
    
    Args:
        include_optional: Whether to include optional dependencies
        
    Returns:
        CheckResult object with detailed version information
    """
    try:
        version_info = check_versions(include_optional)
        details = []
        passed = True
        
        # Prepare detailed information
        for name, info in version_info.items():
            status_icon = "[PASS]" if info['status'] == 'OK' else "[WARN]" if info['status'] == 'WARNING' else "[FAIL]"
            req_text = f"(requires {info['required_version']})" if info['required_version'] else ""
            details.append(
                f"{status_icon} {name}: {info['version']} {req_text} - "
                f"{'Required' if info['required'] else 'Optional'}"
            )
            if info['required'] and not info['compatible']:
                passed = False
        
        # Create rich table for display
        table = Table(title="DEPENDENCY CHECK", title_justify="left", title_style="bold yellow", box=box.ROUNDED, show_lines=True)
        table.add_column("Package", style="bold cyan", no_wrap=True)
        table.add_column("Version", style="bold magenta")
        table.add_column("Type", style="bold green")
        table.add_column("Status", justify="center")
        table.add_column("Description", style="bold blue")
        
        for name, info in version_info.items():
            status_style = "bold green" if info['status'] == 'OK' else "bold yellow" if info['status'] == 'WARNING' else "bold red"
            required_style = "bold green" if info['required'] else "bold yellow"
            required_text = f"[{required_style}]{'Required' if info ['required'] else 'Optional'}[/{required_style}]"
            table.add_row(
                name,
                info['version'],
                required_text,
                f"[{status_style}]{info['status']}[/{status_style}]",
                info['description']
            )
        
        console.print(table)
        
        base_result = CheckResult(
            passed=passed,
            message="Package version check completed",
            level=CheckLevel.CRITICAL if not passed else CheckLevel.IMPORTANT
        )
        
        return (
            base_result
            .with_details("\n".join(details))
            .with_metadata({
                'version_info': version_info,
                'table': table,
                'summary': {
                    'total': len(version_info),
                    'passed': sum(1 for info in version_info.values() if info['status'] == 'OK'),
                    'warnings': sum(1 for info in version_info.values() if info['status'] == 'WARNING'),
                    'missing': sum(1 for info in version_info.values() if info['status'] == 'MISSING')
                }
            })
        )
        
    except ImportError as e:
        return (
            CheckResult(
                passed=False,
                message="Package version check failed - missing dependencies",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Required package not found: {str(e)}")
            .with_exception(e)
        )
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Package version check failed unexpectedly",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Error during version check: {str(e)}")
            .with_exception(e)
        )

def get_dependency_description(dep_name: str) -> str:
    """Get detailed description for a dependency."""
    descriptions = {
        # Core dependencies
        'Python': 'Python programming language runtime',
        'PyTorch': 'Deep learning framework (core dependency)',
        'NumPy': 'Fundamental package for numerical computing',
        'Pandas': 'Data manipulation and analysis toolkit',
        'Scikit-learn': 'Machine learning algorithms and utilities',
        'Optuna': 'Hyperparameter optimization framework',
        'Rich': 'Rich text and beautiful formatting in terminal',
        'Plotly': 'Interactive visualization library',
        'PSUtil': 'Process and system monitoring utilities',
        
        # Optional dependencies
        'ONNX Runtime': 'Cross-platform inference engine for ONNX models',
        'ONNX': 'Open Neural Network Exchange format',
        'NVIDIA ML': 'NVIDIA Management Library for GPU monitoring',
        'Torch JIT': 'PyTorch Just-In-Time compilation for model optimization',
        'Cryptography': 'Cryptographic primitives for model security',
        'Database': 'Database connectivity for model storage and retrieval',
        'Sklearn Anomaly': 'Scikit-learn anomaly detection algorithms',
        'Statsmodels': 'Statistical modeling and econometrics',
        'Numba': 'JIT compiler for numerical functions',
        'Memory Profiler': 'Memory usage tracking and analysis',
        'Line Profiler': 'Line-by-line performance profiling',
        'Packaging': 'Core utilities for Python packaging'
    }
    return descriptions.get(dep_name, 'Additional functionality')

def check_directory_access_wrapper() -> CheckResult:
    """Verify directory access using the setup_directories function."""
    try:
        dirs = setup_directories(logger)
        access_issues = []
        
        # Check each directory
        for name, path in dirs.items():
            if not path.exists():
                access_issues.append(f"Missing: {name} ({path})")
            elif not os.access(path, os.W_OK):
                access_issues.append(f"No write access: {name} ({path})")
        
        passed = len(access_issues) == 0
        base_result = CheckResult(
            passed=passed,
            message="Directory access verification",
            level=CheckLevel.CRITICAL
        )
        
        if passed:
            details = "\n".join(f"{name}: {path}" for name, path in dirs.items())
            return base_result.with_details(details)
        else:
            details = "\n".join([
                "Directory access issues found:",
                *access_issues,
                "",
                "All directories:",
                *[f"- {name}: {path}" for name, path in dirs.items()]
            ])
            return base_result.with_details(details)
            
    except PermissionError as e:
        return (
            CheckResult(
                passed=False,
                message="Permission denied for directory access",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Permission error: {str(e)}")
            .with_exception(e)
        )
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Directory access check failed unexpectedly",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Error: {str(e)}")
            .with_exception(e)
        )

def check_logging_setup() -> CheckResult:
    """
    Verify that logging is configured according to setup_logging().

    Checks:
    - At least one file handler with UTF-8 encoding
    - Log file exists
    - At least one console handler using UnicodeStreamHandler
    - Produces a compliance score (0-100%)
    - Returns human-readable feedback
    - Colorized terminal output (auto-disabled if not TTY)
    """
    try:
        # Initialize base result
        base_result = CheckResult(
            passed=False,
            message="Logging configuration check",
            level=CheckLevel.IMPORTANT
        )

        # Detect if output is a TTY (interactive terminal)
        use_color = sys.stdout.isatty()

        # Get logger and handlers
        logger = logging.getLogger(__name__)
        handlers = logger.handlers

        # Initialize check data
        check_data: Dict[str, any] = {
            'handlers': [],
            'checks': {
                'file_handler_found': False,
                'file_handler_utf8': False,
                'file_handler_exists': False,
                'console_handler_found': False,
                'console_handler_unicode': False
            },
            'feedback': [],
            'compliance_score': 0
        }

        # If no handlers, return immediately
        if not handlers:
            return (
                base_result
                .with_details({
                    'error': 'No handlers configured',
                    'compliance_score': 0,
                    'feedback': ['No logging handlers configured — run setup_logging()']
                })
                .with_message("No logging handlers configured")
            )

        # Analyze each handler
        for handler in handlers:
            handler_info = {
                'type': handler.__class__.__name__,
                'level': logging.getLevelName(handler.level)
            }

            # File handler checks
            if isinstance(handler, logging.FileHandler):
                check_data['checks']['file_handler_found'] = True
                encoding = getattr(handler, 'encoding', '').lower()
                if encoding == 'utf-8':
                    check_data['checks']['file_handler_utf8'] = True

                log_path = Path(getattr(handler, 'baseFilename', ''))
                if log_path.exists():
                    check_data['checks']['file_handler_exists'] = True

                handler_info.update({
                    'encoding': encoding,
                    'file_path': str(log_path),
                    'exists': log_path.exists()
                })

            # Console handler checks (must be UnicodeStreamHandler)
            elif isinstance(handler, UnicodeStreamHandler):
                check_data['checks']['console_handler_found'] = True
                check_data['checks']['console_handler_unicode'] = True
                handler_info['stream'] = getattr(handler.stream, 'name', str(handler.stream))

            check_data['handlers'].append(handler_info)

        # Calculate compliance score
        total_checks = len(check_data['checks'])
        passed_checks = sum(check_data['checks'].values())
        compliance_score = int((passed_checks / total_checks) * 100)
        passed = compliance_score == 100

        # Generate feedback messages
        feedback = []
        if not check_data['checks']['file_handler_found']:
            feedback.append("Missing file handler")
        if not check_data['checks']['file_handler_utf8']:
            feedback.append("File handler not using UTF-8 encoding")
        if not check_data['checks']['file_handler_exists']:
            feedback.append("Log file does not exist")
        if not check_data['checks']['console_handler_found']:
            feedback.append("Missing console handler")
        if not check_data['checks']['console_handler_unicode']:
            feedback.append("Console handler is not Unicode-safe (must use UnicodeStreamHandler)")

        # Prepare final details
        details = {
            **check_data,
            'compliance_score': compliance_score,
            'feedback': feedback,
            'passed_checks': passed_checks,
            'total_checks': total_checks
        }

        return (
            base_result
            .with_passed(passed)
            .with_details(details)
            .with_message(f"Logging configuration {'passed' if passed else 'failed'} ({compliance_score}%)")
        )

    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Logging check failed",
                level=CheckLevel.IMPORTANT
            )
            .with_details({
                'error': str(e),
                'compliance_score': 0,
                'feedback': ['Exception occurred during logging setup check']
            })
            .with_exception(e)
        )

def check_hardware(min_disk_gb: float = 1.0, include_memory_usage: bool = False) -> Dict[str, Dict[str, Any]]:
    """
    Low-level engine for hardware/system validation.
    Collects and validates raw hardware/system information.
    
    Args:
        min_disk_gb: Minimum required disk space in GB
        include_memory_usage: Whether to include current memory usage statistics
    
    Returns:
        Dictionary with detailed metadata for each hardware/system dependency
    """
    hardware_data = {}
    
    # Disk space check
    disk_info = {}
    try:
        usage = shutil.disk_usage('.')
        free_gb = usage.free / (1024**3)
        disk_info.update({
            'available': free_gb >= min_disk_gb,
            'free_gb': free_gb,
            'min_required_gb': min_disk_gb,
            'total_gb': usage.total / (1024**3),
            'used_gb': usage.used / (1024**3),
            'status': 'PASS' if free_gb >= min_disk_gb else 'FAIL',
            'required': True,
            'type': 'storage'
        })
    except Exception as e:
        disk_info.update({
            'available': False,
            'status': 'FAIL',
            'error': str(e),
            'required': True,
            'type': 'storage'
        })
    hardware_data['disk_space'] = disk_info
    
    # Enhanced CPU cores check with usage data
    cpu_info = {}
    try:
        logical_cores = psutil.cpu_count(logical=True)
        physical_cores = psutil.cpu_count(logical=False)
        
        # Get CPU frequency with proper error handling
        cpu_frequency = None
        cpu_frequency_ghz = 0.0
        cpu_frequency_mhz = 0.0
        try:
            cpu_freq = psutil.cpu_freq()
            if cpu_freq:
                cpu_frequency = cpu_freq._asdict()
                cpu_frequency_ghz = cpu_frequency.get('current', 0.0) / 1000 if cpu_frequency.get('current') else 0.0
                cpu_frequency_mhz = cpu_frequency.get('current', 0.0)
        except (AttributeError, NotImplementedError):
            # Some systems may not support CPU frequency monitoring
            pass
        
        cpu_info.update({
            'available': True,
            'logical_cores': logical_cores,
            'physical_cores': physical_cores,
            'hyperthreading': logical_cores != physical_cores if logical_cores and physical_cores else False,
            'status': 'PASS',
            'required': False,
            'type': 'processor',
            'capacity': {
                # Static capacity information
                'logical_cores': logical_cores,
                'physical_cores': physical_cores,
                'frequency_ghz': round(cpu_frequency_ghz, 2) if cpu_frequency_ghz else None,
                'frequency_mhz': round(cpu_frequency_mhz, 2) if cpu_frequency_mhz else None,
                'max_frequency_ghz': round(cpu_frequency.get('max', 0.0) / 1000, 2) if cpu_frequency and cpu_frequency.get('max') else None,
                'min_frequency_ghz': round(cpu_frequency.get('min', 0.0) / 1000, 2) if cpu_frequency and cpu_frequency.get('min') else None
            }
        })
        
        # Add current CPU usage if requested
        if include_memory_usage:
            try:
                # Get CPU usage percentages
                # Shorter interval for responsiveness
                cpu_percent = psutil.cpu_percent(interval=0.1)
                
                # Get CPU times
                cpu_times = None
                try:
                    cpu_times_percent = psutil.cpu_times_percent(interval=0.1)
                    if cpu_times_percent:
                        cpu_times = cpu_times_percent._asdict()
                except (AttributeError, NotImplementedError):
                    pass
                
                # Get CPU stats
                cpu_stats_data = None
                try:
                    cpu_stats = psutil.cpu_stats()
                    if cpu_stats:
                        cpu_stats_data = cpu_stats._asdict()
                except (AttributeError, NotImplementedError):
                    pass
                
                # Get per-core usage
                per_cpu_percent = None
                try:
                    per_cpu_percent = psutil.cpu_percent(interval=0.1, percpu=True)
                except (AttributeError, NotImplementedError):
                    pass
                
                cpu_info['current_usage'] = {
                    'timestamp': datetime.now().isoformat(),
                    'cpu_percent_total': cpu_percent,
                    'cpu_percent_per_core': per_cpu_percent,
                    'cpu_times': cpu_times,
                    'cpu_stats': cpu_stats_data,
                    'frequency_current_ghz': round(cpu_frequency_ghz, 2) if cpu_frequency_ghz else None,
                    'frequency_current_mhz': round(cpu_frequency_mhz, 2) if cpu_frequency_mhz else None
                }
                
            except Exception as usage_error:
                cpu_info['current_usage_error'] = str(usage_error)
        
    except Exception as e:
        cpu_info.update({
            'available': False,
            'status': 'FAIL',
            'error': str(e),
            'required': False,
            'type': 'processor'
        })
    hardware_data['cpu_cores'] = cpu_info
    
    # Enhanced System RAM check with usage data
    ram_info = {}
    try:
        ram = psutil.virtual_memory()
        swap = psutil.swap_memory()
        
        ram_info.update({
            'available': True,
            'ram_total_gb': ram.total / (1024**3),
            'ram_available_gb': ram.available / (1024**3),
            'ram_used_gb': ram.used / (1024**3),
            'ram_percent': ram.percent,
            'swap_total_gb': swap.total / (1024**3),
            'swap_used_gb': swap.used / (1024**3),
            'swap_percent': swap.percent,
            'status': 'PASS',
            'required': False,
            'type': 'memory',
            'capacity': {
                # Static capacity information
                'total_bytes': ram.total,
                'total_gb': ram.total / (1024**3),
                'swap_total_bytes': swap.total,
                'swap_total_gb': swap.total / (1024**3)
            }
        })
        
        # Add current usage data if requested
        if include_memory_usage:
            ram_info['current_usage'] = {
                'timestamp': datetime.now().isoformat(),
                'available_bytes': ram.available,
                'used_bytes': ram.used,
                'percent_used': ram.percent,
                'available_gb': ram.available / (1024**3),
                'used_gb': ram.used / (1024**3),
                'swap_used_bytes': swap.used,
                'swap_used_gb': swap.used / (1024**3),
                'swap_percent': swap.percent
            }
            
    except Exception as e:
        ram_info.update({
            'available': False,
            'status': 'FAIL',
            'error': str(e),
            'required': False,
            'type': 'memory'
        })
    hardware_data['system_ram'] = ram_info
    
    # System architecture check
    arch_info = {}
    try:
        arch_info.update({
            'available': True,
            'architecture': platform.architecture()[0],
            'machine': platform.machine(),
            'system': platform.system(),
            'release': platform.release(),
            'processor': platform.processor(),
            'python_build': ' '.join(platform.python_build()),
            'word_size': platform.architecture()[0],
            'status': 'PASS',
            'required': False,
            'type': 'platform'
        })
    except Exception as e:
        arch_info.update({
            'available': False,
            'status': 'FAIL',
            'error': str(e),
            'required': False,
            'type': 'platform'
        })
    hardware_data['system_architecture'] = arch_info
    
    # Enhanced CUDA check with memory capacity
    cuda_info = {}
    try:
        cuda_available = torch.cuda.is_available()
        cuda_info.update({
            'available': cuda_available,
            'status': 'PASS' if cuda_available else 'WARN',
            'required': False,
            'type': 'gpu'
        })
        
        if cuda_available:
            cuda_info.update({
                'cuda_version': torch.version.cuda,
                'cudnn_version': torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'N/A',
                'gpu_count': torch.cuda.device_count(),
                'gpus': []
            })
            
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                gpu_data = {
                    'name': props.name,
                    'compute_capability': f"{props.major}.{props.minor}",
                    'memory_gb': props.total_memory / (1024**3),
                    'total_memory_bytes': props.total_memory,
                    'multiprocessors': props.multi_processor_count,
                    'capacity': {
                        # Static memory capacity
                        'total_bytes': props.total_memory,
                        'total_gb': props.total_memory / (1024**3)
                    }
                }
                
                # Add current GPU memory usage if requested
                if include_memory_usage:
                    try:
                        torch.cuda.set_device(i)
                        gpu_data['current_usage'] = {
                            'timestamp': datetime.now().isoformat(),
                            'allocated_bytes': torch.cuda.memory_allocated(i),
                            'reserved_bytes': torch.cuda.memory_reserved(i),
                            'max_allocated_bytes': torch.cuda.max_memory_allocated(i),
                            'allocated_mb': torch.cuda.memory_allocated(i) / (1024**2),
                            'reserved_mb': torch.cuda.memory_reserved(i) / (1024**2),
                            'max_allocated_mb': torch.cuda.max_memory_allocated(i) / (1024**2),
                            'percent_allocated': (torch.cuda.memory_allocated(i) / props.total_memory * 100) if props.total_memory > 0 else 0
                        }
                    except Exception as e:
                        gpu_data['current_usage_error'] = str(e)
                
                cuda_info['gpus'].append(gpu_data)
        else:
            cuda_info['details'] = "CUDA not available - Using CPU"
            
    except ImportError:
        cuda_info.update({
            'available': False,
            'status': 'WARN',
            'details': "PyTorch not installed",
            'required': False,
            'type': 'gpu'
        })
    except Exception as e:
        cuda_info.update({
            'available': False,
            'status': 'FAIL',
            'error': str(e),
            'required': False,
            'type': 'gpu'
        })
    hardware_data['cuda'] = cuda_info
    
    # Add process memory usage if requested
    if include_memory_usage:
        try:
            proc = psutil.Process()
            process_mem = proc.memory_info()
            hardware_data['process_memory'] = {
                'timestamp': datetime.now().isoformat(),
                'rss_bytes': process_mem.rss,
                'vms_bytes': process_mem.vms,
                'rss_mb': process_mem.rss / (1024**2),
                'vms_mb': process_mem.vms / (1024**2),
                'percent': proc.memory_percent(),
                # Informational, not a pass/fail check
                'status': 'INFO',
                'type': 'process'
            }
        except Exception as e:
            hardware_data['process_memory'] = {
                'error': str(e),
                'status': 'FAIL',
                'type': 'process'
            }
    
    return hardware_data

def check_hardware_wrapper(min_disk_gb: float = 1.0, include_memory_usage: bool = False) -> CheckResult:
    """
    High-level wrapper and presentation layer for hardware checks.
    Formats the raw data and produces a consistent CheckResult.
    
    Args:
        min_disk_gb: Minimum required disk space in GB
        include_memory_usage: Whether to include current memory usage statistics
    """
    try:
        # Get raw hardware data
        hardware_data = check_hardware(min_disk_gb, include_memory_usage)
        
        # Analyze results and build summary
        summary = {
            'total': len(hardware_data),
            'passed': 0,
            'warnings': 0,
            'failed': 0,
            'required_failed': 0,
            'informational': 0
        }
        
        details_lines = []
        overall_passed = True
        
        # Create rich table for formatted output
        table_title = "Hardware/System Check Results" + (" with Memory Usage" if include_memory_usage else "")
        table = Table(title=table_title, show_header=True, header_style="bold magenta")
        table.add_column("Component", style="cyan", no_wrap=True)
        table.add_column("Status", justify="center")
        table.add_column("Details", style="green")
        
        for component, data in hardware_data.items():
            status = data.get('status', 'UNKNOWN')
            
            # Update summary counts
            if status == 'PASS':
                summary['passed'] += 1
            elif status == 'WARN':
                summary['warnings'] += 1
            elif status == 'FAIL':
                summary['failed'] += 1
                if data.get('required', False):
                    summary['required_failed'] += 1
                    overall_passed = False
            elif status == 'INFO':
                summary['informational'] += 1
            
            # Build details string
            comp_details = format_component_details(component, data, include_memory_usage)
            details_lines.append(comp_details)
            
            # Add to rich table
            status_icon = get_status_icon(status)
            table.add_row(component.replace('_', ' ').title(), status_icon, comp_details)
        
        # Build final details string
        details_summary = f"Hardware Check Summary:\n"
        details_summary += f"Total Checks: {summary['total']}, "
        details_summary += f"Passed: {summary['passed']}, "
        details_summary += f"Warnings: {summary['warnings']}, "
        details_summary += f"Failed: {summary['failed']}"
        if include_memory_usage:
            details_summary += f", Informational: {summary['informational']}"
        details_summary += f"\n\n"
        details_summary += "\n".join(details_lines)
        
        # Create CheckResult
        result = CheckResult(
            passed=overall_passed,
            message="Hardware/System check completed" + (" with memory usage" if include_memory_usage else ""),
            level=CheckLevel.IMPORTANT if summary['required_failed'] > 0 else CheckLevel.INFORMATIONAL,
            details=details_summary
        )
        
        # Add metadata
        result.with_metadata({
            'hardware_data': hardware_data,
            'summary': summary,
            'rich_table': table,
            'include_memory_usage': include_memory_usage
        })
        
        return result
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Hardware check failed unexpectedly",
            level=CheckLevel.CRITICAL,
            details=str(e)
        ).with_exception(e)

def format_component_details(component: str, data: Dict[str, Any], include_memory_usage: bool = False) -> str:
    """Format component details for human-readable output."""
    status = data.get('status', 'UNKNOWN')
    
    if component == 'disk_space':
        if status == 'PASS':
            return f"[OK] Free: {data['free_gb']:.1f}GB (Required: {data['min_required_gb']}GB)"
        elif status == 'FAIL':
            if 'error' in data:
                return f"[X] Error: {data['error']}"
            else:
                return f"[X] Insufficient: {data['free_gb']:.1f}GB (Required: {data['min_required_gb']}GB)"
    
    elif component == 'cpu_cores':
        if status == 'PASS':
            ht_status = "Enabled" if data['hyperthreading'] else "Disabled"
            return f"[OK] {data['logical_cores']} logical, {data['physical_cores']} physical cores ({ht_status})"
        else:
            return f"[X] Error: {data.get('error', 'Unknown error')}"
    
    elif component == 'system_ram':
        if status == 'PASS':
            base_info = f"[OK] RAM: {data['ram_total_gb']:.1f}GB total"
            if include_memory_usage and 'current_usage' in data:
                usage = data['current_usage']
                base_info += f", {usage['used_gb']:.1f}GB used ({usage['percent_used']:.1f}%)"
            else:
                base_info += f", {data['ram_available_gb']:.1f}GB available"
            return base_info
        else:
            return f"[X] Error: {data.get('error', 'Unknown error')}"
    
    elif component == 'system_architecture':
        if status == 'PASS':
            return f"[OK] {data['system']} {data['release']} ({data['architecture']})"
        else:
            return f"[X] Error: {data.get('error', 'Unknown error')}"
    
    elif component == 'cuda':
        if status == 'PASS':
            gpu_count = data.get('gpu_count', 0)
            if gpu_count == 0:
                return "[OK] CUDA available (no GPUs detected)"
            
            gpu_info = f"{gpu_count} GPU(s): "
            gpu_details = []
            for i, gpu in enumerate(data.get('gpus', [])):
                gpu_text = gpu['name']
                if include_memory_usage and 'current_usage' in gpu:
                    usage = gpu['current_usage']
                    gpu_text += f" ({usage['allocated_mb']:.0f}MB/{gpu['memory_gb']:.1f}GB)"
                gpu_details.append(gpu_text)
            
            gpu_info += ", ".join(gpu_details)
            return f"[OK] CUDA available - {gpu_info}"
        elif status == 'WARN':
            return f"[i] {data.get('details', 'CUDA not available')}"
        else:
            return f"[X] Error: {data.get('error', 'Unknown error')}"
    
    elif component == 'process_memory':
        if status == 'INFO' and include_memory_usage:
            return f"Process: {data['rss_mb']:.1f}MB RSS, {data['vms_mb']:.1f}MB VMS ({data['percent']:.1f}%)"
        elif status == 'FAIL':
            return f"[X] Process memory error: {data.get('error', 'Unknown error')}"
        else:
            return "Process memory info not available"
    
    return f"[?] Unknown component: {component}"

def get_status_icon(status: str) -> str:
    """Get status icon for rich table."""
    icons = {
        'PASS': '[green][OK][/green]',
        'WARN': '[yellow][!][/yellow]',
        'FAIL': '[red][X][/red]',
        'INFO': '[blue][i][/blue]',
        'UNKNOWN': '[grey][?][/grey]'
    }
    return icons.get(status, '[grey]?[/grey]')

def check_seed_config(hardware_data: Optional[Dict[str, Any]] = None) -> CheckResult:
    """
    Verify that reproducibility seeds and related configurations are set.

    Checks:
    - PYTHONHASHSEED environment variable set and numeric
    - CUBLAS_WORKSPACE_CONFIG set to ':4096:8'
    - NumPy RNG available and seeded
    - PyTorch RNG available, CUDA deterministic mode on, benchmark off
    - TensorFlow RNG available (if installed) and seeded
    - Hardware-aware CUDA configurations
    - Per-GPU seed validation for multi-GPU setups
    - Produces compliance score (0-100%)
    - Human-readable feedback with colorized PASS/WARN/FAIL
    - Auto-disables colors if not in a TTY
    - Structured details for programmatic use
    - Graceful handling and exception safety
    
    Args:
        hardware_data: Hardware information from check_hardware() (optional)
        
    Returns:
        CheckResult with detailed seed configuration status
    """
    try:
        # Get hardware info if not provided
        if hardware_data is None:
            try:
                hardware_data = check_hardware(include_memory_usage=False)
            except Exception:
                hardware_data = {}
        
        # Extract hardware context
        cuda_info = hardware_data.get('cuda', {})
        cuda_available = cuda_info.get('available', False)
        gpu_count = cuda_info.get('gpu_count', 0)
        gpus = cuda_info.get('gpus', [])
        
        # Initialize base result
        base_result = CheckResult(
            passed=False,
            message="Reproducibility configuration check",
            level=CheckLevel.IMPORTANT
        )
        
        # Initialize check data with hardware awareness
        check_data: Dict[str, any] = {
            'hardware_context': {
                'cuda_available': cuda_available,
                'gpu_count': gpu_count,
                'optimization_level': 'high' if cuda_available else 'standard'
            },
            'checks': {
                'PYTHONHASHSEED': {'passed': False, 'value': None, 'expected': 'numeric'},
                'CUBLAS_WORKSPACE_CONFIG': {'passed': False, 'value': None, 'expected': ':4096:8'},
                'numpy_rng': {'passed': False, 'value': None, 'seeded': False},
                'torch_rng': {'passed': False, 'value': None, 'seeded': False},
                'torch_cuda': {
                    'deterministic': False,
                    'benchmark': None,
                    'tf32_disabled': None,
                    'passed': False,
                    'per_gpu_seeds': [],
                    'global_seed_set': False
                },
                'tensorflow_rng': {'passed': False, 'value': None, 'seeded': False}
            },
            'weights': {
                'PYTHONHASHSEED': 20,
                'CUBLAS_WORKSPACE_CONFIG': 25 if cuda_available else 10,
                'numpy_rng': 15,
                'torch_rng': 15,
                'torch_cuda': 20 if cuda_available else 5,
                # Optional but beneficial
                'tensorflow_rng': 5
            },
            'feedback': [],
            'compliance_score': 0,
            'recommendations': []
        }
        
        # Check PYTHONHASHSEED
        hash_seed = os.environ.get('PYTHONHASHSEED')
        check_data['checks']['PYTHONHASHSEED']['value'] = hash_seed or "Not set"
        if hash_seed is not None and hash_seed.isdigit():
            check_data['checks']['PYTHONHASHSEED']['passed'] = True
        else:
            check_data['feedback'].append("PYTHONHASHSEED not set or invalid - required for hash reproducibility")
            check_data['recommendations'].append("Set PYTHONHASHSEED environment variable to a numeric value (e.g., '42')")
        
        # Check CUBLAS_WORKSPACE_CONFIG (critical for CUDA reproducibility)
        cublas_cfg = os.environ.get('CUBLAS_WORKSPACE_CONFIG')
        check_data['checks']['CUBLAS_WORKSPACE_CONFIG']['value'] = cublas_cfg or "Not set"
        if cublas_cfg == ':4096:8':
            check_data['checks']['CUBLAS_WORKSPACE_CONFIG']['passed'] = True
        else:
            if cuda_available:
                check_data['feedback'].append("CUBLAS_WORKSPACE_CONFIG not set to ':4096:8' - required for CUDA reproducibility")
                check_data['recommendations'].append("Set CUBLAS_WORKSPACE_CONFIG=':4096:8' for deterministic CUDA operations")
            else:
                # Less critical for CPU-only systems
                check_data['checks']['CUBLAS_WORKSPACE_CONFIG']['passed'] = True
        
        # Check NumPy RNG with seed validation
        try:
            # Test NumPy RNG availability
            test_array = np.random.rand(2)
            check_data['checks']['numpy_rng']['passed'] = True
            check_data['checks']['numpy_rng']['value'] = "Available"
            
            # Try to detect if seeded (basic check)
            # Temporary seed
            np.random.seed(12345)
            test1 = np.random.rand(5)
            # Same seed
            np.random.seed(12345)
            test2 = np.random.rand(5)
            if np.array_equal(test1, test2):
                check_data['checks']['numpy_rng']['seeded'] = True
            else:
                check_data['feedback'].append("NumPy RNG may not be properly seeded")
                check_data['recommendations'].append("Call np.random.seed(seed_value) to ensure reproducibility")
                
        except Exception as e:
            check_data['checks']['numpy_rng']['value'] = f"Error: {str(e)}"
            check_data['feedback'].append("NumPy RNG not available or malfunctioning")
        
        # Check PyTorch RNG with comprehensive validation
        try:
            # Test PyTorch RNG availability
            test_tensor = torch.rand(2)
            check_data['checks']['torch_rng']['passed'] = True
            check_data['checks']['torch_rng']['value'] = "Available"
            
            # Test seeding
            torch.manual_seed(12345)
            test1 = torch.rand(5)
            torch.manual_seed(12345)
            test2 = torch.rand(5)
            if torch.equal(test1, test2):
                check_data['checks']['torch_rng']['seeded'] = True
            else:
                check_data['feedback'].append("PyTorch RNG may not be properly seeded")
                check_data['recommendations'].append("Call torch.manual_seed(seed_value) for CPU reproducibility")
            
        except Exception as e:
            check_data['checks']['torch_rng']['value'] = f"Error: {str(e)}"
            check_data['feedback'].append("PyTorch RNG not available or malfunctioning")
        
        # Comprehensive CUDA reproducibility checks
        if cuda_available and gpu_count > 0:
            try:
                # Check deterministic mode
                check_data['checks']['torch_cuda']['deterministic'] = torch.backends.cudnn.deterministic
                if not torch.backends.cudnn.deterministic:
                    check_data['feedback'].append("CUDA deterministic mode disabled - may cause non-reproducible results")
                    check_data['recommendations'].append("Enable with torch.backends.cudnn.deterministic = True")
                
                # Check benchmark mode (should be disabled for reproducibility)
                check_data['checks']['torch_cuda']['benchmark'] = not torch.backends.cudnn.benchmark
                if torch.backends.cudnn.benchmark:
                    check_data['feedback'].append("CUDA benchmark mode enabled - may cause non-reproducible results")
                    check_data['recommendations'].append("Disable with torch.backends.cudnn.benchmark = False")
                
                # Check TF32 settings for modern GPUs
                tf32_properly_configured = True
                for gpu in gpus:
                    compute_cap = float(gpu.get('compute_capability', '0.0'))
                    # Tensor cores available
                    if compute_cap >= 7.0:
                        try:
                            tf32_disabled = not torch.backends.cuda.matmul.allow_tf32
                            check_data['checks']['torch_cuda']['tf32_disabled'] = tf32_disabled
                            if not tf32_disabled:
                                tf32_properly_configured = False
                                check_data['feedback'].append("TF32 enabled on capable GPU - may reduce reproducibility precision")
                                check_data['recommendations'].append("Disable with torch.backends.cuda.matmul.allow_tf32 = False for maximum reproducibility")
                        except AttributeError:
                            # TF32 control not available in this PyTorch version
                            pass
                        break
                
                # Test CUDA seeding
                if torch.cuda.is_available():
                    try:
                        torch.cuda.manual_seed(12345)
                        test1 = torch.cuda.FloatTensor(5).uniform_()
                        torch.cuda.manual_seed(12345)
                        test2 = torch.cuda.FloatTensor(5).uniform_()
                        if torch.equal(test1, test2):
                            check_data['checks']['torch_cuda']['global_seed_set'] = True
                        else:
                            check_data['feedback'].append("CUDA global seed may not be properly set")
                            check_data['recommendations'].append("Call torch.cuda.manual_seed_all(seed) for multi-GPU reproducibility")
                    except Exception as cuda_seed_error:
                        check_data['feedback'].append(f"CUDA seed test failed: {cuda_seed_error}")
                
                # Validate per-GPU seeding for multi-GPU setups
                if gpu_count > 1:
                    per_gpu_seeds_valid = []
                    for gpu_id in range(gpu_count):
                        try:
                            torch.cuda.set_device(gpu_id)
                            torch.cuda.manual_seed(12345 + gpu_id)
                            test_tensor = torch.cuda.FloatTensor(3).uniform_()
                            per_gpu_seeds_valid.append(True)
                        except Exception:
                            per_gpu_seeds_valid.append(False)
                            check_data['feedback'].append(f"Per-GPU seeding failed for GPU {gpu_id}")
                    
                    check_data['checks']['torch_cuda']['per_gpu_seeds'] = per_gpu_seeds_valid
                    if not all(per_gpu_seeds_valid):
                        check_data['recommendations'].append("Ensure per-GPU seeding with individual torch.cuda.manual_seed() calls")
                
                # Calculate CUDA check pass status
                cuda_checks_passed = (
                    check_data['checks']['torch_cuda']['deterministic'] and
                    check_data['checks']['torch_cuda']['benchmark'] and
                    check_data['checks']['torch_cuda']['global_seed_set']
                )
                
                # For multi-GPU, also require per-GPU seeds
                if gpu_count > 1:
                    cuda_checks_passed = cuda_checks_passed and all(check_data['checks']['torch_cuda']['per_gpu_seeds'])
                
                check_data['checks']['torch_cuda']['passed'] = cuda_checks_passed
                
            except Exception as cuda_error:
                check_data['feedback'].append(f"CUDA configuration check failed: {cuda_error}")
                check_data['checks']['torch_cuda']['passed'] = False
        else:
            # CPU-only system - CUDA checks not applicable
            check_data['checks']['torch_cuda']['passed'] = True
            # Not applicable for CPU
            check_data['checks']['torch_cuda']['deterministic'] = True
        
        # Check TensorFlow RNG (optional but comprehensive)
        try:
            version_info = check_versions(include_optional=False)
            tf_available = any('tensorflow' in str(v).lower() for v in version_info.values())
            
            if tf_available:
                import tensorflow as tf
                # Test TensorFlow RNG
                test_tensor = tf.random.uniform((2,))
                check_data['checks']['tensorflow_rng']['passed'] = True
                check_data['checks']['tensorflow_rng']['value'] = "Available"
                
                # Test seeding
                tf.random.set_seed(12345)
                test1 = tf.random.uniform((5,))
                tf.random.set_seed(12345)
                test2 = tf.random.uniform((5,))
                if tf.reduce_all(tf.equal(test1, test2)):
                    check_data['checks']['tensorflow_rng']['seeded'] = True
                else:
                    check_data['feedback'].append("TensorFlow RNG may not be properly seeded")
                    check_data['recommendations'].append("Call tf.random.set_seed(seed_value) for TensorFlow reproducibility")
            else:
                # TensorFlow not installed - not required
                check_data['checks']['tensorflow_rng']['passed'] = True
                check_data['checks']['tensorflow_rng']['value'] = "Not installed (optional)"
                
        except ImportError:
            check_data['checks']['tensorflow_rng']['value'] = "Not installed (optional)"
            # Not required
            check_data['checks']['tensorflow_rng']['passed'] = True
        except Exception as tf_error:
            check_data['checks']['tensorflow_rng']['value'] = f"Error: {str(tf_error)}"
            check_data['feedback'].append("TensorFlow RNG check failed")
        
        # Calculate weighted compliance score
        total_points = sum(check_data['weights'].values())
        earned_points = 0
        
        for check_name, weight in check_data['weights'].items():
            check_status = check_data['checks'][check_name]
            if check_status['passed']:
                earned_points += weight
            # Partial credit for some configurations
            elif check_name == 'torch_cuda' and cuda_available:
                # Partial credit based on individual CUDA settings
                partial_score = 0
                if check_status['deterministic']:
                    partial_score += 0.4
                if check_status['benchmark']:
                    partial_score += 0.3
                if check_status['global_seed_set']:
                    partial_score += 0.3
                earned_points += weight * partial_score
        
        compliance_score = round((earned_points / total_points) * 100, 2)
        
        # Determine pass threshold based on system type
        pass_threshold = 85 if cuda_available else 90
        passed = compliance_score >= pass_threshold
        
        # Add summary feedback
        if not passed:
            check_data['feedback'].append(f"Reproducibility score {compliance_score}% below {pass_threshold}% threshold")
            if cuda_available:
                check_data['recommendations'].append("CUDA systems require comprehensive seed configuration for full reproducibility")
            else:
                check_data['recommendations'].append("CPU systems should achieve >90% reproducibility compliance")
        
        # Prepare final details with hardware context
        details = {
            **check_data,
            'compliance_score': compliance_score,
            'pass_threshold': pass_threshold,
            'passed': passed,
            'summary': {
                'total_checks': len([c for c in check_data['checks'].values() if c.get('value') != "Not installed (optional)"]),
                'passed_checks': len([c for c in check_data['checks'].values() if c['passed'] and c.get('value') != "Not installed (optional)"]),
                'critical_issues': len([f for f in check_data['feedback'] if 'required' in f.lower() or 'critical' in f.lower()]),
                'recommendations_count': len(check_data['recommendations'])
            }
        }
        
        # Create status message with hardware context
        status_msg = f"Reproducibility configuration {'passed' if passed else 'failed'} ({compliance_score}%)"
        if cuda_available:
            status_msg += f" - CUDA system with {gpu_count} GPU(s)"
        else:
            status_msg += " - CPU-only system"
        
        return (
            base_result
            .with_passed(passed)
            .with_details(details)
            .with_message(status_msg)
            .with_metadata({
                'hardware_context': check_data['hardware_context'],
                'compliance_score': compliance_score,
                'recommendations': check_data['recommendations']
            })
        )
        
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Seed configuration check failed",
                level=CheckLevel.IMPORTANT
            )
            .with_details({
                'error': str(e),
                'compliance_score': 0,
                'feedback': ['Exception occurred during seed config check'],
                'hardware_context': hardware_data.get('cuda', {}) if hardware_data else {}
            })
            .with_exception(e)
        )

def check_core_dependencies() -> CheckResult:
    """
    Check core dependencies using the existing check_versions() function.
    Simplified to avoid redundancy.
    """
    try:
        # Use the comprehensive version check, but filter to core only
        version_info = check_versions(include_optional=False)
        
        # Filter only core dependencies (required=True)
        core_deps = {k: v for k, v in version_info.items() if v.get('required', False)}
        
        # Determine overall status
        passed = all(info['compatible'] for info in core_deps.values())
        
        # Prepare summary details
        compatible_count = sum(1 for info in core_deps.values() if info['compatible'])
        total_count = len(core_deps)
        
        details = f"Core Dependencies: {compatible_count}/{total_count} compatible"
        
        base_result = CheckResult(
            passed=passed,
            message=f"Core dependencies check {'passed' if passed else 'failed'}",
            level=CheckLevel.CRITICAL if not passed else CheckLevel.INFORMATIONAL
        )
        
        return (
            base_result
            .with_details(details)
            .with_metadata({
                'core_dependencies': core_deps,
                'summary': {
                    'total': total_count,
                    'compatible': compatible_count,
                    'incompatible': total_count - compatible_count
                }
            })
        )
        
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Core dependencies check failed",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Error checking core dependencies: {str(e)}")
            .with_exception(e)
        )

# Logging and Directory Setup
class UnicodeStreamHandler(logging.StreamHandler):
    """
    A logging StreamHandler that preserves Unicode output on Windows consoles,
    falling back to ASCII-safe output if encoding errors occur.
    """
    def emit(self, record):
        try:
            msg = self.format(record)
            if sys.platform == 'win32':
                try:
                    self.stream.write(msg + self.terminator)
                except UnicodeEncodeError:
                    # Fallback to ASCII-only output
                    msg = msg.encode('ascii', errors='replace').decode('ascii')
                    self.stream.write(msg + self.terminator)
            else:
                self.stream.write(msg + self.terminator)
            self.flush()
        except Exception:
            self.handleError(record)

# Setup logging configuration
def setup_logging(log_dir: Path = None) -> logging.Logger:
    """
    Configure the logger with:
    - UTF-8 file handler
    - Unicode-safe console handler (via UnicodeStreamHandler)

    Features:
    1. If log_dir is None, defaults to 'logs' folder next to this script.
    2. Adds handlers only if they don't already exist.
    3. Falls back to basic logging config if setup fails.
    """
    try:
        # Determine log_dir (default: script's directory / logs)
        if log_dir is None:
            log_dir = Path(__file__).resolve().parent / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)

        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)

        # File Handler
        log_file = log_dir / "deep_learning.log"
        file_handler_exists = any(
            isinstance(h, logging.FileHandler) and getattr(h, 'baseFilename', None) == str(log_file)
            for h in logger.handlers
        )
        if not file_handler_exists:
            file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
            #file_handler.setLevel(logging.DEBUG)
            file_formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)

        # Unicode-Safe Console Handler
        console_handler_exists = any(isinstance(h, UnicodeStreamHandler) for h in logger.handlers)
        if not console_handler_exists:
            console_handler = UnicodeStreamHandler(sys.stdout)
            console_handler.setLevel(logging.INFO)
            console_formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%H:%M:%S'
            )
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)

        return logger

    except Exception as e:
        # Fallback basic configuration if setup fails
        logging.basicConfig(
            level=logging.INFO,
            format='%(levelname)s - %(message)s'
        )
        logger = logging.getLogger(__name__)
        logger.warning(f"Failed to setup proper logging: {e}")
        return logger

# Setup directories and assign global directory variables
def setup_directories(logger: logging.Logger) -> Dict[str, Path]:
    """Create and return essential directories with versioned subdirectories."""
    base_dir = Path(__file__).resolve().parent
    
    dirs = {
        'artifacts': base_dir / "artifacts",
        'datasets': base_dir / "datasets",
        'figures': base_dir / "figures",
        'info': base_dir / "info",
        'metrics': base_dir / "metrics",
        'logs': base_dir / "logs",
        'models': base_dir / "models",
        'data': base_dir / "data",
        'config': base_dir / "config",
        'reports': base_dir / "reports",
        'tensorboard': base_dir / "tensorboard",
        'cache': base_dir / "cache",
        'exports': base_dir / "exports",
        'results': base_dir / "results",
        'checkpoints': base_dir / "checkpoints" / f"checkpoints_v{VERSION_INFO['torch']}"
    }
    
    # Create directories
    for name, path in dirs.items():
        try:
            if not path.exists():
                path.mkdir(parents=True, exist_ok=True)
                if logger:
                    logger.debug(f"Created directory: {path}")
        except PermissionError as e:
            if logger:
                logger.error(f"Permission denied creating directory {path}: {e}")
            raise
        except Exception as e:
            if logger:
                logger.error(f"Failed to create directory {path}: {e}")
            raise
    
    return dirs

def configure_directories(logger: logging.Logger) -> Dict[str, Path]:
    """
    Initialize and assign global directory path variables using
    setup_directories(), ensuring they are accessible across modules.
    """
    try:
        dirs = setup_directories(logger)
        
        # Assign to global variables if needed
        global LOG_DIR, DEFAULT_MODEL_DIR, DATA_DIR, CONFIG_DIR, REPORTS_DIR, TB_DIR, CACHE_DIR, EXPORTS_DIR, CHECKPOINTS_DIR, RESULTS_DIR, ARTIFACTS_DIR, DATASETS_DIR, FIGURES_DIR, INFO_DIR, METRICS_DIR
        ARTIFACTS_DIR = dirs['artifacts']
        DATASETS_DIR = dirs['datasets']
        FIGURES_DIR = dirs['figures']
        INFO_DIR = dirs['info']
        METRICS_DIR = dirs['metrics']
        LOG_DIR = dirs['logs']
        DEFAULT_MODEL_DIR = dirs['models']
        DATA_DIR = dirs['data']
        CONFIG_DIR = dirs['config']
        REPORTS_DIR = dirs['reports']
        TB_DIR = dirs['tensorboard']
        CACHE_DIR = dirs['cache']
        EXPORTS_DIR = dirs['exports']
        CHECKPOINTS_DIR = dirs['checkpoints']
        RESULTS_DIR = dirs['results']
        
        if logger:
            logger.info("Directory configuration completed successfully")
        
        return dirs
        
    except Exception as e:
        if logger:
            logger.critical(f"Directory configuration failed: {e}")
        raise

# Additional system initialization checks
def check_global_exception_handler() -> CheckResult:
    """
    Check if global exception handler is properly configured.
    Enhanced with comprehensive validation and configuration details.
    """
    try:
        # Test if our custom handler is set
        current_handler = sys.excepthook
        is_custom = current_handler.__name__ == 'enhanced_global_exception_handler'
        
        handler_info = {
            'handler_name': current_handler.__name__,
            'handler_module': getattr(current_handler, '__module__', 'unknown'),
            'is_enhanced': is_custom,
            'is_default': current_handler is sys.__excepthook__,
            'handler_doc': getattr(current_handler, '__doc__', '').strip()[:100] if hasattr(current_handler, '__doc__') else None
        }
        
        # Check if logging system is available
        logging_available = logger is not None and logger.handlers
        
        # Check if required directories exist
        log_dir_exists = LOG_DIR.exists() and LOG_DIR.is_dir()
        log_dir_writable = os.access(LOG_DIR, os.W_OK) if log_dir_exists else False
        
        # Comprehensive status check
        all_components_ready = is_custom and logging_available and log_dir_exists and log_dir_writable
        
        details = {
            'handler_info': handler_info,
            'logging_system': {
                'available': logging_available,
                'handlers_count': len(logger.handlers) if logger else 0
            },
            'log_directory': {
                'exists': log_dir_exists,
                'writable': log_dir_writable,
                'path': str(LOG_DIR)
            },
            'dependencies': {
                'psutil_available': 'psutil' in sys.modules,
                'torch_available': 'torch' in sys.modules,
                'rich_available': 'rich' in sys.modules,
                'json_available': 'json' in sys.modules
            },
            'all_components_ready': all_components_ready
        }
        
        # Determine result status
        if all_components_ready:
            message = "Enhanced global exception handler fully configured"
            level = CheckLevel.INFORMATIONAL
            passed = True
        elif is_custom:
            message = "Enhanced exception handler set but some components missing"
            level = CheckLevel.IMPORTANT
            passed = False
        else:
            message = "Using default exception handler"
            level = CheckLevel.IMPORTANT
            passed = False
        
        return CheckResult(
            passed=passed,
            message=message,
            level=level,
            details=details
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Failed to check exception handler",
            level=CheckLevel.IMPORTANT
        ).with_exception(e)

def check_performance_monitoring() -> CheckResult:
    """
    Check if performance monitoring is available and properly configured.
    Enhanced with comprehensive validation of monitoring capabilities.
    """
    try:
        monitoring_status = {
            'decorator_available': False,
            'wrapper_available': False,
            'dependencies': {},
            'hardware_integration': False,
            'logging_integration': False,
            'capabilities': {}
        }
        
        # Check if monitoring functions are available
        monitoring_status['decorator_available'] = 'enhanced_monitor_performance' in globals()
        monitoring_status['wrapper_available'] = 'performance_monitor_wrapper' in globals()
        
        # Check dependencies
        try:
            import time
            monitoring_status['dependencies']['time'] = True
        except ImportError:
            monitoring_status['dependencies']['time'] = False
        
        try:
            import psutil
            monitoring_status['dependencies']['psutil'] = True
        except ImportError:
            monitoring_status['dependencies']['psutil'] = False
        
        try:
            import torch
            monitoring_status['dependencies']['torch'] = True
        except ImportError:
            monitoring_status['dependencies']['torch'] = False
        
        try:
            from functools import wraps
            monitoring_status['dependencies']['functools'] = True
        except ImportError:
            monitoring_status['dependencies']['functools'] = False
        
        # Check hardware integration
        try:
            hw_data = check_hardware(include_memory_usage=False)
            monitoring_status['hardware_integration'] = True
            monitoring_status['hardware_context'] = {
                'cuda_available': hw_data.get('cuda', {}).get('available', False),
                'gpu_count': hw_data.get('cuda', {}).get('gpu_count', 0),
                'memory_monitoring': hw_data.get('system_ram', {}).get('available', False)
            }
        except Exception as e:
            monitoring_status['hardware_integration'] = False
            monitoring_status['hardware_error'] = str(e)
        
        # Check logging integration
        monitoring_status['logging_integration'] = logger is not None and logger.handlers
        
        # Determine monitoring capabilities
        all_deps_available = all(monitoring_status['dependencies'].values())
        basic_monitoring = (
            monitoring_status['decorator_available'] and 
            monitoring_status['wrapper_available'] and 
            all_deps_available
        )
        
        advanced_monitoring = (
            basic_monitoring and 
            monitoring_status['hardware_integration'] and 
            monitoring_status['logging_integration']
        )
        
        monitoring_status['capabilities'] = {
            'basic_timing': all_deps_available and monitoring_status['dependencies']['time'],
            'memory_monitoring': monitoring_status['dependencies']['psutil'],
            'gpu_monitoring': monitoring_status['dependencies']['torch'] and monitoring_status.get('hardware_context', {}).get('cuda_available', False),
            'hardware_aware': monitoring_status['hardware_integration'],
            'logging_integrated': monitoring_status['logging_integration'],
            'comprehensive': advanced_monitoring
        }
        
        # Test monitoring functionality
        if basic_monitoring:
            try:
                @enhanced_monitor_performance(include_memory=False, log_level=logging.DEBUG)
                def test_function():
                    time.sleep(0.001)  # 1ms test
                    return "test_result"
                
                result = test_function()
                test_passed = result == "test_result" and hasattr(test_function, '_performance_metrics')
                monitoring_status['test_results'] = {
                    'basic_test_passed': test_passed,
                    'metrics_stored': hasattr(test_function, '_performance_metrics'),
                    'metrics_count': len(getattr(test_function, '_performance_metrics', []))
                }
            except Exception as e:
                monitoring_status['test_results'] = {
                    'basic_test_passed': False,
                    'test_error': str(e)
                }
        
        # Determine overall status
        if advanced_monitoring:
            passed = True
            message = "Performance monitoring fully configured with hardware awareness"
            level = CheckLevel.INFORMATIONAL
        elif basic_monitoring:
            passed = True
            message = "Basic performance monitoring available"
            level = CheckLevel.INFORMATIONAL
        elif monitoring_status['decorator_available']:
            passed = False
            message = "Performance monitoring available but dependencies missing"
            level = CheckLevel.IMPORTANT
        else:
            passed = False
            message = "Performance monitoring not available"
            level = CheckLevel.IMPORTANT
        
        return CheckResult(
            passed=passed,
            message=message,
            level=level,
            details=monitoring_status
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Failed to check performance monitoring",
            level=CheckLevel.IMPORTANT
        ).with_exception(e)

def check_performance_baseline() -> CheckResult:
    """
    Check if performance baseline can be established and validate system performance.
    """
    try:
        logger.debug("Attempting to establish performance baseline")
        
        # Attempt to establish baseline
        baseline_results = establish_performance_baseline()
        
        # Check if baseline was successful
        if 'error' in baseline_results:
            return CheckResult(
                passed=False,
                message="Failed to establish performance baseline",
                level=CheckLevel.INFORMATIONAL,
                details={
                    'error': baseline_results['error'],
                    'timestamp': baseline_results.get('timestamp')
                }
            )
        
        # Analyze baseline results
        baselines = baseline_results.get('baselines', {})
        summary = baseline_results.get('summary', {})
        hardware_context = baseline_results.get('hardware_context', {})
        
        # Validate individual components
        component_status = {
            'cpu': 'cpu' in baselines and 'cpu_error' not in baselines,
            'memory': 'memory' in baselines and 'memory_error' not in baselines,
            'gpu': hardware_context.get('cuda_available', False) and 'gpu' in baselines and 'gpu_error' not in baselines,
            'io': 'io' in baselines and 'io_error' not in baselines
        }
        
        successful_components = sum(1 for status in component_status.values() if status)
        # CPU, Memory, I/O + GPU if available
        total_testable_components = 3 + (1 if hardware_context.get('cuda_available', False) else 0)
        
        # Performance analysis
        performance_analysis = {
            'cpu_performance': 'unknown',
            'memory_performance': 'unknown',
            'gpu_performance': 'unknown',
            'io_performance': 'unknown',
            'overall_score': 0
        }
        
        score = 0
        max_score = 0
        
        # CPU analysis
        if component_status['cpu']:
            cpu_data = baselines['cpu']
            gflops = cpu_data.get('gflops', 0)
            
            if gflops > 10:
                performance_analysis['cpu_performance'] = 'excellent'
                score += 4
            elif gflops > 5:
                performance_analysis['cpu_performance'] = 'good'
                score += 3
            elif gflops > 2:
                performance_analysis['cpu_performance'] = 'fair'
                score += 2
            else:
                performance_analysis['cpu_performance'] = 'limited'
                score += 1
            
            max_score += 4
        
        # Memory analysis
        if component_status['memory']:
            memory_data = baselines['memory']
            avg_speed = np.mean([
                baseline.get('allocation_speed_mbs', 0) 
                for baseline in memory_data.values() 
                if isinstance(baseline, dict) and 'allocation_speed_mbs' in baseline
            ])
            
            if avg_speed > 500:
                performance_analysis['memory_performance'] = 'excellent'
                score += 4
            elif avg_speed > 200:
                performance_analysis['memory_performance'] = 'good'
                score += 3
            elif avg_speed > 100:
                performance_analysis['memory_performance'] = 'fair'
                score += 2
            else:
                performance_analysis['memory_performance'] = 'limited'
                score += 1
            
            max_score += 4
        
        # GPU analysis
        if component_status['gpu']:
            gpu_data = baselines['gpu']
            max_gpu_gflops = max([
                gpu.get('gflops', 0) 
                for gpu in gpu_data.values() 
                if isinstance(gpu, dict) and 'gflops' in gpu
            ], default=0)
            
            if max_gpu_gflops > 1000:
                performance_analysis['gpu_performance'] = 'excellent'
                score += 6
            elif max_gpu_gflops > 500:
                performance_analysis['gpu_performance'] = 'good'
                score += 4
            elif max_gpu_gflops > 100:
                performance_analysis['gpu_performance'] = 'fair'
                score += 2
            else:
                performance_analysis['gpu_performance'] = 'limited'
                score += 1
            
            max_score += 6
        
        # I/O analysis
        if component_status['io']:
            io_data = baselines['io']
            write_speed = io_data.get('write_speed_mbs', 0)
            read_speed = io_data.get('read_speed_mbs', 0)
            avg_io_speed = (write_speed + read_speed) / 2
            
            if avg_io_speed > 200:
                performance_analysis['io_performance'] = 'excellent'
                score += 3
            elif avg_io_speed > 100:
                performance_analysis['io_performance'] = 'good'
                score += 2
            elif avg_io_speed > 50:
                performance_analysis['io_performance'] = 'fair'
                score += 1
            else:
                performance_analysis['io_performance'] = 'limited'
                score += 1
            
            max_score += 3
        
        # Calculate overall score
        if max_score > 0:
            performance_analysis['overall_score'] = round((score / max_score) * 100, 1)
        
        # Determine system classification
        if performance_analysis['overall_score'] > 80:
            system_class = 'high-performance'
            level = CheckLevel.INFORMATIONAL
        elif performance_analysis['overall_score'] > 60:
            system_class = 'good-performance'
            level = CheckLevel.INFORMATIONAL
        elif performance_analysis['overall_score'] > 40:
            system_class = 'fair-performance'
            level = CheckLevel.INFORMATIONAL
        else:
            system_class = 'limited-performance'
            level = CheckLevel.IMPORTANT
        
        # Create comprehensive details
        details = {
            'baseline_results': baseline_results,
            'component_status': component_status,
            'performance_analysis': performance_analysis,
            'system_classification': system_class,
            'successful_components': successful_components,
            'total_testable_components': total_testable_components,
            'completion_rate': f"{successful_components}/{total_testable_components}"
        }
        
        # Determine pass/fail
        # Allow one component to fail
        passed = successful_components >= (total_testable_components - 1)
        
        message = f"Performance baseline established - {system_class} system ({performance_analysis['overall_score']}% score)"
        
        return CheckResult(
            passed=passed,
            message=message,
            level=level,
            details=details
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Failed to check performance baseline",
            level=CheckLevel.INFORMATIONAL
        ).with_exception(e)

def check_memory_management() -> CheckResult:
    """
    Check if memory management functions are available and properly integrated.
    Enhanced with comprehensive validation of memory management capabilities.
    """
    try:
        memory_status = {
            'functions_available': {},
            'hardware_integration': False,
            'dependencies': {},
            'capabilities': {},
            'test_results': {}
        }
        
        # Check if memory management functions are available
        memory_status['functions_available']['enhanced_clear_memory'] = 'enhanced_clear_memory' in globals()
        memory_status['functions_available']['check_hardware'] = 'check_hardware' in globals()
        
        # Check dependencies
        try:
            import gc
            memory_status['dependencies']['gc'] = True
        except ImportError:
            memory_status['dependencies']['gc'] = False
        
        try:
            import psutil
            memory_status['dependencies']['psutil'] = True
        except ImportError:
            memory_status['dependencies']['psutil'] = False
        
        try:
            import torch
            memory_status['dependencies']['torch'] = True
        except ImportError:
            memory_status['dependencies']['torch'] = False
        
        # Check hardware integration
        if memory_status['functions_available']['check_hardware']:
            try:
                hw_data = check_hardware(include_memory_usage=True)
                memory_status['hardware_integration'] = True
                memory_status['hardware_context'] = {
                    'system_ram_gb': hw_data.get('system_ram', {}).get('ram_total_gb', 0),
                    'cuda_available': hw_data.get('cuda', {}).get('available', False),
                    'gpu_count': hw_data.get('cuda', {}).get('gpu_count', 0),
                    'memory_monitoring': 'current_usage' in hw_data.get('system_ram', {})
                }
            except Exception as e:
                memory_status['hardware_integration'] = False
                memory_status['hardware_error'] = str(e)
        
        # Determine capabilities
        all_deps_available = all(memory_status['dependencies'].values())
        basic_memory_mgmt = (
            memory_status['functions_available']['enhanced_clear_memory'] and 
            memory_status['dependencies']['gc']
        )
        
        advanced_memory_mgmt = (
            basic_memory_mgmt and 
            memory_status['dependencies']['psutil'] and 
            memory_status['hardware_integration']
        )
        
        cuda_memory_mgmt = (
            advanced_memory_mgmt and 
            memory_status['dependencies']['torch'] and 
            memory_status.get('hardware_context', {}).get('cuda_available', False)
        )
        
        memory_status['capabilities'] = {
            'basic_gc': memory_status['dependencies']['gc'],
            'process_memory_monitoring': memory_status['dependencies']['psutil'],
            'cuda_memory_management': cuda_memory_mgmt,
            'hardware_aware_clearing': advanced_memory_mgmt,
            'memory_usage_tracking': memory_status.get('hardware_context', {}).get('memory_monitoring', False),
            'comprehensive_management': cuda_memory_mgmt
        }
        
        # Test memory management functionality
        if basic_memory_mgmt:
            try:
                # Test basic memory clearing
                initial_objects = len(gc.get_objects())
                
                # Create some objects to clean up
                test_objects = [list(range(1000)) for _ in range(10)]
                objects_created = len(gc.get_objects()) - initial_objects
                
                # Clear memory
                del test_objects
                if memory_status['functions_available']['enhanced_clear_memory']:
                    clear_results = enhanced_clear_memory(aggressive=False)
                    test_success = clear_results.get('success', False)
                else:
                    gc.collect()
                    test_success = True
                
                final_objects = len(gc.get_objects())
                objects_cleaned = initial_objects + objects_created - final_objects
                
                memory_status['test_results'] = {
                    'basic_test_passed': test_success,
                    'objects_created': objects_created,
                    'objects_cleaned': objects_cleaned,
                    'cleanup_effectiveness': (objects_cleaned / objects_created * 100) if objects_created > 0 else 0
                }
                
                # Test advanced memory management if available
                if advanced_memory_mgmt:
                    try:
                        proc = psutil.Process()
                        initial_memory = proc.memory_info().rss / 1024 / 1024
                        
                        # Test with hardware awareness
                        clear_results = enhanced_clear_memory(aggressive=True)
                        
                        final_memory = proc.memory_info().rss / 1024 / 1024
                        memory_freed_mb = initial_memory - final_memory
                        
                        memory_status['test_results'].update({
                            'advanced_test_passed': clear_results.get('success', False),
                            'memory_freed_mb': memory_freed_mb,
                            'actions_taken': clear_results.get('actions_taken', []),
                            'hardware_context_used': 'hardware_context' in clear_results
                        })
                        
                    except Exception as e:
                        memory_status['test_results']['advanced_test_error'] = str(e)
                
            except Exception as e:
                memory_status['test_results']['basic_test_error'] = str(e)
        
        # Determine overall status
        if memory_status['capabilities']['comprehensive_management']:
            passed = True
            message = "Comprehensive memory management available with CUDA support"
            level = CheckLevel.INFORMATIONAL
        elif memory_status['capabilities']['hardware_aware_clearing']:
            passed = True
            message = "Hardware-aware memory management available"
            level = CheckLevel.INFORMATIONAL
        elif memory_status['capabilities']['basic_gc']:
            passed = True
            message = "Basic memory management available"
            level = CheckLevel.IMPORTANT
        else:
            passed = False
            message = "Memory management functions not available"
            level = CheckLevel.IMPORTANT
        
        # Add recommendations if needed
        recommendations = []
        if not memory_status['dependencies']['psutil']:
            recommendations.append("Install psutil for process memory monitoring")
        if not memory_status['dependencies']['torch'] and memory_status.get('hardware_context', {}).get('cuda_available', False):
            recommendations.append("Install PyTorch for CUDA memory management")
        if not memory_status['hardware_integration']:
            recommendations.append("Enable hardware integration for optimal memory management")
        
        memory_status['recommendations'] = recommendations
        
        return CheckResult(
            passed=passed,
            message=message,
            level=level,
            details=memory_status
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Failed to check memory management",
            level=CheckLevel.IMPORTANT
        ).with_exception(e)

def check_configuration_system() -> CheckResult:
    """Check if configuration system is properly initialized with enhanced detail collection."""
    try:
        # Suppress individual log messages during system checks
        config_logger = setup_logging(log_dir=LOG_DIR)
        original_level = config_logger.getEffectiveLevel()
        # Only show warnings/errors
        config_logger.setLevel(logging.WARNING)
        
        try:
            # Initialize configuration silently
            config = initialize_config()
            validate_config(config)
            
            # Collect detailed configuration information
            config_sections = {}
            total_params = 0
            
            for section_name, section_data in config.items():
                if isinstance(section_data, dict):
                    param_count = len(section_data)
                    total_params += param_count
                    
                    # Collect key parameters for each section
                    key_params = {}
                    if section_name == 'model':
                        key_params = {
                            'model_type': section_data.get('model_type', 'unknown'),
                            #'input_size': section_data.get('input_size', 'unknown'),
                            'input_dim': section_data.get('input_dim', 'unknown'),
                            #'num_classes': section_data.get('num_classes', 'unknown'),
                            'num_models': section_data.get('num_models', 'unknown'),
                            'dropout_rates': section_data.get('dropout_rates', 'unknown')
                        }
                    elif section_name == 'training':
                        key_params = {
                            'batch_size': section_data.get('batch_size', 'unknown'),
                            'learning_rate': section_data.get('learning_rate', 'unknown'),
                            'epochs': section_data.get('epochs', 'unknown'),
                            'optimizer': section_data.get('optimizer', 'unknown')
                        }
                    elif section_name == 'data':
                        key_params = {
                            'data_path': section_data.get('data_path', 'not_set'),
                            'artifacts_path': section_data.get('artifacts_path', 'not_set'),
                            'validation_split': section_data.get('validation_split', 'unknown'),
                            'preprocessing': section_data.get('preprocessing', 'none')
                        }
                    elif section_name == 'presets':
                        key_params = {
                            'current_preset': section_data.get('current_preset', 'default'),
                            'available_presets': list(section_data.get('available', {}).keys()) if 'available' in section_data else []
                        }
                    else:
                        # For other sections, show first few parameters
                        key_params = dict(list(section_data.items())[:3])
                    
                    config_sections[section_name] = {
                        'parameter_count': param_count,
                        'status': 'loaded',
                        'key_parameters': key_params
                    }
                else:
                    config_sections[section_name] = {
                        'parameter_count': 1,
                        'status': 'loaded',
                        'value': str(section_data)
                    }
            
            config_details = {
                'config_file_exists': CONFIG_FILE.exists(),
                'config_file_path': str(CONFIG_FILE),
                'active_preset': config.get('presets', {}).get('current_preset', 'default'),
                'model_type': config.get('model', {}).get('model_type', 'unknown'),
                'sections_loaded': len(config),
                'total_parameters': total_params,
                'section_details': config_sections,
                'validation_passed': True
            }
            
            return CheckResult(
                passed=True,
                message="Configuration system operational",
                level=CheckLevel.CRITICAL,
                details=config_details
            )
        finally:
            # Restore original logging level
            config_logger.setLevel(original_level)
            
    except ValueError as e:
        return CheckResult(
            passed=False,
            message=f"Configuration validation failed: {str(e)}",
            level=CheckLevel.CRITICAL,
            details={
                'config_file_exists': CONFIG_FILE.exists() if 'CONFIG_FILE' in globals() else False,
                'validation_error': str(e),
                'validation_passed': False
            }
        ).with_exception(e)
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Configuration system initialization failed",
            level=CheckLevel.CRITICAL,
            details={
                'config_file_exists': False,
                'initialization_error': str(e),
                'validation_passed': False
            }
        ).with_exception(e)

def check_configuration_system_wrapper() -> CheckResult:
    """
    Run comprehensive configuration system check with rich table output.
    
    Returns:
        CheckResult object with detailed configuration information and rich display
    """
    try:
        # Get the configuration check results
        config_result = check_configuration_system()
        
        # If the check failed, return it as-is
        if not config_result.passed:
            return config_result
        
        # Display the detailed configuration tables
        display_configuration_details(config_result)
        
        # Prepare summary details for the main table
        if isinstance(config_result.details, dict):
            summary_details = {
                'sections_loaded': config_result.details.get('sections_loaded', 0),
                'total_parameters': config_result.details.get('total_parameters', 0),
                'active_preset': config_result.details.get('active_preset', 'default'),
                'model_type': config_result.details.get('model_type', 'unknown'),
                'config_file_exists': config_result.details.get('config_file_exists', False),
                'validation_passed': config_result.details.get('validation_passed', False),
                # Include full details for potential future use
                'full_details': config_result.details
            }
        else:
            summary_details = config_result.details
        
        return CheckResult(
            passed=config_result.passed,
            message="Configuration system check with detailed breakdown",
            level=CheckLevel.CRITICAL,
            details=summary_details,
            metadata=config_result.metadata,
            exception=config_result.exception
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Configuration system wrapper check failed",
            level=CheckLevel.CRITICAL,
            details={
                'wrapper_error': str(e),
                'validation_passed': False
            }
        ).with_exception(e)

def display_configuration_details(config_result: CheckResult) -> None:
    """
    Display detailed configuration information in a rich table format.
    
    Args:
        config_result: CheckResult from check_configuration_system()
    """
    if not isinstance(config_result.details, dict):
        console.print("\n[bold yellow]No detailed configuration information available[/bold yellow]")
        return
    
    try:
        details = config_result.details
        
        # Main configuration overview table
        overview_table = Table(
            title="\n[bold yellow]Configuration System Overview[/bold yellow]",
            title_justify="left",
            box=box.ROUNDED,
            header_style="bold magenta",
            border_style="cyan",
            show_lines=True,
            width=min(100, console.width - 4)
        )
        
        overview_table.add_column("Property", style="bold yellow", width=20)
        overview_table.add_column("Value", style="bold magenta", width=30)
        overview_table.add_column("Status", style="bold green", width=15, justify="center")
        
        # Add overview rows
        overview_table.add_row(
            "Config File",
            str(details.get('config_file_path', 'Unknown')),
            "EXISTS" if details.get('config_file_exists', False) else "MISSING"
        )
        
        overview_table.add_row(
            "Active Preset",
            details.get('active_preset', 'default'),
            "ACTIVE"
        )
        
        overview_table.add_row(
            "Model Type",
            details.get('model_type', 'unknown'),
            "CONFIGURED"
        )
        
        overview_table.add_row(
            "Sections Loaded",
            str(details.get('sections_loaded', 0)),
            "LOADED"
        )
        
        overview_table.add_row(
            "Total Parameters",
            str(details.get('total_parameters', 0)),
            "COUNTED"
        )
        
        overview_table.add_row(
            "Validation",
            "Passed" if details.get('validation_passed', False) else "Failed",
            "PASS" if details.get('validation_passed', False) else "FAIL"
        )
        
        console.print(overview_table)
        console.print()
        
        # Section details table
        if 'section_details' in details and details['section_details']:
            sections_table = Table(
                title="[bold yellow]Configuration Sections Detail[/bold yellow]",
                title_justify="left",
                box=box.ROUNDED,
                header_style="bold magenta",
                border_style="cyan",
                show_lines=True,
                expand=True,
                width=min(120, console.width - 4)
            )
            
            sections_table.add_column("Section", style="bold yellow", width=15)
            sections_table.add_column("Parameters", style="bold magenta", width=12, justify="center")
            sections_table.add_column("Status", style="bold green", width=10, justify="center")
            sections_table.add_column("Key Configuration", style="bold", min_width=50)
            
            for section_name, section_info in details['section_details'].items():
                # Format key parameters
                key_config_lines = []
                
                if 'key_parameters' in section_info:
                    for param, value in section_info['key_parameters'].items():
                        if isinstance(value, list):
                            value_str = ', '.join(str(v) for v in value) if value else 'none'
                        else:
                            value_str = str(value)
                        
                        # Color coding for specific values
                        if param == 'current_preset':
                            key_config_lines.append(f"[bold white]{param}:[/] [bold green]{value_str}[/]")
                        elif param in ['model_type', 'optimizer']:
                            key_config_lines.append(f"[bold white]{param}:[/] [bold cyan]{value_str}[/]")
                        elif param in ['batch_size', 'learning_rate', 'epochs']:
                            key_config_lines.append(f"[bold white]{param}:[/] [bold yellow]{value_str}[/]")
                        elif 'unknown' in str(value) or 'not_set' in str(value):
                            key_config_lines.append(f"[bold white]{param}:[/] [bold red]{value_str}[/]")
                        else:
                            key_config_lines.append(f"[bold white]{param}:[/] [bold blue]{value_str}[/]")
                
                elif 'value' in section_info:
                    key_config_lines.append(f"[bold]Value: {section_info['value']}[/bold]")
                
                key_config_text = "\n".join(key_config_lines) if key_config_lines else "[bold red]No key parameters[/bold red]"
                
                sections_table.add_row(
                    Text(section_name.upper(), style="bold yellow"),
                    str(section_info.get('parameter_count', 0)),
                    Text(section_info.get('status', 'unknown').upper(), style="bold green"),
                    key_config_text
                )
            
            console.print(sections_table)
        
        # Error information if validation failed
        if not details.get('validation_passed', True):
            error_table = Table(
                title="[bold red]Configuration Errors[/bold red]",
                title_justify="left",
                box=box.ROUNDED,
                header_style="bold bright_white",
                border_style="red",
                width=min(100, console.width - 4)
            )
            
            error_table.add_column("Error Type", style="bold red", width=20)
            error_table.add_column("Details", style="bright_white")
            
            if 'validation_error' in details:
                error_table.add_row("Validation Error", details['validation_error'])
            
            if 'initialization_error' in details:
                error_table.add_row("Initialization Error", details['initialization_error'])
            
            console.print()
            console.print(error_table)
    
    except Exception as e:
        console.print(f"[bold red]Error displaying configuration details: {str(e)}[/bold red]")

def check_model_variants() -> CheckResult:
    """Check if model variants are properly initialized with enhanced detail collection."""
    try:
        # Initialize model variants silently using the enhanced function
        initialize_model_variants(silent=True)
        
        if not MODEL_VARIANTS:
            return CheckResult(
                passed=False,
                message="No model variants available after initialization",
                level=CheckLevel.CRITICAL,
                details={
                    'total_variants': 0,
                    'available_variants': 0,
                    'variant_names': [],
                    'variant_status': {},
                    'initialization_passed': False,
                    'error': 'MODEL_VARIANTS dictionary is empty after initialization',
                    'recommendation': 'Check model dependencies and configuration'
                }
            )
        
        # Validate model variants silently using the enhanced function
        variant_status = validate_model_variants(logger, silent=True)
        
        # Enhanced status categorization based on updated validation function
        available_variants = [
            name for name, status in variant_status.items() 
            if status == 'available'
        ]
        warning_variants = [
            name for name, status in variant_status.items() 
            if status.startswith('warning')
        ]
        failed_variants = [
            name for name, status in variant_status.items() 
            if status.startswith('error') or 'failed' in status or status == 'class_not_found' or status == 'class_not_callable'
        ]
        unknown_variants = [
            name for name, status in variant_status.items() 
            if name not in available_variants + warning_variants + failed_variants
        ]
        
        # Collect comprehensive variant information
        variant_details = {}
        initialization_metrics = {}
        hardware_info = {}
        
        try:
            # Get hardware context for detailed reporting
            hardware_info = check_hardware(include_memory_usage=True)
        except Exception as e:
            hardware_info = {'error': str(e)}
        
        for variant_name, variant_class in MODEL_VARIANTS.items():
            variant_info = {
                'class_name': variant_class.__name__ if variant_class else 'None',
                'status': variant_status.get(variant_name, 'unknown'),
                'available': variant_name in available_variants,
                'has_warnings': variant_name in warning_variants,
                'failed': variant_name in failed_variants
            }
            
            # Enhanced metadata collection
            try:
                # Get class documentation
                if hasattr(variant_class, '__doc__') and variant_class.__doc__:
                    doc_lines = [line.strip() for line in variant_class.__doc__.split('\n') if line.strip()]
                    variant_info['description'] = doc_lines[0] if doc_lines else f"{variant_class.__name__} model variant"
                else:
                    variant_info['description'] = f"{variant_class.__name__} model variant"
                
                # Check for configuration methods
                config_methods = {}
                if hasattr(variant_class, 'get_default_config'):
                    try:
                        default_config = variant_class.get_default_config()
                        if isinstance(default_config, dict):
                            config_methods['default_config'] = True
                            variant_info['config_parameters'] = len(default_config)
                            variant_info['key_config'] = {
                                k: v for k, v in list(default_config.items())[:3]
                            }
                        else:
                            variant_info['config_parameters'] = 'invalid_format'
                            variant_info['key_config'] = {}
                    except Exception as e:
                        variant_info['config_parameters'] = 'error'
                        variant_info['key_config'] = {}
                        variant_info['config_error'] = str(e)
                else:
                    variant_info['config_parameters'] = 'none'
                    variant_info['key_config'] = {}
                
                # Check for other common model methods
                for method_name in ['encode', 'decode', 'forward', 'train', 'eval']:
                    if hasattr(variant_class, method_name):
                        config_methods[method_name] = True
                    else:
                        config_methods[method_name] = False
                
                variant_info['supported_methods'] = config_methods
                
                # Analyze initialization signature
                if hasattr(variant_class, '__init__'):
                    import inspect
                    try:
                        sig = inspect.signature(variant_class.__init__)
                        required_params = [
                            p.name for p in sig.parameters.values() 
                            if p.name != 'self' and p.default == inspect.Parameter.empty
                        ]
                        optional_params = [
                            p.name for p in sig.parameters.values() 
                            if p.name != 'self' and p.default != inspect.Parameter.empty
                        ]
                        
                        variant_info['init_parameters'] = {
                            'required': len(required_params),
                            'optional': len(optional_params),
                            'total': len(required_params) + len(optional_params),
                            'required_names': required_params[:5]  # First 5 required params
                        }
                    except (ValueError, TypeError):
                        variant_info['init_parameters'] = 'signature_unavailable'
                else:
                    variant_info['init_parameters'] = 'no_init_method'
                    
                # Check for model-specific features
                model_features = []
                if variant_name == 'EnhancedAutoencoder':
                    model_features.extend(['advanced_architecture', 'potential_attention'])
                elif variant_name == 'AutoencoderEnsemble':
                    model_features.extend(['multiple_models', 'diversity_mechanisms'])
                elif variant_name == 'SimpleAutoencoder':
                    model_features.extend(['basic_architecture', 'minimal_dependencies'])
                
                variant_info['model_features'] = model_features
                
                # Check device compatibility awareness
                device_aware = any(hasattr(variant_class, attr) for attr in ['device', 'to', 'cuda', 'cpu'])
                variant_info['device_aware'] = device_aware
                
            except Exception as e:
                variant_info['description'] = f"Error analyzing {variant_class.__name__ if variant_class else 'unknown'}"
                variant_info['analysis_error'] = str(e)
                variant_info['config_parameters'] = 'error'
                variant_info['init_parameters'] = 'error'
            
            variant_details[variant_name] = variant_info
        
        # Collect initialization metrics from the validation results
        try:
            # Extract metrics from validation status messages
            validation_metrics = {
                'available_count': len(available_variants),
                'warning_count': len(warning_variants),
                'failed_count': len(failed_variants),
                'unknown_count': len(unknown_variants),
                'total_validated': len(variant_status),
                'success_rate': len(available_variants) / len(MODEL_VARIANTS) * 100 if MODEL_VARIANTS else 0
            }
            
            # Proper success determination logic
            passed = len(available_variants) > 0  # Pass if we have ANY working variants
            fully_operational = len(available_variants) > 0 and len(failed_variants) == 0
            
            # Determine message and level based on results
            if fully_operational:
                message = f"Model variants system fully operational ({len(available_variants)} variants available)"
            elif len(available_variants) > 0:
                message = f"Model variants system operational with warnings ({len(available_variants)} available, {len(warning_variants)} with warnings)"
            elif len(warning_variants) > 0:
                message = f"Model variants system partially operational ({len(warning_variants)} variants with warnings)"
            else:
                message = "Model variants system failed - no operational variants"
            
        except Exception as metrics_error:
            initialization_metrics['metrics_error'] = str(metrics_error)
            passed = False
            message = "Error collecting initialization metrics"
            level = CheckLevel.CRITICAL
        
        # Compile comprehensive details
        model_variants_details = {
            'total_variants': len(MODEL_VARIANTS),
            'available_variants': len(available_variants),
            'warning_variants': len(warning_variants),
            'failed_variants': len(failed_variants),
            'unknown_variants': len(unknown_variants),
            'variant_names': list(MODEL_VARIANTS.keys()),
            'available_names': available_variants,
            'warning_names': warning_variants,
            'failed_names': failed_variants,
            'unknown_names': unknown_variants,
            'variant_status': variant_status,
            'variant_details': variant_details,
            'initialization_passed': len(available_variants) > 0,
            'hardware_context': hardware_info,
            'initialization_summary': {
                'attempted': len(MODEL_VARIANTS),
                'successful': len(available_variants),
                'warnings': len(warning_variants),
                'failed': len(failed_variants),
                'unknown': len(unknown_variants),
                'success_rate': len(available_variants) / len(MODEL_VARIANTS) * 100 if MODEL_VARIANTS else 0,
                'operational_status': 'fully_operational' if fully_operational else 'partial' if passed else 'failed'
            },
            'recommendations': {
                'primary_models': available_variants,
                'fallback_models': warning_variants,
                'models_to_avoid': failed_variants,
                'emergency_fallback': 'SimpleAutoencoder' if 'SimpleAutoencoder' in MODEL_VARIANTS else available_variants[0] if available_variants else None
            }
        }
        
        # Add operational capabilities summary
        capabilities = []
        for variant_name in available_variants + warning_variants:
            if variant_name == 'SimpleAutoencoder':
                capabilities.append('basic_autoencoding')
            elif variant_name == 'EnhancedAutoencoder':
                capabilities.append('advanced_features')
            elif variant_name == 'AutoencoderEnsemble':
                capabilities.append('ensemble_modeling')
            else:
                capabilities.append('custom_implementation')
        
        model_variants_details['capabilities'] = list(set(capabilities))
        
        return CheckResult(
            passed=passed,
            message=message,
            level=CheckLevel.CRITICAL,
            details=model_variants_details
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Model variants system check failed completely",
            level=CheckLevel.CRITICAL,
            details={
                'total_variants': 0,
                'available_variants': 0,
                'variant_names': [],
                'initialization_passed': False,
                'initialization_error': str(e),
                'error_type': type(e).__name__,
                'recommendation': 'Check system dependencies and configuration files'
            }
        ).with_exception(e)

def check_model_variants_wrapper() -> CheckResult:
    """
    Run comprehensive model variants check with rich table output.
    
    This wrapper has been updated to fully leverage the enhanced check_model_variants()
    implementation while maintaining rich display capabilities and comprehensive error handling.
    
    Returns:
        CheckResult object with detailed model variants information and rich display
    """
    try:
        # Get the enhanced model variants check results
        variants_result = check_model_variants()
        
        # If the check failed completely, return it as-is
        if not variants_result.passed and not variants_result.details.get('initialization_passed', False):
            return variants_result
        
        # Display the comprehensive model variants tables with enhanced formatting
        display_model_variants_details(variants_result)
        
        # Prepare enhanced summary details for the main table
        if isinstance(variants_result.details, dict):
            details = variants_result.details
            
            # Enhanced metrics extraction with safe type checking
            total_variants = details.get('total_variants', 0)
            
            # Safe extraction of available variants count
            available_variants_raw = details.get('available_variants', 0)
            if isinstance(available_variants_raw, list):
                available_variants = len(available_variants_raw)
            elif isinstance(available_variants_raw, int):
                available_variants = available_variants_raw
            else:
                available_variants = 0
            
            # Safe extraction of warning variants count
            warning_variants_raw = details.get('warning_variants', [])
            if isinstance(warning_variants_raw, list):
                warning_variants = len(warning_variants_raw)
            else:
                warning_variants = 0
            
            # Safe extraction of failed variants count
            failed_variants_raw = details.get('failed_variants', 0)
            if isinstance(failed_variants_raw, list):
                failed_variants = len(failed_variants_raw)
            elif isinstance(failed_variants_raw, int):
                failed_variants = failed_variants_raw
            else:
                failed_variants = 0
            
            # Enhanced success rate calculation
            initialization_summary = details.get('initialization_summary', {})
            success_rate = initialization_summary.get('success_rate', 0)
            operational_status = initialization_summary.get('operational_status', 'unknown')
            
            # Enhanced capabilities tracking
            capabilities = details.get('capabilities', [])
            if not isinstance(capabilities, list):
                capabilities = []
            
            # Hardware context for system awareness
            hardware_context = details.get('hardware_context', {})
            
            summary_details = {
                'total_variants': total_variants,
                'available_variants': available_variants,
                'warning_variants': warning_variants,
                'failed_variants': failed_variants,
                'variant_names': details.get('variant_names', []),
                'available_names': details.get('available_names', []),
                'warning_names': details.get('warning_names', []),
                'failed_names': details.get('failed_names', []),
                'success_rate': success_rate,
                'operational_status': operational_status,
                'initialization_passed': details.get('initialization_passed', False),
                'capabilities': capabilities,
                'hardware_aware': bool(hardware_context and not hardware_context.get('error')),
                'recommendations': details.get('recommendations', {}),
                'full_details': details
            }
        else:
            summary_details = variants_result.details
        
        # Enhanced message based on operational status
        operational_status = summary_details.get('operational_status', 'unknown')
        if operational_status == 'fully_operational':
            message = f"Model variants system fully operational ({summary_details['available_variants']} variants ready)"
        elif operational_status == 'partial':
            message = f"Model variants system partially operational ({summary_details['available_variants']} available, {summary_details['warning_variants']} with warnings)"
        else:
            message = variants_result.message
        
        return CheckResult(
            passed=variants_result.passed,
            message=message,
            level=variants_result.level,
            details=summary_details,
            metadata=variants_result.metadata,
            exception=variants_result.exception
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Model variants wrapper check failed",
            level=CheckLevel.CRITICAL,
            details={
                'wrapper_error': str(e),
                'error_type': type(e).__name__,
                'initialization_passed': False,
                'recommendation': 'Check system integration and display dependencies'
            }
        ).with_exception(e)

def display_model_variants_details(variants_result: CheckResult) -> None:
    """
    Display comprehensive model variants information in enhanced rich table format.
    
    This function has been updated to fully harmonize with the enhanced check_model_variants()
    implementation, providing detailed visual presentation of all available metrics,
    status categories, and system capabilities.
    
    Args:
        variants_result: CheckResult from check_model_variants() with comprehensive details
    """
    if not isinstance(variants_result.details, dict):
        console.print(Panel.fit(
            "[bold yellow]No detailed model variants information available[/bold yellow]\n"
            f"Details type: [bold yellow]{type(variants_result.details)}[/bold yellow]\n"
            #f"Check status: {'PASSED' if variants_result.passed else 'FAILED'}\n"
            f"Check status: [bold green]PASSED[/bold green]" if variants_result.passed else "[bold red]FAILED[/bold red]\n"
            f"Message: [bold yellow]{variants_result.message}[/bold yellow]",
            title="Model Variants Details",
            border_style="red",
            style="bold red"
        ))
        return
    
    try:
        details = variants_result.details
        
        # Enhanced metrics extraction with comprehensive type safety
        total_variants = details.get('total_variants', 0)
        
        # Safe extraction of all variant categories
        available_variants_raw = details.get('available_variants', 0)
        if isinstance(available_variants_raw, list):
            available_variants = len(available_variants_raw)
            available_names = available_variants_raw
        elif isinstance(available_variants_raw, int):
            available_variants = available_variants_raw
            available_names = details.get('available_names', [])
        else:
            available_variants = 0
            available_names = []
        
        warning_variants_raw = details.get('warning_variants', [])
        if isinstance(warning_variants_raw, list):
            warning_variants = len(warning_variants_raw)
            warning_names = warning_variants_raw
        else:
            warning_variants = 0
            warning_names = []
        
        failed_variants_raw = details.get('failed_variants', 0)
        if isinstance(failed_variants_raw, list):
            failed_variants = len(failed_variants_raw)
            failed_names = failed_variants_raw
        elif isinstance(failed_variants_raw, int):
            failed_variants = failed_variants_raw
            failed_names = details.get('failed_names', [])
        else:
            failed_variants = 0
            failed_names = []
        
        unknown_variants = details.get('unknown_variants', 0)
        unknown_names = details.get('unknown_names', [])
        
        # Enhanced success metrics
        initialization_summary = details.get('initialization_summary', {})
        success_rate = initialization_summary.get('success_rate', 0)
        operational_status = initialization_summary.get('operational_status', 'unknown')
        
        # Hardware context
        hardware_context = details.get('hardware_context', {})
        hardware_error = hardware_context.get('error') if isinstance(hardware_context, dict) else None
        
        # Capabilities summary
        capabilities = details.get('capabilities', [])
        if not isinstance(capabilities, list):
            capabilities = []
        
        # Recommendations
        recommendations = details.get('recommendations', {})
        
        # Main enhanced model variants overview table
        overview_table = Table(
            title="\n[bold cyan]Model Variants System Overview[/bold cyan]",
            title_justify="left",
            box=box.DOUBLE_EDGE,
            header_style="bold magenta",
            border_style="cyan",
            show_lines=True,
            width=min(110, console.width - 4)
        )
        
        overview_table.add_column("Category", style="bold yellow", width=18)
        overview_table.add_column("Count", style="bold white", width=12, justify="center")
        overview_table.add_column("Status", style="bold green", width=15, justify="center")
        overview_table.add_column("Details", style="bold", width=45)
        
        # Add comprehensive overview rows
        overview_table.add_row(
            "Total Variants",
            str(total_variants),
            "REGISTERED",
            f"[bold green]{', '.join(map(str, details.get('variant_names', [])))}[/bold green]" if details.get('variant_names') else "[bold red]No variants registered[/bold red]"
        )
        
        overview_table.add_row(
            "Available Variants",
            str(available_variants),
            "[bold green]READY[/bold green]" if available_variants > 0 else "[bold red]NONE[/bold red]",
            f"[bold green]{', '.join(map(str, available_names))}[/bold green]" if available_names else "[bold red]No available variants[/bold red]"
        )
        
        overview_table.add_row(
            "Warning Variants",
            str(warning_variants),
            "[bold yellow]WARNING[/bold yellow]" if warning_variants > 0 else "[bold green]NONE[/bold green]",
            f"[bold yellow]{', '.join(map(str, warning_names))}[/bold yellow]" if warning_names else "[bold green]No warnings[/bold green]"
        )
        
        overview_table.add_row(
            "Failed Variants",
            str(failed_variants),
            "[bold red]FAILED[/bold red]" if failed_variants > 0 else "[bold green]NONE[/bold green]",
            f"[bold red]{', '.join(map(str, failed_names))}[/bold red]" if failed_names else "[bold green]No failures[/bold green]"
        )
        
        overview_table.add_row(
            "Success Rate",
            f"{success_rate:.1f}%",
            "[bold green]EXCELLENT[/bold green]" if success_rate >= 80 else "[bold cyan]GOOD[/bold cyan]" if success_rate >= 60 else "[bold yellow]FAIR[/bold yellow]" if success_rate > 0 else "[bold red]POOR[/bold red]",
            f"[bold white]Operational:[/bold white] {operational_status.replace('_', ' ').title()}"
        )
        
        overview_table.add_row(
            "Capabilities",
            str(len(capabilities)),
            "[bold green]READY[/bold green]" if capabilities else "[bold red]NONE[/bold red]",
            f"[bold cyan]{', '.join(capabilities)}[/bold cyan]" if capabilities else "[bold red]No capabilities detected[/bold red]"
        )
        
        overview_table.add_row(
            "Hardware Context",
            "Available" if not hardware_error else "Error",
            "[bold green]OK[/bold green]" if not hardware_error else "[bold red]ERROR[/bold red]",
            #f"[bold cyan]{'System aware' if not hardware_error else f'Hardware error: {hardware_error}'}[/bold cyan]"
            f"[bold cyan]SYSTEM-AWARE[/bold cyan]" if not hardware_error else f"[bold red]HARDWARE ERROR: {hardware_error}[/bold red]"
        )
        
        console.print(overview_table)
        console.print()
        
        # Enhanced variant details table with comprehensive information
        variant_details = details.get('variant_details', {})
        if isinstance(variant_details, dict) and variant_details:
            variants_table = Table(
                title="[bold cyan]Model Variants Detailed Analysis[/bold cyan]",
                title_justify="left",
                box=box.DOUBLE_EDGE,
                header_style="bold magenta",
                border_style="blue",
                show_lines=True,
                expand=True,
                width=min(125, console.width - 4)
            )
            
            variants_table.add_column("Variant", style="bold yellow", width=18)
            variants_table.add_column("Class", style="bold magenta", width=20)
            variants_table.add_column("Status", width=12, justify="left")
            variants_table.add_column("Configuration", style="bold", width=15)
            variants_table.add_column("Methods", style="bold", width=10)
            variants_table.add_column("Details", style="bold", min_width=40)
            
            for variant_name, variant_info in variant_details.items():
                if not isinstance(variant_info, dict):
                    continue
                    
                # Enhanced detail formatting
                detail_lines = []
                
                # Description
                description = variant_info.get('description', '')
                if description and isinstance(description, str):
                    detail_lines.append(f"[bold white]Desc:[/] [bold yellow]{description[:60]}{'...' if len(description) > 60 else ''}[/bold yellow]")
                
                # Model features
                model_features = variant_info.get('model_features', [])
                if model_features:
                    detail_lines.append(f"[bold white]Features:[/] [bold cyan]{', '.join(model_features[:3])}[/bold cyan]")
                
                # Device awareness
                if variant_info.get('device_aware'):
                    detail_lines.append(f"[bold white]Device:[/] [bold green]Aware[/bold green]")
                
                # Error information
                analysis_error = variant_info.get('analysis_error')
                if analysis_error:
                    detail_lines.append(f"[bold red]Error: {str(analysis_error)[:50]}...[/bold red]")
                
                details_text = "\n".join(detail_lines) if detail_lines else "[bold blue]Standard implementation[/bold blue]"
                
                # Enhanced status styling
                status = variant_info.get('status', 'unknown')
                status_str = str(status) if status is not None else 'unknown'
                
                if status_str == 'available':
                    status_style = "bold green"
                    status_text = "AVAILABLE"
                elif status_str.startswith('warning'):
                    status_style = "bold yellow"
                    status_text = "WARNING"
                elif status_str.startswith('error') or 'failed' in status_str:
                    status_style = "bold red"
                    status_text = "FAILED"
                else:
                    status_style = "bold white"
                    status_text = f"{status_str.upper()}"
                
                # Configuration information
                config_params = variant_info.get('config_parameters', 'unknown')
                if config_params == 'error':
                    config_text = "[bold red]ERROR[/bold red]"
                elif config_params == 'none':
                    config_text = "[bold]NONE[/bold]"
                elif isinstance(config_params, (int, float)):
                    config_text = f"[bold cyan]{config_params}[/bold cyan]"
                else:
                    config_text = f"[bold yellow]{str(config_params)}[/bold yellow]"
                
                # Methods information
                supported_methods = variant_info.get('supported_methods', {})
                if isinstance(supported_methods, dict):
                    available_methods = [method for method, available in supported_methods.items() if available]
                    methods_text = f"[bold green]{len(available_methods)}[/bold green]" if available_methods else "[bold red]0[/bold red]"
                else:
                    methods_text = "[bold yellow]Unknown[/bold yellow]"
                
                # Safe extraction of class name
                class_name = variant_info.get('class_name', 'Unknown')
                class_name_str = str(class_name) if class_name is not None else 'Unknown'
                
                variants_table.add_row(
                    Text(str(variant_name), style="bold yellow"),
                    class_name_str,
                    Text(status_text, style=status_style),
                    config_text,
                    methods_text,
                    details_text
                )
            
            console.print(variants_table)
            console.print()
        
        # Enhanced recommendations panel
        if recommendations and isinstance(recommendations, dict):
            rec_table = Table(
                title="[bold green]Model Variants Recommendations[/bold green]",
                title_justify="left",
                #box=box.SQUARE,
                box=box.DOUBLE_EDGE,
                header_style="bold white",
                border_style="green",
                width=min(100, console.width - 4)
            )
            
            rec_table.add_column("Category", style="bold yellow", width=20)
            rec_table.add_column("Suggested Models", style="bold cyan")
            
            primary_models = recommendations.get('primary_models', [])
            fallback_models = recommendations.get('fallback_models', [])
            models_to_avoid = recommendations.get('models_to_avoid', [])
            emergency_fallback = recommendations.get('emergency_fallback')
            
            if primary_models:
                rec_table.add_row("Primary Models", ", ".join(map(str, primary_models)), style="bold green")
            if fallback_models:
                rec_table.add_row("Fallback Models", ", ".join(map(str, fallback_models)), style="bold yellow")
            if models_to_avoid:
                rec_table.add_row("Avoid Models", ", ".join(map(str, models_to_avoid)), style="bold red")
            if emergency_fallback:
                rec_table.add_row("Emergency Fallback", str(emergency_fallback), style="bold magenta")
            
            if primary_models or fallback_models or models_to_avoid:
                console.print(rec_table)
                console.print()
        
        # Enhanced operational status summary with corrected styling
        if operational_status == 'fully_operational':
            border_color = "green"
        elif operational_status == 'partial':
            border_color = "yellow"
        else:
            border_color = "red"
        
        status_panel = Panel.fit(
            f"Operational Status: {operational_status.replace('_', ' ').title()}\n"
            f"Available Models: {available_variants}\n"
            f"Models with Warnings: {warning_variants}\n"
            f"Failed Models: {failed_variants}\n"
            f"Overall Success Rate: {success_rate:.1f}%",
            title="MODEL SUMMARY",
            style=f"bold {border_color}",
            border_style=border_color
        )
        
        console.print(status_panel)
        
    except Exception as e:
        console.print(
            Panel.fit(
                f"Error displaying enhanced model variants details:\n"
                f"Error: {str(e)}\n"
                f"Error Type: {type(e).__name__}\n"
                #f"Check Status: {'PASSED' if variants_result.passed else 'FAILED'}\n"
                f"Check status: [bold green]PASSED[/bold green]" if variants_result.passed else "[bold red]FAILED[/bold red]\n"
                f"Message: {variants_result.message}",
                title="Enhanced Display Rendering Error",
                border_style="red",
                style="bold red"
            )
        )
        
        # Comprehensive fallback display
        try:
            #console.print("\n[bold yellow]Comprehensive Fallback Display:[/bold yellow]")
            print(Fore.YELLOW + Style.BRIGHT + "\nComprehensive Fallback Display:")
            
            # Basic check information
            #console.print(f"[cyan]Check Result:[/cyan] {'PASSED' if variants_result.passed else 'FAILED'}")
            #console.print(f"[cyan]Message:[/cyan] {variants_result.message}")
            #console.print(f"[cyan]Level:[/cyan] {variants_result.level}")
            
            variants_result_status = 'PASSED' if variants_result.passed else 'FAILED'
            if variants_result_status == 'PASSED':
                status_color = Fore.GREEN + Style.BRIGHT
            else:
                status_color = Fore.RED + Style.BRIGHT
            
            print(Fore.YELLOW + Style.BRIGHT + "Check Result: " + f"{status_color}" + f"{variants_result_status}")
            print(Fore.YELLOW + Style.BRIGHT + "Message: " + f"{status_color}" + f"{variants_result.message}")
            print(Fore.YELLOW + Style.BRIGHT + "Level: " + f"{status_color}" + f"{variants_result.level}")
            
            if isinstance(variants_result.details, dict):
                details = variants_result.details
                
                # Enhanced metrics extraction
                total_variants = details.get('total_variants', 0)
                
                # Safe variant counting
                available_count = 0
                warning_count = 0
                failed_count = 0
                
                available_names = details.get('available_names', [])
                warning_names = details.get('warning_names', [])
                failed_names = details.get('failed_names', [])
                
                if isinstance(available_names, list):
                    available_count = len(available_names)
                    available_count_status = f"{Fore.GREEN + Style.BRIGHT}{available_count}{Style.RESET_ALL}" if available_count > 0 else f"{Fore.RED + Style.BRIGHT}{available_count}{Style.RESET_ALL}"
                if isinstance(warning_names, list):
                    warning_count = len(warning_names)
                    warning_count_status = f"{Fore.YELLOW + Style.BRIGHT}{warning_count}{Style.RESET_ALL}" if warning_count > 0 else f"{Fore.GREEN + Style.BRIGHT}{warning_count}{Style.RESET_ALL}"
                if isinstance(failed_names, list):
                    failed_count = len(failed_names)
                    failed_count_status = f"{Fore.RED + Style.BRIGHT}{failed_count}{Style.RESET_ALL}" if failed_count > 0 else f"{Fore.GREEN + Style.BRIGHT}{failed_count}{Style.RESET_ALL}"
                
                success_rate = details.get('initialization_summary', {}).get('success_rate', 0)
                success_rate_status = f"{Fore.GREEN + Style.BRIGHT}{success_rate:.1f}%{Style.RESET_ALL}" if success_rate >= 80 else f"{Fore.YELLOW + Style.BRIGHT}{success_rate:.1f}%{Style.RESET_ALL}" if success_rate >= 60 else f"{Fore.RED + Style.BRIGHT}{success_rate:.1f}%{Style.RESET_ALL}"
                
                operational_status = details.get('initialization_summary', {}).get('operational_status', 'unknown')
                operational_status_details = operational_status.replace('_', '').title()
                
                if operational_status_details == 'Fully Operational':
                    operational_status = f"{Fore.GREEN + Style.BRIGHT}{operational_status_details}{Style.RESET_ALL}"
                elif operational_status_details == 'Partial':
                    operational_status = f"{Fore.YELLOW + Style.BRIGHT}{operational_status_details}{Style.RESET_ALL}"
                else:
                    operational_status = f"{Fore.RED + Style.BRIGHT}{operational_status_details}{Style.RESET_ALL}"
                
                # console.print(f"[cyan]Variants:[/cyan] {available_count} available, {warning_count} warnings, {failed_count} failed (of {total_variants} total)")
                # console.print(f"[cyan]Success Rate:[/cyan] {success_rate:.1f}%")
                # console.print(f"[cyan]Operational Status:[/cyan] {operational_status.replace('_', ' ').title()}")
                
                print(Fore.YELLOW + Style.BRIGHT + f"Variants: {available_count_status} available, {warning_count_status} warnings, {failed_count_status} failed (of {total_variants} total)")
                print(Fore.YELLOW + Style.BRIGHT + f"Success Rate: {success_rate_status}")
                print(Fore.YELLOW + Style.BRIGHT + f"Operational Status: {operational_status}")
                
                # Display variant names safely
                if available_names:
                    safe_names = [str(name) for name in available_names if name is not None]
                    #console.print(f"[green]Available:[/green] {', '.join(safe_names)}")
                    print(Fore.YELLOW + Style.BRIGHT + f"Available: " + Fore.GREEN + Style.BRIGHT + f"{', '.join(safe_names)}")
                if warning_names:
                    safe_names = [str(name) for name in warning_names if name is not None]
                    #console.print(f"[yellow]Warnings:[/yellow] {', '.join(safe_names)}")
                    print(Fore.YELLOW + Style.BRIGHT + f"Warnings: " + Fore.GREEN + Style.BRIGHT + f"{', '.join(safe_names)}")
                if failed_names:
                    safe_names = [str(name) for name in failed_names if name is not None]
                    #console.print(f"[red]Failed:[/red] {', '.join(safe_names)}")
                    print(Fore.YELLOW + Style.BRIGHT + f"Failed: " + Fore.RED + Style.BRIGHT + f"{', '.join(safe_names)}")
                
                # Capabilities
                capabilities = details.get('capabilities', [])
                if capabilities:
                    #console.print(f"[cyan]Capabilities:[/cyan] {', '.join(capabilities)}")
                    print(Fore.YELLOW + Style.BRIGHT + f"Capabilities: " + Fore.CYAN + Style.BRIGHT + f"{', '.join(capabilities)}")
                
            else:
                #console.print("[yellow]Details format not recognized for fallback display[/yellow]")
                print(Fore.YELLOW + Style.BRIGHT + "Details format not recognized for fallback display")
                
        except Exception as fallback_error:
            #console.print(f"[red]Comprehensive fallback display also failed: {fallback_error}[/red]")
            print(Fore.RED + Style.BRIGHT + f"Comprehensive fallback display also failed: " + Fore.YELLOW + Style.BRIGHT + f"{str(fallback_error)}")
            
            #console.print(f"[red]Original display error: {str(e)}[/red]")
            print(Fore.RED + Style.BRIGHT + f"Original display error: " + Fore.YELLOW + Style.BRIGHT + f"{str(e)}")

# System and environment configuration
def configure_system() -> Dict[str, Any]:
    """
    Configure system-wide settings for optimal performance and logging.
    
    Returns:
        Dictionary containing all applied configurations with validation data
    """
    config = {
        'torch': {},
        'numpy': {},
        'warnings': {},
        'environment': {},
        'validation': {},
        'optimizations': {},
        'detected_capabilities': {}
    }

    # Get comprehensive system information
    try:
        version_info = check_versions(include_optional=True)
        hardware_data = check_hardware(min_disk_gb=1.0, include_memory_usage=True)
        
        config['validation']['versions'] = version_info
        config['validation']['hardware'] = hardware_data
        
        # Extract key capabilities
        cpu_info = hardware_data.get('cpu_cores', {})
        ram_info = hardware_data.get('system_ram', {})
        cuda_info = hardware_data.get('cuda', {})
        
        logical_cores = cpu_info.get('logical_cores', os.cpu_count() or 1)
        total_ram_gb = ram_info.get('ram_total_gb', 4.0)
        cuda_available = cuda_info.get('available', False)
        gpu_count = cuda_info.get('gpu_count', 0)
        
        config['detected_capabilities'] = {
            'cpu_cores': logical_cores,
            'ram_gb': total_ram_gb,
            'cuda_available': cuda_available,
            'gpu_count': gpu_count,
            'hyperthreading': cpu_info.get('hyperthreading', False)
        }
        
    except Exception as e:
        # Fallback to basic detection
        logical_cores = os.cpu_count() or 1
        total_ram_gb = 4.0
        cuda_available = torch.cuda.is_available()
        gpu_count = torch.cuda.device_count() if cuda_available else 0
        config['validation']['error'] = str(e)

    # Intelligent thread configuration based on detected hardware
    # Conservative approach
    optimal_threads = min(4, max(1, logical_cores // 2))
    if total_ram_gb > 16:
        # More threads if more RAM
        optimal_threads = min(8, logical_cores)
    elif total_ram_gb < 4:
        # Fewer threads if limited RAM
        optimal_threads = min(2, logical_cores)
    
    # Environment variable configurations
    env_vars = {
        # TensorFlow logging
        'TF_CPP_MIN_LOG_LEVEL': '3',
        # Intel MKL warnings
        'KMP_WARNINGS': '0',
        # OpenMP threads - now optimized based on hardware
        'OMP_NUM_THREADS': str(optimal_threads),
        # MKL threads - now optimized based on hardware
        'MKL_NUM_THREADS': str(optimal_threads),
        # For CUDA reproducibility
        'CUBLAS_WORKSPACE_CONFIG': ':4096:8',
        # Disable TensorFlow GPU if CUDA not available
        'CUDA_VISIBLE_DEVICES': '' if not cuda_available else None
    }
    
    # Add memory-aware configurations
    if total_ram_gb > 32:
        env_vars['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    elif total_ram_gb < 8:
        env_vars['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    # Apply environment variables
    for key, value in env_vars.items():
        if value is not None:
            os.environ[key] = value
            config['environment'][key] = value

    # PyTorch configuration
    torch_config = {
        'deterministic': True,
        # Enable benchmark if CUDA available
        'benchmark': cuda_available and gpu_count > 0,
        'float32_matmul_precision': 'high' if cuda_available else 'highest',
        'num_threads': optimal_threads,
        'precision': 4,
        'sci_mode': False,
        'cuda_memory_fraction': 0.8 if cuda_available else None
    }
    
    # Apply PyTorch configurations
    torch.set_num_threads(torch_config['num_threads'])
    if cuda_available:
        torch.backends.cudnn.deterministic = torch_config['deterministic']
        torch.backends.cudnn.benchmark = torch_config['benchmark']
        
        # Configure CUDA memory management based on GPU memory
        for gpu in cuda_info.get('gpus', []):
            gpu_memory_gb = gpu.get('memory_gb', 0)
            if gpu_memory_gb > 16:
                torch.cuda.set_per_process_memory_fraction(0.9)
            elif gpu_memory_gb < 6:
                torch.cuda.set_per_process_memory_fraction(0.7)
    
    torch.set_printoptions(
        precision=torch_config['precision'],
        sci_mode=torch_config['sci_mode']
    )
    
    config['torch'].update(torch_config)

    # NumPy configuration
    np_config = {
        'precision': 4,
        'suppress': True,
        # More output if more RAM
        'threshold': 1000 if total_ram_gb > 8 else 100,
        'linewidth': 120,
        'float_division_warning': False
    }
    
    np.set_printoptions(
        precision=np_config['precision'],
        suppress=np_config['suppress'],
        threshold=np_config['threshold'],
        linewidth=np_config['linewidth']
    )
    config['numpy'].update(np_config)

    # Warning configurations
    warning_config = {
        'ignored_categories': {
            UserWarning: ['joblib', 'torch', 'numpy'],
            FutureWarning: None,
            DeprecationWarning: None,
            ConvergenceWarning: ['sklearn'],
            RuntimeWarning: None,
        },
        'simplefilter': 'ignore'
    }
    
    # Add version-specific warning filters
    torch_info = version_info.get('PyTorch', {})
    if torch_info.get('available', False):
        torch_version = torch_info.get('version', '')
        if torch_version.startswith('2.'):
            warning_config['ignored_categories'][DeprecationWarning] = ['torch']
    
    # Apply warning filters
    for category, modules in warning_config['ignored_categories'].items():
        if modules:
            for module in modules:
                warnings.filterwarnings('ignore', category=category, module=module)
        else:
            warnings.filterwarnings('ignore', category=category)
    
    warnings.simplefilter(warning_config['simplefilter'])
    config['warnings'].update(warning_config)
    
    # Record optimizations applied
    config['optimizations'] = {
        'thread_optimization': f"Set to {optimal_threads} threads based on {logical_cores} cores and {total_ram_gb:.1f}GB RAM",
        'cuda_optimization': f"CUDA benchmark {'enabled' if torch_config['benchmark'] else 'disabled'}",
        'memory_optimization': f"Configured for {total_ram_gb:.1f}GB RAM",
        'version_optimizations': f"Applied optimizations for {len([v for v in version_info.values() if v.get('available', False)])} available packages"
    }

    return config

# Reproducibility configuration
def set_seed(seed: int = 42, hardware_info: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Configure all random seeds for full reproducibility.
    
    Args:
        seed: Base seed value
        hardware_info: Hardware information from check_hardware() (optional)
        
    Returns:
        Dictionary containing the seed configuration with hardware context
    """
    # Get hardware info if not provided
    if hardware_info is None:
        try:
            hardware_info = check_hardware(include_memory_usage=False)
        except Exception:
            hardware_info = {}
    
    cuda_info = hardware_info.get('cuda', {})
    cuda_available = cuda_info.get('available', False)
    gpu_count = cuda_info.get('gpu_count', 0)
    
    seed_config = {
        'base_seed': seed,
        'hardware_context': {
            'cuda_available': cuda_available,
            'gpu_count': gpu_count,
            'optimization_level': 'high' if cuda_available else 'standard'
        },
        'python': {
            'random_seed': seed,
            'hash_seed': seed
        },
        'numpy_seed': seed,
        'torch': {
            'cpu_seed': seed,
            'cuda_seeds': None,
            'cuda_deterministic': False,
            'cuda_benchmark': False
        },
        'environment': {
            'PYTHONHASHSEED': str(seed),
            'CUBLAS_WORKSPACE_CONFIG': ':4096:8'
        },
        'tensorflow_seed': None,
        'per_gpu_seeds': []
    }
    
    # Set Python seeds
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    # Set NumPy seed
    np.random.seed(seed)
    
    # Set PyTorch seeds with hardware awareness
    torch.manual_seed(seed)
    
    if cuda_available:
        # Set different seeds for each GPU to ensure variety while maintaining reproducibility
        gpu_seeds = []
        for i in range(gpu_count):
            # Deterministic but different per GPU
            gpu_seed = seed + i
            torch.cuda.manual_seed(gpu_seed)
            gpu_seeds.append(gpu_seed)
        
        # Also set the global seed
        torch.cuda.manual_seed_all(seed)
        
        # Configure deterministic operations based on GPU capabilities
        torch.backends.cudnn.deterministic = True
        # Disable for reproducibility
        torch.backends.cudnn.benchmark = False
        
        # Use hardware-aware CUDA configuration
        gpus = cuda_info.get('gpus', [])
        if gpus:
            # Configure based on GPU compute capability
            for i, gpu in enumerate(gpus):
                compute_cap = gpu.get('compute_capability', '0.0')
                major_version = int(float(compute_cap))
                if major_version >= 7:  # Tensor cores available
                    # For reproducibility
                    torch.backends.cuda.matmul.allow_tf32 = False
        
        seed_config['torch'].update({
            'cuda_seeds': gpu_seeds,
            'cuda_deterministic': True,
            'cuda_benchmark': False,
            'global_cuda_seed': seed
        })
        seed_config['per_gpu_seeds'] = gpu_seeds
    
    # Set CUDA workspace config for deterministic operations
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    
    # Set TensorFlow seed if available
    try:
        version_info = check_versions(include_optional=False)
        tf_available = any('tensorflow' in str(v).lower() for v in version_info.values())
        if tf_available:
            import tensorflow as tf
            tf.random.set_seed(seed)
            seed_config['tensorflow_seed'] = seed
    except (ImportError, Exception):
        pass
    
    return seed_config

# Hardware and Package Configuration
def setup_gpu(logger: logging.Logger, hardware_data: Optional[Dict[str, Any]] = None) -> torch.device:
    """
    Detect and configure the primary compute device with full hardware awareness.
    
    Args:
        logger: Logger instance for recording device information
        hardware_data: Hardware data from check_hardware() (optional)
        
    Returns:
        Configured torch.device with optimal settings applied
    """
    # Get comprehensive hardware data if not provided
    if hardware_data is None:
        try:
            hardware_data = check_hardware(include_memory_usage=True)
        except Exception as e:
            logger.warning(f"Could not get hardware data: {e}")
            hardware_data = {}
    
    # Extract hardware information
    cuda_info = hardware_data.get('cuda', {})
    cpu_info = hardware_data.get('cpu_cores', {})
    ram_info = hardware_data.get('system_ram', {})
    
    cuda_available = cuda_info.get('available', False)
    gpu_count = cuda_info.get('gpu_count', 0)
    gpus = cuda_info.get('gpus', [])
    logical_cores = cpu_info.get('logical_cores', os.cpu_count() or 1)
    total_ram_gb = ram_info.get('ram_total_gb', 4.0)
    
    device = torch.device('cpu')
    device_info = {
        'type': 'CPU',
        'count': logical_cores,
        'details': cpu_info.get('capacity', {}),
        'optimization_applied': []
    }
    
    if cuda_available and gpu_count > 0:
        # Select the best GPU based on memory and compute capability
        best_gpu_idx = 0
        best_gpu_score = 0
        
        for i, gpu in enumerate(gpus):
            memory_gb = gpu.get('memory_gb', 0)
            compute_cap = float(gpu.get('compute_capability', '0.0'))
            
            # Score based on memory and compute capability
            score = memory_gb * 10 + compute_cap * 100
            
            # Consider current memory usage if available
            if 'current_usage' in gpu:
                usage_percent = gpu['current_usage'].get('percent_allocated', 0)
                # Prefer less utilized GPUs
                score *= (1 - usage_percent / 100)
            
            if score > best_gpu_score:
                best_gpu_score = score
                best_gpu_idx = i
        
        device = torch.device(f'cuda:{best_gpu_idx}')
        selected_gpu = gpus[best_gpu_idx]
        
        device_info.update({
            'type': 'CUDA',
            'device_id': best_gpu_idx,
            'count': gpu_count,
            'selected_gpu': selected_gpu,
            'cuda_version': cuda_info.get('cuda_version'),
            'cudnn_version': cuda_info.get('cudnn_version'),
            'all_gpus': gpus,
            'selection_score': best_gpu_score
        })
        
        # Apply GPU-specific optimizations
        gpu_memory_gb = selected_gpu.get('memory_gb', 0)
        compute_cap = float(selected_gpu.get('compute_capability', '0.0'))
        
        optimizations = []
        
        # Memory management based on GPU memory
        if gpu_memory_gb > 16:
            torch.backends.cudnn.benchmark = True
            torch.cuda.set_per_process_memory_fraction(0.9, best_gpu_idx)
            optimizations.append("High memory GPU: enabled cuDNN benchmark, 90% memory fraction")
        elif gpu_memory_gb > 8:
            torch.backends.cudnn.benchmark = True
            torch.cuda.set_per_process_memory_fraction(0.8, best_gpu_idx)
            optimizations.append("Medium memory GPU: enabled cuDNN benchmark, 80% memory fraction")
        else:
            torch.backends.cudnn.benchmark = False
            torch.cuda.set_per_process_memory_fraction(0.7, best_gpu_idx)
            optimizations.append("Low memory GPU: disabled cuDNN benchmark, 70% memory fraction")
        
        # Compute capability optimizations
        # Turing+ architecture
        if compute_cap >= 7.5:
            torch.backends.cuda.matmul.allow_tf32 = True
            optimizations.append("Modern GPU: enabled TF32 for performance")
        # Volta architecture
        elif compute_cap >= 7.0:
            torch.backends.cuda.matmul.allow_tf32 = False
            optimizations.append("Volta GPU: disabled TF32 for compatibility")
        
        # Multi-GPU optimizations
        if gpu_count > 1:
            # Enable peer-to-peer access if supported
            try:
                for i in range(gpu_count):
                    for j in range(gpu_count):
                        if i != j and torch.cuda.can_device_access_peer(i, j):
                            torch.cuda.device_enable_peer_access(i, j)
                optimizations.append(f"Multi-GPU: enabled peer access for {gpu_count} GPUs")
            except Exception as e:
                logger.debug(f"Could not enable peer access: {e}")
        
        # System RAM vs GPU memory optimization
        if total_ram_gb < gpu_memory_gb:
            torch.backends.cudnn.deterministic = True
            optimizations.append("Low system RAM: enabled deterministic mode to reduce memory usage")
        
        device_info['optimization_applied'] = optimizations
        
        # Set the device for current context
        torch.cuda.set_device(best_gpu_idx)
        
    else:
        # CPU-only optimizations based on hardware
        optimizations = []
        
        if logical_cores > 8:
            torch.set_num_threads(min(8, logical_cores))
            optimizations.append(f"High-core CPU: limited threads to {min(8, logical_cores)}")
        elif logical_cores <= 2:
            torch.set_num_threads(logical_cores)
            optimizations.append(f"Low-core CPU: using all {logical_cores} threads")
        
        if total_ram_gb > 16:
            torch.set_num_interop_threads(4)
            optimizations.append("High RAM: increased interop threads")
        elif total_ram_gb < 8:
            torch.set_num_interop_threads(1)
            optimizations.append("Low RAM: reduced interop threads")
        
        device_info['optimization_applied'] = optimizations
    
    # Log comprehensive device information
    logger.info(f"Primary device configured: {device}")
    logger.info(f"Device type: {device_info['type']}")
    logger.info(f"Available resources: {device_info['count']} {'GPU(s)' if cuda_available else 'CPU cores'}")
    
    if cuda_available:
        selected_gpu = device_info['selected_gpu']
        logger.info(f"Selected GPU: {selected_gpu['name']} ({selected_gpu['memory_gb']:.1f}GB)")
        logger.info(f"Compute capability: {selected_gpu['compute_capability']}")
        
        if 'current_usage' in selected_gpu:
            usage = selected_gpu['current_usage']
            logger.info(f"Current GPU memory usage: {usage['allocated_mb']:.0f}MB allocated")
    
    # Log optimizations applied
    for opt in device_info['optimization_applied']:
        logger.info(f"Applied optimization: {opt}")
    
    return device

def log_system_configuration(
    logger: logging.Logger,
    include_versions: bool = True,
    include_hardware: bool = True,
    include_seed_config: bool = True
) -> None:
    """
    Log system configuration using existing check functions.
    Replaces individual logging functions with unified configuration logging.
    
    Args:
        logger: Logger instance
        include_versions: Whether to log version information
        include_hardware: Whether to log hardware information  
        include_seed_config: Whether to log seed configuration
    """
    try:
        logger.info("=" * 80)
        logger.info("SYSTEM CONFIGURATION SUMMARY")
        logger.info("=" * 80)
        
        # Get system info
        system_info = get_system_info(include_versions, include_hardware)
        
        # Log platform information
        platform_info = system_info.get('platform', {})
        logger.info(f"Platform: {platform_info.get('platform', 'Unknown')}")
        logger.info(f"System: {platform_info.get('system', 'Unknown')} {platform_info.get('release', '')}")
        logger.info(f"Architecture: {platform_info.get('architecture', ['Unknown'])[0]}")
        logger.info(f"Processor: {platform_info.get('processor', 'Unknown')}")
        
        # Log Python information
        python_info = system_info.get('python', {})
        version_info = python_info.get('version_info', {})
        logger.info(f"Python: {version_info.get('major', '?')}.{version_info.get('minor', '?')}.{version_info.get('micro', '?')}")
        
        # Log hardware information if available
        if include_hardware and 'hardware' in system_info:
            hardware_data = system_info['hardware']
            logger.info("\n[Hardware Configuration]")
            
            # CPU information
            cpu_info = hardware_data.get('cpu_cores', {})
            if cpu_info.get('available'):
                logger.info(f"{'CPU Cores':>20}: {cpu_info.get('logical_cores', 'Unknown')} logical, {cpu_info.get('physical_cores', 'Unknown')} physical")
                if cpu_info.get('hyperthreading'):
                    logger.info(f"{'Hyperthreading':>20}: Enabled")
            
            # RAM information
            ram_info = hardware_data.get('system_ram', {})
            if ram_info.get('available'):
                logger.info(f"{'System RAM':>20}: {ram_info.get('ram_total_gb', 0):.1f}GB total, {ram_info.get('ram_available_gb', 0):.1f}GB available")
            
            # GPU information
            cuda_info = hardware_data.get('cuda', {})
            if cuda_info.get('available'):
                gpu_count = cuda_info.get('gpu_count', 0)
                logger.info(f"{'CUDA':>20}: Available with {gpu_count} GPU(s)")
                for i, gpu in enumerate(cuda_info.get('gpus', [])):
                    logger.info(f"{'GPU ' + str(i):>20}: {gpu.get('name', 'Unknown')} ({gpu.get('memory_gb', 0):.1f}GB)")
            else:
                logger.info(f"{'CUDA':>20}: Not available")
            
            # Disk space
            disk_info = hardware_data.get('disk_space', {})
            if disk_info.get('available') is not None:
                logger.info(f"{'Disk Space':>20}: {disk_info.get('free_gb', 0):.1f}GB free of {disk_info.get('total_gb', 0):.1f}GB total")
        
        # Log package versions if available
        if include_versions and 'package_versions' in system_info:
            logger.info("\n[Package Versions]")
            package_versions = system_info['package_versions']
            
            # Core packages first
            core_packages = [name for name, info in package_versions.items() if info.get('required', False)]
            for package in core_packages:
                info = package_versions[package]
                status = "[OK]" if info.get('compatible', False) else "[FAIL]"
                logger.info(f"{status} {package:>18}: {info.get('version', 'Unknown')}")
            
            # Optional packages
            optional_packages = [name for name, info in package_versions.items() if not info.get('required', False)]
            if optional_packages:
                logger.info("\n[Optional Packages]")
                # Limit to avoid spam
                for package in optional_packages[:10]:
                    info = package_versions[package]
                    if info.get('available', False):
                        logger.info(f"- {package:>18}: {info.get('version', 'Available')}")
        
        # Log seed configuration if requested
        if include_seed_config:
            try:
                seed_result = check_seed_config()
                if seed_result.passed:
                    logger.info(f"\n[Reproducibility]: Configured ({seed_result.metadata.get('compliance_score', 0)}% compliance)")
                else:
                    logger.warning(f"\n[Reproducibility]: Issues detected ({seed_result.metadata.get('compliance_score', 0)}% compliance)")
            except Exception as e:
                logger.debug(f"Could not check seed configuration: {e}")
        
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"Failed to log system configuration: {e}")

# Helper functions for system diagnostics and error handling
def enhanced_global_exception_handler(exc_type, exc_value, exc_traceback):
    """
    Enhanced global exception handler with detailed logging and recovery.
    """
    if issubclass(exc_type, KeyboardInterrupt):
        logger.info("System interrupted by user")
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    
    # Log the exception with full context
    logger.critical("CRITICAL: Uncaught exception occurred", exc_info=(exc_type, exc_value, exc_traceback))
    
    try:
        # Get comprehensive system state using existing functions
        system_state = {
            'timestamp': datetime.now().isoformat(),
            'exception_type': exc_type.__name__,
            'exception_module': getattr(exc_type, '__module__', 'unknown'),
            'exception_message': str(exc_value),
            'traceback': traceback.format_exception(exc_type, exc_value, exc_traceback),
            'traceback_summary': traceback.format_exception_only(exc_type, exc_value)
        }
        
        # Add comprehensive system information using existing functions
        try:
            system_state['system_info'] = get_system_info(include_versions=True, include_hardware=True)
        except Exception as e:
            system_state['system_info_error'] = str(e)
        
        # Add hardware state with memory usage
        try:
            system_state['hardware_state'] = check_hardware(include_memory_usage=True)
        except Exception as e:
            system_state['hardware_state_error'] = str(e)
        
        # Add version information
        try:
            system_state['package_versions'] = check_versions(include_optional=True)
        except Exception as e:
            system_state['package_versions_error'] = str(e)
        
        # Add seed configuration state
        try:
            seed_check = check_seed_config()
            system_state['seed_config'] = {
                'passed': seed_check.passed,
                'compliance_score': seed_check.metadata.get('compliance_score', 0),
                'details': seed_check.details
            }
        except Exception as e:
            system_state['seed_config_error'] = str(e)
        
        # Add memory clearing attempt results
        try:
            clear_results = enhanced_clear_memory(aggressive=False)
            system_state['memory_state'] = clear_results
        except Exception as e:
            system_state['memory_state_error'] = str(e)
        
        # Add process information
        try:
            proc = psutil.Process()
            system_state['process_info'] = {
                'pid': proc.pid,
                'name': proc.name(),
                'memory_percent': proc.memory_percent(),
                'cpu_percent': proc.cpu_percent(),
                'num_threads': proc.num_threads(),
                'status': proc.status(),
                'create_time': proc.create_time()
            }
        except Exception as e:
            system_state['process_info_error'] = str(e)
        
        # Save comprehensive error report
        try:
            error_file = LOG_DIR / f"critical_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(error_file, 'w', encoding='utf-8') as f:
                json.dump(system_state, f, indent=2, default=str, ensure_ascii=False)
            logger.info(f"Comprehensive error report saved to: {error_file}")
        except Exception as save_error:
            logger.error(f"Failed to save error report: {save_error}")
        
        # Display user-friendly error in interactive mode
        if hasattr(sys, 'ps1') or sys.stdin.isatty():
            console.print(f"[red]CRITICAL ERROR: {exc_value}[/red]")
            console.print(f"[dim]Comprehensive error report saved to logs directory[/dim]")
            console.print(f"[dim]Exception type: {exc_type.__name__}[/dim]")
            console.print(f"[dim]Module: {getattr(exc_type, '__module__', 'unknown')}[/dim]")
            
            # Show memory state if available
            if 'hardware_state' in system_state:
                ram_info = system_state['hardware_state'].get('system_ram', {})
                if 'current_usage' in ram_info:
                    usage = ram_info['current_usage']
                    console.print(f"[dim]System RAM: {usage.get('used_gb', 0):.1f}GB used ({usage.get('percent_used', 0):.1f}%)[/dim]")
        
        # Attempt emergency cleanup
        try:
            enhanced_clear_memory(aggressive=True)
            logger.info("Emergency memory cleanup completed")
        except Exception as cleanup_error:
            logger.error(f"Emergency cleanup failed: {cleanup_error}")
        
    except Exception as handler_error:
        # Fallback if our enhanced handler fails
        logger.error(f"Enhanced exception handler failed: {handler_error}")
        sys.__excepthook__(exc_type, exc_value, exc_traceback)

def performance_monitor_wrapper(func, include_memory: bool, log_level: int, hardware_data: Optional[Dict[str, Any]] = None):
    """
    Enhanced performance monitoring wrapper with hardware awareness.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        # Get hardware context if not provided
        if hardware_data is None:
            try:
                hw_data = check_hardware(include_memory_usage=include_memory)
            except Exception:
                hw_data = {}
        else:
            hw_data = hardware_data
        
        # Extract hardware capabilities
        cuda_info = hw_data.get('cuda', {})
        ram_info = hw_data.get('system_ram', {})
        cpu_info = hw_data.get('cpu_cores', {})
        
        cuda_available = cuda_info.get('available', False)
        gpu_count = cuda_info.get('gpu_count', 0)
        total_ram_gb = ram_info.get('ram_total_gb', 4.0)
        logical_cores = cpu_info.get('logical_cores', 1)
        
        # Pre-execution metrics with hardware awareness
        start_time = time.time()
        start_metrics = {
            'timestamp': datetime.now().isoformat(),
            'function': func.__name__,
            'hardware_context': {
                'cuda_available': cuda_available,
                'gpu_count': gpu_count,
                'total_ram_gb': total_ram_gb,
                'logical_cores': logical_cores
            }
        }
        
        # Memory monitoring based on system capabilities
        if include_memory:
            try:
                proc = psutil.Process()
                start_metrics['system_memory'] = {
                    'process_rss_mb': proc.memory_info().rss / 1024 / 1024,
                    'process_vms_mb': proc.memory_info().vms / 1024 / 1024,
                    'process_percent': proc.memory_percent()
                }
                
                # Add system RAM if available
                if 'current_usage' in ram_info:
                    start_metrics['system_memory']['system_used_gb'] = ram_info['current_usage']['used_gb']
                    start_metrics['system_memory']['system_percent'] = ram_info['current_usage']['percent_used']
            except Exception as e:
                start_metrics['memory_error'] = str(e)
        
        # GPU monitoring if available
        if cuda_available:
            try:
                start_metrics['gpu_memory'] = {}
                for i in range(gpu_count):
                    start_metrics['gpu_memory'][f'gpu_{i}'] = {
                        'allocated_mb': torch.cuda.memory_allocated(i) / 1024 / 1024,
                        'reserved_mb': torch.cuda.memory_reserved(i) / 1024 / 1024,
                        'max_allocated_mb': torch.cuda.max_memory_allocated(i) / 1024 / 1024
                    }
            except Exception as e:
                start_metrics['gpu_memory_error'] = str(e)
        
        # Execute function with monitoring
        try:
            result = func(*args, **kwargs)
            
            # Post-execution metrics
            end_time = time.time()
            duration = end_time - start_time
            
            end_metrics = {
                'duration': duration,
                'status': 'completed',
                'timestamp': datetime.now().isoformat()
            }
            
            # Memory analysis
            if include_memory:
                try:
                    proc = psutil.Process()
                    end_rss = proc.memory_info().rss / 1024 / 1024
                    end_vms = proc.memory_info().vms / 1024 / 1024
                    
                    end_metrics['memory_usage'] = {
                        'process_rss_mb': end_rss,
                        'process_vms_mb': end_vms,
                        'process_percent': proc.memory_percent(),
                        'rss_delta_mb': end_rss - start_metrics.get('system_memory', {}).get('process_rss_mb', 0),
                        'vms_delta_mb': end_vms - start_metrics.get('system_memory', {}).get('process_vms_mb', 0)
                    }
                    
                    # System memory change if available
                    try:
                        current_hw = check_hardware(include_memory_usage=True)
                        current_ram = current_hw.get('system_ram', {}).get('current_usage', {})
                        if current_ram:
                            end_metrics['memory_usage']['system_used_gb'] = current_ram['used_gb']
                            end_metrics['memory_usage']['system_delta_gb'] = (
                                current_ram['used_gb'] - 
                                start_metrics.get('system_memory', {}).get('system_used_gb', 0)
                            )
                    except Exception:
                        pass
                        
                except Exception as e:
                    end_metrics['memory_error'] = str(e)
            
            # GPU memory analysis
            if cuda_available:
                try:
                    end_metrics['gpu_memory'] = {}
                    total_gpu_delta = 0
                    
                    for i in range(gpu_count):
                        current_allocated = torch.cuda.memory_allocated(i) / 1024 / 1024
                        current_reserved = torch.cuda.memory_reserved(i) / 1024 / 1024
                        
                        start_gpu = start_metrics.get('gpu_memory', {}).get(f'gpu_{i}', {})
                        allocated_delta = current_allocated - start_gpu.get('allocated_mb', 0)
                        reserved_delta = current_reserved - start_gpu.get('reserved_mb', 0)
                        
                        end_metrics['gpu_memory'][f'gpu_{i}'] = {
                            'allocated_mb': current_allocated,
                            'reserved_mb': current_reserved,
                            'allocated_delta_mb': allocated_delta,
                            'reserved_delta_mb': reserved_delta
                        }
                        
                        total_gpu_delta += allocated_delta
                    
                    end_metrics['gpu_memory']['total_delta_mb'] = total_gpu_delta
                    
                except Exception as e:
                    end_metrics['gpu_memory_error'] = str(e)
            
            # Performance analysis and logging
            log_message = f"{func.__name__} completed in {duration:.3f}s"
            
            # Add memory information to log
            if include_memory and 'memory_usage' in end_metrics:
                mem_delta = end_metrics['memory_usage'].get('rss_delta_mb', 0)
                log_message += f", memory: {mem_delta:+.1f}MB"
                
                # Add system memory if significant change
                sys_delta = end_metrics['memory_usage'].get('system_delta_gb', 0)
                # > 100MB change
                if abs(sys_delta) > 0.1:
                    log_message += f" (system: {sys_delta:+.1f}GB)"
            
            # Add GPU information to log
            if cuda_available and 'gpu_memory' in end_metrics:
                total_gpu_delta = end_metrics['gpu_memory'].get('total_delta_mb', 0)
                # > 1MB change
                if abs(total_gpu_delta) > 1:
                    log_message += f", GPU: {total_gpu_delta:+.1f}MB"
            
            # Performance warnings based on hardware
            warnings = []
            if duration > 10:  # Long execution
                warnings.append("long_execution")
            if include_memory and 'memory_usage' in end_metrics:
                mem_delta = end_metrics['memory_usage'].get('rss_delta_mb', 0)
                # > 10% of total RAM
                if mem_delta > (total_ram_gb * 100):
                    warnings.append("high_memory_usage")
            
            if warnings:
                log_message += f" [WARNING: {', '.join(warnings)}]"
                logger.warning(log_message)
            else:
                logger.log(log_level, log_message)
            
            # Store comprehensive metrics for analysis
            combined_metrics = {
                **start_metrics,
                **end_metrics,
                'warnings': warnings
            }
            
            if not hasattr(wrapper, '_performance_metrics'):
                wrapper._performance_metrics = []
            wrapper._performance_metrics.append(combined_metrics)
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            error_message = f"{func.__name__} failed after {duration:.3f}s: {str(e)}"
            
            # Add memory info to error log if available
            if include_memory:
                try:
                    proc = psutil.Process()
                    current_rss = proc.memory_info().rss / 1024 / 1024
                    start_rss = start_metrics.get('system_memory', {}).get('process_rss_mb', 0)
                    mem_at_error = current_rss - start_rss
                    error_message += f", memory at error: {mem_at_error:+.1f}MB"
                except Exception:
                    pass
            
            logger.error(error_message)
            
            # Store error metrics
            error_metrics = {
                **start_metrics,
                'duration': duration,
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            
            if not hasattr(wrapper, '_performance_metrics'):
                wrapper._performance_metrics = []
            wrapper._performance_metrics.append(error_metrics)
            
            raise
    
    return wrapper

def enhanced_monitor_performance(
    include_memory: bool = True,
    log_level: int = logging.DEBUG,
    hardware_data: Optional[Dict[str, Any]] = None
):
    """
    Enhanced performance monitoring decorator with hardware awareness.
    
    Args:
        include_memory: Whether to monitor memory usage
        log_level: Logging level for performance messages
        hardware_data: Pre-fetched hardware data (optional)
        
    Returns:
        Decorator function with hardware-aware monitoring
    """
    def decorator(func):
        return performance_monitor_wrapper(func, include_memory, log_level, hardware_data)
    return decorator

def establish_performance_baseline(hardware_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Run performance tests to establish system baselines with hardware awareness.
    """
    try:
        # Get comprehensive hardware data
        if hardware_data is None:
            hardware_data = check_hardware(include_memory_usage=True)
        
        # Extract hardware capabilities
        cuda_info = hardware_data.get('cuda', {})
        ram_info = hardware_data.get('system_ram', {})
        cpu_info = hardware_data.get('cpu_cores', {})
        
        cuda_available = cuda_info.get('available', False)
        gpu_count = cuda_info.get('gpu_count', 0)
        total_ram_gb = ram_info.get('ram_total_gb', 4.0)
        logical_cores = cpu_info.get('logical_cores', 1)
        
        performance_metrics = {
            'timestamp': datetime.now().isoformat(),
            'hardware_context': {
                'cuda_available': cuda_available,
                'gpu_count': gpu_count,
                'total_ram_gb': total_ram_gb,
                'logical_cores': logical_cores
            },
            'baselines': {},
            'system_state': {}
        }
        
        # Record initial system state
        try:
            proc = psutil.Process()
            performance_metrics['system_state']['initial'] = {
                'memory_rss_mb': proc.memory_info().rss / 1024 / 1024,
                'memory_percent': proc.memory_percent(),
                'cpu_percent': proc.cpu_percent()
            }
        except Exception as e:
            performance_metrics['system_state']['initial_error'] = str(e)
        
        # CPU performance test - scale based on available RAM
        logger.debug("Running CPU performance baseline test")
        try:
            # Adaptive matrix size based on available RAM
            if total_ram_gb > 16:
                # High-end systems
                matrix_size = 2000
            elif total_ram_gb > 8:
                # Mid-range systems
                matrix_size = 1500
            else:
                # Low-end systems
                matrix_size = 1000
            
            start_time = time.time()
            test_array = np.random.rand(matrix_size, matrix_size).astype(np.float32)
            result = np.dot(test_array, test_array.T)
            cpu_time = time.time() - start_time
            
            performance_metrics['baselines']['cpu'] = {
                'matrix_size': matrix_size,
                'computation_time': cpu_time,
                'operations_per_second': (matrix_size ** 3) / cpu_time,
                # Input + output
                'memory_used_mb': test_array.nbytes * 2 / 1024 / 1024,
                'gflops': (2 * matrix_size ** 3) / (cpu_time * 1e9)
            }
            
            del test_array, result
            
        except Exception as e:
            performance_metrics['baselines']['cpu_error'] = str(e)
            logger.warning(f"CPU baseline test failed: {e}")
        
        # Memory allocation test - adaptive based on system RAM
        logger.debug("Running memory allocation baseline test")
        try:
            # Test different allocation sizes in MB
            allocation_sizes = []
            if total_ram_gb > 16:
                allocation_sizes = [100, 500, 1000]
            elif total_ram_gb > 8:
                allocation_sizes = [50, 200, 500]
            else:
                allocation_sizes = [20, 100, 200]
            
            memory_baselines = {}
            
            for size_mb in allocation_sizes:
                try:
                    start_memory = psutil.Process().memory_info().rss
                    start_time = time.time()
                    
                    # Create tensor of specified size
                    # 4 bytes per float32
                    elements = int((size_mb * 1024 * 1024) // 4)
                    test_tensor = torch.randn(elements, dtype=torch.float32)
                    
                    allocation_time = time.time() - start_time
                    end_memory = psutil.Process().memory_info().rss
                    actual_memory_mb = (end_memory - start_memory) / 1024 / 1024
                    
                    memory_baselines[f'{size_mb}mb'] = {
                        'target_size_mb': size_mb,
                        'actual_memory_mb': actual_memory_mb,
                        'allocation_time': allocation_time,
                        'allocation_speed_mbs': size_mb / allocation_time if allocation_time > 0 else 0,
                        'overhead_percent': ((actual_memory_mb - size_mb) / size_mb * 100) if size_mb > 0 else 0
                    }
                    
                    del test_tensor
                    
                except Exception as e:
                    memory_baselines[f'{size_mb}mb_error'] = str(e)
            
            performance_metrics['baselines']['memory'] = memory_baselines
            
        except Exception as e:
            performance_metrics['baselines']['memory_error'] = str(e)
            logger.warning(f"Memory baseline test failed: {e}")
        
        # GPU performance tests if available
        if cuda_available:
            logger.debug(f"Running GPU performance baseline test on {gpu_count} GPU(s)")
            try:
                gpu_baselines = {}
                
                for gpu_id in range(gpu_count):
                    try:
                        torch.cuda.set_device(gpu_id)
                        gpu_props = torch.cuda.get_device_properties(gpu_id)
                        gpu_memory_gb = gpu_props.total_memory / (1024**3)
                        
                        # Adaptive GPU test based on GPU memory
                        if gpu_memory_gb > 8:
                            matrix_size = 2000
                        elif gpu_memory_gb > 4:
                            matrix_size = 1500
                        else:
                            matrix_size = 1000
                        
                        # Warm up GPU
                        warm_tensor = torch.randn(100, 100, device=f'cuda:{gpu_id}')
                        torch.mm(warm_tensor, warm_tensor.t())
                        del warm_tensor
                        
                        torch.cuda.synchronize(gpu_id)
                        start_time = time.time()
                        start_memory = torch.cuda.memory_allocated(gpu_id)
                        
                        # GPU computation test
                        gpu_tensor = torch.randn(matrix_size, matrix_size, device=f'cuda:{gpu_id}', dtype=torch.float32)
                        result = torch.mm(gpu_tensor, gpu_tensor.t())
                        
                        torch.cuda.synchronize(gpu_id)
                        end_time = time.time()
                        end_memory = torch.cuda.memory_allocated(gpu_id)
                        
                        gpu_time = end_time - start_time
                        memory_used_mb = (end_memory - start_memory) / 1024 / 1024
                        
                        gpu_baselines[f'gpu_{gpu_id}'] = {
                            'name': gpu_props.name,
                            'compute_capability': f"{gpu_props.major}.{gpu_props.minor}",
                            'total_memory_gb': gpu_memory_gb,
                            'matrix_size': matrix_size,
                            'computation_time': gpu_time,
                            'memory_used_mb': memory_used_mb,
                            'gflops': (2 * matrix_size ** 3) / (gpu_time * 1e9),
                            'memory_bandwidth_gbs': (memory_used_mb / gpu_time / 1024) if gpu_time > 0 else 0,
                            'performance_ratio_vs_cpu': performance_metrics['baselines'].get('cpu', {}).get('computation_time', 1) / gpu_time if gpu_time > 0 else 0
                        }
                        
                        del gpu_tensor, result
                        
                    except Exception as e:
                        gpu_baselines[f'gpu_{gpu_id}_error'] = str(e)
                        logger.warning(f"GPU {gpu_id} baseline test failed: {e}")
                
                performance_metrics['baselines']['gpu'] = gpu_baselines
                
            except Exception as e:
                performance_metrics['baselines']['gpu_error'] = str(e)
                logger.warning(f"GPU baseline tests failed: {e}")
        
        # I/O performance test
        logger.debug("Running I/O performance baseline test")
        try:
            # 1MB of data
            test_data = b"0" * (1024 * 1024)
            test_file = LOG_DIR / "io_test.tmp"
            
            # Write test
            start_time = time.time()
            with open(test_file, 'wb') as f:
                # Write 10MB total
                for _ in range(10):
                    f.write(test_data)
            write_time = time.time() - start_time
            
            # Read test
            start_time = time.time()
            with open(test_file, 'rb') as f:
                while f.read(1024 * 1024):
                    pass
            read_time = time.time() - start_time
            
            # Cleanup
            test_file.unlink()
            
            performance_metrics['baselines']['io'] = {
                'write_time': write_time,
                'read_time': read_time,
                'write_speed_mbs': 10 / write_time if write_time > 0 else 0,
                'read_speed_mbs': 10 / read_time if read_time > 0 else 0,
                'data_size_mb': 10
            }
            
        except Exception as e:
            performance_metrics['baselines']['io_error'] = str(e)
            logger.warning(f"I/O baseline test failed: {e}")
        
        # Record final system state
        try:
            proc = psutil.Process()
            performance_metrics['system_state']['final'] = {
                'memory_rss_mb': proc.memory_info().rss / 1024 / 1024,
                'memory_percent': proc.memory_percent(),
                'cpu_percent': proc.cpu_percent()
            }
            
            # Calculate system impact
            initial_state = performance_metrics['system_state'].get('initial', {})
            final_state = performance_metrics['system_state']['final']
            
            performance_metrics['system_state']['impact'] = {
                'memory_delta_mb': final_state['memory_rss_mb'] - initial_state.get('memory_rss_mb', 0),
                'cpu_delta_percent': final_state['cpu_percent'] - initial_state.get('cpu_percent', 0)
            }
            
        except Exception as e:
            performance_metrics['system_state']['final_error'] = str(e)
        
        # Cleanup and final memory clear
        try:
            enhanced_clear_memory(aggressive=True)
        except Exception as e:
            performance_metrics['cleanup_error'] = str(e)
        
        # Generate performance summary
        summary = {
            'cpu_performance': 'good' if performance_metrics['baselines'].get('cpu', {}).get('gflops', 0) > 1 else 'limited',
            'memory_performance': 'good' if any(
                baseline.get('allocation_speed_mbs', 0) > 100 
                for baseline in performance_metrics['baselines'].get('memory', {}).values()
                if isinstance(baseline, dict)
            ) else 'limited',
            'gpu_available': cuda_available,
            'io_performance': 'good' if performance_metrics['baselines'].get('io', {}).get('write_speed_mbs', 0) > 50 else 'limited',
            'overall_capability': 'high-end' if (
                cuda_available and 
                total_ram_gb > 16 and 
                logical_cores > 8
            ) else 'standard'
        }
        
        performance_metrics['summary'] = summary
        
        logger.debug(f"Performance baseline established: {summary['overall_capability']} system capability")
        return performance_metrics
        
    except Exception as e:
        logger.error(f"Failed to establish performance baseline: {e}")
        return {
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
            'hardware_context': hardware_data.get('cuda', {}) if hardware_data else {}
        }

def enhanced_clear_memory(aggressive: bool = False, hardware_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Enhanced memory clearing with hardware awareness and detailed reporting.
    
    Args:
        aggressive: Whether to perform aggressive memory clearing
        hardware_data: Hardware data from check_hardware() (optional)
        
    Returns:
        Dictionary with clearing results and memory statistics
    """
    try:
        # Suppress individual log messages during system checks
        # logger = logging.getLogger(__name__)
        # original_level = logger.level
        # logger.setLevel(logging.WARNING)  # Only show warnings/errors
        # LOG ALL DETAILED INFORMATION TO FILE ONLY (no console output)
        # Store the original logger level to suppress console handlers temporarily
        handlers_to_suppress = []
        for handler in logger.handlers:
            if isinstance(handler, logging.StreamHandler) and handler.stream.name in ['<stdout>', '<stderr>']:
                handlers_to_suppress.append(handler)
                handler.setLevel(logging.CRITICAL)  # Temporarily suppress console output
        
        try:
            # Get hardware info if not provided
            if hardware_data is None:
                try:
                    hardware_data = check_hardware(include_memory_usage=True)
                except Exception:
                    hardware_data = {}
            
            cuda_info = hardware_data.get('cuda', {})
            ram_info = hardware_data.get('system_ram', {})
            
            cuda_available = cuda_info.get('available', False)
            gpu_count = cuda_info.get('gpu_count', 0)
            total_ram_gb = ram_info.get('ram_total_gb', 4.0)
            
            clearing_results = {
                'hardware_context': {
                    'cuda_available': cuda_available,
                    'gpu_count': gpu_count,
                    'total_ram_gb': total_ram_gb
                },
                'actions_taken': [],
                'memory_before': {},
                'memory_after': {},
                'success': True
            }
            
            # Capture memory state before clearing
            if 'current_usage' in ram_info:
                clearing_results['memory_before']['system_ram'] = ram_info['current_usage']
            
            if cuda_available and 'gpus' in cuda_info:
                clearing_results['memory_before']['gpu_memory'] = {}
                for i, gpu in enumerate(cuda_info['gpus']):
                    if 'current_usage' in gpu:
                        clearing_results['memory_before']['gpu_memory'][f'gpu_{i}'] = gpu['current_usage']
            
            # Clear Python garbage collection
            gc.collect()
            clearing_results['actions_taken'].append("Python garbage collection")
            
            # Clear PyTorch cache with hardware awareness
            if cuda_available:
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                clearing_results['actions_taken'].append("CUDA cache cleared and synchronized")
                
                if aggressive:
                    # Reset peak memory stats for each GPU
                    for i in range(gpu_count):
                        torch.cuda.reset_peak_memory_stats(i)
                        torch.cuda.reset_accumulated_memory_stats(i)
                    clearing_results['actions_taken'].append(f"Reset memory stats for {gpu_count} GPU(s)")
            
            # Aggressive mode with hardware considerations
            if aggressive:
                # More aggressive GC for systems with limited RAM
                gc_rounds = 5 if total_ram_gb < 8 else 3
                for _ in range(gc_rounds):
                    gc.collect()
                clearing_results['actions_taken'].append(f"Aggressive GC ({gc_rounds} rounds)")
                
                # Additional PyTorch optimizations for low-memory systems
                if total_ram_gb < 8:
                    # Reduce memory overhead
                    torch.backends.cudnn.benchmark = False
                    clearing_results['actions_taken'].append("Disabled cuDNN benchmark for memory conservation")
            
            # Capture memory state after clearing (if hardware data included usage)
            try:
                post_clear_hardware = check_hardware(include_memory_usage=True)
                post_ram_info = post_clear_hardware.get('system_ram', {})
                post_cuda_info = post_clear_hardware.get('cuda', {})
                
                if 'current_usage' in post_ram_info:
                    clearing_results['memory_after']['system_ram'] = post_ram_info['current_usage']
                
                if cuda_available and 'gpus' in post_cuda_info:
                    clearing_results['memory_after']['gpu_memory'] = {}
                    for i, gpu in enumerate(post_cuda_info['gpus']):
                        if 'current_usage' in gpu:
                            clearing_results['memory_after']['gpu_memory'][f'gpu_{i}'] = gpu['current_usage']
            except Exception as e:
                clearing_results['memory_after_error'] = str(e)
            
            logger.debug(f"Memory cleared successfully: {', '.join(clearing_results['actions_taken'])}")
            return clearing_results
        
        finally:
            # Restore original logging level
            #logger.setLevel(original_level)
            # Restore original logger levels
            for handler in handlers_to_suppress:
                #handler.setLevel(logging.NOTSET)
                handler.setLevel(logging.ERROR)
            pass
        
    except Exception as e:
        logger.warning(f"Memory clearing failed: {e}")
        return {
            'success': False,
            'error': str(e),
            'actions_taken': [],
            'hardware_context': hardware_data.get('cuda', {}) if hardware_data else {}
        }

# Utility function for external use
def get_version_info(package_name: str) -> Dict[str, str]:
    """
    Get comprehensive version information for a package.
    
    Args:
        package_name: Name of the package to check
        
    Returns:
        Dictionary with version information and availability status
    """
    version_str = safe_version(package_name)
    
    return {
        'package': package_name,
        'version': version_str,
        'available': version_str not in ['N/A', 'unknown'],
        'importlib_metadata_available': IMPORTLIB_METADATA_AVAILABLE,
        'packaging_available': PACKAGING_AVAILABLE
    }

# List custom presets from config directory
def list_custom_presets() -> List[str]:
    """List all available custom presets."""
    custom_dir = CONFIG_DIR / "custom_presets"
    if not custom_dir.exists():
        return []
    
    return [f.stem.replace("preset_", "") for f in custom_dir.glob("preset_*.json")]

# Model Architecture Options
MODEL_VARIANTS = {
    'SimpleAutoencoder': SimpleAutoencoder,
    'EnhancedAutoencoder': EnhancedAutoencoder,
    'AutoencoderEnsemble': AutoencoderEnsemble
}

# Consolidated Preset Configurations
# Initialize empty PRESET_CONFIGS to avoid forward reference errors
PRESET_CONFIGS = {}

def get_available_presets():
    """Dynamically get available preset names."""
    return list(PRESET_CONFIGS.keys()) if PRESET_CONFIGS else ['default', 'stability', 'performance', 'baseline', 'debug', 'lightweight', 'advanced']

def get_preset_descriptions():
    """Dynamically get preset descriptions."""
    return {k: v.get("metadata", {}).get("description", f"{k.title()} preset") 
            for k, v in PRESET_CONFIGS.items() 
            if isinstance(v, dict)} if PRESET_CONFIGS else {
        'default': 'Default balanced configuration for general use',
        'stability': 'High stability configuration for reliable training',
        'performance': 'High-performance configuration for production deployment',
        'baseline': 'Standardized configuration for benchmarking',
        'debug': 'Lightweight configuration for debugging',
        'lightweight': 'Lightweight configuration for edge devices',
        'advanced': 'Advanced configuration for research experiments'
    }

# Preset Configurations for Testing Different Architectures
STABILITY_PRESET = {
    'metadata': {
        'description': 'High stability configuration for reliable training and robust performance',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'stability', 'recommended_hardware': {'gpu_memory_gb': 4, 'cpu_cores': 2, 'ram_gb': 4},
        'compatibility': ['SimpleAutoencoder', 'EnhancedAutoencoder'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets', 'hyperparameter_optimization']
        }
    },
    'training': {
        'batch_size': 32, 'epochs': 100, 'learning_rate': 0.0005, 'patience': 15, 'weight_decay': 1e-3,
        'gradient_clip': 0.5, 'gradient_accumulation_steps': 1, 'mixed_precision': False,
        'num_workers': 2, 'optimizer': 'Adam', 'scheduler': None, 'scheduler_params': {},
        'early_stopping': True, 'validation_split': 0.25, 'shuffle': True,
        'pin_memory': False, 'persistent_workers': False,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 5, 'lr_factor': 0.7, 'min_lr': 1e-6
    },
    'model': {
        'model_type': 'SimpleAutoencoder', 'input_dim': 15, 'encoding_dim': 8, 'hidden_dims': [64],
        'dropout_rates': [0.3], 'activation': 'relu', 'activation_param': 0.0,
        'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False,
        'diversity_factor': 0.0, 'min_features': 5, 'num_models': 1, 'skip_connection': False,
        'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
        'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu', 'tanh'],
        'available_normalizations': ['batch', 'layer', 'instance', None],
        'available_initializers': ['xavier_uniform', 'xavier_normal', 'kaiming_uniform'],
        'legacy_mode': False, 'use_attention': False
    },
    'security': {
        'percentile': 99, 'attack_threshold': 0.2, 'false_negative_cost': 3.0,
        'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
        'early_warning_threshold': 0.15, 'adaptive_threshold': False, 'confidence_interval': 0.99,
        'detection_methods': ['reconstruction_error', 'statistical_analysis'],
        'alert_levels': ['low', 'medium', 'high', 'critical'], 'threshold_validation': True,
        'robust_detection': True, 'false_positive_tolerance': 0.01
    },
    'data': {
        'normal_samples': 5000, 'attack_samples': 1000, 'features': 15, 'use_real_data': False,
        'data_normalization': 'minmax', 'anomaly_factor': 2.0, 'random_state': 42,
        'validation_split': 0.25, 'test_split': 0.25, 'stratified_split': True,
        'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.05, 'anomaly_sparsity': 0.2, 'noise_factor': 0.02, 'correlation_strength': 0.2, 'outlier_ratio': 0.01},
        'preprocessing': {'remove_outliers': True, 'outlier_threshold': 2.5, 'impute_missing': True, 'imputation_strategy': 'median', 'feature_scaling': 'robust', 'data_cleaning': True},
        'shuffle': True, 'pin_memory': False
    },
    'monitoring': {
        'metrics_frequency': 5, 'checkpoint_frequency': 10, 'tensorboard_logging': False,
        'console_logging_level': 'DEBUG', 'save_best_model': True, 'save_model_history': True,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss', 'learning_rate', 'epoch_time', 'gradient_norm', 'training_stability'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
        'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 5, 'save_checkpoints': True,
        'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False, 'max_scalars': 500, 'max_histograms': 50, 'max_images': 10, 'save_summary': False},
        'stability_metrics': True
    },
    'hardware': {
        'device': 'cpu', 'recommended_gpu_memory': 4, 'minimum_system_requirements': {'cpu_cores': 2, 'ram_gb': 4, 'disk_space': 5},
        'optimal_system_requirements': {'cpu_cores': 4, 'ram_gb': 8, 'disk_space': 10, 'gpu_memory': 4},
        'memory_management': {'max_memory_fraction': 0.7, 'allow_memory_growth': False, 'memory_limit': 2048},
        'performance_optimization': {'use_cuda': False, 'use_amp': False, 'benchmark_mode': False, 'deterministic': True}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "stability"), 'log_dir': str(LOG_DIR / "stability"), 'reports_dir': str(REPORTS_DIR / "stability"),
        'config_dir': str(CONFIG_DIR / "stability"), 'data_dir': str(DATA_DIR / "stability"), 'metrics_dir': str(METRICS_DIR / "stability"),
        'checkpoint_dir': str(Path(CHECKPOINTS_DIR / "stability")), 'tensorboard_dir': str(TB_DIR / "stability"), 'results_dir': str(RESULTS_DIR / "stability"),
        'datasets_dir': str(DATASETS_DIR / "stability"), 'artifacts_dir': str(ARTIFACTS_DIR / "stability"), 'figures_dir': str(FIGURES_DIR / "stability"), 'info_dir': str(INFO_DIR / "stability"),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True, 'parallel_processing': False, 'max_workers': 2, 'export_onnx': True, 'non_interactive': False,
        'cuda_optimizations': False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True, 'optimize_for_mobile': False, 'runtime_validation': True, 'validation_tolerance': 1e-5, 'verbose': False}
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'stability', 'current_override': None, 'override_rules': {'security': False, 'monitoring': False, 'hardware': False},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(), 'auto_apply': False, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': False, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_stability', 'direction': 'minimize', 'n_trials': 50, 'timeout': 1800,
        'sampler': 'RandomSampler', 'pruner': 'NopPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-4, 'high': 1e-2, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [16, 32, 64]},
            'dropout_rate': {'type': 'float', 'low': 0.1, 'high': 0.4}
        },
        'early_stopping': {'enabled': True, 'patience': 8, 'min_improvement': 1e-4}, 'timeout_seconds': 1800, 'trial_epochs': 50, 'trial_patience': 6,
        'cleanup_trials': True, 'generate_plots': False,
        'search_space': {
            'encoding_dim_min': 6, 'encoding_dim_max': 12, 'hidden_layers_min': 1, 'hidden_layers_max': 2,
            'lr_min': 1e-4, 'lr_max': 1e-2, 'batch_sizes': [16, 32, 64],
            'weight_decay_min': 1e-4, 'weight_decay_max': 1e-2, 'dropout_min': 0.1, 'dropout_max': 0.4,
            'activations': ["relu", "leaky_relu"], 'normalizations': [None, "batch"],
            'percentile_min': 95, 'percentile_max': 99
        },
        'hpo_sampler': {'type': 'Random', 'seed': 42, 'consider_prior': False, 'prior_weight': 1.0, 'consider_magic_clip': False, 'consider_endpoints': False, 'n_startup_trials': 10, 'n_ei_candidates': 15, 'multivariate': False},
        'hpo_pruner': {'type': 'Nop', 'n_startup_trials': 0, 'n_warmup_steps': 0, 'interval_steps': 1},
        'scoring': {'use_composite_score': False, 'validation_weight': 0.8, 'test_weight': 0.2, 'complexity_weight': 0.1, 'max_params_penalty': 10000},
        'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/stability_study.db", 'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': True, 'folds': 4, 'stratified': True, 'random_state': 42},
        'metrics': ['mse', 'mae', 'r2_score', 'explained_variance', 'precision', 'recall', 'f1_score', 'auc_roc', 'stability_score'],
        'validation_frequency': 1, 'save_validation_results': True, 'detailed_metrics': True, 'robustness_testing': True
    },
    'experimental': {
        'features': {'advanced_logging': False, 'model_interpretability': False, 'federated_learning': False, 'active_learning': False, 'robust_training': True},
        'settings': {'experimental_mode': False, 'beta_features': False, 'research_mode': False}
    }
}

PERFORMANCE_PRESET = {
    'metadata': {
        'description': 'High-performance configuration for production deployment and optimal throughput',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'performance', 'recommended_hardware': {'gpu_memory_gb': 8, 'cpu_cores': 8, 'ram_gb': 16},
        'compatibility': ['EnhancedAutoencoder', 'AutoencoderEnsemble'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets', 'hyperparameter_optimization']
        }
    },
    'training': {
        'batch_size': 128, 'epochs': 200, 'learning_rate': 0.0001, 'patience': 20, 'weight_decay': 1e-5,
        'gradient_clip': 0.1, 'gradient_accumulation_steps': 8, 'mixed_precision': True if torch.cuda.is_available() else False,
        'num_workers': max(4, os.cpu_count() or 4), 'optimizer': 'AdamW', 'scheduler': 'CosineAnnealingLR',
        'scheduler_params': {'T_max': 200, 'eta_min': 1e-7, 'last_epoch': -1},
        'early_stopping': True, 'validation_split': 0.15, 'shuffle': True,
        'pin_memory': True if torch.cuda.is_available() else False, 'persistent_workers': True,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 8, 'lr_factor': 0.5, 'min_lr': 1e-7
    },
    'model': {
        'model_type': 'AutoencoderEnsemble', 'input_dim': 30, 'encoding_dim': 16, 'hidden_dims': [256, 128, 64],
        'dropout_rates': [0.1, 0.1, 0.05], 'activation': 'gelu', 'activation_param': 0.0,
        'normalization': 'batch', 'use_batch_norm': True, 'use_layer_norm': False,
        'diversity_factor': 0.2, 'min_features': 10, 'num_models': 5, 'skip_connection': True,
        'residual_blocks': True, 'bias': True, 'weight_init': 'kaiming_normal',
        'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu', 'swish'],
        'available_normalizations': ['batch', 'layer', 'instance', 'group'],
        'available_initializers': ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal'],
        'legacy_mode': False, 'use_attention': True
    },
    'security': {
        'percentile': 90, 'attack_threshold': 0.4, 'false_negative_cost': 1.5,
        'enable_security_metrics': True, 'anomaly_threshold_strategy': 'dynamic_percentile',
        'early_warning_threshold': 0.35, 'adaptive_threshold': True, 'confidence_interval': 0.95,
        'detection_methods': ['reconstruction_error', 'statistical_analysis', 'ensemble_voting'],
        'alert_levels': ['low', 'medium', 'high', 'critical'], 'threshold_validation': True,
        'performance_optimized_detection': True, 'real_time_monitoring': True
    },
    'data': {
        'normal_samples': 10000, 'attack_samples': 3000, 'features': 30, 'use_real_data': True, 'data_normalization': 'standard', 'anomaly_factor': 1.2, 'random_state': 42,
        'validation_split': 0.15, 'test_split': 0.15, 'stratified_split': True, 'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.15, 'anomaly_sparsity': 0.4, 'noise_factor': 0.03, 'correlation_strength': 0.35, 'high_performance_mode': True},
        'preprocessing': {'remove_outliers': True, 'outlier_threshold': 2.8, 'impute_missing': True, 'imputation_strategy': 'mean', 'feature_scaling': 'standard', 'streaming_processing': True},
        'shuffle': True, 'pin_memory': True if torch.cuda.is_available() else False
    },
    'monitoring': {
        'metrics_frequency': 20, 'checkpoint_frequency': 25, 'tensorboard_logging': True, 'console_logging_level': 'INFO', 'save_best_model': True, 'save_model_history': True,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss', 'learning_rate', 'epoch_time', 'memory_usage', 'throughput', 'latency', 'gpu_utilization'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
        'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 10, 'save_checkpoints': True,
        'tensorboard': {'export_formats': ["json", "csv"], 'include_histograms': True, 'include_images': True, 'max_scalars': 2000, 'max_histograms': 100, 'max_images': 15, 'save_summary': True},
        'performance_metrics': True
    },
    'hardware': {
        'device': 'cuda' if torch.cuda.is_available() else 'cpu', 'recommended_gpu_memory': 8,
        'minimum_system_requirements': {'cpu_cores': 4, 'ram_gb': 8, 'disk_space': 10},
        'optimal_system_requirements': {'cpu_cores': 8, 'ram_gb': 16, 'disk_space': 20, 'gpu_memory': 8},
        'memory_management': {'max_memory_fraction': 0.85, 'allow_memory_growth': True, 'memory_limit': None},
        'performance_optimization': {'use_cuda': True if torch.cuda.is_available() else False, 'use_amp': True if torch.cuda.is_available() else False, 'benchmark_mode': True, 'deterministic': False, 'cudnn_benchmark': True if torch.cuda.is_available() else False}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "performance"), 'log_dir': str(LOG_DIR / "performance"), 'reports_dir': str(REPORTS_DIR / "performance"),
        'config_dir': str(CONFIG_DIR / "performance"), 'data_dir': str(DATA_DIR / "performance"), 'metrics_dir': str(METRICS_DIR / "performance"),
        'checkpoint_dir': str(CHECKPOINTS_DIR / "performance"), 'tensorboard_dir': str(TB_DIR / "performance"), 'results_dir': str(RESULTS_DIR / "performance"),
        'datasets_dir': str(DATASETS_DIR / "performance"), 'artifacts_dir': str(ARTIFACTS_DIR / "performance"), 'figures_dir': str(FIGURES_DIR / "performance"), 'info_dir': str(INFO_DIR / "performance"),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True, 'parallel_processing': True, 'max_workers': max(4, os.cpu_count() or 4),
        'export_onnx': True, 'non_interactive': False, 'cuda_optimizations': True if torch.cuda.is_available() else False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True, 'optimize_for_mobile': False, 'runtime_validation': True, 'validation_tolerance': 1e-5, 'verbose': True},
        'distributed_training': False
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'performance', 'current_override': None, 'override_rules': {'security': True, 'monitoring': True, 'hardware': True},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(), 'auto_apply': True, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': True, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_performance', 'direction': 'minimize', 'n_trials': 200, 'timeout': 7200,
        'sampler': 'TPESampler', 'pruner': 'HyperbandPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 1e-2, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [64, 128, 256]},
            'encoding_dim': {'type': 'int', 'low': 12, 'high': 32},
            'hidden_dims': {'type': 'suggest', 'options': [[128, 64], [256, 128, 64], [512, 256, 128]]},
            'dropout_rate': {'type': 'float', 'low': 0.0, 'high': 0.2},
            'diversity_factor': {'type': 'float', 'low': 0.1, 'high': 0.3}
        },
        'early_stopping': {'enabled': True, 'patience': 12, 'min_improvement': 1e-5}, 'timeout_seconds': 7200, 'trial_epochs': 80, 'trial_patience': 8,
        'cleanup_trials': True, 'generate_plots': True,
        'search_space': {
            'encoding_dim_min': 12, 'encoding_dim_max': 32, 'hidden_layers_min': 2, 'hidden_layers_max': 3,
            'lr_min': 1e-5, 'lr_max': 1e-2, 'batch_sizes': [64, 128, 256],
            'weight_decay_min': 1e-6, 'weight_decay_max': 1e-4, 'dropout_min': 0.0, 'dropout_max': 0.2,
            'activations': ["relu", "leaky_relu", "gelu", "swish"], 'normalizations': ["batch", "layer"],
            'percentile_min': 85, 'percentile_max': 95
        },
        'hpo_sampler': {'type': 'TPE', 'seed': 42, 'consider_prior': True, 'prior_weight': 1.0, 'consider_magic_clip': True, 'consider_endpoints': False, 'n_startup_trials': 20, 'n_ei_candidates': 24, 'multivariate': True},
        'hpo_pruner': {'type': 'Hyperband', 'n_startup_trials': 10, 'n_warmup_steps': 5, 'interval_steps': 1, 'min_resource': 1, 'max_resource': 'auto', 'reduction_factor': 3},
        'scoring': {'use_composite_score': True, 'validation_weight': 0.6, 'test_weight': 0.2, 'complexity_weight': 0.1, 'performance_weight': 0.1, 'max_params_penalty': 50000},
        'storage': {'enabled': True, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/performance_study.db", 'load_if_exists': True, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': True, 'folds': 3, 'stratified': True, 'random_state': 42},
        'metrics': ['mse', 'mae', 'r2_score', 'explained_variance', 'precision', 'recall', 'f1_score', 'auc_roc', 'inference_time', 'throughput'],
        'validation_frequency': 1, 'save_validation_results': True, 'detailed_metrics': True,
        'performance_benchmarking': True
    },
    'experimental': {
        'features': {'advanced_logging': True, 'model_interpretability': False, 'federated_learning': False, 'active_learning': False, 'performance_tuning': True},
        'settings': {'experimental_mode': False, 'beta_features': True, 'research_mode': False}
    }
}

BASELINE_PRESET = {
    'metadata': {
        'description': 'Standardized configuration for benchmarking and performance comparison',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'baseline', 'recommended_hardware': {'gpu_memory_gb': 6, 'cpu_cores': 4, 'ram_gb': 8},
        'compatibility': ['EnhancedAutoencoder', 'SimpleAutoencoder'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets', 'hyperparameter_optimization']
        }
    },
    'training': {
        'batch_size': 64, 'epochs': 75, 'learning_rate': 0.001, 'patience': 10, 'weight_decay': 1e-4,
        'gradient_clip': 1.0, 'gradient_accumulation_steps': 4, 'mixed_precision': False,
        'num_workers': min(4, os.cpu_count() or 1), 'optimizer': 'Adam', 'scheduler': 'ReduceLROnPlateau',
        'scheduler_params': {'mode': 'min', 'factor': 0.5, 'patience': 5, 'min_lr': 1e-6},
        'early_stopping': True, 'validation_split': 0.2, 'shuffle': True,
        'pin_memory': True if torch.cuda.is_available() else False, 'persistent_workers': False,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 5, 'lr_factor': 0.5, 'min_lr': 1e-6
    },
    'model': {
        'model_type': 'EnhancedAutoencoder', 'input_dim': 20, 'encoding_dim': 12, 'hidden_dims': [128, 64],
        'dropout_rates': [0.25, 0.2], 'activation': 'leaky_relu', 'activation_param': 0.1,
        'normalization': 'batch', 'use_batch_norm': True, 'use_layer_norm': False,
        'diversity_factor': 0.05, 'min_features': 5, 'num_models': 1, 'skip_connection': True,
        'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
        'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu', 'tanh'],
        'available_normalizations': ['batch', 'layer', 'instance', None],
        'available_initializers': ['xavier_uniform', 'xavier_normal', 'kaiming_uniform'],
        'legacy_mode': False, 'use_attention': False
    },
    'security': {
        'percentile': 95, 'attack_threshold': 0.3, 'false_negative_cost': 2.0,
        'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
        'early_warning_threshold': 0.25, 'adaptive_threshold': False, 'confidence_interval': 0.95,
        'detection_methods': ['reconstruction_error', 'statistical_analysis'],
        'alert_levels': ['low', 'medium', 'high', 'critical'], 'threshold_validation': True
    },
    'data': {
        'normal_samples': 8000, 'attack_samples': 2000, 'features': 20, 'use_real_data': False, 'data_normalization': 'standard', 'anomaly_factor': 1.5, 'random_state': 42,
        'validation_split': 0.2, 'test_split': 0.2, 'stratified_split': True, 'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.1, 'anomaly_sparsity': 0.3, 'noise_factor': 0.05, 'correlation_strength': 0.3},
        'preprocessing': {'remove_outliers': True, 'outlier_threshold': 3.0, 'impute_missing': True, 'imputation_strategy': 'mean'},
        'shuffle': True, 'pin_memory': True if torch.cuda.is_available() else False
    },
    'monitoring': {
        'metrics_frequency': 10, 'checkpoint_frequency': 5, 'tensorboard_logging': True,
        'console_logging_level': 'INFO', 'save_best_model': True, 'save_model_history': True,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss', 'learning_rate', 'epoch_time', 'memory_usage'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
        'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 10, 'save_checkpoints': True,
        'tensorboard': {'export_formats': ["json", "csv"], 'include_histograms': True, 'include_images': False, 'max_scalars': 1000, 'max_histograms': 50, 'max_images': 10, 'save_summary': True}
    },
    'hardware': {
        'device': 'auto', 'recommended_gpu_memory': 6, 'minimum_system_requirements': {'cpu_cores': 2, 'ram_gb': 4, 'disk_space': 5},
        'optimal_system_requirements': {'cpu_cores': 4, 'ram_gb': 8, 'disk_space': 10, 'gpu_memory': 6},
        'memory_management': {'max_memory_fraction': 0.8, 'allow_memory_growth': True, 'memory_limit': None},
        'performance_optimization': {'use_cuda': True if torch.cuda.is_available() else False, 'use_amp': False, 'benchmark_mode': False, 'deterministic': True}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "baseline"), 'log_dir': str(LOG_DIR / "baseline"), 'reports_dir': str(REPORTS_DIR / "baseline"),
        'config_dir': str(CONFIG_DIR / "baseline"), 'data_dir': str(DATA_DIR / "baseline"), 'metrics_dir': str(METRICS_DIR / "baseline"),
        'checkpoint_dir': str(CHECKPOINTS_DIR / "baseline"), 'tensorboard_dir': str(TB_DIR / "baseline"), 'results_dir': str(RESULTS_DIR / "baseline"),
        'datasets_dir': str(DATASETS_DIR / "baseline"), 'artifacts_dir': str(ARTIFACTS_DIR / "baseline"), 'figures_dir': str(FIGURES_DIR / "baseline"), 'info_dir': str(INFO_DIR / "baseline"),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True,
        'parallel_processing': True, 'max_workers': min(4, os.cpu_count() or 1),
        'export_onnx': True, 'non_interactive': False, 'cuda_optimizations': True if torch.cuda.is_available() else False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True, 'optimize_for_mobile': False, 'runtime_validation': True, 'validation_tolerance': 1e-5, 'verbose': False}
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'baseline', 'current_override': None, 'override_rules': {'security': False, 'monitoring': False, 'hardware': False},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(), 'auto_apply': False, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': False, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_baseline', 'direction': 'minimize', 'n_trials': 100, 'timeout': 3600,
        'sampler': 'TPESampler', 'pruner': 'MedianPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 1e-1, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [32, 64, 128]},
            'encoding_dim': {'type': 'int', 'low': 8, 'high': 24},
            'hidden_dims': {'type': 'suggest', 'options': [[64], [128, 64], [256, 128]]},
            'dropout_rate': {'type': 'float', 'low': 0.1, 'high': 0.4}
        },
        'early_stopping': {'enabled': True, 'patience': 10, 'min_improvement': 1e-4}, 'timeout_seconds': 3600, 'trial_epochs': 50, 'trial_patience': 7,
        'cleanup_trials': True, 'generate_plots': True,
        'search_space': {
            'encoding_dim_min': 8, 'encoding_dim_max': 24, 'hidden_layers_min': 1, 'hidden_layers_max': 2,
            'lr_min': 1e-5, 'lr_max': 1e-1, 'batch_sizes': [32, 64, 128],
            'weight_decay_min': 1e-5, 'weight_decay_max': 1e-3, 'dropout_min': 0.1, 'dropout_max': 0.4,
            'activations': ["relu", "leaky_relu", "gelu"], 'normalizations': ["batch", None],
            'percentile_min': 90, 'percentile_max': 98
        },
        'hpo_sampler': {'type': 'TPE', 'seed': 42, 'consider_prior': True, 'prior_weight': 1.0, 'consider_magic_clip': True, 'consider_endpoints': False, 'n_startup_trials': 20, 'n_ei_candidates': 24, 'multivariate': True},
        'hpo_pruner': {'type': 'Median', 'n_startup_trials': 5, 'n_warmup_steps': 10, 'interval_steps': 1},
        'scoring': {'use_composite_score': False, 'validation_weight': 0.7, 'test_weight': 0.2, 'complexity_weight': 0.1, 'max_params_penalty': 50000},
        'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/baseline_study.db", 'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': False, 'folds': 3, 'stratified': True, 'random_state': 42},
        'metrics': ['mse', 'mae', 'r2_score', 'explained_variance', 'precision', 'recall', 'f1_score', 'auc_roc'],
        'validation_frequency': 1, 'save_validation_results': True, 'detailed_metrics': False
    },
    'experimental': {
        'features': {'advanced_logging': False, 'model_interpretability': False, 'federated_learning': False, 'active_learning': False},
        'settings': {'experimental_mode': False, 'beta_features': False, 'research_mode': False}
    }
}

DEBUG_PRESET = {
    'metadata': {
        'description': 'Lightweight configuration for debugging and rapid development',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'debug', 'recommended_hardware': {'gpu_memory_gb': 2, 'cpu_cores': 1, 'ram_gb': 2},
        'compatibility': ['SimpleAutoencoder'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets']
        }
    },
    'training': {
        'batch_size': 16, 'epochs': 5, 'learning_rate': 0.01, 'patience': 3, 'weight_decay': 0.0, 'gradient_clip': 5.0, 'gradient_accumulation_steps': 1, 'mixed_precision': False,
        'num_workers': 1, 'optimizer': 'SGD', 'scheduler': None, 'scheduler_params': {}, 'early_stopping': False, 'validation_split': 0.3, 'shuffle': True,
        'pin_memory': False, 'persistent_workers': False, 'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 2, 'lr_factor': 0.5, 'min_lr': 1e-7
    },
    'model': {
        'model_type': 'SimpleAutoencoder', 'input_dim': 10, 'encoding_dim': 4, 'hidden_dims': [32], 'dropout_rates': [0.1], 'activation': 'relu', 'activation_param': 0.0,
        'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False, 'diversity_factor': 0.0, 'min_features': 3, 'num_models': 1, 'skip_connection': False,
        'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform', 'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu'], 'available_normalizations': ['batch', 'layer', None],
        'available_initializers': ['xavier_uniform', 'xavier_normal'], 'legacy_mode': False, 'use_attention': False
    },
    'security': {
        'percentile': 85, 'attack_threshold': 0.5, 'false_negative_cost': 1.0, 'enable_security_metrics': False, 'anomaly_threshold_strategy': 'fixed_percentile',
        'early_warning_threshold': 0.45, 'adaptive_threshold': False, 'confidence_interval': 0.8, 'detection_methods': ['reconstruction_error'], 'alert_levels': ['low', 'medium'],
        'threshold_validation': False
    },
    'data': {
        'normal_samples': 100, 'attack_samples': 50, 'features': 10, 'use_real_data': False, 'data_normalization': 'minmax', 'anomaly_factor': 2.0, 'random_state': 42,
        'validation_split': 0.3, 'test_split': 0.3, 'stratified_split': False, 'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.2, 'anomaly_sparsity': 0.5, 'noise_factor': 0.15, 'correlation_strength': 0.1},
        'preprocessing': {'remove_outliers': False, 'outlier_threshold': 3.0, 'impute_missing': False, 'imputation_strategy': 'mean'},
        'shuffle': True, 'pin_memory': False
    },
    'monitoring': {
        'metrics_frequency': 1, 'checkpoint_frequency': 1, 'tensorboard_logging': False, 'console_logging_level': 'DEBUG', 'save_best_model': False, 'save_model_history': False,
        'metrics_to_track': ['loss', 'reconstruction_error'], 'early_stopping_metric': 'loss', 'checkpoint_format': 'pytorch', 'log_model_summary': True, 'tensorboard_dir': str(TB_DIR),
        'log_frequency': 1, 'save_checkpoints': False,
        'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False, 'max_scalars': 100, 'max_histograms': 10, 'max_images': 5, 'save_summary': False}
    },
    'hardware': {
        'device': 'cpu', 'recommended_gpu_memory': 2, 'minimum_system_requirements': {'cpu_cores': 1, 'ram_gb': 2, 'disk_space': 2},
        'optimal_system_requirements': {'cpu_cores': 2, 'ram_gb': 4, 'disk_space': 5}, 'memory_management': {'max_memory_fraction': 0.5, 'allow_memory_growth': False, 'memory_limit': 512},
        'performance_optimization': {'use_cuda': False, 'use_amp': False, 'benchmark_mode': False, 'deterministic': True}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "debug"), 'log_dir': str(LOG_DIR / "debug"), 'reports_dir': str(REPORTS_DIR / "debug"),
        'config_dir': str(CONFIG_DIR / "debug"), 'data_dir': str(DATA_DIR / "debug"), 'metrics_dir': str(METRICS_DIR / "debug"),
        'checkpoint_dir': str(CHECKPOINTS_DIR / "debug"), 'tensorboard_dir': str(TB_DIR / "debug"), 'results_dir': str(RESULTS_DIR / "debug"),
        'datasets_dir': str(DATASETS_DIR / "debug"), 'artifacts_dir': str(ARTIFACTS_DIR / "debug"), 'figures_dir': str(FIGURES_DIR / "debug"), 'info_dir': str(INFO_DIR / "debug"),
        'debug': True, 'verbose': True, 'random_seed': 42, 'reproducible': False,
        'parallel_processing': False, 'max_workers': 1, 'export_onnx': False, 'non_interactive': False, 'cuda_optimizations': False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': False, 'constant_folding': False, 'optimize_for_mobile': False, 'runtime_validation': False, 'validation_tolerance': 1e-5, 'verbose': False}
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'debug', 'current_override': None, 'override_rules': {'security': False, 'monitoring': False, 'hardware': False},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(), 'auto_apply': False, 'validate_compatibility': False
    },
    'hyperparameter_optimization': {
        'enabled': False, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_debug', 'direction': 'minimize', 'n_trials': 10, 'timeout': 600,
        'sampler': 'RandomSampler', 'pruner': 'NopPruner', 'objective_metric': 'loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-3, 'high': 1e-1, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [8, 16, 32]},
            'encoding_dim': {'type': 'int', 'low': 2, 'high': 8}
        },
        'early_stopping': {'enabled': False, 'patience': 3, 'min_improvement': 1e-2}, 'timeout_seconds': 600, 'trial_epochs': 5, 'trial_patience': 2,
        'cleanup_trials': False, 'generate_plots': False,
        'search_space': {
            'encoding_dim_min': 2, 'encoding_dim_max': 8, 'hidden_layers_min': 1, 'hidden_layers_max': 1,
            'lr_min': 1e-3, 'lr_max': 1e-1, 'batch_sizes': [8, 16, 32],
            'weight_decay_min': 0.0, 'weight_decay_max': 0.0, 'dropout_min': 0.0, 'dropout_max': 0.1,
            'activations': ["relu"], 'normalizations': [None], 'percentile_min': 80, 'percentile_max': 90
        },
        'hpo_sampler': {'type': 'Random', 'seed': 42, 'consider_prior': False, 'prior_weight': 1.0, 'consider_magic_clip': False, 'consider_endpoints': False, 'n_startup_trials': 5, 'n_ei_candidates': 10, 'multivariate': False},
        'hpo_pruner': {'type': 'Nop', 'n_startup_trials': 0, 'n_warmup_steps': 0, 'interval_steps': 1},
        'scoring': {'use_composite_score': False, 'validation_weight': 1.0, 'test_weight': 0.0, 'complexity_weight': 0.0, 'max_params_penalty': 1000},
        'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/debug_study.db", 'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': False, 'folds': 2, 'stratified': False, 'random_state': 42}, 'metrics': ['mse', 'mae'], 'validation_frequency': 1,
        'save_validation_results': False, 'detailed_metrics': False
    },
    'experimental': {
        'features': {'advanced_logging': True, 'model_interpretability': False, 'federated_learning': False, 'active_learning': False},
        'settings': {'experimental_mode': True, 'beta_features': False, 'research_mode': False}
    }
}

LIGHTWEIGHT_PRESET = {
    'metadata': {
        'description': 'Lightweight configuration for edge devices and resource-constrained environments',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'lightweight', 'recommended_hardware': {'gpu_memory_gb': 1, 'cpu_cores': 1, 'ram_gb': 2},
        'compatibility': ['SimpleAutoencoder'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets']
        }
    },
    'training': {
        'batch_size': 8, 'epochs': 30, 'learning_rate': 0.005, 'patience': 5, 'weight_decay': 0.0, 'gradient_clip': 2.0, 'gradient_accumulation_steps': 1, 'mixed_precision': False,
        'num_workers': 1, 'optimizer': 'Adam', 'scheduler': None, 'scheduler_params': {}, 'early_stopping': True, 'validation_split': 0.25, 'shuffle': True,
        'pin_memory': False, 'persistent_workers': False, 'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 3, 'lr_factor': 0.7, 'min_lr': 1e-6
    },
    'model': {
        'model_type': 'SimpleAutoencoder', 'input_dim': 12, 'encoding_dim': 6, 'hidden_dims': [48], 'dropout_rates': [0.15], 'activation': 'relu', 'activation_param': 0.0,
        'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False, 'diversity_factor': 0.0, 'min_features': 4, 'num_models': 1, 'skip_connection': False,
        'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform', 'model_types': list(MODEL_VARIANTS.keys()), 'available_activations': ['relu', 'leaky_relu', 'gelu'],
        'available_normalizations': ['batch', 'layer', None], 'available_initializers': ['xavier_uniform', 'xavier_normal'], 'legacy_mode': False, 'use_attention': False
    },
    'security': {
        'percentile': 92, 'attack_threshold': 0.35, 'false_negative_cost': 1.2, 'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
        'early_warning_threshold': 0.3, 'adaptive_threshold': False, 'confidence_interval': 0.9, 'detection_methods': ['reconstruction_error'], 'alert_levels': ['low', 'medium', 'high'],
        'threshold_validation': False
    },
    'data': {
        'normal_samples': 2000, 'attack_samples': 500, 'features': 12, 'use_real_data': False, 'data_normalization': 'minmax', 'anomaly_factor': 1.8, 'random_state': 42,
        'validation_split': 0.25, 'test_split': 0.25, 'stratified_split': False, 'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.08, 'anomaly_sparsity': 0.25, 'noise_factor': 0.1, 'correlation_strength': 0.2},
        'preprocessing': {'remove_outliers': False, 'outlier_threshold': 3.0, 'impute_missing': True, 'imputation_strategy': 'mean'}, 'shuffle': True, 'pin_memory': False
    },
    'monitoring': {
        'metrics_frequency': 5, 'checkpoint_frequency': 5, 'tensorboard_logging': False, 'console_logging_level': 'INFO', 'save_best_model': True, 'save_model_history': False,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss'], 'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
        'log_model_summary': False, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 5, 'save_checkpoints': True,
        'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False, 'max_scalars': 100, 'max_histograms': 10, 'max_images': 5, 'save_summary': False}
    },
    'hardware': {
        'device': 'cpu', 'recommended_gpu_memory': 1, 'minimum_system_requirements': {'cpu_cores': 1, 'ram_gb': 2, 'disk_space': 2},
        'optimal_system_requirements': {'cpu_cores': 2, 'ram_gb': 4, 'disk_space': 5}, 'memory_management': {'max_memory_fraction': 0.6, 'allow_memory_growth': False, 'memory_limit': 1024},
        'performance_optimization': {'use_cuda': False, 'use_amp': False, 'benchmark_mode': False, 'deterministic': True}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "lightweight"), 'log_dir': str(LOG_DIR / "lightweight"), 'reports_dir': str(REPORTS_DIR / "lightweight"),
        'config_dir': str(CONFIG_DIR / "lightweight"), 'data_dir': str(DATA_DIR / "lightweight"), 'metrics_dir': str(METRICS_DIR / "lightweight"),
        'checkpoint_dir': str(CHECKPOINTS_DIR / "lightweight"), 'tensorboard_dir': str(TB_DIR / "lightweight"), 'results_dir': str(RESULTS_DIR / "lightweight"),
        'datasets_dir': str(DATASETS_DIR / "lightweight"), 'artifacts_dir': str(ARTIFACTS_DIR / "lightweight"), 'figures_dir': str(FIGURES_DIR / "lightweight"), 'info_dir': str(INFO_DIR / "lightweight"),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True, 'parallel_processing': False, 'max_workers': 1, 'export_onnx': True, 'non_interactive': False,
        'cuda_optimizations': False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True, 'optimize_for_mobile': True, 'runtime_validation': True, 'validation_tolerance': 1e-5, 'verbose': False}
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'lightweight', 'current_override': None, 'override_rules': {'security': False, 'monitoring': False, 'hardware': False},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(), 'auto_apply': False, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': False, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_lightweight', 'direction': 'minimize', 'n_trials': 20, 'timeout': 1800,
        'sampler': 'RandomSampler', 'pruner': 'NopPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-4, 'high': 1e-2, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [8, 16, 32]},
            'encoding_dim': {'type': 'int', 'low': 4, 'high': 12},
            'dropout_rate': {'type': 'float', 'low': 0.0, 'high': 0.3}
        },
        'early_stopping': {'enabled': False, 'patience': 5, 'min_improvement': 1e-3}, 'timeout_seconds': 1800, 'trial_epochs': 20, 'trial_patience': 4,
        'cleanup_trials': True, 'generate_plots': False,
        'search_space': {
            'encoding_dim_min': 4, 'encoding_dim_max': 12, 'hidden_layers_min': 1, 'hidden_layers_max': 1,
            'lr_min': 1e-4, 'lr_max': 1e-2, 'batch_sizes': [8, 16, 32],
            'weight_decay_min': 0.0, 'weight_decay_max': 1e-4, 'dropout_min': 0.0, 'dropout_max': 0.3,
            'activations': ["relu", "leaky_relu"], 'normalizations': [None], 'percentile_min': 90, 'percentile_max': 95
        },
        'hpo_sampler': {'type': 'Random', 'seed': 42, 'consider_prior': False, 'prior_weight': 1.0, 'consider_magic_clip': False, 'consider_endpoints': False, 'n_startup_trials': 10, 'n_ei_candidates': 15, 'multivariate': False},
        'hpo_pruner': {'type': 'Nop', 'n_startup_trials': 0, 'n_warmup_steps': 0, 'interval_steps': 1},
        'scoring': {'use_composite_score': False, 'validation_weight': 0.8, 'test_weight': 0.2, 'complexity_weight': 0.1, 'max_params_penalty': 5000},
        'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/lightweight_study.db", 'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': False, 'folds': 3, 'stratified': False, 'random_state': 42}, 'metrics': ['mse', 'mae', 'precision', 'recall', 'f1_score'],
        'validation_frequency': 2, 'save_validation_results': False, 'detailed_metrics': False
    },
    'experimental': {
        'features': {'advanced_logging': False, 'model_interpretability': False, 'federated_learning': False, 'active_learning': False},
        'settings': {'experimental_mode': False, 'beta_features': False, 'research_mode': False}
    }
}

ADVANCED_PRESET = {
    'metadata': {
        'description': 'Advanced configuration for research experiments and high-performance systems',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'advanced', 'recommended_hardware': {'gpu_memory_gb': 16, 'cpu_cores': 16, 'ram_gb': 32},
        'compatibility': ['EnhancedAutoencoder', 'AutoencoderEnsemble'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets', 'hyperparameter_optimization']
        }
    },
    'training': {
        'batch_size': 256, 'epochs': 300, 'learning_rate': 0.00005, 'patience': 30, 'weight_decay': 1e-6,
        'gradient_clip': 0.05, 'gradient_accumulation_steps': 16, 'mixed_precision': True if torch.cuda.is_available() else False,
        'num_workers': max(8, os.cpu_count() or 8), 'optimizer': 'AdamW', 'scheduler': 'CosineAnnealingWarmRestarts',
        'scheduler_params': {'T_0': 50, 'T_mult': 2, 'eta_min': 1e-7, 'last_epoch': -1},
        'early_stopping': True, 'validation_split': 0.1, 'shuffle': True, 'pin_memory': True if torch.cuda.is_available() else False, 'persistent_workers': True,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 10, 'lr_factor': 0.5, 'min_lr': 1e-8
    },
    'model': {
        'model_type': 'AutoencoderEnsemble', 'input_dim': 50, 'encoding_dim': 24, 'hidden_dims': [512, 256, 128, 64],
        'dropout_rates': [0.05, 0.05, 0.03, 0.02], 'activation': 'gelu', 'activation_param': 0.0, 'normalization': 'layer', 'use_batch_norm': False, 'use_layer_norm': True,
        'diversity_factor': 0.3, 'min_features': 15, 'num_models': 7, 'skip_connection': True, 'residual_blocks': True, 'bias': True, 'weight_init': 'kaiming_normal',
        'model_types': list(MODEL_VARIANTS.keys()), 'available_activations': ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish'],
        'available_normalizations': ['batch', 'layer', 'instance', 'group', None], 'available_initializers': ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'orthogonal'],
        'legacy_mode': False, 'use_attention': True
    },
    'security': {
        'percentile': 88, 'attack_threshold': 0.45, 'false_negative_cost': 1.0, 'enable_security_metrics': True, 'anomaly_threshold_strategy': 'dynamic_percentile',
        'early_warning_threshold': 0.4, 'adaptive_threshold': True, 'confidence_interval': 0.99, 'detection_methods': ['reconstruction_error', 'statistical_analysis', 'mahalanobis_distance', 'isolation_forest'],
        'alert_levels': ['low', 'medium', 'high', 'critical'], 'threshold_validation': True, 'ensemble_voting': 'weighted', 'uncertainty_threshold': 0.2
    },
    'data': {
        'normal_samples': 20000, 'attack_samples': 5000, 'features': 50, 'use_real_data': True, 'data_normalization': 'standard', 'anomaly_factor': 1.1, 'random_state': 42,
        'validation_split': 0.1, 'test_split': 0.1, 'stratified_split': True, 'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.2, 'anomaly_sparsity': 0.5, 'noise_factor': 0.02, 'correlation_strength': 0.4, 'feature_interactions': True},
        'preprocessing': {'remove_outliers': True, 'outlier_threshold': 2.5, 'impute_missing': True, 'imputation_strategy': 'median', 'feature_scaling': 'robust', 'dimensionality_reduction': 'auto'},
        'shuffle': True, 'pin_memory': True if torch.cuda.is_available() else False
    },
    'monitoring': {
        'metrics_frequency': 50, 'checkpoint_frequency': 50, 'tensorboard_logging': True, 'console_logging_level': 'DEBUG', 'save_best_model': True, 'save_model_history': True,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss', 'training_accuracy', 'validation_accuracy', 'learning_rate', 'epoch_time', 'memory_usage', 'gradient_norm', 'parameter_updates'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch', 'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 10, 'save_checkpoints': True,
        'tensorboard': {'export_formats': ["json", "csv"], 'include_histograms': True, 'include_images': True, 'max_scalars': 5000, 'max_histograms': 200, 'max_images': 20, 'save_summary': True},
        'profiling_enabled': True, 'performance_metrics': True
    },
    'hardware': {
        'device': 'cuda' if torch.cuda.is_available() else 'cpu', 'recommended_gpu_memory': 16, 'minimum_system_requirements': {'cpu_cores': 8, 'ram_gb': 16, 'disk_space': 20},
        'optimal_system_requirements': {'cpu_cores': 16, 'ram_gb': 32, 'disk_space': 50, 'gpu_memory': 16}, 'memory_management': {'max_memory_fraction': 0.9, 'allow_memory_growth': True, 'memory_limit': None},
        'performance_optimization': {'use_cuda': True if torch.cuda.is_available() else False, 'use_amp': True if torch.cuda.is_available() else False, 'benchmark_mode': True, 'deterministic': False, 'cudnn_benchmark': True if torch.cuda.is_available() else False}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "advanced"), 'log_dir': str(LOG_DIR / "advanced"), 'reports_dir': str(REPORTS_DIR / "advanced"),
        'config_dir': str(CONFIG_DIR / "advanced"), 'data_dir': str(DATA_DIR / "advanced"), 'metrics_dir': str(METRICS_DIR / "advanced"),
        'checkpoint_dir': str(CHECKPOINTS_DIR / "advanced"), 'tensorboard_dir': str(TB_DIR / "advanced"), 'results_dir': str(RESULTS_DIR / "advanced"),
        'datasets_dir': str(DATASETS_DIR / "advanced"), 'artifacts_dir': str(ARTIFACTS_DIR / "advanced"), 'figures_dir': str(FIGURES_DIR / "advanced"), 'info_dir': str(INFO_DIR / "advanced"),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True, 'parallel_processing': True, 'max_workers': max(8, os.cpu_count() or 8),
        'export_onnx': True, 'non_interactive': False, 'cuda_optimizations': True if torch.cuda.is_available() else False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True, 'optimize_for_mobile': False, 'runtime_validation': True, 'validation_tolerance': 1e-5, 'verbose': True},
        'distributed_training': False
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'advanced', 'current_override': None, 'override_rules': {'security': True, 'monitoring': True, 'hardware': True},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(), 'auto_apply': True, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': True, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_advanced', 'direction': 'minimize', 'n_trials': 500, 'timeout': 14400,
        'sampler': 'TPESampler', 'pruner': 'HyperbandPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-6, 'high': 1e-3, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [64, 128, 256, 512]},
            'encoding_dim': {'type': 'int', 'low': 16, 'high': 64},
            'hidden_dims': {'type': 'suggest', 'options': [[256, 128], [512, 256, 128], [1024, 512, 256, 128]]},
            'dropout_rate': {'type': 'float', 'low': 0.0, 'high': 0.3},
            'diversity_factor': {'type': 'float', 'low': 0.1, 'high': 0.5},
            'weight_decay': {'type': 'float', 'low': 1e-7, 'high': 1e-4, 'log': True}
        },
        'early_stopping': {'enabled': True, 'patience': 15, 'min_improvement': 1e-5}, 'timeout_seconds': 14400, 'trial_epochs': 100, 'trial_patience': 10,
        'cleanup_trials': True, 'generate_plots': True,
        'search_space': {
            'encoding_dim_min': 16, 'encoding_dim_max': 64, 'hidden_layers_min': 2, 'hidden_layers_max': 4,
            'lr_min': 1e-6, 'lr_max': 1e-3, 'batch_sizes': [64, 128, 256, 512],
            'weight_decay_min': 1e-7, 'weight_decay_max': 1e-4, 'dropout_min': 0.0, 'dropout_max': 0.3,
            'activations': ["relu", "leaky_relu", "gelu", "swish"], 'normalizations': ["batch", "layer", "instance"],
            'percentile_min': 85, 'percentile_max': 95
        },
        'hpo_sampler': {'type': 'TPE', 'seed': 42, 'consider_prior': True, 'prior_weight': 1.0, 'consider_magic_clip': True, 'consider_endpoints': False, 'n_startup_trials': 20, 'n_ei_candidates': 24, 'multivariate': True},
        'hpo_pruner': {'type': 'Hyperband', 'n_startup_trials': 10, 'n_warmup_steps': 5, 'interval_steps': 1, 'min_resource': 1, 'max_resource': 'auto', 'reduction_factor': 3},
        'scoring': {'use_composite_score': True, 'validation_weight': 0.7, 'test_weight': 0.2, 'complexity_weight': 0.1, 'max_params_penalty': 100000},
        'storage': {'enabled': True, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/advanced_study.db", 'load_if_exists': True, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': True, 'folds': 5, 'stratified': True, 'random_state': 42},
        'metrics': ['mse', 'mae', 'r2_score', 'explained_variance', 'precision', 'recall', 'f1_score', 'auc_roc', 'average_precision', 'brier_score'],
        'validation_frequency': 1, 'save_validation_results': True, 'detailed_metrics': True, 'confidence_intervals': True
    },
    'experimental': {
        'features': {'advanced_logging': True, 'model_interpretability': True, 'federated_learning': False, 'active_learning': True, 'bayesian_optimization': True},
        'settings': {'experimental_mode': True, 'beta_features': True, 'research_mode': True, 'enable_debugging': False}
    }
}

DEFAULT_PRESET = {
    'metadata': {
        'description': 'Default balanced configuration for general use',
        'version': '2.1',
        'config_version': '2.1',
        'config_type': 'autoencoder',
        'created': datetime.now().isoformat(),
        'last_modified': datetime.now().isoformat(),
        'preset_used': 'default',
        'recommended_hardware': {'gpu_memory_gb': 8, 'cpu_cores': 4, 'ram_gb': 8},
        'compatibility': ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble'],
        'system': {
            'python_version': platform.python_version(),
            'platform': platform.platform(),
            'architecture': platform.machine(),
            'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(),
            'os': platform.system(),
            'os_release': platform.release(),
            'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets', 'hyperparameter_optimization']
        }
    },
    'training': {
        'batch_size': 64, 'epochs': 100, 'learning_rate': 0.001, 'patience': 10, 'weight_decay': 1e-4,
        'gradient_clip': 1.0, 'gradient_accumulation_steps': 4, 'mixed_precision': True if torch.cuda.is_available() else False,
        'num_workers': min(4, os.cpu_count() or 1), 'optimizer': 'AdamW', 'scheduler': 'ReduceLROnPlateau',
        'scheduler_params': {'mode': 'min', 'factor': 0.5, 'patience': 5, 'min_lr': 1e-6},
        'early_stopping': True, 'validation_split': 0.2, 'shuffle': True,
        'pin_memory': True if torch.cuda.is_available() else False, 'persistent_workers': False,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 2, 'lr_factor': 0.5, 'min_lr': 1e-7
    },
    'model': {
        'model_type': 'EnhancedAutoencoder', 'input_dim': 20, 'encoding_dim': 12, 'hidden_dims': [128, 64], 'dropout_rates': [0.2, 0.15], 'activation': 'leaky_relu', 'activation_param': 0.2,
        'normalization': 'batch', 'use_batch_norm': True, 'use_layer_norm': False, 'diversity_factor': 0.1, 'min_features': 5, 'num_models': 1, 'skip_connection': True,
        'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform', 'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid'], 'available_normalizations': ['batch', 'layer', 'instance', None],
        'available_initializers': ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal'], 'legacy_mode': False, 'use_attention': False
    },
    'security': {
        'percentile': 95, 'attack_threshold': 0.3, 'false_negative_cost': 2.0, 'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
        'early_warning_threshold': 0.25, 'adaptive_threshold': True, 'confidence_interval': 0.95, 'detection_methods': ['reconstruction_error', 'statistical_analysis'],
        'alert_levels': ['low', 'medium', 'high', 'critical'], 'threshold_validation': True
    },
    'data': {
        'normal_samples': 8000, 'attack_samples': 2000, 'features': 20, 'use_real_data': False, 'data_normalization': 'standard', 'anomaly_factor': 1.5, 'random_state': 42,
        'validation_split': 0.2, 'test_split': 0.2, 'stratified_split': True, 'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.1, 'anomaly_sparsity': 0.3, 'noise_factor': 0.05, 'correlation_strength': 0.3},
        'preprocessing': {'remove_outliers': True, 'outlier_threshold': 3.0, 'impute_missing': True, 'imputation_strategy': 'mean'},
        'shuffle': True, 'pin_memory': True if torch.cuda.is_available() else False
    },
    'monitoring': {
        'metrics_frequency': 10, 'checkpoint_frequency': 5, 'tensorboard_logging': True, 'console_logging_level': 'INFO', 'save_best_model': True, 'save_model_history': True,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss', 'learning_rate', 'epoch_time', 'memory_usage'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch', 'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 1, 'save_checkpoints': True,
        'tensorboard': {'export_formats': ["json", "csv"], 'include_histograms': False, 'include_images': False, 'max_scalars': 1000, 'max_histograms': 100, 'max_images': 10, 'save_summary': True}
    },
    'hardware': {
        'device': 'auto', 'recommended_gpu_memory': 8, 'minimum_system_requirements': {'cpu_cores': 2, 'ram_gb': 4, 'disk_space': 5},
        'optimal_system_requirements': {'cpu_cores': 4, 'ram_gb': 8, 'disk_space': 10, 'gpu_memory': 8},
        'memory_management': {'max_memory_fraction': 0.8, 'allow_memory_growth': True, 'memory_limit': None},
        'performance_optimization': {'use_cuda': True if torch.cuda.is_available() else False, 'use_amp': True if torch.cuda.is_available() else False, 'benchmark_mode': True, 'deterministic': False}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR), 'log_dir': str(LOG_DIR), 'config_dir': str(CONFIG_DIR), 'reports_dir': str(REPORTS_DIR / "default"), 'metrics_dir': str(METRICS_DIR / "default"),
        'data_dir': str(DATA_DIR / "default"), 'checkpoint_dir': str(CHECKPOINTS_DIR / "default"), 'tensorboard_dir': str(TB_DIR / "default"), 'results_dir': str(RESULTS_DIR / "default"),
        'datasets_dir': str(DATASETS_DIR / "default"), 'artifacts_dir': str(ARTIFACTS_DIR / "default"), 'figures_dir': str(FIGURES_DIR / "default"), 'info_dir': str(INFO_DIR / "default"),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True, 'parallel_processing': True, 'max_workers': min(4, os.cpu_count() or 1),
        'export_onnx': False, 'non_interactive': False, 'cuda_optimizations': True if torch.cuda.is_available() else False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True, 'optimize_for_mobile': False, 'runtime_validation': True, 'validation_tolerance': 1e-5, 'verbose': False}
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'default', 'current_override': None, 'override_rules': {'security': False, 'monitoring': True, 'hardware': False},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(), 'auto_apply': False, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': False, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo', 'direction': 'minimize', 'n_trials': 50, 'timeout': 3600,
        'sampler': 'TPESampler', 'pruner': 'MedianPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 1e-1, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [16, 32, 64, 128]},
            'encoding_dim': {'type': 'int', 'low': 4, 'high': 32},
            'hidden_dims': {'type': 'suggest', 'options': [[64], [128, 64], [256, 128, 64]]},
            'dropout_rate': {'type': 'float', 'low': 0.0, 'high': 0.5}
        },
        'early_stopping': {'enabled': True, 'patience': 10, 'min_improvement': 1e-4}, 'timeout_seconds': 3600, 'trial_epochs': 30, 'trial_patience': 5,
        'cleanup_trials': True, 'generate_plots': True,
        'search_space': {
            'encoding_dim_min': 4, 'encoding_dim_max': 64, 'hidden_layers_min': 1, 'hidden_layers_max': 3,
            'lr_min': 1e-5, 'lr_max': 1e-2, 'batch_sizes': [32, 64, 128, 256],
            'weight_decay_min': 1e-6, 'weight_decay_max': 1e-2, 'dropout_min': 0.1, 'dropout_max': 0.5,
            'activations': ["relu", "leaky_relu", "gelu"], 'normalizations': [None, "batch", "layer"],
            'percentile_min': 90, 'percentile_max': 99
        },
        'hpo_sampler': {'type': 'TPE', 'seed': 42, 'consider_prior': True, 'prior_weight': 1.0, 'consider_magic_clip': True, 'consider_endpoints': False, 'n_startup_trials': 10, 'n_ei_candidates': 24, 'multivariate': False},
        'hpo_pruner': {'type': 'Median', 'n_startup_trials': 5, 'n_warmup_steps': 10, 'interval_steps': 1},
        'scoring': {'use_composite_score': False, 'validation_weight': 0.7, 'test_weight': 0.2, 'complexity_weight': 0.1, 'max_params_penalty': 100000},
        'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/study.db", 'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': False, 'folds': 5, 'stratified': True, 'random_state': 42},
        'metrics': ['mse', 'mae', 'r2_score', 'explained_variance', 'precision', 'recall', 'f1_score', 'auc_roc'],
        'validation_frequency': 1, 'save_validation_results': True, 'detailed_metrics': False
    },
    'experimental': {
        'features': {'advanced_logging': False, 'model_interpretability': False, 'federated_learning': False, 'active_learning': False},
        'settings': {'experimental_mode': False, 'beta_features': False, 'research_mode': False}
    }
}

# Populate PRESET_CONFIGS after all presets are defined
PRESET_CONFIGS.update({
    'default': DEFAULT_PRESET,
    'stability': STABILITY_PRESET,
    'performance': PERFORMANCE_PRESET,
    'baseline': BASELINE_PRESET,
    'debug': DEBUG_PRESET,
    'lightweight': LIGHTWEIGHT_PRESET,
    'advanced': ADVANCED_PRESET
})

# Add specialized configs that don't have preset equivalents
PRESET_CONFIGS.update({
    'stability_test': {
        **STABILITY_PRESET,
        'metadata': {
            **STABILITY_PRESET['metadata'],
            'description': 'Stability-focused configuration for architecture testing',
            'config_type': 'architecture_test',
            'base_preset': 'stability'
        },
        'testing': {
            'num_architecture_variants': 5,
            'stability_threshold': 0.95,
            'convergence_tolerance': 1e-4,
            'max_variance': 0.1,
            'test_cycles': 3,
            'stability_metrics': ['loss_variance', 'gradient_norm', 'parameter_updates']
        }
    },
    'performance_test': {
        **PERFORMANCE_PRESET,
        'metadata': {
            **PERFORMANCE_PRESET['metadata'],
            'description': 'Performance-optimized configuration for architecture testing',
            'config_type': 'performance_test',
            'base_preset': 'performance'
        },
        'performance_metrics': {
            'target_throughput': 1000,
            'max_latency': 50,
            'memory_threshold': 0.8,
            'warmup_cycles': 3,
            'measurement_cycles': 5,
            'stability_requirement': 0.9
        },
        'architecture_tests': {
            'variants_to_test': [
                {'name': 'baseline', 'config': 'performance'},
                {'name': 'reduced_ensemble', 'num_models': 3},
                {'name': 'increased_dropout', 'dropout_rates': [0.2, 0.15]},
                {'name': 'batch_norm_only', 'use_batch_norm': True, 'use_layer_norm': False}
            ],
            'comparison_metrics': [
                'throughput', 'latency', 'memory_usage', 
                'reconstruction_error', 'training_stability'
            ]
        }
    }
})

_cached_config = None
_config_cache_time = None

def invalidate_config_cache():
    """Invalidate the configuration cache to force reload with enhanced logging."""
    global _cached_config, _config_cache_time
    
    # Check if cache was active before invalidation
    was_cached = _cached_config is not None and _config_cache_time is not None
    
    if was_cached:
        cache_age = time.time() - _config_cache_time
        logger.debug(f"Invalidating configuration cache (age: {cache_age:.1f}s)")
    
    _cached_config = None
    _config_cache_time = None
    
    # Log cache invalidation for debugging
    if was_cached:
        logger.info("Configuration cache invalidated - next access will reload from source")
    else:
        logger.debug("Cache invalidation requested but no cache was active")

def get_safe_custom_presets() -> List[str]:
    """Safely get custom presets list with error handling."""
    try:
        return list_custom_presets() if 'list_custom_presets' in globals() and callable(list_custom_presets) else []
    except Exception as e:
        logger.debug(f"Could not get custom presets: {e}")
        return []

def _update_preset_information(config: Dict[str, Any]) -> None:
    """Centralized function to update preset information across all functions."""
    try:
        if 'presets' not in config:
            config['presets'] = {}
        
        config['presets'].update({
            'available_presets': get_available_presets(),
            'preset_configs': get_preset_descriptions(),
            'custom_presets_available': get_safe_custom_presets()
        })
    except Exception as e:
        logger.debug(f"Failed to update preset information: {e}")

def ensure_preset_consistency(config: Dict[str, Any]) -> Dict[str, Any]:
    """Ensure preset configuration is consistent and up-to-date."""
    try:
        _update_preset_information(config)
        
        # Validate current preset
        current_preset = config.get('presets', {}).get('current_preset')
        if current_preset and current_preset not in get_available_presets():
            logger.warning(f"Current preset '{current_preset}' is not available, clearing")
            config['presets']['current_preset'] = None
        
        return config
        
    except Exception as e:
        logger.warning(f"Preset consistency check failed: {e}")
        return config



@enhanced_monitor_performance(include_memory=True, log_level=logging.DEBUG)
def get_system_info(
    include_versions: bool = True,
    include_hardware: bool = True,
    include_memory_usage: bool = True,
    include_detailed_analysis: bool = False,
    include_performance_baseline: bool = False,
    include_memory_optimization: bool = False
) -> Dict[str, Any]:
    """
    Gather system information by fully leveraging existing check functions.
    Now enhanced with performance monitoring, baseline establishment, and memory management.
    All helper functions are integrated directly for optimal performance.
    
    Args:
        include_versions: Whether to include version information
        include_hardware: Whether to include hardware information  
        include_memory_usage: Whether to include current memory usage statistics
        include_detailed_analysis: Whether to include performance analysis and recommendations
        include_performance_baseline: Whether to establish performance baseline during collection
        include_memory_optimization: Whether to perform memory optimization during collection
        
    Returns:
        Dictionary with system information, analysis, and recommendations
    """
    try:
        # Initialize system info structure with performance tracking
        system_info = {
            'timestamp': datetime.now().isoformat(),
            'collection_metadata': {
                'include_versions': include_versions,
                'include_hardware': include_hardware,
                'include_memory_usage': include_memory_usage,
                'include_detailed_analysis': include_detailed_analysis,
                'include_performance_baseline': include_performance_baseline,
                'include_memory_optimization': include_memory_optimization,
                'collection_duration_ms': 0,
                'data_sources': [],
                'warnings': [],
                'errors': [],
                'performance_metrics': {},
                'memory_optimization_results': {}
            }
        }
        
        start_time = time.time()
        
        # Pre-collection memory state capture
        if include_memory_usage or include_memory_optimization:
            try:
                # Capture detailed memory state for monitoring system info collection impact
                initial_memory_state = {
                    'timestamp': datetime.now().isoformat()
                }
                
                # Process memory info
                if OPTIONAL_DEPENDENCIES.get('psutil', False):
                    proc = psutil.Process()
                    mem_info = proc.memory_info()
                    initial_memory_state.update({
                        'process_rss_mb': mem_info.rss / (1024**2),
                        'process_vms_mb': mem_info.vms / (1024**2),
                        'process_percent': proc.memory_percent()
                    })
                    
                    # System memory info
                    sys_mem = psutil.virtual_memory()
                    initial_memory_state.update({
                        'system_total_gb': sys_mem.total / (1024**3),
                        'system_available_gb': sys_mem.available / (1024**3),
                        'system_used_gb': sys_mem.used / (1024**3),
                        'system_percent': sys_mem.percent
                    })
                
                # GPU memory if available
                if torch.cuda.is_available():
                    try:
                        gpu_memory = {}
                        for i in range(torch.cuda.device_count()):
                            gpu_memory[f'gpu_{i}'] = {
                                'allocated_mb': torch.cuda.memory_allocated(i) / (1024**2),
                                'reserved_mb': torch.cuda.memory_reserved(i) / (1024**2)
                            }
                        initial_memory_state['gpu_memory'] = gpu_memory
                    except:
                        pass
                
                system_info['collection_metadata']['initial_memory_state'] = initial_memory_state
                logger.debug(f"Initial memory state: {initial_memory_state.get('process_rss_mb', 0):.1f}MB RSS")
            except Exception as e:
                system_info['collection_metadata']['warnings'].append(f"Failed to capture initial memory state: {e}")
        
        # Optional memory optimization before intensive operations
        if include_memory_optimization:
            try:
                logger.debug("Performing pre-collection memory optimization")
                optimization_results = enhanced_clear_memory(aggressive=True)
                system_info['collection_metadata']['memory_optimization_results']['pre_collection'] = optimization_results
                
                if optimization_results.get('success', False):
                    logger.debug(f"Pre-collection optimization: {', '.join(optimization_results.get('actions_taken', []))}")
                else:
                    logger.warning("Pre-collection memory optimization failed")
            except Exception as e:
                system_info['collection_metadata']['errors'].append(f"Pre-collection memory optimization failed: {e}")
        
        # Core platform information (always included) - monitored
        try:
            with _monitored_operation("platform_info_collection") as op_monitor:
                system_info['platform'] = {
                    'system': platform.system(),
                    'release': platform.release(), 
                    'version': platform.version(),
                    'platform': platform.platform(),
                    'architecture': platform.architecture(),
                    'machine': platform.machine(),
                    'processor': platform.processor(),
                    'node': platform.node(),
                    'boot_time': datetime.fromtimestamp(psutil.boot_time()).isoformat() if OPTIONAL_DEPENDENCIES.get('psutil', False) else None
                }
                system_info['collection_metadata']['data_sources'].append('platform')
                system_info['collection_metadata']['performance_metrics']['platform_collection'] = op_monitor.get_metrics()
        except Exception as e:
            system_info['platform_error'] = str(e)
            system_info['collection_metadata']['errors'].append(f"Platform info collection failed: {e}")
        
        # Python runtime information (always included) - monitored
        try:
            with _monitored_operation("python_info_collection") as op_monitor:
                system_info['python'] = {
                    'version': sys.version,
                    'version_info': {
                        'major': sys.version_info.major,
                        'minor': sys.version_info.minor,
                        'micro': sys.version_info.micro,
                        'releaselevel': sys.version_info.releaselevel,
                        'serial': sys.version_info.serial,
                        'version_tuple': tuple(sys.version_info[:3])
                    },
                    'executable': sys.executable,
                    'build': platform.python_build(),
                    'compiler': platform.python_compiler(),
                    'implementation': platform.python_implementation(),
                    'prefix': sys.prefix,
                    'path': sys.path[:5],  # First 5 paths to avoid clutter
                    'modules_count': len(sys.modules),
                    'encoding': {
                        'default': sys.getdefaultencoding(),
                        'filesystem': sys.getfilesystemencoding(),
                        'stdout': getattr(sys.stdout, 'encoding', 'unknown'),
                        'stderr': getattr(sys.stderr, 'encoding', 'unknown')
                    }
                }
                system_info['collection_metadata']['data_sources'].append('python')
                system_info['collection_metadata']['performance_metrics']['python_collection'] = op_monitor.get_metrics()
        except Exception as e:
            system_info['python_error'] = str(e)
            system_info['collection_metadata']['errors'].append(f"Python info collection failed: {e}")
        
        # Package analysis - monitored
        if include_versions:
            try:
                with _monitored_operation("version_analysis") as op_monitor:
                    version_data = check_versions(include_optional=True)
                    system_info['package_versions'] = version_data
                    system_info['collection_metadata']['data_sources'].append('check_versions')
                    
                    # Version analysis
                    version_analysis = {
                        'total_packages': len(version_data),
                        'available_packages': sum(1 for pkg in version_data.values() if pkg.get('available', False)),
                        'compatible_packages': sum(1 for pkg in version_data.values() if pkg.get('compatible', False)),
                        'required_packages': sum(1 for pkg in version_data.values() if pkg.get('required', False)),
                        'missing_required': [name for name, pkg in version_data.items() if pkg.get('required', False) and not pkg.get('available', False)],
                        'incompatible_packages': [name for name, pkg in version_data.items() if pkg.get('available', False) and not pkg.get('compatible', False)],
                        'optional_available': [name for name, pkg in version_data.items() if not pkg.get('required', False) and pkg.get('available', False)],
                        'status_summary': {
                            'OK': sum(1 for pkg in version_data.values() if pkg.get('status') == 'OK'),
                            'WARNING': sum(1 for pkg in version_data.values() if pkg.get('status') == 'WARNING'),
                            'MISSING': sum(1 for pkg in version_data.values() if pkg.get('status') == 'MISSING'),
                            'ERROR': sum(1 for pkg in version_data.values() if pkg.get('status') == 'ERROR')
                        }
                    }
                    
                    # Add environment health assessment
                    version_analysis['environment_health'] = {
                        'overall_status': 'healthy' if len(version_analysis['missing_required']) == 0 else 'degraded',
                        'compatibility_score': (version_analysis['compatible_packages'] / max(version_analysis['available_packages'], 1)) * 100,
                        'completeness_score': (version_analysis['available_packages'] / version_analysis['total_packages']) * 100,
                        'critical_issues': len(version_analysis['missing_required']),
                        'warnings': len(version_analysis['incompatible_packages'])
                    }
                    
                    system_info['package_analysis'] = version_analysis
                    
                    # Store performance metrics
                    version_metrics = op_monitor.get_metrics()
                    version_metrics['packages_analyzed'] = len(version_data)
                    version_metrics['analysis_efficiency'] = len(version_data) / max(version_metrics.get('duration', 0.001), 0.001)
                    system_info['collection_metadata']['performance_metrics']['version_analysis'] = version_metrics
                    
                    # Add warnings for missing critical packages
                    if version_analysis['missing_required']:
                        system_info['collection_metadata']['warnings'].append(f"Missing required packages: {', '.join(version_analysis['missing_required'])}")
                        
            except Exception as e:
                system_info['package_versions_error'] = str(e)
                system_info['collection_metadata']['errors'].append(f"Package version collection failed: {e}")
        
        # Hardware analysis - monitored
        if include_hardware:
            try:
                with _monitored_operation("hardware_analysis") as op_monitor:
                    hardware_data = check_hardware(min_disk_gb=1.0, include_memory_usage=include_memory_usage)
                    system_info['hardware'] = hardware_data
                    system_info['collection_metadata']['data_sources'].append('check_hardware')
                    
                    # Hardware analysis
                    hardware_analysis = {
                        'components_detected': len(hardware_data),
                        'components_healthy': sum(1 for comp in hardware_data.values() if comp.get('status') == 'PASS'),
                        'components_warning': sum(1 for comp in hardware_data.values() if comp.get('status') == 'WARN'),
                        'components_failed': sum(1 for comp in hardware_data.values() if comp.get('status') == 'FAIL'),
                        'required_failures': sum(1 for comp in hardware_data.values() if comp.get('status') == 'FAIL' and comp.get('required', False))
                    }
                    
                    # Extract key hardware capabilities with integrated classification
                    capabilities = {}
                    
                    # CPU capabilities
                    if 'cpu_cores' in hardware_data and hardware_data['cpu_cores'].get('available'):
                        cpu_data = hardware_data['cpu_cores']
                        cores = cpu_data.get('logical_cores', 0)
                        frequency_ghz = cpu_data.get('capacity', {}).get('frequency_ghz', 0.0)
                        
                        # Integrated CPU performance classification
                        if cores >= 16 and frequency_ghz >= 3.0:
                            cpu_perf_class = 'high'
                        elif cores >= 8 and frequency_ghz >= 2.5:
                            cpu_perf_class = 'medium-high'  
                        elif cores >= 4 and frequency_ghz >= 2.0:
                            cpu_perf_class = 'medium'
                        elif cores >= 2:
                            cpu_perf_class = 'low-medium'
                        else:
                            cpu_perf_class = 'low'
                        
                        capabilities['cpu'] = {
                            'logical_cores': cores,
                            'physical_cores': cpu_data.get('physical_cores', 0),
                            'hyperthreading': cpu_data.get('hyperthreading', False),
                            'performance_class': cpu_perf_class,
                            'frequency_ghz': frequency_ghz,
                            'max_frequency_ghz': cpu_data.get('capacity', {}).get('max_frequency_ghz')
                        }
                        
                        if 'current_usage' in cpu_data:
                            capabilities['cpu']['current_load'] = cpu_data['current_usage'].get('cpu_percent_total', 0)
                            capabilities['cpu']['per_core_load'] = cpu_data['current_usage'].get('cpu_percent_per_core', [])
                    
                    # Memory capabilities
                    if 'system_ram' in hardware_data and hardware_data['system_ram'].get('available'):
                        ram_data = hardware_data['system_ram']
                        total_gb = ram_data.get('ram_total_gb', 0)
                        
                        # Integrated memory performance classification
                        if total_gb >= 32:
                            memory_perf_class = 'high'
                        elif total_gb >= 16:
                            memory_perf_class = 'medium-high'
                        elif total_gb >= 8:
                            memory_perf_class = 'medium'
                        elif total_gb >= 4:
                            memory_perf_class = 'low-medium'
                        else:
                            memory_perf_class = 'low'
                        
                        capabilities['memory'] = {
                            'total_gb': total_gb,
                            'performance_class': memory_perf_class,
                            'swap_gb': ram_data.get('swap_total_gb', 0),
                            'has_swap': ram_data.get('swap_total_gb', 0) > 0
                        }
                        
                        if 'current_usage' in ram_data:
                            capabilities['memory'].update({
                                'available_gb': ram_data['current_usage'].get('available_gb', 0),
                                'used_gb': ram_data['current_usage'].get('used_gb', 0),
                                'usage_percent': ram_data['current_usage'].get('percent_used', 0),
                                'swap_used_gb': ram_data['current_usage'].get('swap_used_gb', 0),
                                'swap_usage_percent': ram_data['current_usage'].get('swap_percent', 0)
                            })
                    
                    # GPU capabilities
                    if 'cuda' in hardware_data:
                        cuda_data = hardware_data['cuda']
                        cuda_available = cuda_data.get('available', False)
                        gpus = cuda_data.get('gpus', [])
                        
                        # Integrated GPU performance classification
                        if not cuda_available or not gpus:
                            gpu_perf_class = 'none'
                        else:
                            # Use the best GPU for classification
                            best_memory = max(gpu.get('memory_gb', 0) for gpu in gpus)
                            gpu_count = len(gpus)
                            
                            if best_memory >= 16 and gpu_count >= 2:
                                gpu_perf_class = 'high'
                            elif best_memory >= 12 or (best_memory >= 8 and gpu_count >= 2):
                                gpu_perf_class = 'medium-high'
                            elif best_memory >= 6:
                                gpu_perf_class = 'medium'
                            elif best_memory >= 4:
                                gpu_perf_class = 'low-medium'
                            else:
                                gpu_perf_class = 'low'
                        
                        capabilities['gpu'] = {
                            'available': cuda_available,
                            'count': cuda_data.get('gpu_count', 0),
                            'cuda_version': cuda_data.get('cuda_version'),
                            'cudnn_version': cuda_data.get('cudnn_version'),
                            'performance_class': gpu_perf_class,
                            'total_memory_gb': sum(gpu.get('memory_gb', 0) for gpu in gpus)
                        }
                        
                        if cuda_available and gpus:
                            capabilities['gpu']['devices'] = []
                            for i, gpu in enumerate(gpus):
                                gpu_info = {
                                    'index': i,
                                    'name': gpu.get('name', 'Unknown'),
                                    'memory_gb': gpu.get('memory_gb', 0),
                                    'compute_capability': gpu.get('compute_capability'),
                                    'multiprocessors': gpu.get('multiprocessors', 0)
                                }
                                
                                if 'current_usage' in gpu:
                                    gpu_usage = gpu['current_usage']
                                    gpu_info.update({
                                        'allocated_mb': gpu_usage.get('allocated_mb', 0),
                                        'reserved_mb': gpu_usage.get('reserved_mb', 0),
                                        'utilization_percent': gpu_usage.get('percent_allocated', 0)
                                    })
                                
                                capabilities['gpu']['devices'].append(gpu_info)
                    
                    # Storage capabilities
                    if 'disk_space' in hardware_data and hardware_data['disk_space'].get('available'):
                        disk_data = hardware_data['disk_space']
                        free_gb = disk_data.get('free_gb', 0)
                        total_gb = disk_data.get('total_gb', 0)
                        usage_percent = (disk_data.get('used_gb', 0) / max(total_gb, 1)) * 100
                        
                        # Integrated storage performance classification
                        if free_gb >= 100 and usage_percent < 70:
                            storage_perf_class = 'high'
                        elif free_gb >= 50 and usage_percent < 80:
                            storage_perf_class = 'medium'
                        elif free_gb >= 20 and usage_percent < 90:
                            storage_perf_class = 'low-medium'
                        else:
                            storage_perf_class = 'low'
                        
                        capabilities['storage'] = {
                            'free_gb': free_gb,
                            'total_gb': total_gb,
                            'used_gb': disk_data.get('used_gb', 0),
                            'usage_percent': usage_percent,
                            'performance_class': storage_perf_class
                        }
                    
                    # hardware performance score calculation
                    scores = []
                    
                    # CPU score
                    cpu_class = capabilities.get('cpu', {}).get('performance_class', 'low')
                    cpu_scores = {'high': 100, 'medium-high': 80, 'medium': 60, 'low-medium': 40, 'low': 20}
                    scores.append(cpu_scores.get(cpu_class, 20))
                    
                    # Memory score  
                    memory_class = capabilities.get('memory', {}).get('performance_class', 'low')
                    memory_scores = {'high': 100, 'medium-high': 80, 'medium': 60, 'low-medium': 40, 'low': 20}
                    scores.append(memory_scores.get(memory_class, 20))
                    
                    # GPU score
                    gpu_class = capabilities.get('gpu', {}).get('performance_class', 'none')
                    gpu_scores = {'high': 100, 'medium-high': 80, 'medium': 60, 'low-medium': 40, 'low': 20, 'none': 0}
                    scores.append(gpu_scores.get(gpu_class, 0))
                    
                    # Storage score
                    storage_class = capabilities.get('storage', {}).get('performance_class', 'low')
                    storage_scores = {'high': 100, 'medium': 80, 'low-medium': 60, 'low': 40}
                    scores.append(storage_scores.get(storage_class, 40))
                    
                    performance_score = int(sum(scores) / len(scores))
                    
                    # system performance classification
                    if performance_score >= 80:
                        system_class = 'high_performance'
                    elif performance_score >= 60:
                        system_class = 'standard'
                    else:
                        system_class = 'limited'
                    
                    hardware_analysis.update({
                        'capabilities': capabilities,
                        'overall_health': 'healthy' if hardware_analysis['required_failures'] == 0 else 'degraded',
                        'performance_score': performance_score,
                        'system_class': system_class
                    })
                    
                    system_info['hardware_analysis'] = hardware_analysis
                    
                    # Store performance metrics
                    hardware_metrics = op_monitor.get_metrics()
                    hardware_metrics['components_analyzed'] = len(hardware_data)
                    hardware_metrics['analysis_efficiency'] = len(hardware_data) / max(hardware_metrics.get('duration', 0.001), 0.001)
                    system_info['collection_metadata']['performance_metrics']['hardware_analysis'] = hardware_metrics
                    
                    # Add warnings for hardware issues
                    if hardware_analysis['required_failures'] > 0:
                        failed_components = [name for name, comp in hardware_data.items() if comp.get('status') == 'FAIL' and comp.get('required', False)]
                        system_info['collection_metadata']['warnings'].append(f"Critical hardware failures: {', '.join(failed_components)}")
                        
            except Exception as e:
                system_info['hardware_error'] = str(e)
                system_info['collection_metadata']['errors'].append(f"Hardware collection failed: {e}")
        
        # Performance baseline establishment
        if include_performance_baseline:
            try:
                logger.debug("Establishing performance baseline")
                with _monitored_operation("performance_baseline_establishment") as op_monitor:
                    # Use hardware data from previous step if available
                    baseline_hardware_data = system_info.get('hardware') if include_hardware else None
                    
                    baseline_results = establish_performance_baseline(hardware_data=baseline_hardware_data)
                    system_info['performance_baseline'] = baseline_results
                    system_info['collection_metadata']['data_sources'].append('performance_baseline')
                    
                    # Store baseline establishment metrics
                    baseline_metrics = op_monitor.get_metrics()
                    baseline_metrics['baseline_tests_completed'] = len(baseline_results.get('baselines', {}))
                    baseline_metrics['baseline_success_rate'] = len([b for b in baseline_results.get('baselines', {}).values() if not isinstance(b, str) or not b.endswith('_error')]) / max(len(baseline_results.get('baselines', {})), 1)
                    system_info['collection_metadata']['performance_metrics']['baseline_establishment'] = baseline_metrics
                    
                    # Integrate baseline results with hardware analysis
                    if 'hardware_analysis' in system_info and 'summary' in baseline_results:
                        system_info['hardware_analysis']['performance_baseline'] = baseline_results['summary']
                        
                    logger.debug(f"Performance baseline established: {baseline_results.get('summary', {}).get('overall_capability', 'unknown')} system")
                    
            except Exception as e:
                system_info['performance_baseline_error'] = str(e)
                system_info['collection_metadata']['errors'].append(f"Performance baseline establishment failed: {e}")
        
        # Analysis with performance context
        if include_detailed_analysis:
            try:
                with _monitored_operation("detailed_analysis") as op_monitor:
                    analysis = {
                        'system_recommendations': [],
                        'performance_optimizations': [],
                        'compatibility_issues': [],
                        'resource_warnings': [],
                        'configuration_suggestions': []
                    }
                    
                    # Analyze system for recommendations
                    if 'hardware_analysis' in system_info:
                        hw_analysis = system_info['hardware_analysis']
                        capabilities = hw_analysis.get('capabilities', {})
                        
                        # CPU recommendations
                        if 'cpu' in capabilities:
                            cpu = capabilities['cpu']
                            if cpu.get('logical_cores', 0) < 4:
                                analysis['system_recommendations'].append("Consider upgrading to a system with at least 4 CPU cores for optimal performance")
                            if cpu.get('performance_class') == 'low':
                                analysis['performance_optimizations'].append("CPU performance is limited - consider enabling CPU-optimized algorithms")
                        
                        # Memory recommendations
                        if 'memory' in capabilities:
                            memory = capabilities['memory']
                            if memory.get('total_gb', 0) < 8:
                                analysis['system_recommendations'].append("Consider upgrading to at least 8GB RAM for better performance")
                            if memory.get('usage_percent', 0) > 80:
                                analysis['resource_warnings'].append(f"High memory usage detected ({memory.get('usage_percent', 0):.1f}%) - consider closing unnecessary applications")
                        
                        # GPU recommendations
                        if 'gpu' in capabilities:
                            gpu = capabilities['gpu']
                            if not gpu.get('available'):
                                analysis['performance_optimizations'].append("CUDA not available - training will use CPU (slower performance expected)")
                            elif gpu.get('total_memory_gb', 0) < 6:
                                analysis['configuration_suggestions'].append("Limited GPU memory detected - consider using smaller batch sizes")
                    
                    # Performance baseline integration
                    if include_performance_baseline and 'performance_baseline' in system_info:
                        baseline = system_info['performance_baseline']
                        
                        # CPU performance analysis
                        if 'cpu' in baseline.get('baselines', {}):
                            cpu_baseline = baseline['baselines']['cpu']
                            gflops = cpu_baseline.get('gflops', 0)
                            
                            if gflops < 1:
                                analysis['performance_optimizations'].append(f"CPU performance is very low ({gflops:.2f} GFLOPS) - consider optimized algorithms")
                            elif gflops > 10:
                                analysis['configuration_suggestions'].append(f"Excellent CPU performance ({gflops:.1f} GFLOPS) - can handle complex computations")
                        
                        # Memory performance analysis
                        if 'memory' in baseline.get('baselines', {}):
                            memory_baseline = baseline['baselines']['memory']
                            avg_speed = np.mean([
                                baseline_item.get('allocation_speed_mbs', 0) 
                                for baseline_item in memory_baseline.values() 
                                if isinstance(baseline_item, dict) and 'allocation_speed_mbs' in baseline_item
                            ])
                            
                            if avg_speed < 50:
                                analysis['performance_optimizations'].append(f"Memory allocation is slow ({avg_speed:.0f} MB/s) - consider memory optimization")
                            elif avg_speed > 500:
                                analysis['configuration_suggestions'].append(f"Fast memory allocation ({avg_speed:.0f} MB/s) - can handle large datasets")
                        
                        # GPU performance analysis
                        if 'gpu' in baseline.get('baselines', {}):
                            gpu_baseline = baseline['baselines']['gpu']
                            max_gpu_gflops = max([
                                gpu.get('gflops', 0) 
                                for gpu in gpu_baseline.values() 
                                if isinstance(gpu, dict) and 'gflops' in gpu
                            ], default=0)
                            
                            if max_gpu_gflops > 100:
                                analysis['configuration_suggestions'].append(f"High-performance GPU available ({max_gpu_gflops:.0f} GFLOPS) - enable mixed precision and large batch sizes")
                            elif max_gpu_gflops > 0 and max_gpu_gflops < 50:
                                analysis['performance_optimizations'].append(f"Limited GPU performance ({max_gpu_gflops:.0f} GFLOPS) - use smaller models and batch sizes")
                    
                    # Package compatibility analysis
                    if 'package_analysis' in system_info:
                        pkg_analysis = system_info['package_analysis']
                        
                        if pkg_analysis.get('missing_required'):
                            analysis['compatibility_issues'].extend([f"Missing required package: {pkg}" for pkg in pkg_analysis['missing_required']])
                        
                        if pkg_analysis.get('incompatible_packages'):
                            analysis['compatibility_issues'].extend([f"Incompatible package version: {pkg}" for pkg in pkg_analysis['incompatible_packages']])
                        
                        if pkg_analysis['environment_health']['compatibility_score'] < 90:
                            analysis['system_recommendations'].append("Package environment needs attention - some dependencies may be outdated")
                    
                    # System class based recommendations
                    if 'hardware_analysis' in system_info:
                        system_class = system_info['hardware_analysis'].get('system_class', 'unknown')
                        
                        if system_class == 'high_performance':
                            analysis['configuration_suggestions'].extend([
                                "System is high-performance - consider enabling advanced features",
                                "Large batch sizes and complex models recommended",
                                "Enable mixed precision training for optimal GPU utilization"
                            ])
                        elif system_class == 'standard':
                            analysis['configuration_suggestions'].extend([
                                "Standard system configuration detected",
                                "Use moderate batch sizes and model complexity",
                                "Consider gradient accumulation if memory limited"
                            ])
                        elif system_class == 'limited':
                            analysis['configuration_suggestions'].extend([
                                "Limited system resources detected",
                                "Use small batch sizes and simple models",
                                "Enable aggressive memory management",
                                "Consider using checkpointing to save memory"
                            ])
                    
                    system_info['detailed_analysis'] = analysis
                    system_info['collection_metadata']['data_sources'].append('detailed_analysis')
                    
                    # Store analysis performance metrics
                    analysis_metrics = op_monitor.get_metrics()
                    analysis_metrics['recommendations_generated'] = sum(len(recommendations) for recommendations in analysis.values() if isinstance(recommendations, list))
                    analysis_metrics['analysis_efficiency'] = analysis_metrics['recommendations_generated'] / max(analysis_metrics.get('duration', 0.001), 0.001)
                    system_info['collection_metadata']['performance_metrics']['detailed_analysis'] = analysis_metrics
                    
            except Exception as e:
                system_info['detailed_analysis_error'] = str(e)
                system_info['collection_metadata']['errors'].append(f"Detailed analysis failed: {e}")
        
        # Runtime process information - monitored
        try:
            with _monitored_operation("runtime_info_collection") as op_monitor:
                if OPTIONAL_DEPENDENCIES.get('psutil', False):
                    process = psutil.Process()
                    system_info['runtime'] = {
                        'process_id': os.getpid(),
                        'parent_process_id': os.getppid(),
                        'process_name': process.name(),
                        'process_status': process.status(),
                        'process_create_time': datetime.fromtimestamp(process.create_time()).isoformat(),
                        'working_directory': os.getcwd(),
                        'command_line': ' '.join(sys.argv),
                        'environment_variables': {
                            'PATH': os.environ.get('PATH', ''),
                            'PYTHONPATH': os.environ.get('PYTHONPATH', ''),
                            'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', ''),
                            'OMP_NUM_THREADS': os.environ.get('OMP_NUM_THREADS', ''),
                            'NUMBA_NUM_THREADS': os.environ.get('NUMBA_NUM_THREADS', '')
                        }
                    }
                    
                    if include_memory_usage:
                        memory_info = process.memory_info()
                        system_info['runtime']['memory'] = {
                            'rss_mb': memory_info.rss / (1024**2),
                            'vms_mb': memory_info.vms / (1024**2),
                            'percent': process.memory_percent(),
                            'open_files': len(process.open_files()),
                            'num_threads': process.num_threads()
                        }
                    
                    system_info['collection_metadata']['data_sources'].append('runtime')
                    system_info['collection_metadata']['performance_metrics']['runtime_collection'] = op_monitor.get_metrics()
        except Exception as e:
            system_info['runtime_error'] = str(e)
            system_info['collection_metadata']['errors'].append(f"Runtime info collection failed: {e}")
        
        # Post-collection memory state and optimization
        if include_memory_usage or include_memory_optimization:
            try:
                # Capture memory state for monitoring system info collection impact
                final_memory_state = {
                    'timestamp': datetime.now().isoformat()
                }
                
                # Process memory info
                if OPTIONAL_DEPENDENCIES.get('psutil', False):
                    proc = psutil.Process()
                    mem_info = proc.memory_info()
                    final_memory_state.update({
                        'process_rss_mb': mem_info.rss / (1024**2),
                        'process_vms_mb': mem_info.vms / (1024**2),
                        'process_percent': proc.memory_percent()
                    })
                    
                    # System memory info
                    sys_mem = psutil.virtual_memory()
                    final_memory_state.update({
                        'system_total_gb': sys_mem.total / (1024**3),
                        'system_available_gb': sys_mem.available / (1024**3),
                        'system_used_gb': sys_mem.used / (1024**3),
                        'system_percent': sys_mem.percent
                    })
                
                # GPU memory if available
                if torch.cuda.is_available():
                    try:
                        gpu_memory = {}
                        for i in range(torch.cuda.device_count()):
                            gpu_memory[f'gpu_{i}'] = {
                                'allocated_mb': torch.cuda.memory_allocated(i) / (1024**2),
                                'reserved_mb': torch.cuda.memory_reserved(i) / (1024**2)
                            }
                        final_memory_state['gpu_memory'] = gpu_memory
                    except:
                        pass
                
                system_info['collection_metadata']['final_memory_state'] = final_memory_state
                
                # Calculate memory impact of system info collection
                if 'initial_memory_state' in system_info['collection_metadata']:
                    initial = system_info['collection_metadata']['initial_memory_state']
                    memory_impact = {
                        'rss_delta_mb': final_memory_state.get('process_rss_mb', 0) - initial.get('process_rss_mb', 0),
                        'vms_delta_mb': final_memory_state.get('process_vms_mb', 0) - initial.get('process_vms_mb', 0),
                        'system_delta_gb': final_memory_state.get('system_used_gb', 0) - initial.get('system_used_gb', 0)
                    }
                    system_info['collection_metadata']['memory_impact'] = memory_impact
                    
                    logger.debug(f"System info collection memory impact: {memory_impact['rss_delta_mb']:+.1f}MB RSS")
                
            except Exception as e:
                system_info['collection_metadata']['warnings'].append(f"Failed to capture final memory state: {e}")
        
        # Optional post-collection memory optimization
        if include_memory_optimization:
            try:
                logger.debug("Performing post-collection memory optimization")
                post_optimization_results = enhanced_clear_memory(
                    aggressive=False,  # Less aggressive after collection
                    hardware_data=system_info.get('hardware')
                )
                system_info['collection_metadata']['memory_optimization_results']['post_collection'] = post_optimization_results
                
                if post_optimization_results.get('success', False):
                    logger.debug(f"Post-collection optimization: {', '.join(post_optimization_results.get('actions_taken', []))}")
                    
                    # Calculate optimization effectiveness/impact of memory optimization
                    optimization_impact = {
                        'timestamp': datetime.now().isoformat(),
                        'optimization_effective': False,
                        'improvements': {}
                    }
                    
                    try:
                        # System RAM impact
                        if 'system_ram' in post_optimization_results.get('memory_before', {}) and 'system_ram' in post_optimization_results.get('memory_after', {}):
                            before_ram = post_optimization_results['memory_before']['system_ram']
                            after_ram = post_optimization_results['memory_after']['system_ram']
                            
                            ram_freed = before_ram.get('used_gb', 0) - after_ram.get('used_gb', 0)
                            if ram_freed > 0.1:  # More than 100MB freed
                                optimization_impact['optimization_effective'] = True
                                optimization_impact['improvements']['system_ram_freed_gb'] = ram_freed
                        
                        # GPU memory impact
                        if 'gpu_memory' in post_optimization_results.get('memory_before', {}) and 'gpu_memory' in post_optimization_results.get('memory_after', {}):
                            gpu_improvements = {}
                            
                            for gpu_id in post_optimization_results['memory_before']['gpu_memory']:
                                if gpu_id in post_optimization_results['memory_after']['gpu_memory']:
                                    before_gpu = post_optimization_results['memory_before']['gpu_memory'][gpu_id]
                                    after_gpu = post_optimization_results['memory_after']['gpu_memory'][gpu_id]
                                    
                                    allocated_freed = before_gpu.get('allocated_mb', 0) - after_gpu.get('allocated_mb', 0)
                                    if allocated_freed > 10:  # More than 10MB freed
                                        gpu_improvements[gpu_id] = {
                                            'allocated_freed_mb': allocated_freed,
                                            'reserved_freed_mb': before_gpu.get('reserved_mb', 0) - after_gpu.get('reserved_mb', 0)
                                        }
                                        optimization_impact['optimization_effective'] = True
                            
                            if gpu_improvements:
                                optimization_impact['improvements']['gpu_memory'] = gpu_improvements
                        
                    except Exception as e:
                        optimization_impact['error'] = str(e)
                    
                    system_info['collection_metadata']['optimization_impact'] = optimization_impact
                        
            except Exception as e:
                system_info['collection_metadata']['errors'].append(f"Post-collection memory optimization failed: {e}")
        
        # Add optional dependencies status
        system_info['optional_dependencies'] = {
            'available_features': OPTIONAL_DEPENDENCIES,
            'feature_count': len(OPTIONAL_DEPENDENCIES),
            'enabled_features': sum(1 for enabled in OPTIONAL_DEPENDENCIES.values() if enabled),
            'disabled_features': sum(1 for enabled in OPTIONAL_DEPENDENCIES.values() if not enabled),
            'feature_availability_score': (sum(1 for enabled in OPTIONAL_DEPENDENCIES.values() if enabled) / max(len(OPTIONAL_DEPENDENCIES), 1)) * 100
        }
        
        # Finalize collection metadata with comprehensive performance analysis
        end_time = time.time()
        total_duration = end_time - start_time
        system_info['collection_metadata']['collection_duration_ms'] = round(total_duration * 1000, 2)
        system_info['collection_metadata']['success'] = len(system_info['collection_metadata']['errors']) == 0
        
        # Assess the quality of collected system information
        errors = len(system_info['collection_metadata']['errors'])
        warnings = len(system_info['collection_metadata']['warnings'])
        data_sources = len(system_info['collection_metadata']['data_sources'])
        
        if errors == 0 and warnings == 0:
            data_quality = 'excellent'
        elif errors == 0 and warnings <= 2:
            data_quality = 'good' 
        elif errors == 0:
            data_quality = 'acceptable'
        elif errors <= 2 and data_sources >= 3:
            data_quality = 'degraded'
        else:
            data_quality = 'poor'
        
        system_info['collection_metadata']['data_quality'] = data_quality
        
        # Add performance summary
        performance_metrics = system_info['collection_metadata'].get('performance_metrics', {})
        summary = {
            'total_duration': total_duration,
            'operations_completed': len(performance_metrics),
            'average_operation_duration': 0,
            'slowest_operation': None,
            'fastest_operation': None,
            'total_memory_impact_mb': 0,
            'efficiency_score': 0
        }
        
        try:
            if performance_metrics:
                durations = []
                memory_deltas = []
                
                for op_name, metrics in performance_metrics.items():
                    duration = metrics.get('duration', 0)
                    durations.append(duration)
                    
                    if 'memory_delta' in metrics:
                        memory_deltas.append(metrics['memory_delta'].get('rss_mb', 0))
                    
                    # Track slowest and fastest operations
                    if summary['slowest_operation'] is None or duration > performance_metrics[summary['slowest_operation']].get('duration', 0):
                        summary['slowest_operation'] = op_name
                    
                    if summary['fastest_operation'] is None or duration < performance_metrics[summary['fastest_operation']].get('duration', 0):
                        summary['fastest_operation'] = op_name
                
                if durations:
                    summary['average_operation_duration'] = sum(durations) / len(durations)
                
                if memory_deltas:
                    summary['total_memory_impact_mb'] = sum(memory_deltas)
                
                # Calculate efficiency score (operations per second, weighted by memory efficiency)
                if total_duration > 0:
                    ops_per_second = len(performance_metrics) / total_duration
                    memory_efficiency = max(0, 1 - abs(summary['total_memory_impact_mb']) / 100)  # Penalty for high memory usage
                    base_efficiency = min(ops_per_second * 10, 100)
                    weighted_efficiency = base_efficiency * memory_efficiency
                    summary['efficiency_score'] = round(max(0, min(weighted_efficiency, 100)), 2)
            
        except Exception as e:
            summary['error'] = str(e)
        
        system_info['collection_metadata']['performance_summary'] = summary
        
        return system_info
        
    except Exception as e:
        # Fallback error response with performance context
        error_response = {
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
            'collection_metadata': {
                'success': False,
                'data_quality': 'failed',
                'errors': [f"Critical system info collection failure: {e}"]
            },
            'platform': None,
            'python': None,
            'partial_data': True
        }
        
        # Try to preserve any performance metrics that were collected
        try:
            if 'performance_metrics' in locals():
                error_response['collection_metadata']['performance_metrics'] = locals()['performance_metrics']
        except:
            pass
            
        return error_response

# Helper functions for system info collection
@contextmanager
def _monitored_operation(operation_name: str):
    """Context manager for monitoring individual operations within get_system_info()."""
    class OperationMonitor:
        def __init__(self, name):
            self.name = name
            self.start_time = time.time()
            self.start_memory = None
            self.metrics = {}
            
            # Capture initial memory state if psutil available
            try:
                proc = psutil.Process()
                self.start_memory = {
                    'rss_mb': proc.memory_info().rss / (1024**2),
                    'vms_mb': proc.memory_info().vms / (1024**2)
                }
            except:
                pass
        
        def get_metrics(self):
            end_time = time.time()
            duration = end_time - self.start_time
            
            self.metrics = {
                'operation': self.name,
                'duration': duration,
                'timestamp': datetime.now().isoformat()
            }
            
            # Add memory metrics if available
            if self.start_memory:
                try:
                    proc = psutil.Process()
                    end_memory = {
                        'rss_mb': proc.memory_info().rss / (1024**2),
                        'vms_mb': proc.memory_info().vms / (1024**2)
                    }
                    self.metrics['memory_delta'] = {
                        'rss_mb': end_memory['rss_mb'] - self.start_memory['rss_mb'],
                        'vms_mb': end_memory['vms_mb'] - self.start_memory['vms_mb']
                    }
                except:
                    pass
            
            return self.metrics
    
    monitor = OperationMonitor(operation_name)
    try:
        yield monitor
    finally:
        # Ensure metrics are available even if operation fails
        monitor.get_metrics()

def update_global_config(config: Dict[str, Any]) -> None:
    """Update module-level constants from config with enhanced validation, logging, preset support,
    and intelligent memory management.
    
    This function synchronizes global configuration variables with the provided configuration
    dictionary, ensuring type safety, value validation, comprehensive change tracking, and
    optimal memory usage during intensive configuration processing.
    
    Args:
        config: Configuration dictionary to update from
        
    Raises:
        ValueError: If any configuration values are invalid
        TypeError: If any configuration values are of incorrect type
        KeyError: If required configuration sections are missing
    """
    if not isinstance(config, dict):
        raise TypeError(f"Configuration must be a dictionary, got {type(config).__name__}")
    
    # INITIAL MEMORY OPTIMIZATION - Get hardware context early for memory-aware processing
    hardware_data = None
    total_ram_gb = 8.0  # Conservative default
    
    try:
        hardware_data = check_hardware(include_memory_usage=True)
        total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        
        # Initial memory cleanup for memory-constrained systems before processing large configs
        if total_ram_gb < 16 and len(str(config)) > 100000:  # Large config (>100KB serialized)
            initial_clear_results = enhanced_clear_memory(
                aggressive=total_ram_gb < 8,  # More aggressive on low-memory systems
                hardware_data=hardware_data
            )
            
            if initial_clear_results.get('success'):
                logger.debug(f"Initial memory optimization for config processing: {', '.join(initial_clear_results.get('actions_taken', []))}")
    except Exception as e:
        logger.debug(f"Initial memory optimization failed: {e}")
    
    # Validate config structure with better error messages
    required_sections = ['training', 'model', 'security', 'data']
    missing_sections = [section for section in required_sections if section not in config]
    if missing_sections:
        raise KeyError(f"Missing required configuration sections: {missing_sections}")
    
    # Initialize comprehensive change tracking
    changes = {
        'metadata': {
            'config_version': '2.1',
            'update_time': datetime.now().isoformat(),
            'source': config.get('metadata', {}).get('preset_used', 'manual'),
            'total_changes': 0,
            'memory_optimizations_applied': 0,
            'large_config_processing': len(str(config)) > 50000
        },
        'training': {},
        'model': {},
        'security': {},
        'data': {},
        'system': {}
    }
    
    # Training configuration updates with memory optimization checkpoints
    try:
        training_config = config.get('training', {})
        
        # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before processing large training configs
        if len(str(training_config)) > 20000 and total_ram_gb < 16:
            try:
                training_clear_results = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if training_clear_results.get('success'):
                    changes['metadata']['memory_optimizations_applied'] += 1
                    logger.debug("Memory optimized before training config processing")
            except Exception as e:
                logger.debug(f"Training config memory optimization failed: {e}")
        
        # Training parameter updates
        global DEFAULT_BATCH_SIZE, DEFAULT_EPOCHS, EARLY_STOPPING_PATIENCE, LEARNING_RATE
        global WEIGHT_DECAY, GRADIENT_CLIP, GRADIENT_ACCUMULATION_STEPS, MIXED_PRECISION, NUM_WORKERS
        
        if 'batch_size' in training_config:
            old_value = DEFAULT_BATCH_SIZE
            new_value = max(1, min(2048, int(training_config['batch_size'])))
            if old_value != new_value:
                DEFAULT_BATCH_SIZE = new_value
                changes['training']['batch_size'] = {'from': old_value, 'to': new_value}
        
        if 'epochs' in training_config:
            old_value = DEFAULT_EPOCHS
            new_value = max(1, min(10000, int(training_config['epochs'])))
            if old_value != new_value:
                DEFAULT_EPOCHS = new_value
                changes['training']['epochs'] = {'from': old_value, 'to': new_value}
        
        if 'patience' in training_config:
            old_value = EARLY_STOPPING_PATIENCE
            new_value = max(1, min(1000, int(training_config['patience'])))
            if old_value != new_value:
                EARLY_STOPPING_PATIENCE = new_value
                changes['training']['patience'] = {'from': old_value, 'to': new_value}
        
        if 'learning_rate' in training_config:
            old_value = LEARNING_RATE
            new_value = max(1e-8, min(1.0, float(training_config['learning_rate'])))
            if old_value != new_value:
                LEARNING_RATE = new_value
                changes['training']['learning_rate'] = {'from': old_value, 'to': new_value}
        
        if 'weight_decay' in training_config:
            old_value = WEIGHT_DECAY
            new_value = max(0.0, min(1.0, float(training_config['weight_decay'])))
            if old_value != new_value:
                WEIGHT_DECAY = new_value
                changes['training']['weight_decay'] = {'from': old_value, 'to': new_value}
        
        if 'gradient_clip' in training_config:
            old_value = GRADIENT_CLIP
            new_value = max(0.01, min(100.0, float(training_config['gradient_clip'])))
            if old_value != new_value:
                GRADIENT_CLIP = new_value
                changes['training']['gradient_clip'] = {'from': old_value, 'to': new_value}
        
        if 'gradient_accumulation_steps' in training_config:
            old_value = GRADIENT_ACCUMULATION_STEPS
            new_value = max(1, min(256, int(training_config['gradient_accumulation_steps'])))
            if old_value != new_value:
                GRADIENT_ACCUMULATION_STEPS = new_value
                changes['training']['gradient_accumulation_steps'] = {'from': old_value, 'to': new_value}
        
        if 'mixed_precision' in training_config:
            old_value = MIXED_PRECISION
            new_value = bool(training_config['mixed_precision'])
            if old_value != new_value:
                MIXED_PRECISION = new_value
                changes['training']['mixed_precision'] = {'from': old_value, 'to': new_value}
        
        if 'num_workers' in training_config:
            old_value = NUM_WORKERS
            new_value = max(0, min(32, int(training_config['num_workers'])))
            if old_value != new_value:
                NUM_WORKERS = new_value
                changes['training']['num_workers'] = {'from': old_value, 'to': new_value}
                
    except Exception as e:
        logger.error(f"Error updating training configuration: {e}")
        raise ValueError(f"Invalid training configuration: {e}")
    
    # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before model config processing
    try:
        model_config = config.get('model', {})
        
        if len(str(model_config)) > 15000 and total_ram_gb < 16:
            model_clear_results = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if model_clear_results.get('success'):
                changes['metadata']['memory_optimizations_applied'] += 1
                logger.debug("Memory optimized before model config processing")
    
        # Model parameter updates
        global DEFAULT_ENCODING_DIM, HIDDEN_LAYER_SIZES, DROPOUT_RATES, ACTIVATION, ACTIVATION_PARAM
        global NORMALIZATION, USE_BATCH_NORM, USE_LAYER_NORM, DIVERSITY_FACTOR, MIN_FEATURES, NUM_MODELS
        
        if 'encoding_dim' in model_config:
            old_value = DEFAULT_ENCODING_DIM
            new_value = max(1, min(1024, int(model_config['encoding_dim'])))
            if old_value != new_value:
                DEFAULT_ENCODING_DIM = new_value
                changes['model']['encoding_dim'] = {'from': old_value, 'to': new_value}
        
        if 'hidden_dims' in model_config:
            old_value = HIDDEN_LAYER_SIZES
            new_dims = model_config['hidden_dims']
            if isinstance(new_dims, (list, tuple)) and all(isinstance(d, int) and d > 0 for d in new_dims):
                new_value = list(new_dims)
                if old_value != new_value:
                    HIDDEN_LAYER_SIZES = new_value
                    changes['model']['hidden_dims'] = {'from': old_value, 'to': new_value}
        
        if 'dropout_rates' in model_config:
            old_value = DROPOUT_RATES
            new_rates = model_config['dropout_rates']
            if isinstance(new_rates, (list, tuple)) and all(isinstance(r, (int, float)) and 0 <= r < 1 for r in new_rates):
                new_value = list(new_rates)
                if old_value != new_value:
                    DROPOUT_RATES = new_value
                    changes['model']['dropout_rates'] = {'from': old_value, 'to': new_value}
        
        valid_activations = ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu']
        if 'activation' in model_config:
            old_value = ACTIVATION
            new_value = model_config['activation']
            if isinstance(new_value, str) and new_value.lower() in valid_activations:
                new_value = new_value.lower()
                if old_value != new_value:
                    ACTIVATION = new_value
                    changes['model']['activation'] = {'from': old_value, 'to': new_value}
        
        if 'activation_param' in model_config:
            old_value = ACTIVATION_PARAM
            new_value = max(0.0, min(1.0, float(model_config['activation_param'])))
            if old_value != new_value:
                ACTIVATION_PARAM = new_value
                changes['model']['activation_param'] = {'from': old_value, 'to': new_value}
        
        valid_normalizations = ['batch', 'layer', 'instance', 'group', None, 'none']
        if 'normalization' in model_config:
            old_value = NORMALIZATION
            new_value = model_config['normalization']
            if new_value in valid_normalizations:
                if new_value == 'none':
                    new_value = None
                if old_value != new_value:
                    NORMALIZATION = new_value
                    changes['model']['normalization'] = {'from': old_value, 'to': new_value}
        
        if 'use_batch_norm' in model_config:
            old_value = USE_BATCH_NORM
            new_value = bool(model_config['use_batch_norm'])
            if old_value != new_value:
                USE_BATCH_NORM = new_value
                changes['model']['use_batch_norm'] = {'from': old_value, 'to': new_value}
        
        if 'use_layer_norm' in model_config:
            old_value = USE_LAYER_NORM
            new_value = bool(model_config['use_layer_norm'])
            if old_value != new_value:
                USE_LAYER_NORM = new_value
                changes['model']['use_layer_norm'] = {'from': old_value, 'to': new_value}
        
        if 'diversity_factor' in model_config:
            old_value = DIVERSITY_FACTOR
            new_value = max(0.0, min(1.0, float(model_config['diversity_factor'])))
            if old_value != new_value:
                DIVERSITY_FACTOR = new_value
                changes['model']['diversity_factor'] = {'from': old_value, 'to': new_value}
        
        if 'min_features' in model_config:
            old_value = MIN_FEATURES
            new_value = max(1, min(10000, int(model_config['min_features'])))
            if old_value != new_value:
                MIN_FEATURES = new_value
                changes['model']['min_features'] = {'from': old_value, 'to': new_value}
        
        if 'num_models' in model_config:
            old_value = NUM_MODELS
            new_value = max(1, min(20, int(model_config['num_models'])))
            if old_value != new_value:
                NUM_MODELS = new_value
                changes['model']['num_models'] = {'from': old_value, 'to': new_value}
    
    except Exception as e:
        logger.error(f"Error updating model configuration: {e}")
        raise ValueError(f"Invalid model configuration: {e}")
    
    # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before security config processing
    try:
        security_config = config.get('security', {})
        
        if len(str(security_config)) > 10000 and total_ram_gb < 16:
            security_clear_results = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if security_clear_results.get('success'):
                changes['metadata']['memory_optimizations_applied'] += 1
                logger.debug("Memory optimized before security config processing")
    
        # Security parameter updates
        global DEFAULT_PERCENTILE, DEFAULT_ATTACK_THRESHOLD, FALSE_NEGATIVE_COST, SECURITY_METRICS
        
        if 'percentile' in security_config:
            old_value = DEFAULT_PERCENTILE
            new_value = max(50.0, min(99.99, float(security_config['percentile'])))
            if old_value != new_value:
                DEFAULT_PERCENTILE = new_value
                changes['security']['percentile'] = {'from': old_value, 'to': new_value}
        
        if 'attack_threshold' in security_config:
            old_value = DEFAULT_ATTACK_THRESHOLD
            new_value = max(0.001, min(1.0, float(security_config['attack_threshold'])))
            if old_value != new_value:
                DEFAULT_ATTACK_THRESHOLD = new_value
                changes['security']['attack_threshold'] = {'from': old_value, 'to': new_value}
        
        if 'false_negative_cost' in security_config:
            old_value = FALSE_NEGATIVE_COST
            new_value = max(0.1, min(100.0, float(security_config['false_negative_cost'])))
            if old_value != new_value:
                FALSE_NEGATIVE_COST = new_value
                changes['security']['false_negative_cost'] = {'from': old_value, 'to': new_value}
        
        if 'enable_security_metrics' in security_config:
            old_value = SECURITY_METRICS
            new_value = bool(security_config['enable_security_metrics'])
            if old_value != new_value:
                SECURITY_METRICS = new_value
                changes['security']['enable_security_metrics'] = {'from': old_value, 'to': new_value}
    
    except Exception as e:
        logger.error(f"Error updating security configuration: {e}")
        raise ValueError(f"Invalid security configuration: {e}")
    
    # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before data config processing
    try:
        data_config = config.get('data', {})
        
        if len(str(data_config)) > 10000 and total_ram_gb < 16:
            data_clear_results = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if data_clear_results.get('success'):
                changes['metadata']['memory_optimizations_applied'] += 1
                logger.debug("Memory optimized before data config processing")
    
        # Data parameter updates
        global NORMAL_SAMPLES, ATTACK_SAMPLES, FEATURES, ANOMALY_FACTOR, RANDOM_STATE
        
        if 'normal_samples' in data_config:
            old_value = NORMAL_SAMPLES
            new_value = max(10, min(1000000, int(data_config['normal_samples'])))
            if old_value != new_value:
                NORMAL_SAMPLES = new_value
                changes['data']['normal_samples'] = {'from': old_value, 'to': new_value}
        
        if 'attack_samples' in data_config:
            old_value = ATTACK_SAMPLES
            new_value = max(5, min(100000, int(data_config['attack_samples'])))
            if old_value != new_value:
                ATTACK_SAMPLES = new_value
                changes['data']['attack_samples'] = {'from': old_value, 'to': new_value}
        
        if 'features' in data_config:
            old_value = FEATURES
            new_value = max(1, min(10000, int(data_config['features'])))
            if old_value != new_value:
                FEATURES = new_value
                changes['data']['features'] = {'from': old_value, 'to': new_value}
        
        if 'anomaly_factor' in data_config:
            old_value = ANOMALY_FACTOR
            new_value = max(0.1, min(100.0, float(data_config['anomaly_factor'])))
            if old_value != new_value:
                ANOMALY_FACTOR = new_value
                changes['data']['anomaly_factor'] = {'from': old_value, 'to': new_value}
        
        if 'random_state' in data_config:
            old_value = RANDOM_STATE
            new_value = int(data_config['random_state'])
            if old_value != new_value:
                RANDOM_STATE = new_value
                changes['data']['random_state'] = {'from': old_value, 'to': new_value}
    
    except Exception as e:
        logger.error(f"Error updating data configuration: {e}")
        raise ValueError(f"Invalid data configuration: {e}")
    
    # Handle preset application with comprehensive validation and memory optimization
    try:
        preset_config = config.get('presets', {})
        
        # Clear memory before intensive preset processing if needed
        if len(str(preset_config)) > 5000 and total_ram_gb < 16:
            preset_clear_results = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if preset_clear_results.get('success'):
                changes['metadata']['memory_optimizations_applied'] += 1
                logger.debug("Memory optimized before preset processing")
        
        current_preset = preset_config.get('current_preset')
        if current_preset and current_preset in PRESET_CONFIGS:
            logger.debug(f"Applying preset configuration: {current_preset}")
            
            # Apply preset-specific optimizations
            preset_data = PRESET_CONFIGS[current_preset]
            if isinstance(preset_data, dict):
                # Record preset application
                changes['system']['preset_applied'] = {
                    'preset': current_preset,
                    'description': preset_data.get('metadata', {}).get('description', 'No description'),
                    'compatibility': preset_data.get('metadata', {}).get('compatibility', [])
                }
                
                # Update global constants from preset if they haven't been explicitly overridden
                preset_training = preset_data.get('training', {})
                preset_model = preset_data.get('model', {})
                
                # Only apply preset values if they haven't been changed by direct config
                if 'batch_size' not in changes['training'] and 'batch_size' in preset_training:
                    DEFAULT_BATCH_SIZE = max(1, min(2048, int(preset_training['batch_size'])))
                    changes['training']['batch_size_from_preset'] = DEFAULT_BATCH_SIZE
                
                if 'epochs' not in changes['training'] and 'epochs' in preset_training:
                    DEFAULT_EPOCHS = max(1, min(10000, int(preset_training['epochs'])))
                    changes['training']['epochs_from_preset'] = DEFAULT_EPOCHS
                
                if 'encoding_dim' not in changes['model'] and 'encoding_dim' in preset_model:
                    DEFAULT_ENCODING_DIM = max(1, min(1024, int(preset_model['encoding_dim'])))
                    changes['model']['encoding_dim_from_preset'] = DEFAULT_ENCODING_DIM
    
    except Exception as e:
        logger.warning(f"Error applying preset configuration: {e}")
        changes['system']['preset_error'] = str(e)
    
    # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
    # Aggressive cleanup after configuration processing completion
    try:
        final_clear_results = enhanced_clear_memory(
            aggressive=True,  # Aggressive final cleanup
            hardware_data=hardware_data
        )
        
        if final_clear_results.get('success'):
            changes['metadata']['final_memory_cleanup'] = {
                'actions': final_clear_results.get('actions_taken', []),
                'timestamp': datetime.now().isoformat(),
                'memory_impact': {
                    'optimization_effective': True,
                    'final_cleanup_performed': True
                }
            }
            
            changes['metadata']['memory_optimizations_applied'] += 1
            logger.debug(f"Final memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
            
    except Exception as e:
        logger.debug(f"Final memory optimization failed: {e}")
        changes['metadata']['final_cleanup_error'] = str(e)
    
    # Calculate total changes and log summary
    total_changes = sum(len(section) for section in changes.values() 
                       if isinstance(section, dict) and section != changes['metadata'])
    changes['metadata']['total_changes'] = total_changes
    
    if total_changes > 0:
        logger.debug(f"Updated {total_changes} global configuration parameters")
        logger.debug(f"Applied {changes['metadata']['memory_optimizations_applied']} memory optimizations during processing")
        
        # Log significant changes
        for section_name, section_changes in changes.items():
            if isinstance(section_changes, dict) and section_changes and section_name != 'metadata':
                logger.debug(f"Updated {section_name}: {list(section_changes.keys())}")
        
        # Log preset application if it occurred
        if 'preset_applied' in changes.get('system', {}):
            preset_info = changes['system']['preset_applied']
            logger.debug(f"Applied preset '{preset_info['preset']}': {preset_info['description']}")
    else:
        logger.debug("No global configuration changes applied")
    
    # Final validation of global state
    try:
        validate_global_config_state()
        logger.debug("Global configuration state validation passed")
    except Exception as e:
        logger.error(f"Global configuration state validation failed: {e}")
        raise ValueError(f"Global configuration state became invalid: {e}")
    
    # Save change log for audit trail
    try:
        save_change_log(changes)
    except Exception as e:
        logger.debug(f"Failed to save configuration change log: {e}")



def load_config(config_path: Path = CONFIG_FILE) -> Dict[str, Any]:
    """Load config file with enhanced validation, error recovery, migration support,
    integrated named configuration functionality, and intelligent memory management 
    for optimal performance during large file processing.
    
    Args:
        config_path: Path to the configuration file or name of saved/named configuration
        
    Returns:
        Dictionary containing the loaded configuration
        
    Raises:
        ValueError: If configuration format is invalid
        FileNotFoundError: If configuration file not found
    """
    try:
        # INITIAL MEMORY OPTIMIZATION - Get hardware context early for memory-aware processing
        hardware_data = None
        total_ram_gb = 8.0  # Conservative default
        
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        except Exception as e:
            logger.debug(f"Hardware detection failed during load_config: {e}")
        
        # Handle named configuration loading
        if isinstance(config_path, str) or (isinstance(config_path, Path) and not config_path.exists()):
            config_name = str(config_path)
            
            # First check if it's a named configuration
            registry_path = CONFIG_DIR / "named_configs_registry.json"
            if registry_path.exists():
                try:
                    with open(registry_path, 'r', encoding='utf-8') as f:
                        registry = json.load(f)
                    named_configs = registry.get("configs", {})
                    
                    if config_name in named_configs:
                        actual_config_path = Path(named_configs[config_name]["path"])
                        
                        if not actual_config_path.exists():
                            raise FileNotFoundError(f"Named configuration file not found: {actual_config_path}")
                        
                        # MEMORY OPTIMIZATION - Clear memory before recursive call
                        if total_ram_gb < 8:
                            try:
                                pre_recursive_clear = enhanced_clear_memory(
                                    aggressive=True,
                                    hardware_data=hardware_data
                                )
                                if pre_recursive_clear.get('success'):
                                    logger.debug("Memory optimized before named config recursive load")
                            except Exception as e:
                                logger.debug(f"Pre-recursive memory optimization failed: {e}")
                        
                        # Update last accessed time in registry
                        try:
                            update_named_config_registry(config_name, actual_config_path, {
                                "created": named_configs[config_name].get("created"),
                                "modified": named_configs[config_name].get("modified"),
                                "config": {
                                    "preset_used": named_configs[config_name].get("preset_used"),
                                    "model_type": named_configs[config_name].get("model_type"),
                                    "checksum": named_configs[config_name].get("checksum")
                                }
                            })
                        except Exception as e:
                            logger.debug(f"Failed to update access time for {config_name}: {e}")
                        
                        # Load the actual config file
                        return load_config(actual_config_path)
                        
                except Exception as e:
                    logger.warning(f"Failed to check named config registry: {e}")
            
            # If not a named config, try regular saved config
            regular_config_path = CONFIG_DIR / f"{config_name}.json"
            if regular_config_path.exists():
                return load_config(regular_config_path)
            
            raise FileNotFoundError(f"Configuration '{config_name}' not found")
        
        # Standard file loading logic with memory optimization
        try:
            if not config_path.exists():
                logger.info(f"No configuration file found at {config_path}")
                return {}
            
            # Check file size and basic validity with memory considerations
            file_size = config_path.stat().st_size
            if file_size == 0:
                logger.warning(f"Configuration file {config_path} is empty")
                return {}
            
            # 10MB limit
            if file_size > 10 * 1024 * 1024:
                logger.warning(f"Configuration file {config_path} is unusually large ({file_size} bytes)")
            
            # MEMORY OPTIMIZATION - Clear memory before loading large files
            large_file_threshold = 1024 * 1024  # 1MB
            if file_size > large_file_threshold or total_ram_gb < 8:
                try:
                    pre_load_clear = enhanced_clear_memory(
                        aggressive=file_size > large_file_threshold * 5 or total_ram_gb < 4,
                        hardware_data=hardware_data
                    )
                    if pre_load_clear.get('success'):
                        logger.debug(f"Memory optimized before loading {file_size} byte config file")
                except Exception as e:
                    logger.debug(f"Pre-load memory optimization failed: {e}")
            
            # Load with enhanced error handling
            logger.debug(f"Loading configuration from {config_path} ({file_size} bytes)")
            
            with open(config_path, 'r', encoding='utf-8') as f:
                try:
                    config_data = json.load(f)
                except json.JSONDecodeError as e:
                    # Try to recover from common JSON errors
                    logger.error(f"JSON decode error in {config_path}: {e}")
                    
                    # MEMORY OPTIMIZATION - Clear memory before intensive recovery operations
                    try:
                        recovery_clear = enhanced_clear_memory(
                            aggressive=total_ram_gb < 8,
                            hardware_data=hardware_data
                        )
                        if recovery_clear.get('success'):
                            logger.debug("Memory optimized before JSON recovery")
                    except Exception as e:
                        logger.debug(f"Pre-recovery memory optimization failed: {e}")
                    
                    # Attempt basic recovery
                    f.seek(0)
                    content = f.read()
                    
                    # Try to fix common issues
                    recovered_config = attempt_json_recovery(content, config_path)
                    if recovered_config:
                        config_data = recovered_config
                        logger.warning("Configuration recovered from JSON errors")
                    else:
                        raise ValueError(f"Cannot parse configuration file: {e}")
            
            # MEMORY OPTIMIZATION - Clear memory after file loading for large configs
            if file_size > large_file_threshold and total_ram_gb < 16:
                try:
                    post_load_clear = enhanced_clear_memory(
                        aggressive=file_size > large_file_threshold * 10,
                        hardware_data=hardware_data
                    )
                    if post_load_clear.get('success'):
                        logger.debug("Memory optimized after config file loading")
                except Exception as e:
                    logger.debug(f"Post-load memory optimization failed: {e}")
            
            # Validate basic structure
            if not isinstance(config_data, dict):
                raise ValueError("Configuration file must contain a JSON object")
            
            # Handle different configuration formats with memory optimization for large configs
            if 'config' in config_data and 'metadata' in config_data:
                # New format with metadata
                loaded_config = config_data['config']
                metadata = config_data['metadata']
                
                # MEMORY OPTIMIZATION - Clear memory before intensive metadata processing
                if len(str(metadata)) > 10000 and total_ram_gb < 16:
                    try:
                        metadata_clear = enhanced_clear_memory(
                            aggressive=False,
                            hardware_data=hardware_data
                        )
                        if metadata_clear.get('success'):
                            logger.debug("Memory optimized before metadata processing")
                    except Exception as e:
                        logger.debug(f"Metadata memory optimization failed: {e}")
                
                logger.debug(f"Loaded configuration with metadata: version={metadata.get('version', 'unknown')}")
                
                # Check version compatibility
                file_version = metadata.get('version', '1.0')
                if file_version != '2.1':
                    logger.info(f"Configuration version {file_version} detected, may need migration")
                    # The migration will be handled by the caller if needed
                
                # Verify checksum if present
                expected_checksum = metadata.get('config', {}).get('checksum')
                if expected_checksum:
                    actual_checksum = generate_config_checksum(loaded_config)
                    if actual_checksum != expected_checksum:
                        logger.warning("Configuration checksum mismatch - file may have been modified externally")
                
                # MEMORY OPTIMIZATION - Clear memory before preset information update
                if len(str(loaded_config)) > 50000 and total_ram_gb < 16:
                    try:
                        preset_clear = enhanced_clear_memory(
                            aggressive=False,
                            hardware_data=hardware_data
                        )
                        if preset_clear.get('success'):
                            logger.debug("Memory optimized before preset information update")
                    except Exception as e:
                        logger.debug(f"Preset memory optimization failed: {e}")
                
                # Ensure preset information is current
                if 'presets' in loaded_config:
                    try:
                        loaded_config['presets']['available_presets'] = get_available_presets()
                        loaded_config['presets']['preset_configs'] = get_preset_descriptions()
                        loaded_config['presets']['custom_presets_available'] = get_safe_custom_presets()
                    except Exception as e:
                        logger.debug(f"Failed to update preset information: {e}")
                
            else:
                # Legacy format - assume it's the configuration directly
                loaded_config = config_data
                logger.info("Loaded legacy configuration format")
                
                # MEMORY OPTIMIZATION - Clear memory before legacy format processing
                if len(str(loaded_config)) > 25000 and total_ram_gb < 16:
                    try:
                        legacy_clear = enhanced_clear_memory(
                            aggressive=len(str(loaded_config)) > 50000,
                            hardware_data=hardware_data
                        )
                        if legacy_clear.get('success'):
                            logger.debug("Memory optimized before legacy format processing")
                    except Exception as e:
                        logger.debug(f"Legacy format memory optimization failed: {e}")
                
                # Attempt to add current preset information to legacy configs
                try:
                    if 'presets' not in loaded_config:
                        loaded_config['presets'] = {}
                    loaded_config['presets']['available_presets'] = get_available_presets()
                    loaded_config['presets']['preset_configs'] = get_preset_descriptions()
                    loaded_config['presets']['custom_presets_available'] = get_safe_custom_presets()
                except Exception as e:
                    logger.debug(f"Failed to add preset information to legacy config: {e}")
            
            # Validate loaded configuration structure
            if not isinstance(loaded_config, dict):
                raise ValueError("Invalid configuration structure")
            
            # Basic sanity checks
            if not loaded_config:
                logger.warning("Configuration is empty")
                return {}
            
            # Enhanced validation for critical sections
            required_sections = ['training', 'model', 'security', 'data']
            missing_sections = [section for section in required_sections if section not in loaded_config]
            if missing_sections:
                logger.warning(f"Missing required sections: {missing_sections}")
                # Don't fail loading, but warn - these will be filled by defaults
            
            # Validate model type compatibility with current MODEL_VARIANTS
            model_config = loaded_config.get('model', {})
            model_type = model_config.get('model_type')
            if model_type and MODEL_VARIANTS and model_type not in MODEL_VARIANTS:
                logger.warning(f"Model type '{model_type}' not available in current MODEL_VARIANTS")
                # Don't modify the loaded config, let the caller handle this
            
            # MEMORY OPTIMIZATION - Clear memory before final processing steps
            if len(str(loaded_config)) > 75000 and total_ram_gb < 16:
                try:
                    final_processing_clear = enhanced_clear_memory(
                        aggressive=len(str(loaded_config)) > 150000,
                        hardware_data=hardware_data
                    )
                    if final_processing_clear.get('success'):
                        logger.debug("Memory optimized before final config processing")
                except Exception as e:
                    logger.debug(f"Final processing memory optimization failed: {e}")
            
            # Log loading statistics
            section_count = len([k for k, v in loaded_config.items() if isinstance(v, dict)])
            total_params = sum(len(v) if isinstance(v, dict) else 1 for v in loaded_config.values())
            
            logger.debug(f"Successfully loaded configuration: {section_count} sections, {total_params} parameters")
            logger.debug(f"Configuration sections: {list(loaded_config.keys())}")
            
            # Add load metadata
            if 'metadata' not in loaded_config:
                loaded_config['metadata'] = {}
            loaded_config['metadata']['last_loaded'] = datetime.now().isoformat()
            loaded_config['metadata']['loaded_from'] = str(config_path)
            
            # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
            # Aggressive cleanup after configuration loading completion
            try:
                final_clear_results = enhanced_clear_memory(
                    aggressive=True,  # Aggressive final cleanup
                    hardware_data=hardware_data
                )
                
                if final_clear_results.get('success'):
                    logger.debug(f"Final load memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
                    
            except Exception as e:
                logger.debug(f"Final load memory optimization failed: {e}")
            
            return loaded_config
            
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in config file {config_path}: {str(e)}")
            raise ValueError(f"Configuration file contains invalid JSON: {e}")
        except FileNotFoundError:
            logger.info(f"Configuration file not found: {config_path}")
            return {}
        except PermissionError as e:
            logger.error(f"Permission denied reading config file {config_path}: {e}")
            raise ValueError(f"Cannot read configuration file: permission denied")
        except Exception as e:
            logger.error(f"Error loading config from {config_path}: {str(e)}", exc_info=True)
            
            # MEMORY OPTIMIZATION - Clear memory before backup loading attempt
            try:
                backup_clear = enhanced_clear_memory(
                    aggressive=total_ram_gb < 8,
                    hardware_data=hardware_data
                )
                if backup_clear.get('success'):
                    logger.debug("Memory optimized before backup loading attempt")
            except Exception as cleanup_error:
                logger.debug(f"Pre-backup memory optimization failed: {cleanup_error}")
            
            # Try to load from backup if available
            backup_config = try_load_from_backup(config_path)
            if backup_config:
                logger.warning("Loaded configuration from backup due to primary file error")
                return backup_config
            
            raise ValueError(f"Failed to load configuration: {str(e)}")
            
    except Exception as e:
        logger.error(f"Critical error in load_config: {e}", exc_info=True)
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after load_config error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        # Re-raise the original exception
        raise

def update_named_config_registry(name: str, config_path: Path, metadata: Dict[str, Any]) -> None:
    """Update the named configuration registry for easier management with enhanced error handling."""
    try:
        registry_path = CONFIG_DIR / "named_configs_registry.json"
        
        # Ensure CONFIG_DIR exists
        CONFIG_DIR.mkdir(parents=True, exist_ok=True)
        
        # Load existing registry or create new
        registry = {
            "version": "2.1",
            "created": datetime.now().isoformat(),
            "configs": {}
        }
        
        if registry_path.exists():
            try:
                with open(registry_path, 'r', encoding='utf-8') as f:
                    existing_registry = json.load(f)
                    if isinstance(existing_registry, dict) and 'configs' in existing_registry:
                        registry = existing_registry
                        logger.debug(f"Loaded existing named config registry with {len(registry['configs'])} entries")
                    else:
                        logger.warning("Invalid registry format, creating new registry")
            except json.JSONDecodeError as e:
                logger.warning(f"Registry file corrupted, creating new: {e}")
            except Exception as e:
                logger.warning(f"Failed to load existing registry: {e}")
        
        # Extract configuration details safely
        config_dict = metadata.get("config", {})
        
        # Update registry entry with comprehensive information
        registry_entry = {
            "path": str(config_path),
            "created": metadata.get("created", datetime.now().isoformat()),
            "modified": metadata.get("modified", datetime.now().isoformat()),
            "last_accessed": datetime.now().isoformat(),
            "preset_used": config_dict.get("preset_used", "unknown"),
            "model_type": config_dict.get("model_type", "unknown"),
            "checksum": config_dict.get("checksum", "unknown"),
            "file_size": config_path.stat().st_size if config_path.exists() else 0,
            "config_version": metadata.get("version", "2.1"),
            "registry_version": "2.1"
        }
        
        # Add additional metadata if available
        if "system" in metadata:
            registry_entry["system_info"] = {
                "hostname": metadata["system"].get("hostname", "unknown"),
                "os": metadata["system"].get("os", "unknown"),
                "created_by": "deep_learning_config_system"
            }
        
        registry["configs"][name] = registry_entry
        registry["modified"] = datetime.now().isoformat()
        registry["total_configs"] = len(registry["configs"])
        
        # Add registry metadata
        if "metadata" not in registry:
            registry["metadata"] = {}
        registry["metadata"].update({
            "last_updated": datetime.now().isoformat(),
            "update_count": registry.get("metadata", {}).get("update_count", 0) + 1,
            "schema_version": "2.1"
        })
        
        # Atomic write operation with backup
        temp_path = registry_path.with_suffix(f".tmp_{int(time.time())}")
        try:
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(registry, f, indent=2, ensure_ascii=False, sort_keys=True)
            
            # Verify the written file
            with open(temp_path, 'r', encoding='utf-8') as f:
                verification_data = json.load(f)
                if not verification_data.get('configs'):
                    raise ValueError("Registry verification failed")
            
            # Create backup of existing registry if it exists
            if registry_path.exists():
                backup_path = registry_path.with_suffix(f".backup_{int(time.time())}")
                shutil.copy2(registry_path, backup_path)
                # Keep only last 5 backups
                cleanup_registry_backups(registry_path.parent, 5)
            
            # Atomic replacement
            if os.name == 'nt' and registry_path.exists():
                registry_path.unlink()
            temp_path.replace(registry_path)
            
        except Exception as write_error:
            if temp_path.exists():
                try:
                    temp_path.unlink()
                except:
                    pass
            raise RuntimeError(f"Failed to write registry: {write_error}") from write_error
        
        logger.debug(f"Updated named configuration registry: {name} -> {config_path}")
        
    except Exception as e:
        logger.error(f"Failed to update named configuration registry: {e}", exc_info=True)
        # Don't raise exception to avoid breaking config save operations

def cleanup_registry_backups(registry_dir: Path, keep_count: int):
    """Clean up old registry backup files."""
    try:
        backup_pattern = "named_configs_registry.backup_*"
        backup_files = list(registry_dir.glob(backup_pattern))
        
        if len(backup_files) > keep_count:
            # Sort by modification time (newest first)
            backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # Remove old backups
            for old_backup in backup_files[keep_count:]:
                old_backup.unlink()
                logger.debug(f"Removed old registry backup: {old_backup}")
                
    except Exception as e:
        logger.debug(f"Failed to cleanup registry backups: {e}")

def generate_config_checksum(config: Dict[str, Any]) -> str:
    """Generate a checksum for configuration integrity verification with enhanced stability.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        String checksum (SHA-256 hash of serialized config)
    """
    try:
        # Create a deep copy to avoid modifying original config
        config_copy = deepcopy(config)
        
        # Remove volatile/timestamp fields that shouldn't affect checksum
        volatile_fields = [
            'last_loaded', 'last_accessed', 'last_modified', 'modified',
            'created', 'timestamp', 'save_time', 'load_time',
            'runtime_id', 'process_id', 'collection_duration_ms'
        ]
        
        def remove_volatile_fields(obj, path=""):
            """Recursively remove volatile fields from nested dictionaries."""
            if isinstance(obj, dict):
                # Create new dict without volatile fields
                cleaned = {}
                for key, value in obj.items():
                    if key not in volatile_fields:
                        cleaned[key] = remove_volatile_fields(value, f"{path}.{key}" if path else key)
                return cleaned
            elif isinstance(obj, list):
                return [remove_volatile_fields(item, f"{path}[{i}]") for i, item in enumerate(obj)]
            else:
                return obj
        
        cleaned_config = remove_volatile_fields(config_copy)
        
        # Create a stable, normalized string representation
        # Sort keys at all levels for consistency
        config_str = json.dumps(
            cleaned_config,
            sort_keys=True,
            ensure_ascii=True,
            separators=(',', ':'),
            # Handle any non-serializable objects
            default=str
        )
        
        # Generate SHA-256 hash
        checksum = hashlib.sha256(config_str.encode('utf-8')).hexdigest()
        
        # Return first 16 characters for brevity while maintaining good collision resistance
        short_checksum = checksum[:16]
        
        logger.debug(f"Generated config checksum: {short_checksum} (from {len(config_str)} chars)")
        
        return short_checksum
        
    except Exception as e:
        logger.warning(f"Failed to generate config checksum: {e}")
        # Fallback checksum based on timestamp and basic config info
        try:
            fallback_data = {
                'timestamp': datetime.now().isoformat(),
                'config_keys': sorted(config.keys()) if isinstance(config, dict) else [],
                'config_size': len(str(config))
            }
            fallback_str = json.dumps(fallback_data, sort_keys=True)
            fallback_hash = hashlib.sha256(fallback_str.encode('utf-8')).hexdigest()
            # 'fb' prefix indicates fallback
            return f"fb_{fallback_hash[:14]}"
        except:
            # Minimal fallback
            return f"err_{int(time.time() * 1000) % 1000000:06d}"

def attempt_json_recovery(content: str, config_path: Path) -> Optional[Dict]:
    """Attempt to recover from common JSON formatting errors with enhanced recovery strategies."""
    try:
        logger.info(f"Attempting JSON recovery for {config_path}")
        
        # Strategy 1: Remove trailing commas before closing brackets/braces
        fixed_content = content
        
        # Remove trailing commas before closing brackets
        fixed_content = re.sub(r',(\s*[\]}])', r'\1', fixed_content)
        
        # Remove comments (// and /* */ style)
        fixed_content = re.sub(r'//.*?$', '', fixed_content, flags=re.MULTILINE)
        fixed_content = re.sub(r'/\*.*?\*/', '', fixed_content, flags=re.DOTALL)
        
        # Try parsing the fixed content
        try:
            recovered_data = json.loads(fixed_content)
            logger.info(f"Successfully recovered JSON using trailing comma fix")
            return recovered_data
        except json.JSONDecodeError:
            pass
        
        # Strategy 2: Fix common quote issues
        # Replace single quotes with double quotes (be careful about strings)
        fixed_content = content
        
        # Simple single quote to double quote replacement
        # This is naive but works for many simple cases
        fixed_content = re.sub(r"'([^']*)'(\s*:)", r'"\1"\2', fixed_content)
        fixed_content = re.sub(r":\s*'([^']*)'", r': "\1"', fixed_content)
        
        try:
            recovered_data = json.loads(fixed_content)
            logger.info(f"Successfully recovered JSON using quote fix")
            return recovered_data
        except json.JSONDecodeError:
            pass
        
        # Strategy 3: Try to fix missing quotes around keys
        fixed_content = content
        
        # Add quotes around unquoted keys (simple pattern)
        fixed_content = re.sub(r'(\n\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', fixed_content)
        fixed_content = re.sub(r'(\{\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', fixed_content)
        
        try:
            recovered_data = json.loads(fixed_content)
            logger.info(f"Successfully recovered JSON using key quote fix")
            return recovered_data
        except json.JSONDecodeError:
            pass
        
        # Strategy 4: Try to extract valid JSON from corrupted file
        # Look for the largest valid JSON object/array
        json_start_patterns = [r'\{', r'\[']
        json_end_patterns = [r'\}', r'\]']
        
        for start_pattern, end_pattern in zip(json_start_patterns, json_end_patterns):
            matches = list(re.finditer(start_pattern, content))
            for match in matches:
                start_pos = match.start()
                
                # Try to find matching closing bracket/brace
                bracket_count = 0
                end_pos = -1
                in_string = False
                escape_next = False
                
                for i, char in enumerate(content[start_pos:], start_pos):
                    if escape_next:
                        escape_next = False
                        continue
                    
                    if char == '\\':
                        escape_next = True
                        continue
                    
                    if char == '"' and not escape_next:
                        in_string = not in_string
                        continue
                    
                    if not in_string:
                        if re.match(start_pattern, char):
                            bracket_count += 1
                        elif re.match(end_pattern, char):
                            bracket_count -= 1
                            if bracket_count == 0:
                                end_pos = i + 1
                                break
                
                if end_pos > start_pos:
                    try:
                        candidate_json = content[start_pos:end_pos]
                        recovered_data = json.loads(candidate_json)
                        logger.info(f"Successfully recovered JSON by extraction ({len(candidate_json)} chars)")
                        return recovered_data
                    except json.JSONDecodeError:
                        continue
        
        # Strategy 5: Try eval() for Python-like dictionaries (DANGEROUS - only for trusted configs)
        if config_path and config_path.suffix == '.json':
            try:
                # Only attempt this if the content looks like a Python dict
                if content.strip().startswith('{') and 'True' in content or 'False' in content:
                    logger.warning("Attempting dangerous eval() recovery - ensure config file is trusted")
                    
                    # Basic safety checks
                    dangerous_patterns = ['import', 'exec', 'eval', '__', 'open', 'file']
                    if not any(pattern in content.lower() for pattern in dangerous_patterns):
                        
                        # Replace Python booleans with JSON booleans
                        python_content = content
                        python_content = re.sub(r'\bTrue\b', 'true', python_content)
                        python_content = re.sub(r'\bFalse\b', 'false', python_content)
                        python_content = re.sub(r'\bNone\b', 'null', python_content)
                        
                        try:
                            recovered_data = json.loads(python_content)
                            logger.warning("Successfully recovered JSON using Python-to-JSON conversion")
                            return recovered_data
                        except json.JSONDecodeError:
                            pass
            except Exception as eval_error:
                logger.debug(f"Eval recovery failed safely: {eval_error}")
        
        # Strategy 6: Line-by-line recovery for corrupted multi-line JSON
        lines = content.split('\n')
        recovered_lines = []
        
        for line in lines:
            # Skip obviously corrupted lines
            if line.strip() and not line.strip().startswith('#'):
                # Try to fix common issues in individual lines
                fixed_line = line
                
                # Remove trailing commas at end of line
                fixed_line = re.sub(r',\s*$', '', fixed_line)
                
                # Add missing commas between adjacent values
                if '"' in fixed_line and ':' in fixed_line:
                    # This line looks like it contains a key-value pair
                    recovered_lines.append(fixed_line)
                elif line.strip() in ['{', '}', '[', ']']:
                    # Structural characters
                    recovered_lines.append(fixed_line)
        
        if recovered_lines:
            try:
                reconstructed = '\n'.join(recovered_lines)
                recovered_data = json.loads(reconstructed)
                logger.info(f"Successfully recovered JSON using line-by-line reconstruction")
                return recovered_data
            except json.JSONDecodeError:
                pass
        
        logger.warning(f"All JSON recovery strategies failed for {config_path}")
        return None
        
    except Exception as e:
        logger.warning(f"JSON recovery attempt failed with exception: {e}")
        return None

def try_load_from_backup(config_path: Path) -> Optional[Dict]:
    """Try to load configuration from the most recent backup with enhanced backup discovery."""
    try:
        logger.info(f"Attempting to load from backup for {config_path}")
        
        # Strategy 1: Look in standard backup directory
        backup_dir = config_path.parent / "backups"
        backup_configs = []
        
        if backup_dir.exists():
            # Find backup files for this config
            backup_pattern = f"{config_path.stem}_v*{config_path.suffix}"
            backup_files = list(backup_dir.glob(backup_pattern))
            
            for backup_file in backup_files:
                try:
                    stat_info = backup_file.stat()
                    backup_configs.append({
                        'path': backup_file,
                        'mtime': stat_info.st_mtime,
                        'size': stat_info.st_size,
                        'age_hours': (time.time() - stat_info.st_mtime) / 3600
                    })
                except OSError:
                    continue
        
        # Strategy 2: Look for .bak files in the same directory
        bak_file = config_path.with_suffix(config_path.suffix + '.bak')
        if bak_file.exists():
            try:
                stat_info = bak_file.stat()
                backup_configs.append({
                    'path': bak_file,
                    'mtime': stat_info.st_mtime,
                    'size': stat_info.st_size,
                    'age_hours': (time.time() - stat_info.st_mtime) / 3600,
                    'type': 'bak_file'
                })
            except OSError:
                pass
        
        # Strategy 3: Look for auto-saved configurations
        auto_save_patterns = [
            config_path.with_name(f"{config_path.stem}.autosave{config_path.suffix}"),
            config_path.with_name(f"{config_path.stem}_backup{config_path.suffix}"),
            config_path.with_name(f"backup_{config_path.name}")
        ]
        
        for auto_save_path in auto_save_patterns:
            if auto_save_path.exists():
                try:
                    stat_info = auto_save_path.stat()
                    backup_configs.append({
                        'path': auto_save_path,
                        'mtime': stat_info.st_mtime,
                        'size': stat_info.st_size,
                        'age_hours': (time.time() - stat_info.st_mtime) / 3600,
                        'type': 'auto_save'
                    })
                except OSError:
                    continue
        
        if not backup_configs:
            logger.info("No backup configurations found")
            return None
        
        # Sort by modification time (newest first)
        backup_configs.sort(key=lambda x: x['mtime'], reverse=True)
        
        logger.info(f"Found {len(backup_configs)} potential backup configurations")
        
        # Try to load backups in order of preference
        for i, backup_info in enumerate(backup_configs):
            backup_path = backup_info['path']
            age_hours = backup_info['age_hours']
            
            try:
                logger.info(f"Trying backup {i+1}: {backup_path} (age: {age_hours:.1f}h)")
                
                # Basic file validation
                if backup_info['size'] == 0:
                    logger.debug(f"Skipping empty backup: {backup_path}")
                    continue
                
                # Skip very old backups (older than 30 days) unless it's the only option
                if age_hours > 24 * 30 and i < len(backup_configs) - 1:
                    logger.debug(f"Skipping old backup: {backup_path} ({age_hours:.1f}h old)")
                    continue
                
                # Try to load the backup
                backup_config = load_config(backup_path)
                
                if backup_config:
                    # Validate the backup configuration
                    if isinstance(backup_config, dict) and backup_config:
                        # Check if it has required sections
                        required_sections = ['training', 'model', 'data']
                        missing_sections = [s for s in required_sections if s not in backup_config]
                        
                        # Allow one missing section
                        if len(missing_sections) <= 1:
                            logger.info(f"Successfully loaded backup from: {backup_path}")
                            
                            # Add backup metadata to the loaded config
                            if 'metadata' not in backup_config:
                                backup_config['metadata'] = {}
                            
                            backup_config['metadata']['loaded_from_backup'] = {
                                'backup_path': str(backup_path),
                                'backup_age_hours': age_hours,
                                'original_path': str(config_path),
                                'loaded_at': datetime.now().isoformat(),
                                'backup_type': backup_info.get('type', 'standard')
                            }
                            
                            return backup_config
                        else:
                            logger.debug(f"Backup missing too many required sections: {missing_sections}")
                    else:
                        logger.debug(f"Backup config is not a valid dictionary: {type(backup_config)}")
                else:
                    logger.debug(f"Backup config is empty: {backup_path}")
                    
            except Exception as e:
                logger.debug(f"Failed to load backup {backup_path}: {e}")
                continue
        
        logger.warning("All backup loading attempts failed")
        return None
        
    except Exception as e:
        logger.warning(f"Error during backup loading process: {e}")
        return None

# Extracted validation logic from validate_config() into reusable validators
class ConfigSectionValidators:
    """Centralized validation logic extracted from validate_config() for reuse across update functions."""
    
    @staticmethod
    def validate_model_parameters(model: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        """Extract model validation logic from validate_config()."""
        errors = []
        warnings = []
        
        # Model type validation with MODEL_VARIANTS checking
        model_type = model.get('model_type', 'SimpleAutoencoder')
        if MODEL_VARIANTS:
            if model_type not in MODEL_VARIANTS:
                errors.append(f"Invalid model_type: {model_type}, available: {list(MODEL_VARIANTS.keys())}")
        else:
            valid_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
            if model_type not in valid_types:
                errors.append(f"Invalid model_type: {model_type}, expected one of: {valid_types}")
        
        # Encoding dimension validation
        encoding_dim = model.get('encoding_dim', 8)
        if not isinstance(encoding_dim, int) or encoding_dim <= 0:
            errors.append(f"Invalid encoding_dim: {encoding_dim}, must be positive integer")
        elif encoding_dim > 1000:
            warnings.append(f"Very large encoding_dim: {encoding_dim}, may cause memory issues")
        elif encoding_dim < 2:
            warnings.append(f"Very small encoding_dim: {encoding_dim}, may limit expressiveness")
        
        # Architecture validation
        hidden_dims = model.get('hidden_dims', [])
        if not isinstance(hidden_dims, list):
            errors.append("hidden_dims must be a list")
        elif not hidden_dims:
            errors.append("hidden_dims cannot be empty")
        else:
            invalid_dims = [dim for dim in hidden_dims if not isinstance(dim, int) or dim <= 0]
            if invalid_dims:
                errors.append(f"Invalid hidden dimensions: {invalid_dims}")
            
            if len(hidden_dims) > 10:
                warnings.append(f"Very deep architecture ({len(hidden_dims)} layers) may be hard to train")
            
            large_dims = [dim for dim in hidden_dims if dim > 2048]
            if large_dims:
                warnings.append(f"Very large hidden dimensions: {large_dims}")
        
        # Dropout validation
        dropout_rates = model.get('dropout_rates', [])
        if not isinstance(dropout_rates, list):
            errors.append("dropout_rates must be a list")
        elif not dropout_rates:
            errors.append("dropout_rates cannot be empty")
        else:
            invalid_rates = [rate for rate in dropout_rates if not isinstance(rate, (int, float)) or not (0 <= rate < 1)]
            if invalid_rates:
                errors.append(f"Invalid dropout rates: {invalid_rates}, must be between 0 and 1")
            
            if len(hidden_dims) != len(dropout_rates):
                errors.append(f"Length mismatch: {len(hidden_dims)} hidden layers, {len(dropout_rates)} dropout rates")
            
            if all(rate == 0 for rate in dropout_rates):
                warnings.append("No dropout applied (all rates are 0)")
            elif any(rate > 0.8 for rate in dropout_rates):
                warnings.append("Very high dropout rates may hurt performance")
        
        # Activation function validation
        activation = model.get('activation', 'relu')
        available_activations = model.get('available_activations', ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish'])
        if activation not in available_activations:
            errors.append(f"Invalid activation: {activation}, available: {available_activations}")
        
        # Normalization validation
        normalization = model.get('normalization')
        available_normalizations = model.get('available_normalizations', ['batch', 'layer', 'instance', 'group', None])
        if normalization not in available_normalizations:
            errors.append(f"Invalid normalization: {normalization}, available: {available_normalizations}")

        use_batch_norm = model.get('use_batch_norm', False)
        use_layer_norm = model.get('use_layer_norm', False)

        # Fix inconsistencies: ensure normalization setting matches use_*_norm flags
        if normalization is None:
            # If no normalization specified, both flags should be False
            if use_batch_norm or use_layer_norm:
                warnings.append("normalization=None but batch_norm or layer_norm enabled - auto-correcting to disable both")
                model['use_batch_norm'] = False
                model['use_layer_norm'] = False
        elif normalization == 'batch':
            # If batch normalization specified, use_batch_norm should be True, use_layer_norm should be False
            if not use_batch_norm:
                warnings.append("normalization='batch' but use_batch_norm=False - auto-correcting to use_batch_norm=True")
                model['use_batch_norm'] = True
            if use_layer_norm:
                warnings.append("normalization='batch' but use_layer_norm=True - auto-correcting to use_layer_norm=False")
                model['use_layer_norm'] = False
        elif normalization == 'layer':
            # If layer normalization specified, use_layer_norm should be True, use_batch_norm should be False
            if not use_layer_norm:
                warnings.append("normalization='layer' but use_layer_norm=False - auto-correcting to use_layer_norm=True")
                model['use_layer_norm'] = True
            if use_batch_norm:
                warnings.append("normalization='layer' but use_batch_norm=True - auto-correcting to use_batch_norm=False")
                model['use_batch_norm'] = False
        elif normalization in ['instance', 'group']:
            # For instance/group normalization, both batch_norm and layer_norm should be False
            if use_batch_norm or use_layer_norm:
                warnings.append(f"normalization='{normalization}' but batch_norm or layer_norm enabled - auto-correcting to disable both")
                model['use_batch_norm'] = False
                model['use_layer_norm'] = False

        # Final validation check - should not have both enabled after auto-corrections
        final_use_batch_norm = model.get('use_batch_norm', False)
        final_use_layer_norm = model.get('use_layer_norm', False)
        if final_use_batch_norm and final_use_layer_norm:
            warnings.append("Both batch_norm and layer_norm still enabled after auto-correction - may cause conflicts")
        
        # Advanced features validation
        if model.get('skip_connection', False) and len(hidden_dims) < 2:
            warnings.append("Skip connections are most effective with deeper architectures")
        
        if model.get('residual_blocks', False) and len(hidden_dims) < 3:
            warnings.append("Residual blocks are most effective with deeper architectures")
        
        if model.get('use_attention', False) and model_type == 'SimpleAutoencoder':
            warnings.append("Attention mechanism may not be supported in SimpleAutoencoder")
        
        # Ensemble-specific validation
        if model_type == 'AutoencoderEnsemble':
            num_models = model.get('num_models', 1)
            if not isinstance(num_models, int) or num_models < 1:
                errors.append(f"Invalid num_models: {num_models}, must be positive integer")
            elif num_models < 2:
                warnings.append("Ensemble should have at least 2 models for effectiveness")
            elif num_models > 20:
                warnings.append(f"Large ensemble ({num_models} models) may require significant memory")
        
        return errors, warnings
    
    @staticmethod
    def validate_training_parameters(training: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        """Extract training validation logic from validate_config()."""
        errors = []
        warnings = []
        
        # Core parameter validation
        batch_size = training.get('batch_size', 32)
        if not isinstance(batch_size, int) or batch_size < 1:
            errors.append(f"Invalid batch_size: {batch_size}, must be positive integer")
        elif batch_size > 2048:
            warnings.append(f"Very large batch_size: {batch_size}, may cause memory issues")
        elif batch_size < 2:
            warnings.append(f"Very small batch_size: {batch_size}, may cause training instability")
        
        epochs = training.get('epochs', 100)
        if not isinstance(epochs, int) or epochs < 1:
            errors.append(f"Invalid epochs: {epochs}, must be positive integer")
        elif epochs > 5000:
            warnings.append(f"Very high epoch count: {epochs}, consider early stopping")
        elif epochs < 5:
            warnings.append(f"Very low epoch count: {epochs}, may not converge properly")
        
        learning_rate = training.get('learning_rate', 0.001)
        if not isinstance(learning_rate, (int, float)) or learning_rate <= 0:
            errors.append(f"Invalid learning_rate: {learning_rate}, must be positive number")
        elif learning_rate > 1.0:
            warnings.append(f"Very high learning_rate: {learning_rate}, may cause instability")
        elif learning_rate < 1e-8:
            warnings.append(f"Very low learning_rate: {learning_rate}, may not converge")
        
        # Optimizer validation
        optimizer = training.get('optimizer', 'Adam')
        valid_optimizers = ['SGD', 'Adam', 'AdamW', 'RMSprop', 'Adagrad', 'LBFGS']
        if optimizer not in valid_optimizers:
            errors.append(f"Invalid optimizer: {optimizer}, must be one of {valid_optimizers}")
        
        # Scheduler validation
        scheduler = training.get('scheduler')
        if scheduler is not None:
            valid_schedulers = [
                'StepLR', 'MultiStepLR', 'ExponentialLR', 'CosineAnnealingLR',
                'ReduceLROnPlateau', 'CosineAnnealingWarmRestarts', 'OneCycleLR', 'CyclicLR'
            ]
            if scheduler not in valid_schedulers:
                errors.append(f"Invalid scheduler: {scheduler}, must be one of {valid_schedulers}")
        
        # Hardware-aware validation
        if training.get('mixed_precision', False) and not torch.cuda.is_available():
            warnings.append("mixed_precision enabled but CUDA not available")
        
        num_workers = training.get('num_workers', 1)
        max_workers = (os.cpu_count() or 1) * 2
        if not isinstance(num_workers, int) or num_workers < 0:
            errors.append("num_workers must be non-negative integer")
        elif num_workers > max_workers:
            warnings.append(f"num_workers ({num_workers}) exceeds recommended max ({max_workers})")
        
        return errors, warnings
    
    @staticmethod
    def validate_presets_parameters(presets: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        """Extract presets validation logic from validate_config()."""
        errors = []
        warnings = []
        
        current_preset = presets.get('current_preset')
        if current_preset is not None:
            available_presets = get_available_presets()
            if current_preset not in available_presets:
                errors.append(f"Invalid current_preset: {current_preset}, available: {available_presets}")
        
        # Override rules validation
        if 'override_rules' in presets:
            override_rules = presets['override_rules']
            if not isinstance(override_rules, dict):
                errors.append("override_rules must be a dictionary")
            else:
                valid_sections = ['security', 'monitoring', 'hardware', 'training', 'model', 'data']
                for section, enabled in override_rules.items():
                    if section not in valid_sections:
                        warnings.append(f"Unknown override rule section: {section}")
                    elif not isinstance(enabled, bool):
                        errors.append(f"Override rule for {section} must be boolean")
        
        auto_apply = presets.get('auto_apply', False)
        if not isinstance(auto_apply, bool):
            errors.append("auto_apply must be boolean")
        
        return errors, warnings
    
    @staticmethod
    def validate_cross_section_compatibility(config: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        """Extract cross-section validation logic from validate_config()."""
        errors = []
        warnings = []
        
        # Model-Training compatibility
        if 'model' in config and 'training' in config:
            model_config = config['model']
            training_config = config['training']
            
            if model_config.get('use_batch_norm') and training_config.get('batch_size', 32) < 2:
                errors.append("Batch normalization requires training batch_size >= 2")
            
            if training_config.get('mixed_precision') and model_config.get('model_type') == 'SimpleAutoencoder':
                warnings.append("Mixed precision may not be fully supported with SimpleAutoencoder")
        
        # Hardware-Training compatibility
        if 'hardware' in config and 'training' in config:
            hardware_config = config['hardware']
            training_config = config['training']
            
            device = hardware_config.get('device', 'auto')
            use_cuda = hardware_config.get('performance_optimization', {}).get('use_cuda', False)
            
            if device == 'cpu' and use_cuda:
                warnings.append("Device set to CPU but use_cuda=True in performance optimization")
            elif device == 'cuda' and not use_cuda:
                warnings.append("Device set to CUDA but use_cuda=False in performance optimization")
        
        # Data-Model compatibility
        if 'data' in config and 'model' in config:
            data_config = config['data']
            model_config = config['model']
            
            data_features = data_config.get('features', 20)
            min_features = model_config.get('min_features', 5)
            
            if data_features < min_features:
                errors.append(f"Data features ({data_features}) < model min_features ({min_features})")
        
        return errors, warnings

# Simplified parameter validators using the extracted logic
class ParameterValidator:
    """Simplified parameter validation using extracted validation logic."""
    
    @staticmethod
    def validate_and_update_with_feedback(section_name: str, result_config: Dict, 
                                        update_config: Dict, changes_made: List[Dict]) -> bool:
        """
        Validate and update configuration section using extracted validation logic.
        
        Returns:
            bool: True if validation passed (warnings allowed), False if errors found
        """
        # Create temporary config for validation
        temp_config = result_config.copy()
        temp_config.update(update_config)
        
        # Get validation results based on section
        if section_name == 'model':
            errors, warnings = ConfigSectionValidators.validate_model_parameters(temp_config)
        elif section_name == 'training':
            errors, warnings = ConfigSectionValidators.validate_training_parameters(temp_config)
        elif section_name == 'presets':
            errors, warnings = ConfigSectionValidators.validate_presets_parameters(temp_config)
        else:
            # For other sections, allow updates without validation
            errors, warnings = [], []
        
        # Log validation results
        for error in errors:
            logger.error(f"{section_name}: {error}")
        
        for warning in warnings:
            logger.warning(f"{section_name}: {warning}")
        
        # If validation passed (no errors), apply updates
        if not errors:
            for key, value in update_config.items():
                if value is not None:
                    old_value = result_config.get(key)
                    if old_value != value:
                        result_config[key] = value
                        changes_made.append({
                            'key': f'{section_name}.{key}',
                            'old_value': old_value,
                            'new_value': value,
                            'timestamp': datetime.now().isoformat(),
                            'validation_passed': True,
                            'warnings': len(warnings)
                        })
            return True
        else:
            # Log validation failure
            changes_made.append({
                'key': f'{section_name}.validation_failed',
                'old_value': None,
                'new_value': f'Errors: {len(errors)}, Warnings: {len(warnings)}',
                'timestamp': datetime.now().isoformat(),
                'errors': errors,
                'warnings': warnings
            })
            return False

def deep_update(original: Dict[str, Any], update: Dict[str, Any]) -> Dict[str, Any]:
    """Simplified recursive update using centralized validation."""
    if not isinstance(original, dict) or not isinstance(update, dict):
        raise ValueError("Both original and update must be dictionaries")
    
    changes_made = []
    
    # Handle sections with specialized logic
    specialized_sections = {'model', 'training', 'presets', 'metadata'}
    
    for key, value in update.items():
        try:
            if key in original and isinstance(original[key], dict) and isinstance(value, dict):
                if key == 'model':
                    # Integrated deep_update_model_section functionality
                    result_model = original[key].copy()
                    
                    # Use centralized validation for basic parameters
                    basic_params = {k: v for k, v in value.items() 
                                  if k not in ['hidden_dims', 'dropout_rates', 'num_models', 'diversity_factor']}
                    
                    validation_passed = ParameterValidator.validate_and_update_with_feedback(
                        'model', result_model, basic_params, changes_made
                    )
                    
                    if not validation_passed:
                        logger.warning("Model validation failed, skipping basic parameter updates")
                    
                    # Handle architecture updates (hidden_dims and dropout_rates with length matching)
                    if 'hidden_dims' in value or 'dropout_rates' in value:
                        # Validate and normalize architecture parameters
                        hidden_dims = value.get('hidden_dims', result_model.get('hidden_dims', []))
                        dropout_rates = value.get('dropout_rates', result_model.get('dropout_rates', []))
                        
                        # Ensure both are lists
                        if not isinstance(hidden_dims, list):
                            hidden_dims = [hidden_dims] if isinstance(hidden_dims, int) else []
                        if not isinstance(dropout_rates, list):
                            dropout_rates = [dropout_rates] if isinstance(dropout_rates, (int, float)) else []
                        
                        # Match lengths
                        if len(hidden_dims) != len(dropout_rates):
                            target_length = max(len(hidden_dims), len(dropout_rates))
                            
                            # Extend shorter list
                            while len(hidden_dims) < target_length:
                                hidden_dims.append(hidden_dims[-1] if hidden_dims else 64)
                            while len(dropout_rates) < target_length:
                                dropout_rates.append(dropout_rates[-1] if dropout_rates else 0.2)
                        
                        # Update with validated values
                        result_model['hidden_dims'] = hidden_dims
                        result_model['dropout_rates'] = dropout_rates
                        
                        changes_made.append({
                            'key': 'model.architecture_sync',
                            'old_value': {'hidden': len(result_model.get('hidden_dims', [])), 
                                         'dropout': len(result_model.get('dropout_rates', []))},
                            'new_value': {'hidden': len(hidden_dims), 'dropout': len(dropout_rates)},
                            'timestamp': datetime.now().isoformat()
                        })
                    
                    # Handle ensemble-specific parameters with validation
                    if result_model.get('model_type') == 'AutoencoderEnsemble':
                        for param in ['num_models', 'diversity_factor']:
                            if param in value:
                                param_value = value[param]
                                
                                # Simple validation
                                if param == 'num_models' and isinstance(param_value, int) and 1 <= param_value <= 20:
                                    result_model[param] = param_value
                                elif param == 'diversity_factor' and isinstance(param_value, (int, float)) and 0 <= param_value <= 1:
                                    result_model[param] = param_value
                                else:
                                    logger.warning(f"Invalid ensemble parameter {param}: {param_value}")
                    
                    # Ensure model metadata lists are consistent
                    if 'available_activations' not in result_model:
                        result_model['available_activations'] = ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish']
                    
                    if 'available_normalizations' not in result_model:
                        result_model['available_normalizations'] = ['batch', 'layer', 'instance', 'group', None]
                    
                    original[key] = result_model
                    
                elif key == 'training':
                    # Integrated deep_update_training_section functionality
                    result_training = original[key].copy()
                    
                    # Use centralized validation for basic parameters
                    basic_params = {k: v for k, v in value.items() 
                                  if k not in ['scheduler_params', 'adam_betas', 'adam_eps']}
                    
                    validation_passed = ParameterValidator.validate_and_update_with_feedback(
                        'training', result_training, basic_params, changes_made
                    )
                    
                    if not validation_passed:
                        logger.warning("Training validation failed, skipping basic parameter updates")
                    
                    # Handle optimizer-specific parameters
                    optimizer = result_training.get('optimizer', 'Adam')
                    
                    if optimizer in ['Adam', 'AdamW']:
                        if 'adam_betas' in value:
                            betas = value['adam_betas']
                            if isinstance(betas, (list, tuple)) and len(betas) == 2:
                                result_training['adam_betas'] = betas
                        
                        if 'adam_eps' in value:
                            eps = value['adam_eps']
                            if isinstance(eps, (int, float)) and eps > 0:
                                result_training['adam_eps'] = eps
                    
                    # Handle scheduler parameters
                    if 'scheduler_params' in value and result_training.get('scheduler'):
                        result_training['scheduler_params'] = value['scheduler_params']
                    
                    original[key] = result_training
                    
                elif key == 'presets':
                    # Integrated deep_update_presets_section functionality
                    result_presets = original[key].copy()
                    
                    # Use centralized validation
                    validation_passed = ParameterValidator.validate_and_update_with_feedback(
                        'presets', result_presets, value, changes_made
                    )
                    
                    if not validation_passed:
                        logger.warning("Presets validation failed, skipping updates")
                    else:
                        # Update dynamic preset information
                        try:
                            result_presets['available_presets'] = get_available_presets()
                            result_presets['last_dynamic_update'] = datetime.now().isoformat()
                        except Exception as e:
                            logger.debug(f"Failed to update dynamic preset info: {e}")
                        
                        # Generate simple preset recommendations
                        try:
                            if torch.cuda.is_available():
                                result_presets['system_recommendations'] = ['performance', 'advanced']
                            else:
                                result_presets['system_recommendations'] = ['lightweight', 'debug']
                        except Exception as e:
                            logger.debug(f"Failed to generate preset recommendations: {e}")
                        
                        # Update preset usage statistics
                        if 'usage_statistics' not in result_presets:
                            result_presets['usage_statistics'] = {
                                'total_preset_changes': 0,
                                'last_preset_change': datetime.now().isoformat()
                            }
                        
                        if 'current_preset' in value:
                            stats = result_presets['usage_statistics']
                            stats['total_preset_changes'] += 1
                            stats['last_preset_change'] = datetime.now().isoformat()
                    
                    original[key] = result_presets
                    
                elif key == 'metadata':
                    # Integrated deep_update_metadata_section functionality
                    result_metadata = deepcopy(original[key]) if original[key] else {}
                    
                    # Metadata doesn't need the same validation as other sections
                    # Just handle the special metadata logic
                    current_time = datetime.now().isoformat()
                    result_metadata['modified'] = current_time
                    result_metadata['last_updated'] = current_time
                    
                    # Handle version tracking
                    if 'version' in value:
                        old_version = result_metadata.get('version')
                        result_metadata['version'] = value['version']
                        changes_made.append({
                            'key': 'metadata.version',
                            'old_value': old_version,
                            'new_value': value['version'],
                            'timestamp': current_time
                        })
                    
                    # Handle system information
                    if 'system' in value:
                        if 'system' not in result_metadata:
                            result_metadata['system'] = {}
                        result_metadata['system'].update(value['system'])
                    
                    # Handle preset tracking
                    preset_fields = ['preset_used', 'current_preset', 'preset_source']
                    for field in preset_fields:
                        if field in value:
                            result_metadata[field] = value[field]
                    
                    # Handle migration metadata
                    if 'migration' in value:
                        result_metadata['migration'] = value['migration']
                    
                    # Handle validation metadata
                    if 'validation' in value:
                        result_metadata['validation'] = value['validation']
                    
                    # Handle remaining fields
                    for meta_key, meta_value in value.items():
                        if meta_key not in ['version', 'system', 'preset_used', 'migration', 'validation'] and meta_value is not None:
                            old_value = result_metadata.get(meta_key)
                            if old_value != meta_value:
                                result_metadata[meta_key] = meta_value
                                changes_made.append({
                                    'key': f'metadata.{meta_key}',
                                    'old_value': old_value,
                                    'new_value': meta_value,
                                    'timestamp': current_time
                                })
                    
                    original[key] = result_metadata
                    
                else:
                    # Standard recursive update for other sections
                    original[key] = deep_update(original[key], value)
            else:
                # Handle non-dict values
                if value is not None:
                    original[key] = value
        except Exception as e:
            logger.warning(f"Error updating key '{key}': {e}")
            continue
    
    # Apply cross-section validation using extracted logic
    if len([k for k in update.keys() if k in specialized_sections]) > 1:
        errors, warnings = ConfigSectionValidators.validate_cross_section_compatibility(original)
        
        for error in errors:
            logger.error(f"Cross-section validation error: {error}")
        
        for warning in warnings:
            logger.warning(f"Cross-section validation warning: {warning}")
        
        if errors:
            changes_made.append({
                'key': 'cross_section_validation',
                'old_value': 'unknown',
                'new_value': f'Failed with {len(errors)} errors',
                'timestamp': datetime.now().isoformat(),
                'errors': errors
            })
    
    # Log summary
    if changes_made:
        logger.debug(f"deep_update made {len(changes_made)} changes")
    
    return original

def get_current_config() -> Dict[str, Any]:
    """Return comprehensive configuration with preset awareness, caching, and validation.
    
    This function provides the current active configuration with intelligent fallback
    mechanisms, system-aware defaults, and comprehensive error recovery. It integrates
    with the updated preset system, configuration caching, and validation framework.
    
    Returns:
        Dictionary containing all configuration parameters with metadata,
        respecting any active preset configuration and system optimizations.
        
    Raises:
        RuntimeError: If configuration cannot be retrieved or validated
    """
    global _cached_config, _config_cache_time
    
    current_time = time.time()
    
    # Check cache validity (30 seconds) with comprehensive validation
    if (_cached_config is not None and _config_cache_time is not None and 
        current_time - _config_cache_time < 30):
        return _cached_config
    
    # Step 1: Attempt to load existing configuration
    loaded_config = None
    current_preset = None
    
    try:
        loaded_config = load_config()
        if loaded_config and isinstance(loaded_config, dict):
            current_preset = loaded_config.get('presets', {}).get('current_preset')
            logger.debug(f"Loaded existing configuration with preset: {current_preset}")
        else:
            logger.debug("No valid existing configuration found")
    except Exception as e:
        logger.debug(f"Failed to load existing configuration: {e}")
    
    # Step 2: Determine base configuration source with intelligent selection
    base_config = None
    config_source = None
    
    # Priority 1: Active preset configuration
    if current_preset and current_preset in PRESET_CONFIGS:
        try:
            base_config = deepcopy(PRESET_CONFIGS[current_preset])
            config_source = f'preset_{current_preset}'
            logger.debug(f"Using preset configuration: {current_preset}")
        except Exception as e:
            logger.warning(f"Failed to load preset {current_preset}: {e}")
    
    # Priority 2: Default preset if available
    if base_config is None and 'default' in PRESET_CONFIGS:
        try:
            base_config = deepcopy(PRESET_CONFIGS['default'])
            config_source = 'preset_default'
            logger.debug("Using default preset configuration")
        except Exception as e:
            logger.warning(f"Failed to load default preset: {e}")
    
    # Priority 3: System-aware default configuration
    if base_config is None:
        try:
            base_config = get_default_config()
            config_source = 'system_aware_default'
            logger.debug("Using system-aware default configuration")
        except Exception as e:
            logger.warning(f"Failed to generate system-aware defaults: {e}")
    
    # Priority 4: Minimal fallback configuration
    if base_config is None:
        try:
            base_config = _create_minimal_fallback_config('standard')
            config_source = 'minimal_fallback'
            logger.warning("Using minimal fallback configuration")
        except Exception as e:
            logger.error(f"Failed to create minimal fallback: {e}")
    
    # Priority 5: Emergency fallback
    if base_config is None:
        try:
            base_config = _create_minimal_fallback_config('emergency')
            config_source = 'emergency_fallback'
            logger.critical("Using emergency fallback configuration")
        except Exception as e:
            logger.critical(f"Emergency fallback failed: {e}")
            raise RuntimeError("Complete configuration system failure") from e
    
    # Step 3: Merge with loaded configuration if available
    if loaded_config and base_config:
        try:
            # Use deep_update to intelligently merge configurations
            merged_config = deep_update(base_config, loaded_config)
            config_source = f'{config_source}_merged'
            logger.debug("Successfully merged loaded configuration with base configuration")
            base_config = merged_config
        except Exception as e:
            logger.warning(f"Failed to merge configurations: {e}")
            # Continue with base_config only
    
    # Step 6: Add comprehensive runtime metadata
    current_timestamp = datetime.now().isoformat()
    
    # Ensure metadata section exists
    if 'metadata' not in base_config:
        base_config['metadata'] = {}
    
    # Update core metadata
    base_config['metadata'].update({
        'last_accessed': current_timestamp,
        'config_loaded_at': current_timestamp,
        'config_source': config_source,
        'config_version': '2.1',
        'generation_method': 'get_current_config_v2'
    })
    
    # Add runtime information
    if 'runtime' not in base_config:
        base_config['runtime'] = {}
    
    base_config['runtime'].update({
        'config_generated_at': current_timestamp,
        'config_loaded_at': current_timestamp,
        'config_source': config_source,
        'runtime_id': hashlib.md5(current_timestamp.encode()).hexdigest()[:8],
        'process_id': os.getpid(),
        'cache_status': 'refreshed',
        'validation_status': 'passed',
        'system_integration': True
    })
    
    # Step 7: Validate model variants compatibility
    try:
        model_type = base_config.get('model', {}).get('model_type')
        if model_type and MODEL_VARIANTS:
            if model_type not in MODEL_VARIANTS:
                logger.warning(f"Model type '{model_type}' not available in MODEL_VARIANTS")
                # Try to initialize model variants
                try:
                    #initialize_model_variants(silent=True)
                    initialize_model_variants(silent=False)
                    if model_type not in MODEL_VARIANTS:
                        # Fallback to available model type
                        available_types = list(MODEL_VARIANTS.keys())
                        fallback_type = available_types[0] if available_types else 'SimpleAutoencoder'
                        logger.warning(f"Falling back to model type: {fallback_type}")
                        base_config['model']['model_type'] = fallback_type
                except Exception as init_error:
                    logger.warning(f"Model variants initialization failed: {init_error}")
    except Exception as e:
        logger.debug(f"Model variants validation failed: {e}")
    
    # Step 8: Update preset information dynamically in case PRESET_CONFIGS was populated after creation
    try:
        if 'presets' in base_config:
            base_config['presets']['available_presets'] = get_available_presets()
            base_config['presets']['preset_configs'] = get_preset_descriptions()
            base_config['presets']['custom_presets_available'] = get_safe_custom_presets()
    except Exception as e:
        logger.debug(f"Failed to update preset information: {e}")
    
    # Step 10: Cache the final configuration
    _cached_config = base_config
    _config_cache_time = current_time
    
    # Log success with comprehensive statistics
    
    #return deepcopy(base_config)
    return base_config

def list_saved_configs() -> Dict[str, Any]:
    """List all saved configurations with metadata, including named configurations."""
    try:
        # Get regular config files
        config_files = list(CONFIG_DIR.glob("*.json"))
        config_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        # Get named configurations from registry
        named_configs = {}
        registry_path = CONFIG_DIR / "named_configs_registry.json"
        if registry_path.exists():
            try:
                with open(registry_path, 'r', encoding='utf-8') as f:
                    registry = json.load(f)
                named_configs = registry.get("configs", {})
            except Exception as e:
                logger.warning(f"Failed to read named config registry: {e}")
        
        # Build comprehensive result with metadata
        result = {
            "regular_configs": [],
            "named_configs": named_configs,
            "all_configs": {}
        }
        
        # Process regular config files
        for f in config_files:
            if f.stem == "current" or f.stem == "named_configs_registry":
                continue
                
            config_info = {
                "name": f.stem,
                "path": str(f),
                "type": "regular",
                "file_size": f.stat().st_size,
                "modified_time": f.stat().st_mtime,
                "modified": datetime.fromtimestamp(f.stat().st_mtime).isoformat(),
                "metadata": {}
            }
            
            # Try to load metadata from config file
            try:
                config_data = load_config(f)
                if isinstance(config_data, dict):
                    # Extract metadata from the loaded config structure
                    config_info["model_type"] = config_data.get('model', {}).get('model_type', 'N/A')
                    config_info["epochs"] = config_data.get('training', {}).get('epochs', 'N/A')
                    config_info["preset_used"] = config_data.get('presets', {}).get('current_preset', 'none')
                    
                    # Try to get additional metadata if available
                    try:
                        with open(f, 'r') as config_file:
                            full_config_data = json.load(config_file)
                        if isinstance(full_config_data, dict) and 'metadata' in full_config_data:
                            config_info["metadata"] = full_config_data.get('metadata', {})
                            if 'created' in config_info["metadata"]:
                                config_info["created"] = config_info["metadata"]["created"]
                    except Exception:
                        pass
            except Exception as e:
                logger.debug(f"Could not read metadata for {f.stem}: {e}")
                config_info["model_type"] = 'N/A'
                config_info["epochs"] = 'N/A'
                config_info["preset_used"] = 'none'
            
            result["regular_configs"].append(config_info)
            result["all_configs"][f.stem] = config_info
        
        # Add named configs to all_configs with enhanced metadata
        for name, named_info in named_configs.items():
            if name not in result["all_configs"]:
                enhanced_named_info = {
                    "name": name,
                    "path": named_info.get("path", ""),
                    "type": "named",
                    "metadata": named_info.get("config", {}),
                    "created": named_info.get("created"),
                    "modified": named_info.get("modified"),
                    "model_type": named_info.get("model_type", 'N/A'),
                    "preset_used": named_info.get("preset_used", 'none')
                }
                
                # Add file size and modification time if path exists
                try:
                    named_path = Path(named_info.get("path", ""))
                    if named_path.exists():
                        stat_info = named_path.stat()
                        enhanced_named_info["file_size"] = stat_info.st_size
                        enhanced_named_info["modified_time"] = stat_info.st_mtime
                        # Use file modification time if not in registry
                        if not enhanced_named_info.get("modified"):
                            enhanced_named_info["modified"] = datetime.fromtimestamp(stat_info.st_mtime).isoformat()
                except Exception:
                    enhanced_named_info["file_size"] = 0
                    enhanced_named_info["modified_time"] = 0
                
                result["all_configs"][name] = enhanced_named_info
        
        return result
        
    except Exception as e:
        logger.error(f"Error listing saved configs: {e}")
        return {"regular_configs": [], "named_configs": {}, "all_configs": {}}

def load_saved_config_interactive():
    """Interactive loading of saved configurations with support for named configs."""
    try:
        # clear screen and show banner
        console.clear()
        show_banner()
        
        saved_configs_info = list_saved_configs()
        regular_configs = saved_configs_info["regular_configs"]
        named_configs = saved_configs_info["named_configs"]
        all_configs = saved_configs_info["all_configs"]
        
        if not regular_configs and not named_configs:
            no_config_panel = Panel.fit(
                "[bold red]No saved configurations found[/bold red]\n"
                "Use 'save_config()' to save your current configuration first.",
                title="[bold yellow]WARNING[/bold yellow]",
                border_style="yellow",
                padding=(1, 2)
            )
            console.print(no_config_panel)
            return
        
        console.print("\n[bold yellow]Available Saved Configurations[/bold yellow]\n")
        
        # Create main table for saved configurations
        config_table = Table(
            box=box.ROUNDED,
            header_style="bold bright_magenta",
            border_style="bright_magenta",
            show_header=True,
            show_lines=True,
            width=min(100, console.width - 4)
        )
        
        # Define columns
        config_table.add_column("#", style="bold cyan", width=3, justify="center")
        config_table.add_column("Config Name", style="bold green", width=25)
        config_table.add_column("Type", style="bold yellow", width=10)
        config_table.add_column("File Info", style="bold blue", width=20)
        config_table.add_column("Config Details", style="bold", width=40)
        
        config_names = list(all_configs.keys())
        
        for i, config_name in enumerate(config_names, 1):
            config_info = all_configs[config_name]
            file_info = ""
            config_details = ""
            config_type = config_info.get("type", "regular")
            
            try:
                if config_type == "regular":
                    config_path = Path(config_info["path"])
                    if config_path.exists():
                        file_size = config_path.stat().st_size
                        file_time = datetime.fromtimestamp(config_path.stat().st_mtime)
                        file_info = f"{file_size/1024:.1f}KB\n{file_time.strftime('%Y-%m-%d')}"
                    
                    # Extract key details from metadata
                    metadata = config_info.get("metadata", {})
                    model_type = config_info.get("model_type", 'N/A')
                    epochs = config_info.get("epochs", 'N/A')
                    preset = metadata.get('preset_used', 'None')
                    
                    config_details = f"Model: {model_type}\nEpochs: {epochs}, Preset: {preset}"
                
                else:  # named config
                    named_info = named_configs[config_name]
                    config_path = Path(named_info.get("path", ""))
                    
                    if config_path.exists():
                        file_size = config_path.stat().st_size
                        file_time = datetime.fromtimestamp(config_path.stat().st_mtime)
                        file_info = f"{file_size/1024:.1f}KB\n{file_time.strftime('%Y-%m-%d')}"
                    
                    config_data = named_info.get("config", {})
                    model_type = config_data.get('model_type', 'N/A')
                    preset = config_data.get('preset_used', 'None')
                    
                    config_details = f"Model: {model_type}\nPreset: {preset}\n[Named Config]"
                
            except Exception as e:
                config_details = f"[bold red]Error reading: {str(e)[:30]}...[/bold red]"
            
            config_table.add_row(
                str(i),
                config_name,
                "[bold green]Named[/bold green]" if config_type == "named" else "Regular",
                file_info,
                config_details
            )
        
        console.print(config_table)
        
        # Selection options
        max_choice = len(config_names)
        cancel_message = f"Select [bold red]0[/bold red] to cancel selection"
        selection_message = f"Select Configuration between (1-{max_choice})"
        
        # Get user input
        choice = console.input(f"\n[bold yellow]{selection_message} or {cancel_message}: [/bold yellow]").strip()
        
        if choice == "0":
            console.print("[bold red]Loading cancelled[/bold red]")
            return
        
        if choice.isdigit() and 1 <= int(choice) <= max_choice:
            selected_name = config_names[int(choice)-1]
            config_info = all_configs[selected_name]
            
            # Show confirmation panel with file details
            try:
                if config_info["type"] == "regular":
                    config_path = Path(config_info["path"])
                    file_size = config_path.stat().st_size
                    file_time = datetime.fromtimestamp(config_path.stat().st_mtime)
                    file_info = f"Size: {file_size/1024:.1f}KB, Modified: {file_time.strftime('%Y-%m-%d %H:%M')}"
                    
                    # Load config for preview using the unified load_config function
                    config_data = load_config(config_path)
                    
                    metadata = config_info.get("metadata", {})
                    model_type = config_data.get('model', {}).get('model_type', 'N/A')
                    epochs = config_data.get('training', {}).get('epochs', 'N/A')
                    batch_size = config_data.get('training', {}).get('batch_size', 'N/A')
                    preset = metadata.get('preset_used', 'None')
                    
                else:  # named config
                    named_info = named_configs[selected_name]
                    config_path = Path(named_info.get("path", ""))
                    
                    if config_path.exists():
                        file_size = config_path.stat().st_size
                        file_time = datetime.fromtimestamp(config_path.stat().st_mtime)
                        file_info = f"Size: {file_size/1024:.1f}KB, Modified: {file_time.strftime('%Y-%m-%d %H:%M')}"
                    
                    config_data = named_info.get("config", {})
                    model_type = config_data.get('model_type', 'N/A')
                    preset = config_data.get('preset_used', 'None')
                    epochs = 'N/A'
                    batch_size = 'N/A'
                    metadata = {"created": named_info.get("created", "Unknown")}
                
                confirm_panel = Panel.fit(
                    f"[bold]Configuration:[/bold] [green]{selected_name}[/green]\n"
                    f"[bold]Type:[/bold] {'Named' if config_info['type'] == 'named' else 'Regular'}\n"
                    f"[bold]File:[/bold] {file_info}\n"
                    f"[bold]Model:[/bold] {model_type}\n"
                    f"[bold]Training:[/bold] {epochs} epochs, Batch: {batch_size}\n"
                    f"[bold]Preset:[/bold] {preset}\n"
                    f"[bold]Created:[/bold] {metadata.get('created', 'Unknown')}",
                    title="[bold]CONFIGURATION PREVIEW[/bold]",
                    border_style="green",
                    padding=(1, 2)
                )
                console.print(confirm_panel)
                
            except Exception as e:
                error_panel = Panel.fit(
                    f"[bold red]Error reading configuration: {e}[/bold red]",
                    title="[bold]ERROR[/bold]",
                    border_style="red",
                    padding=(1, 2)
                )
                console.print(error_panel)
                return
            
            # Confirmation prompt
            confirm = console.input("\n[bold yellow]Load this configuration? (Y/n): [/bold yellow]").lower().strip()
            
            if confirm in ('', 'y', 'yes'):
                try:
                    # Use the unified load_config function
                    config = load_config(selected_name)
                    update_global_config(config)
                    
                    success_panel = Panel.fit(
                        f"[bold green]Successfully loaded configuration: {selected_name}[/bold green]\n"
                        f"All settings have been updated from the saved configuration.",
                        title="[bold]SUCCESS[/bold]",
                        border_style="green",
                        padding=(1, 2)
                    )
                    console.print(success_panel)
                    
                except Exception as e:
                    error_panel = Panel.fit(
                        f"[bold red]Failed to load configuration: {e}[/bold red]",
                        title="[bold]ERROR[/bold]",
                        border_style="red",
                        padding=(1, 2)
                    )
                    console.print(error_panel)
            else:
                console.print("[bold red]Loading cancelled[/bold red]")
        else:
            error_panel = Panel.fit(
                f"[bold yellow]Invalid selection: {choice}[/bold yellow]\n"
                f"{selection_message} or {cancel_message}:",
                title="[bold]WARNING[/bold]",
                border_style="yellow",
                padding=(1, 2)
            )
            console.print(error_panel)
            
    except Exception as e:
        error_panel = Panel.fit(
            f"[bold red]Error in configuration loading: {e}[/bold red]",
            title="[bold]FAILED[/bold]",
            border_style="red",
            padding=(1, 2)
        )
        console.print(error_panel)
        logger.error(f"Configuration loading failed: {e}")

# Helper functions for preset and configuration management
def migrate_config(legacy_config: Dict, new_template: Dict = None) -> Dict:
    """Migrate an older configuration to the current version using enhanced preset matching.
    
    Args:
        legacy_config: The old configuration dictionary to migrate from
        new_template: Optional template to use as base (defaults to best matching preset)
        
    Returns:
        New configuration dictionary with migrated values
        
    Raises:
        ValueError: If legacy_config is invalid
    """
    if not isinstance(legacy_config, dict):
        raise ValueError("legacy_config must be a dictionary")
    
    logger.info("Migrating configuration to current format")
    
    # Determine the best template to use
    if new_template is None:
        # Try to find the best matching preset
        try:
            if PRESET_CONFIGS:
                # Use convert_legacy_config's scoring system to find best match
                def score_preset_simple(preset_cfg: Dict[str, Any]) -> float:
                    """Simplified scoring for migration."""
                    score = 0.0
                    total_checks = 0
                    
                    # Check training parameters
                    for param in ['batch_size', 'learning_rate', 'epochs']:
                        if param in legacy_config and param in preset_cfg.get('training', {}):
                            legacy_val = legacy_config[param]
                            preset_val = preset_cfg['training'][param]
                            if isinstance(legacy_val, (int, float)) and isinstance(preset_val, (int, float)):
                                similarity = 1 - min(abs(legacy_val - preset_val) / max(abs(legacy_val), abs(preset_val), 1), 1)
                                score += similarity
                            elif legacy_val == preset_val:
                                score += 1
                            total_checks += 1
                    
                    # Check model parameters
                    for param in ['model_type', 'encoding_dim', 'activation']:
                        if param in legacy_config and param in preset_cfg.get('model', {}):
                            if legacy_config[param] == preset_cfg['model'][param]:
                                score += 1
                            total_checks += 1
                    
                    return score / max(total_checks, 1)
                
                best_preset = None
                best_score = 0
                
                for preset_name, preset_cfg in PRESET_CONFIGS.items():
                    score = score_preset_simple(preset_cfg)
                    if score > best_score:
                        best_score = score
                        best_preset = preset_name
                
                if best_preset and best_score > 0.3:
                    new_template = PRESET_CONFIGS[best_preset]
                    logger.info(f"Using {best_preset} preset as migration template (score: {best_score:.2f})")
                else:
                    new_template = DEFAULT_PRESET
                    logger.info("Using DEFAULT_PRESET as migration template")
            else:
                new_template = DEFAULT_PRESET
                logger.info("PRESET_CONFIGS not available, using DEFAULT_PRESET")
        except Exception as e:
            logger.warning(f"Error selecting migration template: {e}")
            new_template = DEFAULT_PRESET
    
    # Create base configuration from template
    migrated_config = deepcopy(new_template)
    
    # Enhanced key mapping with better fallback handling
    key_mapping = {
        # Direct mappings for training parameters
        'batch_size': ('training', 'batch_size'),
        'epochs': ('training', 'epochs'),
        'learning_rate': ('training', 'learning_rate'),
        'patience': ('training', 'patience'),
        'weight_decay': ('training', 'weight_decay'),
        'gradient_clip': ('training', 'gradient_clip'),
        'gradient_accumulation_steps': ('training', 'gradient_accumulation_steps'),
        'mixed_precision': ('training', 'mixed_precision'),
        'num_workers': ('training', 'num_workers'),
        'optimizer': ('training', 'optimizer'),
        'scheduler': ('training', 'scheduler'),
        
        # Direct mappings for model parameters
        'model_type': ('model', 'model_type'),
        'encoding_dim': ('model', 'encoding_dim'),
        'hidden_dims': ('model', 'hidden_dims'),
        'dropout_rates': ('model', 'dropout_rates'),
        'activation': ('model', 'activation'),
        'activation_param': ('model', 'activation_param'),
        'normalization': ('model', 'normalization'),
        'use_batch_norm': ('model', 'use_batch_norm'),
        'use_layer_norm': ('model', 'use_layer_norm'),
        'diversity_factor': ('model', 'diversity_factor'),
        'min_features': ('model', 'min_features'),
        'skip_connection': ('model', 'skip_connection'),
        'residual_blocks': ('model', 'residual_blocks'),
        'num_models': ('model', 'num_models'),
        
        # Direct mappings for security parameters
        'percentile': ('security', 'percentile'),
        'attack_threshold': ('security', 'attack_threshold'),
        'false_negative_cost': ('security', 'false_negative_cost'),
        'enable_security_metrics': ('security', 'enable_security_metrics'),
        'anomaly_threshold_strategy': ('security', 'anomaly_threshold_strategy'),
        'early_warning_threshold': ('security', 'early_warning_threshold'),
        
        # Direct mappings for data parameters
        'normal_samples': ('data', 'normal_samples'),
        'attack_samples': ('data', 'attack_samples'),
        'features': ('data', 'features'),
        # Avoid conflict with model normalization
        'data_normalization': ('data', 'normalization'),
        'anomaly_factor': ('data', 'anomaly_factor'),
        'random_state': ('data', 'random_state'),
        'validation_split': ('data', 'validation_split'),
        'test_split': ('data', 'test_split'),
        
        # Nested parameters
        'synthetic_generation.cluster_variance': ('data', 'synthetic_generation', 'cluster_variance'),
        'synthetic_generation.anomaly_sparsity': ('data', 'synthetic_generation', 'anomaly_sparsity'),
        
        # Direct mappings for monitoring
        'metrics_frequency': ('monitoring', 'metrics_frequency'),
        'checkpoint_frequency': ('monitoring', 'checkpoint_frequency'),
        'tensorboard_logging': ('monitoring', 'tensorboard_logging'),
        'console_logging_level': ('monitoring', 'console_logging_level'),
        
        # Legacy parameter mappings with transformations
        # Common legacy name
        'hidden_layer_sizes': ('model', 'hidden_dims'),
        # Single rate to list
        'dropout_rate': ('model', 'dropout_rates'),
        # Alternative name
        'n_epochs': ('training', 'epochs'),
        # Short form
        'lr': ('training', 'learning_rate'),
        # Boolean to patience value
        'early_stopping': ('training', 'patience'),
    }
    
    # Track migration statistics
    migration_stats = {
        'mapped_keys': 0,
        'skipped_keys': 0,
        'transformed_keys': 0,
        'invalid_values': 0,
        'auto_fixed': 0
    }
    
    # Apply mapped values with enhanced handling
    def set_nested_value(config_dict: Dict, path: tuple, value: Any) -> bool:
        """Set a nested value in the configuration."""
        try:
            target = config_dict
            for key in path[:-1]:
                if key not in target:
                    target[key] = {}
                target = target[key]
            target[path[-1]] = value
            return True
        except Exception as e:
            logger.warning(f"Failed to set nested value at {path}: {e}")
            return False
    
    def get_nested_value(config_dict: Dict, keys: List[str]) -> Any:
        """Get a nested value from legacy config."""
        try:
            value = config_dict
            for key in keys:
                value = value[key]
            return value
        except (KeyError, TypeError):
            return None
    
    # Process all legacy configuration keys
    for legacy_key, new_path in key_mapping.items():
        # Handle dot notation for nested legacy keys
        legacy_keys = legacy_key.split('.')
        legacy_value = get_nested_value(legacy_config, legacy_keys)
        
        if legacy_value is None:
            migration_stats['skipped_keys'] += 1
            continue
        
        try:
            # Handle special transformations
            if legacy_key == 'dropout_rate' and not isinstance(legacy_value, list):
                # Convert single dropout rate to list
                legacy_value = [float(legacy_value)]
                migration_stats['transformed_keys'] += 1
                logger.info(f"Converted single dropout_rate to list: {legacy_value}")
            
            elif legacy_key == 'early_stopping' and isinstance(legacy_value, bool):
                # Convert boolean early stopping to patience value
                legacy_value = 10 if legacy_value else 0
                migration_stats['transformed_keys'] += 1
                logger.info(f"Converted early_stopping boolean to patience: {legacy_value}")
            
            elif legacy_key == 'hidden_layer_sizes' and not isinstance(legacy_value, list):
                # Convert single size to list
                legacy_value = [int(legacy_value)]
                migration_stats['transformed_keys'] += 1
                logger.info(f"Converted single hidden_layer_size to list: {legacy_value}")
            
            # Validate the value before setting
            if isinstance(new_path, tuple):
                # Check if the value is reasonable for the parameter
                param_name = new_path[-1]
                
                if param_name in ['batch_size', 'epochs'] and (not isinstance(legacy_value, int) or legacy_value < 1):
                    logger.warning(f"Invalid {param_name} value {legacy_value}, using template default")
                    migration_stats['invalid_values'] += 1
                    continue
                
                elif param_name == 'learning_rate' and (not isinstance(legacy_value, (int, float)) or legacy_value <= 0):
                    logger.warning(f"Invalid learning_rate value {legacy_value}, using template default")
                    migration_stats['invalid_values'] += 1
                    continue
                
                elif param_name in ['hidden_dims', 'dropout_rates'] and not isinstance(legacy_value, list):
                    logger.warning(f"Invalid {param_name} value {legacy_value}, using template default")
                    migration_stats['invalid_values'] += 1
                    continue
                
                # Set the value
                if set_nested_value(migrated_config, new_path, legacy_value):
                    migration_stats['mapped_keys'] += 1
                else:
                    migration_stats['invalid_values'] += 1
            
        except Exception as e:
            logger.warning(f"Could not migrate {legacy_key}: {str(e)}")
            migration_stats['invalid_values'] += 1
            continue
    
    # Handle any remaining unmapped keys
    unmapped_keys = set(legacy_config.keys()) - set(k.split('.')[0] for k in key_mapping.keys())
    if unmapped_keys:
        logger.info(f"Unmapped legacy keys found: {list(unmapped_keys)}")
        
        # Try to map some common patterns
        for key in unmapped_keys:
            value = legacy_config[key]
            
            # Try common training parameter patterns
            if 'batch' in key.lower():
                if isinstance(value, int) and value > 0:
                    migrated_config.setdefault('training', {})['batch_size'] = value
                    migration_stats['auto_fixed'] += 1
                    logger.info(f"Auto-mapped {key} -> training.batch_size")
            
            elif 'epoch' in key.lower():
                if isinstance(value, int) and value > 0:
                    migrated_config.setdefault('training', {})['epochs'] = value
                    migration_stats['auto_fixed'] += 1
                    logger.info(f"Auto-mapped {key} -> training.epochs")
            
            elif 'learn' in key.lower() or 'lr' in key.lower():
                if isinstance(value, (int, float)) and value > 0:
                    migrated_config.setdefault('training', {})['learning_rate'] = value
                    migration_stats['auto_fixed'] += 1
                    logger.info(f"Auto-mapped {key} -> training.learning_rate")
    
    # Add comprehensive migration metadata
    migrated_config['metadata']['migration'] = {
        'source_version': legacy_config.get('version', '1.x'),
        'target_version': migrated_config.get('metadata', {}).get('version', '2.1'),
        'timestamp': datetime.now().isoformat(),
        'stats': migration_stats,
        'template_used': new_template.get('metadata', {}).get('description', 'Unknown'),
        'compatibility_checked': True,
        'legacy_keys_count': len(legacy_config),
        'success_rate': migration_stats['mapped_keys'] / max(len(legacy_config), 1)
    }
    
    # Validate and auto-fix the migrated configuration
    try:
        validate_config(migrated_config)
        logger.info("Migrated configuration passed validation")
    except ValueError as e:
        logger.warning(f"Migrated config validation issues: {e}")
        # The validation function should have auto-fixed issues
        migrated_config['metadata']['migration']['validation_fixes_applied'] = True
    
    # Log migration summary
    total_keys = len(legacy_config)
    success_rate = (migration_stats['mapped_keys'] + migration_stats['auto_fixed']) / max(total_keys, 1) * 100
    
    logger.info(f"Migration completed:")
    logger.info(f"  - Total legacy keys: {total_keys}")
    logger.info(f"  - Successfully mapped: {migration_stats['mapped_keys']}")
    logger.info(f"  - Auto-fixed: {migration_stats['auto_fixed']}")
    logger.info(f"  - Transformed: {migration_stats['transformed_keys']}")
    logger.info(f"  - Skipped: {migration_stats['skipped_keys']}")
    logger.info(f"  - Invalid: {migration_stats['invalid_values']}")
    logger.info(f"  - Success rate: {success_rate:.1f}%")
    
    return migrated_config

def convert_legacy_config(
    legacy_config: Dict[str, Any],
    config: Optional[Dict[str, Any]] = None,
    preset_similarity_threshold: Optional[float] = None
) -> Dict[str, Any]:
    """Convert legacy configuration to current format using intelligent preset matching.
    
    Args:
        legacy_config: The old configuration dictionary to convert
        config: Current configuration dictionary (for migration settings)
        preset_similarity_threshold: Override for similarity threshold (0-1)
        
    Returns:
        New configuration dictionary in current format with metadata
        
    Raises:
        ValueError: If legacy_config is invalid or conversion fails
    """
    # Initial Validation
    if not isinstance(legacy_config, dict):
        raise ValueError("legacy_config must be a dictionary")
    
    if not legacy_config:
        raise ValueError("legacy_config cannot be empty")
    
    logger.info("Initiating legacy configuration conversion with preset matching")
    
    # Step 1: Threshold Determination with Enhanced Logic
    def determine_threshold() -> float:
        """Determine the appropriate similarity threshold with enhanced fallbacks."""
        # Argument precedence
        if preset_similarity_threshold is not None:
            if 0 < preset_similarity_threshold <= 1:
                logger.info(f"Using provided threshold: {preset_similarity_threshold:.3f}")
                return preset_similarity_threshold
            logger.warning(f"Invalid threshold {preset_similarity_threshold}, using fallbacks")
        
        # Config file precedence
        if config and config.get("migration", {}).get("preset_similarity_threshold"):
            try:
                threshold = float(config["migration"]["preset_similarity_threshold"])
                if 0 < threshold <= 1:
                    logger.info(f"Using config threshold: {threshold:.3f}")
                    return threshold
                logger.warning(f"Invalid config threshold {threshold}, using fallbacks")
            except (ValueError, TypeError) as e:
                logger.warning(f"Error parsing config threshold: {e}")
        
        # Adaptive threshold based on legacy config complexity
        complexity_score = 0
        # Number of keys
        complexity_score += len(legacy_config) * 0.1
        # Nested dicts
        complexity_score += sum(1 for v in legacy_config.values() if isinstance(v, dict)) * 0.2
        # Lists
        complexity_score += sum(1 for v in legacy_config.values() if isinstance(v, list)) * 0.1
        
        # Adjust threshold based on complexity
        if complexity_score > 10:
            # More lenient for complex configs
            adaptive_threshold = 0.15
        elif complexity_score > 5:
            # Moderate
            adaptive_threshold = 0.10
        else:
            # Strict for simple configs
            adaptive_threshold = 0.05
        
        logger.info(f"Using adaptive threshold based on complexity ({complexity_score:.1f}): {adaptive_threshold:.3f}")
        return adaptive_threshold
    
    similarity_threshold = determine_threshold()
    
    # Step 2: Enhanced Preset Scoring System
    class AdvancedPresetScorer:
        """Enhanced scoring engine with machine learning-inspired features."""
        
        def __init__(self, legacy_config: Dict[str, Any]):
            self.legacy = legacy_config
            self.weights = {
                'training': 0.35,
                'model': 0.40,
                'security': 0.15,
                'data': 0.10
            }
            
            # Feature importance weights based on common usage patterns
            self.feature_weights = {
                # Most important
                'model_type': 0.30,
                'encoding_dim': 0.20,
                'batch_size': 0.15,
                'learning_rate': 0.15,
                'hidden_dims': 0.10,
                'activation': 0.10,
                'percentile': 0.08,
                'features': 0.05,
                'normalization': 0.05,
                'use_batch_norm': 0.05,
                'num_models': 0.05,
                'diversity_factor': 0.03,
                'optimizer': 0.03,
                'scheduler': 0.02
            }
            
            # Value ranges for normalization
            self.value_ranges = {
                'batch_size': (8, 256),
                'learning_rate': (1e-5, 1e-1),
                'encoding_dim': (4, 24),
                'percentile': (85, 99),
                'features': (10, 50),
                'normal_samples': (100, 20000),
                'attack_samples': (50, 5000),
                'epochs': (5, 300),
                'patience': (3, 30),
                'weight_decay': (0, 1e-2),
                'gradient_clip': (0.1, 5.0),
                'diversity_factor': (0, 1),
                'anomaly_factor': (1, 3)
            }
            
            # Pattern matching for string values
            self.string_patterns = {
                'activation': {
                    'relu': ['relu', 'ReLU'],
                    'leaky_relu': ['leaky_relu', 'leakyrelu', 'LeakyReLU'],
                    'gelu': ['gelu', 'GELU']
                },
                'optimizer': {
                    'Adam': ['adam', 'Adam'],
                    'AdamW': ['adamw', 'AdamW'],
                    'SGD': ['sgd', 'SGD']
                },
                'normalization': {
                    'batch': ['batch', 'batch_norm', 'BatchNorm'],
                    'layer': ['layer', 'layer_norm', 'LayerNorm'],
                    None: ['none', 'None', None]
                }
            }
        
        def normalize_value(self, key: str, value: Any) -> float:
            """Enhanced value normalization with outlier handling."""
            if key not in self.value_ranges:
                # Neutral value for unknown parameters
                return 0.5
            
            min_val, max_val = self.value_ranges[key]
            
            if isinstance(value, (int, float)):
                # Handle outliers by capping
                capped_value = max(min_val, min(value, max_val))
                normalized = (capped_value - min_val) / (max_val - min_val)
                
                # Apply log scaling for learning rate and weight decay
                if key in ['learning_rate', 'weight_decay'] and value > 0:
                    log_normalized = (np.log10(value) - np.log10(min_val)) / (np.log10(max_val) - np.log10(min_val))
                    normalized = max(0, min(1, log_normalized))
                
                return normalized
            
            return 0.5
        
        def compare_numeric_enhanced(self, key: str, preset_val: Any) -> float:
            """Enhanced numeric comparison with weighted similarity."""
            if key not in self.legacy:
                # Penalty for missing values
                return 0.3
            
            legacy_val = self.legacy[key]
            if not isinstance(legacy_val, (int, float)) or not isinstance(preset_val, (int, float)):
                return 0
            
            # Normalize both values
            norm_legacy = self.normalize_value(key, legacy_val)
            norm_preset = self.normalize_value(key, preset_val)
            
            # Calculate similarity with sigmoid-like function for smoother scoring
            diff = abs(norm_preset - norm_legacy)
            # Sigmoid-like curve
            similarity = 1 / (1 + diff * 2)
            
            # Apply importance weighting
            weight = self.feature_weights.get(key, 0.1)
            return similarity * weight
        
        def compare_string_enhanced(self, key: str, preset_val: Any) -> float:
            """Enhanced string comparison with pattern matching."""
            if key not in self.legacy:
                return 0.3
            
            legacy_val = self.legacy[key]
            
            # Direct match
            if legacy_val == preset_val:
                return self.feature_weights.get(key, 0.1)
            
            # Pattern matching
            if key in self.string_patterns:
                for standard_val, patterns in self.string_patterns[key].items():
                    if legacy_val in patterns and preset_val == standard_val:
                        # Slight penalty for pattern match
                        return self.feature_weights.get(key, 0.1) * 0.9
                    elif preset_val in patterns and legacy_val == standard_val:
                        return self.feature_weights.get(key, 0.1) * 0.9
            
            return 0
        
        def compare_list_enhanced(self, key: str, preset_val: Any) -> float:
            """Enhanced list comparison with element-wise similarity."""
            if key not in self.legacy:
                return 0.3
            
            legacy_list = self.legacy[key] if isinstance(self.legacy[key], list) else []
            preset_list = preset_val if isinstance(preset_val, list) else []
            
            if not legacy_list and not preset_list:
                return self.feature_weights.get(key, 0.1)
            
            if not legacy_list or not preset_list:
                # Some penalty for missing data
                return 0.1
            
            # Length similarity
            max_len = max(len(legacy_list), len(preset_list))
            min_len = min(len(legacy_list), len(preset_list))
            length_similarity = min_len / max_len
            
            # Element-wise similarity
            element_scores = []
            for i in range(min_len):
                if isinstance(legacy_list[i], (int, float)) and isinstance(preset_list[i], (int, float)):
                    # Numeric similarity
                    diff = abs(legacy_list[i] - preset_list[i]) / max(abs(legacy_list[i]), abs(preset_list[i]), 1e-6)
                    element_scores.append(max(0, 1 - diff))
                else:
                    # Exact match
                    element_scores.append(1.0 if legacy_list[i] == preset_list[i] else 0.0)
            
            avg_element_similarity = sum(element_scores) / len(element_scores) if element_scores else 0
            
            # Combine similarities
            total_similarity = (length_similarity * 0.3 + avg_element_similarity * 0.7)
            return total_similarity * self.feature_weights.get(key, 0.1)
        
        def score_preset_comprehensive(self, preset_name: str, preset_cfg: Dict[str, Any]) -> Dict[str, Any]:
            """Comprehensive preset scoring with detailed breakdown."""
            section_scores = defaultdict(float)
            parameter_scores = {}
            
            # Training parameters
            training_params = [
                ('batch_size', self.compare_numeric_enhanced),
                ('learning_rate', self.compare_numeric_enhanced),
                ('epochs', self.compare_numeric_enhanced),
                ('patience', self.compare_numeric_enhanced),
                ('weight_decay', self.compare_numeric_enhanced),
                ('gradient_clip', self.compare_numeric_enhanced),
                ('mixed_precision', self.compare_string_enhanced),
                ('optimizer', self.compare_string_enhanced),
                ('scheduler', self.compare_string_enhanced),
                ('num_workers', self.compare_numeric_enhanced)
            ]
            
            for param, compare_fn in training_params:
                if param in preset_cfg.get('training', {}):
                    score = compare_fn(param, preset_cfg['training'][param])
                    section_scores['training'] += score
                    parameter_scores[f'training.{param}'] = score
            
            # Model parameters
            model_params = [
                ('model_type', self.compare_string_enhanced),
                ('encoding_dim', self.compare_numeric_enhanced),
                ('hidden_dims', self.compare_list_enhanced),
                ('dropout_rates', self.compare_list_enhanced),
                ('activation', self.compare_string_enhanced),
                ('activation_param', self.compare_numeric_enhanced),
                ('normalization', self.compare_string_enhanced),
                ('use_batch_norm', self.compare_string_enhanced),
                ('use_layer_norm', self.compare_string_enhanced),
                ('skip_connection', self.compare_string_enhanced),
                ('residual_blocks', self.compare_string_enhanced),
                ('num_models', self.compare_numeric_enhanced),
                ('diversity_factor', self.compare_numeric_enhanced)
            ]
            
            for param, compare_fn in model_params:
                if param in preset_cfg.get('model', {}):
                    score = compare_fn(param, preset_cfg['model'][param])
                    section_scores['model'] += score
                    parameter_scores[f'model.{param}'] = score
            
            # Security parameters
            security_params = [
                ('percentile', self.compare_numeric_enhanced),
                ('attack_threshold', self.compare_numeric_enhanced),
                ('false_negative_cost', self.compare_numeric_enhanced),
                ('enable_security_metrics', self.compare_string_enhanced),
                ('anomaly_threshold_strategy', self.compare_string_enhanced)
            ]
            
            for param, compare_fn in security_params:
                if param in preset_cfg.get('security', {}):
                    score = compare_fn(param, preset_cfg['security'][param])
                    section_scores['security'] += score
                    parameter_scores[f'security.{param}'] = score
            
            # Data parameters
            data_params = [
                ('features', self.compare_numeric_enhanced),
                ('normalization', self.compare_string_enhanced),
                ('anomaly_factor', self.compare_numeric_enhanced),
                ('normal_samples', self.compare_numeric_enhanced),
                ('attack_samples', self.compare_numeric_enhanced),
                ('validation_split', self.compare_numeric_enhanced),
                ('test_split', self.compare_numeric_enhanced)
            ]
            
            for param, compare_fn in data_params:
                if param in preset_cfg.get('data', {}):
                    score = compare_fn(param, preset_cfg['data'][param])
                    section_scores['data'] += score
                    parameter_scores[f'data.{param}'] = score
            
            # Apply section weights
            weighted_sections = {}
            for section in section_scores:
                weighted_sections[section] = section_scores[section] * self.weights.get(section, 0.1)
            
            total_score = sum(weighted_sections.values())
            
            return {
                'name': preset_name,
                'total_score': total_score,
                'section_scores': dict(section_scores),
                'weighted_section_scores': weighted_sections,
                'parameter_scores': parameter_scores,
                'config': preset_cfg,
                'compatibility': preset_cfg.get('metadata', {}).get('compatibility', [])
            }

    # Step 3: Score All Available Presets
    try:
        if not PRESET_CONFIGS:
            logger.warning("PRESET_CONFIGS not available, using basic migration")
            return migrate_config(legacy_config)
        
        scorer = AdvancedPresetScorer(legacy_config)
        preset_scores = []
        
        for name, cfg in PRESET_CONFIGS.items():
            try:
                score_result = scorer.score_preset_comprehensive(name, cfg)
                preset_scores.append(score_result)
            except Exception as e:
                logger.warning(f"Error scoring preset {name}: {e}")
                continue
        
        if not preset_scores:
            logger.warning("No presets could be scored, using basic migration")
            return migrate_config(legacy_config)
        
        preset_scores.sort(key=lambda x: x['total_score'], reverse=True)
        
    except Exception as e:
        logger.error(f"Error during preset scoring: {e}")
        return migrate_config(legacy_config)
    
    # Step 4: Enhanced Results Analysis
    def analyze_results_enhanced(scores: List[Dict[str, Any]]) -> Tuple[List[str], Dict[str, Any], Dict[str, Any]]:
        """Enhanced analysis with confidence scoring."""
        if not scores:
            return [], {}, {}
        
        best_score = scores[0]['total_score']
        close_presets = []
        analysis = {
            'best_score': best_score,
            'score_distribution': [s['total_score'] for s in scores[:5]],
            'confidence': 'high' if best_score > 0.7 else 'medium' if best_score > 0.4 else 'low'
        }
        
        # Dynamic threshold adjustment based on score distribution
        effective_threshold = similarity_threshold
        
        if len(scores) > 1:
            second_best = scores[1]['total_score']
            score_gap = best_score - second_best
            
            if score_gap < 0.1 and best_score > 0.3:
                effective_threshold = max(similarity_threshold, 0.15)
                logger.info(f"Close scores detected, adjusting threshold to {effective_threshold:.3f}")
            elif best_score < 0.3:
                effective_threshold = min(similarity_threshold * 2, 0.2)
                logger.info(f"Low best score, relaxing threshold to {effective_threshold:.3f}")
        
        # Find close matches
        for score in scores:
            if (best_score - score['total_score']) <= effective_threshold:
                close_presets.append(score['name'])
            else:
                break
        
        return close_presets, scores[0], analysis
    
    close_presets, best_preset, analysis = analyze_results_enhanced(preset_scores)
    
    # Step 5: Enhanced Reporting
    def generate_enhanced_report(scores: List[Dict[str, Any]], close_presets: List[str], analysis: Dict[str, Any]) -> None:
        """Generate comprehensive conversion report."""
        logger.info("\n" + "="*80)
        logger.info("LEGACY CONFIGURATION CONVERSION REPORT")
        logger.info("="*80)
        
        logger.info(f"Analysis Confidence: {analysis['confidence'].upper()}")
        logger.info(f"Best Score: {analysis['best_score']:.3f}")
        logger.info(f"Threshold Used: {similarity_threshold:.3f}")
        
        logger.info("\nTop 10 Preset Matches:")
        logger.info(f"{'Rank':<5} {'Preset':<20} {'Total':<8} {'Training':<9} {'Model':<8} {'Security':<9} {'Data':<8}")
        logger.info("-" * 80)
        
        for i, score in enumerate(scores[:10], 1):
            logger.info(
                f"{i:<5} "
                f"{score['name']:<20} "
                f"{score['total_score']:.3f}    "
                f"{score['section_scores'].get('training', 0):.3f}     "
                f"{score['section_scores'].get('model', 0):.3f}    "
                f"{score['section_scores'].get('security', 0):.3f}     "
                f"{score['section_scores'].get('data', 0):.3f}"
            )
        
        if close_presets:
            logger.info(f"\nClose Matches (within threshold {similarity_threshold:.3f}):")
            for i, preset in enumerate(close_presets, 1):
                preset_score = next(s for s in scores if s['name'] == preset)
                logger.info(f"  {i}. {preset:<18} (score={preset_score['total_score']:.3f})")
            
            logger.info(f"\nRecommended: {close_presets[0]}")
        else:
            logger.info("\nNo close matches found - will use best available or default")
        
        # Show parameter-level analysis for best match
        if scores and 'parameter_scores' in scores[0]:
            logger.info(f"\nParameter Analysis for '{scores[0]['name']}':")
            param_scores = scores[0]['parameter_scores']
            sorted_params = sorted(param_scores.items(), key=lambda x: x[1], reverse=True)
            
            # Top 15 parameters
            for param, score in sorted_params[:15]:
                # Only show meaningful scores
                if score > 0.1:
                    logger.info(f"  {param:<30} {score:.3f}")

    generate_enhanced_report(preset_scores, close_presets, analysis)
    
    # Step 6: Smart Selection Logic
    def smart_select_preset(close_presets: List[str], scores: List[Dict[str, Any]], analysis: Dict[str, Any]) -> str:
        """Smart preset selection with fallback logic."""
        if not close_presets:
            # No close matches - use best available if reasonable, otherwise default
            if scores and scores[0]['total_score'] > 0.2:
                logger.info(f"Using best available preset: {scores[0]['name']} (score: {scores[0]['total_score']:.3f})")
                return scores[0]['name']
            else:
                logger.info("No reasonable matches found, using default preset")
                return "default"
        
        if len(close_presets) == 1:
            return close_presets[0]
        
        # Multiple close matches - use additional criteria
        best_candidate = close_presets[0]
        
        # Prefer presets with model type compatibility
        legacy_model_type = legacy_config.get('model_type')
        if legacy_model_type:
            for preset_name in close_presets:
                preset_score = next(s for s in scores if s['name'] == preset_name)
                compatibility = preset_score.get('compatibility', [])
                if legacy_model_type in compatibility:
                    logger.info(f"Selected {preset_name} for model type compatibility with {legacy_model_type}")
                    return preset_name
        
        # Interactive selection for terminal environments
        if sys.stdin.isatty():
            return interactive_select_enhanced(close_presets, scores)
        
        return best_candidate
    
    def interactive_select_enhanced(close_presets: List[str], scores: List[Dict[str, Any]]) -> str:
        """Enhanced interactive selection with detailed comparisons."""
        print(f"\nFound {len(close_presets)} similar presets:")
        
        for i, name in enumerate(close_presets, 1):
            score = next(s for s in scores if s['name'] == name)
            print(f"  {i}. {name:<18} (score={score['total_score']:.3f})")
        
        print("  a. Show detailed analysis")
        print("  c. Compare presets side-by-side")
        print("  d. Use default preset")
        
        while True:
            try:
                choice = input(f"\nSelect [1-{len(close_presets)}], or (a/c/d): ").strip().lower()
                
                if choice == 'a':
                    # Show detailed analysis
                    preset_name = input("Enter preset name for analysis: ").strip()
                    preset_data = next((s for s in scores if s['name'] == preset_name), None)
                    if preset_data:
                        print(f"\nDetailed Analysis for '{preset_name}':")
                        print(f"Total Score: {preset_data['total_score']:.3f}")
                        print(f"Section Scores:")
                        for section, score in preset_data['section_scores'].items():
                            print(f"  {section}: {score:.3f}")
                        
                        if 'parameter_scores' in preset_data:
                            print(f"\nTop Parameter Matches:")
                            sorted_params = sorted(preset_data['parameter_scores'].items(), 
                                                 key=lambda x: x[1], reverse=True)
                            for param, score in sorted_params[:10]:
                                if score > 0.1:
                                    print(f"  {param}: {score:.3f}")
                    else:
                        print("Preset not found")
                    continue
                
                elif choice == 'c':
                    # Compare presets
                    if len(close_presets) >= 2:
                        print(f"\nComparison of top {min(3, len(close_presets))} presets:")
                        print(f"{'Parameter':<25}", end="")
                        for name in close_presets[:3]:
                            print(f"{name[:15]:<16}", end="")
                        print()
                        print("-" * (25 + 16 * min(3, len(close_presets))))
                        
                        # Compare key parameters
                        key_params = ['model.model_type', 'model.encoding_dim', 'training.batch_size', 
                                    'training.learning_rate', 'security.percentile']
                        
                        for param in key_params:
                            print(f"{param:<25}", end="")
                            for name in close_presets[:3]:
                                preset_data = next(s for s in scores if s['name'] == name)
                                section, key = param.split('.')
                                value = preset_data['config'].get(section, {}).get(key, 'N/A')
                                print(f"{str(value)[:15]:<16}", end="")
                            print()
                    continue
                
                elif choice == 'd':
                    return "default"
                
                # Numeric selection
                choice_idx = int(choice) - 1
                if 0 <= choice_idx < len(close_presets):
                    return close_presets[choice_idx]
                print(f"Please enter 1-{len(close_presets)}, or a/c/d")
                
            except ValueError:
                print(f"Please enter 1-{len(close_presets)}, or a/c/d")
    
    selected_preset = smart_select_preset(close_presets, preset_scores, analysis)
    
    # Step 7: Create Final Configuration
    try:
        if selected_preset not in PRESET_CONFIGS:
            logger.warning(f"Selected preset '{selected_preset}' not found, using default")
            selected_preset = "default"
        
        new_config = migrate_config(legacy_config, PRESET_CONFIGS[selected_preset])
        
        # Add comprehensive conversion metadata
        new_config['metadata']['conversion'] = {
            'method': 'advanced_preset_matching',
            'selected_preset': selected_preset,
            'similar_presets': [p for p in close_presets if p != selected_preset],
            'similarity_threshold': similarity_threshold,
            'best_score': best_preset['total_score'],
            'confidence': analysis['confidence'],
            'timestamp': datetime.now().isoformat(),
            'legacy_keys_analyzed': len(legacy_config),
            'presets_evaluated': len(preset_scores),
            'selection_method': 'interactive' if sys.stdin.isatty() and len(close_presets) > 1 else 'automatic'
        }
        
        # Validate the final configuration
        try:
            validate_config(new_config)
            logger.info("Converted configuration passed validation")
        except ValueError as e:
            logger.warning(f"Validation issues with converted config: {e}")
            new_config['metadata']['conversion']['validation_warnings'] = str(e)
        
        logger.info(f"Legacy configuration successfully converted using preset: {selected_preset}")
        logger.info(f"Conversion confidence: {analysis['confidence']}")
        
        return new_config
        
    except Exception as e:
        logger.error(f"Error creating final configuration: {e}")
        # Fallback to basic migration
        logger.info("Falling back to basic migration")
        return migrate_config(legacy_config)

def validate_config(config: Dict[str, Any], strict: bool = False) -> Tuple[bool, List[str], List[str]]:
    """
    Comprehensive configuration validation with enhanced preset compatibility,
    hardware requirement validation, automatic error correction capabilities,
    and intelligent memory management for optimal performance.
    
    This function provides deep validation of all configuration sections including
    the new preset features, enhanced model configurations, and system-aware validation.
    
    Args:
        config: Configuration dictionary to validate
        strict: If True, apply stricter validation rules and fail on warnings
        
    Returns:
        Tuple containing:
        - bool: True if configuration is valid (or auto-corrected)
        - List[str]: List of validation errors found
        - List[str]: List of validation warnings and recommendations
        
    Raises:
        ValueError: Only if configuration is fundamentally invalid and cannot be auto-corrected
    """
    errors = []
    warnings = []
    auto_fixes = []
    
    try:
        # Input validation
        if not isinstance(config, dict):
            raise ValueError("Configuration must be a dictionary")
        
        if not config:
            raise ValueError("Configuration cannot be empty")
        
        # INITIAL MEMORY OPTIMIZATION - Get hardware context early
        hardware_data = None
        total_ram_gb = 8.0  # Conservative default
        cuda_available = False
        
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
            cuda_available = hardware_data.get('cuda', {}).get('available', False)
        except Exception as e:
            logger.debug(f"Hardware detection failed during validation: {e}")
        
        # Track validation context
        validation_context = {
            'timestamp': datetime.now().isoformat(),
            'strict_mode': strict,
            'config_size': len(str(config)),
            'sections_present': list(config.keys()),
            'auto_fixes_applied': 0,
            'hardware_context': {
                'total_ram_gb': total_ram_gb,
                'cuda_available': cuda_available
            }
        }
        
        # MEMORY OPTIMIZATION - Clear memory before intensive validation for large configs
        config_size_mb = len(str(config)) / (1024 * 1024)
        if config_size_mb > 1.0 or total_ram_gb < 8:
            try:
                pre_validation_clear = enhanced_clear_memory(
                    aggressive=config_size_mb > 5.0 or total_ram_gb < 4,
                    hardware_data=hardware_data
                )
                if pre_validation_clear.get('success'):
                    logger.debug(f"Memory optimized before validation of {config_size_mb:.1f}MB config")
            except Exception as e:
                logger.debug(f"Pre-validation memory optimization failed: {e}")
        
        # 1. STRUCTURAL VALIDATION
        logger.debug("Starting structural validation")
        
        # Check for required top-level sections
        required_sections = ['training', 'model', 'security', 'data']
        missing_sections = [section for section in required_sections if section not in config]
        
        if missing_sections:
            errors.append(f"Missing required configuration sections: {missing_sections}")
            if strict:
                return False, errors, warnings
        
        # Check for optional but recommended sections
        recommended_sections = ['metadata', 'hardware', 'monitoring', 'system', 'presets']
        missing_recommended = [section for section in recommended_sections if section not in config]
        
        if missing_recommended:
            warnings.append(f"Missing recommended sections: {missing_recommended}")
        
        # 2. METADATA VALIDATION WITH ENHANCED CHECKS
        logger.debug("Validating metadata section")
        
        if 'metadata' in config:
            metadata = config['metadata']
            if not isinstance(metadata, dict):
                errors.append("Metadata section must be a dictionary")
            else:
                # Version compatibility validation
                config_version = metadata.get('config_version', '1.0')
                if config_version not in ['2.1', '2.0', '1.9', '1.8']:
                    warnings.append(f"Unknown config version '{config_version}', expecting '2.1'")
                
                # Schema validation
                if 'validation' in metadata:
                    validation_info = metadata['validation']
                    schema_version = validation_info.get('schema_version', '1.0')
                    if schema_version != '2.1':
                        warnings.append(f"Schema version mismatch: {schema_version} != 2.1")
                    
                    # Check required sections against metadata
                    metadata_required = validation_info.get('required_sections', [])
                    if metadata_required != required_sections:
                        warnings.append("Metadata required_sections don't match validation requirements")
                
                # System information validation
                if 'system' in metadata:
                    system_info = metadata['system']
                    
                    # Critical system fields
                    critical_fields = ['python_version', 'pytorch_version', 'os']
                    missing_system = [field for field in critical_fields if field not in system_info]
                    if missing_system:
                        warnings.append(f"Missing system information: {missing_system}")
                    
                    # CUDA consistency check
                    cuda_available_meta = system_info.get('cuda_available', False)
                    cuda_devices = system_info.get('cuda_devices', 0)
                    if cuda_available_meta and cuda_devices == 0:
                        warnings.append("CUDA reported as available but no devices found")
                    elif not cuda_available_meta and cuda_devices > 0:
                        warnings.append("CUDA devices reported but CUDA not available")
                
                # Hardware requirements validation
                if 'recommended_hardware' in metadata:
                    hw_req = metadata['recommended_hardware']
                    
                    # Validate reasonable hardware requirements
                    gpu_memory = hw_req.get('gpu_memory_gb', 0)
                    if gpu_memory > 80:
                        warnings.append(f"Very high GPU memory requirement: {gpu_memory}GB")
                    elif gpu_memory > 0 and gpu_memory < 1:
                        warnings.append(f"Very low GPU memory requirement: {gpu_memory}GB")
                    
                    cpu_cores = hw_req.get('cpu_cores', 1)
                    if cpu_cores > 128:
                        warnings.append(f"Very high CPU core requirement: {cpu_cores}")
                    elif cpu_cores < 1:
                        errors.append(f"Invalid CPU core requirement: {cpu_cores}")
                
                # Preset compatibility validation
                if 'compatibility' in metadata:
                    compatibility = metadata['compatibility']
                    if isinstance(compatibility, list):
                        # Check against available model types
                        if MODEL_VARIANTS:
                            invalid_models = [model for model in compatibility if model not in MODEL_VARIANTS]
                            if invalid_models:
                                warnings.append(f"Compatibility includes unavailable models: {invalid_models}")
                    else:
                        warnings.append("Compatibility field should be a list of model types")
        
        # MEMORY OPTIMIZATION - Clear memory after metadata validation for large configs
        if len(str(metadata)) > 50000 and total_ram_gb < 16:
            try:
                metadata_clear = enhanced_clear_memory(
                    aggressive=False,
                    hardware_data=hardware_data
                )
                if metadata_clear.get('success'):
                    logger.debug("Memory optimized after metadata validation")
            except Exception as e:
                logger.debug(f"Post-metadata memory optimization failed: {e}")
        
        # 3. TRAINING CONFIGURATION VALIDATION WITH PRESET AWARENESS
        logger.debug("Validating training section")
        
        if 'training' in config:
            training = config['training']
            if not isinstance(training, dict):
                errors.append("Training section must be a dictionary")
            else:
                # Core parameter validation with auto-correction
                batch_size = training.get('batch_size', 32)
                if not isinstance(batch_size, int) or batch_size < 1:
                    errors.append(f"Invalid batch_size: {batch_size}, must be positive integer")
                elif batch_size > 2048:
                    warnings.append(f"Very large batch_size: {batch_size}, may cause memory issues")
                elif batch_size < 2:
                    warnings.append(f"Very small batch_size: {batch_size}, may cause training instability")
                
                epochs = training.get('epochs', 100)
                if not isinstance(epochs, int) or epochs < 1:
                    errors.append(f"Invalid epochs: {epochs}, must be positive integer")
                elif epochs > 5000:
                    warnings.append(f"Very high epoch count: {epochs}, consider early stopping")
                elif epochs < 5:
                    warnings.append(f"Very low epoch count: {epochs}, may not converge properly")
                
                learning_rate = training.get('learning_rate', 0.001)
                if not isinstance(learning_rate, (int, float)) or learning_rate <= 0:
                    errors.append(f"Invalid learning_rate: {learning_rate}, must be positive number")
                elif learning_rate > 1.0:
                    warnings.append(f"Very high learning_rate: {learning_rate}, may cause instability")
                elif learning_rate < 1e-8:
                    warnings.append(f"Very low learning_rate: {learning_rate}, may not converge")
                
                patience = training.get('patience', 10)
                if not isinstance(patience, int) or patience < 0:
                    errors.append(f"Invalid patience: {patience}, must be non-negative integer")
                elif patience > 1000:
                    warnings.append(f"Very high patience: {patience}, training may run too long")
                
                # Enhanced optimizer validation
                optimizer = training.get('optimizer', 'Adam')
                valid_optimizers = ['SGD', 'Adam', 'AdamW', 'RMSprop', 'Adagrad', 'LBFGS']
                if optimizer not in valid_optimizers:
                    errors.append(f"Invalid optimizer: {optimizer}, must be one of {valid_optimizers}")
                
                # Optimizer-specific parameter validation
                if optimizer == 'Adam' or optimizer == 'AdamW':
                    adam_betas = training.get('adam_betas', (0.9, 0.999))
                    if not (isinstance(adam_betas, (list, tuple)) and len(adam_betas) == 2):
                        warnings.append("adam_betas should be a tuple/list of 2 values")
                    elif not all(0 < beta < 1 for beta in adam_betas):
                        errors.append("adam_betas values must be between 0 and 1")
                    
                    adam_eps = training.get('adam_eps', 1e-8)
                    if not isinstance(adam_eps, (int, float)) or adam_eps <= 0:
                        errors.append("adam_eps must be positive number")
                
                # Scheduler validation with parameter checking
                scheduler = training.get('scheduler')
                if scheduler is not None:
                    valid_schedulers = [
                        'StepLR', 'MultiStepLR', 'ExponentialLR', 'CosineAnnealingLR',
                        'ReduceLROnPlateau', 'CosineAnnealingWarmRestarts', 'OneCycleLR', 'CyclicLR'
                    ]
                    if scheduler not in valid_schedulers:
                        errors.append(f"Invalid scheduler: {scheduler}, must be one of {valid_schedulers}")
                    
                    # Validate scheduler parameters
                    scheduler_params = training.get('scheduler_params', {})
                    if scheduler == 'ReduceLROnPlateau':
                        mode = scheduler_params.get('mode', 'min')
                        if mode not in ['min', 'max']:
                            errors.append("ReduceLROnPlateau mode must be 'min' or 'max'")
                        
                        factor = scheduler_params.get('factor', 0.5)
                        if not isinstance(factor, (int, float)) or not (0 < factor < 1):
                            errors.append("ReduceLROnPlateau factor must be between 0 and 1")
                    
                    elif scheduler == 'CosineAnnealingLR':
                        T_max = scheduler_params.get('T_max')
                        if T_max and (not isinstance(T_max, int) or T_max <= 0):
                            errors.append("CosineAnnealingLR T_max must be positive integer")
                
                # Advanced training parameter validation
                gradient_clip = training.get('gradient_clip', 1.0)
                if not isinstance(gradient_clip, (int, float)) or gradient_clip < 0:
                    errors.append("gradient_clip must be non-negative number")
                elif gradient_clip > 100:
                    warnings.append(f"Very high gradient clipping: {gradient_clip}")
                
                gradient_accumulation = training.get('gradient_accumulation_steps', 1)
                if not isinstance(gradient_accumulation, int) or gradient_accumulation < 1:
                    errors.append("gradient_accumulation_steps must be positive integer")
                elif gradient_accumulation > 64:
                    warnings.append(f"High gradient accumulation: {gradient_accumulation}, may slow training")
                
                # Hardware-aware validation
                mixed_precision = training.get('mixed_precision', False)
                if mixed_precision and not cuda_available:
                    warnings.append("mixed_precision enabled but CUDA not available")
                
                num_workers = training.get('num_workers', 1)
                max_workers = (os.cpu_count() or 1) * 2
                if not isinstance(num_workers, int) or num_workers < 0:
                    errors.append("num_workers must be non-negative integer")
                elif num_workers > max_workers:
                    warnings.append(f"num_workers ({num_workers}) exceeds recommended max ({max_workers})")
                
                pin_memory = training.get('pin_memory', False)
                if pin_memory and not cuda_available:
                    warnings.append("pin_memory enabled but CUDA not available")
                
                # Cross-parameter validation
                effective_batch_size = batch_size * gradient_accumulation
                if effective_batch_size > 1024:
                    warnings.append(f"Large effective batch size ({effective_batch_size}) may hurt generalization")
                elif effective_batch_size < 4:
                    warnings.append(f"Small effective batch size ({effective_batch_size}) may cause noisy gradients")
        
        # 4. MODEL CONFIGURATION VALIDATION WITH ENHANCED PRESET SUPPORT
        logger.debug("Validating model section")
        
        if 'model' in config:
            model = config['model']
            if not isinstance(model, dict):
                errors.append("Model section must be a dictionary")
            else:
                # MEMORY OPTIMIZATION - Clear memory before intensive model validation for complex models
                model_complexity = len(str(model))
                if model_complexity > 10000 and total_ram_gb < 16:
                    try:
                        model_validation_clear = enhanced_clear_memory(
                            aggressive=model_complexity > 50000,
                            hardware_data=hardware_data
                        )
                        if model_validation_clear.get('success'):
                            logger.debug("Memory optimized before model validation")
                    except Exception as e:
                        logger.debug(f"Model validation memory optimization failed: {e}")
                
                # Model type validation with MODEL_VARIANTS checking
                model_type = model.get('model_type', 'SimpleAutoencoder')
                
                if MODEL_VARIANTS:
                    if model_type not in MODEL_VARIANTS:
                        errors.append(f"Invalid model_type: {model_type}, available: {list(MODEL_VARIANTS.keys())}")
                else:
                    # Fallback validation
                    valid_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                    if model_type not in valid_types:
                        errors.append(f"Invalid model_type: {model_type}, expected one of: {valid_types}")
                
                # Encoding dimension validation
                encoding_dim = model.get('encoding_dim', 8)
                if not isinstance(encoding_dim, int) or encoding_dim <= 0:
                    errors.append(f"Invalid encoding_dim: {encoding_dim}, must be positive integer")
                elif encoding_dim > 1000:
                    warnings.append(f"Very large encoding_dim: {encoding_dim}, may cause memory issues")
                elif encoding_dim < 2:
                    warnings.append(f"Very small encoding_dim: {encoding_dim}, may limit expressiveness")
                
                # Architecture validation with enhanced checks
                hidden_dims = model.get('hidden_dims', [])
                if not isinstance(hidden_dims, list):
                    errors.append("hidden_dims must be a list")
                elif not hidden_dims:
                    errors.append("hidden_dims cannot be empty")
                else:
                    # Validate all dimensions
                    invalid_dims = [dim for dim in hidden_dims if not isinstance(dim, int) or dim <= 0]
                    if invalid_dims:
                        errors.append(f"Invalid hidden dimensions: {invalid_dims}")
                    
                    # Architecture complexity checks
                    if len(hidden_dims) > 10:
                        warnings.append(f"Very deep architecture ({len(hidden_dims)} layers) may be hard to train")
                    
                    large_dims = [dim for dim in hidden_dims if dim > 2048]
                    if large_dims:
                        warnings.append(f"Very large hidden dimensions: {large_dims}")
                    
                    # Check for decreasing pattern
                    if len(hidden_dims) > 1:
                        increasing_count = sum(1 for i in range(1, len(hidden_dims)) if hidden_dims[i] > hidden_dims[i-1])
                        if increasing_count > len(hidden_dims) // 2:
                            warnings.append("Hidden dimensions mostly increase - consider decreasing pattern for autoencoders")
                
                # Dropout validation with length matching
                dropout_rates = model.get('dropout_rates', [])
                if not isinstance(dropout_rates, list):
                    errors.append("dropout_rates must be a list")
                elif not dropout_rates:
                    errors.append("dropout_rates cannot be empty")
                else:
                    # Validate all rates
                    invalid_rates = [rate for rate in dropout_rates if not isinstance(rate, (int, float)) or not (0 <= rate < 1)]
                    if invalid_rates:
                        errors.append(f"Invalid dropout rates: {invalid_rates}, must be between 0 and 1")
                    
                    # Length compatibility
                    if len(hidden_dims) != len(dropout_rates):
                        errors.append(f"Length mismatch: {len(hidden_dims)} hidden layers, {len(dropout_rates)} dropout rates")
                    
                    # Dropout strategy validation
                    if all(rate == 0 for rate in dropout_rates):
                        warnings.append("No dropout applied (all rates are 0)")
                    elif any(rate > 0.8 for rate in dropout_rates):
                        warnings.append("Very high dropout rates may hurt performance")
                
                # Activation function validation
                activation = model.get('activation', 'relu')
                available_activations = model.get('available_activations', ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish'])
                if activation not in available_activations:
                    errors.append(f"Invalid activation: {activation}, available: {available_activations}")
                
                activation_param = model.get('activation_param', 0.0)
                if not isinstance(activation_param, (int, float)):
                    errors.append("activation_param must be a number")
                elif activation == 'leaky_relu' and activation_param <= 0:
                    warnings.append("leaky_relu typically uses positive activation_param")
                
                # Normalization validation with consistency checks
                normalization = model.get('normalization')
                available_normalizations = model.get('available_normalizations', ['batch', 'layer', 'instance', 'group', None])
                if normalization not in available_normalizations:
                    errors.append(f"Invalid normalization: {normalization}, available: {available_normalizations}")

                use_batch_norm = model.get('use_batch_norm', False)
                use_layer_norm = model.get('use_layer_norm', False)

                # Fix inconsistencies: ensure normalization setting matches use_*_norm flags
                if normalization is None:
                    # If no normalization specified, both flags should be False
                    if use_batch_norm or use_layer_norm:
                        warnings.append("normalization=None but batch_norm or layer_norm enabled - auto-correcting to disable both")
                        model['use_batch_norm'] = False
                        model['use_layer_norm'] = False
                elif normalization == 'batch':
                    # If batch normalization specified, use_batch_norm should be True, use_layer_norm should be False
                    if not use_batch_norm:
                        warnings.append("normalization='batch' but use_batch_norm=False - auto-correcting to use_batch_norm=True")
                        model['use_batch_norm'] = True
                    if use_layer_norm:
                        warnings.append("normalization='batch' but use_layer_norm=True - auto-correcting to use_layer_norm=False")
                        model['use_layer_norm'] = False
                elif normalization == 'layer':
                    # If layer normalization specified, use_layer_norm should be True, use_batch_norm should be False
                    if not use_layer_norm:
                        warnings.append("normalization='layer' but use_layer_norm=False - auto-correcting to use_layer_norm=True")
                        model['use_layer_norm'] = True
                    if use_batch_norm:
                        warnings.append("normalization='layer' but use_batch_norm=True - auto-correcting to use_batch_norm=False")
                        model['use_batch_norm'] = False
                elif normalization in ['instance', 'group']:
                    # For instance/group normalization, both batch_norm and layer_norm should be False
                    if use_batch_norm or use_layer_norm:
                        warnings.append(f"normalization='{normalization}' but batch_norm or layer_norm enabled - auto-correcting to disable both")
                        model['use_batch_norm'] = False
                        model['use_layer_norm'] = False

                # Final validation check - should not have both enabled after auto-corrections
                use_batch_norm = model.get('use_batch_norm', False)
                use_layer_norm = model.get('use_layer_norm', False)
                if use_batch_norm and use_layer_norm:
                    warnings.append("Both batch_norm and layer_norm still enabled after auto-correction - may cause conflicts")

                # Batch norm compatibility with batch size
                if use_batch_norm and 'training' in config:
                    batch_size = config['training'].get('batch_size', 32)
                    if batch_size < 2:
                        errors.append("Batch normalization requires training batch_size >= 2")
                
                # Weight initialization validation
                weight_init = model.get('weight_init', 'xavier_uniform')
                available_initializers = model.get('available_initializers', ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'orthogonal'])
                if weight_init not in available_initializers:
                    errors.append(f"Invalid weight_init: {weight_init}, available: {available_initializers}")
                
                # Advanced features validation
                skip_connection = model.get('skip_connection', False)
                residual_blocks = model.get('residual_blocks', False)
                use_attention = model.get('use_attention', False)
                
                if skip_connection and len(hidden_dims) < 2:
                    warnings.append("Skip connections are most effective with deeper architectures")
                
                if residual_blocks and len(hidden_dims) < 3:
                    warnings.append("Residual blocks are most effective with deeper architectures")
                
                if use_attention and model_type == 'SimpleAutoencoder':
                    warnings.append("Attention mechanism may not be supported in SimpleAutoencoder")
                
                # Ensemble-specific validation
                if model_type == 'AutoencoderEnsemble':
                    num_models = model.get('num_models', 1)
                    if not isinstance(num_models, int) or num_models < 1:
                        errors.append(f"Invalid num_models: {num_models}, must be positive integer")
                    elif num_models < 2:
                        warnings.append("Ensemble should have at least 2 models for effectiveness")
                    elif num_models > 20:
                        warnings.append(f"Large ensemble ({num_models} models) may require significant memory")
                    
                    diversity_factor = model.get('diversity_factor', 0.1)
                    if not isinstance(diversity_factor, (int, float)) or not (0 <= diversity_factor <= 1):
                        errors.append(f"Invalid diversity_factor: {diversity_factor}, must be between 0 and 1")
                    elif diversity_factor == 0:
                        warnings.append("diversity_factor=0 may reduce ensemble effectiveness")
                
                # Feature requirements validation
                min_features = model.get('min_features', 5)
                if not isinstance(min_features, int) or min_features <= 0:
                    errors.append(f"Invalid min_features: {min_features}, must be positive integer")
                
                # Cross-validation with data configuration
                if 'data' in config:
                    data_features = config['data'].get('features', 20)
                    if data_features < min_features:
                        errors.append(f"Data features ({data_features}) < model min_features ({min_features})")
        
        # 5. SECURITY CONFIGURATION VALIDATION
        logger.debug("Validating security section")
        
        if 'security' in config:
            security = config['security']
            if not isinstance(security, dict):
                errors.append("Security section must be a dictionary")
            else:
                # Threshold validation
                percentile = security.get('percentile', 95)
                if not isinstance(percentile, (int, float)) or not (0 < percentile <= 100):
                    errors.append(f"Invalid percentile: {percentile}, must be between 0 and 100")
                elif percentile < 50:
                    warnings.append(f"Low percentile threshold ({percentile}) may cause many false positives")
                elif percentile > 99.9:
                    warnings.append(f"Very high percentile ({percentile}) may miss attacks")
                
                attack_threshold = security.get('attack_threshold', 0.3)
                if not isinstance(attack_threshold, (int, float)) or attack_threshold < 0:
                    errors.append(f"Invalid attack_threshold: {attack_threshold}, must be non-negative")
                elif attack_threshold > 10:
                    warnings.append(f"Very high attack threshold ({attack_threshold})")
                
                false_negative_cost = security.get('false_negative_cost', 2.0)
                if not isinstance(false_negative_cost, (int, float)) or false_negative_cost < 0:
                    errors.append(f"Invalid false_negative_cost: {false_negative_cost}, must be non-negative")
                
                # Advanced security features validation
                threshold_strategy = security.get('anomaly_threshold_strategy', 'fixed_percentile')
                valid_strategies = ['fixed_percentile', 'dynamic_percentile', 'adaptive', 'statistical']
                if threshold_strategy not in valid_strategies:
                    errors.append(f"Invalid threshold strategy: {threshold_strategy}, available: {valid_strategies}")
                
                detection_methods = security.get('detection_methods', ['reconstruction_error'])
                if not isinstance(detection_methods, list):
                    errors.append("detection_methods must be a list")
                else:
                    valid_methods = [
                        'reconstruction_error', 'statistical_analysis', 'mahalanobis_distance',
                        'isolation_forest', 'ensemble_voting', 'neural_network'
                    ]
                    invalid_methods = [method for method in detection_methods if method not in valid_methods]
                    if invalid_methods:
                        errors.append(f"Invalid detection methods: {invalid_methods}")
                
                # Alert levels validation
                alert_levels = security.get('alert_levels', ['low', 'medium', 'high'])
                if not isinstance(alert_levels, list) or len(alert_levels) < 2:
                    errors.append("alert_levels must be a list with at least 2 levels")
                
                # Confidence interval validation
                confidence_interval = security.get('confidence_interval', 0.95)
                if not isinstance(confidence_interval, (int, float)) or not (0 < confidence_interval < 1):
                    errors.append(f"Invalid confidence_interval: {confidence_interval}, must be between 0 and 1")
        
        # 6. DATA CONFIGURATION VALIDATION
        logger.debug("Validating data section")
        
        if 'data' in config:
            data = config['data']
            if not isinstance(data, dict):
                errors.append("Data section must be a dictionary")
            else:
                # Sample size validation
                normal_samples = data.get('normal_samples', 1000)
                if not isinstance(normal_samples, int) or normal_samples < 1:
                    errors.append(f"Invalid normal_samples: {normal_samples}, must be positive integer")
                elif normal_samples < 100:
                    warnings.append(f"Small normal sample size ({normal_samples}) may not be representative")
                elif normal_samples > 1000000:
                    warnings.append(f"Large normal sample size ({normal_samples}) may require significant memory")
                
                attack_samples = data.get('attack_samples', 200)
                if not isinstance(attack_samples, int) or attack_samples < 1:
                    errors.append(f"Invalid attack_samples: {attack_samples}, must be positive integer")
                elif attack_samples < 50:
                    warnings.append(f"Small attack sample size ({attack_samples}) may not be representative")
                
                # Feature validation
                features = data.get('features', 20)
                if not isinstance(features, int) or features < 1:
                    errors.append(f"Invalid features: {features}, must be positive integer")
                elif features > 10000:
                    warnings.append(f"Very high feature count ({features}) may cause curse of dimensionality")
                
                # Split validation
                validation_split = data.get('validation_split', 0.2)
                if not isinstance(validation_split, (int, float)) or not (0 < validation_split < 1):
                    errors.append(f"Invalid validation_split: {validation_split}, must be between 0 and 1")
                
                test_split = data.get('test_split', 0.2)
                if not isinstance(test_split, (int, float)) or not (0 < test_split < 1):
                    errors.append(f"Invalid test_split: {test_split}, must be between 0 and 1")
                
                # Total split validation
                total_split = validation_split + test_split
                if total_split >= 1.0:
                    errors.append(f"Combined splits ({total_split:.2f}) must be < 1.0")
                elif total_split > 0.8:
                    warnings.append(f"High combined split ratio ({total_split:.2f}) leaves little training data")
                
                # Normalization validation
                #normalization = data.get('normalization', 'standard')
                normalization = data.get('data_normalization', 'standard')
                valid_normalizations = ['standard', 'minmax', 'robust', 'quantile', 'none']
                if normalization not in valid_normalizations:
                    errors.append(f"Invalid normalization: {normalization}, available: {valid_normalizations}")
                
                # Anomaly factor validation
                anomaly_factor = data.get('anomaly_factor', 1.5)
                if not isinstance(anomaly_factor, (int, float)) or anomaly_factor <= 0:
                    errors.append(f"Invalid anomaly_factor: {anomaly_factor}, must be positive")
                elif anomaly_factor > 10:
                    warnings.append(f"Very high anomaly factor ({anomaly_factor})")
                
                # Synthetic data generation validation
                if 'synthetic_generation' in data:
                    synthetic = data['synthetic_generation']
                    if isinstance(synthetic, dict):
                        cluster_variance = synthetic.get('cluster_variance', 0.1)
                        if not isinstance(cluster_variance, (int, float)) or cluster_variance < 0:
                            errors.append("cluster_variance must be non-negative")
                        
                        anomaly_sparsity = synthetic.get('anomaly_sparsity', 0.3)
                        if not isinstance(anomaly_sparsity, (int, float)) or not (0 <= anomaly_sparsity <= 1):
                            errors.append("anomaly_sparsity must be between 0 and 1")
                
                # Preprocessing validation
                if 'preprocessing' in data:
                    preprocessing = data['preprocessing']
                    if isinstance(preprocessing, dict):
                        outlier_threshold = preprocessing.get('outlier_threshold', 3.0)
                        if not isinstance(outlier_threshold, (int, float)) or outlier_threshold <= 0:
                            errors.append("outlier_threshold must be positive")
                        elif outlier_threshold < 1:
                            warnings.append("Very low outlier threshold may remove too much data")
                        elif outlier_threshold > 5:
                            warnings.append("Very high outlier threshold may not remove outliers effectively")
        
        # MEMORY OPTIMIZATION - Clear memory before hardware validation for low-memory systems
        if total_ram_gb < 8:
            try:
                mid_validation_clear = enhanced_clear_memory(
                    aggressive=True,
                    hardware_data=hardware_data
                )
                if mid_validation_clear.get('success'):
                    logger.debug("Memory optimized mid-validation for low-memory system")
            except Exception as e:
                logger.debug(f"Mid-validation memory optimization failed: {e}")
        
        # 7. HARDWARE CONFIGURATION VALIDATION
        logger.debug("Validating hardware section")
        
        if 'hardware' in config:
            hardware = config['hardware']
            if not isinstance(hardware, dict):
                errors.append("Hardware section must be a dictionary")
            else:
                # Device validation
                device = hardware.get('device', 'auto')
                valid_devices = ['auto', 'cpu', 'cuda', 'mps']
                if device not in valid_devices:
                    errors.append(f"Invalid device: {device}, available: {valid_devices}")
                
                # Device availability check
                if device == 'cuda' and not cuda_available:
                    warnings.append("CUDA device specified but CUDA not available")
                elif device == 'mps' and not (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()):
                    warnings.append("MPS device specified but MPS not available")
                
                # Memory requirements validation
                recommended_gpu_memory = hardware.get('recommended_gpu_memory', 4)
                if not isinstance(recommended_gpu_memory, (int, float)) or recommended_gpu_memory < 0:
                    errors.append("recommended_gpu_memory must be non-negative")
                elif recommended_gpu_memory > 80:
                    warnings.append(f"Very high GPU memory requirement: {recommended_gpu_memory}GB")
                
                # System requirements validation
                if 'minimum_system_requirements' in hardware:
                    min_req = hardware['minimum_system_requirements']
                    if isinstance(min_req, dict):
                        cpu_cores = min_req.get('cpu_cores', 1)
                        if not isinstance(cpu_cores, int) or cpu_cores < 1:
                            errors.append("minimum cpu_cores must be positive integer")
                        
                        ram_gb = min_req.get('ram_gb', 2)
                        if not isinstance(ram_gb, (int, float)) or ram_gb < 0.5:
                            errors.append("minimum ram_gb must be >= 0.5")
                
                # Performance optimization validation
                if 'performance_optimization' in hardware:
                    perf_opt = hardware['performance_optimization']
                    if isinstance(perf_opt, dict):
                        use_cuda = perf_opt.get('use_cuda', False)
                        if use_cuda and not cuda_available:
                            warnings.append("use_cuda enabled but CUDA not available")
                        
                        use_amp = perf_opt.get('use_amp', False)
                        if use_amp and not cuda_available:
                            warnings.append("Automatic Mixed Precision requires CUDA")
                        
                        benchmark_mode = perf_opt.get('benchmark_mode', False)
                        deterministic = perf_opt.get('deterministic', False)
                        if benchmark_mode and deterministic:
                            warnings.append("benchmark_mode and deterministic may conflict")
        
        # 8. PRESET CONFIGURATION VALIDATION
        logger.debug("Validating presets section")
        
        if 'presets' in config:
            presets = config['presets']
            if not isinstance(presets, dict):
                errors.append("Presets section must be a dictionary")
            else:
                # Current preset validation
                current_preset = presets.get('current_preset')
                if current_preset is not None:
                    available_presets = get_available_presets()
                    if current_preset not in available_presets:
                        errors.append(f"Invalid current_preset: {current_preset}, available: {available_presets}")
                    else:
                        # Preset compatibility validation
                        try:
                            model_type = config.get('model', {}).get('model_type', 'SimpleAutoencoder')
                            if not validate_model_preset_compatibility(model_type, config):
                                warnings.append(f"Model type '{model_type}' may not be compatible with preset '{current_preset}'")
                        except Exception as e:
                            warnings.append(f"Preset compatibility check failed: {e}")
                
                # Override rules validation
                if 'override_rules' in presets:
                    override_rules = presets['override_rules']
                    if not isinstance(override_rules, dict):
                        errors.append("override_rules must be a dictionary")
                    else:
                        valid_sections = ['security', 'monitoring', 'hardware', 'training', 'model', 'data']
                        for section, enabled in override_rules.items():
                            if section not in valid_sections:
                                warnings.append(f"Unknown override rule section: {section}")
                            elif not isinstance(enabled, bool):
                                errors.append(f"Override rule for {section} must be boolean")
                
                # Auto-apply validation
                auto_apply = presets.get('auto_apply', False)
                if not isinstance(auto_apply, bool):
                    errors.append("auto_apply must be boolean")
        
        # 9. HYPERPARAMETER OPTIMIZATION VALIDATION
        logger.debug("Validating hyperparameter optimization section")
        
        if 'hyperparameter_optimization' in config:
            hpo = config['hyperparameter_optimization']
            if not isinstance(hpo, dict):
                errors.append("hyperparameter_optimization section must be a dictionary")
            else:
                enabled = hpo.get('enabled', False)
                if enabled:
                    # Strategy validation
                    strategy = hpo.get('strategy', 'optuna')
                    valid_strategies = ['optuna', 'hyperopt', 'skopt', 'random', 'grid']
                    if strategy not in valid_strategies:
                        errors.append(f"Invalid HPO strategy: {strategy}, available: {valid_strategies}")
                    
                    # Trials validation
                    n_trials = hpo.get('n_trials', 50)
                    if not isinstance(n_trials, int) or n_trials < 1:
                        errors.append("n_trials must be positive integer")
                    elif n_trials > 10000:
                        warnings.append(f"Very high trial count ({n_trials}) may take very long")
                    
                    # Timeout validation
                    timeout = hpo.get('timeout', 3600)
                    if not isinstance(timeout, (int, float)) or timeout < 60:
                        warnings.append("Very short HPO timeout may not find good solutions")
                    elif timeout > 86400:
                        warnings.append("Very long HPO timeout (>24h) may be excessive")
                    
                    # Optimization space validation
                    if 'optimization_space' in hpo:
                        opt_space = hpo['optimization_space']
                        if not isinstance(opt_space, dict):
                            errors.append("optimization_space must be a dictionary")
                        elif not opt_space:
                            warnings.append("Empty optimization space - HPO will have no effect")
        
        # 10. CROSS-SECTION VALIDATION
        logger.debug("Performing cross-section validation")
        
        # Model-Training compatibility
        if 'model' in config and 'training' in config:
            model_config = config['model']
            training_config = config['training']
            
            # Batch normalization vs batch size
            if model_config.get('use_batch_norm') and training_config.get('batch_size', 32) < 2:
                errors.append("Batch normalization requires training batch_size >= 2")
            
            # Mixed precision compatibility
            if training_config.get('mixed_precision') and model_config.get('model_type') == 'SimpleAutoencoder':
                warnings.append("Mixed precision may not be fully supported with SimpleAutoencoder")
            
            # Memory estimation
            try:
                batch_size = training_config.get('batch_size', 32)
                hidden_dims = model_config.get('hidden_dims', [])
                if hidden_dims:
                    model_complexity = sum(hidden_dims) + model_config.get('encoding_dim', 8)
                    # Rough MB estimate
                    memory_estimate = batch_size * model_complexity * 4 / (1024**2)
                    
                    # > 1GB
                    if memory_estimate > 1024:
                        warnings.append(f"High memory usage estimated: ~{memory_estimate:.0f}MB")
            except Exception:
                # Skip memory estimation if it fails
                pass
        
        # Hardware-Training compatibility
        if 'hardware' in config and 'training' in config:
            hardware_config = config['hardware']
            training_config = config['training']
            
            # CUDA settings consistency
            device = hardware_config.get('device', 'auto')
            use_cuda = hardware_config.get('performance_optimization', {}).get('use_cuda', False)
            
            if device == 'cpu' and use_cuda:
                warnings.append("Device set to CPU but use_cuda=True in performance optimization")
            elif device == 'cuda' and not use_cuda:
                warnings.append("Device set to CUDA but use_cuda=False in performance optimization")
            
            # Mixed precision consistency
            use_amp = hardware_config.get('performance_optimization', {}).get('use_amp', False)
            mixed_precision = training_config.get('mixed_precision', False)
            
            if use_amp and not mixed_precision:
                warnings.append("AMP enabled in hardware but mixed_precision=False in training")
            elif mixed_precision and not use_amp:
                warnings.append("mixed_precision=True in training but AMP not enabled in hardware")
        
        # Data-Model compatibility
        if 'data' in config and 'model' in config:
            data_config = config['data']
            model_config = config['model']
            
            # Feature count compatibility
            data_features = data_config.get('features', 20)
            min_features = model_config.get('min_features', 5)
            
            if data_features < min_features:
                errors.append(f"Data features ({data_features}) < model min_features ({min_features})")
            
            # Sample size vs model complexity
            normal_samples = data_config.get('normal_samples', 1000)
            hidden_dims = model_config.get('hidden_dims', [])
            # Rough parameter estimate
            total_params = sum(hidden_dims) if hidden_dims else 100
            
            if normal_samples < total_params:
                warnings.append(f"Small dataset ({normal_samples}) for model complexity (~{total_params} params)")
        
        # MEMORY OPTIMIZATION - Clear memory before strict mode validation for low-memory systems
        if strict and total_ram_gb < 8:
            try:
                strict_mode_clear = enhanced_clear_memory(
                    aggressive=True,
                    hardware_data=hardware_data
                )
                if strict_mode_clear.get('success'):
                    logger.debug("Memory optimized before strict mode validation")
            except Exception as e:
                logger.debug(f"Strict mode memory optimization failed: {e}")
        
        # 11. STRICT MODE ADDITIONAL VALIDATIONS
        if strict:
            logger.debug("Applying strict mode validations")
            
            # Require all recommended sections
            if missing_recommended:
                errors.extend([f"Strict mode: missing section '{section}'" for section in missing_recommended])
            
            # Stricter parameter bounds
            if 'training' in config:
                training = config['training']
                
                lr = training.get('learning_rate', 0.001)
                if lr > 0.1:
                    errors.append("Strict mode: learning_rate must be <= 0.1")
                elif lr < 1e-6:
                    errors.append("Strict mode: learning_rate must be >= 1e-6")
                
                batch_size = training.get('batch_size', 32)
                if batch_size > 512:
                    errors.append("Strict mode: batch_size must be <= 512")
                elif batch_size < 8:
                    errors.append("Strict mode: batch_size must be >= 8")
            
            # Require specific metadata fields
            if 'metadata' in config:
                metadata = config['metadata']
                required_strict_metadata = ['description', 'version', 'config_version']
                missing_metadata = [field for field in required_strict_metadata if field not in metadata]
                if missing_metadata:
                    errors.extend([f"Strict mode: missing metadata field '{field}'" for field in missing_metadata])
            
            # Hardware requirements in strict mode
            if 'hardware' in config:
                hardware = config['hardware']
                if 'minimum_system_requirements' not in hardware:
                    errors.append("Strict mode: hardware section must specify minimum_system_requirements")
            
            # Convert strict warnings to errors
            if any("Very high" in warning or "Very low" in warning for warning in warnings):
                extreme_warnings = [w for w in warnings if "Very high" in w or "Very low" in w]
                errors.extend([f"Strict mode: {warning}" for warning in extreme_warnings])
        
        # 12. FINAL VALIDATION SUMMARY
        logger.debug("Generating validation summary")
        
        # Calculate validation statistics
        total_checks = len(errors) + len(warnings)
        error_count = len(errors)
        warning_count = len(warnings)
        
        # Add validation context to config if requested
        validation_context.update({
            'total_checks': total_checks,
            'error_count': error_count,
            'warning_count': warning_count,
            'validation_passed': error_count == 0,
            'validation_quality': (
                'excellent' if error_count == 0 and warning_count == 0 else
                'good' if error_count == 0 and warning_count <= 5 else
                'needs_attention' if error_count == 0 else
                'failed'
            )
        })
        
        # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
        # Aggressive cleanup after validation completion
        try:
            final_clear_results = enhanced_clear_memory(
                aggressive=True,  # Aggressive final cleanup
                hardware_data=hardware_data
            )
            
            if final_clear_results.get('success'):
                logger.debug(f"Final validation memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
                
        except Exception as e:
            logger.debug(f"Final validation memory optimization failed: {e}")
        
        # Log validation summary
        if error_count == 0:
            if warning_count == 0:
                logger.debug("Configuration validation passed with no issues")
            else:
                logger.warning(f"Configuration validation passed with {warning_count} warnings")
        else:
            logger.error(f"Configuration validation failed with {error_count} errors and {warning_count} warnings")
            # Log each error and warning
            for err in errors:
                logger.error(f"Validation Error: {err}")
            for warn in warnings:
                logger.warning(f"Validation Warning: {warn}")
        
        # Return results
        is_valid = error_count == 0
        return is_valid, errors, warnings
        
    except Exception as e:
        error_msg = f"Configuration validation failed with exception: {str(e)}"
        logger.error(error_msg, exc_info=True)
        errors.append(error_msg)
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after validation error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        return False, errors, warnings

def validate_model_preset_compatibility(model_type: str, config: Dict[str, Any]) -> bool:
    """
    Comprehensive validation of model type compatibility with preset configurations.
    
    This function has been updated to work harmoniously with initialize_model_variants() and 
    validate_model_variants(), providing thorough compatibility validation for all preset 
    configurations including DEFAULT_PRESET, STABILITY_PRESET, PERFORMANCE_PRESET, and 
    custom configurations while maintaining consistency with the validation approach.
    
    Args:
        model_type: The model type to validate (e.g., 'SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble')
        config: Configuration dictionary (can be preset or full config structure)
        
    Returns:
        bool: True if compatible, False otherwise with detailed logging of incompatibility reasons
    """
    compatibility_start_time = time.time()
    validation_details = {
        'model_type': model_type,
        'compatibility_checks': [],
        'warnings': [],
        'errors': [],
        'recommendations': []
    }
    
    try:
        # Phase 1: Basic Input Validation (aligned with validate_model_variants approach)
        if not model_type or not isinstance(model_type, str):
            validation_details['errors'].append('Invalid model_type: must be non-empty string')
            logger.debug("validate_model_preset_compatibility: Invalid model_type provided")
            return False
        
        if not config or not isinstance(config, dict):
            validation_details['errors'].append('Invalid config: must be non-empty dictionary')
            logger.debug("validate_model_preset_compatibility: Invalid config provided")
            return False
        
        validation_details['compatibility_checks'].append('basic_input_validation')
        
        # Phase 2: Model Variants Availability Check (harmonized with initialize_model_variants)
        if not MODEL_VARIANTS:
            logger.debug("MODEL_VARIANTS not initialized, attempting initialization for compatibility check")
            try:
                #initialize_model_variants(silent=True)
                initialize_model_variants(silent=False)
            except Exception as e:
                validation_details['warnings'].append(f'MODEL_VARIANTS initialization failed: {str(e)}')
                logger.warning(f"Failed to initialize model variants for compatibility check: {e}")
                # Fallback to basic string validation (consistent with existing approach)
                valid_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                if model_type not in valid_types:
                    validation_details['errors'].append(f'Model type "{model_type}" not in known types: {valid_types}')
                    return False
                validation_details['warnings'].append('Using fallback validation without MODEL_VARIANTS')
        
        if MODEL_VARIANTS and model_type not in MODEL_VARIANTS:
            validation_details['errors'].append(f'Model type "{model_type}" not found in MODEL_VARIANTS')
            logger.debug(f"Model type '{model_type}' not found in MODEL_VARIANTS: {list(MODEL_VARIANTS.keys())}")
            return False
        
        validation_details['compatibility_checks'].append('model_variants_validation')
        
        # Phase 3: Configuration Structure Analysis (enhanced but aligned)
        try:
            config_structure_type = 'unknown'
            metadata = config.get('metadata', {})
            model_config = config.get('model', {})
            training_config = config.get('training', {})
            
            # Determine configuration structure type
            if 'metadata' in config and 'model' in config and 'training' in config:
                config_structure_type = 'full_preset'
                preset_name = config.get('presets', {}).get('current_preset') or metadata.get('preset_used')
            elif 'preset_used' in config or 'preset_used' in metadata:
                config_structure_type = 'preset_reference'
                preset_name = config.get('preset_used') or metadata.get('preset_used')
            elif model_config:
                config_structure_type = 'partial_config'
                preset_name = model_config.get('preset_used')
            else:
                config_structure_type = 'minimal_config'
                preset_name = None
            
            validation_details['config_structure_type'] = config_structure_type
            validation_details['preset_name'] = preset_name
            validation_details['compatibility_checks'].append('config_structure_analysis')
            
        except Exception as e:
            validation_details['warnings'].append(f'Config structure analysis failed: {str(e)}')
            logger.debug(f"Config structure analysis failed: {e}")
        
        # Phase 4: Explicit Compatibility List Validation
        try:
            compatible_models = metadata.get('compatibility', [])
            if compatible_models and isinstance(compatible_models, list):
                if model_type not in compatible_models:
                    validation_details['errors'].append(
                        f'Model type "{model_type}" not in explicit compatibility list: {compatible_models}'
                    )
                    logger.debug(f"Model type '{model_type}' not in compatibility list: {compatible_models}")
                    return False
                else:
                    validation_details['compatibility_checks'].append('explicit_compatibility_list')
                    logger.debug(f"Model type '{model_type}' found in explicit compatibility list")
            
        except Exception as e:
            validation_details['warnings'].append(f'Explicit compatibility validation failed: {str(e)}')
            logger.debug(f"Error validating explicit compatibility: {e}")
        
        # Phase 5: Model-Specific Architecture Requirements Validation (aligned with test configurations)
        try:
            if model_type == 'SimpleAutoencoder':
                validation_details['compatibility_checks'].append('simpleautoencoder_validation')
                
                # Simple autoencoder requirements - basic parameters only (consistent with test config)
                encoding_dim = model_config.get('encoding_dim', 12)
                if not isinstance(encoding_dim, (int, float)) or encoding_dim <= 0:
                    validation_details['errors'].append(f'SimpleAutoencoder: Invalid encoding_dim: {encoding_dim}')
                    logger.debug(f"SimpleAutoencoder: Invalid encoding_dim: {encoding_dim}")
                    return False
                
                # Ensure simple architecture (no complex features) - aligned with _create_model_test_config
                if model_config.get('use_attention', False):
                    validation_details['warnings'].append('SimpleAutoencoder: use_attention should be False for simple architecture')
                
                if model_config.get('residual_blocks', False):
                    validation_details['warnings'].append('SimpleAutoencoder: residual_blocks should be False for simple architecture')
                
                # Validate hidden dimensions for simple architecture
                hidden_dims = model_config.get('hidden_dims', [128])
                if isinstance(hidden_dims, list) and len(hidden_dims) > 2:
                    validation_details['warnings'].append(f'SimpleAutoencoder: Deep architecture with {len(hidden_dims)} layers may be too complex')
                
                # Validate dropout rates (consistent with _validate_and_adjust_parameters approach)
                dropout_rates = model_config.get('dropout_rates', [0.2])
                if isinstance(dropout_rates, list):
                    invalid_rates = [r for r in dropout_rates if not isinstance(r, (int, float)) or r < 0 or r >= 1]
                    if invalid_rates:
                        validation_details['errors'].append(f'SimpleAutoencoder: Invalid dropout rates: {invalid_rates}')
                        return False
                
                validation_details['recommendations'].append('SimpleAutoencoder works best with simple architectures and basic features')
                
            elif model_type == 'EnhancedAutoencoder':
                validation_details['compatibility_checks'].append('enhancedautoencoder_validation')
                
                # Enhanced autoencoder requirements - supports advanced features
                encoding_dim = model_config.get('encoding_dim', 32)
                if not isinstance(encoding_dim, (int, float)) or encoding_dim <= 0:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Invalid encoding_dim: {encoding_dim}')
                    return False
                
                # Validate hidden dimensions (consistent with parameter validation)
                hidden_dims = model_config.get('hidden_dims', [256, 128, 64])
                if not isinstance(hidden_dims, list) or not hidden_dims:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Invalid hidden_dims: {hidden_dims}')
                    return False
                
                # Check for invalid dimension values (aligned with _validate_and_adjust_parameters)
                invalid_dims = [dim for dim in hidden_dims if not isinstance(dim, (int, float)) or dim <= 0]
                if invalid_dims:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Invalid dimension values: {invalid_dims}')
                    return False
                
                # Validate dropout rates (consistent with existing validation approach)
                dropout_rates = model_config.get('dropout_rates', [0.2, 0.15, 0.1])
                if not isinstance(dropout_rates, list) or not dropout_rates:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Invalid dropout_rates: {dropout_rates}')
                    return False
                
                invalid_rates = [r for r in dropout_rates if not isinstance(r, (int, float)) or r < 0 or r >= 1]
                if invalid_rates:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Invalid dropout rate values: {invalid_rates}')
                    return False
                
                # Advanced feature compatibility checks (aligned with test configurations)
                use_attention = model_config.get('use_attention', False)
                if use_attention and encoding_dim < 32:
                    validation_details['warnings'].append(f'EnhancedAutoencoder: Attention mechanism may not be effective with small encoding_dim: {encoding_dim}')
                
                residual_blocks = model_config.get('residual_blocks', False)
                if residual_blocks and not hidden_dims:
                    validation_details['warnings'].append('EnhancedAutoencoder: Residual blocks require hidden layers')
                
                # Normalization compatibility (consistent with _extract_and_validate_config_param)
                normalization = model_config.get('normalization', 'batch')
                available_normalizations = model_config.get('available_normalizations', ['batch', 'layer', 'instance', 'group', 'none'])
                if normalization and normalization not in available_normalizations:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Normalization "{normalization}" not in available list: {available_normalizations}')
                    return False
                
                validation_details['recommendations'].append('EnhancedAutoencoder supports advanced features like attention and residual connections')
                
            elif model_type == 'AutoencoderEnsemble':
                validation_details['compatibility_checks'].append('autoencoder_ensemble_validation')
                
                # Ensemble-specific requirements (aligned with test configuration constraints)
                num_models = model_config.get('num_models', 3)
                if not isinstance(num_models, int) or num_models < 1:
                    validation_details['errors'].append(f'AutoencoderEnsemble: Invalid num_models: {num_models} (must be positive integer)')
                    return False
                
                if num_models > 20:
                    validation_details['warnings'].append(f'AutoencoderEnsemble: Large ensemble size ({num_models}) may be memory intensive')
                
                diversity_factor = model_config.get('diversity_factor', 0.3)
                if not isinstance(diversity_factor, (int, float)) or not 0 <= diversity_factor <= 1:
                    validation_details['errors'].append(f'AutoencoderEnsemble: Invalid diversity_factor: {diversity_factor} (must be between 0 and 1)')
                    return False
                
                # Basic architecture validation (same as enhanced, consistent with test configs)
                encoding_dim = model_config.get('encoding_dim', 24)
                if not isinstance(encoding_dim, (int, float)) or encoding_dim <= 0:
                    validation_details['errors'].append(f'AutoencoderEnsemble: Invalid encoding_dim: {encoding_dim}')
                    return False
                
                hidden_dims = model_config.get('hidden_dims', [192, 96, 48])
                if not isinstance(hidden_dims, list) or not hidden_dims:
                    validation_details['errors'].append(f'AutoencoderEnsemble: Invalid hidden_dims: {hidden_dims}')
                    return False
                
                # Memory and computational requirements (aligned with resource estimation approach)
                total_params_estimate = sum(hidden_dims) * num_models + encoding_dim * num_models
                if total_params_estimate > 1_000_000:  # 1M parameters threshold
                    validation_details['warnings'].append(f'AutoencoderEnsemble: Large parameter count estimate ({total_params_estimate:,}) may require significant memory')
                
                validation_details['recommendations'].append('AutoencoderEnsemble provides improved robustness through model diversity')
                
            else:
                validation_details['warnings'].append(f'Unknown model type "{model_type}" - using generic validation')
                
        except Exception as e:
            validation_details['errors'].append(f'Model-specific validation failed: {str(e)}')
            logger.error(f"Model-specific validation failed for {model_type}: {e}")
            return False
        
        # Phase 6: Preset-Specific Compatibility Validation (enhanced but consistent)
        try:
            if preset_name and preset_name in globals().get('PRESET_CONFIGS', {}):
                validation_details['compatibility_checks'].append('preset_specific_validation')
                
                preset_config = globals()['PRESET_CONFIGS'][preset_name]
                preset_metadata = preset_config.get('metadata', {})
                preset_compatible_models = preset_metadata.get('compatibility', [])
                
                if preset_compatible_models and model_type not in preset_compatible_models:
                    validation_details['errors'].append(f'Model type "{model_type}" not compatible with preset "{preset_name}" (compatible: {preset_compatible_models})')
                    logger.debug(f"Model type '{model_type}' not compatible with preset '{preset_name}'")
                    return False
                
                # Validate preset's model configuration compatibility
                preset_model_config = preset_config.get('model', {})
                preset_model_type = preset_model_config.get('model_type')
                
                if preset_model_type and preset_model_type != model_type:
                    validation_details['warnings'].append(f'Preset "{preset_name}" configured for "{preset_model_type}", requested "{model_type}" (may work but not optimal)')
                
                # Check preset-specific constraints (aligned with configuration validation)
                if preset_name == 'STABILITY_PRESET':
                    # Stability preset should use conservative settings
                    if model_config.get('dropout_rates', []):
                        high_dropout = [r for r in model_config['dropout_rates'] if isinstance(r, (int, float)) and r > 0.3]
                        if high_dropout:
                            validation_details['warnings'].append(f'STABILITY_PRESET: High dropout rates may impact stability: {high_dropout}')
                    
                elif preset_name == 'PERFORMANCE_PRESET':
                    # Performance preset should support advanced features
                    if model_type == 'SimpleAutoencoder':
                        validation_details['warnings'].append('PERFORMANCE_PRESET: SimpleAutoencoder may not utilize performance optimizations fully')
                    
                    mixed_precision = training_config.get('mixed_precision', False)
                    if not mixed_precision and torch.cuda.is_available():
                        validation_details['recommendations'].append('PERFORMANCE_PRESET: Consider enabling mixed_precision for better performance')
                
                elif preset_name == 'DEFAULT_PRESET':
                    # Default preset should work with all model types
                    pass
                
                validation_details['preset_validation_completed'] = preset_name
                
        except Exception as e:
            validation_details['warnings'].append(f'Preset-specific validation failed: {str(e)}')
            logger.debug(f"Error during preset validation: {e}")
        
        # Phase 7: Hardware Requirements Validation (aligned with system analysis approach)
        try:
            validation_details['compatibility_checks'].append('hardware_requirements_validation')
            
            hardware_config = config.get('hardware', {})
            if hardware_config:
                # Memory requirements estimation (consistent with resource estimation)
                min_gpu_memory = hardware_config.get('minimum_system_requirements', {}).get('gpu_memory_gb', 0)
                
                # Model-specific memory requirements in GB (aligned with resource analysis)
                memory_requirement = 0
                if model_type == 'SimpleAutoencoder':
                    memory_requirement = 1
                elif model_type == 'EnhancedAutoencoder':
                    memory_requirement = 2
                elif model_type == 'AutoencoderEnsemble':
                    num_models = model_config.get('num_models', 3)
                    memory_requirement = 1.5 * num_models
                
                if min_gpu_memory > 0 and memory_requirement > min_gpu_memory:
                    validation_details['warnings'].append(f'Estimated memory requirement ({memory_requirement:.1f}GB) exceeds minimum specified ({min_gpu_memory}GB)')
                
                # Device compatibility (consistent with hardware context validation)
                device = hardware_config.get('device', 'auto')
                mixed_precision = training_config.get('mixed_precision', False)
                
                if mixed_precision and device == 'cpu':
                    validation_details['warnings'].append('Mixed precision enabled but device is CPU (mixed precision requires CUDA)')
                
                # CUDA availability check (aligned with system validation)
                if device == 'cuda' and not torch.cuda.is_available():
                    validation_details['errors'].append('CUDA device specified but CUDA is not available')
                    return False
                
        except Exception as e:
            validation_details['warnings'].append(f'Hardware requirements validation failed: {str(e)}')
            logger.debug(f"Error during hardware validation: {e}")
        
        # Phase 8: Activation Function Compatibility (consistent with config parameter extraction)
        try:
            validation_details['compatibility_checks'].append('activation_compatibility_validation')
            
            activation = model_config.get('activation', 'leaky_relu')
            available_activations = model_config.get('available_activations', [
                'relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu', 'prelu'
            ])
            
            if activation and activation not in available_activations:
                validation_details['errors'].append(f'Activation function "{activation}" not in available list: {available_activations}')
                return False
            
            # Model-specific activation recommendations (aligned with test configurations)
            if model_type == 'SimpleAutoencoder' and activation in ['gelu', 'swish', 'selu']:
                validation_details['warnings'].append(f'SimpleAutoencoder: Advanced activation "{activation}" may be overkill for simple architecture')
            
            activation_param = model_config.get('activation_param', 0.2)
            if activation == 'leaky_relu' and not isinstance(activation_param, (int, float)):
                validation_details['errors'].append(f'LeakyReLU requires numeric activation_param, got: {activation_param}')
                return False
            
        except Exception as e:
            validation_details['warnings'].append(f'Activation compatibility validation failed: {str(e)}')
            logger.debug(f"Error during activation validation: {e}")
        
        # Phase 9: Training Configuration Compatibility (aligned with functional testing approach)
        try:
            validation_details['compatibility_checks'].append('training_compatibility_validation')
            
            if training_config:
                batch_size = training_config.get('batch_size', 32)
                
                # Batch normalization compatibility (consistent with test scenario filtering)
                use_batch_norm = model_config.get('use_batch_norm', False)
                normalization = model_config.get('normalization', 'batch')
                
                if (use_batch_norm or normalization == 'batch') and batch_size < 2:
                    validation_details['errors'].append(f'Batch size {batch_size} too small for batch normalization (minimum 2)')
                    return False
                
                if batch_size < 1:
                    validation_details['errors'].append(f'Invalid batch size: {batch_size}')
                    return False
                
                # Ensemble-specific training compatibility (aligned with scaling analysis)
                if model_type == 'AutoencoderEnsemble':
                    num_models = model_config.get('num_models', 3)
                    if batch_size < num_models:
                        validation_details['warnings'].append(f'Batch size ({batch_size}) smaller than ensemble size ({num_models}) may impact training efficiency')
                
                # Learning rate validation (consistent with parameter extraction validation)
                learning_rate = training_config.get('learning_rate', 0.001)
                if not isinstance(learning_rate, (int, float)) or learning_rate <= 0:
                    validation_details['errors'].append(f'Invalid learning rate: {learning_rate}')
                    return False
                
                # Optimizer compatibility (aligned with resource estimation approach)
                optimizer = training_config.get('optimizer', 'AdamW')
                available_optimizers = ['Adam', 'AdamW', 'SGD', 'RMSprop', 'Adagrad']
                if optimizer not in available_optimizers:
                    validation_details['warnings'].append(f'Optimizer "{optimizer}" may not be supported (available: {available_optimizers})')
                
        except Exception as e:
            validation_details['warnings'].append(f'Training compatibility validation failed: {str(e)}')
            logger.debug(f"Error during training validation: {e}")
        
        # Phase 10: Data Configuration Compatibility (aligned with input dimension validation)
        try:
            validation_details['compatibility_checks'].append('data_compatibility_validation')
            
            data_config = config.get('data', {})
            if data_config:
                features = data_config.get('features', 20)
                min_features = model_config.get('min_features', 5)
                
                if not isinstance(features, int) or features < min_features:
                    validation_details['errors'].append(f'Feature count {features} below minimum required {min_features}')
                    return False
                
                # Model-specific feature requirements (consistent with architectural analysis)
                if model_type == 'EnhancedAutoencoder' and features < 10:
                    validation_details['warnings'].append(f'EnhancedAutoencoder: Small feature count ({features}) may not benefit from advanced features')
                
                encoding_dim = model_config.get('encoding_dim', 16)
                if encoding_dim >= features:
                    validation_details['warnings'].append(f'Encoding dimension ({encoding_dim}) should be smaller than input features ({features}) for compression')
                
        except Exception as e:
            validation_details['warnings'].append(f'Data compatibility validation failed: {str(e)}')
            logger.debug(f"Error during data validation: {e}")
        
        # Phase 11: Experimental Features Validation (consistent with configuration analysis)
        try:
            validation_details['compatibility_checks'].append('experimental_features_validation')
            
            experimental_config = config.get('experimental', {})
            if experimental_config:
                experimental_features = experimental_config.get('experimental_features', {})
                
                if experimental_features and experimental_features.get('enabled', False):
                    if model_type == 'SimpleAutoencoder':
                        validation_details['warnings'].append('SimpleAutoencoder: Experimental features may not be supported')
                    
                    validation_details['recommendations'].append('Experimental features should be used with caution in production')
                
        except Exception as e:
            validation_details['warnings'].append(f'Experimental features validation failed: {str(e)}')
            logger.debug(f"Error during experimental features validation: {e}")
        
        # Phase 12: Comprehensive Results Analysis and Memory Optimization (aligned with existing functions)
        validation_time = time.time() - compatibility_start_time
        validation_details['validation_time_seconds'] = validation_time
        validation_details['total_checks_performed'] = len(validation_details['compatibility_checks'])
        validation_details['total_warnings'] = len(validation_details['warnings'])
        validation_details['total_errors'] = len(validation_details['errors'])
        
        # Determine final compatibility result (consistent with validate_model_variants approach)
        has_critical_errors = len(validation_details['errors']) > 0
        has_warnings = len(validation_details['warnings']) > 0
        
        if has_critical_errors:
            logger.debug(f"Model type '{model_type}' is NOT compatible: {len(validation_details['errors'])} errors found")
            # Log first 3 errors for debugging
            logger.debug(f"Compatibility errors: {validation_details['errors'][:3]}...")
            return False
        
        # Log detailed results (aligned with existing logging approach)
        if has_warnings:
            logger.debug(f"Model type '{model_type}' is compatible with {len(validation_details['warnings'])} warnings")
            # Log first 2 warnings for debugging
            logger.debug(f"Compatibility warnings: {validation_details['warnings'][:2]}...")
        else:
            logger.debug(f"Model type '{model_type}' is fully compatible - all {validation_details['total_checks_performed']} checks passed")
        
        # Store validation details for debugging (consistent with existing approach)
        if hasattr(validate_model_preset_compatibility, 'last_validation_details'):
            validate_model_preset_compatibility.last_validation_details = validation_details
        
        return True
        
    except Exception as e:
        validation_details['errors'].append(f'Unexpected validation error: {str(e)}')
        logger.error(f"Unexpected error during model-preset compatibility validation: {e}")
        logger.debug(f"Validation details when error occurred: {validation_details}")
        
        # In case of validation errors, default to compatible to avoid blocking functionality
        # but log the issue for investigation (consistent with existing error handling)
        logger.warning(f"Defaulting to compatible due to validation error for {model_type}")
        return True
    
    finally:
        # Always log final timing information (aligned with existing timing approach)
        total_time = time.time() - compatibility_start_time
        logger.debug(f"Model-preset compatibility validation completed in {total_time:.3f}s: "
                    f"model='{model_type}', checks={validation_details.get('total_checks_performed', 0)}, "
                    f"warnings={validation_details.get('total_warnings', 0)}, "
                    f"errors={validation_details.get('total_errors', 0)}")

def validate_config_interactive(silent: bool = False):
    """
    Interactive configuration validation with comprehensive analysis and recommendations.
    
    This enhanced implementation provides:
    - Interactive validation with detailed progress tracking
    - Comprehensive system analysis and recommendations
    - Preset compatibility checking and suggestions
    - Resource requirement estimation and warnings
    - Actionable recommendations for configuration improvements
    - User-friendly error reporting and auto-fix suggestions
    
    Args:
        silent: If True, suppress detailed logging messages and progress bars during system checks
    """
    try:
        # Clear screen and show banner
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        metadata_config = config.get('metadata', {})
        training_config = config.get('training', {})
        model_config = config.get('model', {})
        data_config = config.get('data', {})
        system_config = config.get('system', {})
        presets_config = config.get('presets', {})
        hpo_config = config.get('hyperparameter_optimization', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        config_status = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
            config_status = config["runtime"].get("config_status", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
            config_status = config["metadata"].get("config_status", "Unknown")
        
        # Get configuration health if available
        health_status = "unknown"
        health_color = Fore.WHITE
        if "runtime" in config and isinstance(config["runtime"], dict):
            runtime = config["runtime"]
            if "configuration_health" in runtime:
                health = runtime["configuration_health"]
                health_status = health.get("status", "unknown")
                if health_status in ["healthy", "passed"]:
                    health_color = Fore.GREEN
                elif health_status in ["needs_attention", "warning"]:
                    health_color = Fore.YELLOW
                else:
                    health_color = Fore.RED
        
        preset_count = len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        config_version = metadata_config.get('config_version', 'unknown')
        
        # Header with context display
        if not silent:
            print(Fore.MAGENTA + Style.BRIGHT + "INTERACTIVE CONFIGURATION VALIDATION")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
            print(Fore.YELLOW + Style.BRIGHT + "Configuration Context:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Version: " + Fore.CYAN + Style.BRIGHT + f"{config_version}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Status: " + health_color + Style.BRIGHT + f"{config_status}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Sections: " + Fore.CYAN + Style.BRIGHT + f"{len(config) if isinstance(config, dict) else 'Unknown'}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Health: " + health_color + Style.BRIGHT + f"{health_status}")
            
            # Display key parameters
            print(Fore.CYAN + Style.BRIGHT + "\nKey Configuration Parameters:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Training Batch Size: " + Fore.CYAN + Style.BRIGHT + f"{training_config.get('batch_size', 'N/A')}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Training Epochs: " + Fore.CYAN + Style.BRIGHT + f"{training_config.get('epochs', 'N/A')}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Encoding Dimension: " + Fore.CYAN + Style.BRIGHT + f"{model_config.get('encoding_dim', 'N/A')}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.CYAN + Style.BRIGHT + f"{training_config.get('learning_rate', 'N/A')}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Optimizer: " + Fore.CYAN + Style.BRIGHT + f"{training_config.get('optimizer', 'N/A')}")
        
        # Initialize validation context
        validation_context = {
            'start_time': time.time(),
            'config_loaded': False,
            'validation_passed': False,
            'system_analysis_completed': False,
            'recommendations_generated': False
        }
        
        # Step 1: Load and analyze configuration with hardware context
        if not silent:
            print(Fore.YELLOW + Style.BRIGHT + "\nLoading Configuration and System Analysis...")
        
        hardware_data = None
        system_analysis = None
        
        # Use text-based progress if silent mode is enabled
        if silent:
            try:
                # Get hardware information
                hardware_data = check_hardware(include_memory_usage=True)
                
                # Perform system analysis
                system_analysis = get_system_info(
                    include_versions=True,
                    include_hardware=True,
                    include_memory_usage=True,
                    include_detailed_analysis=True
                )
                validation_context['system_analysis_completed'] = True
                validation_context['config_loaded'] = True
                
            except Exception as e:
                print(Fore.RED + Style.BRIGHT + f"\nError loading configuration: {str(e)}")
                logger.error(f"Configuration load failed: {e}", exc_info=True)
                return
        else:
            # Use progress bar for non-silent mode
            with alive_bar(5, title='Loading Configuration\t') as bar:
                try:
                    # Load configuration (already loaded from banner)
                    bar.text = 'Configuration loaded from banner...'
                    validation_context['config_loaded'] = True
                    bar()
                    time.sleep(0.1)
                    
                    # Get hardware information
                    bar.text = 'Detecting hardware capabilities...'
                    hardware_data = check_hardware(include_memory_usage=True)
                    bar()
                    time.sleep(0.1)
                    
                    # Perform system analysis
                    bar.text = 'Analyzing system configuration...'
                    system_analysis = get_system_info(
                        include_versions=True,
                        include_hardware=True,
                        include_memory_usage=True,
                        include_detailed_analysis=True
                    )
                    validation_context['system_analysis_completed'] = True
                    bar()
                    time.sleep(0.1)
                    
                    # Extract configuration details
                    bar.text = 'Extracting configuration details...'
                    config_version = config.get('metadata', {}).get('config_version', 'unknown')
                    bar()
                    time.sleep(0.1)
                    
                    # Complete loading phase
                    bar.text = 'Configuration loaded successfully'
                    bar()
                    
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + f"\nError loading configuration: {str(e)}")
                    logger.error(f"Configuration load failed: {e}", exc_info=True)
                    return
        
        # Display hardware summary
        if not silent:
            print(Fore.YELLOW + Style.BRIGHT + "\nHARDWARE SUMMARY")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Hardware-aware system class detection
        cuda_available = hardware_data.get('cuda', {}).get('available', False)
        memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)
        
        # Determine system performance class for resource optimization
        if cuda_available and memory_gb >= 16 and cpu_cores >= 8:
            system_class = "high_performance"
        elif cuda_available and memory_gb >= 8:
            system_class = "performance"
        elif memory_gb >= 4:
            system_class = "standard"
        else:
            system_class = "limited"
        
        # Display current hardware context
        if not silent:
            print(Fore.MAGENTA + Style.BRIGHT + "Hardware Context:")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.CYAN + Style.BRIGHT + f"{cuda_available}")
            if cuda_available:
                gpu_count = hardware_data.get('cuda', {}).get('gpu_count', 0)
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ GPU Count: " + Fore.CYAN + Style.BRIGHT + f"{gpu_count}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ System Memory: " + Fore.CYAN + Style.BRIGHT + f"{memory_gb:.1f}GB")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ CPU Cores: " + Fore.CYAN + Style.BRIGHT + f"{cpu_cores}")
            print(Fore.WHITE + Style.BRIGHT + f"  └─ System Class: " + Fore.CYAN + Style.BRIGHT + f"{system_class}")
        
        # Step 2: Perform comprehensive validation
        if not silent:
            print(Fore.YELLOW + Style.BRIGHT + "\nVALIDATION PROCESS")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)

        validation_errors = []
        validation_warnings = []
        validation_passed = False
        checks_performed = 0  # Initialize checks counter

        # Use text-based progress if silent mode is enabled
        if silent:
            try:
                # Run validation phases without progress bars
                validation_passed, validation_errors, validation_warnings = validate_config(config, strict=False)
                checks_performed += 1  # Count comprehensive validation
                validation_context['validation_passed'] = validation_passed
                
                # Check preset compatibility
                is_compatible = validate_model_preset_compatibility(model_type, config)
                checks_performed += 1  # Count preset compatibility check
                
            except Exception as e:
                validation_errors.append(f"Validation process failed: {str(e)}")
                logger.error(f"Validation error: {e}", exc_info=True)
        else:
            # Use progress bar for non-silent mode
            with alive_bar(10, title='Validating Configuration') as bar:
                try:
                    # Phase 1: Structural validation
                    bar.text = 'Validating configuration structure...'
                    checks_performed += 1
                    bar()
                    time.sleep(0.1)
                    
                    # Phase 2: Training configuration
                    bar.text = 'Validating training parameters...'
                    checks_performed += 1
                    bar()
                    time.sleep(0.1)
                    
                    # Phase 3: Model configuration
                    bar.text = 'Validating model architecture...'
                    checks_performed += 1
                    bar()
                    time.sleep(0.1)
                    
                    # Phase 4: Security configuration
                    bar.text = 'Validating security settings...'
                    checks_performed += 1
                    bar()
                    time.sleep(0.1)
                    
                    # Phase 5: Data configuration
                    bar.text = 'Validating data parameters...'
                    checks_performed += 1
                    bar()
                    time.sleep(0.1)
                    
                    # Phase 6: Hardware compatibility
                    bar.text = 'Checking hardware compatibility...'
                    checks_performed += 1
                    bar()
                    time.sleep(0.1)
                    
                    # Phase 7: Preset compatibility
                    bar.text = 'Validating preset compatibility...'
                    is_compatible = validate_model_preset_compatibility(model_type, config)
                    checks_performed += 1
                    bar()
                    time.sleep(0.1)
                    
                    # Phase 8: Cross-section validation
                    bar.text = 'Performing cross-section validation...'
                    checks_performed += 1
                    bar()
                    time.sleep(0.1)
                    
                    # Phase 9: Comprehensive validation
                    bar.text = 'Running comprehensive validation checks...'
                    validation_passed, validation_errors, validation_warnings = validate_config(config, strict=False)
                    checks_performed += 1
                    validation_context['validation_passed'] = validation_passed
                    bar()
                    time.sleep(0.1)
                    
                    # Phase 10: Complete
                    bar.text = 'Validation complete'
                    bar()
                    
                except Exception as e:
                    validation_errors.append(f"Validation process failed: {str(e)}")
                    logger.error(f"Validation error: {e}", exc_info=True)

        # Display validation results
        if not silent:
            if validation_passed:
                print(Fore.GREEN + Style.BRIGHT + "\n" + "-" * 40)
                print(Fore.GREEN + Style.BRIGHT + "VALIDATION RESULTS:")
                print(Fore.GREEN + Style.BRIGHT + "\nVALIDATION PASSED")
                print(Fore.GREEN + Style.BRIGHT + f"Configuration is valid with {len(validation_warnings)} warnings")
                print(Fore.GREEN + Style.BRIGHT + "-" * 40)
            else:
                print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                print(Fore.RED + Style.BRIGHT + "VALIDATION RESULTS:")
                print(Fore.RED + Style.BRIGHT + "\nVALIDATION FAILED")
                print(Fore.RED + Style.BRIGHT + f"Found {len(validation_errors)} errors and {len(validation_warnings)} warnings")
                print(Fore.RED + Style.BRIGHT + "-" * 40)
            
            # Display errors
            if validation_errors:
                print(Fore.RED + Style.BRIGHT + "\nValidation Errors:")
                for i, error in enumerate(validation_errors, 1):
                    prefix = "  └─" if i == len(validation_errors) else "  ├─"
                    print(Fore.RED + Style.BRIGHT + f"{prefix} {error}")
            
            # Display warnings
            if validation_warnings:
                print(Fore.YELLOW + Style.BRIGHT + "\nValidation Warnings:")
                for i, warning in enumerate(validation_warnings, 1):
                    prefix = "  └─" if i == len(validation_warnings) else "  ├─"
                    print(Fore.YELLOW + Style.BRIGHT + f"{prefix} {warning}")

        # Step 3: Configuration Analysis
        if validation_context['config_loaded'] and validation_context['system_analysis_completed']:
            if not silent:
                print(Fore.YELLOW + Style.BRIGHT + "\nCONFIGURATION ANALYSIS")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            analysis_results = {}
            
            # Use text-based progress if silent mode is enabled
            if silent:
                try:
                    # Run analysis phases without progress bars
                    complexity = estimate_config_complexity(config)
                    analysis_results['complexity'] = complexity
                    checks_performed += 1
                    
                    memory_req = estimate_memory_requirements(config)
                    analysis_results['memory'] = memory_req
                    checks_performed += 1
                    
                    training_time = estimate_training_time(config)
                    analysis_results['training_time'] = training_time
                    checks_performed += 1
                    
                    resource_level = determine_resource_level(config)
                    analysis_results['resource_level'] = resource_level
                    checks_performed += 1
                    
                    preset_recommendations = determine_preset_recommendations(config)
                    analysis_results['recommendations'] = preset_recommendations
                    checks_performed += 1
                    
                    validation_context['recommendations_generated'] = True
                    
                except Exception as e:
                    if not silent:
                        print(Fore.RED + Style.BRIGHT + f"  └─ Analysis failed: {str(e)}")
                    logger.error(f"Configuration analysis failed: {e}", exc_info=True)
            else:
                # Use progress bar for non-silent mode
                with alive_bar(6, title='Analyzing Configuration\t') as bar:
                    try:
                        # Complexity analysis
                        bar.text = 'Analyzing configuration complexity...'
                        complexity = estimate_config_complexity(config)
                        analysis_results['complexity'] = complexity
                        checks_performed += 1
                        bar()
                        time.sleep(0.1)
                        
                        # Resource estimation
                        bar.text = 'Estimating resource requirements...'
                        memory_req = estimate_memory_requirements(config)
                        analysis_results['memory'] = memory_req
                        checks_performed += 1
                        bar()
                        time.sleep(0.1)
                        
                        # Training time estimation
                        bar.text = 'Estimating training time...'
                        training_time = estimate_training_time(config)
                        analysis_results['training_time'] = training_time
                        checks_performed += 1
                        bar()
                        time.sleep(0.1)
                        
                        # Resource level determination
                        bar.text = 'Determining resource level...'
                        resource_level = determine_resource_level(config)
                        analysis_results['resource_level'] = resource_level
                        checks_performed += 1
                        bar()
                        time.sleep(0.1)
                        
                        # Preset recommendations
                        bar.text = 'Generating preset recommendations...'
                        preset_recommendations = determine_preset_recommendations(config)
                        analysis_results['recommendations'] = preset_recommendations
                        checks_performed += 1
                        bar()
                        time.sleep(0.1)
                        
                        # Complete analysis
                        bar.text = 'Analysis complete'
                        validation_context['recommendations_generated'] = True
                        bar()
                        
                    except Exception as e:
                        print(Fore.RED + Style.BRIGHT + f"  └─ Analysis failed: {str(e)}")
                        logger.error(f"Configuration analysis failed: {e}", exc_info=True)
            
            # Display analysis results
            if not silent:
                print(Fore.CYAN + Style.BRIGHT + "\nAnalysis Results:")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Configuration Complexity: " + Fore.CYAN + Style.BRIGHT + f"{complexity}")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Memory Requirements: " + Fore.CYAN + Style.BRIGHT + f"{memory_req}")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Training Time Estimate: " + Fore.CYAN + Style.BRIGHT + f"{training_time}")
                print(Fore.WHITE + Style.BRIGHT + f"  └─ Resource Level: " + Fore.CYAN + Style.BRIGHT + f"{resource_level}")

        # Step 4: System Compatibility Check
        compatibility = {}
        if system_analysis and not silent:
            print(Fore.YELLOW + Style.BRIGHT + "\nSYSTEM COMPATIBILITY")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            # Check preset-system compatibility
            compatibility = _check_preset_system_compatibility(config, system_analysis)
            checks_performed += 1
            
            if compatibility.get('overall_compatible', True):
                print(Fore.GREEN + Style.BRIGHT + "\nSystem is compatible with current configuration")
            else:
                print(Fore.YELLOW + Style.BRIGHT + "\nSystem compatibility issues detected")
            
            if compatibility.get('warnings'):
                print(Fore.YELLOW + Style.BRIGHT + "\nCompatibility Warnings:")
                for i, warning in enumerate(compatibility['warnings'], 1):
                    prefix = "  └─" if i == len(compatibility['warnings']) else "  ├─"
                    print(Fore.YELLOW + Style.BRIGHT + f"{prefix} {warning}")
            
            if compatibility.get('recommendations'):
                print(Fore.CYAN + Style.BRIGHT + "\nCompatibility Recommendations:")
                for i, rec in enumerate(compatibility['recommendations'], 1):
                    prefix = "  └─" if i == len(compatibility['recommendations']) else "  ├─"
                    print(Fore.CYAN + Style.BRIGHT + f"{prefix} {rec}")
            
            # Recommend optimal preset
            recommended_preset = _recommend_preset_for_system(system_analysis)
            checks_performed += 1
            if recommended_preset != preset_name:
                print(Fore.CYAN + Style.BRIGHT + f"\nRecommended Preset for Your System: " + Fore.YELLOW + Style.BRIGHT + f"{recommended_preset}")
                print(Fore.WHITE + Style.BRIGHT + f"Current preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")

        # Step 5: Usage Recommendations
        if analysis_results.get('recommendations') and not silent:
            print(Fore.YELLOW + Style.BRIGHT + "\nRECOMMENDED USE CASES")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            print(Fore.GREEN + Style.BRIGHT + "This configuration is recommended for:")
            
            # Group recommendations by category
            recommendations = analysis_results['recommendations']
            max_display = 10
            
            for i, rec in enumerate(recommendations[:max_display], 1):
                prefix = "  └─" if i == len(recommendations[:max_display]) else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {rec}")
            
            if len(recommendations) > max_display:
                print(Fore.GREEN + Style.BRIGHT + f"  └─ ... and {len(recommendations) - max_display} more use cases")

        # Step 6: Summary and Final Report
        if not silent:
            print(Fore.YELLOW + Style.BRIGHT + "\nVALIDATION SUMMARY")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            # Use the actual checks_performed count instead of calculating from errors/warnings
            if validation_passed:
                if len(validation_warnings) == 0:
                    validation_quality = "Excellent"
                    quality_color = Fore.GREEN
                elif len(validation_warnings) <= 5:
                    validation_quality = "Good"
                    quality_color = Fore.GREEN
                else:
                    validation_quality = "Acceptable"
                    quality_color = Fore.YELLOW
            else:
                validation_quality = "Failed"
                quality_color = Fore.RED
            
            print(Fore.CYAN + Style.BRIGHT + "Summary Information:")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Validation Status: " + quality_color + Style.BRIGHT + f"{validation_quality}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Total Checks Performed: " + Fore.CYAN + Style.BRIGHT + f"{checks_performed}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Errors Found: " + Fore.RED + Style.BRIGHT + f"{len(validation_errors)}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Warnings Found: " + Fore.YELLOW + Style.BRIGHT + f"{len(validation_warnings)}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ System Compatibility: " + (Fore.GREEN + Style.BRIGHT + "Compatible" if compatibility.get('overall_compatible', True) else Fore.YELLOW + Style.BRIGHT + "Issues Detected"))
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset Compatibility: " + (Fore.GREEN + Style.BRIGHT + "Compatible" if is_compatible else Fore.RED + Style.BRIGHT + "Incompatible"))
            
            elapsed_time = time.time() - validation_context['start_time']
            print(Fore.WHITE + Style.BRIGHT + f"  └─ Validation Time: " + Fore.CYAN + Style.BRIGHT + f"{elapsed_time:.2f} seconds")
            
            # Step 7: Interactive Options
            #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-" * 40)
            print(Fore.YELLOW + Style.BRIGHT + "VALIDATION INTERACTIVE MENU")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            print(Fore.YELLOW + Style.BRIGHT + "Validation Context:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Status: " + quality_color + Style.BRIGHT + f"{validation_quality}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Errors: " + Fore.RED + Style.BRIGHT + f"{len(validation_errors)}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Warnings: " + Fore.YELLOW + Style.BRIGHT + f"{len(validation_warnings)}")
            
            # Prepare menu categories with styling
            categories = []
            categories.append("1. View Detailed Validation Report " + Fore.GREEN + Style.BRIGHT + "(Comprehensive Analysis)")
            categories.append("2. Export Validation Results " + Fore.GREEN + Style.BRIGHT + "(JSON/TXT Formats)")
            categories.append("3. Apply Recommended Fixes " + Fore.GREEN + Style.BRIGHT + "(Auto-correction)")
            categories.append("4. Switch to Recommended Preset " + Fore.GREEN + Style.BRIGHT + "(System Optimized)")
            categories.append("5. Show Configuration Comparison " + Fore.GREEN + Style.BRIGHT + "(Before/After)")
            categories.append("6. View System Compatibility " + Fore.GREEN + Style.BRIGHT + "(Hardware Analysis)")
            categories.append("7. Generate Fix Report " + Fore.GREEN + Style.BRIGHT + "(Action Plan)")
            categories.append(Fore.RED + Style.BRIGHT + "0. Return to Main Menu")
            
            def display_validation_menu():
                """Display the validation menu categories with styling."""
                print(Fore.YELLOW + Style.BRIGHT + "\nAvailable Validation Options:")
                for category in categories:
                    if category.startswith("0."):
                        print(f"  {category}")
                    else:
                        main_part, separator, desc_part = category.partition(' (')
                        if separator:
                            print(f"  {Fore.WHITE + Style.BRIGHT}{main_part}{Style.RESET_ALL}{separator}{desc_part}")
                        else:
                            print(f"  {Fore.WHITE + Style.BRIGHT}{category}{Style.RESET_ALL}")
            
            # Display menu initially
            display_validation_menu()
            
            # Validation report function
            def _show_detailed_validation_report():
                """Display validation report with formatting."""
                try:
                    print(Fore.GREEN + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.GREEN + Style.BRIGHT + "VALIDATION REPORT")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    print(Fore.CYAN + Style.BRIGHT + "Detailed Validation Report")
                    print(Fore.WHITE + Style.BRIGHT + f"Configuration: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
                    print(Fore.WHITE + Style.BRIGHT + f"Model Type: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
                    print(Fore.WHITE + Style.BRIGHT + f"Validation Quality: " + quality_color + Style.BRIGHT + f"{validation_quality}")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nConfiguration Details:")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ Configuration Version: " + Fore.CYAN + Style.BRIGHT + f"{config.get('metadata', {}).get('config_version', 'unknown')}")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model Type: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ Complexity: " + Fore.CYAN + Style.BRIGHT + f"{analysis_results.get('complexity', 'N/A')}")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ Memory Requirements: " + Fore.CYAN + Style.BRIGHT + f"{analysis_results.get('memory', 'N/A')}")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ Training Time: " + Fore.CYAN + Style.BRIGHT + f"{analysis_results.get('training_time', 'N/A')}")
                    print(Fore.WHITE + Style.BRIGHT + f"  └─ Resource Level: " + Fore.CYAN + Style.BRIGHT + f"{analysis_results.get('resource_level', 'N/A')}")
                    
                    # Show detailed error analysis if any
                    if validation_errors:
                        print(Fore.RED + Style.BRIGHT + "\nDetailed Error Analysis:")
                        for i, error in enumerate(validation_errors, 1):
                            print(Fore.RED + Style.BRIGHT + f"  {i}. {error}")
                    
                    # Show detailed warning analysis if any
                    if validation_warnings:
                        print(Fore.YELLOW + Style.BRIGHT + "\nDetailed Warning Analysis:")
                        for i, warning in enumerate(validation_warnings, 1):
                            print(Fore.YELLOW + Style.BRIGHT + f"  {i}. {warning}")
                            
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "REPORT ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error displaying detailed report: {str(e)}")
                    print(Fore.YELLOW + Style.BRIGHT + "Please check the validation logs for more information.")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
            
            # Export function
            def _export_validation_results():
                """Export validation results with multiple format options."""
                try:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    
                    # Ask for format preference
                    print(Fore.CYAN + Style.BRIGHT + "\nExport Formats Available:")
                    print(Fore.WHITE + Style.BRIGHT + "1. JSON Report " + Fore.GREEN + Style.BRIGHT + "(Machine-readable)")
                    print(Fore.WHITE + Style.BRIGHT + "2. Text Summary " + Fore.GREEN + Style.BRIGHT + "(Human-readable)")
                    print(Fore.WHITE + Style.BRIGHT + "3. Both Formats " + Fore.GREEN + Style.BRIGHT + "(Complete Set)")
                    
                    format_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect export format (1-3): ").strip()
                    
                    report_data = {
                        'timestamp': datetime.now().isoformat(),
                        'validation_passed': validation_passed,
                        'validation_quality': validation_quality,
                        'errors': validation_errors,
                        'warnings': validation_warnings,
                        'analysis': analysis_results,
                        'compatibility': compatibility,
                        'system_info': {
                            'current_preset': preset_name,
                            'model_type': model_type,
                            'config_version': config.get('metadata', {}).get('config_version', 'unknown'),
                            'system_class': system_class,
                            'cuda_available': cuda_available,
                            'memory_gb': memory_gb
                        }
                    }
                    
                    export_success = False
                    
                    if format_choice in ['1', '3']:  # JSON export
                        json_file = REPORTS_DIR / f"validation_report_{timestamp}.json"
                        try:
                            with open(json_file, 'w', encoding='utf-8') as f:
                                json.dump(report_data, f, indent=2, ensure_ascii=False)
                            print(Fore.GREEN + Style.BRIGHT + f"  JSON report exported to: {json_file}")
                            export_success = True
                        except Exception as e:
                            print(Fore.RED + Style.BRIGHT + f"  Failed to export JSON: {e}")
                    
                    if format_choice in ['2', '3']:  # Text export
                        txt_file = REPORTS_DIR / f"validation_summary_{timestamp}.txt"
                        try:
                            with open(txt_file, 'w', encoding='utf-8') as f:
                                f.write(f"Validation Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                                f.write("=" * 50 + "\n\n")
                                f.write(f"Preset: {preset_name}\n")
                                f.write(f"Model Type: {model_type}\n")
                                f.write(f"Validation Status: {validation_quality}\n")
                                f.write(f"Errors: {len(validation_errors)}\n")
                                f.write(f"Warnings: {len(validation_warnings)}\n\n")
                                
                                if validation_errors:
                                    f.write("ERRORS:\n")
                                    for error in validation_errors:
                                        f.write(f"  - {error}\n")
                                    f.write("\n")
                                
                                if validation_warnings:
                                    f.write("WARNINGS:\n")
                                    for warning in validation_warnings:
                                        f.write(f"  - {warning}\n")
                                    f.write("\n")
                                
                                f.write("ANALYSIS:\n")
                                f.write(f"  Complexity: {analysis_results.get('complexity', 'N/A')}\n")
                                f.write(f"  Memory Requirements: {analysis_results.get('memory', 'N/A')}\n")
                                f.write(f"  Training Time: {analysis_results.get('training_time', 'N/A')}\n")
                                f.write(f"  Resource Level: {analysis_results.get('resource_level', 'N/A')}\n")
                            
                            print(Fore.GREEN + Style.BRIGHT + f"  Text summary exported to: {txt_file}")
                            export_success = True
                        except Exception as e:
                            print(Fore.RED + Style.BRIGHT + f"  Failed to export text: {e}")
                    
                    if export_success:
                        print(Fore.GREEN + Style.BRIGHT + "\n" + "-" * 40)
                        print(Fore.GREEN + Style.BRIGHT + "EXPORT SUCCESS")
                        print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                        print(Fore.GREEN + Style.BRIGHT + "Validation results exported successfully!")
                        print(Fore.WHITE + Style.BRIGHT + "Files saved to reports directory.")
                        print(Fore.CYAN + Style.BRIGHT + "Use these reports for:")
                        print(Fore.WHITE + Style.BRIGHT + "- Configuration debugging")
                        print(Fore.WHITE + Style.BRIGHT + "- Performance analysis")
                        print(Fore.WHITE + Style.BRIGHT + "- System optimization")
                        print(Fore.WHITE + Style.BRIGHT + "- Documentation purposes")
                        print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    else:
                        print(Fore.RED + Style.BRIGHT + "\nNo files were exported due to errors.")
                        
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "EXPORT ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Export failed: {str(e)}")
                    print(Fore.YELLOW + Style.BRIGHT + "Please check directory permissions and try again.")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
            
            # Preset switching
            def _switch_to_recommended_preset():
                """Switch to system-recommended preset with confirmation."""
                try:
                    if system_analysis:
                        recommended = _recommend_preset_for_system(system_analysis)
                        print(Fore.CYAN + Style.BRIGHT + "\n" + "-" * 40)
                        print(Fore.CYAN + Style.BRIGHT + "PRESET RECOMMENDATION")
                        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                        print(Fore.WHITE + Style.BRIGHT + "System Analysis Complete")
                        print(Fore.WHITE + Style.BRIGHT + f"Current Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
                        print(Fore.WHITE + Style.BRIGHT + f"Recommended Preset: " + Fore.YELLOW + Style.BRIGHT + f"{recommended}")
                        print(Fore.WHITE + Style.BRIGHT + f"System Class: " + Fore.CYAN + Style.BRIGHT + f"{system_class}")
                        print(Fore.WHITE + Style.BRIGHT + f"Memory: " + Fore.CYAN + Style.BRIGHT + f"{memory_gb:.1f}GB")
                        print(Fore.WHITE + Style.BRIGHT + f"CUDA: " + Fore.CYAN + Style.BRIGHT + f"{'Available' if cuda_available else 'Not Available'}")
                        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                        
                        if recommended != preset_name:
                            response = input(Fore.YELLOW + Style.BRIGHT + f"Switch to '{recommended}' preset? (y/N): ").strip().lower()
                            if response == 'y':
                                try:
                                    # Load and apply preset
                                    if recommended in PRESET_CONFIGS:
                                        new_config = deepcopy(PRESET_CONFIGS[recommended])
                                        update_global_config(new_config)
                                        save_config(new_config)
                                        
                                        print(Fore.GREEN + Style.BRIGHT + "\n" + "-" * 40)
                                        print(Fore.GREEN + Style.BRIGHT + "PRESET SWITCHED")
                                        print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                                        print(Fore.GREEN + Style.BRIGHT + f"Successfully switched to {recommended} preset!")
                                        print(Fore.WHITE + Style.BRIGHT + "Configuration has been updated and saved.")
                                        print(Fore.YELLOW + Style.BRIGHT + "Please restart validation to verify new configuration.")
                                        print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                                        return True  # Indicate that preset was changed
                                    else:
                                        print(Fore.RED + Style.BRIGHT + f"Preset '{recommended}' not found in available presets")
                                except Exception as e:
                                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                                    print(Fore.RED + Style.BRIGHT + "SWITCH ERROR")
                                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                                    print(Fore.RED + Style.BRIGHT + f"Failed to switch preset: {str(e)}")
                                    print(Fore.YELLOW + Style.BRIGHT + "Please check preset configuration and try again.")
                                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                            else:
                                print(Fore.YELLOW + Style.BRIGHT + "Preset switch cancelled.")
                        else:
                            print(Fore.GREEN + Style.BRIGHT + "Already using recommended preset")
                    else:
                        print(Fore.RED + Style.BRIGHT + "System analysis not available for preset recommendation.")
                    
                    return False
                    
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "PRESET ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Preset switch error: {str(e)}")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    return False
            
            # Compatibility display
            def _show_system_compatibility():
                """Display system compatibility information."""
                try:
                    print(Fore.GREEN + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.GREEN + Style.BRIGHT + "SYSTEM COMPATIBILITY")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    print(Fore.CYAN + Style.BRIGHT + "System Compatibility Analysis")
                    print(Fore.WHITE + Style.BRIGHT + f"Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
                    print(Fore.WHITE + Style.BRIGHT + f"System Class: " + Fore.CYAN + Style.BRIGHT + f"{system_class}")
                    compatible_status = "Compatible" if compatibility.get('overall_compatible', True) else "Issues Detected"
                    status_color = Fore.GREEN if compatibility.get('overall_compatible', True) else Fore.YELLOW
                    print(Fore.WHITE + Style.BRIGHT + f"Overall Compatible: " + status_color + Style.BRIGHT + f"{compatible_status}")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nHardware Analysis:")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ CUDA Available: " + (Fore.GREEN + Style.BRIGHT + "Yes" if cuda_available else Fore.YELLOW + Style.BRIGHT + "No"))
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ System Memory: " + Fore.CYAN + Style.BRIGHT + f"{memory_gb:.1f}GB")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ CPU Cores: " + Fore.CYAN + Style.BRIGHT + f"{cpu_cores}")
                    print(Fore.WHITE + Style.BRIGHT + f"  └─ Performance Class: " + Fore.CYAN + Style.BRIGHT + f"{system_class}")
                    
                    if compatibility.get('warnings'):
                        print(Fore.YELLOW + Style.BRIGHT + "\nCompatibility Warnings:")
                        for i, warning in enumerate(compatibility['warnings'], 1):
                            print(Fore.YELLOW + Style.BRIGHT + f"  {i}. {warning}")
                    
                    if compatibility.get('recommendations'):
                        print(Fore.CYAN + Style.BRIGHT + "\nOptimization Recommendations:")
                        for i, rec in enumerate(compatibility['recommendations'], 1):
                            print(Fore.CYAN + Style.BRIGHT + f"  {i}. {rec}")
                            
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "COMPATIBILITY ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error displaying compatibility: {str(e)}")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
            
            # Handle user interaction with error handling
            while True:
                try:
                    choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-7): ").strip()
                    
                    if choice == '0':
                        print(Fore.RED + Style.BRIGHT + "\nValidation session ended")
                        break
                    
                    elif choice == '1':
                        _show_detailed_validation_report()
                        display_validation_menu()
                    
                    elif choice == '2':
                        _export_validation_results()
                        display_validation_menu()
                    
                    elif choice == '3':
                        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-" * 40)
                        print(Fore.YELLOW + Style.BRIGHT + "AUTO-FIX ANALYSIS")
                        print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
                        print(Fore.WHITE + Style.BRIGHT + "Auto-fix functionality analysis:")
                        print(Fore.WHITE + Style.BRIGHT + f"- Found {len(validation_errors)} errors to address")
                        print(Fore.WHITE + Style.BRIGHT + f"- Found {len(validation_warnings)} warnings to review")
                        compat_status = "Good" if compatibility.get('overall_compatible', True) else "Needs Attention"
                        print(Fore.WHITE + Style.BRIGHT + f"- System compatibility: {compat_status}")
                        print(Fore.CYAN + Style.BRIGHT + "\nAuto-fix capabilities:")
                        print(Fore.GREEN + Style.BRIGHT + "- Basic configuration corrections")
                        print(Fore.GREEN + Style.BRIGHT + "- Preset optimization suggestions")
                        print(Fore.RED + Style.BRIGHT + "- Advanced error resolution (manual required)")
                        print(Fore.RED + Style.BRIGHT + "- Complex dependency fixes (manual required)")
                        print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
                        print(Fore.YELLOW + Style.BRIGHT + "\nPlease manually address the validation issues listed above.")
                        print(Fore.YELLOW + Style.BRIGHT + "For complex issues, consider using the preset switch option.")
                        display_validation_menu()
                    
                    elif choice == '4':
                        preset_changed = _switch_to_recommended_preset()
                        if preset_changed:
                            break  # Exit validation menu if preset was changed
                        else:
                            display_validation_menu()
                    
                    elif choice == '5':
                        print(Fore.CYAN + Style.BRIGHT + "\n" + "-" * 40)
                        print(Fore.CYAN + Style.BRIGHT + "CONFIGURATION COMPARISON")
                        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                        print(Fore.WHITE + Style.BRIGHT + "Configuration comparison provides:")
                        print(Fore.WHITE + Style.BRIGHT + "- Side-by-side analysis of current vs recommended settings")
                        print(Fore.WHITE + Style.BRIGHT + "- Performance impact assessment")
                        print(Fore.WHITE + Style.BRIGHT + "- Resource utilization comparison")
                        print(Fore.WHITE + Style.BRIGHT + "- Compatibility scoring")
                        print(Fore.CYAN + Style.BRIGHT + "\nThis feature requires:")
                        print(Fore.GREEN + Style.BRIGHT + "- Multiple configuration profiles")
                        print(Fore.GREEN + Style.BRIGHT + "- Performance benchmarking data")
                        print(Fore.GREEN + Style.BRIGHT + "- Advanced analysis algorithms")
                        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                        print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration comparison feature coming soon!")
                        display_validation_menu()
                    
                    elif choice == '6':
                        _show_system_compatibility()
                        display_validation_menu()
                    
                    elif choice == '7':
                        print(Fore.GREEN + Style.BRIGHT + "\n" + "-" * 40)
                        print(Fore.GREEN + Style.BRIGHT + "FIX ACTION PLAN")
                        print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                        print(Fore.WHITE + Style.BRIGHT + "Fix Report Generation")
                        print(Fore.WHITE + Style.BRIGHT + "Based on current validation results:")
                        print(Fore.RED + Style.BRIGHT + f"- {len(validation_errors)} critical issues to resolve")
                        print(Fore.YELLOW + Style.BRIGHT + f"- {len(validation_warnings)} recommendations to consider")
                        print(Fore.CYAN + Style.BRIGHT + "- System optimization opportunities available")
                        print(Fore.GREEN + Style.BRIGHT + "\nAction Plan:")
                        print(Fore.WHITE + Style.BRIGHT + "1. Review all validation errors above")
                        print(Fore.WHITE + Style.BRIGHT + "2. Address compatibility warnings")
                        print(Fore.WHITE + Style.BRIGHT + "3. Consider preset optimization")
                        print(Fore.WHITE + Style.BRIGHT + "4. Verify hardware requirements")
                        print(Fore.WHITE + Style.BRIGHT + "5. Test configuration changes")
                        print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                        display_validation_menu()
                    
                    else:
                        print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-7.")
                        display_validation_menu()
                        
                except KeyboardInterrupt:
                    print(Fore.RED + Style.BRIGHT + "\nValidation menu interrupted by user")
                    break
                except Exception as e:
                    logger.error(f"Validation menu error: {e}", exc_info=True)
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "MENU ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Unexpected error in validation menu: {str(e)}")
                    print(Fore.YELLOW + Style.BRIGHT + "Please try again or check the logs for details.")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    display_validation_menu()
        
        # Final message
        if not silent:
            if validation_passed:
                print(Fore.GREEN + Style.BRIGHT + "\n" + "-" * 40)
                print(Fore.GREEN + Style.BRIGHT + "VALIDATION SUCCESS")
                print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                print(Fore.GREEN + Style.BRIGHT + "Configuration validation completed successfully!")
                print(Fore.GREEN + Style.BRIGHT + "- All validation checks passed")
                print(Fore.GREEN + Style.BRIGHT + "- System compatibility verified")
                print(Fore.GREEN + Style.BRIGHT + "- Configuration ready for use")
                print(Fore.GREEN + Style.BRIGHT + f"- {len(validation_warnings)} warnings reviewed")
                print(Fore.GREEN + Style.BRIGHT + "-" * 40)
            else:
                print(Fore.YELLOW + Style.BRIGHT + "\n" + "-" * 40)
                print(Fore.YELLOW + Style.BRIGHT + "VALIDATION ISSUES")
                print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
                print(Fore.YELLOW + Style.BRIGHT + "Configuration validation completed with issues")
                print(Fore.RED + Style.BRIGHT + f"- {len(validation_errors)} errors need attention")
                print(Fore.YELLOW + Style.BRIGHT + f"- {len(validation_warnings)} warnings to review")
                print(Fore.WHITE + Style.BRIGHT + "Please address the issues before proceeding")
                print(Fore.CYAN + Style.BRIGHT + "Use the interactive menu for detailed analysis")
                print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
        
        # Return validation results for programmatic use
        return {
            'validation_passed': validation_passed,
            'errors': validation_errors,
            'warnings': validation_warnings,
            'analysis': analysis_results,
            'compatibility': compatibility if system_analysis else {},
            'system_info': {
                'current_preset': preset_name,
                'model_type': model_type,
                'config_version': config.get('metadata', {}).get('config_version', 'unknown')
            }
        }
        
    except KeyboardInterrupt:
        if not silent:
            print(Fore.RED + Style.BRIGHT + "\nValidation cancelled by user")
        return {'validation_passed': False, 'errors': ['Validation cancelled by user'], 'warnings': []}
    except Exception as e:
        if not silent:
            print(Fore.RED + Style.BRIGHT + f"\nUnexpected error during validation: {e}")
        logger.error(f"Interactive validation failed: {e}", exc_info=True)
        
        if not silent and input(Fore.YELLOW + Style.BRIGHT + "\nShow detailed error information? (y/N): ").lower().strip() == 'y':
            print(Fore.RED + Style.BRIGHT + "\nError Details:")
            print(str(e))
            print(traceback.format_exc())
        
        return {'validation_passed': False, 'errors': [f'Unexpected error: {str(e)}'], 'warnings': []}

def get_default_config() -> Dict[str, Any]:
    """Get comprehensive default system configuration leveraging PRESET_CONFIGS and system analysis.
    
    This function loads the default preset from PRESET_CONFIGS and enhances it with
    comprehensive system information and runtime-specific updates. It provides intelligent
    system-aware configuration with performance optimizations and compatibility checks.
    
    Returns:
        Dictionary containing the complete default configuration with system analysis
    """
    global _cached_config, _config_cache_time
    
    try:
        current_time = datetime.now().isoformat()
        
        # Check cache validity (30 seconds)
        if (_cached_config is not None and _config_cache_time is not None and 
            time.time() - _config_cache_time < 30):
            logger.debug("Returning cached default configuration")
            return _cached_config
        
        # INITIAL MEMORY OPTIMIZATION - Get hardware context early for memory-aware processing
        hardware_data = None
        total_ram_gb = 8.0  # Conservative default
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        except Exception as e:
            logger.debug(f"Hardware detection failed: {e}")
            hardware_data = {}
        
        # PRE-PROCESSING MEMORY OPTIMIZATION - Clear memory before intensive operations
        if total_ram_gb < 8:
            try:
                pre_clear = enhanced_clear_memory(
                    aggressive=total_ram_gb < 4,
                    hardware_data=hardware_data
                )
                if pre_clear.get('success'):
                    logger.debug("Pre-processing memory optimization completed")
            except Exception as e:
                logger.debug(f"Pre-processing memory optimization failed: {e}")
        
        # Load base configuration from PRESET_CONFIGS
        if 'default' in PRESET_CONFIGS and PRESET_CONFIGS['default']:
            base_config = deepcopy(PRESET_CONFIGS['default'])
            logger.debug("Loaded default configuration from PRESET_CONFIGS")
            config_source = 'PRESET_CONFIGS[default]'
        else:
            logger.warning("Default preset not found in PRESET_CONFIGS, creating minimal fallback")
            base_config = _create_minimal_fallback_config('minimal')
            config_source = 'minimal_fallback'
        
        # Validate configuration structure
        try:
            validate_config(base_config)
            logger.debug("Configuration validation passed")
        except Exception as e:
            logger.error(f"Configuration validation failed: {e}")
            # Use standard fallback instead of emergency
            base_config = _create_minimal_fallback_config('standard')
            config_source = 'validation_failure_fallback'
            # Re-validate fallback
            try:
                validate_config(base_config)
            except Exception as validation_error:
                logger.critical(f"Even fallback configuration failed validation: {validation_error}")
                # Use emergency fallback
                base_config = _create_minimal_fallback_config('emergency')
                config_source = 'emergency_fallback'
        
        # MEMORY OPTIMIZATION - Clear memory after validation for low-memory systems
        if total_ram_gb < 8:
            try:
                post_validation_clear = enhanced_clear_memory(
                    aggressive=total_ram_gb < 4,
                    hardware_data=hardware_data
                )
                if post_validation_clear.get('success'):
                    logger.debug("Post-validation memory optimization completed")
            except Exception as e:
                logger.debug(f"Post-validation memory optimization failed: {e}")
        
        # Gather comprehensive system information with memory-aware approach
        try:
            # For systems with limited RAM, use basic system info to avoid memory pressure
            if total_ram_gb < 4:
                system_analysis = _get_basic_system_info()
                logger.debug("Using basic system info due to memory constraints")
            else:
                system_analysis = get_system_info(
                    include_versions=True,
                    include_hardware=True, 
                    include_memory_usage=True,
                    include_detailed_analysis=total_ram_gb >= 8  # Only detailed analysis for systems with adequate RAM
                )
                analysis_duration = system_analysis.get('collection_metadata', {}).get('collection_duration_ms', 0)
                logger.debug(f"System analysis completed in {analysis_duration:.1f}ms")
        except Exception as e:
            logger.warning(f"System analysis failed, using basic system info: {e}")
            system_analysis = _get_basic_system_info()
        
        # MEMORY OPTIMIZATION - Clear memory after intensive system analysis
        system_analysis_size = len(str(system_analysis)) / (1024 * 1024)  # Size in MB
        if system_analysis_size > 1.0 or total_ram_gb < 8:
            try:
                post_analysis_clear = enhanced_clear_memory(
                    aggressive=system_analysis_size > 5.0 or total_ram_gb < 4,
                    hardware_data=hardware_data
                )
                if post_analysis_clear.get('success'):
                    logger.debug(f"Post-analysis memory optimization completed for {system_analysis_size:.1f}MB data")
            except Exception as e:
                logger.debug(f"Post-analysis memory optimization failed: {e}")
        
        # Update metadata with comprehensive system information
        if 'metadata' in base_config:
            base_config['metadata'].update({
                'last_accessed': current_time,
                'config_loaded_at': current_time,
                'config_source': config_source,
                'system_analysis_timestamp': system_analysis.get('timestamp', current_time),
                'system_analysis_quality': system_analysis.get('collection_metadata', {}).get('data_quality', 'unknown'),
                'config_generation_method': 'system_aware_default'
            })
            
            # Enhanced system information from analysis
            if 'system' in base_config['metadata']:
                base_config['metadata']['system'].update({
                    # Core system info from analysis
                    'python_version': system_analysis.get('python', {}).get('version_info', {}).get('version_tuple', platform.python_version()),
                    'platform': system_analysis.get('platform', {}).get('platform', platform.platform()),
                    'architecture': system_analysis.get('platform', {}).get('architecture', [platform.machine(), ''])[0] if isinstance(system_analysis.get('platform', {}).get('architecture', platform.machine()), list) else platform.machine(),
                    'processor': system_analysis.get('platform', {}).get('processor', platform.processor() or 'unknown'),
                    'hostname': system_analysis.get('platform', {}).get('node', platform.node()),
                    'os': system_analysis.get('platform', {}).get('system', platform.system()),
                    'os_release': system_analysis.get('platform', {}).get('release', platform.release()),
                    
                    # Hardware capabilities from analysis
                    'cpu_count': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('cpu', {}).get('logical_cores', os.cpu_count() or 1),
                    'cpu_performance_class': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('cpu', {}).get('performance_class', 'unknown'),
                    'memory_gb': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('memory', {}).get('total_gb', 0),
                    'memory_performance_class': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('memory', {}).get('performance_class', 'unknown'),
                    'system_performance_class': system_analysis.get('hardware_analysis', {}).get('system_class', 'unknown'),
                    'hardware_performance_score': system_analysis.get('hardware_analysis', {}).get('performance_score', 0),
                    
                    # Package environment health
                    'environment_health': system_analysis.get('package_analysis', {}).get('environment_health', {}),
                    'package_compatibility_score': system_analysis.get('package_analysis', {}).get('environment_health', {}).get('compatibility_score', 0),
                    
                    # Dynamic CUDA information
                    'cuda_available': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('available', False),
                    'cuda_devices': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('count', 0),
                    'cuda_total_memory_gb': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('total_memory_gb', 0),
                    'gpu_performance_class': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('performance_class', 'none'),
                })
                
                # Add PyTorch version information from package analysis
                if 'package_versions' in system_analysis:
                    torch_info = system_analysis['package_versions'].get('torch', {})
                    base_config['metadata']['system'].update({
                        'pytorch_version': torch_info.get('version', 'unknown'),
                        'pytorch_status': torch_info.get('status', 'unknown'),
                        'pytorch_compatible': torch_info.get('compatible', False),
                        'cuda_version': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('cuda_version', 'unknown')
                    })
        
        # MEMORY OPTIMIZATION - Clear memory after metadata processing for large configs
        if len(str(base_config.get('metadata', {}))) > 50000 and total_ram_gb < 16:
            try:
                metadata_clear = enhanced_clear_memory(
                    aggressive=False,
                    hardware_data=hardware_data
                )
                if metadata_clear.get('success'):
                    logger.debug("Metadata processing memory optimization completed")
            except Exception as e:
                logger.debug(f"Metadata processing memory optimization failed: {e}")
        
        # Apply system-aware optimizations to training configuration
        if 'training' in base_config and 'hardware_analysis' in system_analysis:
            hardware_caps = system_analysis['hardware_analysis'].get('capabilities', {})
            system_class = system_analysis['hardware_analysis'].get('system_class', 'unknown')
            
            # CPU-based optimizations
            cpu_info = hardware_caps.get('cpu', {})
            logical_cores = cpu_info.get('logical_cores', 1)
            
            # Optimize num_workers based on CPU cores and system class
            if system_class == 'high_performance':
                optimal_workers = min(8, max(4, logical_cores // 2))
            elif system_class == 'standard':
                optimal_workers = min(4, max(2, logical_cores // 2))
            else:  # limited
                optimal_workers = min(2, max(1, logical_cores // 4))
            
            # Only update if different from preset value
            current_workers = base_config['training'].get('num_workers', 1)
            if abs(optimal_workers - current_workers) > 1:
                base_config['training']['num_workers'] = optimal_workers
                logger.debug(f"Optimized num_workers: {current_workers} -> {optimal_workers}")
            
            # Memory-based optimizations
            memory_info = hardware_caps.get('memory', {})
            total_memory_gb = memory_info.get('total_gb', 0)
            current_batch_size = base_config['training'].get('batch_size', 64)
            
            # Adjust batch size based on available memory
            if total_memory_gb >= 32:
                # High memory system - can handle larger batches
                if current_batch_size < 128:
                    new_batch_size = min(128, current_batch_size * 2)
                    base_config['training']['batch_size'] = new_batch_size
                    logger.debug(f"Increased batch_size for high memory: {current_batch_size} -> {new_batch_size}")
            elif total_memory_gb > 0 and total_memory_gb < 8:
                # Low memory system - reduce batch size
                new_batch_size = min(32, current_batch_size)
                base_config['training']['batch_size'] = new_batch_size
                base_config['training']['gradient_accumulation_steps'] = max(2, base_config['training'].get('gradient_accumulation_steps', 1))
                logger.debug(f"Reduced batch_size for low memory: {current_batch_size} -> {new_batch_size}")
            
            # GPU-based optimizations  
            gpu_info = hardware_caps.get('gpu', {})
            cuda_available = gpu_info.get('available', False)
            gpu_memory_gb = gpu_info.get('total_memory_gb', 0)
            
            # Update CUDA-dependent settings
            base_config['training']['pin_memory'] = cuda_available
            base_config['training']['mixed_precision'] = cuda_available and gpu_memory_gb >= 4
            
            # Adjust batch size based on GPU memory
            if cuda_available and gpu_memory_gb > 0:
                current_batch_size = base_config['training'].get('batch_size', 64)
                
                if gpu_memory_gb >= 16:
                    # High-end GPU - can handle larger models and batches
                    new_batch_size = min(256, current_batch_size * 2)
                    if new_batch_size != current_batch_size:
                        base_config['training']['batch_size'] = new_batch_size
                        logger.debug(f"Increased batch_size for high-end GPU: {current_batch_size} -> {new_batch_size}")
                elif gpu_memory_gb < 4:
                    # Limited GPU memory - reduce batch size
                    new_batch_size = min(16, current_batch_size // 2)
                    base_config['training']['batch_size'] = max(1, new_batch_size)
                    base_config['training']['gradient_accumulation_steps'] = max(4, base_config['training'].get('gradient_accumulation_steps', 1))
                    logger.debug(f"Reduced batch_size for limited GPU: {current_batch_size} -> {new_batch_size}")
        
        # Apply system-aware model configuration optimizations
        if 'model' in base_config and 'hardware_analysis' in system_analysis:
            hardware_caps = system_analysis['hardware_analysis'].get('capabilities', {})
            system_class = system_analysis['hardware_analysis'].get('system_class', 'unknown')
            
            # Adjust model complexity based on system capabilities
            if system_class == 'limited':
                # Reduce model complexity for limited systems
                current_encoding_dim = base_config['model'].get('encoding_dim', 12)
                new_encoding_dim = min(8, current_encoding_dim)
                if new_encoding_dim != current_encoding_dim:
                    base_config['model']['encoding_dim'] = new_encoding_dim
                    logger.debug(f"Reduced encoding_dim for limited system: {current_encoding_dim} -> {new_encoding_dim}")
                
                # Reduce hidden layer sizes
                current_dims = base_config['model'].get('hidden_dims', [128, 64])
                new_dims = [min(64, dim) for dim in current_dims]
                if new_dims != current_dims:
                    base_config['model']['hidden_dims'] = new_dims
                    # Update dropout rates to match
                    current_dropout = base_config['model'].get('dropout_rates', [0.2, 0.15])
                    if len(current_dropout) > len(new_dims):
                        base_config['model']['dropout_rates'] = current_dropout[:len(new_dims)]
                    elif len(current_dropout) < len(new_dims):
                        base_config['model']['dropout_rates'] = current_dropout + [0.2] * (len(new_dims) - len(current_dropout))
                    logger.debug(f"Reduced hidden_dims for limited system: {current_dims} -> {new_dims}")
                
                # Force single model for ensembles
                if base_config['model'].get('model_type') == 'AutoencoderEnsemble':
                    base_config['model']['num_models'] = 1
                    logger.debug("Reduced ensemble size to 1 for limited system")
                
                # Reduce memory usage by disabling normalization
                base_config['model']['use_batch_norm'] = True
                base_config['model']['use_layer_norm'] = False
                logger.debug("Disabled normalization for limited system")
                
            elif system_class == 'high_performance':
                # Increase model complexity for high-performance systems
                gpu_info = hardware_caps.get('gpu', {})
                if gpu_info.get('available') and gpu_info.get('total_memory_gb', 0) >= 8:
                    # Can handle more complex models
                    current_encoding_dim = base_config['model'].get('encoding_dim', 12)
                    new_encoding_dim = max(16, current_encoding_dim)
                    if new_encoding_dim != current_encoding_dim:
                        base_config['model']['encoding_dim'] = new_encoding_dim
                        logger.debug(f"Increased encoding_dim for high-performance system: {current_encoding_dim} -> {new_encoding_dim}")
                    
                    current_dims = base_config['model'].get('hidden_dims', [128, 64])
                    if len(current_dims) < 3:
                        new_dims = [256, 128, 64]
                        base_config['model']['hidden_dims'] = new_dims
                        # Update dropout rates to match
                        base_config['model']['dropout_rates'] = [0.2, 0.15, 0.1]
                        logger.debug(f"Enhanced hidden_dims for high-performance system: {current_dims} -> {new_dims}")
        
        # MEMORY OPTIMIZATION - Clear memory after configuration optimizations
        config_complexity = len(str(base_config.get('model', {}))) + len(str(base_config.get('training', {})))
        if config_complexity > 10000 and total_ram_gb < 16:
            try:
                config_optimization_clear = enhanced_clear_memory(
                    aggressive=config_complexity > 50000,
                    hardware_data=hardware_data
                )
                if config_optimization_clear.get('success'):
                    logger.debug("Configuration optimization memory management completed")
            except Exception as e:
                logger.debug(f"Configuration optimization memory management failed: {e}")
        
        # Update hardware configuration with system-specific recommendations
        if 'hardware' in base_config and 'hardware_analysis' in system_analysis:
            hardware_caps = system_analysis['hardware_analysis'].get('capabilities', {})
            
            # Update hardware requirements based on actual system
            memory_gb = hardware_caps.get('memory', {}).get('total_gb', 0)
            gpu_memory_gb = hardware_caps.get('gpu', {}).get('total_memory_gb', 0)
            
            base_config['hardware'].update({
                'detected_gpu_memory': gpu_memory_gb,
                'detected_system_memory': memory_gb,
                'recommended_gpu_memory': max(4, min(gpu_memory_gb, base_config['hardware'].get('recommended_gpu_memory', 8))) if gpu_memory_gb > 0 else base_config['hardware'].get('recommended_gpu_memory', 8),
                'system_performance_class': system_analysis['hardware_analysis'].get('system_class', 'unknown'),
                'optimization_recommendations': system_analysis.get('detailed_analysis', {}).get('configuration_suggestions', [])
            })
            
            # Update performance optimization flags
            cuda_available = hardware_caps.get('gpu', {}).get('available', False)
            base_config['hardware']['performance_optimization'].update({
                'use_cuda': cuda_available,
                'use_amp': cuda_available and gpu_memory_gb >= 4,
                # Enable benchmarking if CUDA available
                'benchmark_mode': cuda_available,
                # Deterministic mode for CPU-only systems
                'deterministic': not cuda_available
            })
        
        # Update system paths and ensure directories exist
        if 'system' in base_config:
            # Ensure all required directories exist
            required_dirs = ['model_dir', 'log_dir', 'config_dir', 'data_dir', 'checkpoint_dir']
            for dir_key in required_dirs:
                if dir_key in base_config['system']:
                    dir_path = Path(base_config['system'][dir_key])
                    try:
                        dir_path.mkdir(parents=True, exist_ok=True)
                        logger.debug(f"Ensured directory exists: {dir_path}")
                    except Exception as e:
                        logger.warning(f"Could not create directory {dir_path}: {e}")
            
            # Update system configuration with current environment
            base_config['system'].update({
                'python_executable': sys.executable,
                'working_directory': str(Path.cwd()),
                'environment_health': system_analysis.get('package_analysis', {}).get('environment_health', {}).get('overall_status', 'unknown')
            })
        
        # Update preset information with dynamic data
        if 'presets' in base_config:
            base_config['presets'].update({
                'available_presets': get_available_presets(),
                'current_preset': 'default',
                'preset_configs': get_preset_descriptions(),
                'custom_presets_available': get_safe_custom_presets(),
                'system_recommended_preset': _recommend_preset_for_system(system_analysis),
                'preset_compatibility': _check_preset_system_compatibility(base_config, system_analysis)
            })
        
        # Add comprehensive runtime configuration
        base_config['runtime'] = {
            'config_loaded_at': current_time,
            'config_source': config_source,
            'runtime_id': hashlib.md5(current_time.encode()).hexdigest()[:8] if 'hashlib' in globals() else 'unknown',
            'process_id': os.getpid(),
            'working_directory': str(Path.cwd()),
            'python_executable': sys.executable,
            
            # System analysis integration
            'system_analysis_completed': 'hardware_analysis' in system_analysis,
            'system_performance_score': system_analysis.get('hardware_analysis', {}).get('performance_score', 0),
            'system_class': system_analysis.get('hardware_analysis', {}).get('system_class', 'unknown'),
            'environment_health': system_analysis.get('package_analysis', {}).get('environment_health', {}).get('overall_status', 'unknown'),
            
            # Configuration optimizations applied
            'optimizations_applied': {
                'training_optimized': 'training' in base_config and 'hardware_analysis' in system_analysis,
                'model_optimized': 'model' in base_config and 'hardware_analysis' in system_analysis,
                'hardware_optimized': 'hardware' in base_config and 'hardware_analysis' in system_analysis,
                'system_aware': True,
                'preset_based': config_source.startswith('PRESET_CONFIGS')
            },
            
            # Resource availability
            'resource_status': {
                'cuda_available': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('available', False),
                'memory_adequate': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('memory', {}).get('total_gb', 0) >= 4,
                'cpu_adequate': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('cpu', {}).get('logical_cores', 0) >= 2,
                'disk_adequate': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('storage', {}).get('free_gb', 0) >= 5
            }
        }
        
        # Add warnings and recommendations from system analysis
        if 'detailed_analysis' in system_analysis:
            analysis = system_analysis['detailed_analysis']
            
            base_config['runtime']['system_warnings'] = []
            base_config['runtime']['recommendations'] = []
            
            # Collect all warnings and recommendations
            for category in ['system_recommendations', 'performance_optimizations', 'compatibility_issues', 'resource_warnings']:
                if category in analysis and analysis[category]:
                    base_config['runtime']['system_warnings'].extend(analysis[category])
            
            if 'configuration_suggestions' in analysis:
                base_config['runtime']['recommendations'].extend(analysis['configuration_suggestions'])
            
            # Add configuration health status
            warning_count = len(base_config['runtime']['system_warnings'])
            critical_issues = len([w for w in base_config['runtime']['system_warnings'] 
                                 if any(keyword in w.lower() for keyword in ['critical', 'missing required', 'failed', 'error'])])
            
            base_config['runtime']['configuration_health'] = {
                'status': 'healthy' if warning_count == 0 else 'needs_attention' if critical_issues == 0 else 'critical',
                'warning_count': warning_count,
                'recommendation_count': len(base_config['runtime']['recommendations']),
                'critical_issues': critical_issues,
                'overall_score': max(0, 100 - (warning_count * 10) - (critical_issues * 25))
            }
        
        # Ensure model variants are initialized and compatible
        if not MODEL_VARIANTS:
            try:
                logger.debug("MODEL_VARIANTS not initialized, attempting initialization")
                #initialize_model_variants(silent=True)
                initialize_model_variants(silent=False)
            except Exception as e:
                logger.warning(f"Failed to initialize model variants: {e}")
        
        # Validate model type compatibility
        if MODEL_VARIANTS:
            model_type = base_config.get('model', {}).get('model_type', 'SimpleAutoencoder')
            if model_type not in MODEL_VARIANTS:
                logger.warning(f"Model type '{model_type}' not available, falling back to SimpleAutoencoder")
                base_config['model']['model_type'] = 'SimpleAutoencoder'
                # Simplify configuration for SimpleAutoencoder
                base_config['model']['hidden_dims'] = [base_config['model']['hidden_dims'][0]] if base_config['model'].get('hidden_dims') else [64]
                base_config['model']['dropout_rates'] = [base_config['model']['dropout_rates'][0]] if base_config['model'].get('dropout_rates') else [0.2]
        
        # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
        # Aggressive cleanup after all processing is complete
        try:
            final_clear_results = enhanced_clear_memory(
                aggressive=True,  # Aggressive final cleanup
                hardware_data=hardware_data
            )
            
            if final_clear_results.get('success'):
                actions_taken = final_clear_results.get('actions_taken', [])
                logger.debug(f"Final memory optimization completed: {', '.join(actions_taken)}")
                
                # Add memory optimization summary to runtime
                if 'memory_optimization_summary' not in base_config['runtime']:
                    base_config['runtime']['memory_optimization_summary'] = {
                        'optimizations_performed': len(actions_taken),
                        'final_cleanup': True,
                        'hardware_aware': True,
                        'aggressive_mode': True
                    }
                
        except Exception as e:
            logger.debug(f"Final memory optimization failed: {e}")
        
        # Cache the configuration
        _cached_config = base_config
        _config_cache_time = time.time()
        
        # Log configuration summary
        model_info = base_config.get('model', {})
        training_info = base_config.get('training', {})
        system_class = system_analysis.get('hardware_analysis', {}).get('system_class', 'unknown')
        
        logger.info("Successfully generated system-aware default configuration:")
        logger.info(f"  - System Class: {system_class}")
        logger.info(f"  - Model: {model_info.get('model_type', 'unknown')} (encoding_dim={model_info.get('encoding_dim', 'unknown')})")
        logger.info(f"  - Training: batch_size={training_info.get('batch_size', 'unknown')}, epochs={training_info.get('epochs', 'unknown')}")
        logger.info(f"  - Hardware: CUDA={'available' if base_config.get('hardware', {}).get('performance_optimization', {}).get('use_cuda', False) else 'disabled'}")
        logger.info(f"  - Config Source: {config_source}")
        
        return base_config
        
    except Exception as e:
        logger.error(f"Failed to generate default configuration: {e}", exc_info=True)
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        # Return minimal fallback with error information
        try:
            fallback_config = _create_minimal_fallback_config('minimal')
        except Exception as fallback_error:
            logger.critical(f"Even minimal fallback failed: {fallback_error}")
            # Last resort configuration
            fallback_config = {
                'metadata': {
                    'version': '2.1', 'config_version': '2.1',
                    'created': datetime.now().isoformat() if 'datetime' in globals() else 'unknown',
                    'description': 'Emergency last resort configuration',
                    'preset_used': 'emergency_last_resort',
                    'compatibility': ['SimpleAutoencoder'],
                    'system': {
                        'python_version': platform.python_version() if 'platform' in globals() else 'unknown',
                        'os': platform.system() if 'platform' in globals() else 'unknown'
                    }
                },
                'training': {
                    'batch_size': 8, 'epochs': 5, 'learning_rate': 0.01, 'num_workers': 1,
                    'optimizer': 'SGD', 'mixed_precision': False, 'patience': 3
                },
                'model': {
                    'model_type': 'SimpleAutoencoder', 'encoding_dim': 4, 'hidden_dims': [32],
                    'dropout_rates': [0.1], 'activation': 'relu', 'num_models': 1
                },
                'data': {
                    'normal_samples': 100, 'attack_samples': 20, 'features': 8, 'validation_split': 0.3
                },
                'security': {
                    'percentile': 90, 'attack_threshold': 0.5, 'enable_security_metrics': False
                },
                'hardware': {
                    'device': 'cpu', 'recommended_gpu_memory': 1,
                    'performance_optimization': {'use_cuda': False, 'use_amp': False, 'deterministic': True}
                },
                'system': {
                    'model_dir': './models/emergency', 'debug': True, 'random_seed': 42
                },
                'presets': {
                    'current_preset': 'emergency_last_resort', 'available_presets': []
                }
            }
        
        fallback_config['runtime'] = {
            'config_loaded_at': datetime.now().isoformat() if 'datetime' in globals() else 'unknown',
            'config_source': 'emergency_fallback',
            'error': str(e),
            'system_analysis_failed': True,
            'configuration_health': {
                'status': 'critical',
                'error_message': str(e),
                'fallback_used': True
            }
        }
        
        # Cache the fallback
        _cached_config = fallback_config
        _config_cache_time = time.time()
        
        return fallback_config

def _get_basic_system_info() -> Dict[str, Any]:
    """Get basic system information when full analysis fails."""
    try:
        return {
            'timestamp': datetime.now().isoformat(),
            'platform': {
                'system': platform.system(),
                'platform': platform.platform(),
                'machine': platform.machine(),
                'node': platform.node()
            },
            'python': {
                'version_info': {
                    'version_tuple': tuple(sys.version_info[:3])
                }
            },
            'hardware_analysis': {
                'capabilities': {
                    'cpu': {
                        'logical_cores': os.cpu_count() or 1,
                        'performance_class': 'unknown'
                    },
                    'memory': {
                        'total_gb': 0,
                        'performance_class': 'unknown'
                    },
                    'gpu': {
                        'available': torch.cuda.is_available() if 'torch' in globals() else False,
                        'count': torch.cuda.device_count() if 'torch' in globals() and torch.cuda.is_available() else 0,
                        'performance_class': 'unknown'
                    }
                },
                'system_class': 'unknown',
                'performance_score': 50
            },
            'package_analysis': {
                'environment_health': {
                    'overall_status': 'unknown',
                    'compatibility_score': 0
                }
            },
            'collection_metadata': {
                'data_quality': 'basic',
                'errors': ['Full system analysis failed, using basic info']
            }
        }
    except Exception:
        return {
            'timestamp': datetime.now().isoformat(),
            'hardware_analysis': {'system_class': 'unknown', 'performance_score': 0},
            'package_analysis': {'environment_health': {'overall_status': 'unknown'}},
            'collection_metadata': {'data_quality': 'minimal', 'errors': ['Basic system info collection failed']}
        }

def reset_config() -> None:
    """Reset configuration to default values with comprehensive cleanup.
    
    This function now leverages get_default_config() to obtain a fresh, system-aware
    default configuration instead of creating one from scratch, reducing code
    redundancy and ensuring consistency with the default configuration generation logic.
    """
    try:
        logger.info("Resetting configuration to system-aware defaults...")
        
        # Clear any cached configurations to ensure fresh generation
        global _cached_config, _config_cache_time
        _cached_config = None
        _config_cache_time = None
        
        # Get fresh default configuration using get_default_config()
        # This automatically includes system analysis, hardware optimization, and preset integration
        try:
            default_config = get_default_config()
            logger.debug("Successfully obtained fresh default configuration from get_default_config()")
        except Exception as e:
            logger.error(f"Failed to get default configuration: {e}")
            logger.warning("Falling back to minimal emergency configuration")
            
            # Emergency fallback - create absolute minimal configuration
            default_config = _create_minimal_fallback_config('emergency')
            logger.warning("Using emergency fallback configuration due to get_default_config() failure")
        
        # Validate the default configuration before applying
        try:
            validate_config(default_config)
            logger.debug("Default configuration validation passed")
        except Exception as validation_error:
            logger.error(f"Default configuration validation failed: {validation_error}")
            logger.warning("Attempting to fix validation issues automatically")
            
            # Try to fix common validation issues
            try:
                # Ensure required sections exist
                required_sections = ['training', 'model', 'security', 'data']
                for section in required_sections:
                    if section not in default_config:
                        logger.warning(f"Adding missing section: {section}")
                        if section == 'training':
                            default_config[section] = {
                                'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001,
                                'num_workers': 1, 'optimizer': 'Adam'
                            }
                        elif section == 'model':
                            default_config[section] = {
                                'model_type': 'SimpleAutoencoder', 'encoding_dim': 8,
                                'hidden_dims': [64], 'dropout_rates': [0.2]
                            }
                        elif section == 'security':
                            default_config[section] = {
                                'percentile': 95, 'attack_threshold': 0.3,
                                'enable_security_metrics': True
                            }
                        elif section == 'data':
                            default_config[section] = {
                                'normal_samples': 1000, 'attack_samples': 200,
                                'features': 10, 'validation_split': 0.2
                            }
                
                # Re-validate after fixes
                validate_config(default_config)
                logger.info("Successfully fixed validation issues in default configuration")
                
            except Exception as fix_error:
                logger.critical(f"Could not fix default configuration: {fix_error}")
                # Last resort - use emergency fallback
                default_config = _create_minimal_fallback_config('emergency')
                logger.critical("Using emergency fallback configuration due to validation failures")
        
        # Ensure metadata reflects the reset operation
        if 'metadata' not in default_config:
            default_config['metadata'] = {}
        
        default_config['metadata'].update({
            'modified': datetime.now().isoformat(),
            'reset_timestamp': datetime.now().isoformat(),
            'reset_reason': 'manual_reset_to_defaults',
            'config_source': 'reset_to_system_aware_defaults'
        })
        
        # Add reset operation to runtime information
        if 'runtime' not in default_config:
            default_config['runtime'] = {}
        
        default_config['runtime'].update({
            'last_reset': datetime.now().isoformat(),
            'reset_method': 'get_default_config',
            'config_generation_method': 'system_aware_reset'
        })
        
        # Update preset information to reflect reset
        if 'presets' in default_config:
            default_config['presets']['current_preset'] = 'default'
            default_config['presets']['last_reset'] = datetime.now().isoformat()
            default_config['presets']['reset_to_preset'] = 'default'
        
        # Update global configuration with the new default
        try:
            update_global_config(default_config)
            logger.debug("Successfully updated global configuration")
        except Exception as e:
            logger.error(f"Failed to update global configuration: {e}")
            # Continue anyway as this isn't critical for reset operation
        
        # Save the reset configuration to file
        try:
            save_config(default_config)
            logger.debug("Successfully saved reset configuration to file")
        except Exception as e:
            logger.error(f"Failed to save reset configuration: {e}")
            # Log but don't fail - the configuration is still reset in memory
        
        # Clear any model variants cache to ensure fresh initialization
        global MODEL_VARIANTS
        if MODEL_VARIANTS:
            logger.debug("Clearing model variants cache for fresh initialization")
            MODEL_VARIANTS.clear()
        
        # Re-initialize model variants with the new configuration
        try:
            initialize_model_variants(silent=False)
            logger.debug("Successfully re-initialized model variants")
        except Exception as e:
            logger.warning(f"Failed to re-initialize model variants: {e}")
            # Non-critical for reset operation
        
        # Force memory cleanup after reset
        try:
            enhanced_clear_memory()
            logger.debug("Completed memory cleanup after reset")
        except Exception as e:
            logger.debug(f"Memory cleanup had issues: {e}")
        
        # Log successful reset with configuration summary
        model_info = default_config.get('model', {})
        training_info = default_config.get('training', {})
        system_class = default_config.get('runtime', {}).get('system_class', 'unknown')
        
        logger.info("Configuration reset completed successfully:")
        logger.info(f"  - System Class: {system_class}")
        logger.info(f"  - Model: {model_info.get('model_type', 'unknown')} (encoding_dim={model_info.get('encoding_dim', 'unknown')})")
        logger.info(f"  - Training: batch_size={training_info.get('batch_size', 'unknown')}, epochs={training_info.get('epochs', 'unknown')}")
        logger.info(f"  - Config Source: {default_config.get('runtime', {}).get('config_source', 'unknown')}")
        
        # Verify reset by checking key parameters
        verification_checks = [
            ('training.batch_size', training_info.get('batch_size')),
            ('model.model_type', model_info.get('model_type')),
            ('model.encoding_dim', model_info.get('encoding_dim')),
            ('security.percentile', default_config.get('security', {}).get('percentile')),
            ('data.normal_samples', default_config.get('data', {}).get('normal_samples'))
        ]
        
        logger.debug("Reset verification checks:")
        for param_name, param_value in verification_checks:
            logger.debug(f"  - {param_name}: {param_value}")
        
        logger.info("[SUCCESS] Configuration has been reset to system-aware defaults")
        
    except Exception as e:
        logger.error(f"Critical failure during configuration reset: {e}", exc_info=True)
        
        # Emergency recovery attempt
        try:
            logger.critical("Attempting emergency configuration recovery...")
            
            # Create absolute minimal configuration
            emergency_config = {
                'metadata': {
                    'version': '2.1',
                    'config_version': '2.1',
                    'created': datetime.now().isoformat(),
                    'description': 'Emergency recovery configuration',
                    'preset_used': 'emergency_recovery',
                    'reset_failed': True,
                    'original_error': str(e)
                },
                'training': {
                    'batch_size': 16, 'epochs': 5, 'learning_rate': 0.001,
                    'num_workers': 1, 'optimizer': 'SGD', 'patience': 3
                },
                'model': {
                    'model_type': 'SimpleAutoencoder', 'encoding_dim': 4,
                    'hidden_dims': [32], 'dropout_rates': [0.1], 'activation': 'relu'
                },
                'data': {
                    'normal_samples': 100, 'attack_samples': 20, 'features': 8,
                    'validation_split': 0.3, 'normalization': 'minmax'
                },
                'security': {
                    'percentile': 90, 'attack_threshold': 0.5,
                    'enable_security_metrics': False
                },
                'system': {
                    'model_dir': './models/emergency', 'debug': True,
                    'random_seed': 42, 'export_onnx': False
                },
                'hardware': {
                    'device': 'cpu', 'recommended_gpu_memory': 1,
                    'performance_optimization': {'use_cuda': False, 'use_amp': False}
                },
                'presets': {
                    'current_preset': 'emergency_recovery',
                    'available_presets': []
                },
                'runtime': {
                    'config_source': 'emergency_recovery',
                    'reset_failed': True,
                    'emergency_recovery': True,
                    'recovery_timestamp': datetime.now().isoformat()
                }
            }
            
            # Try to apply emergency configuration
            update_global_config(emergency_config)
            
            # Try to save emergency configuration
            try:
                save_config(emergency_config)
            except:
                logger.critical("Could not save emergency configuration to file")
            
            logger.critical("[EMERGENCY] Applied minimal emergency configuration")
            logger.critical("System is in degraded state - manual intervention recommended")
            
        except Exception as recovery_error:
            logger.critical(f"Emergency recovery also failed: {recovery_error}")
            logger.critical("System configuration is in critical failure state")
            raise RuntimeError(f"Configuration reset failed completely: {str(e)}. Recovery failed: {str(recovery_error)}")
        
        # Re-raise the original exception with context
        raise RuntimeError(f"Configuration reset failed: {str(e)}. Emergency recovery applied.")

def reset_config_interactive():
    """Interactive configuration reset with enhanced confirmation and feedback."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        show_banner()
        
        # Get current configuration info for display
        try:
            current_config = get_current_config()
            current_preset = current_config.get('presets', {}).get('current_preset', 'unknown')
            current_model = current_config.get('model', {}).get('model_type', 'unknown')
            system_class = current_config.get('runtime', {}).get('system_class', 'unknown')
        except Exception as e:
            logger.debug(f"Could not get current config for display: {e}")
            current_preset = 'unknown'
            current_model = 'unknown'
            system_class = 'unknown'
        
        # Display current configuration summary
        print(Fore.CYAN + Style.BRIGHT + "\n" + "="*40)
        print(Fore.YELLOW + Style.BRIGHT + "CONFIGURATION RESET")
        print(Fore.CYAN + Style.BRIGHT + "="*40)
        print(Fore.GREEN + Style.BRIGHT + f"\nCurrent Preset: {current_preset}")
        print(Fore.GREEN + Style.BRIGHT + f"Current Model:  {current_model}")
        print(Fore.GREEN + Style.BRIGHT + f"System Class:   {system_class}")
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "="*60)
        print(Fore.RED + Style.BRIGHT + "This will reset your configuration to system-aware defaults.")
        print(Fore.RED + Style.BRIGHT + "All current settings will be lost and cannot be recovered.")
        print(Fore.RED + Style.BRIGHT + "The new configuration will be optimized for your system.")
        print(Fore.YELLOW + Style.BRIGHT + "="*60)
        
        # Get user confirmation with multiple prompts for safety
        response1 = input(Fore.YELLOW + Style.BRIGHT + "\nDo you want to reset configuration to defaults? (y/N): ").lower().strip()
        
        if response1 != 'y':
            print(Fore.RED + Style.BRIGHT + "Configuration reset cancelled.")
            return
        
        print(Fore.YELLOW + Style.BRIGHT + "\nWARNING! This action cannot be undone!")
        reset_message = (Fore.RED + Style.BRIGHT + "RESET")
        response2 = input(Fore.YELLOW + Style.BRIGHT + f"Are you absolutely sure? Type '{reset_message}' to confirm: ").lower().strip()
        
        if response2 != 'reset':
            print(Fore.RED + Style.BRIGHT + "Configuration reset cancelled.")
            return
        
        # Perform the reset
        print(Fore.GREEN + Style.BRIGHT + "\nResetting configuration...")
        try:
            reset_config()
            
            # Get new configuration summary
            try:
                new_config = get_current_config()
                new_preset = new_config.get('presets', {}).get('current_preset', 'unknown')
                new_model = new_config.get('model', {}).get('model_type', 'unknown')
                new_system_class = new_config.get('runtime', {}).get('system_class', 'unknown')
                new_batch_size = new_config.get('training', {}).get('batch_size', 'unknown')
                new_epochs = new_config.get('training', {}).get('epochs', 'unknown')
            except Exception as e:
                logger.debug(f"Could not get new config for display: {e}")
                new_preset = 'unknown'
                new_model = 'unknown'
                new_system_class = 'unknown'
                new_batch_size = 'unknown'
                new_epochs = 'unknown'
            
            print(Fore.GREEN + Style.BRIGHT + "\n" + "="*60)
            print(Fore.GREEN + Style.BRIGHT + "RESET COMPLETED SUCCESSFULLY")
            print(Fore.GREEN + Style.BRIGHT + "="*60)
            print(Fore.GREEN + Style.BRIGHT + f"\nNew Preset:       {new_preset}")
            print(Fore.GREEN + Style.BRIGHT + f"New Model:        {new_model}")
            print(Fore.GREEN + Style.BRIGHT + f"System Class:     {new_system_class}")
            print(Fore.GREEN + Style.BRIGHT + f"Batch Size:       {new_batch_size}")
            print(Fore.GREEN + Style.BRIGHT + f"Epochs:           {new_epochs}")
            print(Fore.GREEN + Style.BRIGHT + "\n" + "="*60)
            print(Fore.GREEN + Style.BRIGHT + "\nConfiguration has been reset to optimized defaults for your system.")
            print(Fore.GREEN + Style.BRIGHT + "You can now customize the settings as needed.")
            
        except Exception as e:
            print(Fore.RED + Style.BRIGHT + f"\nERROR: Configuration reset failed: {e}")
            print(Fore.RED + Style.BRIGHT + "System may be in an inconsistent state.")
            print(Fore.RED + Style.BRIGHT + "Please check logs for details and consider manual recovery.")
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\n\nConfiguration reset cancelled by user.")
    except Exception as e:
        print(Fore.RED + Style.BRIGHT + f"\nUnexpected error during interactive reset: {e}")
        logger.error(f"Interactive reset failed: {e}", exc_info=True)

def _create_minimal_fallback_config(fallback_level: str = 'minimal') -> Dict[str, Any]:
    """Create fallback configuration with different levels of completeness.
    
    Args:
        fallback_level: Level of fallback configuration
            - 'minimal': Ultra-minimal for emergency use
            - 'standard': More complete fallback with enhanced features
            - 'emergency': Absolute minimal when everything fails
    
    Returns:
        Dictionary containing fallback configuration
    """
    try:
        current_time = datetime.now().isoformat()
        
        # Common base configuration matching updated preset structure
        base_config = {
            'metadata': {
                'description': f'{fallback_level.title()} fallback configuration for system recovery',
                'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
                'created': current_time, 'last_modified': current_time,
                'preset_used': f'{fallback_level}_fallback',
                'recommended_hardware': {'gpu_memory_gb': 1, 'cpu_cores': 1, 'ram_gb': 2},
                'compatibility': ['SimpleAutoencoder'],
                'system': {
                    'python_version': platform.python_version(),
                    'platform': platform.platform(),
                    'architecture': platform.machine(),
                    'processor': platform.processor() or 'unknown',
                    'pytorch_version': torch.__version__ if 'torch' in globals() else 'unknown',
                    'cuda_available': torch.cuda.is_available() if 'torch' in globals() and hasattr(torch, 'cuda') else False,
                    'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
                    'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
                    'hostname': platform.node(), 'os': platform.system(),
                    'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
                },
                'validation': {
                    'schema_version': '2.1',
                    'required_sections': ['training', 'model', 'security', 'data'],
                    'optional_sections': ['monitoring', 'hardware', 'presets']
                },
                'fallback_info': {
                    'is_fallback': True, 'level': fallback_level,
                    'creation_time': current_time,
                    'reason': 'Configuration system failure - using safe defaults',
                    'warnings': [
                        f'This is a {fallback_level} fallback configuration',
                        'Limited functionality available',
                        'Recommend fixing configuration system'
                    ]
                }
            }
        }
        
        # Configuration based on fallback level
        if fallback_level == 'emergency':
            # Absolute minimal configuration
            base_config.update({
                'training': {
                    'batch_size': 8, 'epochs': 2, 'learning_rate': 0.01, 'patience': 2, 'weight_decay': 0.0,
                    'gradient_clip': 5.0, 'gradient_accumulation_steps': 1, 'mixed_precision': False,
                    'num_workers': 1, 'optimizer': 'SGD', 'scheduler': None, 'scheduler_params': {},
                    'early_stopping': False, 'validation_split': 0.4, 'shuffle': True,
                    'pin_memory': False, 'persistent_workers': False,
                    'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 2, 'lr_factor': 0.5, 'min_lr': 1e-7
                },
                'model': {
                    'model_type': 'SimpleAutoencoder', 'input_dim': 8, 'encoding_dim': 2, 'hidden_dims': [16],
                    'dropout_rates': [0.0], 'activation': 'relu', 'activation_param': 0.0,
                    'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False,
                    'diversity_factor': 0.0, 'min_features': 2, 'num_models': 1, 'skip_connection': False,
                    'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
                    'model_types': ['SimpleAutoencoder'],
                    'available_activations': ['relu'],
                    'available_normalizations': [None],
                    'available_initializers': ['xavier_uniform'],
                    'legacy_mode': False, 'use_attention': False
                },
                'security': {
                    'percentile': 85, 'attack_threshold': 0.5, 'false_negative_cost': 1.0,
                    'enable_security_metrics': False, 'anomaly_threshold_strategy': 'fixed_percentile',
                    'early_warning_threshold': 0.45, 'adaptive_threshold': False, 'confidence_interval': 0.8,
                    'detection_methods': ['reconstruction_error'], 'alert_levels': ['low', 'medium'],
                    'threshold_validation': False
                },
                'data': {
                    'normal_samples': 50, 'attack_samples': 10, 'features': 4, 'use_real_data': False,
                    'data_normalization': 'minmax', 'anomaly_factor': 2.0, 'random_state': 42,
                    'validation_split': 0.4, 'test_split': 0.4, 'stratified_split': False,
                    'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
                    'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
                    'synthetic_generation': {'cluster_variance': 0.2, 'anomaly_sparsity': 0.5},
                    'preprocessing': {'remove_outliers': False, 'impute_missing': False},
                    'shuffle': True, 'pin_memory': False
                },
                'monitoring': {
                    'metrics_frequency': 1, 'checkpoint_frequency': 1, 'tensorboard_logging': False,
                    'console_logging_level': 'ERROR', 'save_best_model': False, 'save_model_history': False,
                    'metrics_to_track': ['loss'], 'early_stopping_metric': 'loss',
                    'checkpoint_format': 'pytorch', 'log_model_summary': False,
                    'tensorboard_dir': str(TB_DIR), 'log_frequency': 1, 'save_checkpoints': False,
                    'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False, 'max_scalars': 50, 'max_histograms': 0, 'max_images': 0, 'save_summary': False}
                }
            })
            
        elif fallback_level == 'standard':
            # Enhanced fallback with more features
            safe_batch_size = 16
            safe_epochs = 5
            safe_workers = min(2, os.cpu_count() or 1)
            
            base_config.update({
                'training': {
                    'batch_size': safe_batch_size, 'epochs': safe_epochs, 'learning_rate': 0.001, 
                    'patience': 3, 'weight_decay': 0.0, 'gradient_clip': 1.0, 'gradient_accumulation_steps': 1, 
                    'mixed_precision': False, 'num_workers': safe_workers, 'optimizer': 'Adam', 'scheduler': None, 
                    'scheduler_params': {}, 'early_stopping': True, 'validation_split': 0.3, 'shuffle': True,
                    'pin_memory': False, 'persistent_workers': False,
                    'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 3, 'lr_factor': 0.7, 'min_lr': 1e-6
                },
                'model': {
                    'model_type': 'SimpleAutoencoder', 'input_dim': 16, 'encoding_dim': 4, 'hidden_dims': [32],
                    'dropout_rates': [0.1], 'activation': 'relu', 'activation_param': 0.0,
                    'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False,
                    'diversity_factor': 0.0, 'min_features': 3, 'num_models': 1, 'skip_connection': False,
                    'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
                    'model_types': list(MODEL_VARIANTS.keys()) if 'MODEL_VARIANTS' in globals() else ['SimpleAutoencoder'],
                    'available_activations': ['relu', 'leaky_relu'],
                    'available_normalizations': [None, 'batch'],
                    'available_initializers': ['xavier_uniform', 'xavier_normal'],
                    'legacy_mode': False, 'use_attention': False
                },
                'security': {
                    'percentile': 90, 'attack_threshold': 0.4, 'false_negative_cost': 1.5,
                    'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
                    'early_warning_threshold': 0.35, 'adaptive_threshold': False, 'confidence_interval': 0.9,
                    'detection_methods': ['reconstruction_error'], 'alert_levels': ['low', 'medium', 'high'],
                    'threshold_validation': True
                },
                'data': {
                    'normal_samples': 200, 'attack_samples': 50, 'features': 8, 'use_real_data': False,
                    'data_normalization': 'minmax', 'anomaly_factor': 2.0, 'random_state': 42,
                    'validation_split': 0.3, 'test_split': 0.3, 'stratified_split': False,
                    'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
                    'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
                    'synthetic_generation': {'cluster_variance': 0.1, 'anomaly_sparsity': 0.3, 'noise_factor': 0.05},
                    'preprocessing': {'remove_outliers': False, 'impute_missing': True, 'imputation_strategy': 'mean'},
                    'shuffle': True, 'pin_memory': False
                },
                'monitoring': {
                    'metrics_frequency': 2, 'checkpoint_frequency': 5, 'tensorboard_logging': False,
                    'console_logging_level': 'WARNING', 'save_best_model': True, 'save_model_history': False,
                    'metrics_to_track': ['loss', 'reconstruction_error'], 'early_stopping_metric': 'loss',
                    'checkpoint_format': 'pytorch', 'log_model_summary': False,
                    'tensorboard_dir': str(TB_DIR), 'log_frequency': 2, 'save_checkpoints': True,
                    'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False, 'max_scalars': 100, 'max_histograms': 10, 'max_images': 0, 'save_summary': False}
                },
                'fallback_info': {
                    'is_fallback': True, 'level': 'standard', 'creation_time': current_time,
                    'reason': 'Configuration system failure - using enhanced safe defaults',
                    'recommendations': [
                        'Check configuration file format', 'Verify preset definitions',
                        'Check file permissions', 'Review system requirements'
                    ],
                    'limitations': [
                        'Limited model architectures', 'Reduced functionality',
                        'Basic monitoring only', 'CPU-only execution'
                    ]
                }
            })
            
        else:
            # 'minimal' - default case with balanced functionality
            base_config.update({
                'training': {
                    'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'patience': 5, 'weight_decay': 1e-4,
                    'gradient_clip': 1.0, 'gradient_accumulation_steps': 2, 'mixed_precision': False,
                    'num_workers': min(2, os.cpu_count() or 1), 'optimizer': 'Adam', 'scheduler': None,
                    'scheduler_params': {}, 'early_stopping': True, 'validation_split': 0.2, 'shuffle': True,
                    'pin_memory': False, 'persistent_workers': False,
                    'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 3, 'lr_factor': 0.7, 'min_lr': 1e-6
                },
                'model': {
                    'model_type': 'SimpleAutoencoder', 'input_dim': 20, 'encoding_dim': 8, 'hidden_dims': [64],
                    'dropout_rates': [0.2], 'activation': 'relu', 'activation_param': 0.0,
                    'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False,
                    'diversity_factor': 0.0, 'min_features': 5, 'num_models': 1, 'skip_connection': False,
                    'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
                    'model_types': list(MODEL_VARIANTS.keys()) if 'MODEL_VARIANTS' in globals() else ['SimpleAutoencoder'],
                    'available_activations': ['relu', 'leaky_relu', 'gelu'],
                    'available_normalizations': [None, 'batch'],
                    'available_initializers': ['xavier_uniform', 'xavier_normal'],
                    'legacy_mode': False, 'use_attention': False
                },
                'security': {
                    'percentile': 95, 'attack_threshold': 0.3, 'false_negative_cost': 2.0,
                    'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
                    'early_warning_threshold': 0.25, 'adaptive_threshold': False, 'confidence_interval': 0.95,
                    'detection_methods': ['reconstruction_error'], 'alert_levels': ['low', 'medium', 'high'],
                    'threshold_validation': True
                },
                'data': {
                    'normal_samples': 1000, 'attack_samples': 200, 'features': 10, 'use_real_data': False,
                    'data_normalization': 'minmax', 'anomaly_factor': 2.0, 'random_state': 42,
                    'validation_split': 0.2, 'test_split': 0.2, 'stratified_split': True,
                    'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
                    'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
                    'synthetic_generation': {'cluster_variance': 0.1, 'anomaly_sparsity': 0.3, 'noise_factor': 0.05},
                    'preprocessing': {'remove_outliers': True, 'impute_missing': True, 'imputation_strategy': 'mean'},
                    'shuffle': True, 'pin_memory': False
                },
                'monitoring': {
                    'metrics_frequency': 5, 'checkpoint_frequency': 10, 'tensorboard_logging': False,
                    'console_logging_level': 'INFO', 'save_best_model': True, 'save_model_history': False,
                    'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss'],
                    'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
                    'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 5, 'save_checkpoints': True,
                    'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False, 'max_scalars': 200, 'max_histograms': 20, 'max_images': 5, 'save_summary': False}
                }
            })

        # Common sections for all fallback levels
        base_config.update({
            'hardware': {
                'device': 'cpu', 'recommended_gpu_memory': 1,
                'minimum_system_requirements': {'cpu_cores': 1, 'ram_gb': 1, 'disk_space': 2},
                'optimal_system_requirements': {'cpu_cores': 2, 'ram_gb': 2, 'disk_space': 5},
                'memory_management': {'max_memory_fraction': 0.5, 'allow_memory_growth': False, 'memory_limit': 1024},
                'performance_optimization': {'use_cuda': False, 'use_amp': False, 'benchmark_mode': False, 'deterministic': True}
            },
            'system': {
                'model_dir': str(Path('./models/fallback')), 'log_dir': str(Path('./logs/fallback')),
                'config_dir': str(Path('./config/fallback')), 'data_dir': str(Path('./data/fallback')),
                'checkpoint_dir': str(Path('./checkpoints/fallback')),
                'debug': True if fallback_level == 'emergency' else False, 'verbose': True,
                'random_seed': 42, 'reproducible': True, 'parallel_processing': False, 'max_workers': 1,
                'export_onnx': False, 'non_interactive': False, 'cuda_optimizations': False,
                'onnx_export': {'opset_version': 14, 'dynamic_axes': False, 'constant_folding': False, 'optimize_for_mobile': False, 'runtime_validation': False, 'validation_tolerance': 1e-5, 'verbose': False}
            },
            'presets': {
                'available_presets': [], 'current_preset': f'{fallback_level}_fallback', 'current_override': None,
                'override_rules': {'security': False, 'monitoring': False, 'hardware': False},
                'preset_configs': {}, 'custom_presets_available': [],
                'auto_apply': False, 'validate_compatibility': False
            },
            'hyperparameter_optimization': {
                'enabled': False, 'strategy': 'optuna', 'study_name': f'autoencoder_hpo_{fallback_level}',
                'direction': 'minimize', 'n_trials': 5, 'timeout': 300,
                'sampler': 'RandomSampler', 'pruner': 'NopPruner', 'objective_metric': 'loss',
                'optimization_space': {
                    'learning_rate': {'type': 'float', 'low': 1e-3, 'high': 1e-1, 'log': True},
                    'batch_size': {'type': 'categorical', 'choices': [8, 16, 32]}
                },
                'early_stopping': {'enabled': False, 'patience': 2, 'min_improvement': 1e-2},
                'timeout_seconds': 300, 'trial_epochs': 3, 'trial_patience': 2,
                'cleanup_trials': False, 'generate_plots': False,
                'search_space': {
                    'encoding_dim_min': 2, 'encoding_dim_max': 8, 'hidden_layers_min': 1, 'hidden_layers_max': 1,
                    'lr_min': 1e-3, 'lr_max': 1e-1, 'batch_sizes': [8, 16, 32],
                    'weight_decay_min': 0.0, 'weight_decay_max': 0.0, 'dropout_min': 0.0, 'dropout_max': 0.1,
                    'activations': ["relu"], 'normalizations': [None], 'percentile_min': 80, 'percentile_max': 90
                },
                'hpo_sampler': {'type': 'Random', 'seed': 42, 'consider_prior': False, 'prior_weight': 1.0, 'consider_magic_clip': False, 'consider_endpoints': False, 'n_startup_trials': 2, 'n_ei_candidates': 5, 'multivariate': False},
                'hpo_pruner': {'type': 'Nop', 'n_startup_trials': 0, 'n_warmup_steps': 0, 'interval_steps': 1},
                'scoring': {'use_composite_score': False, 'validation_weight': 1.0, 'test_weight': 0.0, 'complexity_weight': 0.0, 'max_params_penalty': 1000},
                'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/{fallback_level}_study.db", 'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
            },
            'validation': {
                'cross_validation': {'enabled': False, 'folds': 2, 'stratified': False, 'random_state': 42},
                'metrics': ['mse', 'mae'], 'validation_frequency': 1,
                'save_validation_results': False, 'detailed_metrics': False
            },
            'experimental': {
                'features': {'advanced_logging': True if fallback_level == 'emergency' else False, 'model_interpretability': False, 'federated_learning': False, 'active_learning': False},
                'settings': {'experimental_mode': True if fallback_level == 'emergency' else False, 'beta_features': False, 'research_mode': False}
            }
        })
        
        # Add runtime information
        base_config['runtime'] = {
            'config_loaded_at': current_time,
            'config_source': f'{fallback_level}_fallback',
            'process_id': os.getpid(),
            'configuration_health': {
                'status': f'{fallback_level}_fallback',
                'last_validation': current_time,
                'validation_errors': []
            }
        }
        
        # Log the fallback creation
        if fallback_level == 'emergency':
            logger.critical("Created emergency fallback configuration - system in critical state")
        elif fallback_level == 'standard':
            logger.warning("Created standard fallback configuration due to system failure")
            logger.info(f"Fallback config: {base_config['data']['features']} features, "
                       f"{base_config['training']['batch_size']} batch size, "
                       f"{base_config['model']['model_type']} model")
        else:
            logger.info(f"Created {fallback_level} fallback configuration")
        
        return base_config
        
    except Exception as e:
        logger.critical(f"Failed to create {fallback_level} fallback configuration: {e}")
        
        # Last resort emergency config - absolute minimal
        return {
            'metadata': {
                'version': '2.1', 'config_version': '2.1',
                'created': datetime.now().isoformat() if 'datetime' in globals() else 'unknown',
                'description': 'Emergency minimal configuration - last resort',
                'preset_used': 'emergency_last_resort',
                'config_source': 'last_resort_fallback',
                'compatibility': ['SimpleAutoencoder']
            },
            'training': {
                'batch_size': 8, 'epochs': 2, 'learning_rate': 0.01, 'num_workers': 1, 'optimizer': 'SGD'
            },
            'model': {
                'model_type': 'SimpleAutoencoder', 'encoding_dim': 2, 'hidden_dims': [16],
                'dropout_rates': [0.0], 'activation': 'relu'
            },
            'data': {
                'normal_samples': 50, 'attack_samples': 10, 'features': 4, 'validation_split': 0.4
            },
            'security': {
                'percentile': 90, 'attack_threshold': 0.5, 'enable_security_metrics': False
            },
            'system': {
                'model_dir': './models/emergency', 'debug': True, 'random_seed': 42
            },
            'presets': {
                'current_preset': 'emergency_last_resort', 'available_presets': []
            },
            'fallback_info': {
                'is_fallback': True, 'level': 'emergency_last_resort', 'critical_error': str(e)
            },
            'runtime': {
                'config_source': 'last_resort_fallback',
                'configuration_health': {'status': 'critical'}
            }
        }

def _recommend_preset_for_system(system_analysis: Dict[str, Any]) -> str:
    """Recommend optimal preset based on system analysis."""
    try:
        system_class = system_analysis.get('hardware_analysis', {}).get('system_class', 'unknown')
        performance_score = system_analysis.get('hardware_analysis', {}).get('performance_score', 0)
        gpu_available = system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('available', False)
        memory_gb = system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('memory', {}).get('total_gb', 0)
        
        if system_class == 'high_performance' and gpu_available and memory_gb >= 16:
            return 'advanced'
        elif system_class == 'high_performance' and gpu_available:
            return 'performance'
        elif system_class == 'standard':
            return 'default'
        elif memory_gb < 4 or performance_score < 40:
            return 'lightweight'
        else:
            return 'default'
            
    except Exception:
        return 'default'

def _check_preset_system_compatibility(config: Dict[str, Any], system_analysis: Dict[str, Any]) -> Dict[str, Any]:
    """Check compatibility between preset configuration and system capabilities."""
    try:
        compatibility = {
            'overall_compatible': True,
            'warnings': [],
            'recommendations': []
        }
        
        # Check memory requirements
        training_config = config.get('training', {})
        model_config = config.get('model', {})
        system_memory = system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('memory', {}).get('total_gb', 0)
        
        # Estimate memory requirements
        batch_size = training_config.get('batch_size', 64)
        model_complexity = len(model_config.get('hidden_dims', [])) + 1
        # Rough estimate in GB
        estimated_memory_need = (batch_size * model_complexity) / 1000
        
        if system_memory > 0 and estimated_memory_need > system_memory * 0.8:
            compatibility['overall_compatible'] = False
            compatibility['warnings'].append(f"High memory usage expected ({estimated_memory_need:.1f}GB) for available memory ({system_memory:.1f}GB)")
            compatibility['recommendations'].append("Consider reducing batch size or model complexity")
        
        # Check GPU requirements
        cuda_available = system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('available', False)
        mixed_precision = training_config.get('mixed_precision', False)
        
        if mixed_precision and not cuda_available:
            compatibility['warnings'].append("Mixed precision training enabled but CUDA not available")
            compatibility['recommendations'].append("Disable mixed precision for CPU-only systems")
        
        # Check model type compatibility
        model_type = model_config.get('model_type', '')
        num_models = model_config.get('num_models', 1)
        
        if 'Ensemble' in model_type and num_models > 1 and system_analysis.get('hardware_analysis', {}).get('system_class') == 'limited':
            compatibility['warnings'].append("Ensemble model may be too complex for limited system resources")
            compatibility['recommendations'].append("Consider using SimpleAutoencoder for better compatibility")
        
        return compatibility
        
    except Exception as e:
        return {
            'overall_compatible': True,
            'warnings': [f"Compatibility check failed: {str(e)}"],
            'recommendations': []
        }

def save_custom_preset(name: str, config: Dict) -> Path:
    """Save a custom preset configuration with enhanced validation and metadata.
    
    Args:
        name: Name for the custom preset
        config: Configuration dictionary to save as preset
        
    Returns:
        Path: Path to the saved preset file
        
    Raises:
        ValueError: If name or config is invalid
        RuntimeError: If save operation fails
    """
    try:
        # Input validation
        if not isinstance(name, str) or not name.strip():
            raise ValueError("Preset name must be a non-empty string")
        
        if not isinstance(config, dict) or not config:
            raise ValueError("Configuration must be a non-empty dictionary")
        
        # Sanitize the name
        safe_name = "".join(c for c in name.strip() if c.isalnum() or c in (' ', '_', '-')).strip()
        safe_name = safe_name.replace(' ', '_').lower()
        
        if not safe_name:
            raise ValueError(f"Invalid preset name '{name}' - must contain alphanumeric characters")
        
        if len(safe_name) > 50:
            safe_name = safe_name[:50]
            logger.warning(f"Preset name truncated to: {safe_name}")
        
        # Setup custom presets directory
        custom_dir = CONFIG_DIR / "deep_learning_custom_presets"
        custom_dir.mkdir(parents=True, exist_ok=True)
        
        filename = f"preset_{safe_name}.json"
        filepath = custom_dir / filename
        
        # Check for name conflicts
        if filepath.exists():
            # Create backup of existing preset
            backup_path = filepath.with_suffix(f".backup_{int(time.time())}.json")
            shutil.copy2(filepath, backup_path)
            logger.info(f"Existing preset backed up to: {backup_path}")
        
        # Validate configuration structure
        try:
            validate_config(config)
            logger.info("Custom preset configuration passed validation")
        except ValueError as e:
            logger.warning(f"Custom preset validation issues (will save anyway): {e}")
        
        # Extract metadata from config
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        security_config = config.get('security', {})
        data_config = config.get('data', {})
        
        # Determine model compatibility
        model_type = model_config.get('model_type', 'unknown')
        compatibility = []
        
        if MODEL_VARIANTS:
            if model_type in MODEL_VARIANTS:
                compatibility.append(model_type)
            # Add other compatible types based on configuration
            if model_type == 'SimpleAutoencoder':
                compatibility.extend(['EnhancedAutoencoder', 'AutoencoderEnsemble'])
            elif model_type == 'EnhancedAutoencoder':
                compatibility.extend(['SimpleAutoencoder', 'AutoencoderEnsemble'])
            elif model_type == 'AutoencoderEnsemble':
                compatibility.append('EnhancedAutoencoder')
        else:
            compatibility = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
        
        # Generate comprehensive preset metadata
        preset_metadata = {
            "name": name,
            "safe_name": safe_name,
            "description": f"Custom preset '{name}' - created from current configuration",
            "created": datetime.now().isoformat(),
            "modified": datetime.now().isoformat(),
            "version": "2.1",
            "preset_type": "custom",
            "model_type": model_type,
            "compatibility": list(set(compatibility)),
            "system": {
                "python_version": platform.python_version(),
                "pytorch_version": getattr(torch, '__version__', 'unknown') if 'torch' in globals() else 'unknown',
                "cuda_available": torch.cuda.is_available() if 'torch' in globals() and hasattr(torch, 'cuda') else False,
                "hostname": platform.node(),
                "os": platform.system(),
                "created_by": "save_custom_preset"
            },
            "configuration_summary": {
                "batch_size": training_config.get('batch_size'),
                "learning_rate": training_config.get('learning_rate'),
                "encoding_dim": model_config.get('encoding_dim'),
                "hidden_layers": len(model_config.get('hidden_dims', [])),
                "features": data_config.get('features'),
                "security_percentile": security_config.get('percentile'),
                "total_sections": len(config),
                "estimated_complexity": estimate_config_complexity(config)
            },
            "usage_guidelines": {
                "recommended_for": determine_preset_recommendations(config),
                "memory_requirements": estimate_memory_requirements(config),
                "training_time_estimate": estimate_training_time(config),
                "resource_level": determine_resource_level(config)
            },
            "validation": {
                "config_validated": True,
                "validation_timestamp": datetime.now().isoformat(),
                "warnings": [],
                "auto_fixes_applied": []
            },
            "checksum": generate_config_checksum(config)
        }
        
        # Create complete preset structure
        preset_data = {
            "metadata": preset_metadata,
            "config": deepcopy(config)
        }
        
        # Add preset-specific enhancements
        preset_data["config"]["metadata"] = preset_data["config"].get("metadata", {})
        preset_data["config"]["metadata"]["preset_used"] = safe_name
        preset_data["config"]["metadata"]["is_custom_preset"] = True
        
        # Atomic write operation
        temp_path = filepath.with_suffix(f".tmp_{int(time.time())}")
        try:
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(preset_data, f, indent=4, ensure_ascii=False, sort_keys=False)
            
            # Verify the written file
            with open(temp_path, 'r', encoding='utf-8') as f:
                verification_data = json.load(f)
                if not verification_data.get('config') or not verification_data.get('metadata'):
                    raise ValueError("Verification failed: preset data is incomplete")
            
            # Atomic replacement
            if os.name == 'nt' and filepath.exists():
                filepath.unlink()
            temp_path.replace(filepath)
            
        except Exception as e:
            if temp_path.exists():
                try:
                    temp_path.unlink()
                except:
                    pass
            raise RuntimeError(f"Failed to write preset file: {e}") from e
        
        # Update global preset registry if available
        try:
            if 'PRESET_CONFIGS' in globals() and PRESET_CONFIGS is not None:
                PRESET_CONFIGS[safe_name] = preset_data["config"]
                logger.info(f"Added custom preset '{safe_name}' to global registry")
            
            # Refresh available presets
            invalidate_config_cache()
            
        except Exception as e:
            logger.warning(f"Could not update global preset registry: {e}")
        
        # Log success with statistics
        file_size = filepath.stat().st_size
        logger.info(f"Custom preset '{name}' saved successfully:")
        logger.info(f"  - File: {filepath}")
        logger.info(f"  - Size: {file_size} bytes")
        logger.info(f"  - Model type: {model_type}")
        logger.info(f"  - Compatible with: {', '.join(compatibility)}")
        logger.info(f"  - Resource level: {preset_metadata['usage_guidelines']['resource_level']}")
        
        return filepath
        
    except ValueError:
        # Re-raise validation errors
        raise
    except Exception as e:
        logger.error(f"Failed to save custom preset '{name}': {str(e)}", exc_info=True)
        raise RuntimeError(f"Custom preset save failed: {str(e)}") from e

# Helper functions for save_custom_preset
def estimate_config_complexity(config: Dict[str, Any]) -> str:
    """Estimate configuration complexity level with comprehensive analysis.
    
    Args:
        config: Configuration dictionary to analyze
        
    Returns:
        String indicating complexity level: 'low', 'medium', 'high', or 'very_high'
    """
    try:
        complexity_score = 0.0
        analysis_factors = []
        
        # Model architecture complexity
        model_config = config.get('model', {})
        model_type = model_config.get('model_type', 'SimpleAutoencoder')
        
        # Base complexity by model type
        if model_type == 'SimpleAutoencoder':
            complexity_score += 1.0
            analysis_factors.append('simple_model')
        elif model_type == 'EnhancedAutoencoder':
            complexity_score += 3.0
            analysis_factors.append('enhanced_model')
        elif model_type == 'AutoencoderEnsemble':
            complexity_score += 5.0
            analysis_factors.append('ensemble_model')
            
            # Ensemble-specific complexity
            num_models = model_config.get('num_models', 3)
            if num_models > 5:
                complexity_score += 2.0
                analysis_factors.append('large_ensemble')
            elif num_models > 3:
                complexity_score += 1.0
                analysis_factors.append('medium_ensemble')
        
        # Hidden layer complexity
        hidden_dims = model_config.get('hidden_dims', [])
        if isinstance(hidden_dims, list):
            # Number of layers
            layer_count = len(hidden_dims)
            complexity_score += layer_count * 0.5
            if layer_count > 4:
                analysis_factors.append('deep_architecture')
            
            # Layer sizes
            large_layers = sum(1 for dim in hidden_dims if isinstance(dim, (int, float)) and dim > 256)
            medium_layers = sum(1 for dim in hidden_dims if isinstance(dim, (int, float)) and 128 <= dim <= 256)
            
            complexity_score += large_layers * 1.0
            complexity_score += medium_layers * 0.5
            
            if large_layers > 0:
                analysis_factors.append('large_hidden_layers')
            
            # Non-standard architectures
            if any(dim > 512 for dim in hidden_dims if isinstance(dim, (int, float))):
                complexity_score += 1.5
                analysis_factors.append('very_large_layers')
        
        # Encoding dimension complexity
        encoding_dim = model_config.get('encoding_dim', 12)
        if isinstance(encoding_dim, (int, float)):
            if encoding_dim > 50:
                complexity_score += 1.0
                analysis_factors.append('large_encoding_dim')
            elif encoding_dim > 24:
                complexity_score += 0.5
                analysis_factors.append('medium_encoding_dim')
        
        # Activation and normalization complexity
        activation = model_config.get('activation', 'relu')
        if activation in ['gelu', 'swish', 'mish']:
            complexity_score += 0.5
            analysis_factors.append('advanced_activation')
        elif activation in ['leaky_relu', 'elu']:
            complexity_score += 0.2
            analysis_factors.append('parameterized_activation')
        
        normalization = model_config.get('normalization')
        if normalization == 'batch':
            complexity_score += 0.3
            analysis_factors.append('batch_normalization')
        elif normalization == 'layer':
            complexity_score += 0.4
            analysis_factors.append('layer_normalization')
        elif normalization == 'group':
            complexity_score += 0.6
            analysis_factors.append('group_normalization')
        
        # Advanced features
        if model_config.get('use_batch_norm', False):
            complexity_score += 0.3
            analysis_factors.append('batch_norm_layers')
        
        if model_config.get('use_layer_norm', False):
            complexity_score += 0.4
            analysis_factors.append('layer_norm_layers')
        
        if model_config.get('skip_connection', False):
            complexity_score += 0.5
            analysis_factors.append('skip_connections')
        
        if model_config.get('residual_blocks', False):
            complexity_score += 1.0
            analysis_factors.append('residual_architecture')
        
        # Training complexity
        training_config = config.get('training', {})
        
        # Mixed precision and advanced training features
        if training_config.get('mixed_precision', False):
            complexity_score += 0.5
            analysis_factors.append('mixed_precision')
        
        gradient_accumulation_steps = training_config.get('gradient_accumulation_steps', 1)
        if gradient_accumulation_steps > 1:
            complexity_score += 0.3 * min(gradient_accumulation_steps / 2, 2)
            analysis_factors.append('gradient_accumulation')
        
        # Advanced optimizers and schedulers
        optimizer = training_config.get('optimizer', 'Adam')
        if optimizer in ['AdamW', 'RMSprop', 'Adagrad']:
            complexity_score += 0.2
            analysis_factors.append('advanced_optimizer')
        elif optimizer == 'LBFGS':
            complexity_score += 0.5
            analysis_factors.append('second_order_optimizer')
        
        scheduler = training_config.get('scheduler')
        if scheduler in ['CosineAnnealingLR', 'ReduceLROnPlateau']:
            complexity_score += 0.2
            analysis_factors.append('adaptive_scheduler')
        elif scheduler in ['CyclicLR', 'OneCycleLR']:
            complexity_score += 0.4
            analysis_factors.append('cyclic_scheduler')
        
        # Regularization complexity
        dropout_rates = model_config.get('dropout_rates', [])
        if isinstance(dropout_rates, list) and len(dropout_rates) > 2:
            complexity_score += 0.3
            analysis_factors.append('complex_dropout')
        
        weight_decay = training_config.get('weight_decay', 0)
        if isinstance(weight_decay, (int, float)) and weight_decay > 1e-3:
            complexity_score += 0.1
            analysis_factors.append('strong_regularization')
        
        # Data complexity factors
        data_config = config.get('data', {})
        features = data_config.get('features', 20)
        if isinstance(features, int):
            if features > 100:
                complexity_score += 1.0
                analysis_factors.append('high_dimensional_data')
            elif features > 50:
                complexity_score += 0.5
                analysis_factors.append('medium_dimensional_data')
        
        # Advanced data preprocessing
        if data_config.get('synthetic_generation', {}).get('cluster_variance', 1.0) != 1.0:
            complexity_score += 0.2
            analysis_factors.append('custom_data_generation')
        
        # Security and monitoring complexity
        security_config = config.get('security', {})
        if security_config.get('enable_security_metrics', False):
            complexity_score += 0.3
            analysis_factors.append('security_monitoring')
        
        monitoring_config = config.get('monitoring', {})
        if monitoring_config.get('tensorboard_logging', False):
            complexity_score += 0.2
            analysis_factors.append('tensorboard_logging')
        
        if monitoring_config.get('wandb_logging', False):
            complexity_score += 0.3
            analysis_factors.append('wandb_integration')
        
        # Hardware-specific complexity
        hardware_config = config.get('hardware', {})
        if hardware_config.get('distributed_training', False):
            complexity_score += 2.0
            analysis_factors.append('distributed_training')
        
        if hardware_config.get('multi_gpu', False):
            complexity_score += 1.0
            analysis_factors.append('multi_gpu_training')
        
        # Determine complexity level with more granular categories
        if complexity_score < 2.0:
            level = 'low'
        elif complexity_score < 5.0:
            level = 'medium'
        elif complexity_score < 10.0:
            level = 'high'
        else:
            level = 'very_high'
        
        # Log analysis for debugging
        logger.debug(f"Complexity analysis: score={complexity_score:.2f}, level={level}, factors={analysis_factors}")
        
        return level
        
    except Exception as e:
        logger.warning(f"Error estimating config complexity: {e}")
        return 'unknown'

def determine_preset_recommendations(config: Dict[str, Any]) -> List[str]:
    """Determine comprehensive recommendations for what this preset is suitable for.
    
    Args:
        config: Configuration dictionary to analyze
        
    Returns:
        List of recommendation strings
    """
    recommendations = []
    
    try:
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        security_config = config.get('security', {})
        
        # Model type based recommendations
        model_type = model_config.get('model_type', '')
        if model_type == 'SimpleAutoencoder':
            recommendations.extend([
                'debugging and development',
                'prototyping new features',
                'resource-constrained environments',
                'educational purposes',
                'baseline comparisons',
                'rapid experimentation'
            ])
        elif model_type == 'EnhancedAutoencoder':
            recommendations.extend([
                'production deployment',
                'balanced performance needs',
                'configurable complexity scenarios',
                'standard anomaly detection tasks',
                'research and development',
                'performance optimization studies'
            ])
        elif model_type == 'AutoencoderEnsemble':
            recommendations.extend([
                'high accuracy requirements',
                'critical applications',
                'robust anomaly detection',
                'production systems with high stakes',
                'research requiring state-of-the-art performance',
                'applications where false negatives are costly'
            ])
        
        # Complexity-based recommendations
        complexity = estimate_config_complexity(config)
        if complexity == 'low':
            recommendations.extend([
                'beginner-friendly setups',
                'quick validation experiments',
                'resource-limited testing',
                'CI/CD pipeline integration'
            ])
        elif complexity == 'medium':
            recommendations.extend([
                'balanced complexity needs',
                'typical production workloads',
                'standard research applications'
            ])
        elif complexity == 'high':
            recommendations.extend([
                'advanced users',
                'complex anomaly patterns',
                'high-performance computing environments',
                'research pushing boundaries'
            ])
        elif complexity == 'very_high':
            recommendations.extend([
                'expert users only',
                'cutting-edge research',
                'specialized high-performance applications',
                'dedicated infrastructure requirements'
            ])
        
        # Training configuration recommendations
        batch_size = training_config.get('batch_size', 32)
        if batch_size <= 8:
            recommendations.extend([
                'severely memory-constrained environments',
                'edge computing applications',
                'single-sample inference needs'
            ])
        elif batch_size <= 32:
            recommendations.extend([
                'memory-constrained environments',
                'standard development setups',
                'typical research configurations'
            ])
        elif batch_size <= 128:
            recommendations.extend([
                'high-throughput scenarios',
                'batch processing applications',
                'GPU-optimized training'
            ])
        else:
            recommendations.extend([
                'very high-throughput scenarios',
                'large-scale data processing',
                'distributed computing environments'
            ])
        
        # Learning rate recommendations
        learning_rate = training_config.get('learning_rate', 0.001)
        if isinstance(learning_rate, (int, float)):
            if learning_rate >= 0.01:
                recommendations.append('fast convergence requirements')
            elif learning_rate <= 0.0001:
                recommendations.append('stable, fine-tuned training')
        
        # Mixed precision recommendations
        if training_config.get('mixed_precision', False):
            recommendations.extend([
                'GPU-accelerated training',
                'memory efficiency requirements',
                'modern hardware utilization'
            ])
        
        # Advanced training features
        if training_config.get('gradient_accumulation_steps', 1) > 1:
            recommendations.append('limited memory with large effective batch size needs')
        
        # Hardware-specific recommendations
        encoding_dim = model_config.get('encoding_dim', 12)
        hidden_dims = model_config.get('hidden_dims', [])
        
        total_params_estimate = 0
        if isinstance(hidden_dims, list) and hidden_dims:
            features = data_config.get('features', 20)
            total_params_estimate = features * hidden_dims[0]
            for i in range(len(hidden_dims) - 1):
                total_params_estimate += hidden_dims[i] * hidden_dims[i + 1]
            total_params_estimate += hidden_dims[-1] * encoding_dim
        
        if model_type == 'AutoencoderEnsemble':
            num_models = model_config.get('num_models', 3)
            total_params_estimate *= num_models
        
        if total_params_estimate < 10000:
            recommendations.extend([
                'CPU-only environments',
                'minimal resource scenarios',
                'embedded systems (with modifications)'
            ])
        elif total_params_estimate < 100000:
            recommendations.extend([
                'standard desktop environments',
                'entry-level GPU systems',
                'typical cloud instances'
            ])
        elif total_params_estimate < 1000000:
            recommendations.extend([
                'mid-range GPU systems',
                'professional workstations',
                'dedicated training servers'
            ])
        else:
            recommendations.extend([
                'high-end GPU systems',
                'specialized ML infrastructure',
                'enterprise-grade hardware'
            ])
        
        # Data characteristics recommendations
        features = data_config.get('features', 20)
        if isinstance(features, int):
            if features <= 10:
                recommendations.append('low-dimensional data analysis')
            elif features <= 50:
                recommendations.append('medium-dimensional data analysis')
            else:
                recommendations.append('high-dimensional data analysis')
        
        normal_samples = data_config.get('normal_samples', 8000)
        attack_samples = data_config.get('attack_samples', 2000)
        if isinstance(normal_samples, int) and isinstance(attack_samples, int):
            total_samples = normal_samples + attack_samples
            if total_samples < 1000:
                recommendations.append('small dataset scenarios')
            elif total_samples < 10000:
                recommendations.append('medium dataset scenarios')
            else:
                recommendations.append('large dataset scenarios')
        
        # Security-specific recommendations
        percentile = security_config.get('percentile', 95)
        if isinstance(percentile, (int, float)):
            if percentile >= 99:
                recommendations.append('high-security applications')
            elif percentile >= 95:
                recommendations.append('standard security requirements')
            else:
                recommendations.append('relaxed security thresholds')
        
        if security_config.get('enable_security_metrics', False):
            recommendations.append('security-focused deployments')
        
        # Use case pattern matching
        if 'high accuracy' in ' '.join(recommendations) and 'GPU' in ' '.join(recommendations):
            recommendations.append('mission-critical anomaly detection systems')
        
        if 'memory-constrained' in ' '.join(recommendations) and 'CPU' in ' '.join(recommendations):
            recommendations.append('IoT and edge computing deployments')
        
        if 'research' in ' '.join(recommendations) and complexity in ['high', 'very_high']:
            recommendations.append('academic and industrial research projects')
        
        # Remove duplicates and sort
        recommendations = list(set(recommendations))
        recommendations.sort()
        
        # Ensure we have at least one recommendation
        if not recommendations:
            recommendations = ['general purpose anomaly detection']
        
        return recommendations
        
    except Exception as e:
        logger.warning(f"Error determining preset recommendations: {e}")
        return ['general purpose']

def estimate_memory_requirements(config: Dict[str, Any]) -> str:
    """Estimate comprehensive memory requirements for the configuration.
    
    Args:
        config: Configuration dictionary to analyze
        
    Returns:
        Formatted string describing memory requirements
    """
    try:
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        hardware_config = config.get('hardware', {})
        
        # Extract key parameters
        model_type = model_config.get('model_type', 'SimpleAutoencoder')
        encoding_dim = model_config.get('encoding_dim', 12)
        hidden_dims = model_config.get('hidden_dims', [128, 64])
        features = data_config.get('features', 20)
        batch_size = training_config.get('batch_size', 64)
        mixed_precision = training_config.get('mixed_precision', False)
        
        # Normalize hidden_dims
        if not isinstance(hidden_dims, list):
            hidden_dims = [hidden_dims] if isinstance(hidden_dims, int) else [64]
        
        # Parameter count estimation with more accuracy
        total_params = 0
        
        if model_type == 'SimpleAutoencoder':
            # Encoder: input -> encoding
            # weights + bias
            total_params += features * encoding_dim + encoding_dim
            # Decoder: encoding -> output
            # weights + bias
            total_params += encoding_dim * features + features
            
        elif model_type == 'EnhancedAutoencoder':
            current_dim = features
            
            # Encoder layers
            for hidden_dim in hidden_dims:
                # weights + bias
                total_params += current_dim * hidden_dim + hidden_dim
                current_dim = hidden_dim
            
            # Bottleneck layer
            total_params += current_dim * encoding_dim + encoding_dim
            
            # Decoder layers (reverse)
            current_dim = encoding_dim
            for hidden_dim in reversed(hidden_dims):
                total_params += current_dim * hidden_dim + hidden_dim
                current_dim = hidden_dim
            
            # Output layer
            total_params += current_dim * features + features
            
            # Normalization parameters
            normalization = model_config.get('normalization')
            if normalization in ['batch', 'layer']:
                # Each layer has scale and shift parameters
                norm_params = sum(hidden_dims) * 2 + encoding_dim * 2
                total_params += norm_params
            
        elif model_type == 'AutoencoderEnsemble':
            num_models = model_config.get('num_models', 3)
            
            # Estimate single model parameters (simplified as Enhanced)
            single_model_params = 0
            if hidden_dims:
                current_dim = features
                # Simplified estimation
                for hidden_dim in hidden_dims[:min(2, len(hidden_dims))]:
                    single_model_params += current_dim * hidden_dim + hidden_dim
                    current_dim = hidden_dim
                single_model_params += current_dim * encoding_dim + encoding_dim
                single_model_params += encoding_dim * current_dim + current_dim
                single_model_params += current_dim * features + features
            else:
                single_model_params += features * encoding_dim + encoding_dim
                single_model_params += encoding_dim * features + features
            
            total_params = single_model_params * num_models
        
        # Memory calculations (in bytes)
        # float32 vs float16
        bytes_per_param = 4 if not mixed_precision else 2
        
        # Model parameters memory
        param_memory = total_params * bytes_per_param
        
        # Gradient memory (same size as parameters during training)
        gradient_memory = param_memory
        
        # Optimizer state memory (Adam uses 2x parameters for momentum and variance)
        optimizer = training_config.get('optimizer', 'Adam')
        if optimizer in ['Adam', 'AdamW']:
            optimizer_memory = param_memory * 2
        elif optimizer in ['SGD']:
            momentum = training_config.get('momentum', 0.9)
            optimizer_memory = param_memory if momentum > 0 else 0
        else:
            optimizer_memory = param_memory  # Conservative estimate
        
        # Activation memory (depends on batch size and architecture)
        activation_memory = 0
        
        # Input/output activations
        activation_memory += batch_size * features * bytes_per_param * 2
        
        # Hidden layer activations
        if model_type == 'EnhancedAutoencoder':
            for hidden_dim in hidden_dims:
                activation_memory += batch_size * hidden_dim * bytes_per_param
        
        # Encoding layer activation
        activation_memory += batch_size * encoding_dim * bytes_per_param
        
        # Ensemble multiplier
        if model_type == 'AutoencoderEnsemble':
            num_models = model_config.get('num_models', 3)
            activation_memory *= num_models
        
        # Additional memory for data loading and preprocessing
        data_memory = 0
        
        # Training data in memory
        normal_samples = data_config.get('normal_samples', 8000)
        attack_samples = data_config.get('attack_samples', 2000)
        total_samples = normal_samples + attack_samples
        
        # Assume data is kept in memory during training
        data_memory += total_samples * features * bytes_per_param
        
        # Validation and test sets
        validation_split = data_config.get('validation_split', 0.2)
        test_split = data_config.get('test_split', 0.2)
        data_memory += total_samples * (validation_split + test_split) * features * bytes_per_param
        
        # Buffer for data loading and augmentation
        data_memory *= 1.5
        
        # GPU-specific considerations
        gpu_overhead = 0
        if hardware_config.get('device', 'auto') != 'cpu':
            # GPU memory fragmentation and CUDA overhead
            # At least 100MB overhead
            gpu_overhead = max(param_memory * 0.1, 100 * 1024 * 1024)
        
        # Total memory calculation
        training_memory = param_memory + gradient_memory + optimizer_memory + activation_memory
        total_memory = training_memory + data_memory + gpu_overhead
        
        # Convert to human-readable format
        def format_memory(bytes_val):
            if bytes_val < 1024 ** 2:
                return f"{bytes_val / 1024:.1f} KB"
            elif bytes_val < 1024 ** 3:
                return f"{bytes_val / (1024 ** 2):.1f} MB"
            else:
                return f"{bytes_val / (1024 ** 3):.2f} GB"
        
        # Categorize memory requirements
        total_memory_mb = total_memory / (1024 ** 2)
        
        if total_memory_mb < 50:
            category = "Very Low"
            recommendation = "Suitable for any system"
        elif total_memory_mb < 200:
            category = "Low"
            recommendation = "Standard desktop/laptop"
        elif total_memory_mb < 1000:
            category = "Medium"
            recommendation = "8GB+ RAM, entry GPU"
        elif total_memory_mb < 4000:
            category = "High"
            recommendation = "16GB+ RAM, mid-range GPU"
        elif total_memory_mb < 16000:
            category = "Very High"
            recommendation = "32GB+ RAM, high-end GPU"
        else:
            category = "Extreme"
            recommendation = "Specialized hardware required"
        
        # Create detailed breakdown
        breakdown = {
            'model_parameters': format_memory(param_memory),
            'gradients': format_memory(gradient_memory),
            'optimizer_state': format_memory(optimizer_memory),
            'activations': format_memory(activation_memory),
            'data_storage': format_memory(data_memory),
            'gpu_overhead': format_memory(gpu_overhead) if gpu_overhead > 0 else "N/A",
            'total_training': format_memory(training_memory),
            'total_with_data': format_memory(total_memory)
        }
        
        # Format final result
        result = f"{category} ({format_memory(total_memory)}) - {recommendation}"
        
        # Add breakdown in debug mode
        logger.debug(f"Memory estimation breakdown: {breakdown}")
        
        return result
        
    except Exception as e:
        logger.warning(f"Error estimating memory requirements: {e}")
        return "Unknown - estimation failed"

def estimate_training_time(config: Dict[str, Any]) -> str:
    """Estimate training time based on configuration complexity and data size.
    
    Args:
        config: Configuration dictionary to analyze
        
    Returns:
        Formatted string describing estimated training time
    """
    try:
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        hardware_config = config.get('hardware', {})
        
        # Extract key parameters
        model_type = model_config.get('model_type', 'SimpleAutoencoder')
        encoding_dim = model_config.get('encoding_dim', 12)
        hidden_dims = model_config.get('hidden_dims', [128, 64])
        features = data_config.get('features', 20)
        batch_size = training_config.get('batch_size', 64)
        epochs = training_config.get('epochs', 100)
        mixed_precision = training_config.get('mixed_precision', False)
        
        # Data size
        normal_samples = data_config.get('normal_samples', 8000)
        attack_samples = data_config.get('attack_samples', 2000)
        total_samples = normal_samples + attack_samples
        
        # Calculate basic metrics
        steps_per_epoch = max(1, total_samples // batch_size)
        total_steps = steps_per_epoch * epochs
        
        # Base time per step estimation (in seconds)
        # 1ms baseline for very simple operations
        base_time_per_step = 0.001
        
        # Model complexity multiplier
        complexity_multiplier = 1.0
        
        if model_type == 'SimpleAutoencoder':
            complexity_multiplier = 1.0
        elif model_type == 'EnhancedAutoencoder':
            complexity_multiplier = 2.0
            
            # Hidden layer complexity
            if isinstance(hidden_dims, list):
                layer_complexity = len(hidden_dims) * 0.5
                size_complexity = sum(dim for dim in hidden_dims if isinstance(dim, (int, float))) / 1000
                complexity_multiplier += layer_complexity + size_complexity
            
            # Normalization overhead
            normalization = model_config.get('normalization')
            if normalization == 'batch':
                complexity_multiplier *= 1.2
            elif normalization == 'layer':
                complexity_multiplier *= 1.3
                
        elif model_type == 'AutoencoderEnsemble':
            num_models = model_config.get('num_models', 3)
            # Base ensemble overhead + linear scaling
            complexity_multiplier = 1.5 * num_models
        
        # Feature dimension impact
        if isinstance(features, int):
            # Normalize to 20 features baseline
            feature_multiplier = max(1.0, features / 20)
            complexity_multiplier *= feature_multiplier
        
        # Batch size impact (smaller batches = more overhead)
        if batch_size < 32:
            complexity_multiplier *= 1.5
        elif batch_size > 128:
            # Better GPU utilization
            complexity_multiplier *= 0.8
        
        # Activation function impact
        activation = model_config.get('activation', 'relu')
        if activation in ['gelu', 'swish']:
            complexity_multiplier *= 1.1
        elif activation in ['tanh', 'sigmoid']:
            complexity_multiplier *= 1.05
        
        # Hardware considerations
        device = hardware_config.get('device', 'auto')
        hardware_multiplier = 1.0
        
        # Try to detect actual hardware or use config
        try:
            if torch.cuda.is_available() and device != 'cpu':
                # GPU training - much faster
                hardware_multiplier = 0.1
                
                # GPU-specific optimizations
                if mixed_precision:
                    # Mixed precision speedup
                    hardware_multiplier *= 0.7
                
                # Multi-GPU
                if hardware_config.get('multi_gpu', False):
                    gpu_count = torch.cuda.device_count()
                    # Diminishing returns after 4 GPUs
                    hardware_multiplier /= min(gpu_count, 4)
                    
            else:
                # CPU training
                hardware_multiplier = 1.0
                
                # CPU-specific considerations
                num_workers = training_config.get('num_workers', 1)
                if num_workers > 1:
                    hardware_multiplier *= max(0.5, 1.0 / min(num_workers, 8))
                    
        except Exception:
            # Fallback assumptions
            if device == 'cpu':
                hardware_multiplier = 1.0
            else:
                # Assume some GPU acceleration
                hardware_multiplier = 0.2
        
        # Training-specific factors
        optimizer = training_config.get('optimizer', 'Adam')
        if optimizer in ['LBFGS']:
            # Second-order methods are much slower
            complexity_multiplier *= 3.0
        elif optimizer in ['AdamW', 'RMSprop']:
            complexity_multiplier *= 1.1
        
        # Gradient accumulation
        grad_accum_steps = training_config.get('gradient_accumulation_steps', 1)
        if grad_accum_steps > 1:
            # Some overhead for accumulation
            complexity_multiplier *= 1.2
        
        # Early stopping consideration
        patience = training_config.get('patience', 0)
        early_stop_factor = 1.0
        if patience > 0:
            # Assume early stopping might reduce training by 20-50%
            early_stop_factor = 0.7
        
        # Calculate total time
        time_per_step = base_time_per_step * complexity_multiplier * hardware_multiplier
        total_time_seconds = total_steps * time_per_step * early_stop_factor
        
        # Add overhead for data loading, checkpointing, etc.
        overhead_factor = 1.3
        total_time_seconds *= overhead_factor
        
        # Convert to human-readable format
        def format_time(seconds):
            if seconds < 60:
                return f"{seconds:.0f} seconds"
            elif seconds < 3600:
                return f"{seconds / 60:.1f} minutes"
            elif seconds < 86400:
                return f"{seconds / 3600:.1f} hours"
            else:
                return f"{seconds / 86400:.1f} days"
        
        # Determine category
        if total_time_seconds < 60:
            category = "Very Fast"
        # 10 minutes
        elif total_time_seconds < 600:
            category = "Fast"
        # 1 hour
        elif total_time_seconds < 3600:
            category = "Moderate"
        # 4 hours
        elif total_time_seconds < 14400:
            category = "Slow"
        # 1 day
        elif total_time_seconds < 86400:
            category = "Very Slow"
        else:
            category = "Extremely Slow"
        
        # Create estimate ranges (±50% uncertainty)
        min_time = total_time_seconds * 0.5
        max_time = total_time_seconds * 1.5
        
        # Format result
        if min_time < 60 and max_time > 60:
            result = f"{category} ({format_time(min_time)} - {format_time(max_time)})"
        else:
            result = f"{category} (~{format_time(total_time_seconds)})"
        
        # Add context information
        context_info = []
        if hardware_multiplier <= 0.2:
            context_info.append("GPU-accelerated")
        else:
            context_info.append("CPU-based")
            
        if mixed_precision and hardware_multiplier <= 0.2:
            context_info.append("mixed precision")
            
        if early_stop_factor < 1.0:
            context_info.append("with early stopping")
            
        if context_info:
            result += f" ({', '.join(context_info)})"
        
        # Debug information
        logger.debug(f"Training time estimation: steps={total_steps}, complexity_mult={complexity_multiplier:.2f}, "
                    f"hardware_mult={hardware_multiplier:.2f}, time_per_step={time_per_step:.6f}s")
        
        return result
        
    except Exception as e:
        logger.warning(f"Error estimating training time: {e}")
        return "Unknown - estimation failed"

def determine_resource_level(config: Dict[str, Any]) -> str:
    """Determine overall resource level required for the configuration.
    
    Args:
        config: Configuration dictionary to analyze
        
    Returns:
        String indicating resource level: 'minimal', 'low', 'medium', 'high', or 'extreme'
    """
    try:
        # Get individual assessments
        complexity = estimate_config_complexity(config)
        memory_req = estimate_memory_requirements(config)
        training_time = estimate_training_time(config)
        
        # Extract key indicators from other assessments
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        hardware_config = config.get('hardware', {})
        
        # Initialize scoring system
        resource_score = 0.0
        factors = []
        
        # Complexity contribution (30% weight)
        complexity_scores = {
            'low': 1.0,
            'medium': 2.5,
            'high': 4.0,
            'very_high': 6.0,
            'unknown': 2.0
        }
        resource_score += complexity_scores.get(complexity, 2.0) * 0.3
        factors.append(f"complexity_{complexity}")
        
        # Memory contribution (25% weight)
        if 'Very Low' in memory_req or 'KB' in memory_req:
            memory_score = 0.5
        elif 'Low' in memory_req and 'MB' in memory_req:
            memory_score = 1.0
        elif 'Medium' in memory_req:
            memory_score = 2.0
        elif 'High' in memory_req and 'GB' not in memory_req:
            memory_score = 3.5
        elif 'Very High' in memory_req or 'GB' in memory_req:
            memory_score = 5.0
        elif 'Extreme' in memory_req:
            memory_score = 6.0
        else:
            memory_score = 2.0
        
        resource_score += memory_score * 0.25
        factors.append(f"memory_score_{memory_score}")
        
        # Training time contribution (20% weight)
        if 'Very Fast' in training_time or 'Fast' in training_time:
            time_score = 1.0
        elif 'Moderate' in training_time:
            time_score = 2.0
        elif 'Slow' in training_time:
            time_score = 3.0
        elif 'Very Slow' in training_time:
            time_score = 4.0
        elif 'Extremely Slow' in training_time:
            time_score = 5.0
        else:
            time_score = 2.0
        
        resource_score += time_score * 0.20
        factors.append(f"time_score_{time_score}")
        
        # Model-specific factors (15% weight)
        model_type = model_config.get('model_type', 'SimpleAutoencoder')
        if model_type == 'SimpleAutoencoder':
            model_score = 1.0
        elif model_type == 'EnhancedAutoencoder':
            model_score = 2.0
            
            # Enhanced model complexity factors
            hidden_dims = model_config.get('hidden_dims', [])
            if isinstance(hidden_dims, list):
                if len(hidden_dims) > 3:
                    model_score += 0.5
                if any(dim > 256 for dim in hidden_dims if isinstance(dim, (int, float))):
                    model_score += 0.5
                    
        elif model_type == 'AutoencoderEnsemble':
            num_models = model_config.get('num_models', 3)
            model_score = 2.0 + (num_models - 1) * 0.5
        else:
            model_score = 2.0
        
        resource_score += model_score * 0.15
        factors.append(f"model_score_{model_score}")
        
        # Hardware requirements (10% weight)
        hardware_score = 1.0
        
        # GPU requirements
        if hardware_config.get('multi_gpu', False):
            hardware_score += 2.0
            factors.append("multi_gpu")
        elif hardware_config.get('device', 'auto') != 'cpu':
            hardware_score += 1.0
            factors.append("gpu_required")
        
        # Distributed training
        if hardware_config.get('distributed_training', False):
            hardware_score += 2.0
            factors.append("distributed")
        
        # Mixed precision (actually reduces requirements)
        if training_config.get('mixed_precision', False):
            hardware_score -= 0.2
            factors.append("mixed_precision_benefit")
        
        resource_score += hardware_score * 0.10
        factors.append(f"hardware_score_{hardware_score}")
        
        # Data size and complexity factors (bonus/penalty)
        features = data_config.get('features', 20)
        normal_samples = data_config.get('normal_samples', 8000)
        attack_samples = data_config.get('attack_samples', 2000)
        
        if isinstance(features, int) and features > 100:
            resource_score += 0.5
            factors.append("high_dimensional")
        
        total_samples = (normal_samples if isinstance(normal_samples, int) else 8000) + \
                       (attack_samples if isinstance(attack_samples, int) else 2000)
        
        if total_samples > 50000:
            resource_score += 0.5
            factors.append("large_dataset")
        elif total_samples < 1000:
            resource_score -= 0.2
            factors.append("small_dataset_benefit")
        
        # Batch size considerations
        batch_size = training_config.get('batch_size', 64)
        if isinstance(batch_size, int):
            if batch_size > 256:
                resource_score += 0.3
                factors.append("large_batch")
            elif batch_size < 8:
                resource_score -= 0.1
                factors.append("small_batch_benefit")
        
        # Determine final resource level
        if resource_score < 1.5:
            level = 'minimal'
            description = "Basic CPU, <4GB RAM"
        elif resource_score < 2.5:
            level = 'low'
            description = "Standard desktop, 4-8GB RAM"
        elif resource_score < 4.0:
            level = 'medium'
            description = "Mid-range GPU, 8-16GB RAM"
        elif resource_score < 5.5:
            level = 'high'
            description = "High-end GPU, 16-32GB RAM"
        else:
            level = 'extreme'
            description = "Specialized hardware, >32GB RAM"
        
        # Create detailed result
        result = f"{level} ({description})"
        
        # Debug information
        logger.debug(f"Resource level determination: score={resource_score:.2f}, level={level}, factors={factors}")
        
        return result
        
    except Exception as e:
        logger.warning(f"Error determining resource level: {e}")
        return 'unknown - assessment failed'

def get_config_cache_info() -> Dict[str, Any]:
    """Get information about the current configuration cache state."""
    global _cached_config, _config_cache_time
    
    current_time = time.time()
    cache_info = {
        'cache_exists': _cached_config is not None,
        'cache_time': _config_cache_time,
        'current_time': current_time,
        'cache_age_seconds': (current_time - _config_cache_time) if _config_cache_time else None,
        'cache_fresh': (_cached_config is not None and _config_cache_time is not None and 
                       current_time - _config_cache_time < 30),
        'cache_source': _cached_config.get('runtime', {}).get('config_source') if _cached_config else None,
        'active_preset': _cached_config.get('presets', {}).get('current_preset') if _cached_config else None
    }
    
    return cache_info

def save_config_interactive():
    """Interactive configuration saving with enhanced options, better error handling,
    and intelligent memory management for optimal performance during extensive operations."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        show_banner()
        
        # INITIAL MEMORY OPTIMIZATION - Get hardware context early for memory-aware processing
        hardware_data = None
        total_ram_gb = 8.0  # Conservative default
        
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
            
            # Initial memory cleanup for memory-constrained systems before interactive operations
            initial_clear_results = enhanced_clear_memory(
                aggressive=total_ram_gb < 8,  # More aggressive on low-memory systems
                hardware_data=hardware_data
            )
            
            if initial_clear_results.get('success'):
                logger.debug(f"Initial memory optimization for interactive save: {', '.join(initial_clear_results.get('actions_taken', []))}")
        except Exception as e:
            logger.debug(f"Initial memory optimization failed: {e}")
        
        config = get_current_config()
        
        #print(Fore.CYAN + Style.BRIGHT + "\n" + "="*40)
        print(Fore.GREEN + Style.BRIGHT + "INTERACTIVE CONFIGURATION SAVE")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Show current configuration summary
        preset_used = config.get('presets', {}).get('current_preset', 'none')
        model_type = config.get('model', {}).get('model_type', 'unknown')
        print(Fore.YELLOW + Style.BRIGHT + f"Current configuration:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_used}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model type: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Sections: " + Fore.YELLOW + Style.BRIGHT + f"{len(config)}")
        
        # Get save options
        print(Fore.YELLOW + Style.BRIGHT + f"\nSave options:")
        print(Fore.WHITE + Style.BRIGHT + f"1. Save to default location " + Fore.GREEN + Style.BRIGHT + f"({CONFIG_FILE})")
        print(Fore.WHITE + Style.BRIGHT + f"2. Save with a specific name")
        print(Fore.WHITE + Style.BRIGHT + f"3. Save to custom path")
        print(Fore.WHITE + Style.BRIGHT + f"4. Save both to default and with name")
        print(Fore.WHITE + Style.BRIGHT + f"5. Show named configurations")
        print(Fore.RED + Style.BRIGHT + f"0. Cancel")
        
        while True:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect option (0-5): ").strip()
                
                if choice == '1':
                    # Save to default location
                    try:
                        # MEMORY OPTIMIZATION - Clear memory before save operation
                        if total_ram_gb < 16:
                            try:
                                pre_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                                if pre_save_clear.get('success'):
                                    logger.debug("Memory optimized before default save")
                            except Exception as e:
                                logger.debug(f"Pre-save memory optimization failed: {e}")
                        
                        saved_path = save_config(config)
                        print(Fore.GREEN + Style.BRIGHT + f"Configuration saved to: {saved_path}")
                        break
                    except Exception as e:
                        print(Fore.RED + Style.BRIGHT + f"Failed to save configuration: {e}")
                        break
                
                elif choice == '2':
                    # Save with specific name
                    name = input(Fore.YELLOW + Style.BRIGHT + "Enter configuration name: ").strip()
                    if name:
                        try:
                            # MEMORY OPTIMIZATION - Clear memory before save operation
                            if total_ram_gb < 16:
                                try:
                                    pre_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                                    if pre_save_clear.get('success'):
                                        logger.debug("Memory optimized before named save")
                                except Exception as e:
                                    logger.debug(f"Pre-save memory optimization failed: {e}")
                            
                            saved_path = save_config(config, name=name)
                            print(Fore.GREEN + Style.BRIGHT + f"Configuration saved as '{name}': {saved_path}")
                            break
                        except Exception as e:
                            print(Fore.RED + Style.BRIGHT + f"Failed to save configuration '{name}': {e}")
                            break
                    else:
                        print(Fore.YELLOW + Style.BRIGHT + "Name cannot be empty")
                
                elif choice == '3':
                    # Save to custom path
                    path_str = input(Fore.YELLOW + Style.BRIGHT + "Enter custom path: ").strip()
                    if path_str:
                        try:
                            # MEMORY OPTIMIZATION - Clear memory before save operation
                            if total_ram_gb < 16:
                                try:
                                    pre_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                                    if pre_save_clear.get('success'):
                                        logger.debug("Memory optimized before custom path save")
                                except Exception as e:
                                    logger.debug(f"Pre-save memory optimization failed: {e}")
                            
                            custom_path = Path(path_str)
                            saved_path = save_config(config, config_path=custom_path)
                            print(Fore.GREEN + Style.BRIGHT + f"Configuration saved to: {saved_path}")
                            break
                        except Exception as e:
                            print(Fore.RED + Style.BRIGHT + f"Failed to save to '{path_str}': {e}")
                            break
                    else:
                        print(Fore.YELLOW + Style.BRIGHT + "Path cannot be empty")
                
                elif choice == '4':
                    # Save both to default and with name
                    name = input(Fore.YELLOW + Style.BRIGHT + "Enter configuration name: ").strip()
                    if name:
                        try:
                            # MEMORY OPTIMIZATION - Clear memory before dual save operation
                            if total_ram_gb < 16:
                                try:
                                    pre_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                                    if pre_save_clear.get('success'):
                                        logger.debug("Memory optimized before dual save")
                                except Exception as e:
                                    logger.debug(f"Pre-save memory optimization failed: {e}")
                            
                            # This will save to both default location and as named config
                            saved_path = save_config(config, config_path=CONFIG_FILE, name=name)
                            print(Fore.GREEN + Style.BRIGHT + f"Configuration saved to default location and as '{name}'")
                            print(Fore.GREEN + Style.BRIGHT + f"  - Primary: {saved_path}")
                            break
                        except Exception as e:
                            print(Fore.RED + Style.BRIGHT + f"Failed to save configuration: {e}")
                            break
                    else:
                        print(Fore.YELLOW + Style.BRIGHT + "Name cannot be empty")
                
                elif choice == '5':
                    # Show named configurations with improved error handling and memory optimization
                    try:
                        # MEMORY OPTIMIZATION - Clear memory before intensive list operation
                        try:
                            pre_list_clear = enhanced_clear_memory(
                                aggressive=total_ram_gb < 8,  # More aggressive for low-memory systems
                                hardware_data=hardware_data
                            )
                            if pre_list_clear.get('success'):
                                logger.debug(f"Memory optimized before config listing: {', '.join(pre_list_clear.get('actions_taken', []))}")
                        except Exception as e:
                            logger.debug(f"Pre-list memory optimization failed: {e}")
                        
                        saved_configs_info = list_saved_configs()
                        
                        # Ensure we have the expected dictionary structure
                        if not isinstance(saved_configs_info, dict):
                            print(Fore.RED + Style.BRIGHT + "Invalid configuration data format")
                            continue
                        
                        all_configs = saved_configs_info.get("all_configs", {})
                        named_configs = saved_configs_info.get("named_configs", {})
                        
                        if all_configs:
                            print(Fore.YELLOW + Style.BRIGHT + f"\nExisting configurations:")
                            print(Fore.WHITE + Style.BRIGHT + f"{'Name':<20} {'Type':<8} {'Model Type':<15} {'Preset':<15} {'Modified':<20}")
                            print("-" * 85)
                            
                            # MEMORY OPTIMIZATION - Process configs in chunks for large lists
                            config_items = list(all_configs.items())
                            chunk_size = 50 if total_ram_gb >= 8 else 25  # Smaller chunks on low-memory systems
                            
                            for i in range(0, len(config_items), chunk_size):
                                chunk = config_items[i:i + chunk_size]
                                
                                for name, config_info in chunk:
                                    try:
                                        # Safely extract information with defaults
                                        config_type = config_info.get('type', 'unknown')[:7]
                                        model_type = str(config_info.get('model_type', 'N/A'))[:14]
                                        preset = str(config_info.get('preset_used', 'none'))[:14]
                                        
                                        # Handle modified time
                                        modified = config_info.get('modified', '')
                                        if modified:
                                            try:
                                                # Parse ISO format and format for display
                                                if 'T' in modified:
                                                    dt = datetime.fromisoformat(modified.replace('Z', '+00:00'))
                                                    modified_str = dt.strftime('%Y-%m-%d %H:%M')[:19]
                                                else:
                                                    modified_str = modified[:19]
                                            except Exception:
                                                modified_str = 'unknown'[:19]
                                        else:
                                            modified_str = 'unknown'[:19]
                                        
                                        print(Fore.WHITE + Style.BRIGHT + f"{name[:19]:<20} {config_type:<8} {model_type:<15} {preset:<15} {modified_str:<20}")
                                        
                                    except Exception as e:
                                        logger.debug(f"Error displaying config info for {name}: {e}")
                                        print(Fore.RED + Style.BRIGHT + f"{name[:19]:<20} {'error':<8} {'N/A':<15} {'N/A':<15} {'unknown':<20}")
                                
                                # MEMORY OPTIMIZATION - Clear memory between chunks for very large lists
                                if len(config_items) > 100 and total_ram_gb < 16 and i + chunk_size < len(config_items):
                                    try:
                                        chunk_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                                        if chunk_clear.get('success'):
                                            logger.debug(f"Memory optimized between config chunks at position {i + chunk_size}")
                                    except Exception as e:
                                        logger.debug(f"Chunk memory optimization failed: {e}")
                            
                            print(Fore.YELLOW + Style.BRIGHT + f"\nTotal configurations: {len(all_configs)}")
                            if named_configs:
                                print(Fore.YELLOW + Style.BRIGHT + f"Named configurations: {len(named_configs)}")
                        else:
                            print(Fore.RED + Style.BRIGHT + "\nNo configurations found.")
                        
                        # MEMORY OPTIMIZATION - Clear memory after displaying large config lists
                        if len(all_configs) > 50:
                            try:
                                post_list_clear = enhanced_clear_memory(
                                    aggressive=len(all_configs) > 100,  # More aggressive for very large lists
                                    hardware_data=hardware_data
                                )
                                if post_list_clear.get('success'):
                                    logger.debug(f"Post-list memory optimization: {', '.join(post_list_clear.get('actions_taken', []))}")
                            except Exception as e:
                                logger.debug(f"Post-list memory optimization failed: {e}")
                            
                    except Exception as e:
                        print(Fore.RED + Style.BRIGHT + f"Failed to list configurations: {e}")
                        logger.error(Fore.RED + Style.BRIGHT + f"Configuration listing failed: {e}")
                    
                    # Continue the loop to show options again
                
                elif choice == '0':
                    print("Save cancelled.")
                    break
                
                else:
                    print("Please enter 0-5")
                    
            except KeyboardInterrupt:
                print(Fore.RED + Style.BRIGHT + "\nSave cancelled.")
                break
            except Exception as e:
                print(Fore.RED + Style.BRIGHT + f"Error during interactive save: {e}")
                logger.error(f"Interactive save error: {e}")
                break
        
        # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
        # Aggressive cleanup after interactive session completion
        try:
            final_clear_results = enhanced_clear_memory(
                aggressive=True,  # Aggressive final cleanup
                hardware_data=hardware_data
            )
            
            if final_clear_results.get('success'):
                logger.debug(f"Final interactive save memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
                
        except Exception as e:
            logger.debug(f"Final interactive save memory optimization failed: {e}")
                
    except Exception as e:
        print(Fore.RED + Style.BRIGHT + f"Failed to start interactive save: {e}")
        logger.error(f"Interactive save initialization failed: {e}")
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after interactive save error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")

def save_config(config: Dict, config_path: Optional[Union[Path, str]] = None, name: Optional[str] = None) -> Path:
    """Save config with enhanced metadata, backup handling, validation, named configuration support,
    and intelligent memory management for optimal performance.
    
    This function can save configurations in multiple ways:
    1. To the default config file (when no parameters provided)
    2. To a specific path (when config_path provided)  
    3. As a named configuration (when name provided)
    4. Both to default location and as named config (when both provided)
    
    Args:
        config: Configuration dictionary to save
        config_path: Optional path where to save the configuration. If None, uses CONFIG_FILE
        name: Optional name for creating a named configuration file. Creates file as "{name}.json"
        
    Returns:
        Path: The primary path where the configuration was saved
        
    Raises:
        RuntimeError: If configuration save fails
        ValueError: If configuration is invalid or parameters are invalid
        TypeError: If parameters are of wrong type
    """
    try:
        # INITIAL MEMORY OPTIMIZATION - Get hardware context early for memory-aware processing
        hardware_data = None
        total_ram_gb = 8.0  # Conservative default
        
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
            
            # Initial memory cleanup for memory-constrained systems before processing large configs
            if total_ram_gb < 16 and len(str(config)) > 50000:  # Large config (>50KB serialized)
                initial_clear_results = enhanced_clear_memory(
                    aggressive=total_ram_gb < 8,  # More aggressive on low-memory systems
                    hardware_data=hardware_data
                )
                
                if initial_clear_results.get('success'):
                    logger.debug(f"Initial memory optimization for config saving: {', '.join(initial_clear_results.get('actions_taken', []))}")
        except Exception as e:
            logger.debug(f"Initial memory optimization failed: {e}")
        
        # Input validation
        if not isinstance(config, dict):
            raise TypeError("Configuration must be a dictionary")
        
        if config_path is not None and not isinstance(config_path, (str, Path)):
            raise TypeError("config_path must be a string or Path object")
        
        if name is not None and not isinstance(name, str):
            raise TypeError("name must be a string")
        
        if name is not None and not name.strip():
            raise ValueError("name cannot be empty or whitespace-only")
        
        # Validate configuration before saving
        logger.debug("Validating configuration before save")
        validate_config(config)
        
        # MEMORY OPTIMIZATION CHECKPOINT - Clear memory after validation for large configs
        if len(str(config)) > 75000 and total_ram_gb < 16:  # Very large config
            try:
                post_validation_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if post_validation_clear.get('success'):
                    logger.debug(f"Post-validation memory optimization: {', '.join(post_validation_clear.get('actions_taken', []))}")
            except Exception as e:
                logger.debug(f"Post-validation memory optimization failed: {e}")
        
        # Determine primary save path
        primary_path = None
        if config_path is not None:
            primary_path = Path(config_path)
        elif name is not None:
            # When only name is provided, create named config in CONFIG_DIR
            CONFIG_DIR.mkdir(parents=True, exist_ok=True)
            primary_path = CONFIG_DIR / f"{name.strip()}.json"
        else:
            # Default behavior - save to CONFIG_FILE
            primary_path = CONFIG_FILE
        
        # Sanitize name if provided for additional operations
        safe_name = None
        if name is not None:
            safe_name = "".join(c for c in name.strip() if c.isalnum() or c in (' ', '_', '-')).strip()
            safe_name = safe_name.replace(' ', '_').lower()
            
            if not safe_name:
                raise ValueError(f"Invalid configuration name '{name}' - must contain alphanumeric characters")
            
            if len(safe_name) > 50:
                safe_name = safe_name[:50]
                logger.warning(f"Configuration name truncated to: {safe_name}")
        
        # Prepare comprehensive metadata
        save_metadata = {
            "created": config.get('metadata', {}).get('created', datetime.now().isoformat()),
            "modified": datetime.now().isoformat(),
            "version": "2.1",
            "system": {
                "python_version": platform.python_version(),
                "pytorch_version": torch.__version__ if 'torch' in globals() else "unknown",
                "cuda_available": torch.cuda.is_available() if 'torch' in globals() else False,
                "hostname": platform.node(),
                "os": platform.system(),
                "platform_release": platform.release(),
                "architecture": platform.machine()
            },
            "config": {
                "preset_used": config.get('presets', {}).get('current_preset', 'none'),
                "model_type": config.get('model', {}).get('model_type', 'unknown'),
                "sections": list(config.keys()),
                "total_parameters": sum(len(v) if isinstance(v, dict) else 1 for v in config.values()),
                "checksum": generate_config_checksum(config)
            },
            "save_info": {
                "save_reason": "named_save" if name else "manual_save",
                "backup_created": False,
                "atomic_write": True,
                "named_config": safe_name,
                "primary_path": str(primary_path),
                "save_mode": "named_only" if name and config_path is None else "path_specified" if config_path else "default"
            }
        }
        
        # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before metadata processing for large configs
        if len(str(save_metadata)) > 10000 and total_ram_gb < 16:
            try:
                pre_metadata_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if pre_metadata_clear.get('success'):
                    logger.debug(f"Pre-metadata memory optimization: {', '.join(pre_metadata_clear.get('actions_taken', []))}")
            except Exception as e:
                logger.debug(f"Pre-metadata memory optimization failed: {e}")
        
        # Create comprehensive configuration structure
        full_config = {
            "metadata": save_metadata,
            "config": config
        }
        
        # Function to perform atomic save operation
        def atomic_save_to_path(target_path: Path, config_data: Dict, create_backup: bool = True) -> bool:
            """Perform atomic save to a specific path with backup handling."""
            backup_created = False
            
            # Enhanced backup handling with versioning and cleanup
            if create_backup and target_path.exists():
                backup_created = create_config_backup(target_path, save_metadata)
            
            # Ensure directory exists
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            # MEMORY OPTIMIZATION - Clear memory before intensive file operations
            if total_ram_gb < 8:
                try:
                    pre_write_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
                    if pre_write_clear.get('success'):
                        logger.debug("Memory optimized before atomic write operation")
                except Exception as e:
                    logger.debug(f"Pre-write memory optimization failed: {e}")
            
            # Atomic write operation with enhanced error handling
            temp_path = target_path.with_suffix(f".tmp_{int(time.time())}_{os.getpid()}")
            try:
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(config_data, f, indent=4, ensure_ascii=False, sort_keys=False)
                
                # Verify the written file can be read back
                with open(temp_path, 'r', encoding='utf-8') as f:
                    verification_data = json.load(f)
                    if not verification_data.get('config'):
                        raise ValueError("Verification failed: saved config is empty or invalid")
                
                # MEMORY OPTIMIZATION - Clear memory after file verification
                if total_ram_gb < 16:
                    try:
                        post_verify_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                        if post_verify_clear.get('success'):
                            logger.debug("Memory optimized after file verification")
                    except Exception as e:
                        logger.debug(f"Post-verification memory optimization failed: {e}")
                
                # Atomic replacement
                # Windows requires unlinking existing file before replacement
                if os.name == 'nt':
                    if target_path.exists():
                        # Remove existing file on Windows
                        target_path.unlink()
                temp_path.replace(target_path)
                
                logger.info(f"Configuration successfully saved to {target_path}")
                return backup_created
                
            except Exception as e:
                # Clean up temp file if write failed
                if temp_path.exists():
                    try:
                        temp_path.unlink()
                    except:
                        pass
                raise RuntimeError(f"Failed to write configuration file {target_path}: {e}") from e
        
        # Save to primary path
        primary_backup_created = atomic_save_to_path(primary_path, full_config, create_backup=True)
        save_metadata["save_info"]["backup_created"] = primary_backup_created
        
        # Handle additional save scenarios
        additional_saves = []
        
        # If both name and config_path are provided, also save to CONFIG_DIR as named config
        if name is not None and config_path is not None:
            named_path = CONFIG_DIR / f"{safe_name}.json"
            if named_path != primary_path:  # Avoid duplicate saves
                try:
                    named_backup_created = atomic_save_to_path(named_path, full_config, create_backup=True)
                    additional_saves.append({
                        'path': named_path,
                        'type': 'named_config',
                        'backup_created': named_backup_created
                    })
                    logger.info(f"Configuration also saved as named config: {named_path}")
                except Exception as e:
                    logger.warning(f"Failed to save additional named configuration to {named_path}: {e}")
        
        # MEMORY OPTIMIZATION CHECKPOINT - Clear memory between save operations
        if len(additional_saves) > 0 and total_ram_gb < 16:
            try:
                mid_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if mid_save_clear.get('success'):
                    logger.debug(f"Mid-save memory optimization: {', '.join(mid_save_clear.get('actions_taken', []))}")
            except Exception as e:
                logger.debug(f"Mid-save memory optimization failed: {e}")
        
        # If name provided but saving to default location, also create quick-access named file
        elif name is not None and config_path is None and primary_path == CONFIG_FILE:
            # This case is already handled by primary_path logic above
            pass
        
        # If saving to custom path but no name, offer to create a named version
        elif name is None and config_path is not None and config_path != CONFIG_FILE:
            # Extract a reasonable name from the path
            auto_name = Path(config_path).stem
            if auto_name and auto_name != "config":
                try:
                    auto_named_path = CONFIG_DIR / f"{auto_name}.json"
                    if auto_named_path != primary_path:
                        auto_backup_created = atomic_save_to_path(auto_named_path, full_config, create_backup=False)
                        additional_saves.append({
                            'path': auto_named_path,
                            'type': 'auto_named',
                            'backup_created': auto_backup_created
                        })
                        logger.info(f"Configuration auto-saved with name '{auto_name}': {auto_named_path}")
                except Exception as e:
                    logger.debug(f"Failed to create auto-named configuration: {e}")
        
        # Update metadata with additional save information
        if additional_saves:
            save_metadata["save_info"]["additional_saves"] = additional_saves
            save_metadata["save_info"]["total_locations"] = 1 + len(additional_saves)
        
        # Invalidate cache after successful save
        invalidate_config_cache()
        
        # Handle preset-specific save operations
        current_preset = config.get('presets', {}).get('current_preset')
        if current_preset and current_preset not in PRESET_CONFIGS:
            try:
                custom_preset_path = save_custom_preset(current_preset, config)
                logger.info(f"Custom preset '{current_preset}' saved to {custom_preset_path}")
                save_metadata["save_info"]["custom_preset_saved"] = str(custom_preset_path)
            except Exception as e:
                logger.warning(f"Failed to save custom preset '{current_preset}': {e}")
                save_metadata["save_info"]["custom_preset_error"] = str(e)
        
        # Create/update named configuration registry
        if name is not None:
            try:
                update_named_config_registry(safe_name, primary_path, save_metadata)
            except Exception as e:
                logger.warning(f"Failed to update named configuration registry: {e}")
        
        # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
        # Aggressive cleanup after save operations completion
        try:
            final_clear_results = enhanced_clear_memory(
                aggressive=True,  # Aggressive final cleanup
                hardware_data=hardware_data
            )
            
            if final_clear_results.get('success'):
                logger.debug(f"Final save memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
                
        except Exception as e:
            logger.debug(f"Final save memory optimization failed: {e}")
        
        # Log comprehensive save statistics
        config_size = primary_path.stat().st_size if primary_path.exists() else 0
        total_locations = 1 + len(additional_saves)
        
        logger.info(f"Configuration save completed:")
        logger.info(f"  - Primary location: {primary_path}")
        logger.info(f"  - File size: {config_size} bytes")
        logger.info(f"  - Sections: {len(config)}")
        logger.info(f"  - Total save locations: {total_locations}")
        logger.info(f"  - Memory optimizations applied during save process")
        
        if primary_backup_created:
            logger.info("  - Previous configuration backed up")
        
        if additional_saves:
            logger.info(f"  - Additional saves: {len(additional_saves)}")
            for save_info in additional_saves:
                logger.info(f"    * {save_info['type']}: {save_info['path']}")
        
        return primary_path
        
    except ValueError as e:
        logger.error(f"Configuration validation failed during save: {e}")
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after save error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        raise
    except Exception as e:
        logger.error(f"Failed to save configuration: {str(e)}", exc_info=True)
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after save error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        raise RuntimeError(f"Configuration save failed: {str(e)}") from e

def create_config_backup(config_path: Path, save_metadata: Dict) -> bool:
    """Create a backup of the existing configuration with enhanced versioning."""
    try:
        backup_dir = config_path.parent / "backups"
        backup_dir.mkdir(exist_ok=True)
        
        # Enhanced backup naming with metadata
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        version = save_metadata.get('version', '2.1')
        preset_used = save_metadata.get('config', {}).get('preset_used', 'unknown')
        
        backup_name = f"{config_path.stem}_v{version}_{preset_used}_{timestamp}{config_path.suffix}"
        backup_path = backup_dir / backup_name
        
        # Copy with metadata preservation
        shutil.copy2(config_path, backup_path)
        
        # Cleanup old backups (keep last 10)
        cleanup_old_backups(backup_dir, config_path.stem, keep_count=10)
        
        logger.info(f"Configuration backup created: {backup_path}")
        return True
        
    except Exception as e:
        logger.warning(f"Failed to create backup: {e}")
        return False

def cleanup_old_backups(backup_dir: Path, config_stem: str, keep_count: int = 10):
    """Clean up old backup files keeping only the most recent ones."""
    try:
        backup_pattern = f"{config_stem}_v*"
        backup_files = list(backup_dir.glob(backup_pattern))
        
        if len(backup_files) > keep_count:
            # Sort by modification time (newest first)
            backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # Remove old backups
            for old_backup in backup_files[keep_count:]:
                old_backup.unlink()
                logger.debug(f"Removed old backup: {old_backup}")
                
    except Exception as e:
        logger.debug(f"Failed to cleanup old backups: {e}")

def prompt_user_for_migration_fallback() -> bool:
    """Prompt user for migration failure handling."""
    if sys.stdin.isatty():
        try:
            response = input("Configuration migration failed. Use default configuration? [Y/n]: ").strip().lower()
            return response in ['', 'y', 'yes']
        except:
            # Default to yes if input fails
            return True
    # Non-interactive mode defaults to yes
    return True

def handle_validation_failure(error: ValueError, failed_config: Dict, default_config: Dict) -> bool:
    """Handle configuration validation failure with user interaction."""
    logger.error(f"Configuration validation error: {error}")
    
    if sys.stdin.isatty():
        try:
            print(f"\nConfiguration validation failed: {error}")
            print("Options:")
            print("1. Use default configuration (recommended)")
            print("2. Attempt to fix and retry")
            print("3. Exit with error")
            
            choice = input("Choose option [1-3]: ").strip()
            
            if choice == '1':
                return True
            elif choice == '2':
                # Could implement basic fix attempts here
                logger.info("Automatic fixes not yet implemented")
                return True
            else:
                return False
                
        except:
            # Default to using default config if input fails
            return True
    
    # Non-interactive mode uses default config
    return True

def initialize_config(config_path: Path = CONFIG_FILE) -> Dict[str, Any]:
    """Initialize or load configuration with preset awareness, validation, fallback handling, and intelligent memory management.
    
    Args:
        config_path: Path to the configuration file
        
    Returns:
        Dictionary containing the initialized configuration
        
    Raises:
        ValueError: If configuration cannot be initialized
        RuntimeError: If configuration initialization fails
    """
    initialization_start_time = time.time()
    
    try:
        logger.info(f"Initializing configuration from {config_path}")
        
        # Get hardware context early and perform initial memory cleanup
        hardware_data = None
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 4.0)
            
            # Initial aggressive memory cleanup for low-memory systems
            initial_clear_results = enhanced_clear_memory(
                aggressive=total_ram_gb < 8,  # More aggressive on low-memory systems
                hardware_data=hardware_data
            )
            
            if initial_clear_results.get('success'):
                logger.debug(f"Initial memory optimization: {', '.join(initial_clear_results.get('actions_taken', []))}")
            
        except Exception as e:
            logger.debug(f"Initial memory optimization failed: {e}")
            total_ram_gb = 4.0
        
        # Track memory optimization throughout initialization
        memory_optimization_log = {
            'initial_cleanup': {
                'success': initial_clear_results.get('success', False) if 'initial_clear_results' in locals() else False,
                'actions': initial_clear_results.get('actions_taken', []) if 'initial_clear_results' in locals() else [],
                'timestamp': datetime.now().isoformat()
            },
            'hardware_context': {
                'total_ram_gb': total_ram_gb,
                'cuda_available': hardware_data.get('cuda', {}).get('available', False) if hardware_data else False,
                'memory_pressure': 'high' if total_ram_gb < 8 else 'medium' if total_ram_gb < 16 else 'low'
            },
            'checkpoints': []
        }
        
        # Step 1: Configuration Loading with Memory Management
        loaded_config = None
        load_error = None
        load_method = None
        
        # Clear memory before intensive file operations on low-memory systems
        if total_ram_gb < 8:
            try:
                pre_load_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if pre_load_clear.get('success'):
                    memory_optimization_log['checkpoints'].append({
                        'stage': 'pre_load',
                        'actions': pre_load_clear.get('actions_taken', []),
                        'timestamp': datetime.now().isoformat()
                    })
            except Exception as e:
                logger.debug(f"Pre-load memory optimization failed: {e}")
        
        try:
            loaded_config = load_config(config_path)
            if loaded_config:
                load_method = "primary_path"
                logger.info("Successfully loaded existing configuration from primary path")
                
                # Clear memory after loading large configurations
                if len(str(loaded_config)) > 50000:  # Large config (>50KB serialized)
                    try:
                        post_load_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                        if post_load_clear.get('success'):
                            memory_optimization_log['checkpoints'].append({
                                'stage': 'post_large_config_load',
                                'actions': post_load_clear.get('actions_taken', []),
                                'timestamp': datetime.now().isoformat()
                            })
                    except Exception as e:
                        logger.debug(f"Post-load memory optimization failed: {e}")
            else:
                logger.debug("Primary config file exists but returned empty configuration")
                load_method = "empty_primary"
        except FileNotFoundError:
            logger.info(f"No configuration file found at {config_path} - will create new")
            load_error = "file_not_found"
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"Configuration file corrupted or invalid: {e}")
            load_error = f"corrupted_config: {str(e)}"
        except PermissionError as e:
            logger.error(f"Permission denied accessing {config_path}: {e}")
            load_error = f"permission_denied: {str(e)}"
        except Exception as e:
            logger.error(f"Unexpected error loading configuration: {e}")
            load_error = f"unexpected_error: {str(e)}"
        
        # Step 2: Memory-Optimized Fallback Processing
        if not loaded_config and load_error:
            logger.info(f"Primary configuration load failed ({load_error}), attempting fallback strategies")
            
            # Clear memory before fallback processing
            try:
                fallback_clear = enhanced_clear_memory(
                    aggressive=True,  # More aggressive clearing before fallbacks
                    hardware_data=hardware_data
                )
                
                if fallback_clear.get('success'):
                    memory_optimization_log['checkpoints'].append({
                        'stage': 'pre_fallback',
                        'actions': fallback_clear.get('actions_taken', []),
                        'timestamp': datetime.now().isoformat()
                    })
            except Exception as e:
                logger.debug(f"Pre-fallback memory optimization failed: {e}")
            
            # Fallback 1: Try backup configurations
            try:
                backup_config = try_load_from_backup(config_path)
                if backup_config:
                    loaded_config = backup_config
                    load_method = "backup_recovery"
                    logger.info("Successfully loaded configuration from backup")
                else:
                    logger.debug("No valid backup configurations found")
            except Exception as e:
                logger.debug(f"Backup recovery failed: {e}")
            
            # Fallback 2: Try loading from alternative locations
            if not loaded_config:
                alternative_paths = [
                    CONFIG_DIR / "config.json.bak",
                    CONFIG_DIR / "last_known_good.json",
                    CONFIG_DIR / "autobackup.json",
                    config_path.parent / f"{config_path.stem}_backup{config_path.suffix}",
                    config_path.parent / f"backup_{config_path.name}"
                ]
                
                for alt_path in alternative_paths:
                    if alt_path.exists() and alt_path != config_path:
                        try:
                            logger.debug(f"Trying alternative configuration: {alt_path}")
                            alt_config = load_config(alt_path)
                            if alt_config and isinstance(alt_config, dict):
                                loaded_config = alt_config
                                load_method = f"alternative_path_{alt_path.name}"
                                logger.info(f"Successfully loaded configuration from alternative path: {alt_path}")
                                break
                        except Exception as e:
                            logger.debug(f"Alternative path {alt_path} failed: {e}")
                            continue
            
            # Memory cleanup between fallback strategies for memory-constrained systems
            if total_ram_gb < 16 and not loaded_config:
                try:
                    enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                except Exception as e:
                    logger.debug(f"Intermediate fallback memory cleanup failed: {e}")
            
            # Fallback 3: Try to recover from corrupted file
            if not loaded_config and config_path.exists():
                try:
                    logger.info("Attempting to recover from corrupted configuration file")
                    with open(config_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    recovered_config = attempt_json_recovery(content, config_path)
                    if recovered_config:
                        loaded_config = recovered_config
                        load_method = "json_recovery"
                        logger.info("Successfully recovered configuration from corrupted file")
                        
                        # Save recovered config as backup using the dedicated function
                        try:
                            save_metadata = {
                                'version': recovered_config.get('metadata', {}).get('version', '2.1'),
                                'config': {
                                    'preset_used': recovered_config.get('system', {}).get('preset_applied', {}).get('preset', 'recovered'),
                                    'recovery_source': 'json_recovery'
                                }
                            }
                            create_config_backup(config_path, save_metadata)
                            logger.info("Recovered configuration saved as backup using dedicated backup function")
                        except Exception as e:
                            logger.warning(f"Failed to save recovery backup using dedicated function: {e}")
                    else:
                        logger.debug("JSON recovery failed - file too corrupted")
                        
                except Exception as e:
                    logger.debug(f"Configuration recovery attempt failed: {e}")
            
            # Fallback 4: Check for named configurations that could serve as templates
            if not loaded_config:
                try:
                    logger.debug("Checking for suitable named configurations as templates")
                    saved_configs_info = list_saved_configs()
                    
                    # Look for the most recently modified named config
                    named_configs = saved_configs_info.get("named_configs", {})
                    if named_configs:
                        # Sort by modification time
                        sorted_configs = sorted(
                            named_configs.items(),
                            key=lambda x: x[1].get("modified_time", 0),
                            reverse=True
                        )
                        
                        # Try top 3
                        for config_name, config_info in sorted_configs[:3]:
                            try:
                                # This will handle named config loading
                                named_config = load_config(config_name)
                                if named_config and isinstance(named_config, dict):
                                    loaded_config = named_config
                                    load_method = f"named_config_template_{config_name}"
                                    logger.info(f"Using named configuration '{config_name}' as template")
                                    break
                            except Exception as e:
                                logger.debug(f"Named config '{config_name}' failed: {e}")
                                continue
                                
                except Exception as e:
                    logger.debug(f"Named configuration fallback failed: {e}")
        
        # Step 3: Memory-Optimized Template Generation
        default_config = None
        template_source = None
        
        # Clear memory before template generation for memory-constrained systems
        if total_ram_gb < 16:
            try:
                pre_template_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if pre_template_clear.get('success'):
                    memory_optimization_log['checkpoints'].append({
                        'stage': 'pre_template_generation',
                        'actions': pre_template_clear.get('actions_taken', []),
                        'timestamp': datetime.now().isoformat()
                    })
            except Exception as e:
                logger.debug(f"Pre-template memory optimization failed: {e}")
        
        try:
            default_config = get_current_config()
            template_source = "get_current_config"
            logger.debug("Successfully obtained current configuration template")
            
            # Clear memory after large template generation
            if len(str(default_config)) > 100000:  # Very large template
                try:
                    post_template_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                    if post_template_clear.get('success'):
                        memory_optimization_log['checkpoints'].append({
                            'stage': 'post_large_template',
                            'actions': post_template_clear.get('actions_taken', []),
                            'timestamp': datetime.now().isoformat()
                        })
                except Exception as e:
                    logger.debug(f"Post-template memory optimization failed: {e}")
                    
        except Exception as e:
            logger.warning(f"Failed to get current config template: {e}")
            
            # Fallback to preset-based configuration
            try:
                if PRESET_CONFIGS and 'default' in PRESET_CONFIGS:
                    default_config = deepcopy(PRESET_CONFIGS['default'])
                    template_source = "default_preset"
                    logger.info("Using default preset as configuration template")
                elif PRESET_CONFIGS:
                    # Use first available preset
                    first_preset = next(iter(PRESET_CONFIGS.keys()))
                    default_config = deepcopy(PRESET_CONFIGS[first_preset])
                    template_source = f"preset_{first_preset}"
                    logger.info(f"Using preset '{first_preset}' as configuration template")
                else:
                    # Use DEFAULT_PRESET if available
                    if 'DEFAULT_PRESET' in globals() and DEFAULT_PRESET:
                        default_config = deepcopy(DEFAULT_PRESET)
                        template_source = "DEFAULT_PRESET_global"
                        logger.info("Using DEFAULT_PRESET global as fallback template")
                    else:
                        raise RuntimeError("No configuration template available")
                        
            except Exception as template_error:
                logger.error(f"Failed to get any configuration template: {template_error}")
                
                # Last resort - create minimal fallback
                try:
                    default_config = _create_minimal_fallback_config('standard')
                    template_source = "minimal_fallback"
                    logger.warning("Using minimal fallback configuration as template")
                except Exception as minimal_error:
                    logger.critical(f"Even minimal fallback failed: {minimal_error}")
                    raise RuntimeError("No configuration template available") from e
        
        # Step 4: Memory-Optimized Configuration Processing and Merging
        merged_config = None
        
        if loaded_config:
            logger.info(f"Processing loaded configuration (source: {load_method})")
            
            # Clear memory before intensive merge operations for memory-constrained systems
            if total_ram_gb < 8:
                try:
                    pre_merge_clear = enhanced_clear_memory(
                        aggressive=True,  # Aggressive on low-memory systems
                        hardware_data=hardware_data
                    )
                    
                    if pre_merge_clear.get('success'):
                        memory_optimization_log['checkpoints'].append({
                            'stage': 'pre_merge',
                            'actions': pre_merge_clear.get('actions_taken', []),
                            'timestamp': datetime.now().isoformat()
                        })
                except Exception as e:
                    logger.debug(f"Pre-merge memory optimization failed: {e}")
            
            # Check version compatibility and migration needs
            loaded_version = loaded_config.get('metadata', {}).get('version', '1.0')
            default_version = default_config.get('metadata', {}).get('version', '2.1')
            
            if loaded_version != default_version:
                logger.info(f"Configuration migration needed: {loaded_version} -> {default_version}")
                try:
                    # Clear memory before memory-intensive migration
                    if total_ram_gb < 16:
                        try:
                            enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                        except Exception as e:
                            logger.debug(f"Pre-migration memory cleanup failed: {e}")
                    
                    # Use the enhanced migration function
                    migrated_config = migrate_config(loaded_config, default_config)
                    logger.info("Configuration migration completed successfully")
                    loaded_config = migrated_config
                    
                    # Add migration metadata
                    if 'metadata' not in loaded_config:
                        loaded_config['metadata'] = {}
                    loaded_config['metadata']['migration_applied'] = {
                        'from_version': loaded_version,
                        'to_version': default_version,
                        'migration_time': datetime.now().isoformat(),
                        'migration_method': 'initialize_config'
                    }
                    
                    # Clear memory after migration
                    if total_ram_gb < 16:
                        try:
                            post_migration_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                            if post_migration_clear.get('success'):
                                memory_optimization_log['checkpoints'].append({
                                    'stage': 'post_migration',
                                    'actions': post_migration_clear.get('actions_taken', []),
                                    'timestamp': datetime.now().isoformat()
                                })
                        except Exception as e:
                            logger.debug(f"Post-migration memory optimization failed: {e}")
                    
                except Exception as migration_error:
                    logger.error(f"Configuration migration failed: {migration_error}")
                    
                    # Handle migration failure
                    if prompt_user_for_migration_fallback():
                        logger.warning("Migration failed - using default configuration")
                        loaded_config = default_config
                        load_method = "migration_fallback_to_default"
                    else:
                        # Try to use the loaded config anyway with warnings
                        logger.warning("Migration failed - attempting to use original loaded config with risks")
                        loaded_config['metadata'] = loaded_config.get('metadata', {})
                        loaded_config['metadata']['migration_failed'] = {
                            'error': str(migration_error),
                            'time': datetime.now().isoformat(),
                            'risk_level': 'high'
                        }
            
            # Merge configurations with preference for loaded config
            try:
                merged_config = deep_update(deepcopy(default_config), loaded_config)
                logger.info("Configuration merge completed successfully")
                
                # Add merge metadata
                if 'metadata' not in merged_config:
                    merged_config['metadata'] = {}
                merged_config['metadata']['initialization'] = {
                    'load_method': load_method,
                    'template_source': template_source,
                    'merge_successful': True,
                    'initialization_time': datetime.now().isoformat()
                }
                
                # Clear memory after merge for large configs
                if len(str(merged_config)) > 75000:  # Large merged config
                    try:
                        post_merge_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                        if post_merge_clear.get('success'):
                            memory_optimization_log['checkpoints'].append({
                                'stage': 'post_large_merge',
                                'actions': post_merge_clear.get('actions_taken', []),
                                'timestamp': datetime.now().isoformat()
                            })
                    except Exception as e:
                        logger.debug(f"Post-merge memory optimization failed: {e}")
                
            except Exception as merge_error:
                logger.error(f"Configuration merge failed: {merge_error}")
                
                # Decide which config to use
                if load_method in ["primary_path", "backup_recovery", "json_recovery"]:
                    # Trust the loaded config over the template
                    merged_config = loaded_config
                    logger.warning("Using loaded configuration without merge due to merge failure")
                else:
                    # Use the template as it's more reliable
                    merged_config = default_config
                    logger.warning("Using template configuration due to merge failure")
                
                # Add error metadata
                if 'metadata' not in merged_config:
                    merged_config['metadata'] = {}
                merged_config['metadata']['initialization'] = {
                    'load_method': load_method,
                    'template_source': template_source,
                    'merge_failed': str(merge_error),
                    'initialization_time': datetime.now().isoformat()
                }
        else:
            logger.info("No existing configuration found or loaded, using template")
            merged_config = default_config
            
            # Add initialization metadata
            if 'metadata' not in merged_config:
                merged_config['metadata'] = {}
            merged_config['metadata']['initialization'] = {
                'load_method': 'no_existing_config',
                'template_source': template_source,
                'load_error': load_error,
                'initialization_time': datetime.now().isoformat()
            }
        
        # Step 5: Memory-Optimized Validation
        validation_passed = False
        validation_errors = []
        
        # Clear memory before validation
        try:
            pre_validation_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if pre_validation_clear.get('success'):
                memory_optimization_log['checkpoints'].append({
                    'stage': 'pre_validation',
                    'actions': pre_validation_clear.get('actions_taken', []),
                    'timestamp': datetime.now().isoformat()
                })
        except Exception as e:
            logger.debug(f"Pre-validation memory optimization failed: {e}")
        
        try:
            validate_config(merged_config)
            logger.info("Final configuration passed validation")
            validation_passed = True
        except ValueError as validation_error:
            validation_errors.append(str(validation_error))
            logger.error(f"Final configuration validation failed: {validation_error}")
            
            # Handle validation failure with recovery options
            recovery_successful = False
            
            # Recovery attempt 1: Try auto-repair
            try:
                logger.info("Attempting automatic configuration repair")
                
                # Use centralized validation to identify and fix issues
                is_valid, errors, warnings = ConfigSectionValidators.validate_cross_section_compatibility(merged_config)
                
                if not is_valid:
                    # Apply basic fixes for common issues
                    if 'model' in merged_config and 'training' in merged_config:
                        # Fix batch normalization compatibility
                        if (merged_config['model'].get('use_batch_norm', False) and 
                            merged_config['training'].get('batch_size', 32) < 2):
                            merged_config['training']['batch_size'] = 2
                            logger.info("Auto-fixed: Increased batch_size for batch normalization compatibility")
                    
                    # Re-validate after fixes
                    validate_config(merged_config)
                    recovery_successful = True
                    logger.info("Automatic configuration repair successful")
                    
            except Exception as repair_error:
                logger.debug(f"Automatic repair failed: {repair_error}")
            
            # Recovery attempt 2: Use a known good configuration
            if not recovery_successful:
                recovery_options = []
                
                # Try default preset if available and different from current
                if (PRESET_CONFIGS and 'default' in PRESET_CONFIGS and 
                    template_source != "default_preset"):
                    recovery_options.append(('default_preset', PRESET_CONFIGS['default']))
                
                # Try minimal fallback
                recovery_options.append(('minimal_fallback', _create_minimal_fallback_config('standard')))
                
                for recovery_name, recovery_config in recovery_options:
                    try:
                        validate_config(recovery_config)
                        logger.warning(f"Using {recovery_name} due to validation failure")
                        merged_config = recovery_config
                        recovery_successful = True
                        
                        # Update metadata to reflect recovery
                        if 'metadata' not in merged_config:
                            merged_config['metadata'] = {}
                        merged_config['metadata']['initialization']['validation_recovery'] = {
                            'original_errors': validation_errors,
                            'recovery_method': recovery_name,
                            'recovery_time': datetime.now().isoformat()
                        }
                        break
                        
                    except Exception as recovery_error:
                        logger.debug(f"Recovery option {recovery_name} failed: {recovery_error}")
                        continue
            
            # If all recovery failed, decide based on user input or policy
            if not recovery_successful:
                if handle_validation_failure(validation_error, merged_config, default_config):
                    merged_config = default_config
                    logger.warning("Using default configuration due to validation failure")
                else:
                    # User chose to exit or no recovery possible
                    raise ValueError(f"Configuration validation failed and recovery unsuccessful: {validation_error}")
        
        # Step 6: Memory-Optimized Preset Consistency
        try:
            # Clear memory before preset processing
            pre_preset_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if pre_preset_clear.get('success'):
                memory_optimization_log['checkpoints'].append({
                    'stage': 'pre_preset_consistency',
                    'actions': pre_preset_clear.get('actions_taken', []),
                    'timestamp': datetime.now().isoformat()
                })
            
            merged_config = ensure_preset_consistency(merged_config)
            logger.debug("Preset consistency check completed")
        except Exception as preset_error:
            logger.warning(f"Preset consistency check failed: {preset_error}")
        
        # Step 7: Memory-Optimized Configuration Saving
        save_successful = False
        
        # Clear memory before save operations
        try:
            pre_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if pre_save_clear.get('success'):
                memory_optimization_log['checkpoints'].append({
                    'stage': 'pre_save',
                    'actions': pre_save_clear.get('actions_taken', []),
                    'timestamp': datetime.now().isoformat()
                })
        except Exception as e:
            logger.debug(f"Pre-save memory optimization failed: {e}")
        
        try:
            # Create backup if original file exists and is valid using the dedicated backup function
            if config_path.exists() and loaded_config and load_method == "primary_path":
                try:
                    # Prepare metadata for the backup function
                    save_metadata = {
                        'version': loaded_config.get('metadata', {}).get('version', '2.1'),
                        'config': {
                            'preset_used': loaded_config.get('system', {}).get('preset_applied', {}).get('preset', 'unknown'),
                            'backup_reason': 'initialization_backup',
                            'load_method': load_method
                        }
                    }
                    
                    # Use the dedicated backup function
                    backup_created = create_config_backup(config_path, save_metadata)
                    if backup_created:
                        logger.info("Configuration backup created using dedicated backup function")
                    else:
                        logger.warning("Failed to create backup using dedicated function, falling back to simple backup")
                        # Fallback to simple backup
                        backup_path = config_path.with_suffix(f".backup_init_{int(time.time())}.json")
                        shutil.copy2(config_path, backup_path)
                        logger.info(f"Created fallback backup: {backup_path}")
                        
                except Exception as backup_error:
                    logger.warning(f"Failed to create backup using dedicated function: {backup_error}")
                    # Fallback to simple backup
                    try:
                        backup_path = config_path.with_suffix(f".backup_init_{int(time.time())}.json")
                        shutil.copy2(config_path, backup_path)
                        logger.info(f"Created fallback backup after error: {backup_path}")
                    except Exception as fallback_error:
                        logger.warning(f"Even fallback backup failed: {fallback_error}")
            
            save_config(merged_config, config_path)
            save_successful = True
            logger.info("Initialized configuration saved successfully")
            
        except Exception as save_error:
            logger.error(f"Failed to save initialized configuration: {save_error}")
            
            # Try to save to alternative location
            try:
                fallback_save_path = config_path.with_suffix(f".fallback_{int(time.time())}.json")
                save_config(merged_config, fallback_save_path)
                logger.warning(f"Configuration saved to fallback location: {fallback_save_path}")
                save_successful = True
            except Exception as fallback_save_error:
                logger.error(f"Failed to save to fallback location: {fallback_save_error}")
                # Continue with in-memory config even if save fails
        
        # Aggressive cleanup after initialization completion
        try:
            final_clear_results = enhanced_clear_memory(
                aggressive=True,  # Aggressive final cleanup
                hardware_data=hardware_data
            )
            
            if final_clear_results.get('success'):
                memory_optimization_log['final_cleanup'] = {
                    'actions': final_clear_results.get('actions_taken', []),
                    'timestamp': datetime.now().isoformat(),
                    'memory_impact': {
                        'optimization_effective': True,
                        'final_cleanup_performed': True
                    }
                }
                
                logger.debug(f"Final memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
                
        except Exception as e:
            logger.debug(f"Final memory optimization failed: {e}")
            memory_optimization_log['final_cleanup_error'] = str(e)
        
        # Add memory optimization metadata
        initialization_time = time.time() - initialization_start_time
        total_memory_actions = (
            len(memory_optimization_log.get('initial_cleanup', {}).get('actions', [])) +
            sum(len(cp.get('actions', [])) for cp in memory_optimization_log.get('checkpoints', [])) +
            len(memory_optimization_log.get('final_cleanup', {}).get('actions', []))
        )
        
        if 'metadata' not in merged_config:
            merged_config['metadata'] = {}
        
        initialization_summary = {
            'config_path': str(config_path),
            'load_method': load_method,
            'load_error': load_error,
            'template_source': template_source,
            'validation_passed': validation_passed,
            'validation_errors': validation_errors if not validation_passed else [],
            'save_successful': save_successful,
            'initialization_completed': datetime.now().isoformat(),
            'initialization_duration': initialization_time,
            'memory_optimization': memory_optimization_log,
            'total_memory_actions': total_memory_actions,
            'fallback_strategies_used': []
        }
        
        # Track which fallback strategies were used
        if load_error:
            initialization_summary['fallback_strategies_used'].append('load_fallback')
        if not validation_passed:
            initialization_summary['fallback_strategies_used'].append('validation_recovery')
        if not save_successful:
            initialization_summary['fallback_strategies_used'].append('save_fallback')
        
        merged_config['metadata']['initialization'] = initialization_summary
        
        # Logging with memory optimization summary
        preset_used = merged_config.get('presets', {}).get('current_preset', 'none')
        model_type = merged_config.get('model', {}).get('model_type', 'unknown')
        total_sections = len(merged_config)
        
        logger.info("Configuration initialization completed successfully:")
        logger.info(f"  - Load method: {load_method or 'template_only'}")
        logger.info(f"  - Template source: {template_source}")
        logger.info(f"  - Preset: {preset_used}")
        logger.info(f"  - Model type: {model_type}")
        logger.info(f"  - Total sections: {total_sections}")
        logger.info(f"  - Initialization time: {initialization_time:.3f}s")
        logger.info(f"  - Memory optimization: {total_memory_actions} actions across {len(memory_optimization_log.get('checkpoints', [])) + 2} stages")
        logger.info(f"  - Validation: {'PASSED' if validation_passed else 'RECOVERED'}")
        logger.info(f"  - Save: {'SUCCESSFUL' if save_successful else 'FALLBACK'}")
        
        if initialization_summary['fallback_strategies_used']:
            logger.info(f"  - Fallback strategies used: {', '.join(initialization_summary['fallback_strategies_used'])}")
        
        return merged_config
        
    except Exception as e:
        logger.error(f"Configuration initialization failed: {str(e)}", exc_info=True)
        
        # Final emergency cleanup
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        # Emergency fallback configuration
        try:
            logger.critical("Attempting emergency fallback configuration")
            fallback_config = _create_minimal_fallback_config('emergency')
            
            # Add emergency metadata with memory optimization context
            fallback_config['metadata'] = fallback_config.get('metadata', {})
            fallback_config['metadata']['emergency_initialization'] = {
                'original_error': str(e),
                'emergency_fallback_used': True,
                'initialization_time': datetime.now().isoformat(),
                'risk_level': 'critical',
                'memory_optimization_attempted': 'memory_optimization_log' in locals()
            }
            
            logger.critical("Emergency fallback configuration activated - system may have limited functionality")
            return fallback_config
            
        except Exception as fallback_error:
            logger.critical(f"Even emergency fallback configuration failed: {fallback_error}")
            raise RuntimeError(f"Complete configuration initialization failure: {str(e)}. Emergency fallback also failed: {str(fallback_error)}") from e



# Helper functions for the updated configuration system
def save_change_log(changes: Dict[str, Any]) -> None:
    """
    Save configuration change log for audit trail using consolidated daily files.
    
    This function has been updated to follow the same pattern as save_initialization_report(),
    creating consolidated daily log files with entries appended as array elements for better
    organization, easier analysis, and improved audit trail capabilities.
    
    Args:
        changes: Dictionary containing configuration change information with metadata
        
    The changes dictionary should contain:
        - section: Configuration section affected
        - parameter: Parameter name that changed
        - old_value: Previous value
        - new_value: New value
        - source: Source of the change (e.g., 'user_input', 'auto_optimization')
        - timestamp: When the change occurred (added automatically if not present)
        - additional metadata as needed
    """
    try:
        # Ensure we have a timestamp
        #from datetime import datetime
        if 'timestamp' not in changes:
            changes['timestamp'] = datetime.now().isoformat()
        
        # Get or create log directory
        log_dir = Path(globals().get('LOG_DIR', './logs'))
        log_dir.mkdir(exist_ok=True, parents=True)
        change_log_dir = log_dir / "deep_learning_config_changes"
        change_log_dir.mkdir(exist_ok=True, parents=True)
        
        # Create timestamp for file naming
        timestamp_obj = datetime.now()
        date_str = timestamp_obj.strftime('%Y%m%d')
        time_str = timestamp_obj.strftime('%H%M%S')
        sequence_id = f"{date_str}_{time_str}"
        
        # Define consolidated log file path (daily file)
        consolidated_log_file = change_log_dir / f"config_changes_{date_str}.json"
        
        # Create entry metadata
        entry_metadata = {
            'sequence_id': sequence_id,
            'timestamp': timestamp_obj.isoformat(),
            'time_str': time_str,
            'entry_type': 'configuration_change'
        }
        
        # Create the change entry
        change_entry = {
            **entry_metadata,
            'data': changes
        }
        
        # Load existing log data or create new structure
        if consolidated_log_file.exists():
            try:
                with open(consolidated_log_file, 'r', encoding='utf-8') as f:
                    log_data = json.load(f)
                
                # Validate structure
                if not isinstance(log_data, dict) or 'changes' not in log_data:
                    logger.warning(f"Invalid log structure in {consolidated_log_file}, recreating")
                    log_data = {
                        'date': date_str,
                        'changes': [],
                        'metadata': {
                            'version': '2.0',
                            'created_at': timestamp_obj.isoformat(),
                            'total_changes': 0,
                            'last_updated': timestamp_obj.isoformat(),
                            'log_type': 'consolidated_config_changes'
                        }
                    }
                
                logger.debug(f"Loaded existing change log from {consolidated_log_file}")
                
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Failed to load existing change log, creating new: {e}")
                log_data = {
                    'date': date_str,
                    'changes': [],
                    'metadata': {
                        'version': '2.0',
                        'created_at': timestamp_obj.isoformat(),
                        'total_changes': 0,
                        'last_updated': timestamp_obj.isoformat(),
                        'log_type': 'consolidated_config_changes'
                    }
                }
        else:
            # Create new log structure
            log_data = {
                'date': date_str,
                'changes': [change_entry],
                'metadata': {
                    'version': '2.0',
                    'created_at': timestamp_obj.isoformat(),
                    'total_changes': 1,
                    'last_updated': timestamp_obj.isoformat(),
                    'log_type': 'consolidated_config_changes'
                }
            }
        
        # Append new change entry if we loaded existing data
        if consolidated_log_file.exists():
            log_data['changes'].append(change_entry)
            log_data['metadata']['total_changes'] = len(log_data['changes'])
            log_data['metadata']['last_updated'] = timestamp_obj.isoformat()
        
        # Keep only last 500 changes per day to prevent files from growing too large
        max_changes_per_day = 500
        if len(log_data['changes']) > max_changes_per_day:
            logger.debug(f"Trimming change log to last {max_changes_per_day} entries")
            log_data['changes'] = log_data['changes'][-max_changes_per_day:]
            log_data['metadata']['total_changes'] = len(log_data['changes'])
            log_data['metadata']['trimmed'] = True
            log_data['metadata']['trim_threshold'] = max_changes_per_day
        
        # Generate analytics if this is a significant number of changes
        if log_data['metadata']['total_changes'] % 50 == 0:
            try:
                analytics = {
                    'date': date_str,
                    'total_changes': log_data['metadata']['total_changes'],
                    'generated_at': datetime.now().isoformat(),
                    'by_section': {},
                    'by_parameter': {},
                    'by_source': {},
                    'frequent_changes': [],
                    'recent_changes': []
                }
                
                # Analyze changes
                for entry in log_data['changes']:
                    change_data = entry.get('data', {})
                    
                    # Extract source from metadata
                    source = change_data.get('metadata', {}).get('source', 'unknown')
                    analytics['by_source'][source] = analytics['by_source'].get(source, 0) + 1
                    
                    # Count changes by section and parameter from the nested structure
                    for section_name, section_data in change_data.items():
                        if section_name == 'metadata' or section_name == 'timestamp':
                            continue
                            
                        if section_name not in analytics['by_section']:
                            analytics['by_section'][section_name] = 0
                        analytics['by_section'][section_name] += 1
                        
                        # Count individual parameter changes within the section
                        for param_name, param_data in section_data.items():
                            if isinstance(param_data, dict) and 'from' in param_data and 'to' in param_data:
                                full_param_name = f"{section_name}.{param_name}"
                                analytics['by_parameter'][full_param_name] = analytics['by_parameter'].get(full_param_name, 0) + 1
                
                # Get most frequent changes
                sorted_params = sorted(analytics['by_parameter'].items(), key=lambda x: x[1], reverse=True)
                analytics['frequent_changes'] = [
                    {'parameter': param, 'count': count}
                    for param, count in sorted_params[:10]
                ]
                
                # Get recent changes (last 10)
                analytics['recent_changes'] = []
                for entry in log_data['changes'][-10:]:
                    change_data = entry.get('data', {})
                    source = change_data.get('metadata', {}).get('source', 'unknown')
                    
                    # Extract first section and parameter for recent changes display
                    first_section = None
                    first_param = None
                    for section_name, section_data in change_data.items():
                        if section_name == 'metadata' or section_name == 'timestamp':
                            continue
                        if isinstance(section_data, dict):
                            for param_name in section_data.keys():
                                first_section = section_name
                                first_param = param_name
                                break
                        if first_section:
                            break
                    
                    analytics['recent_changes'].append({
                        'timestamp': entry.get('timestamp'),
                        'section': first_section or 'unknown',
                        'parameter': first_param or 'unknown',
                        'source': source
                    })
                
                # Save analytics
                analytics_file = change_log_dir / f"config_changes_analytics_{date_str}.json"
                with open(analytics_file, 'w', encoding='utf-8') as f:
                    json.dump(analytics, f, indent=2, ensure_ascii=False)
                
                logger.debug(f"Generated change analytics: {analytics_file}")
                
            except Exception as analytics_error:
                logger.debug(f"Failed to generate change analytics: {analytics_error}")
        
        # Save updated log with proper error handling
        try:
            with open(consolidated_log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False, default=str)
            logger.debug(f"Saved change log to {consolidated_log_file}")
            logger.debug(f"Total changes in log: {log_data['metadata']['total_changes']}")
            
        except Exception as write_error:
            logger.error(f"Failed to write change log: {write_error}")
            
            # Try to save to backup location
            try:
                backup_file = change_log_dir / f"config_changes_{date_str}_backup.json"
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(log_data, f, indent=2, ensure_ascii=False, default=str)
                logger.warning(f"Saved change log to backup location: {backup_file}")
            except Exception as backup_error:
                logger.error(f"Failed to save change log backup: {backup_error}")
        
        # Also append to a human-readable summary file for easier review
        try:
            summary_file = change_log_dir / f"config_changes_summary_{date_str}.txt"
            
            # Extract meaningful change information from the nested structure
            source = changes.get('metadata', {}).get('source', 'unknown')
            preset_info = changes.get('system', {}).get('preset_applied', {})
            preset_name = preset_info.get('preset', 'unknown') if isinstance(preset_info, dict) else 'unknown'
            
            # Count total parameter changes
            total_param_changes = 0
            changed_sections = []
            
            for section_name, section_data in changes.items():
                if section_name in ['metadata', 'timestamp']:
                    continue
                    
                if isinstance(section_data, dict):
                    section_changes = 0
                    for param_name, param_data in section_data.items():
                        if isinstance(param_data, dict) and 'from' in param_data and 'to' in param_data:
                            section_changes += 1
                            total_param_changes += 1
                    
                    if section_changes > 0:
                        changed_sections.append(f"{section_name}({section_changes})")
            
            # Get sample changes for display (first 3 parameters)
            sample_changes = []
            for section_name, section_data in changes.items():
                if section_name in ['metadata', 'timestamp']:
                    continue
                    
                if isinstance(section_data, dict):
                    for param_name, param_data in section_data.items():
                        if isinstance(param_data, dict) and 'from' in param_data and 'to' in param_data:
                            sample_changes.append(f"{section_name}.{param_name}: {param_data['from']} → {param_data['to']}")
                            if len(sample_changes) >= 3:
                                break
                if len(sample_changes) >= 3:
                    break
            
            # Format change summary with extracted information
            change_summary = (
                f"\n{'=' * 80}\n"
                f"Configuration Change - {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"Sequence: {sequence_id} (Total today: {log_data['metadata']['total_changes']})\n"
                f"{'=' * 80}\n"
                f"Source: {source}\n"
                f"Preset: {preset_name}\n"
                f"Total Changes: {total_param_changes} parameters across {len(changed_sections)} sections\n"
                f"Sections: {', '.join(changed_sections) if changed_sections else 'none'}\n"
            )
            
            # Add sample changes
            if sample_changes:
                change_summary += f"Sample Changes:\n"
                for change in sample_changes:
                    change_summary += f"  - {change}\n"
            
            # Add metadata information if present
            metadata = changes.get('metadata', {})
            if 'config_version' in metadata:
                change_summary += f"Config Version: {metadata['config_version']}\n"
            if 'memory_optimizations_applied' in metadata:
                change_summary += f"Memory Optimizations: {metadata['memory_optimizations_applied']}\n"
            
            change_summary += f"{'=' * 80}\n"
            
            # Append to summary file
            with open(summary_file, 'a', encoding='utf-8') as f:
                f.write(change_summary)
            
            logger.debug(f"Appended change summary to {summary_file}")
            
        except Exception as summary_error:
            logger.debug(f"Failed to update summary file: {summary_error}")
        
        # Update latest change log symlink/pointer
        try:
            latest_log_pointer = change_log_dir / "latest_config_changes.json"
            if latest_log_pointer.exists() or latest_log_pointer.is_symlink():
                latest_log_pointer.unlink()
            
            try:
                # Try to create symlink
                latest_log_pointer.symlink_to(consolidated_log_file.name)
            except (OSError, NotImplementedError):
                # Fallback to text file with path
                with open(change_log_dir / "latest_config_changes.txt", 'w') as f:
                    f.write(f"Latest log: {consolidated_log_file.name}\n")
                    f.write(f"Updated: {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')}\n")
            
        except Exception as symlink_error:
            logger.debug(f"Failed to update latest log pointer: {symlink_error}")
        
    except Exception as e:
        logger.error(f"Failed to save change log: {e}", exc_info=True)
        
        # Emergency fallback - try to save to a simple timestamped file
        try:
            emergency_file = Path(globals().get('LOG_DIR', './logs')) / f"config_change_emergency_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            emergency_file.parent.mkdir(exist_ok=True, parents=True)
            
            with open(emergency_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': datetime.now().isoformat(),
                    'change': changes,
                    'error': str(e),
                    'emergency_save': True
                }, f, indent=2, default=str)
            
            logger.warning(f"Emergency change log saved to: {emergency_file}")
            
        except Exception as emergency_error:
            logger.critical(f"Failed to save emergency change log: {emergency_error}")

def validate_global_config_state() -> None:
    """Validate the current global configuration state for consistency."""
    try:
        # Check critical variables exist and have valid values
        required_globals = {
            'DEFAULT_BATCH_SIZE': (int, lambda x: x > 0),
            'DEFAULT_EPOCHS': (int, lambda x: x > 0),
            'LEARNING_RATE': ((int, float), lambda x: x > 0),
            'FEATURES': (int, lambda x: x > 0),
            'NORMAL_SAMPLES': (int, lambda x: x > 0)
        }
        
        for var_name, (expected_type, validator) in required_globals.items():
            if var_name not in globals():
                raise ValueError(f"Required global variable {var_name} is missing")
            
            value = globals()[var_name]
            if not isinstance(value, expected_type):
                raise TypeError(f"{var_name} must be of type {expected_type}, got {type(value)}")
            
            if not validator(value):
                raise ValueError(f"{var_name} has invalid value: {value}")
        
        # Check list length compatibility
        hidden_dims = globals().get('HIDDEN_LAYER_SIZES', [])
        dropout_rates = globals().get('DROPOUT_RATES', [])
        
        if len(hidden_dims) != len(dropout_rates):
            raise ValueError(f"HIDDEN_LAYER_SIZES and DROPOUT_RATES must have same length: "
                           f"{len(hidden_dims)} != {len(dropout_rates)}")
        
    except Exception as e:
        logger.error(f"Global configuration state validation failed: {e}")
        raise

# System initialization validation and setup
def initialize_system() -> Tuple[Dict[str, Any], Dict[str, Any], logging.Logger]:
    """
    Initialize the complete system with comprehensive setup and validation.
    
    This function performs a complete system initialization by leveraging the
    existing System Check Framework and loading_screen functionality.
    
    Returns:
        Tuple containing:
            - Dict: Comprehensive system status and configuration
            - Dict: Active configuration dictionary
            - Logger: Configured logger instance
    
    Raises:
        RuntimeError: If critical system components fail to initialize
        SystemExit: If user chooses to quit during initialization
    """
    initialization_start = time.time()
    console = Console()
    
    # Console capability detection
    is_tty = sys.stdout.isatty()
    supports_color = is_tty and hasattr(sys.stdout, 'isatty')
    console_width = max(60, getattr(console, 'width', 80))
    
    # Create initialization status table with enhanced styling
    init_table = Table(
        title=f"\n[bold]DEEP LEARNING SYSTEM INITIALIZATION[/bold]",
        box=box.DOUBLE_EDGE,
        header_style="bold yellow",
        border_style="bold cyan",
        title_style="bold green",
        title_justify="left",
        show_lines=True,
        expand=True,
        width=min(140, console.width - 4)
    )
    
    init_table.add_column("Step", style="bold cyan", width=28, no_wrap=True)
    init_table.add_column("Status", width=8, justify="left")
    init_table.add_column("Duration", width=8, justify="left", style="bold magenta")
    init_table.add_column("Details", style="bold", min_width=60)
    init_table.add_column("Health", width=10, justify="left", style="bold")
    
    initialization_steps = []
    
    def add_step(step_name: str, status: str, duration: float, details: str, health_score: Optional[float] = None):
        """Add a step to the initialization tracking with enhanced metadata."""
        status_styles = {
            "SUCCESS": "bold green",
            "FAILED": "bold red", 
            "WARNING": "bold yellow",
            "SKIPPED": "bold blue",
            "PARTIAL": "bold orange1"
        }
        status_style = status_styles.get(status, "bold white")
        
        # Format health score
        health_display = ""
        if health_score is not None:
            if health_score >= 90:
                health_display = f"[bold green]{health_score:.0f}%[/bold green]"
            elif health_score >= 70:
                health_display = f"[bold yellow]{health_score:.0f}%[/bold yellow]"
            else:
                health_display = f"[bold red]{health_score:.0f}%[/bold red]"
        
        initialization_steps.append((step_name, status, duration, details, status_style, health_display))
    
    # Initialize system state tracking
    system_state = {
        'initialization_start': datetime.now(),
        'steps_completed': [],
        'errors_encountered': [],
        'warnings_issued': [],
        'health_metrics': {}
    }
    
    try:
        # Step 1: Early Setup - Basic configuration with hardware detection
        step_start = time.time()

        # Use the existing setup_directories function for consistency
        try:
            log_dir = Path(__file__).resolve().parent / "logs"
            logger = setup_logging(log_dir)
            
            # Use the existing directory setup function
            essential_dirs = setup_directories(logger)
            directories_created = list(essential_dirs.keys())
            
            # Validate write access for all directories
            write_access_issues = []
            for dir_name, dir_path in essential_dirs.items():
                try:
                    # Verify write access
                    test_file = dir_path / ".write_test"
                    test_file.touch()
                    test_file.unlink()
                except Exception as e:
                    write_access_issues.append(f"Write access failed for {dir_name}: {e}")
                    system_state['errors_encountered'].append(f"Write access failed for {dir_name}: {e}")
            
            # Remove directories that failed write access from directories_created
            if write_access_issues:
                for issue in write_access_issues:
                    # Extract directory name from error message more safely
                    try:
                        dir_name = issue.split(' ')[4].rstrip(':')
                        if dir_name in directories_created:
                            directories_created.remove(dir_name)
                    except (IndexError, ValueError):
                        # If parsing fails, log but continue
                        logger.debug(f"Could not parse directory name from error: {issue}")

        except Exception as e:
            # Fallback to minimal directory creation if setup_directories fails
            system_state['errors_encountered'].append(f"setup_directories failed: {e}")
            
            # Create minimal essential directories manually
            base_dir = Path(__file__).resolve().parent
            essential_dirs = {
                'artifacts': base_dir / "artifacts",
                'datasets': base_dir / "datasets",
                'figures': base_dir / "figures",
                'info': base_dir / "info",
                'metrics': base_dir / "metrics",
                'logs': base_dir / "logs",
                'models': base_dir / "models",
                'data': base_dir / "data",
                'config': base_dir / "config",
                'reports': base_dir / "reports",
                'tensorboard': base_dir / "tensorboard",
                'cache': base_dir / "cache",
                'exports': base_dir / "exports",
                'checkpoints': base_dir / "checkpoints" / f"checkpoints_v{VERSION_INFO['torch']}"
            }
            
            directories_created = []
            for dir_name, dir_path in essential_dirs.items():
                try:
                    dir_path.mkdir(parents=True, exist_ok=True)
                    # Verify write access
                    test_file = dir_path / ".write_test"
                    test_file.touch()
                    test_file.unlink()
                    directories_created.append(dir_name)
                except Exception as e:
                    system_state['errors_encountered'].append(f"Directory creation failed for {dir_name}: {e}")
        
        # Early hardware detection for adaptive configuration
        try:
            initial_hardware = check_hardware(min_disk_gb=1.0, include_memory_usage=True)
            cuda_available = initial_hardware.get('cuda', {}).get('available', False)
            total_ram_gb = initial_hardware.get('system_ram', {}).get('ram_total_gb', 4.0)
            logical_cores = initial_hardware.get('cpu_cores', {}).get('logical_cores', 1)
            system_state['hardware_detected'] = initial_hardware
        except Exception as e:
            cuda_available = torch.cuda.is_available()
            total_ram_gb = psutil.virtual_memory().total / (1024**3)
            logical_cores = os.cpu_count() or 1
            system_state['errors_encountered'].append(f"Hardware detection failed: {e}")
        
        # Hardware-aware system configuration
        config_result = configure_system()
        
        # Seed configuration with hardware awareness
        seed_config = set_seed(42, system_state.get('hardware_detected'))

        # Configure global directories to ensure consistency across the system
        try:
            configure_directories(logger)
        except Exception as e:
            logger.warning(f"configure_directories failed: {e}")
            system_state['warnings_issued'].append(f"Global directory configuration failed: {e}")

        step_duration = time.time() - step_start
        
        # Calculate early setup health score
        early_health = 100.0
        if system_state['errors_encountered']:
            early_health -= len(system_state['errors_encountered']) * 15
        if len(directories_created) < len(essential_dirs):
            early_health -= 20
        early_health = max(0, early_health)
        
        system_state['health_metrics']['early_setup'] = early_health
        
        add_step(
            "Early Setup", 
            "SUCCESS" if early_health >= 80 else "WARNING", 
            step_duration,
            f"Hardware-aware configuration applied\n"
            f"Directories: {', '.join(directories_created)}\n" 
            f"Seed configured: {seed_config.get('base_seed', 42)}\n"
            f"Hardware context: {'CUDA' if cuda_available else 'CPU'} ({total_ram_gb:.1f}GB RAM)",
            early_health
        )
        
        # Logging initialization 
        logger.info("=" * 80)
        logger.info("DEEP LEARNING SYSTEM INITIALIZATION STARTING")
        logger.info("=" * 80)
        logger.info(f"Hardware detected: {logical_cores} CPU cores, {total_ram_gb:.1f}GB RAM, CUDA: {cuda_available}")
        logger.info(f"Essential directories: {len(directories_created)}/{len(essential_dirs)} created successfully")
        
        # Step 2: System Validation
        step_start = time.time()
        
        # Set global exception handler
        sys.excepthook = enhanced_global_exception_handler
        
        # Initialize global function availability
        global enhanced_clear_memory, enhanced_monitor_performance, establish_performance_baseline
        
        # Run system checks with diagnostics
        try:
            system_ready, system_check_results = loading_screen(
                logger=logger,
                extended=True,
                include_performance=True,
                hardware_data=system_state.get('hardware_detected')
            )
            
            if not system_ready:
                raise RuntimeError("System checks failed or user cancelled initialization")
            
            # Extract results from the loading_screen checks
            try:
                # Store check results in system state
                system_state['system_check_results'] = system_check_results or {}
                
                # Build summary from existing results
                additional_checks = []
                
                if system_check_results:
                    if 'hardware' in system_check_results and system_check_results['hardware'].passed:
                        additional_checks.append("Hardware validation: PASSED")
                    else:
                        additional_checks.append("Hardware validation: ISSUES")
                    
                    if 'logging_setup' in system_check_results:
                        result = system_check_results['logging_setup']
                        compliance_score = result.details.get('compliance_score', 0) if isinstance(result.details, dict) else 0
                        additional_checks.append(f"Logging setup: {'PASSED' if result.passed else 'ISSUES'} ({compliance_score}% compliance)")
                    
                    if 'seed_config' in system_check_results:
                        result = system_check_results['seed_config']
                        compliance_score = result.metadata.get('compliance_score', 0) if result.metadata else 0
                        additional_checks.append(f"Reproducibility: {'PASSED' if result.passed else 'ISSUES'} ({compliance_score}% compliance)")
                    
                    if 'performance_monitoring' in system_check_results:
                        result = system_check_results['performance_monitoring']
                        additional_checks.append(f"Performance monitoring: {'AVAILABLE' if result.passed else 'LIMITED'}")
                    
                    if 'memory_management' in system_check_results:
                        result = system_check_results['memory_management']
                        comprehensive = False
                        if result.passed and isinstance(result.details, dict):
                            comprehensive = result.details.get('capabilities', {}).get('comprehensive_management', False)
                        additional_checks.append(f"Memory management: {'COMPREHENSIVE' if comprehensive else 'BASIC' if result.passed else 'LIMITED'}")
                else:
                    additional_checks.append("System check results not available")
                
            except Exception as e:
                additional_checks = [f"Error extracting system check results: {e}"]
                system_state['errors_encountered'].append(f"System check result extraction failed: {e}")
            
            step_duration = time.time() - step_start
            
            # Calculate system validation health score based on results
            validation_health = 100.0
            if system_check_results:
                passed_checks = sum(1 for result in system_check_results.values() if result and result.passed)
                total_checks = len(system_check_results)
                if total_checks > 0:
                    validation_health = (passed_checks / total_checks) * 100
            
            system_state['health_metrics']['system_validation'] = validation_health
            
            add_step(
                "System Validation",
                "SUCCESS" if validation_health >= 80 else "PARTIAL" if validation_health >= 60 else "WARNING",
                step_duration,
                f"Extended system checks completed\n" + "\n".join(additional_checks),
                validation_health
            )
            
        except Exception as e:
            step_duration = time.time() - step_start
            system_state['errors_encountered'].append(f"System validation failed: {e}")
            system_state['health_metrics']['system_validation'] = 0
            
            add_step(
                "System Validation",
                "FAILED",
                step_duration,
                f"System validation failed: {str(e)}\nCritical components may be unavailable",
                0
            )
            raise RuntimeError(f"System validation failed: {e}")
        
        # Step 3: Configuration System with Validation
        step_start = time.time()
        config_details = []
        config_changes = []
        config_health = 100.0
        
        try:
            # Load and validate configuration with error handling
            try:
                config = initialize_config()
                config_details.append(f"Configuration loaded from: {CONFIG_FILE.name}")
                
                # Configuration validation
                try:
                    validate_config(config)
                    config_details.append("Configuration validation: PASSED")
                except ValueError as validation_error:
                    config_health -= 30
                    config_details.append(f"Configuration validation: FAILED - {str(validation_error)}")
                    system_state['warnings_issued'].append(f"Configuration validation failed: {validation_error}")
                    
                    # Interactive error handling
                    if sys.stdin.isatty():
                        console.print(Panel.fit(
                            f"Configuration Validation Issue\n"
                            f"Issue detected: {str(validation_error)}\n\n"
                            f"Available options:\n"
                            f"1. Use default configuration (recommended)\n"
                            f"2. Continue with current configuration (may cause issues)\n"
                            f"3. Exit initialization\n",
                            border_style="yellow",
                            style="bold yellow",
                            title="Configuration Problem",
                            padding=(1, 2)
                        ))
                        
                        choice = Prompt.ask(
                            "Choose an option",
                            choices=["1", "2", "3"],
                            default="1"
                        )
                        
                        if choice == "1":
                            config = get_default_config()
                            save_config(config)
                            config_details.append("Applied default configuration")
                            config_changes.append({
                                'section': 'SYSTEM',
                                'parameter': 'configuration_source',
                                'old_value': 'user_config',
                                'new_value': 'default_config',
                                'source': 'validation_fallback'
                            })
                            # Partial recovery
                            config_health = 85
                        elif choice == "2":
                            config_details.append("Continuing with invalid configuration (user choice)")
                            # Significant health reduction
                            config_health = 50
                        else:
                            # choice == "3"
                            logger.info("User chose to exit due to configuration issues")
                            raise SystemExit("Configuration validation failed and user chose to exit")
                    else:
                        # Non-interactive mode - use defaults
                        config = get_default_config()
                        save_config(config)
                        config_details.append("Applied default configuration (non-interactive mode)")
                        # Partial recovery
                        config_health = 85
                
            except Exception as config_error:
                # Ensure logging level is restored
                config_health -= 50
                config = get_default_config()
                config_details.append(f"Emergency fallback to default config: {str(config_error)}")
                system_state['errors_encountered'].append(f"Configuration loading failed: {config_error}")
            
            # Apply configuration with change tracking
            previous_config = get_current_config() if hasattr(sys.modules[__name__], 'CURRENT_CONFIG') else {}
            update_global_config(config)
            
            # Configuration metadata
            preset_name = config.get('_preset_name', 'custom')
            config_details.append(f"Active preset: {preset_name}")
            config_details.append(f"Configuration parameters: {len(config)} total")
            
            # Detect and track configuration changes
            if hasattr(config, '_changes_applied'):
                config_changes.extend(config._changes_applied)
            
            # Hardware-specific configuration adjustments
            if system_state.get('hardware_detected'):
                hw = system_state['hardware_detected']
                if hw.get('cuda', {}).get('available'):
                    gpu_count = hw.get('cuda', {}).get('gpu_count', 0)
                    config_details.append(f"GPU acceleration: {gpu_count} device(s) detected")
                    if gpu_count > 1:
                        config_details.append("Multi-GPU configuration applied")
                else:
                    config_details.append("CPU-only configuration applied")
            
            # Display configuration changes if any were made
            if config_changes and len(config_changes) > 0:
                try:
                    display_configuration_changes(config_changes, console, logger)
                except Exception as display_error:
                    logger.debug(f"Could not display configuration changes: {display_error}")
            
            step_duration = time.time() - step_start
            system_state['health_metrics']['configuration'] = config_health
            system_state['active_config'] = config
            system_state['config_changes'] = config_changes
            
            add_step(
                "Configuration System",
                "SUCCESS" if config_health >= 80 else "PARTIAL" if config_health >= 60 else "WARNING",
                step_duration,
                "\n".join(config_details),
                config_health
            )
            
        except Exception as e:
            step_duration = time.time() - step_start
            config = get_default_config()
            update_global_config(config)
            system_state['errors_encountered'].append(f"Configuration system failed: {e}")
            system_state['health_metrics']['configuration'] = 25
            system_state['active_config'] = config
            
            add_step(
                "Configuration System",
                "FAILED",
                step_duration,
                f"Configuration system failed: {str(e)}\nUsing emergency default configuration",
                25
            )
        
        # Step 4: Model Variants Initialization
        step_start = time.time()
        model_details = []
        model_health = 100.0
        
        try:
            # Initialize model variants
            initialize_model_variants(silent=True)
            
            if not MODEL_VARIANTS:
                raise RuntimeError("No model variants could be initialized")
            
            # Validation of model variants
            #model_status = validate_model_variants(logger, silent=True)
            variant_status = validate_model_variants(logger, silent=True)
            available_variants = [name for name, status in variant_status.items() if status == 'available']
            failed_variants = [name for name, status in variant_status.items() if status != 'available']
            
            if not available_variants:
                raise RuntimeError("No working model variants available")
            
            # Calculate model health based on availability
            total_variants = len(MODEL_VARIANTS)
            available_count = len(available_variants)
            model_health = (available_count / total_variants) * 100 if total_variants > 0 else 0
            
            model_details.append(f"Initialized: {available_count}/{total_variants} variants")
            model_details.append(f"Available variants: {', '.join(available_variants)}")
            
            if failed_variants:
                model_details.append(f"Failed variants: {', '.join(failed_variants)}")
                system_state['warnings_issued'].extend([f"Model variant failed: {name}" for name in failed_variants])
            
            # Model validation with performance testing
            if available_variants:
                try:
                    # Test a representative model variant
                    test_variant = available_variants[0]
                    test_model = MODEL_VARIANTS[test_variant]
                    
                    # Quick validation test
                    # Dummy input for model validation
                    test_input = torch.randn(1, 10)
                    if cuda_available and hasattr(test_model, 'cuda'):
                        try:
                            test_model.cuda()
                            test_input = test_input.cuda()
                        except Exception:
                            # Fall back to CPU
                            pass
                    
                    # Quick forward pass test
                    with torch.no_grad():
                        _ = test_model(test_input)
                    
                    model_details.append(f"Model validation: PASSED (tested {test_variant})")
                    
                except Exception as test_error:
                    model_health -= 20
                    model_details.append(f"Model validation: PARTIAL - {str(test_error)}")
                    system_state['warnings_issued'].append(f"Model validation test failed: {test_error}")
            
            step_duration = time.time() - step_start
            system_state['health_metrics']['models'] = model_health
            system_state['model_variants'] = {
                'total': total_variants,
                'available': available_count,
                'available_names': available_variants,
                'failed_names': failed_variants,
                'status': variant_status
            }
            
            add_step(
                "Model Variants",
                "SUCCESS" if model_health >= 80 else "PARTIAL" if model_health >= 60 else "WARNING",
                step_duration,
                "\n".join(model_details),
                model_health
            )
            
        except Exception as e:
            step_duration = time.time() - step_start
            system_state['errors_encountered'].append(f"Model initialization failed: {e}")
            system_state['health_metrics']['models'] = 0
            add_step(
                "Model Variants",
                "FAILED",
                step_duration,
                f"Model initialization failed: {str(e)}\nDeep learning models unavailable",
                0
            )
            raise RuntimeError(f"Model initialization failed: {e}")
        
        # Step 5: Performance Baseline with Hardware Optimization
        step_start = time.time()
        performance_health = 100.0
        
        try:
            # Establish performance baseline
            # Pass hardware data for optimized testing
            performance_metrics = establish_performance_baseline(hardware_data=system_state.get('hardware_detected'))
            
            # Analyze performance results
            baseline_details = []
            if 'error' in performance_metrics:
                performance_health = 25
                baseline_details.append(f"Baseline failed: {performance_metrics['error']}")
                system_state['warnings_issued'].append(f"Performance baseline failed: {performance_metrics['error']}")
            else:
                # Analyze individual component performance
                baselines = performance_metrics.get('baselines', {})
                summary = performance_metrics.get('summary', {})
                
                # CPU performance analysis
                if 'cpu' in baselines and 'cpu_error' not in baselines:
                    cpu_gflops = baselines['cpu'].get('gflops', 0)
                    baseline_details.append(f"CPU: {cpu_gflops:.2f} GFLOPS")
                    if cpu_gflops < 1:
                        performance_health -= 15
                else:
                    baseline_details.append("CPU: benchmark failed")
                    performance_health -= 20
                
                # Memory performance analysis
                if 'memory' in baselines and 'memory_error' not in baselines:
                    mem_speeds = [
                        baseline.get('allocation_speed_mbs', 0) 
                        for baseline in baselines['memory'].values() 
                        if isinstance(baseline, dict)
                    ]
                    avg_speed = np.mean(mem_speeds) if mem_speeds else 0
                    baseline_details.append(f"Memory: {avg_speed:.1f} MB/s average")
                    if avg_speed < 100:
                        performance_health -= 10
                else:
                    baseline_details.append("Memory: benchmark failed")
                    performance_health -= 15
                
                # GPU performance analysis
                if cuda_available:
                    if 'gpu' in baselines and 'gpu_error' not in baselines:
                        gpu_gflops = []
                        for gpu_data in baselines['gpu'].values():
                            if isinstance(gpu_data, dict):
                                gpu_gflops.append(gpu_data.get('gflops', 0))
                        
                        if gpu_gflops:
                            max_gflops = max(gpu_gflops)
                            baseline_details.append(f"GPU: {max_gflops:.1f} GFLOPS (best)")
                            if max_gflops < 100:
                                performance_health -= 10
                        else:
                            baseline_details.append("GPU: no performance data")
                            performance_health -= 15
                    else:
                        baseline_details.append("GPU: benchmark failed")
                        performance_health -= 15
                else:
                    baseline_details.append("GPU: not available (CPU-only system)")
                
                # I/O performance analysis
                if 'io' in baselines and 'io_error' not in baselines:
                    io_data = baselines['io']
                    write_speed = io_data.get('write_speed_mbs', 0)
                    read_speed = io_data.get('read_speed_mbs', 0)
                    baseline_details.append(f"I/O: {write_speed:.1f}/{read_speed:.1f} MB/s (write/read)")
                    if write_speed < 50 or read_speed < 50:
                        performance_health -= 5
                else:
                    baseline_details.append("I/O: benchmark failed")
                    performance_health -= 10
                
                # Overall system capability
                overall_capability = summary.get('overall_capability', 'standard')
                baseline_details.append(f"System class: {overall_capability}")
            
            step_duration = time.time() - step_start
            system_state['health_metrics']['performance'] = max(0, performance_health)
            system_state['performance_baseline'] = performance_metrics
            
            add_step(
                "Performance Baseline",
                "SUCCESS" if performance_health >= 80 else "PARTIAL" if performance_health >= 60 else "WARNING",
                step_duration,
                "\n".join(baseline_details),
                max(0, performance_health)
            )
            
        except Exception as e:
            step_duration = time.time() - step_start
            performance_metrics = {'baseline_failed': str(e)}
            system_state['errors_encountered'].append(f"Performance baseline failed: {e}")
            system_state['health_metrics']['performance'] = 25
            system_state['performance_baseline'] = performance_metrics
            add_step(
                "Performance Baseline",
                "WARNING",
                step_duration,
                f"Performance baseline failed: {str(e)}\nContinuing with default performance assumptions",
                25
            )
        
        # Step 6: System Integration and Final Validation
        step_start = time.time()
        integration_health = 100.0
        integration_details = []
        
        try:
            # Validate system integration
            integration_checks = []
            
            # Use results from the system checks that were already run
            existing_results = system_state.get('system_check_results', {})
            
            # Check global exception handler (extract from existing results)
            if 'exception_handler' in existing_results:
                exception_result = existing_results['exception_handler']
                if exception_result.passed:
                    integration_checks.append("Exception handler: CONFIGURED")
                else:
                    integration_checks.append("Exception handler: BASIC")
                    integration_health -= 10
            else:
                integration_checks.append("Exception handler: UNKNOWN")
                integration_health -= 15
            
            # Validate logging system integration (from existing results)
            if 'logging_setup' in existing_results:
                logging_check = existing_results['logging_setup']
                if logging_check.passed:
                    compliance = logging_check.details.get('compliance_score', 0) if isinstance(logging_check.details, dict) else 0
                    integration_checks.append(f"Logging integration: PASSED ({compliance}%)")
                    if compliance < 100:
                        integration_health -= (100 - compliance) * 0.1
                else:
                    integration_checks.append("Logging integration: ISSUES")
                    integration_health -= 20
            else:
                integration_checks.append("Logging integration: UNKNOWN")
                integration_health -= 20
            
            # Validate hardware integration (from existing results)
            if 'hardware' in existing_results:
                hardware_check = existing_results['hardware']
                if hardware_check.passed:
                    integration_checks.append("Hardware integration: OPTIMAL")
                else:
                    integration_checks.append("Hardware integration: SUBOPTIMAL")
                    integration_health -= 15
            else:
                integration_checks.append("Hardware integration: UNKNOWN")
                integration_health -= 15
            
            # Validate model-hardware integration
            if system_state.get('model_variants', {}).get('available', 0) > 0:
                if cuda_available:
                    integration_checks.append("Model-GPU integration: ENABLED")
                else:
                    integration_checks.append("Model-CPU integration: ENABLED")
            else:
                integration_checks.append("Model integration: FAILED")
                integration_health -= 25
            
            # Final memory cleanup and validation
            try:
                cleanup_result = enhanced_clear_memory(
                    aggressive=False, 
                    hardware_data=system_state.get('hardware_detected')
                )
                if cleanup_result.get('success', False):
                    integration_checks.append("Memory management: VALIDATED")
                else:
                    integration_checks.append("Memory management: ISSUES")
                    integration_health -= 10
            except Exception as e:
                integration_checks.append(f"Memory management: ERROR - {e}")
                integration_health -= 15
            
            integration_details = integration_checks
            
            step_duration = time.time() - step_start
            system_state['health_metrics']['integration'] = max(0, integration_health)
            
            add_step(
                "System Integration Validation",
                "SUCCESS" if integration_health >= 90 else "PARTIAL" if integration_health >= 70 else "WARNING",
                step_duration,
                "\n".join(integration_details),
                max(0, integration_health)
            )
            
        except Exception as e:
            step_duration = time.time() - step_start
            system_state['errors_encountered'].append(f"System integration failed: {e}")
            system_state['health_metrics']['integration'] = 50
            
            add_step(
                "System Integration Validation",
                "FAILED",
                step_duration,
                f"Integration validation failed: {str(e)}\nSystem may have compatibility issues",
                50
            )
        
        # Calculate overall system health and display results
        initialization_time = time.time() - initialization_start
        
        # Calculate weighted overall health score
        health_weights = {
            'early_setup': 0.15,
            'system_validation': 0.20,
            'configuration': 0.20,
            'models': 0.20,
            'performance': 0.10,
            'integration': 0.15
        }
        
        overall_health = 0.0
        for metric, weight in health_weights.items():
            health_value = system_state['health_metrics'].get(metric, 0)
            overall_health += health_value * weight
        
        # Add initialization table rows
        for step_name, status, duration, details, status_style, health_display in initialization_steps:
            init_table.add_row(
                Text(step_name, style="bold cyan"),
                Text(status, style=status_style),
                Text(f"{duration:.2f}s", style="dim"),
                Text(details, style="dim"),
                health_display
            )
        
        # Add summary row
        summary_status = "SUCCESS" if overall_health >= 80 else "PARTIAL" if overall_health >= 60 else "WARNING"
        summary_style = "bold white on green" if overall_health >= 80 else "bold white on yellow" if overall_health >= 60 else "bold white on red"
        
        init_table.add_row(
            Text("SYSTEM INITIALIZATION", style="bold white on yellow"),
            Text(summary_status, style=summary_style),
            Text(f"{initialization_time:.2f}s", style="bold white on yellow"),
            Text(f"Overall system health: {overall_health:.1f}% | Ready for deep learning operations", style=summary_style),
            f"[bold white on {'green' if overall_health >= 80 else 'yellow' if overall_health >= 60 else 'red'}]{overall_health:.0f}%[/]"
        )
        
        # Display the initialization table
        console.print(init_table)
        
        # Add user prompt based on initialization outcome
        banner_width = min(console_width - 8, 100)
        try:
            if overall_health >= 80:
                # Success scenario - all systems optimal
                overall_health_score = f"[bold cyan]{overall_health:.1f}%[/]"
                overall_duration = f"[bold cyan]{initialization_time:.2f} seconds[/]"
                model_variants_available = f"[bold cyan]{system_state.get('model_variants', {}).get('available', 0)}[/]"
                hardware = f"[bold cyan]{'GPU-accelerated' if cuda_available else 'CPU-only'} ({logical_cores} cores, {total_ram_gb:.1f} GB RAM)[/]"
                
                success_message = (
                    f"SYSTEM INITIALIZATION COMPLETED SUCCESSFULLY\n\n"
                    f"Overall Health Score: {overall_health_score}\n"
                    f"All critical systems are operational and optimized.\n"
                    f"The system is ready for deep learning operations.\n\n"
                    f"Duration: {overall_duration}\n"
                    f"Model Variants Available: {model_variants_available}\n"
                    f"Hardware: {hardware}\n\n"
                )
                try:
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            f"{success_message}",
                            border_style="green",
                            title="SUCCESS",
                            style="bold green",
                            padding=(1, 3),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        print(Fore.GREEN + Style.BRIGHT + f"\nSUCCESS:\n{success_message}")
                except Exception:
                    print(Fore.GREEN + Style.BRIGHT + f"\nSUCCESS:\n{success_message}")
                
                # Handle user choice with safe input
                user_choice = None
                max_attempts = 3
                
                for attempt in range(max_attempts):
                    try:
                        response = input(Fore.YELLOW + Style.BRIGHT + "\nContinue to system? (Y/n/q): ").strip().lower()
                        
                        if response in ['y', 'yes', '']:
                            user_choice = True
                            break
                        elif response in ['n', 'no', 'q', 'quit']:
                            user_choice = False
                            break
                        else:
                            if attempt < max_attempts - 1:
                                print(Fore.YELLOW + Style.BRIGHT + "\nPlease enter 'y' for yes or 'n' for no or 'q' for quit.")
                            
                    except (EOFError, KeyboardInterrupt):
                        user_choice = False
                        break
                    except Exception as input_error:
                        if logger:
                            logger.debug(f"Input error on attempt {attempt + 1}: {input_error}")
                        if attempt < max_attempts - 1:
                            print(Fore.RED + Style.BRIGHT + "\nInput error, please try again.")
                
                # Default to continue if no valid choice after max attempts
                if user_choice is None:
                    user_choice = True
                    print(Fore.CYAN + Style.BRIGHT + "\nUsing default choice: continue")
                
                # Handle user choice
                if user_choice is False:
                    try:
                        quit_message = (
                            "USER CHOSE TO QUIT\n\n"
                            "You chose to quit despite successful initialization.\n"
                            "System initialization cancelled."
                        )
                        if supports_color and banner_width > 60:
                            console.print(Panel.fit(
                                f"{quit_message}",
                                border_style="red",
                                style="bold red",
                                title="QUIT",
                                padding=(1, 3),
                                width=min(banner_width, console_width - 4)
                            ))
                        else:
                            print(Fore.RED + Style.BRIGHT + f"\nQUIT:\n{quit_message}")
                    except Exception:
                        print(Fore.RED + Style.BRIGHT + f"\nQUIT:\n{quit_message}")
                    
                    if logger:
                        logger.info("User chose to quit after successful system initialization")
                    print(Fore.RED + Style.BRIGHT + "\nExiting system initialization...")
                    
                    time.sleep(2)
                    sys.exit(0)
                
                # User chose to continue
                try:
                    continue_message = (
                        "CONTINUING TO SYSTEM\n\n"
                        "All systems initialized successfully - proceeding to main system."
                    )
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            f"{continue_message}",
                            border_style="green",
                            title="PROCEEDING",
                            style="bold green",
                            padding=(1, 2),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{continue_message}")
                except Exception:
                    print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{continue_message}")
                if logger:
                    logger.info(f"All systems initialized successfully (health: {overall_health:.1f}%) - user chose to continue")
                    logger.info(f"Model variants available: {system_state.get('model_variants', {}).get('available', 0)}")
                
            elif overall_health >= 60:
                # Partial success - some issues but system can continue
                warning_message = (
                    f"SYSTEM INITIALIZATION COMPLETED WITH WARNINGS\n\n"
                    f"Overall Health Score: {overall_health:.1f}%\n"
                    f"Some components have issues but the system can operate.\n"
                    f"Review the warnings above for details.\n\n"
                    f"Duration: {initialization_time:.2f} seconds\n"
                    f"Errors: {len(system_state.get('errors_encountered', []))}\n"
                    f"Warnings: {len(system_state.get('warnings_issued', []))}\n\n"
                )
                
                try:
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            f"{warning_message}",
                            border_style="yellow",
                            title="WARNING",
                            subtitle="Issues Detected",
                            style="bold yellow",
                            padding=(1, 3),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        print(Fore.YELLOW + Style.BRIGHT + f"\nWARNING:\n{warning_message}")
                except Exception:
                    print(Fore.YELLOW + Style.BRIGHT + f"\nWARNING:\n{warning_message}")
                
                # User choice with timeout
                user_choice = None
                max_attempts = 3
                
                for attempt in range(max_attempts):
                    try:
                        response = input(Fore.YELLOW + Style.BRIGHT + "\nContinue with reduced functionality? (Y/n/q): ").strip().lower()
                        
                        if response in ['y', 'yes', '']:
                            user_choice = True
                            break
                        elif response in ['n', 'no', 'q', 'quit']:
                            user_choice = False
                            break
                        else:
                            if attempt < max_attempts - 1:
                                print(Fore.YELLOW + Style.BRIGHT + "\nPlease enter 'y' for yes or 'n' for no or 'q' for quit.")
                            
                    except (EOFError, KeyboardInterrupt):
                        user_choice = False
                        break
                    except Exception as input_error:
                        if logger:
                            logger.debug(f"Input error on attempt {attempt + 1}: {input_error}")
                        if attempt < max_attempts - 1:
                            print(Fore.RED + Style.BRIGHT + "\nInput error, please try again.")
                
                # Default to continue if no valid choice
                if user_choice is None:
                    user_choice = True
                    print(Fore.CYAN + Style.BRIGHT + "\nUsing default choice: continue")
                
                if not user_choice:
                    try:
                        cancel_message = (
                            "INITIALIZATION CANCELLED\n\n"
                            "You chose to exit due to initialization warnings.\n"
                            "Please review the issues and try again."
                        )
                        if supports_color and banner_width > 60:
                            console.print(Panel.fit(
                                f"{cancel_message}",
                                border_style="red",
                                title="CANCELLED",
                                style="bold red",
                                padding=(1, 3),
                                width=min(banner_width, console_width - 4)
                            ))
                        else:
                            print(Fore.RED + Style.BRIGHT + f"\nCANCELLED:\n{cancel_message}")
                    except Exception:
                        print(Fore.RED + Style.BRIGHT + f"\nCANCELLED:\n{cancel_message}")
                    
                    if logger:
                        logger.warning("User chose to exit after seeing initialization warnings")
                    print(Fore.RED + Style.BRIGHT + "\nExiting system initialization...")
                    
                    time.sleep(2)
                    sys.exit(0)
                
                # User chose to continue
                try:
                    continue_message = (
                        "CONTINUING WITH WARNINGS\n\n"
                        "You chose to continue despite the warnings.\n"
                        "Some functionality may be limited."
                    )
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            f"{continue_message}",
                            border_style="green",
                            title="CONTINUING",
                            style="bold green",
                            padding=(1, 2),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        print(Fore.GREEN + Style.BRIGHT + f"\nCONTINUING:\n{continue_message}")
                except Exception:
                    print(Fore.GREEN + Style.BRIGHT + f"\nCONTINUING:\n{continue_message}")
                
                if logger:
                    logger.info(f"User chose to continue despite {overall_health:.1f}% health score")
                    logger.info(f"System has {len(system_state.get('errors_encountered', []))} errors and {len(system_state.get('warnings_issued', []))} warnings")
            
            else:
                # Critical issues - system health too low
                critical_message = (
                    f"CRITICAL SYSTEM INITIALIZATION ISSUES\n\n"
                    f"Overall Health Score: {overall_health:.1f}%\n"
                    f"The system has significant issues that may affect operation.\n"
                    f"Critical errors: {len([e for e in system_state.get('errors_encountered', []) if 'failed' in e.lower()])}\n"
                    f"Total errors: {len(system_state.get('errors_encountered', []))}\n"
                    f"Warnings: {len(system_state.get('warnings_issued', []))}\n\n"
                    f"It is strongly recommended to resolve these issues before continuing.\n\n"
                )
                try:
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            f"{critical_message}",
                            border_style="red",
                            title="WARNING",
                            subtitle="Critical Issues Detected",
                            style="bold red",
                            padding=(1, 3),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        print(Fore.RED + Style.BRIGHT + f"\nCRITICAL ISSUES:\n{critical_message}")
                except Exception:
                    print(Fore.RED + Style.BRIGHT + f"\nCRITICAL ISSUES:\n{critical_message}")
                
                # User choice with default to No
                user_choice = None
                max_attempts = 3
                
                for attempt in range(max_attempts):
                    try:
                        response = input(Fore.YELLOW + Style.BRIGHT + "\nContinue anyway? (y/N/q): ").strip().lower()
                        
                        if response in ['y', 'yes']:
                            user_choice = True
                            break
                        elif response in ['n', 'no', 'q', 'quit', '']:
                            user_choice = False
                            break
                        else:
                            if attempt < max_attempts - 1:
                                print(Fore.YELLOW + Style.BRIGHT + "\nPlease enter 'y' for yes or 'n' for no or 'q' for quit. Default is 'n'.")
                            
                    except (EOFError, KeyboardInterrupt):
                        user_choice = False
                        break
                    except Exception as input_error:
                        if logger:
                            logger.debug(f"Input error on attempt {attempt + 1}: {input_error}")
                        if attempt < max_attempts - 1:
                            print(Fore.RED + Style.BRIGHT + "\nInput error, please try again.")
                
                # Default to quit if no valid choice
                if user_choice is None:
                    user_choice = False
                    print(Fore.RED + Style.BRIGHT + "\nUsing default choice: quit")
                
                if not user_choice:
                    try:
                        quit_message = (
                            "INITIALIZATION TERMINATED\n\n"
                            "You chose to exit due to critical initialization issues.\n"
                            "Please review the error messages above and resolve the issues.\n\n"
                            "Check the logs and initialization report for detailed information."
                        )
                        if supports_color and banner_width > 60:
                            console.print(Panel.fit(
                                f"{quit_message}",
                                border_style="red",
                                title="TERMINATED",
                                style="bold red",
                                padding=(1, 3),
                                width=min(banner_width, console_width - 4)
                            ))
                        else:
                            print(Fore.RED + Style.BRIGHT + f"\nTERMINATED:\n{quit_message}")
                    except Exception:
                        print(Fore.RED + Style.BRIGHT + f"\nTERMINATED:\n{quit_message}")
                    
                    if logger:
                        logger.critical("User chose to exit due to critical initialization issues")
                        logger.critical(f"System health score: {overall_health:.1f}%")
                        for error in system_state.get('errors_encountered', []):
                            logger.error(f"  - {error}")
                    print(Fore.RED + Style.BRIGHT + "\nExiting due to critical system issues...")
                    
                    time.sleep(3)
                    sys.exit(1)
                
                # User chose to continue despite critical issues
                try:
                    risky_continue_message = (
                        "CONTINUING WITH CRITICAL ISSUES\n\n"
                        "You chose to continue despite critical system issues.\n"
                        "The system may not function correctly.\n\n"
                        "Proceed with caution and monitor for errors."
                    )
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            f"{risky_continue_message}",
                            border_style="red",
                            title="PROCEEDING",
                            subtitle="With Caution",
                            style="bold yellow",
                            padding=(1, 2),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{risky_continue_message}")
                except Exception:
                    print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{risky_continue_message}")
                
                if logger:
                    logger.warning(f"User chose to continue despite critical issues (health: {overall_health:.1f}%)")
                    logger.warning("System may not function correctly - monitoring recommended")
        
        except Exception as prompt_error:
            # If prompt handling fails, continue with warning
            print(Fore.RED + Style.BRIGHT + f"\nError in user prompt handling: {prompt_error}")
            
            if logger:
                logger.error(f"Error in initialization prompt: {prompt_error}")
            
            print(Fore.GREEN + Style.BRIGHT + "\nContinuing with system initialization...")
        
        # Clean up input buffer
        try:
            time.sleep(0.05)
            sys.stdout.flush()
            sys.stderr.flush()
            
            if hasattr(select, 'select') and sys.stdin.isatty():
                while select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], []):
                    line = sys.stdin.readline()
                    if not line:
                        break
            
            try:
                import msvcrt
                while msvcrt.kbhit():
                    msvcrt.getch()
            except ImportError:
                pass
            
            sys.stdin.flush()
            
        except Exception as cleanup_error:
            if logger:
                logger.debug(f"Input buffer cleanup failed: {cleanup_error}")
        
        # Clear console before continuing
        try:
            if is_tty:
                console.clear()
        except Exception:
            pass
        
        # Create system status report
        system_status = {
            'initialization': {
                'start_time': system_state['initialization_start'],
                'end_time': datetime.now(),
                'duration_seconds': initialization_time,
                'status': 'success',
                'method': 'optimized_system_check_framework',
                'overall_health_score': overall_health,
                'health_metrics': system_state['health_metrics'],
                'steps_completed': len(initialization_steps),
                'errors_count': len(system_state.get('errors_encountered', [])),
                'warnings_count': len(system_state.get('warnings_issued', []))
            },
            'system': {
                'platform': platform.platform(),
                'python_version': sys.version.split()[0],
                'pytorch_version': torch.__version__,
                'cuda_available': cuda_available,
                'cuda_device_count': torch.cuda.device_count() if cuda_available else 0,
                'cpu_cores': logical_cores,
                'total_ram_gb': total_ram_gb,
                'working_directory': str(Path.cwd()),
                'datasets_directory': str(essential_dirs.get('datasets', Path(__file__).resolve().parent / "datasets")),
                'artifacts_directory': str(essential_dirs.get('artifacts', Path(__file__).resolve().parent / "artifacts")),
                'figures_directory': str(essential_dirs.get('figures', Path(__file__).resolve().parent / "figures")),
                'info_directory': str(essential_dirs.get('info', Path(__file__).resolve().parent / "info")),
                'metrics_directory': str(essential_dirs.get('metrics', Path(__file__).resolve().parent / "metrics")),
                'log_directory': str(essential_dirs.get('logs', Path(__file__).resolve().parent / "logs")),
                'model_directory': str(essential_dirs.get('models', Path(__file__).resolve().parent / "models")),
                'config_directory': str(essential_dirs.get('config', Path(__file__).resolve().parent / "config")),
                'report_directory': str(essential_dirs.get('reports', Path(__file__).resolve().parent / "reports")),
                'cache_directory': str(essential_dirs.get('cache', Path(__file__).resolve().parent / "cache")),
                'export_directory': str(essential_dirs.get('exports', Path(__file__).resolve().parent / "exports")),
                'checkpoint_directory': str(essential_dirs.get('checkpoints', Path(__file__).resolve().parent / "checkpoints")),
                'tensorboard_directory': str(essential_dirs.get('tensorboard', Path(__file__).resolve().parent / "tensorboard")),
                'data_directory': str(essential_dirs.get('data', Path(__file__).resolve().parent / "data"))
            },
            'config': {
                'active_config': system_state.get('active_config', config),
                'config_file': str(CONFIG_FILE),
                'preset_name': config.get('_preset_name', 'custom'),
                'available_presets': list(PRESET_CONFIGS.keys()),
                'validation_status': 'passed' if system_state['health_metrics'].get('configuration', 0) >= 80 else 'issues',
                'changes_applied': len(system_state.get('config_changes', [])),
                'hardware_optimized': True
            },
            'hardware': {
                'detailed_info': system_state.get('hardware_detected', {}),
                'cpu_count': logical_cores,
                'memory_gb': total_ram_gb,
                'disk_space_gb': shutil.disk_usage('.').free / (1024**3),
                'cuda_available': cuda_available,
                'cuda_devices': [
                    {
                        'id': i,
                        'name': torch.cuda.get_device_name(i),
                        'memory_gb': torch.cuda.get_device_properties(i).total_memory / (1024**3),
                        'compute_capability': f"{torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}"
                    } for i in range(torch.cuda.device_count())
                ] if cuda_available else []
            },
            'models': {
                'variants_total': system_state.get('model_variants', {}).get('total', 0),
                'variants_available': system_state.get('model_variants', {}).get('available', 0),
                'variant_names': system_state.get('model_variants', {}).get('available_names', []),
                'variant_status': system_state.get('model_variants', {}).get('status', {}),
                'validation_passed': system_state['health_metrics'].get('models', 0) >= 80
            },
            'performance': system_state.get('performance_baseline', {}),
            'dependencies': {
                'torch_version': torch.__version__,
                'python_version': sys.version_info[:3],
                'platform': platform.system(),
                'optional_available': {
                    name: available for name, available in OPTIONAL_DEPENDENCIES.items()
                },
                'check_results': system_state.get('system_check_results', {})
            },
            'diagnostics': {
                'errors_encountered': system_state.get('errors_encountered', []),
                'warnings_issued': system_state.get('warnings_issued', []),
                'system_checks_passed': sum(1 for result in system_state.get('system_check_results', {}).values() if result and result.passed),
                'system_checks_total': len(system_state.get('system_check_results', {})),
                'integration_status': 'optimal' if system_state['health_metrics'].get('integration', 0) >= 90 else 'good' if system_state['health_metrics'].get('integration', 0) >= 70 else 'suboptimal'
            }
        }
        
        # Save initialization report
        try:
            save_initialization_report(system_status, essential_dirs['reports'])
            logger.debug(f"Initialization report saved to {essential_dirs['reports']}")
        except Exception as e:
            logger.warning(f"Failed to save initialization report: {e}")
        
        # Final logging summary with health metrics
        logger.debug("-" * 40)
        logger.debug("DEEP LEARNING SYSTEM INITIALIZATION COMPLETED")
        logger.debug("-" * 40)
        logger.debug(f"Overall Health Score: {overall_health:.1f}%")
        logger.debug(f"Initialization Time: {initialization_time:.2f} seconds")
        logger.debug(f"Configuration: {config.get('_preset_name', 'custom')} preset")
        logger.debug(f"Model Variants: {system_state.get('model_variants', {}).get('available', 0)}/{system_state.get('model_variants', {}).get('total', 0)} available")
        logger.debug(f"Hardware: {'GPU-accelerated' if cuda_available else 'CPU-only'} ({logical_cores} cores, {total_ram_gb:.1f}GB RAM)")
        logger.debug(f"System Status: {'OPTIMAL' if overall_health >= 90 else 'GOOD' if overall_health >= 70 else 'SUBOPTIMAL' if overall_health >= 50 else 'CRITICAL'}")
        
        if system_state.get('errors_encountered'):
            logger.warning(f"Errors encountered: {len(system_state['errors_encountered'])}")
            for error in system_state['errors_encountered']:
                logger.warning(f"  - {error}")
        
        if system_state.get('warnings_issued'):
            logger.debug(f"Warnings issued: {len(system_state['warnings_issued'])}")
            # Limit to first 5
            for warning in system_state['warnings_issued'][:5]:
                logger.debug(f"  - {warning}")
        
        logger.debug("-" * 40)
        
        return system_status, config, logger
        
    except KeyboardInterrupt:
        # Handle user interruption gracefully
        for step_name, status, duration, details, status_style, health_display in initialization_steps:
            init_table.add_row(
                Text(step_name, style="bold cyan"),
                Text(status, style=status_style),
                Text(f"{duration:.2f}s", style="dim"),
                Text(details, style="dim"),
                health_display
            )
        
        init_table.add_row(
            Text("SYSTEM INITIALIZATION", style="bold white on red"),
            Text("INTERRUPTED", style="bold white on red"),
            Text(f"{time.time() - initialization_start:.2f}s", style="bold white"),
            Text("System initialization was cancelled by user (Ctrl+C)", style="bright_white"),
            "[bold red]0%[/]"
        )
        
        console.print(init_table)
        logger.warning("System initialization interrupted by user (Ctrl+C)")
        sys.exit(0)
        
    except SystemExit:
        # User chose to quit during loading_screen or configuration
        logger.debug("System initialization cancelled by user choice")
        raise
        
    except Exception as e:
        # Handle critical initialization failures
        initialization_time = time.time() - initialization_start
        
        # Add completed steps to table
        for step_name, status, duration, details, status_style, health_display in initialization_steps:
            init_table.add_row(
                Text(step_name, style="bold cyan"),
                Text(status, style=status_style),
                Text(f"{duration:.2f}s", style="dim"),
                Text(details, style="dim"),
                health_display
            )
        
        init_table.add_row(
            Text("SYSTEM INITIALIZATION", style="bold white on red"),
            Text("CRITICAL FAILURE", style="bold white on red"),
            Text(f"{initialization_time:.2f}s", style="bold white"),
            Text(f"Critical error: {str(e)}\nType: {type(e).__name__}\nSystem cannot continue", style="bright_white"),
            "[bold red]0%[/]"
        )
        
        console.print(init_table)
        
        # Create error status report
        error_status = {
            'initialization': {
                'start_time': system_state.get('initialization_start', datetime.now()),
                'end_time': datetime.now(),
                'duration_seconds': initialization_time,
                'status': 'critical_failure',
                'error': str(e),
                'error_type': type(e).__name__,
                'overall_health_score': 0,
                'steps_attempted': len(initialization_steps),
                'errors_encountered': system_state.get('errors_encountered', []) + [str(e)],
                'warnings_issued': system_state.get('warnings_issued', [])
            },
            'system': {
                'platform': platform.platform(),
                'python_version': sys.version.split()[0],
                'hardware_detected': system_state.get('hardware_detected'),
                'final_state': 'unusable'
            },
            'diagnostics': {
                'critical_failure_point': initialization_steps[-1][0] if initialization_steps else 'unknown',
                'recovery_possible': False,
                'suggested_actions': [
                    "Check system requirements",
                    "Verify Python environment", 
                    "Check available disk space and memory",
                    "Review error logs for specific issues"
                ]
            }
        }
        
        logger.critical("-" * 40)
        logger.critical("CRITICAL: SYSTEM INITIALIZATION FAILED")
        logger.critical("-" * 40)
        logger.critical(f"Error: {str(e)}")
        logger.critical(f"Error Type: {type(e).__name__}")
        logger.critical(f"Duration: {initialization_time:.2f} seconds")
        logger.critical(f"Steps completed: {len(initialization_steps)}")
        logger.critical(f"Total errors: {len(system_state.get('errors_encountered', [])) + 1}")
        logger.exception("Detailed error information:")
        logger.critical("-" * 40)
        
        # Save error report
        try:
            #from datetime import datetime
            error_timestamp = datetime.now()
            date_str = error_timestamp.strftime('%Y%m%d')
            time_str = error_timestamp.strftime('%H%M%S')
            sequence_id = f"{date_str}_{time_str}"
            
            reports_dir = essential_dirs.get('reports', Path(__file__).resolve().parent / "reports")
            reports_dir.mkdir(parents=True, exist_ok=True)
            
            # Define consolidated error report file path (daily file)
            consolidated_error_file = reports_dir / f"system_initialization_errors_{date_str}.json"
            
            # Create a fully serializable error status
            serializable_error_status = {
                'initialization': {
                    'start_time': system_state.get('initialization_start', datetime.now()).isoformat(),
                    'end_time': datetime.now().isoformat(),
                    'duration_seconds': initialization_time,
                    'status': 'critical_failure',
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'overall_health_score': 0,
                    'steps_attempted': len(initialization_steps),
                    'errors_encountered': system_state.get('errors_encountered', []) + [str(e)],
                    'warnings_issued': system_state.get('warnings_issued', []),
                    'critical_failure_point': initialization_steps[-1][0] if initialization_steps else 'unknown'
                },
                'system': {
                    'platform': platform.platform(),
                    'python_version': sys.version.split()[0],
                    'pytorch_version': torch.__version__,
                    'cuda_available': cuda_available,
                    'cpu_cores': logical_cores,
                    'total_ram_gb': total_ram_gb,
                    'working_directory': str(Path.cwd()),
                    'final_state': 'unusable'
                },
                'hardware': {
                    # Convert hardware detected to serializable format
                    'cpu_cores': logical_cores,
                    'memory_gb': total_ram_gb,
                    'cuda_available': cuda_available,
                    'cuda_device_count': torch.cuda.device_count() if cuda_available else 0,
                    'disk_space_gb': shutil.disk_usage('.').free / (1024**3) if hasattr(shutil, 'disk_usage') else 0
                },
                'steps_completed': [
                    {
                        'step_name': step[0],
                        'status': step[1],
                        'duration': step[2],
                        'health_score': float(step[6]) if step[6] and step[6].replace('%', '').replace('[bold red]', '').replace('[/]', '').strip().isdigit() else 0.0
                    }
                    for step in initialization_steps
                ] if initialization_steps else [],
                'health_metrics': {
                    metric: float(score) if isinstance(score, (int, float)) else 0.0
                    for metric, score in system_state.get('health_metrics', {}).items()
                },
                'diagnostics': {
                    'recovery_possible': False,
                    'suggested_actions': [
                        "Check system requirements",
                        "Verify Python environment", 
                        "Check available disk space and memory",
                        "Review error logs for specific issues",
                        "Check for conflicting dependencies"
                    ],
                    'system_checks_passed': sum(1 for result in system_state.get('system_check_results', {}).values() if hasattr(result, 'passed') and result.passed),
                    'system_checks_total': len(system_state.get('system_check_results', {})),
                    'model_variants_available': system_state.get('model_variants', {}).get('available', 0),
                    'model_variants_total': system_state.get('model_variants', {}).get('total', 0)
                }
            }
            
            # Create entry metadata
            entry_metadata = {
                'sequence_id': sequence_id,
                'timestamp': error_timestamp.isoformat(),
                'time_str': time_str,
                'entry_type': 'critical_initialization_failure',
                'error_type': type(e).__name__,
                'initialization_duration_seconds': float(initialization_time),
                'steps_completed': len(initialization_steps),
                'failure_point': initialization_steps[-1][0] if initialization_steps else 'unknown'
            }
            
            # Create the error entry with serializable data
            error_entry = {
                **entry_metadata,
                'data': serializable_error_status
            }
            
            # Load existing error log data or create new structure
            if consolidated_error_file.exists():
                try:
                    with open(consolidated_error_file, 'r', encoding='utf-8') as f:
                        error_log_data = json.load(f)
                    
                    # Validate structure
                    if not isinstance(error_log_data, dict) or 'errors' not in error_log_data:
                        logger.warning(f"Invalid error log structure in {consolidated_error_file}, recreating")
                        error_log_data = {
                            'date': date_str,
                            'errors': [],
                            'metadata': {
                                'version': '2.0',
                                'created_at': error_timestamp.isoformat(),
                                'total_errors': 0,
                                'last_updated': error_timestamp.isoformat(),
                                'log_type': 'consolidated_initialization_errors'
                            }
                        }
                    
                    logger.debug(f"Loaded existing error log from {consolidated_error_file}")
                    
                except (json.JSONDecodeError, IOError) as load_error:
                    logger.warning(f"Failed to load existing error log, creating new: {load_error}")
                    error_log_data = {
                        'date': date_str,
                        'errors': [],
                        'metadata': {
                            'version': '2.0',
                            'created_at': error_timestamp.isoformat(),
                            'total_errors': 0,
                            'last_updated': error_timestamp.isoformat(),
                            'log_type': 'consolidated_initialization_errors'
                        }
                    }
            else:
                # Create new error log structure
                error_log_data = {
                    'date': date_str,
                    'errors': [error_entry],
                    'metadata': {
                        'version': '2.0',
                        'created_at': error_timestamp.isoformat(),
                        'total_errors': 1,
                        'last_updated': error_timestamp.isoformat(),
                        'log_type': 'consolidated_initialization_errors'
                    }
                }
            
            # Append new error entry if we loaded existing data
            if consolidated_error_file.exists():
                error_log_data['errors'].append(error_entry)
                error_log_data['metadata']['total_errors'] = len(error_log_data['errors'])
                error_log_data['metadata']['last_updated'] = error_timestamp.isoformat()
            
            # Keep only last 100 errors per day to prevent files from growing too large
            max_errors_per_day = 100
            if len(error_log_data['errors']) > max_errors_per_day:
                logger.debug(f"Trimming error log to last {max_errors_per_day} entries")
                error_log_data['errors'] = error_log_data['errors'][-max_errors_per_day:]
                error_log_data['metadata']['total_errors'] = len(error_log_data['errors'])
                error_log_data['metadata']['trimmed'] = True
                error_log_data['metadata']['trim_threshold'] = max_errors_per_day
            
            # Save updated error log - no need for default=str since everything is serializable
            try:
                with open(consolidated_error_file, 'w', encoding='utf-8') as f:
                    json.dump(error_log_data, f, indent=2, ensure_ascii=False)
                logger.error(f"Saved critical failure to consolidated error log: {consolidated_error_file}")
                logger.error(f"Total errors in log today: {error_log_data['metadata']['total_errors']}")
                
            except Exception as write_error:
                logger.error(f"Failed to write consolidated error log: {write_error}")
                
                # Try to save to backup location
                try:
                    backup_file = reports_dir / f"system_initialization_errors_{date_str}_backup.json"
                    with open(backup_file, 'w', encoding='utf-8') as f:
                        json.dump(error_log_data, f, indent=2, ensure_ascii=False)
                    logger.warning(f"Saved error log to backup location: {backup_file}")
                except Exception as backup_error:
                    logger.error(f"Failed to save error log backup: {backup_error}")
            
            # Also append to a human-readable summary file for easier review
            try:
                summary_file = reports_dir / f"system_initialization_errors_summary_{date_str}.txt"
                
                # Format error summary
                error_summary = (
                    f"\n{'=' * 80}\n"
                    f"CRITICAL INITIALIZATION FAILURE - {error_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n"
                    f"Sequence: {sequence_id} (Total today: {error_log_data['metadata']['total_errors']})\n"
                    f"{'=' * 80}\n"
                    f"Error Type: {type(e).__name__}\n"
                    f"Error Message: {str(e)}\n"
                    f"Initialization Duration: {initialization_time:.2f} seconds\n"
                    f"Steps Completed: {len(initialization_steps)}/{len(initialization_steps) + 1}\n"
                    f"Failure Point: {initialization_steps[-1][0] if initialization_steps else 'unknown'}\n"
                )
                
                # Add system context
                error_summary += f"Platform: {platform.platform()}\n"
                error_summary += f"Python Version: {sys.version.split()[0]}\n"
                error_summary += f"PyTorch Version: {torch.__version__}\n"
                error_summary += f"CUDA Available: {cuda_available}\n"
                
                # Add recent errors if available
                if system_state.get('errors_encountered'):
                    error_summary += f"Previous Errors: {len(system_state['errors_encountered'])}\n"
                    # Show last 3 errors
                    for prev_error in system_state['errors_encountered'][-3:]:
                        error_summary += f"  - {prev_error[:100]}...\n" if len(prev_error) > 100 else f"  - {prev_error}\n"
                
                error_summary += f"{'=' * 80}\n"
                
                # Append to summary file
                with open(summary_file, 'a', encoding='utf-8') as f:
                    f.write(error_summary)
                
                logger.debug(f"Appended error summary to {summary_file}")
                
            except Exception as summary_error:
                logger.debug(f"Failed to update error summary file: {summary_error}")
            
            # Update latest error log symlink/pointer
            try:
                latest_error_pointer = reports_dir / "latest_initialization_errors.json"
                if latest_error_pointer.exists() or latest_error_pointer.is_symlink():
                    latest_error_pointer.unlink()
                
                try:
                    # Try to create symlink
                    latest_error_pointer.symlink_to(consolidated_error_file.name)
                except (OSError, NotImplementedError):
                    # Fallback to text file with path
                    with open(reports_dir / "latest_initialization_errors.txt", 'w') as f:
                        f.write(f"Latest error log: {consolidated_error_file.name}\n")
                        f.write(f"Updated: {error_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"Total errors today: {error_log_data['metadata']['total_errors']}\n")
                
            except Exception as symlink_error:
                logger.debug(f"Failed to update latest error pointer: {symlink_error}")
            
            # Generate error analytics if this is a significant number of errors
            if error_log_data['metadata']['total_errors'] % 20 == 0:
                try:
                    # Generate error analytics
                    analytics = {
                        'date': date_str,
                        'total_errors': error_log_data['metadata']['total_errors'],
                        'generated_at': datetime.now().isoformat(),
                        'by_error_type': {},
                        'by_failure_point': {},
                        'recent_errors': []
                    }
                    
                    # Analyze errors
                    for entry in error_log_data['errors']:
                        # Count by error type
                        error_type = entry.get('error_type', 'unknown')
                        analytics['by_error_type'][error_type] = analytics['by_error_type'].get(error_type, 0) + 1
                        
                        # Count by failure point
                        failure_point = entry.get('failure_point', 'unknown')
                        analytics['by_failure_point'][failure_point] = analytics['by_failure_point'].get(failure_point, 0) + 1
                    
                    # Get recent errors (last 10)
                    analytics['recent_errors'] = [
                        {
                            'timestamp': entry.get('timestamp'),
                            'error_type': entry.get('error_type'),
                            'failure_point': entry.get('failure_point'),
                            'steps_completed': entry.get('steps_completed')
                        }
                        for entry in error_log_data['errors'][-10:]
                    ]
                    
                    # Save analytics
                    analytics_file = reports_dir / f"initialization_errors_analytics_{date_str}.json"
                    with open(analytics_file, 'w', encoding='utf-8') as f:
                        json.dump(analytics, f, indent=2, ensure_ascii=False)
                    
                    logger.debug(f"Generated error analytics: {analytics_file}")
                    
                except Exception as analytics_error:
                    logger.debug(f"Failed to generate error analytics: {analytics_error}")
            
        except Exception as save_error:
            logger.error(f"Failed to save critical failure report using consolidated system: {save_error}")
            
            # Emergency fallback - try to save to a simple timestamped file
            try:
                emergency_file = Path(globals().get('LOG_DIR', './logs')) / f"init_critical_failure_emergency_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                emergency_file.parent.mkdir(exist_ok=True, parents=True)
                
                with open(emergency_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'timestamp': datetime.now().isoformat(),
                        'error': str(e),
                        'error_type': type(e).__name__,
                        'save_error': str(save_error),
                        'emergency_save': True,
                        'steps_completed': len(initialization_steps),
                        'initialization_time': float(initialization_time)
                    }, f, indent=2, ensure_ascii=False)
                
                logger.warning(f"Emergency error report saved to: {emergency_file}")
                
            except Exception as emergency_error:
                logger.critical(f"Failed to save emergency error report: {emergency_error}")

        raise RuntimeError(f"System initialization failed: {e}") from e

def display_configuration_changes(changes: List[Dict], console: Console = None, logger: logging.Logger = None):
    """
    Display configuration changes in a rich table format with enhanced styling and comprehensive information.
    
    This updated function provides improved visual presentation of configuration changes
    that occur during system initialization, including change categorization, impact assessment,
    and detailed formatting that matches the enhanced initialization system's visual style.
    
    Args:
        changes: List of configuration change dictionaries with enhanced metadata
        console: Rich console instance (creates new if None)
        logger: Logger for summary and detailed change logging
    """
    if not changes:
        return
    
    if console is None:
        console = Console()
    
    # Enhanced validation of changes structure
    valid_changes = []
    invalid_changes = []
    
    for change in changes:
        if isinstance(change, dict) and all(key in change for key in ['section', 'parameter']):
            valid_changes.append(change)
        else:
            invalid_changes.append(change)
            if logger:
                logger.debug(f"Invalid change structure detected: {change}")
    
    if not valid_changes:
        if logger:
            logger.warning(f"No valid configuration changes to display ({len(invalid_changes)} invalid entries)")
        return
    
    # Create enhanced configuration changes table with improved styling
    config_table = Table(
        title=f"\n[bold bright_green]CONFIGURATION CHANGES APPLIED[/bold bright_green]",
        box=box.DOUBLE_EDGE,
        header_style="bold bright_white on blue",
        border_style="bright_cyan",
        title_style="bold bright_green",
        title_justify="center",
        show_lines=True,
        expand=True,
        width=min(120, console.width - 4),
        caption="[dim]Changes applied during system initialization[/dim]",
        caption_justify="center"
    )
    
    # Enhanced column structure with better width management
    config_table.add_column("Section", style="bold cyan", width=14, no_wrap=True)
    config_table.add_column("Parameter", style="bold yellow", width=22, no_wrap=False)
    config_table.add_column("Previous", style="dim red", width=18, justify="center", no_wrap=False)
    config_table.add_column("Current", style="bold green", width=18, justify="center", no_wrap=False)
    config_table.add_column("Source", style="dim blue", width=12, justify="center", no_wrap=True)
    config_table.add_column("Impact", style="bold", width=10, justify="center", no_wrap=True)
    
    # Enhanced change categorization and analysis
    change_categories = {
        'CRITICAL': [],      # Changes that significantly affect system behavior
        'IMPORTANT': [],     # Changes that modify key functionality
        'MINOR': [],         # Changes that have limited impact
        'AUTOMATIC': [],     # Changes applied automatically by the system
        'USER_REQUESTED': [], # Changes explicitly requested by user
        'FALLBACK': []       # Changes applied due to errors or fallbacks
    }
    
    section_groups = {}
    total_changes = len(valid_changes)
    
    # Process each change and categorize/assess impact
    for change in valid_changes:
        section = change.get('section', 'UNKNOWN').upper()
        parameter = change.get('parameter', 'unknown')
        old_value = change.get('old_value', 'N/A')
        new_value = change.get('new_value', 'N/A')
        source = change.get('source', 'unknown')
        
        # Categorize change
        category = 'MINOR'  # Default
        # Critical changes that significantly affect system behavior
        critical_params = {
            'SYSTEM': ['device', 'cuda_optimizations', 'random_seed', 'reproducible'],
            'TRAINING': ['batch_size', 'learning_rate', 'optimizer', 'mixed_precision'],
            'MODEL': ['model_type', 'encoding_dim', 'hidden_dims', 'num_models'],
            'SECURITY': ['percentile', 'attack_threshold', 'enable_security_metrics'],
            'DATA': ['normal_samples', 'attack_samples', 'features', 'use_real_data']
        }
        
        # Check if this is a critical parameter
        if parameter in critical_params.get(section, []):
            category = 'CRITICAL'
        # Check source-based categorization
        elif source in ['validation_fallback', 'error_recovery', 'emergency_default']:
            category = 'FALLBACK'
        elif source in ['user_input', 'interactive', 'manual']:
            category = 'USER_REQUESTED'
        elif source in ['auto_optimization', 'hardware_detection', 'preset_application']:
            category = 'AUTOMATIC'
        # Important changes that modify key functionality
        elif section in ['TRAINING', 'MODEL', 'SECURITY']:
            category = 'IMPORTANT'
        
        change_categories[category].append(change)
        
        # Assess change impact
        impact_level = 'LOW'  # Default
        
        # High impact parameters
        high_impact_params = {
            'model_type', 'device', 'cuda_optimizations', 'batch_size', 
            'learning_rate', 'num_models', 'encoding_dim', 'use_real_data'
        }
        
        if parameter in high_impact_params:
            impact_level = 'HIGH'
        # Medium impact parameters
        elif parameter in {
            'hidden_dims', 'dropout_rates', 'optimizer', 'mixed_precision',
            'percentile', 'attack_threshold', 'normalization'
        }:
            impact_level = 'MEDIUM'
        # Check for significant value changes
        elif isinstance(old_value, (int, float)) and isinstance(new_value, (int, float)):
            if abs(old_value - new_value) / max(abs(old_value), abs(new_value), 1) > 0.5:
                impact_level = 'MEDIUM'
        # Low impact for specific parameters
        elif parameter in ['verbose', 'debug', 'log_frequency', 'metrics_frequency']:
            impact_level = 'LOW'
        
        # Format values for display
        def format_value(value, max_length=15):
            if value is None:
                return "None"
            elif isinstance(value, bool):
                return "True" if value else "False"
            elif isinstance(value, (list, tuple)):
                if len(value) <= 3:
                    formatted = str(value)
                else:
                    formatted = f"[{len(value)} items]"
            elif isinstance(value, dict):
                formatted = f"{{{len(value)} keys}}"
            else:
                formatted = str(value)
            
            # Truncate if too long
            if len(formatted) > max_length:
                return formatted[:max_length-3] + "..."
            
            return formatted
        
        formatted_old = format_value(old_value)
        formatted_new = format_value(new_value)
        
        # Format source display
        source_mapping = {
            'validation_fallback': 'Fallback',
            'error_recovery': 'Recovery',
            'emergency_default': 'Emergency',
            'user_input': 'User',
            'interactive': 'User',
            'manual': 'Manual',
            'auto_optimization': 'Auto',
            'hardware_detection': 'Hardware',
            'preset_application': 'Preset',
            'system_default': 'Default',
            'config_file': 'Config',
            'unknown': 'Unknown'
        }
        source_display = source_mapping.get(source, source.title())
        
        # Group by section for organized display
        if section not in section_groups:
            section_groups[section] = []
        section_groups[section].append({
            **change,
            'category': category,
            'formatted_old': formatted_old,
            'formatted_new': formatted_new,
            'impact_level': impact_level,
            'source_display': source_display
        })
    
    # Sort sections by importance and alphabetically
    section_priority = {
        'SYSTEM': 1, 'TRAINING': 2, 'MODEL': 3, 'SECURITY': 4, 
        'DATA': 5, 'MONITORING': 6, 'HARDWARE': 7, 'PRESETS': 8
    }
    
    sorted_sections = sorted(section_groups.keys(), 
                           key=lambda x: (section_priority.get(x, 99), x))
    
    # Add table rows with enhanced formatting and organization
    for section_name in sorted_sections:
        section_changes = section_groups[section_name]
        
        # Add prominent section header with change count
        config_table.add_row(
            Text(f"{section_name}", style="bold white on blue"),
            Text(f"({len(section_changes)} changes)", style="bold white on blue", justify="center"),
            "",
            "",
            "",
            "",
            style="bold white on blue"
        )
        
        # Sort changes within section by impact level and parameter name
        impact_priority = {'HIGH': 1, 'MEDIUM': 2, 'LOW': 3, 'NONE': 4}
        sorted_changes = sorted(section_changes, 
                              key=lambda x: (impact_priority.get(x['impact_level'], 99), 
                                           x.get('parameter', '')))
        
        # Add changes for this section with enhanced styling
        for change_info in sorted_changes:
            parameter = change_info.get('parameter', 'unknown')
            formatted_old = change_info['formatted_old']
            formatted_new = change_info['formatted_new']
            source_display = change_info['source_display']
            impact_level = change_info['impact_level']
            category = change_info['category']
            
            # Format impact display
            impact_styles = {
                'HIGH': ("HIGH", "bold red"),
                'MEDIUM': ("MED", "bold yellow"),
                'LOW': ("LOW", "bold green"),
                'NONE': ("NONE", "dim")
            }
            impact_text, impact_style = impact_styles.get(impact_level, (impact_level, "dim"))
            impact_display = Text(impact_text, style=impact_style)
            
            # Get parameter style
            if category == 'CRITICAL':
                param_style = "bold red"
            elif category == 'IMPORTANT':
                param_style = "bold yellow"
            elif category == 'FALLBACK':
                param_style = "bold magenta"
            elif impact_level == 'HIGH':
                param_style = "bold bright_yellow"
            else:
                param_style = "bold white"
            
            # Get new value style
            new_value_styles = {
                'HIGH': "bold bright_green",
                'MEDIUM': "bold green",
                'LOW': "green",
                'NONE': "dim green"
            }
            new_value_style = new_value_styles.get(impact_level, "green")
            
            # Add parameter row with appropriate styling
            config_table.add_row(
                "",  # Empty section column for parameter rows
                Text(parameter, style=param_style),
                Text(formatted_old, style="dim red"),
                Text(formatted_new, style=new_value_style),
                Text(source_display, style="dim blue"),
                impact_display
            )
    
    # Add comprehensive summary row with statistics
    critical_count = len(change_categories['CRITICAL'])
    important_count = len(change_categories['IMPORTANT'])
    minor_count = len(change_categories['MINOR'])
    
    config_table.add_row(
        Text("SUMMARY", style="bold bright_white on black"),
        Text(f"{total_changes} total changes", style="bold white"),
        Text(f"Critical: {critical_count}", style="bold red" if critical_count > 0 else "dim"),
        Text(f"Important: {important_count}", style="bold yellow" if important_count > 0 else "dim"),
        Text(f"Minor: {minor_count}", style="bold green" if minor_count > 0 else "dim"),
        Text("APPLIED", style="bold bright_green"),
        style="bold bright_white on black"
    )
    
    # Display the enhanced table
    console.print(config_table)
    
    # Enhanced post-table information display
    critical_changes = change_categories['CRITICAL']
    important_changes = change_categories['IMPORTANT']
    fallback_changes = change_categories['FALLBACK']
    
    if critical_changes or important_changes or fallback_changes:
        console.print()  # Add spacing
        
        # Create impact summary panel
        summary_content = []
        
        if critical_changes:
            summary_content.append(f"[bold red]Critical Changes ({len(critical_changes)}):[/bold red]")
            for change in critical_changes[:3]:  # Show first 3
                param = change.get('parameter', 'unknown')
                section = change.get('section', 'unknown')
                summary_content.append(f"  - {section}.{param}")
            if len(critical_changes) > 3:
                summary_content.append(f"  - ... and {len(critical_changes) - 3} more")
            summary_content.append("")
        
        if important_changes:
            summary_content.append(f"[bold yellow]Important Changes ({len(important_changes)}):[/bold yellow]")
            for change in important_changes[:3]:  # Show first 3
                param = change.get('parameter', 'unknown')
                section = change.get('section', 'unknown')
                summary_content.append(f"  - {section}.{param}")
            if len(important_changes) > 3:
                summary_content.append(f"  - ... and {len(important_changes) - 3} more")
            summary_content.append("")
        
        if fallback_changes:
            summary_content.append(f"[bold magenta]Fallback Changes ({len(fallback_changes)}):[/bold magenta]")
            summary_content.append("These changes were applied due to errors or validation failures:")
            for change in fallback_changes[:2]:  # Show first 2
                param = change.get('parameter', 'unknown')
                section = change.get('section', 'unknown')
                source = change.get('source', 'unknown')
                summary_content.append(f"  - {section}.{param} ({source})")
            if len(fallback_changes) > 2:
                summary_content.append(f"  - ... and {len(fallback_changes) - 2} more")
        
        # Display impact summary panel
        console.print(Panel.fit(
            "\n".join(summary_content),
            title="[bold]Change Impact Summary[/bold]",
            border_style="yellow",
            padding=(1, 2)
        ))
    
    # Enhanced logging with comprehensive details
    if logger:
        # Log summary statistics
        total_changes = len(valid_changes)
        critical_count = len(change_categories['CRITICAL'])
        important_count = len(change_categories['IMPORTANT'])
        fallback_count = len(change_categories['FALLBACK'])
        
        logger.info(f"Configuration changes summary:")
        logger.info(f"  - Total valid changes: {total_changes}")
        logger.info(f"  - Critical changes: {critical_count}")
        logger.info(f"  - Important changes: {important_count}")
        logger.info(f"  - Fallback changes: {fallback_count}")
        
        if invalid_changes:
            logger.warning(f"  - Invalid change entries: {len(invalid_changes)}")
        
        # Log section breakdown
        section_counts = {}
        for change in valid_changes:
            section = change.get('section', 'UNKNOWN')
            section_counts[section] = section_counts.get(section, 0) + 1
        
        if section_counts:
            section_summary = ", ".join([f"{section}: {count}" for section, count in section_counts.items()])
            logger.info(f"  - Changes by section: {section_summary}")
        
        # Log critical changes in detail
        if critical_count > 0:
            logger.warning("Critical configuration changes detected:")
            for change in change_categories['CRITICAL']:
                section = change.get('section', 'unknown')
                parameter = change.get('parameter', 'unknown')
                old_val = change.get('old_value', 'N/A')
                new_val = change.get('new_value', 'N/A')
                source = change.get('source', 'unknown')
                logger.warning(f"  - {section}.{parameter}: {old_val} -> {new_val} (source: {source})")
        
        # Log fallback changes with warnings
        if fallback_count > 0:
            logger.warning("Fallback configuration changes applied due to errors:")
            for change in change_categories['FALLBACK']:
                section = change.get('section', 'unknown')
                parameter = change.get('parameter', 'unknown')
                source = change.get('source', 'unknown')
                logger.warning(f"  - {section}.{parameter} (fallback source: {source})")
        
        # Log detailed change information at debug level
        logger.debug("Detailed configuration changes:")
        for change in valid_changes:
            change_str = (
                f"  - {change.get('section', 'unknown')}.{change.get('parameter', 'unknown')}: "
                f"{change.get('old_value', 'N/A')} -> {change.get('new_value', 'N/A')} "
                f"(source: {change.get('source', 'unknown')})"
            )
            logger.debug(change_str)

def save_initialization_report(system_status: Dict[str, Any], report_dir: Path) -> None:
    """
    Save an initialization report to disk with multiple formats.
    
    This function creates both machine-readable (JSON) and human-readable (TXT)
    reports that match the comprehensive system_status structure from initialize_system.
    
    Args:
        system_status: Complete system status dictionary from initialize_system
        report_dir: Directory to save the report (should be a Path object)
        
    Raises:
        Exception: If report saving fails (logged but not re-raised to avoid interrupting initialization)
    """
    from datetime import datetime
    
    # Define file variables at the top to ensure they're in scope for error handling
    consolidated_files = {}
    report_data, status_data, diagnostics_data, dashboard_json_file_data = None, None, None, None
    
    try:
        # Determine report_dir (default: script's directory / reports)
        if report_dir is None:
            report_dir = Path(__file__).resolve().parent / "reports"
        report_dir.mkdir(parents=True, exist_ok=True)
        
        # Ensure report_dir is a Path object
        if not isinstance(report_dir, Path):
            report_dir = Path(__file__).resolve().parent / "reports"
        
        # Create timestamp for report files
        timestamp_obj = datetime.now()
        date_str = timestamp_obj.strftime('%Y%m%d')
        time_str = timestamp_obj.strftime('%H%M%S')
        timestamp = f"{date_str}_{time_str}"
        
        # System status with additional diagnostic information using existing functions
        enhanced_status = system_status.copy()
        
        # Add hardware information using check_hardware()
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            enhanced_status['detailed_hardware'] = hardware_data
        except Exception as e:
            enhanced_status['detailed_hardware_error'] = str(e)
        
        # Add version information using check_versions()
        try:
            version_info = check_versions(include_optional=True)
            enhanced_status['detailed_versions'] = version_info
        except Exception as e:
            enhanced_status['detailed_versions_error'] = str(e)
        
        # Add seed configuration status using check_seed_config()
        try:
            seed_result = check_seed_config(enhanced_status.get('detailed_hardware'))
            enhanced_status['reproducibility'] = {
                'seed_config_passed': seed_result.passed,
                'compliance_score': seed_result.metadata.get('compliance_score', 0) if seed_result.metadata else 0,
                'recommendations': seed_result.metadata.get('recommendations', []) if seed_result.metadata else [],
                'details': seed_result.details
            }
        except Exception as e:
            enhanced_status['reproducibility_error'] = str(e)
        
        # Add logging configuration status using check_logging_setup()
        try:
            logging_result = check_logging_setup()
            enhanced_status['logging_config'] = {
                'passed': logging_result.passed,
                'compliance_score': logging_result.details.get('compliance_score', 0) if isinstance(logging_result.details, dict) else 0,
                'handlers_count': len(logging_result.details.get('handlers', [])) if isinstance(logging_result.details, dict) else 0,
                'feedback': logging_result.details.get('feedback', []) if isinstance(logging_result.details, dict) else []
            }
        except Exception as e:
            enhanced_status['logging_config_error'] = str(e)
        
        # Add performance monitoring status
        try:
            perf_monitoring_result = check_performance_monitoring()
            enhanced_status['performance_monitoring'] = {
                'available': perf_monitoring_result.passed,
                'capabilities': perf_monitoring_result.details.get('capabilities', {}) if isinstance(perf_monitoring_result.details, dict) else {},
                'hardware_integration': perf_monitoring_result.details.get('hardware_integration', False) if isinstance(perf_monitoring_result.details, dict) else False
            }
        except Exception as e:
            enhanced_status['performance_monitoring_error'] = str(e)
        
        # Add memory management status
        try:
            memory_mgmt_result = check_memory_management()
            enhanced_status['memory_management'] = {
                'comprehensive': memory_mgmt_result.passed,
                'capabilities': memory_mgmt_result.details.get('capabilities', {}) if isinstance(memory_mgmt_result.details, dict) else {},
                'test_results': memory_mgmt_result.details.get('test_results', {}) if isinstance(memory_mgmt_result.details, dict) else {}
            }
        except Exception as e:
            enhanced_status['memory_management_error'] = str(e)
        
        # Add system configuration information using get_system_info()
        try:
            system_info = get_system_info(include_versions=True, include_hardware=True)
            enhanced_status['comprehensive_system_info'] = system_info
        except Exception as e:
            enhanced_status['comprehensive_system_info_error'] = str(e)
        
        # Create a serializable version of the report
        serializable_status = {}
        for key, value in enhanced_status.items():
            try:
                # Test if the value is JSON serializable
                json.dumps(value, default=str)
                serializable_status[key] = value
            except (TypeError, ValueError):
                # Convert problematic values to strings
                serializable_status[key] = str(value)
        
        # Add metadata to the JSON report
        serializable_status['_metadata'] = {
            'report_version': '3.0',
            'generated_at': timestamp_obj.isoformat(),
            'report_type': 'comprehensive_system_initialization',
            'format': 'json',
            'generator': 'initialize_system',
            'enhancement_level': 'full_diagnostic',
            'check_functions_used': [
                'check_hardware',
                'check_versions', 
                'check_seed_config',
                'check_logging_setup',
                'check_performance_monitoring',
                'check_memory_management',
                'get_system_info'
            ],
            'python_version': sys.version,
            'platform': platform.platform(),
            'working_directory': str(Path.cwd())
        }
        
        # Calculate system health score for summary
        checks_performed = []
        checks_passed = []
        
        detailed_hw = enhanced_status.get('detailed_hardware', {})
        detailed_versions = enhanced_status.get('detailed_versions', {})
        reproducibility = enhanced_status.get('reproducibility', {})
        logging_config = enhanced_status.get('logging_config', {})
        perf_monitoring = enhanced_status.get('performance_monitoring', {})
        memory_mgmt = enhanced_status.get('memory_management', {})
        
        if 'detailed_hardware' in enhanced_status:
            checks_performed.append('Hardware Analysis')
            if detailed_hw.get('cuda', {}).get('status') in ['PASS', 'WARN']:
                checks_passed.append('Hardware Analysis')
        
        if 'detailed_versions' in enhanced_status:
            checks_performed.append('Version Validation')
            compatible_count = sum(1 for v in detailed_versions.values() if isinstance(v, dict) and v.get('compatible', False))
            if compatible_count > 0:
                checks_passed.append('Version Validation')
        
        if 'reproducibility' in enhanced_status:
            checks_performed.append('Reproducibility Config')
            if reproducibility.get('seed_config_passed', False):
                checks_passed.append('Reproducibility Config')
        
        if 'logging_config' in enhanced_status:
            checks_performed.append('Logging Configuration')
            if logging_config.get('passed', False):
                checks_passed.append('Logging Configuration')
        
        if 'performance_monitoring' in enhanced_status:
            checks_performed.append('Performance Monitoring')
            if perf_monitoring.get('available', False):
                checks_passed.append('Performance Monitoring')
        
        if 'memory_management' in enhanced_status:
            checks_performed.append('Memory Management')
            if memory_mgmt.get('comprehensive', False):
                checks_passed.append('Memory Management')
        
        system_health_score = (len(checks_passed) / max(len(checks_performed), 1) * 100) if checks_performed else 0
        
        # Create compact status data
        compact_status = {
            'status': enhanced_status.get('initialization', {}).get('status', 'unknown'),
            'timestamp': timestamp_obj.isoformat(),
            'duration_seconds': enhanced_status.get('initialization', {}).get('duration_seconds', 0),
            'cuda_available': enhanced_status.get('system', {}).get('cuda_available', False),
            'model_variants': enhanced_status.get('models', {}).get('variants_available', 0),
            'config_preset': enhanced_status.get('config', {}).get('preset_name', 'unknown'),
            'errors': enhanced_status.get('initialization', {}).get('error') is not None,
            'system_health_score': system_health_score,
            'hardware_status': enhanced_status.get('detailed_hardware', {}).get('cuda', {}).get('status', 'unknown'),
            'reproducibility_score': enhanced_status.get('reproducibility', {}).get('compliance_score', 0),
            'logging_compliance': enhanced_status.get('logging_config', {}).get('compliance_score', 0),
            'performance_monitoring': enhanced_status.get('performance_monitoring', {}).get('available', False),
            'memory_management': enhanced_status.get('memory_management', {}).get('comprehensive', False)
        }
        
        # Create diagnostic data
        diagnostic_data = {
            'metadata': {
                'timestamp': timestamp_obj.isoformat(),
                'report_type': 'diagnostic',
                'check_framework_version': '3.0'
            },
            'hardware_analysis': enhanced_status.get('detailed_hardware', {}),
            'version_validation': enhanced_status.get('detailed_versions', {}),
            'reproducibility_config': enhanced_status.get('reproducibility', {}),
            'logging_config': enhanced_status.get('logging_config', {}),
            'performance_monitoring': enhanced_status.get('performance_monitoring', {}),
            'memory_management': enhanced_status.get('memory_management', {}),
            'system_info': enhanced_status.get('comprehensive_system_info', {})
        }
        
        # Create dashboard data for JSON storage
        dashboard_json_data = {
            'timestamp': timestamp_obj.isoformat(),
            'health_score': system_health_score,
            'status': compact_status['status'],
            'duration_seconds': compact_status['duration_seconds'],
            'cuda_available': compact_status['cuda_available'],
            'model_variants': compact_status['model_variants'],
            'reproducibility_score': compact_status.get('reproducibility_score', 0),
            'logging_compliance': compact_status.get('logging_compliance', 0),
            'performance_monitoring': compact_status.get('performance_monitoring', False),
            'memory_management': compact_status.get('memory_management', False)
        }
        
        # DAILY CONSOLIDATED FILES IMPLEMENTATION
        # Define all the consolidated file paths
        consolidated_files = {
            'report': report_dir / f"deep_init_report_{date_str}.json",
            'summary': report_dir / f"deep_init_summary_{date_str}.txt", 
            'status': report_dir / f"deep_init_status_{date_str}.json",
            'diagnostics': report_dir / f"deep_init_diagnostics_{date_str}.json",
            'dashboard_json': report_dir / f"deep_system_dashboard_data_{date_str}.json",
            'dashboard_html': report_dir / f"deep_system_dashboard_{date_str}.html"
        }
        
        # Create entry metadata
        entry_metadata = {
            'sequence_id': timestamp,
            'timestamp': timestamp_obj.isoformat(),
            'time_str': time_str
        }
        
        # 1. Save to consolidated JSON report (append as array element)
        report_entry = {
            **entry_metadata,
            'data': serializable_status
        }
        
        # Use proper file handling with context managers
        if consolidated_files['report'].exists():
            try:
                with open(consolidated_files['report'], 'r', encoding='utf-8') as f:
                    report_data = json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Failed to load existing consolidated file, creating new: {e}")
                report_data = {
                    'date': date_str,
                    'reports': [],
                    'metadata': {
                        'version': '3.0',
                        'created_at': timestamp_obj.isoformat(),
                        'total_reports': 0,
                        'last_updated': timestamp_obj.isoformat(),
                        'report_type': 'consolidated_daily'
                    }
                }
        else:
            report_data = {
                'date': date_str,
                'reports': [report_entry],
                'metadata': {
                    'version': '3.0',
                    'created_at': timestamp_obj.isoformat(),
                    'total_reports': 1,
                    'last_updated': timestamp_obj.isoformat(),
                    'report_type': 'consolidated_daily'
                }
            }
        
        # Only append if we loaded existing data
        if consolidated_files['report'].exists():
            report_data['reports'].append(report_entry)
            report_data['metadata']['total_reports'] = len(report_data['reports'])
            report_data['metadata']['last_updated'] = timestamp_obj.isoformat()
        
        # Save the report data
        with open(consolidated_files['report'], 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, default=str, ensure_ascii=False)
        
        # 2. Append to consolidated summary file
        summary_header = f"""
{'=' * 80}
INITIALIZATION REPORT - {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')}
Sequence: {time_str} (Total today: {report_data['metadata']['total_reports']})
{'=' * 80}
"""
        
        # Create the complete summary content as a string first
        init_info = enhanced_status.get('initialization', {})
        sys_info = enhanced_status.get('system', {})
        detailed_hw = enhanced_status.get('detailed_hardware', {})
        config_info = enhanced_status.get('config', {})
        model_info = enhanced_status.get('models', {})
        performance = enhanced_status.get('performance', {})
        deps_info = enhanced_status.get('dependencies', {})
        
        # Build summary content as a single string to avoid file handle issues
        summary_content = summary_header
        
        # Initialization Status
        summary_content += "INITIALIZATION STATUS\n"
        summary_content += "-" * 30 + "\n"
        summary_content += f"Status: {init_info.get('status', 'unknown').upper()}\n"
        summary_content += f"Duration: {init_info.get('duration_seconds', 0):.3f} seconds\n"
        summary_content += f"Method: {init_info.get('method', 'unknown')}\n"
        summary_content += f"Start Time: {init_info.get('start_time', 'unknown')}\n"
        summary_content += f"End Time: {init_info.get('end_time', 'unknown')}\n"
        
        if 'error' in init_info:
            summary_content += f"Error: {init_info['error']}\n"
            summary_content += f"Error Type: {init_info.get('error_type', 'unknown')}\n"
        summary_content += "\n"
        
        # System Environment
        summary_content += "SYSTEM ENVIRONMENT\n"
        summary_content += "-" * 20 + "\n"
        summary_content += f"Platform: {sys_info.get('platform', 'unknown')}\n"
        summary_content += f"Python Version: {sys_info.get('python_version', 'unknown')}\n"
        summary_content += f"PyTorch Version: {sys_info.get('pytorch_version', 'unknown')}\n"
        
        # Hardware details
        cpu_info = detailed_hw.get('cpu_cores', {})
        if cpu_info.get('available'):
            summary_content += f"CPU: {cpu_info.get('logical_cores', '?')} logical cores, {cpu_info.get('physical_cores', '?')} physical\n"
            if 'capacity' in cpu_info:
                freq = cpu_info['capacity'].get('frequency_ghz')
                if freq:
                    summary_content += f"CPU Frequency: {freq} GHz\n"
        
        ram_info = detailed_hw.get('system_ram', {})
        if ram_info.get('available'):
            summary_content += f"System RAM: {ram_info.get('ram_total_gb', 0):.1f}GB total, {ram_info.get('ram_available_gb', 0):.1f}GB available\n"
            if ram_info.get('ram_percent'):
                summary_content += f"RAM Usage: {ram_info.get('ram_percent', 0):.1f}%\n"
        
        disk_info = detailed_hw.get('disk_space', {})
        if disk_info.get('available') is not None:
            summary_content += f"Disk Space: {disk_info.get('free_gb', 0):.1f}GB free of {disk_info.get('total_gb', 0):.1f}GB total\n"
        
        # CUDA information
        cuda_info = detailed_hw.get('cuda', {})
        summary_content += f"CUDA Available: {cuda_info.get('available', False)}\n"
        if cuda_info.get('available'):
            summary_content += f"CUDA Version: {cuda_info.get('cuda_version', 'unknown')}\n"
            summary_content += f"cuDNN Version: {cuda_info.get('cudnn_version', 'unknown')}\n"
            summary_content += f"GPU Count: {cuda_info.get('gpu_count', 0)}\n"
            
            for i, gpu in enumerate(cuda_info.get('gpus', [])):
                summary_content += f"  GPU {i}: {gpu.get('name', 'Unknown')} ({gpu.get('memory_gb', 0):.1f}GB)\n"
                summary_content += f"    Compute Capability: {gpu.get('compute_capability', 'unknown')}\n"
                if 'current_usage' in gpu:
                    usage = gpu['current_usage']
                    summary_content += f"    Memory Usage: {usage.get('allocated_mb', 0):.0f}MB allocated ({usage.get('percent_allocated', 0):.1f}%)\n"
        
        summary_content += f"Working Directory: {sys_info.get('working_directory', 'unknown')}\n"
        summary_content += f"Log Directory: {sys_info.get('log_directory', 'unknown')}\n"
        summary_content += f"Model Directory: {sys_info.get('model_directory', 'unknown')}\n"
        summary_content += f"Config Directory: {sys_info.get('config_directory', 'unknown')}\n"
        summary_content += f"Report Directory: {sys_info.get('report_directory', 'unknown')}\n"
        summary_content += "\n"
        
        # Configuration Information
        summary_content += "CONFIGURATION\n"
        summary_content += "-" * 15 + "\n"
        summary_content += f"Preset Name: {config_info.get('preset_name', 'custom')}\n"
        summary_content += f"Validation Status: {config_info.get('validation_status', 'unknown')}\n"
        summary_content += f"Config File: {config_info.get('config_file', 'unknown')}\n"
        available_presets = config_info.get('available_presets', [])
        summary_content += f"Available Presets: {', '.join(available_presets) if available_presets else 'none'}\n"
        
        # Configuration parameters
        active_config = config_info.get('active_config', {})
        if isinstance(active_config, dict) and active_config:
            summary_content += "Key Configuration Parameters:\n"
            # Limit to first 20 for readability
            for key, value in list(active_config.items())[:20]:
                if not key.startswith('_'):
                    if isinstance(value, dict):
                        summary_content += f"  {key}: {len(value)} items\n"
                    elif isinstance(value, (list, tuple)):
                        summary_content += f"  {key}: [{len(value)} items]\n"
                    else:
                        str_value = str(value)
                        if len(str_value) > 60:
                            str_value = str_value[:57] + "..."
                        summary_content += f"  {key}: {str_value}\n"
            
            if len(active_config) > 20:
                summary_content += f"  ... and {len(active_config) - 20} more parameters\n"
        summary_content += "\n"
        
        # Model Variants Information
        summary_content += "MODEL VARIANTS\n"
        summary_content += "-" * 15 + "\n"
        summary_content += f"Available Variants: {model_info.get('variants_available', 0)}\n"
        
        variant_names = model_info.get('variant_names', [])
        if variant_names:
            summary_content += f"Variant Names: {', '.join(variant_names)}\n"
        else:
            summary_content += "Variant Names: None\n"
        
        # Variant status
        variant_status = model_info.get('variant_status', {})
        if variant_status:
            summary_content += "Variant Status Details:\n"
            for name, status in variant_status.items():
                status_indicator = "[OK]" if status == 'available' else "[MISSING]"
                summary_content += f"  {status_indicator} {name}: {status}\n"
        summary_content += "\n"
        
        # Performance Metrics
        summary_content += "PERFORMANCE BASELINE\n"
        summary_content += "-" * 22 + "\n"
        
        if performance:
            if 'baseline_failed' in performance:
                summary_content += f"Baseline establishment failed: {performance['baseline_failed']}\n"
            elif 'summary' in performance:
                summary = performance['summary']
                summary_content += f"Overall System Capability: {summary.get('overall_capability', 'unknown').upper()}\n"
                summary_content += f"CPU Performance: {summary.get('cpu_performance', 'unknown')}\n"
                summary_content += f"Memory Performance: {summary.get('memory_performance', 'unknown')}\n"
                summary_content += f"GPU Available: {summary.get('gpu_available', False)}\n"
                summary_content += f"I/O Performance: {summary.get('io_performance', 'unknown')}\n"
                
                # Detailed baseline metrics
                baselines = performance.get('baselines', {})
                if 'cpu' in baselines:
                    cpu_baseline = baselines['cpu']
                    summary_content += f"\nCPU Benchmark:\n"
                    summary_content += f"  Matrix Size: {cpu_baseline.get('matrix_size', 'unknown')}\n"
                    summary_content += f"  Computation Time: {cpu_baseline.get('computation_time', 0):.4f}s\n"
                    summary_content += f"  GFLOPS: {cpu_baseline.get('gflops', 0):.2f}\n"
                
                if 'gpu' in baselines:
                    gpu_baselines = baselines['gpu']
                    summary_content += f"\nGPU Benchmarks:\n"
                    for gpu_name, gpu_data in gpu_baselines.items():
                        if isinstance(gpu_data, dict) and 'gflops' in gpu_data:
                            summary_content += f"  {gpu_name}: {gpu_data.get('gflops', 0):.2f} GFLOPS\n"
            
            else:
                summary_content += "Performance Metrics:\n"
                for metric, value in performance.items():
                    if isinstance(value, (int, float)):
                        summary_content += f"  {metric}: {value:.4f}\n"
                    else:
                        summary_content += f"  {metric}: {value}\n"
        else:
            summary_content += "No performance metrics available\n"
        summary_content += "\n"
        
        # Dependencies Information
        summary_content += "DEPENDENCIES\n"
        summary_content += "-" * 14 + "\n"
        summary_content += f"PyTorch Version: {deps_info.get('torch_version', 'unknown')}\n"
        summary_content += f"Python Version: {deps_info.get('python_version', 'unknown')}\n"
        summary_content += f"Platform: {deps_info.get('platform', 'unknown')}\n"
        
        # Dependency status from check_versions
        if detailed_versions:
            summary_content += "\nCore Dependencies:\n"
            core_deps = {k: v for k, v in detailed_versions.items() if isinstance(v, dict) and v.get('required', False)}
            for name, info in core_deps.items():
                status_icon = "[OK]" if info.get('compatible', False) else "[ERROR]"
                summary_content += f"  {status_icon} {name}: {info.get('version', 'unknown')}"
                if info.get('required_version'):
                    summary_content += f" (requires {info['required_version']})"
                summary_content += f" - {info.get('status', 'unknown')}\n"
            
            optional_deps = {k: v for k, v in detailed_versions.items() if isinstance(v, dict) and not v.get('required', False)}
            if optional_deps:
                summary_content += "\nOptional Dependencies:\n"
                # Limit for readability
                for name, info in list(optional_deps.items())[:15]:
                    if info.get('available', False):
                        summary_content += f"  [OK] {name}: {info.get('version', 'Available')}\n"
                    else:
                        summary_content += f"  - {name}: Not Available\n"
                
                if len(optional_deps) > 15:
                    summary_content += f"  ... and {len(optional_deps) - 15} more optional dependencies\n"
        else:
            # Fallback to basic optional dependencies
            optional_deps = deps_info.get('optional_available', {})
            if optional_deps:
                summary_content += "Optional Dependencies:\n"
                for name, available in optional_deps.items():
                    status_indicator = "[OK]" if available else "[MISSING]"
                    summary_content += f"  {status_indicator} {name}: {'Available' if available else 'Not Available'}\n"
        summary_content += "\n"
        
        # Reproducibility Status
        summary_content += "REPRODUCIBILITY CONFIGURATION\n"
        summary_content += "-" * 32 + "\n"
        if reproducibility:
            summary_content += f"Seed Configuration: {'PASSED' if reproducibility.get('seed_config_passed', False) else 'FAILED'}\n"
            summary_content += f"Compliance Score: {reproducibility.get('compliance_score', 0):.1f}%\n"
            recommendations = reproducibility.get('recommendations', [])
            if recommendations:
                summary_content += "Recommendations:\n"
                # Limit to first 5
                for rec in recommendations[:5]:
                    summary_content += f"  - {rec}\n"
        else:
            summary_content += "Reproducibility check not performed\n"
        summary_content += "\n"
        
        # Logging Configuration Status
        summary_content += "LOGGING CONFIGURATION\n"
        summary_content += "-" * 22 + "\n"
        if logging_config:
            summary_content += f"Configuration Status: {'PASSED' if logging_config.get('passed', False) else 'FAILED'}\n"
            summary_content += f"Compliance Score: {logging_config.get('compliance_score', 0):.1f}%\n"
            summary_content += f"Active Handlers: {logging_config.get('handlers_count', 0)}\n"
            feedback = logging_config.get('feedback', [])
            if feedback:
                summary_content += "Configuration Issues:\n"
                for issue in feedback:
                    summary_content += f"  - {issue}\n"
        else:
            summary_content += "Logging configuration check not performed\n"
        summary_content += "\n"
        
        # Performance Monitoring Status
        summary_content += "PERFORMANCE MONITORING\n"
        summary_content += "-" * 23 + "\n"
        if perf_monitoring:
            summary_content += f"Monitoring Available: {'YES' if perf_monitoring.get('available', False) else 'NO'}\n"
            summary_content += f"Hardware Integration: {'YES' if perf_monitoring.get('hardware_integration', False) else 'NO'}\n"
            capabilities = perf_monitoring.get('capabilities', {})
            if capabilities:
                summary_content += "Capabilities:\n"
                for capability, enabled in capabilities.items():
                    status = "[OK]" if enabled else "[ERROR]"
                    summary_content += f"  {status} {capability.replace('_', ' ').title()}\n"
        else:
            summary_content += "Performance monitoring check not performed\n"
        summary_content += "\n"
        
        # Memory Management Status
        summary_content += "MEMORY MANAGEMENT\n"
        summary_content += "-" * 18 + "\n"
        if memory_mgmt:
            summary_content += f"Comprehensive Management: {'AVAILABLE' if memory_mgmt.get('comprehensive', False) else 'LIMITED'}\n"
            capabilities = memory_mgmt.get('capabilities', {})
            if capabilities:
                summary_content += "Capabilities:\n"
                for capability, enabled in capabilities.items():
                    status = "[OK]" if enabled else "[ERROR]"
                    summary_content += f"  {status} {capability.replace('_', ' ').title()}\n"
            
            test_results = memory_mgmt.get('test_results', {})
            if test_results.get('basic_test_passed'):
                summary_content += f"Memory Cleanup Test: PASSED\n"
                if 'cleanup_effectiveness' in test_results:
                    summary_content += f"Cleanup Effectiveness: {test_results['cleanup_effectiveness']:.1f}%\n"
        else:
            summary_content += "Memory management check not performed\n"
        summary_content += "\n"
        
        # System Architecture Details
        arch_info = detailed_hw.get('system_architecture', {})
        if arch_info.get('available'):
            summary_content += "SYSTEM ARCHITECTURE\n"
            summary_content += "-" * 20 + "\n"
            summary_content += f"Architecture: {arch_info.get('architecture', 'unknown')}\n"
            summary_content += f"Machine Type: {arch_info.get('machine', 'unknown')}\n"
            summary_content += f"System: {arch_info.get('system', 'unknown')} {arch_info.get('release', '')}\n"
            summary_content += f"Processor: {arch_info.get('processor', 'unknown')}\n"
            summary_content += f"Python Build: {arch_info.get('python_build', 'unknown')}\n"
            summary_content += "\n"
        
        # Footer with Diagnostic Summary
        summary_content += "=" * 80 + "\n"
        summary_content += "DIAGNOSTIC SUMMARY\n"
        summary_content += "=" * 80 + "\n"
        
        summary_content += f"Diagnostic Checks Performed: {len(checks_performed)}\n"
        summary_content += f"Checks Passed: {len(checks_passed)}\n"
        summary_content += f"System Health Score: {system_health_score:.1f}%\n"
        summary_content += f"\nPassed Checks: {', '.join(checks_passed) if checks_passed else 'None'}\n"
        
        failed_checks = [check for check in checks_performed if check not in checks_passed]
        if failed_checks:
            summary_content += f"Failed Checks: {', '.join(failed_checks)}\n"
        
        summary_content += f"\nReport Generated: {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')}\n"
        summary_content += "End of Report Entry\n"
        summary_content += "=" * 80 + "\n\n"
        
        # NOW write the complete content in a single operation
        with open(consolidated_files['summary'], 'a', encoding='utf-8') as f:
            f.write(summary_content)
        
        # 3. Save to consolidated status file (append as array element)
        status_entry = {
            **entry_metadata,
            'data': compact_status
        }
        
        if consolidated_files['status'].exists():
            try:
                with open(consolidated_files['status'], 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
                status_data['entries'].append(status_entry)
                status_data['metadata']['total_entries'] = len(status_data['entries'])
                status_data['metadata']['last_updated'] = timestamp_obj.isoformat()
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Failed to load status file, creating new: {e}")
                status_data = {
                    'date': date_str,
                    'entries': [status_entry],
                    'metadata': {
                        'version': '3.0',
                        'created_at': timestamp_obj.isoformat(),
                        'total_entries': 1,
                        'last_updated': timestamp_obj.isoformat(),
                        'report_type': 'consolidated_status'
                    }
                }
        else:
            status_data = {
                'date': date_str,
                'entries': [status_entry],
                'metadata': {
                    'version': '3.0',
                    'created_at': timestamp_obj.isoformat(),
                    'total_entries': 1,
                    'last_updated': timestamp_obj.isoformat(),
                    'report_type': 'consolidated_status'
                }
            }
        
        with open(consolidated_files['status'], 'w', encoding='utf-8') as f:
            json.dump(status_data, f, indent=2, default=str, ensure_ascii=False)
        
        # 4. Save to consolidated diagnostics file (append as array element)
        diagnostics_entry = {
            **entry_metadata,
            'data': diagnostic_data
        }
        
        if consolidated_files['diagnostics'].exists():
            try:
                with open(consolidated_files['diagnostics'], 'r', encoding='utf-8') as f:
                    diagnostics_data = json.load(f)
                diagnostics_data['entries'].append(diagnostics_entry)
                diagnostics_data['metadata']['total_entries'] = len(diagnostics_data['entries'])
                diagnostics_data['metadata']['last_updated'] = timestamp_obj.isoformat()
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Failed to load diagnostics file, creating new: {e}")
                diagnostics_data = {
                    'date': date_str,
                    'entries': [diagnostics_entry],
                    'metadata': {
                        'version': '3.0',
                        'created_at': timestamp_obj.isoformat(),
                        'total_entries': 1,
                        'last_updated': timestamp_obj.isoformat(),
                        'report_type': 'consolidated_diagnostics'
                    }
                }
        else:
            diagnostics_data = {
                'date': date_str,
                'entries': [diagnostics_entry],
                'metadata': {
                    'version': '3.0',
                    'created_at': timestamp_obj.isoformat(),
                    'total_entries': 1,
                    'last_updated': timestamp_obj.isoformat(),
                    'report_type': 'consolidated_diagnostics'
                }
            }
        
        with open(consolidated_files['diagnostics'], 'w', encoding='utf-8') as f:
            json.dump(diagnostics_data, f, indent=2, default=str, ensure_ascii=False)
        
        # 5. Save to consolidated dashboard JSON file (append as array element)
        dashboard_json_entry = {
            **entry_metadata,
            'data': dashboard_json_data
        }
        
        if consolidated_files['dashboard_json'].exists():
            try:
                with open(consolidated_files['dashboard_json'], 'r', encoding='utf-8') as f:
                    dashboard_json_file_data = json.load(f)
                dashboard_json_file_data['entries'].append(dashboard_json_entry)
                dashboard_json_file_data['metadata']['total_entries'] = len(dashboard_json_file_data['entries'])
                dashboard_json_file_data['metadata']['last_updated'] = timestamp_obj.isoformat()
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Failed to load dashboard JSON file, creating new: {e}")
                dashboard_json_file_data = {
                    'date': date_str,
                    'entries': [dashboard_json_entry],
                    'metadata': {
                        'version': '3.0',
                        'created_at': timestamp_obj.isoformat(),
                        'total_entries': 1,
                        'last_updated': timestamp_obj.isoformat(),
                        'report_type': 'consolidated_dashboard_data'
                    }
                }
        else:
            dashboard_json_file_data = {
                'date': date_str,
                'entries': [dashboard_json_entry],
                'metadata': {
                    'version': '3.0',
                    'created_at': timestamp_obj.isoformat(),
                    'total_entries': 1,
                    'last_updated': timestamp_obj.isoformat(),
                    'report_type': 'consolidated_dashboard_data'
                }
            }
        
        with open(consolidated_files['dashboard_json'], 'w', encoding='utf-8') as f:
            json.dump(dashboard_json_file_data, f, indent=2, default=str, ensure_ascii=False)
        
        # 6. Create/Update HTML dashboard file
        # Build GPU information section separately to avoid nested f-string issues
        gpu_info_html = ""
        if cuda_info.get('available'):
            gpu_info_html = f"""
            <div class="metric-grid">
                <div class="metric-card">
                    <div class="metric-title">CUDA Version</div>
                    <div class="metric-value">{cuda_info.get('cuda_version', 'unknown')}</div>
                </div>
                <div class="metric-card">
                    <div class="metric-title">cuDNN Version</div>
                    <div class="metric-value">{cuda_info.get('cudnn_version', 'unknown')}</div>
                </div>
                <div class="metric-card">
                    <div class="metric-title">GPU Count</div>
                    <div class="metric-value">{cuda_info.get('gpu_count', 0)}</div>
                </div>
            </div>
            """
            
            for i, gpu in enumerate(cuda_info.get('gpus', [])):
                memory_usage_html = ""
                if 'current_usage' in gpu:
                    usage = gpu['current_usage']
                    memory_usage_html = f"""
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {usage.get('percent_allocated', 0)}%" 
                            data-value="{usage.get('allocated_mb', 0):.0f}MB ({usage.get('percent_allocated', 0):.1f}%)"></div>
                    </div>
                    <div class="metric-description">
                        <span>Memory Usage:</span>
                        <span class="value">{usage.get('allocated_mb', 0):.0f}MB allocated ({usage.get('percent_allocated', 0):.1f}%)</span>
                    </div>
                    """
                
                gpu_info_html += f"""
                <div class="entry">
                    <h4>GPU {i}: {gpu.get('name', 'Unknown')}</h4>
                    <div class="metric-description">
                        <span>Memory:</span>
                        <span class="value">{gpu.get('memory_gb', 0):.1f}GB</span>
                    </div>
                    <div class="metric-description">
                        <span>Compute Capability:</span>
                        <span class="value">{gpu.get('compute_capability', 'unknown')}</span>
                    </div>
                    {memory_usage_html}
                </div>
                """
        else:
            gpu_info_html = """
            <div class="metric-card">
                <div class="metric-title">CUDA Status</div>
                <div class="metric-value status-error">Not Available</div>
                <div class="metric-description">CUDA is not available on this system</div>
            </div>
            """
        
        # Build performance section
        performance_html = """
        <div class="metric-card">
            <div class="metric-title">Performance Metrics</div>
            <div class="metric-value status-warning">Not Available</div>
            <div class="metric-description">No performance metrics collected</div>
        </div>
        """

        if performance:
            cpu_baseline_html = ""
            if 'cpu' in performance.get('baselines', {}):
                cpu_baseline = performance['baselines']['cpu']
                cpu_baseline_html = f"""
                <div class="entry">
                    <h4>CPU Benchmark</h4>
                    <div class="metric-description">
                        <span>Matrix Size:</span>
                        <span class="value">{cpu_baseline.get('matrix_size', 'unknown')}</span>
                    </div>
                    <div class="metric-description">
                        <span>Computation Time:</span>
                        <span class="value">{cpu_baseline.get('computation_time', 0):.4f}s</span>
                    </div>
                    <div class="metric-description">
                        <span>GFLOPS:</span>
                        <span class="value">{cpu_baseline.get('gflops', 0):.2f}</span>
                    </div>
                </div>
                """
            
            gpu_benchmarks_html = ""
            if 'gpu' in performance.get('baselines', {}):
                gpu_benchmarks = ""
                for gpu_name, gpu_data in performance['baselines']['gpu'].items():
                    if isinstance(gpu_data, dict) and 'gflops' in gpu_data:
                        gpu_benchmarks += f"""
                        <div class="metric-description">
                            <span>{gpu_name}:</span>
                            <span class="value">{gpu_data.get('gflops', 0):.2f} GFLOPS</span>
                        </div>
                        """
                
                if gpu_benchmarks:
                    gpu_benchmarks_html = f"""
                    <div class="entry">
                        <h4>GPU Benchmarks</h4>
                        {gpu_benchmarks}
                    </div>
                    """
            
            performance_html = f"""
            <div class="stats-grid">
                <div class="metric-card">
                    <h4>System Capability</h4>
                    <div class="metric-description">
                        <span>Overall:</span>
                        <span class="value {'status-success' if performance.get('summary', {}).get('overall_capability', '').lower() == 'high' else 'status-warning' if performance.get('summary', {}).get('overall_capability', '').lower() == 'medium' else 'status-error'}">
                            {performance.get('summary', {}).get('overall_capability', 'unknown').upper()}
                        </span>
                    </div>
                    <div class="metric-description">
                        <span>CPU:</span>
                        <span class="value">{performance.get('summary', {}).get('cpu_performance', 'unknown')}</span>
                    </div>
                    <div class="metric-description">
                        <span>Memory:</span>
                        <span class="value">{performance.get('summary', {}).get('memory_performance', 'unknown')}</span>
                    </div>
                    <div class="metric-description">
                        <span>I/O:</span>
                        <span class="value">{performance.get('summary', {}).get('io_performance', 'unknown')}</span>
                    </div>
                </div>
            </div>
            
            <h3>Benchmark Results</h3>
            {cpu_baseline_html}
            {gpu_benchmarks_html}
            """
        
        # Build dependencies tables
        core_deps_html = "<tr><td colspan='3'>No dependency information available</td></tr>"
        optional_deps_html = "<tr><td colspan='2'>No optional dependencies information available</td></tr>"

        if detailed_versions:
            core_deps_rows = []
            optional_deps_rows = []
            
            for name, info in detailed_versions.items():
                if info.get('required', False):
                    status_class = 'status-success' if info.get('compatible') else 'status-error'
                    status_text = '✅ Compatible' if info.get('compatible') else '❌ Incompatible'
                    core_deps_rows.append(f"""
                    <tr>
                        <td>{name}</td>
                        <td>{info.get('version', 'unknown')}</td>
                        <td><span class="{status_class}">{status_text}</span></td>
                    </tr>
                    """)
                else:
                    status_class = 'status-success' if info.get('available') else 'status-warning'
                    status_text = '✅ Available' if info.get('available') else '⚠️ Not Available'
                    optional_deps_rows.append(f"""
                    <tr>
                        <td>{name}</td>
                        <td><span class="{status_class}">{status_text}</span></td>
                    </tr>
                    """)
            
            if core_deps_rows:
                core_deps_html = "".join(core_deps_rows)
            if optional_deps_rows:
                optional_deps_html = "".join(optional_deps_rows)
        
        # Build quick actions section
        quick_actions_html = f"""
        <div class="analysis-section" style="background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%); border-left: 5px solid #ff9800;">
            <h2 style="color: #e65100;">Quick Actions</h2>
            <div class="metric-grid">
                <div class="metric-card" style="cursor: pointer;" onclick="showTab('history')">
                    <div class="metric-title">View Initialization History</div>
                    <div class="metric-value">{dashboard_json_file_data['metadata']['total_entries']}</div>
                    <div class="metric-description">Total initialization entries available</div>
                </div>
                
                <div class="metric-card" style="cursor: pointer;" onclick="showTab('performance')">
                    <div class="metric-title">Run Performance Test</div>
                    <div class="metric-value">{'Available' if performance else 'Setup Required'}</div>
                    <div class="metric-description">Benchmark system performance</div>
                </div>
                
                <div class="metric-card" style="cursor: pointer;" onclick="showTab('configuration')">
                    <div class="metric-title">Configuration Manager</div>
                    <div class="metric-value">{len(config_info.get('available_presets', []))}</div>
                    <div class="metric-description">Available presets</div>
                </div>
                
                <div class="metric-card" style="cursor: pointer;" onclick="showTab('dependencies')">
                    <div class="metric-title">Dependency Check</div>
                    <div class="metric-value">{len([d for d in detailed_versions.values() if d.get('required', False)]) if detailed_versions else 0}</div>
                    <div class="metric-description">Core dependencies</div>
                </div>
            </div>
            
            <div style="margin-top: 20px;">
                <h3 style="color: #e65100;">Recent Initializations</h3>
                <div class="metric-grid">
        """

        # Add recent initializations using REAL DATA from entries
        recent_entries = dashboard_json_file_data.get('entries', [])
        if len(recent_entries) > 1:
            # Get the last 4 entries (excluding current one if it's already there)
            display_entries = recent_entries[-4:-1] if len(recent_entries) > 4 else recent_entries[:-1] if len(recent_entries) > 1 else []
            display_entries.reverse()  # Show most recent first
            
            for i, entry in enumerate(display_entries):
                entry_data = entry.get('data', {})
                entry_time = entry.get('timestamp', 'Unknown time')
                
                # Parse time for display
                try:
                    from datetime import datetime
                    parsed_time = datetime.fromisoformat(entry_time.replace('Z', '+00:00'))
                    display_time = parsed_time.strftime('%H:%M:%S')
                except:
                    display_time = entry_time.split('T')[1][:8] if 'T' in entry_time else entry_time
                
                # Extract status and health data
                entry_status = entry_data.get('status', 'unknown')
                entry_health = entry_data.get('health_score', 0)
                entry_duration = entry_data.get('duration_seconds', 0)
                
                # Determine status styling
                if entry_status.lower() == 'success':
                    status_class = "history-status-success"
                    status_display = "SUCCESS"
                elif entry_status.lower() in ['warning', 'partial']:
                    status_class = "history-status-warning"
                    status_display = "WARNING"
                else:
                    status_class = "history-status-error"
                    status_display = "ERROR"
                
                quick_actions_html += f"""
                    <div class="metric-card" style="cursor: pointer;" onclick="viewInitializationDetails({len(recent_entries) - len(display_entries) + i})">
                        <div class="metric-title">Initialization at {display_time}</div>
                        <div class="metric-value {status_class}">{status_display}</div>
                        <div class="metric-description">
                            Health: {entry_health:.1f}% | Duration: {entry_duration:.2f}s
                        </div>
                        <div class="metric-description">
                            Models: {entry_data.get('model_variants', 0)} | 
                            CUDA: {'Available' if entry_data.get('cuda_available', False) else 'Not Available'}
                        </div>
                        <button class="details-button" style="margin-top: 10px; padding: 5px 10px; background: #2196f3; color: white; border: none; border-radius: 4px; cursor: pointer;">View Details</button>
                    </div>
                """
        else:
            quick_actions_html += """
                <div class="metric-card">
                    <div class="metric-title">No previous initializations</div>
                    <div class="metric-value">N/A</div>
                    <div class="metric-description">This is the first initialization today</div>
                </div>
            """

        quick_actions_html += """
                </div>
            </div>
        </div>
        """
        
        # Update the history tab content in the tab container section:
        history_tab_content = f"""
        <div id="history" class="tab-content">
            <h3>Initialization History</h3>
            
            <div class="metric-grid">
                <div class="metric-card">
                    <div class="metric-title">Total Entries</div>
                    <div class="metric-value">{dashboard_json_file_data['metadata']['total_entries']}</div>
                </div>
                
                <div class="metric-card">
                    <div class="metric-title">Successful Initializations</div>
                    <div class="metric-value">{
                        len([e for e in dashboard_json_file_data.get('entries', []) 
                            if e.get('data', {}).get('status', '').lower() == 'success'])
                    }</div>
                </div>
                
                <div class="metric-card">
                    <div class="metric-title">Average Health Score</div>
                    <div class="metric-value">{
                        sum(e.get('data', {}).get('health_score', 0) for e in dashboard_json_file_data.get('entries', [])) / 
                        len(dashboard_json_file_data.get('entries', [1])) 
                        if dashboard_json_file_data.get('entries') else system_health_score
                    :.1f}%</div>
                </div>
                
                <div class="metric-card">
                    <div class="metric-title">Average Duration</div>
                    <div class="metric-value">{
                        sum(e.get('data', {}).get('duration_seconds', 0) for e in dashboard_json_file_data.get('entries', [])) / 
                        len(dashboard_json_file_data.get('entries', [1])) 
                        if dashboard_json_file_data.get('entries') else compact_status['duration_seconds']
                    :.2f}s</div>
                </div>
            </div>
            
            <div class="history-viewer">
                <div class="history-header">
                    Initialization History - Sorted by Most Recent ({len(dashboard_json_file_data.get('entries', []))} entries)
                    <button class="details-button" style="float: right;" onclick="exportDashboardData()">Export Data</button>
                </div>
                <div class="history-content">
        """
        
        # Add actual history entries
        all_entries = dashboard_json_file_data.get('entries', [])
        if all_entries:
            # Reverse to show most recent first
            for i, entry in enumerate(reversed(all_entries)):
                entry_data = entry.get('data', {})
                entry_timestamp = entry.get('timestamp', 'Unknown time')
                
                # Parse timestamp for better display
                try:
                    parsed_time = datetime.fromisoformat(entry_timestamp.replace('Z', '+00:00'))
                    display_timestamp = parsed_time.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    display_timestamp = entry_timestamp
                
                # Extract entry details
                entry_status = entry_data.get('status', 'unknown').lower()
                entry_health = entry_data.get('health_score', 0)
                entry_duration = entry_data.get('duration_seconds', 0)
                entry_models = entry_data.get('model_variants', 0)
                entry_cuda = entry_data.get('cuda_available', False)
                entry_repro_score = entry_data.get('reproducibility_score', 0)
                entry_logging_score = entry_data.get('logging_compliance', 0)
                entry_perf_monitor = entry_data.get('performance_monitoring', False)
                entry_mem_mgmt = entry_data.get('memory_management', False)
                
                # Determine status styling
                if entry_status == 'success':
                    status_class = 'history-status-success'
                    status_display = 'SUCCESS'
                elif entry_status in ['warning', 'partial']:
                    status_class = 'history-status-warning'
                    status_display = 'WARNING'
                else:
                    status_class = 'history-status-error'
                    status_display = 'ERROR'
                
                history_tab_content += f"""
                        <div class="history-item" onclick="viewInitializationDetails({len(all_entries) - 1 - i})" style="cursor: pointer; transition: background-color 0.2s;">
                            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 8px;">
                                <span class="history-time" style="font-weight: bold; color: #1976d2; font-size: 1.1em;">{display_timestamp}</span>
                                <span class="history-status {status_class}" style="padding: 4px 12px; border-radius: 6px; font-weight: bold; font-size: 0.85em;">{status_display}</span>
                            </div>
                            
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-top: 10px;">
                                <div>
                                    <div style="margin-bottom: 4px;"><strong>Performance:</strong></div>
                                    <div style="font-size: 0.9em; color: #666;">
                                        Health: <span style="font-weight: 600; color: {'#4CAF50' if entry_health > 80 else '#FF9800' if entry_health > 60 else '#F44336'};">{entry_health:.1f}%</span> | 
                                        Duration: <span style="font-weight: 600;">{entry_duration:.2f}s</span>
                                    </div>
                                    <div style="font-size: 0.9em; color: #666;">
                                        Models: <span style="font-weight: 600;">{entry_models}</span> | 
                                        CUDA: <span style="font-weight: 600; color: {'#4CAF50' if entry_cuda else '#F44336'};">{'Available' if entry_cuda else 'Not Available'}</span>
                                    </div>
                                </div>
                                
                                <div>
                                    <div style="margin-bottom: 4px;"><strong>Configuration Scores:</strong></div>
                                    <div style="font-size: 0.9em; color: #666;">
                                        Reproducibility: <span style="font-weight: 600;">{entry_repro_score:.1f}%</span> | 
                                        Logging: <span style="font-weight: 600;">{entry_logging_score:.1f}%</span>
                                    </div>
                                    <div style="font-size: 0.9em; color: #666;">
                                        Perf Monitor: <span style="font-weight: 600; color: {'#4CAF50' if entry_perf_monitor else '#F44336'};">{'Yes' if entry_perf_monitor else 'No'}</span> | 
                                        Memory Mgmt: <span style="font-weight: 600; color: {'#4CAF50' if entry_mem_mgmt else '#FF9800'};">{'Full' if entry_mem_mgmt else 'Basic'}</span>
                                    </div>
                                </div>
                            </div>
                            
                            <div style="margin-top: 10px; padding-top: 8px; border-top: 1px solid #eee;">
                                <button class="details-button" style="padding: 5px 10px; background: #2196f3; color: white; border: none; border-radius: 4px; font-size: 0.85em; cursor: pointer;">View Full Details</button>
                            </div>
                        </div>
                """
        else:
            history_tab_content += """
                        <div class="history-item">
                            <div style="text-align: center; color: #666; padding: 20px;">
                                No initialization history available. Previous entries will appear here.
                            </div>
                        </div>
            """
        
        history_tab_content += """
                </div>
            </div>
            
            <div style="margin-top: 20px;">
                <h4>Initialization Trends & Statistics</h4>
                <div class="metric-grid">
        """
        
        # Calculate real statistics from the data
        if all_entries:
            success_rate = len([e for e in all_entries if e.get('data', {}).get('status', '').lower() == 'success']) / len(all_entries) * 100
            avg_duration = sum(e.get('data', {}).get('duration_seconds', 0) for e in all_entries) / len(all_entries)
            avg_health = sum(e.get('data', {}).get('health_score', 0) for e in all_entries) / len(all_entries)
            avg_models = sum(e.get('data', {}).get('model_variants', 0) for e in all_entries) / len(all_entries)
            
            # Calculate trends (compare recent vs older entries)
            if len(all_entries) >= 2:
                recent_half = all_entries[len(all_entries)//2:]
                older_half = all_entries[:len(all_entries)//2]
                
                recent_avg_health = sum(e.get('data', {}).get('health_score', 0) for e in recent_half) / len(recent_half)
                older_avg_health = sum(e.get('data', {}).get('health_score', 0) for e in older_half) / len(older_half)
                health_trend = recent_avg_health - older_avg_health
                
                recent_avg_duration = sum(e.get('data', {}).get('duration_seconds', 0) for e in recent_half) / len(recent_half)
                older_avg_duration = sum(e.get('data', {}).get('duration_seconds', 0) for e in older_half) / len(older_half)
                duration_trend = recent_avg_duration - older_avg_duration
            else:
                health_trend = 0
                duration_trend = 0
                
            history_tab_content += f"""
                    <div class="metric-card">
                        <div class="metric-title">Success Rate</div>
                        <div class="metric-value" style="color: {'#4CAF50' if success_rate > 80 else '#FF9800' if success_rate > 60 else '#F44336'};">{success_rate:.1f}%</div>
                        <div class="metric-description">{len([e for e in all_entries if e.get('data', {}).get('status', '').lower() == 'success'])} out of {len(all_entries)} successful</div>
                    </div>
                    
                    <div class="metric-card">
                        <div class="metric-title">Avg. Duration</div>
                        <div class="metric-value">{avg_duration:.2f}s</div>
                        <div class="metric-description">
                            Trend: <span style="color: {'#4CAF50' if duration_trend < 0 else '#F44336' if duration_trend > 0 else '#666'};">
                                {'↓ Faster' if duration_trend < -0.1 else '↑ Slower' if duration_trend > 0.1 else '→ Stable'}
                            </span>
                        </div>
                    </div>
                    
                    <div class="metric-card">
                        <div class="metric-title">Avg. Health Score</div>
                        <div class="metric-value" style="color: {'#4CAF50' if avg_health > 80 else '#FF9800' if avg_health > 60 else '#F44336'};">{avg_health:.1f}%</div>
                        <div class="metric-description">
                            Trend: <span style="color: {'#4CAF50' if health_trend > 0 else '#F44336' if health_trend < 0 else '#666'};">
                                {'↑ Improving' if health_trend > 1 else '↓ Declining' if health_trend < -1 else '→ Stable'}
                            </span>
                        </div>
                    </div>
                    
                    <div class="metric-card">
                        <div class="metric-title">Avg. Models</div>
                        <div class="metric-value">{avg_models:.1f}</div>
                        <div class="metric-description">Average model variants per initialization</div>
                    </div>
            """
        else:
            history_tab_content += f"""
                    <div class="metric-card">
                        <div class="metric-title">Success Rate</div>
                        <div class="metric-value">100%</div>
                        <div class="metric-description">Current initialization only</div>
                    </div>
                    
                    <div class="metric-card">
                        <div class="metric-title">Avg. Duration</div>
                        <div class="metric-value">{compact_status['duration_seconds']:.2f}s</div>
                        <div class="metric-description">Current initialization only</div>
                    </div>
                    
                    <div class="metric-card">
                        <div class="metric-title">Health Score</div>
                        <div class="metric-value">{system_health_score:.1f}%</div>
                        <div class="metric-description">Current initialization only</div>
                    </div>
                    
                    <div class="metric-card">
                        <div class="metric-title">Model Variants</div>
                        <div class="metric-value">{compact_status['model_variants']}</div>
                        <div class="metric-description">Current initialization only</div>
                    </div>
            """
        
        history_tab_content += """
                </div>
            </div>
        </div>
        """
        
        # Build full HTML content
        html_content = f"""<!DOCTYPE html>
<html>
<head>
    <title>Deep Learning System Dashboard - {date_str}</title>
    <style>
        body {{ 
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
            margin: 0; 
            padding: 20px; 
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            color: #333;
            line-height: 1.6;
        }}
        
        .container {{ 
            max-width: 1400px; 
            margin: 0 auto; 
            background: white; 
            padding: 30px; 
            border-radius: 12px; 
            box-shadow: 0 15px 35px rgba(0,0,0,0.1);
        }}
        
        .header {{ 
            text-align: center; 
            margin-bottom: 40px; 
            padding-bottom: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            margin: -30px -30px 40px -30px;
            padding: 40px 30px 20px 30px;
            border-radius: 12px 12px 0 0;
            position: relative;
            overflow: hidden;
        }}
        
        .header::before {{
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" preserveAspectRatio="none"><path d="M0,0 L100,0 L100,100 Z" fill="rgba(255,255,255,0.05)"/></svg>');
            background-size: cover;
        }}
        
        .header h1 {{ 
            margin: 0; 
            font-size: 2.8em; 
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            position: relative;
            z-index: 1;
        }}
        
        .header .subtitle {{
            font-size: 1.2em;
            margin-top: 10px;
            opacity: 0.9;
            position: relative;
            z-index: 1;
        }}
        
        .header-stats {{
            display: flex;
            justify-content: center;
            gap: 30px;
            margin-top: 20px;
            flex-wrap: wrap;
        }}
        
        .header-stat {{
            background: rgba(255, 255, 255, 0.15);
            padding: 10px 20px;
            border-radius: 8px;
            backdrop-filter: blur(5px);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }}
        
        .header-stat .number {{
            font-size: 1.4em;
            font-weight: bold;
        }}
        
        .header-stat .label {{
            font-size: 0.9em;
            opacity: 0.8;
        }}
        
        .analysis-section {{ 
            background: linear-gradient(135deg, #e3f2fd 0%, #f0f8ff 100%); 
            border-left: 5px solid #2196f3; 
            padding: 25px; 
            margin: 25px 0; 
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.08);
        }}
        
        .analysis-section h2 {{
            margin-top: 0;
            color: #1976d2;
            font-size: 1.8em;
            display: flex;
            align-items: center;
            gap: 10px;
        }}
        
        .analysis-section h2::before {{
            content: '📊';
            font-size: 1.2em;
        }}
        
        .health-score {{ 
            font-size: 3.5em; 
            font-weight: bold; 
            text-align: center; 
            padding: 40px; 
            border-radius: 15px; 
            margin: 25px 0;
            color: white;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            position: relative;
            overflow: hidden;
        }}
        
        .health-score.excellent {{ 
            background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%);
        }}
        
        .health-score.good {{ 
            background: linear-gradient(135deg, #8BC34A 0%, #7CB342 100%);
        }}
        
        .health-score.fair {{ 
            background: linear-gradient(135deg, #FF9800 0%, #F57C00 100%);
        }}
        
        .health-score.poor {{ 
            background: linear-gradient(135deg, #F44336 0%, #D32F2F 100%);
        }}
        
        .health-score .sub-score {{
            font-size: 0.4em;
            margin-top: 10px;
            opacity: 0.9;
        }}
        
        .metric-grid {{ 
            display: grid; 
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); 
            gap: 20px; 
            margin: 25px 0; 
        }}
        
        .metric-card {{ 
            background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
            padding: 25px; 
            border-radius: 10px; 
            border-left: 5px solid #2196f3;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(0,0,0,0.08);
            position: relative;
            overflow: hidden;
        }}
        
        .metric-card::before {{
            content: '';
            position: absolute;
            top: 0;
            right: 0;
            width: 30px;
            height: 30px;
            background: linear-gradient(135deg, transparent 50%, #2196f3 50%);
        }}
        
        .metric-card:hover {{ 
            transform: translateY(-5px); 
            box-shadow: 0 8px 25px rgba(0,0,0,0.15);
            border-left-color: #1976d2;
        }}
        
        .metric-title {{ 
            font-weight: 600; 
            color: #2196f3; 
            margin-bottom: 12px; 
            font-size: 1.1em;
            display: flex;
            align-items: center;
            gap: 8px;
        }}
        
        .metric-card:nth-child(1) .metric-title::before {{ content: '🚀'; }}
        .metric-card:nth-child(2) .metric-title::before {{ content: '💻'; }}
        .metric-card:nth-child(3) .metric-title::before {{ content: '⚙️'; }}
        .metric-card:nth-child(4) .metric-title::before {{ content: '📈'; }}
        
        .metric-value {{ 
            font-size: 2.2em; 
            font-weight: bold; 
            color: #333; 
            margin-bottom: 8px;
        }}
        
        .metric-description {{
            font-size: 0.9em;
            color: #666;
            margin-top: 8px;
            display: flex;
            justify-content: space-between;
        }}
        
        .metric-description .value {{
            font-weight: 600;
            color: #2196f3;
        }}
        
        .progress-bar {{ 
            width: 100%; 
            height: 24px; 
            background: #e0e0e0; 
            border-radius: 12px; 
            overflow: hidden;
            margin: 12px 0;
            position: relative;
        }}
        
        .progress-fill {{ 
            height: 100%; 
            background: linear-gradient(90deg, #4CAF50, #8BC34A); 
            border-radius: 12px; 
            transition: width 0.5s ease;
            position: relative;
        }}
        
        .progress-fill::after {{
            content: attr(data-value);
            position: absolute;
            right: 10px;
            top: 50%;
            transform: translateY(-50%);
            color: white;
            font-size: 0.8em;
            font-weight: bold;
            text-shadow: 1px 1px 2px rgba(0,0,0,0.5);
        }}
        
        .status-indicator {{
            display: inline-block;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 600;
            margin-left: 8px;
        }}
        
        .status-success {{
            background: #e8f5e9;
            color: #2e7d32;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 600;
        }}
        
        .status-warning {{
            background: #fff3e0;
            color: #f57c00;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 600;
        }}
        
        .status-error {{
            background: #ffebee;
            color: #c62828;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 600;
        }}
        
        .entry {{ 
            background: #fff; 
            border: 1px solid #e0e0e0; 
            padding: 20px; 
            border-radius: 8px; 
            box-shadow: 0 3px 10px rgba(0,0,0,0.08);
            transition: all 0.2s ease;
            margin: 15px 0;
        }}
        
        .entry:hover {{
            box-shadow: 0 5px 20px rgba(0,0,0,0.15);
            transform: translateY(-2px);
        }}
        
        .entry h4 {{
            color: #1976d2;
            margin-top: 0;
            font-size: 1.3em;
            border-bottom: 2px solid #e3f2fd;
            padding-bottom: 10px;
        }}
        
        .tab-container {{ 
            margin: 40px 0; 
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
        }}
        
        .tab-buttons {{ 
            display: flex; 
            background: linear-gradient(135deg, #2196f3 0%, #1976d2 100%);
            flex-wrap: wrap;
        }}
        
        .tab-button {{ 
            flex: 1; 
            min-width: 140px;
            padding: 18px 12px; 
            background: transparent;
            border: none; 
            cursor: pointer; 
            font-size: 16px; 
            font-weight: 600;
            transition: all 0.3s ease;
            color: rgba(255,255,255,0.8);
            border-right: 1px solid rgba(255,255,255,0.2);
        }}
        
        .tab-button:last-child {{
            border-right: none;
        }}
        
        .tab-button:hover {{
            background: rgba(255,255,255,0.1);
            color: white;
        }}
        
        .tab-button.active {{ 
            background: rgba(255,255,255,0.2);
            color: white;
            box-shadow: inset 0 -3px 0 #fff;
        }}
        
        .tab-content {{ 
            display: none; 
            padding: 30px; 
            background: white; 
            min-height: 400px;
        }}
        
        .tab-content.active {{ 
            display: block; 
            animation: fadeIn 0.3s ease;
        }}
        
        @keyframes fadeIn {{
            from {{ opacity: 0; transform: translateY(10px); }}
            to {{ opacity: 1; transform: translateY(0); }}
        }}
        
        .tab-content h3 {{
            color: #1976d2;
            font-size: 1.8em;
            margin-top: 0;
            margin-bottom: 25px;
            border-bottom: 2px solid #e3f2fd;
            padding-bottom: 10px;
        }}
        
        table {{ 
            width: 100%; 
            border-collapse: collapse; 
            margin: 25px 0; 
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 15px rgba(0,0,0,0.08);
        }}
        
        th, td {{ 
            padding: 15px 12px; 
            text-align: left; 
            border-bottom: 1px solid #e0e0e0; 
        }}
        
        th {{ 
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            font-weight: 600; 
            color: #1976d2; 
            font-size: 1.05em;
        }}
        
        tr:hover {{ 
            background: #f5f5f5; 
        }}
        
        tr:last-child td {{
            border-bottom: none;
        }}
        
        .footer {{ 
            text-align: center; 
            margin-top: 50px; 
            padding: 30px 0; 
            border-top: 2px solid #e0e0e0; 
            color: #666; 
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            margin-left: -30px;
            margin-right: -30px;
            margin-bottom: -30px;
            border-radius: 0 0 12px 12px;
        }}
        
        .collapsible {{
            display: none;
        }}
        
        .toggle {{
            cursor: pointer;
            color: #2196f3;
            font-weight: bold;
        }}
        
        /* Add these new styles for the history viewer */
        .history-viewer {{
            margin-top: 20px;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            overflow: hidden;
        }}
        
        .history-header {{
            background: #f5f5f5;
            padding: 15px;
            font-weight: bold;
            border-bottom: 1px solid #e0e0e0;
        }}
        
        .history-content {{
            padding: 15px;
            max-height: 300px;
            overflow-y: auto;
        }}
        
        .history-item {{
            padding: 10px;
            border-bottom: 1px solid #eee;
            cursor: pointer;
        }}
        
        .history-item:hover {{
            background: #f9f9f9;
        }}
        
        .history-item:last-child {{
            border-bottom: none;
        }}
        
        .history-time {{
            font-weight: bold;
            color: #1976d2;
        }}
        
        .history-status {{
            display: inline-block;
            padding: 3px 8px;
            border-radius: 4px;
            font-size: 0.8em;
            margin-left: 10px;
        }}
        
        .history-status-success {{
            background: #e8f5e9;
            color: #2e7d32;
        }}
        
        .history-status-warning {{
            background: #fff3e0;
            color: #f57c00;
        }}
        
        .history-status-error {{
            background: #ffebee;
            color: #c62828;
        }}
    </style>
    <script>
        // Store the actual entries data for JavaScript access
        const dashboardData = {json.dumps(dashboard_json_file_data, default=str, indent=2)};
        const currentSystemStatus = {json.dumps(compact_status, default=str, indent=2)};
        
        function showTab(tabName) {{
            var contents = document.querySelectorAll('.tab-content');
            contents.forEach(function(content) {{
                content.classList.remove('active');
            }});
            
            var buttons = document.querySelectorAll('.tab-button');
            buttons.forEach(function(button) {{
                button.classList.remove('active');
            }});
            
            document.getElementById(tabName).classList.add('active');
            event.target.classList.add('active');
        }}
        
        function toggleSection(sectionId) {{
            const section = document.getElementById(sectionId);
            section.style.display = section.style.display === 'none' ? 'block' : 'none';
        }}
        
        document.addEventListener('DOMContentLoaded', function() {{
            var firstTab = document.querySelector('.tab-button');
            var firstContent = document.querySelector('.tab-content');
            if (firstTab && firstContent) {{
                firstTab.classList.add('active');
                firstContent.classList.add('active');
            }}
        }});
        
        // Enhanced function to view initialization details with REAL DATA
        function viewInitializationDetails(entryIndex) {{
            const entries = dashboardData.entries || [];
            if (entryIndex >= 0 && entryIndex < entries.length) {{
                const entry = entries[entryIndex];
                const entryData = entry.data || {{}};
                
                // Create detailed modal content
                const modalContent = `
                    <div style="max-width: 800px; max-height: 80vh; overflow-y: auto; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 10px 30px rgba(0,0,0,0.3);">
                        <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 20px; padding-bottom: 10px; border-bottom: 2px solid #2196f3;">
                            <h2 style="margin: 0; color: #1976d2;">Initialization Details</h2>
                            <button onclick="closeModal()" style="background: #f44336; color: white; border: none; padding: 8px 15px; border-radius: 4px; cursor: pointer;">Close</button>
                        </div>
                        
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 20px;">
                            <div>
                                <h3 style="color: #1976d2; margin-bottom: 10px;">Basic Information</h3>
                                <p><strong>Timestamp:</strong> ${{entry.timestamp || 'Unknown'}}</p>
                                <p><strong>Status:</strong> <span style="color: ${{entryData.status === 'success' ? '#4CAF50' : '#F44336'}};">${{entryData.status?.toUpperCase() || 'UNKNOWN'}}</span></p>
                                <p><strong>Duration:</strong> ${{entryData.duration_seconds || 0}} seconds</p>
                                <p><strong>Health Score:</strong> <span style="color: ${{entryData.health_score > 80 ? '#4CAF50' : entryData.health_score > 60 ? '#FF9800' : '#F44336'}};">${{entryData.health_score || 0}}%</span></p>
                            </div>
                            
                            <div>
                                <h3 style="color: #1976d2; margin-bottom: 10px;">System Resources</h3>
                                <p><strong>CUDA Available:</strong> <span style="color: ${{entryData.cuda_available ? '#4CAF50' : '#F44336'}};">${{entryData.cuda_available ? 'Yes' : 'No'}}</span></p>
                                <p><strong>Model Variants:</strong> ${{entryData.model_variants || 0}}</p>
                                <p><strong>Reproducibility Score:</strong> ${{entryData.reproducibility_score || 0}}%</p>
                                <p><strong>Logging Compliance:</strong> ${{entryData.logging_compliance || 0}}%</p>
                            </div>
                        </div>
                        
                        <div>
                            <h3 style="color: #1976d2; margin-bottom: 10px;">Advanced Features</h3>
                            <p><strong>Performance Monitoring:</strong> <span style="color: ${{entryData.performance_monitoring ? '#4CAF50' : '#F44336'}};">${{entryData.performance_monitoring ? 'Available' : 'Not Available'}}</span></p>
                            <p><strong>Memory Management:</strong> <span style="color: ${{entryData.memory_management ? '#4CAF50' : '#FF9800'}};">${{entryData.memory_management ? 'Comprehensive' : 'Basic'}}</span></p>
                        </div>
                        
                        <div style="margin-top: 20px; padding-top: 15px; border-top: 1px solid #eee;">
                            <button onclick="exportEntryData(${{entryIndex}})" style="background: #2196f3; color: white; border: none; padding: 10px 20px; border-radius: 4px; cursor: pointer; margin-right: 10px;">Export This Entry</button>
                            <button onclick="compareWithCurrent(${{entryIndex}})" style="background: #FF9800; color: white; border: none; padding: 10px 20px; border-radius: 4px; cursor: pointer;">Compare with Current</button>
                        </div>
                    </div>
                `;
                
                // Create modal overlay
                const modalOverlay = document.createElement('div');
                modalOverlay.id = 'detailModal';
                modalOverlay.style.cssText = 'position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0,0,0,0.5); display: flex; justify-content: center; align-items: center; z-index: 1000;';
                modalOverlay.innerHTML = modalContent;
                
                document.body.appendChild(modalOverlay);
            }} else {{
                alert('Invalid entry index: ' + entryIndex);
            }}
        }}
        
        function closeModal() {{
            const modal = document.getElementById('detailModal');
            if (modal) {{
                document.body.removeChild(modal);
            }}
        }}
        
        function exportEntryData(entryIndex) {{
            const entries = dashboardData.entries || [];
            if (entryIndex >= 0 && entryIndex < entries.length) {{
                const entry = entries[entryIndex];
                const dataStr = "data:text/json;charset=utf-8," + encodeURIComponent(JSON.stringify(entry, null, 2));
                
                const downloadAnchorNode = document.createElement('a');
                downloadAnchorNode.setAttribute("href", dataStr);
                downloadAnchorNode.setAttribute("download", "initialization_entry_" + entryIndex + "_" + new Date().toISOString().split('T')[0] + ".json");
                document.body.appendChild(downloadAnchorNode);
                downloadAnchorNode.click();
                downloadAnchorNode.remove();
                
                alert('Entry data exported successfully!');
            }}
        }}
        
        function compareWithCurrent(entryIndex) {{
            const entries = dashboardData.entries || [];
            if (entryIndex >= 0 && entryIndex < entries.length) {{
                const historicalEntry = entries[entryIndex].data || {{}};
                const currentEntry = currentSystemStatus;
                
                const comparison = `
                    <div style="max-width: 900px; max-height: 80vh; overflow-y: auto; padding: 20px; background: white; border-radius: 8px;">
                        <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 20px; padding-bottom: 10px; border-bottom: 2px solid #2196f3;">
                            <h2 style="margin: 0; color: #1976d2;">Comparison with Current Initialization</h2>
                            <button onclick="closeModal()" style="background: #f44336; color: white; border: none; padding: 8px 15px; border-radius: 4px; cursor: pointer;">Close</button>
                        </div>
                        
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div>
                                <h3 style="color: #1976d2;">Historical Entry</h3>
                                <p><strong>Health Score:</strong> ${{historicalEntry.health_score || 0}}%</p>
                                <p><strong>Duration:</strong> ${{historicalEntry.duration_seconds || 0}}s</p>
                                <p><strong>Model Variants:</strong> ${{historicalEntry.model_variants || 0}}</p>
                                <p><strong>CUDA:</strong> ${{historicalEntry.cuda_available ? 'Available' : 'Not Available'}}</p>
                                <p><strong>Reproducibility:</strong> ${{historicalEntry.reproducibility_score || 0}}%</p>
                                <p><strong>Logging Compliance:</strong> ${{historicalEntry.logging_compliance || 0}}%</p>
                            </div>
                            
                            <div>
                                <h3 style="color: #1976d2;">Current Initialization</h3>
                                <p><strong>Health Score:</strong> ${{currentEntry.system_health_score || 0}}%</p>
                                <p><strong>Duration:</strong> ${{currentEntry.duration_seconds || 0}}s</p>
                                <p><strong>Model Variants:</strong> ${{currentEntry.model_variants || 0}}</p>
                                <p><strong>CUDA:</strong> ${{currentEntry.cuda_available ? 'Available' : 'Not Available'}}</p>
                                <p><strong>Reproducibility:</strong> ${{currentEntry.reproducibility_score || 0}}%</p>
                                <p><strong>Logging Compliance:</strong> ${{currentEntry.logging_compliance || 0}}%</p>
                            </div>
                        </div>
                        
                        <div style="margin-top: 20px; padding-top: 15px; border-top: 1px solid #eee;">
                            <h3 style="color: #1976d2;">Changes</h3>
                            <p><strong>Health Score:</strong> <span style="color: ${{(currentEntry.system_health_score || 0) > (historicalEntry.health_score || 0) ? '#4CAF50' : '#F44336'}};">${{((currentEntry.system_health_score || 0) - (historicalEntry.health_score || 0) > 0 ? '+' : '') + ((currentEntry.system_health_score || 0) - (historicalEntry.health_score || 0)).toFixed(1)}}%</span></p>
                            <p><strong>Duration:</strong> <span style="color: ${{(currentEntry.duration_seconds || 0) < (historicalEntry.duration_seconds || 0) ? '#4CAF50' : '#F44336'}};">${{((currentEntry.duration_seconds || 0) - (historicalEntry.duration_seconds || 0) > 0 ? '+' : '') + ((currentEntry.duration_seconds || 0) - (historicalEntry.duration_seconds || 0)).toFixed(2)}}s</span></p>
                            <p><strong>Model Variants:</strong> <span style="color: ${{(currentEntry.model_variants || 0) >= (historicalEntry.model_variants || 0) ? '#4CAF50' : '#F44336'}};">${{((currentEntry.model_variants || 0) - (historicalEntry.model_variants || 0) > 0 ? '+' : '') + ((currentEntry.model_variants || 0) - (historicalEntry.model_variants || 0))}}</span></p>
                        </div>
                    </div>
                `;
                
                const modalOverlay = document.createElement('div');
                modalOverlay.id = 'detailModal';
                modalOverlay.style.cssText = 'position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0,0,0,0.5); display: flex; justify-content: center; align-items: center; z-index: 1000;';
                modalOverlay.innerHTML = comparison;
                
                document.body.appendChild(modalOverlay);
            }}
        }}
        
        // Enhanced function to export current dashboard data with REAL DATA
        function exportDashboardData() {{
            const exportData = {{
                generated_at: new Date().toISOString(),
                dashboard_metadata: dashboardData.metadata,
                current_status: currentSystemStatus,
                all_entries: dashboardData.entries,
                export_type: 'complete_dashboard_export',
                export_version: '1.0'
            }};
            
            const dataStr = "data:text/json;charset=utf-8," + encodeURIComponent(JSON.stringify(exportData, null, 2));
            
            const downloadAnchorNode = document.createElement('a');
            downloadAnchorNode.setAttribute("href", dataStr);
            downloadAnchorNode.setAttribute("download", "complete_dashboard_export_" + new Date().toISOString().replace(/[:.]/g, '-') + ".json");
            document.body.appendChild(downloadAnchorNode);
            downloadAnchorNode.click();
            downloadAnchorNode.remove();
            
            alert('Complete dashboard data exported successfully!');
        }}
        
        // Close modal when clicking outside
        document.addEventListener('click', function(event) {{
            if (event.target.id === 'detailModal') {{
                closeModal();
            }}
        }});
    </script>
</head>
<body>
    <div class="container">
        <!-- Enhanced Header Section -->
        <div class="header">
            <h1>Deep Learning System Dashboard</h1>
            <div class="subtitle">
                Comprehensive system status and performance monitoring
            </div>
            <div class="header-stats">
                <div class="header-stat">
                    <div class="number">{dashboard_json_file_data['metadata']['total_entries']}</div>
                    <div class="label">Total Initializations</div>
                </div>
                <div class="header-stat">
                    <div class="number">{time_str}</div>
                    <div class="label">Current Sequence</div>
                </div>
                <div class="header-stat">
                    <div class="number">{date_str}</div>
                    <div class="label">Session Date</div>
                </div>
            </div>
        </div>
        
        <!-- QUICK ACTIONS SECTION -->
        {quick_actions_html}

        <!-- Enhanced Current System Status Section -->
        <div class="analysis-section">
            <h2>Current System Status</h2>
            
            <!-- Health Score with visual indicator -->
            <div class="health-score {'excellent' if system_health_score > 80 else 'good' if system_health_score > 60 else 'fair' if system_health_score > 40 else 'poor'}">
                {system_health_score:.1f}
                <div class="sub-score">System Health Score</div>
            </div>
            
            <div class="metric-grid">
                <!-- Initialization Card -->
                <div class="metric-card">
                    <div class="metric-title">Initialization</div>
                    <div class="metric-value {'status-success' if compact_status['status'] == 'success' else 'status-warning' if compact_status['status'] == 'warning' else 'status-error'}">
                        {compact_status['status'].upper()}
                    </div>
                    <div class="metric-description">
                        <span>Duration:</span>
                        <span class="value">{compact_status['duration_seconds']:.2f}s</span>
                    </div>
                    <div class="metric-description">
                        <span>Method:</span>
                        <span class="value">{init_info.get('method', 'unknown')}</span>
                    </div>
                </div>
                
                <!-- Hardware Card -->
                <div class="metric-card">
                    <div class="metric-title">CUDA GPU</div>
                    <div class="metric-value {'status-success' if compact_status['cuda_available'] else 'status-error'}">
                        {'CUDA Available' if compact_status['cuda_available'] else 'CUDA Not Found'}
                    </div>
                    <div class="metric-description">
                        <span>GPU Count:</span>
                        <span class="value">{cuda_info.get('gpu_count', 0) if cuda_info.get('available') else 0}</span>
                    </div>
                    <div class="metric-description">
                        <span>Model Variants:</span>
                        <span class="value">{compact_status['model_variants']}</span>
                    </div>
                </div>
                
                <!-- Configuration Scores Card -->
                <div class="metric-card">
                    <div class="metric-title">Configuration Scores</div>
                    
                    <div class="metric-description">
                        <span>Seed Reproducibility:</span>
                        <span class="value">{compact_status.get('reproducibility_score', 0):.1f}%</span>
                    </div>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {compact_status.get('reproducibility_score', 0)}%" data-value="{compact_status.get('reproducibility_score', 0):.1f}%"></div>
                    </div>
                    
                    <div class="metric-description">
                        <span>Logging Compliance:</span>
                        <span class="value">{compact_status.get('logging_compliance', 0):.1f}%</span>
                    </div>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {compact_status.get('logging_compliance', 0)}%" data-value="{compact_status.get('logging_compliance', 0):.1f}%"></div>
                    </div>
                    
                    <div class="metric-description">
                        <span>Performance Monitoring:</span>
                        <span class="value {'status-success' if compact_status.get('performance_monitoring') else 'status-error'}">
                            {'Available' if compact_status.get('performance_monitoring') else 'Not Available'}
                        </span>
                    </div>
                    
                    <div class="metric-description">
                        <span>Memory Management:</span>
                        <span class="value {'status-success' if compact_status.get('memory_management') else 'status-warning'}">
                            {'Comprehensive' if compact_status.get('memory_management') else 'Basic'}
                        </span>
                    </div>
                </div>
                
                <!-- System Resources Card -->
                <div class="metric-card">
                    <div class="metric-title">System Resources</div>
                    
                    <div class="metric-description">
                        <span>CPU Cores (logical):</span>
                        <span class="value">{cpu_info.get('logical_cores', '?')}</span>
                    </div>
                    
                    <div class="metric-description">
                        <span>CPU Cores (physical):</span>
                        <span class="value">{cpu_info.get('physical_cores', '?')}</span>
                    </div>
                    
                    <div class="metric-description">
                        <span>Total RAM:</span>
                        <span class="value">{ram_info.get('ram_total_gb', 0):.1f} GB</span>
                    </div>
                    
                    <div class="metric-description">
                        <span>Available RAM:</span>
                        <span class="value">{ram_info.get('ram_available_gb', 0):.1f} GB</span>
                    </div>
                    
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {ram_info.get('ram_percent', 0)}%" data-value="{ram_info.get('ram_percent', 0):.1f}%"></div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Tab Container for other sections -->
        <div class="tab-container">
            <div class="tab-buttons">
                <button class="tab-button" onclick="showTab('hardware')">Hardware</button>
                <button class="tab-button" onclick="showTab('configuration')">Configuration</button>
                <button class="tab-button" onclick="showTab('performance')">Performance</button>
                <button class="tab-button" onclick="showTab('dependencies')">Dependencies</button>
                <button class="tab-button" onclick="showTab('history')">History</button>
                <button class="tab-button" onclick="showTab('reports')">Reports</button>
            </div>
            
            <div id="hardware" class="tab-content">
                <h3>Hardware Details</h3>
                <div class="metric-grid">
                    <div class="metric-card">
                        <div class="metric-title">CPU Information</div>
                        <div class="metric-description">
                            <span>Cores:</span>
                            <span class="value">{cpu_info.get('logical_cores', 'N/A')} logical, {cpu_info.get('physical_cores', 'N/A')} physical</span>
                        </div>
                        <div class="metric-description">
                            <span>Frequency:</span>
                            <span class="value">{cpu_info.get('capacity', {}).get('frequency_ghz', 'N/A')} GHz</span>
                        </div>
                    </div>
                    
                    <div class="metric-card">
                        <div class="metric-title">Memory</div>
                        <div class="metric-description">
                            <span>Total:</span>
                            <span class="value">{ram_info.get('ram_total_gb', 0):.1f}GB</span>
                        </div>
                        <div class="metric-description">
                            <span>Available:</span>
                            <span class="value">{ram_info.get('ram_available_gb', 0):.1f}GB</span>
                        </div>
                        <div class="progress-bar">
                            <div class="progress-fill" style="width: {ram_info.get('ram_percent', 0)}%" data-value="{ram_info.get('ram_percent', 0):.1f}%"></div>
                        </div>
                    </div>
                    
                    <div class="metric-card">
                        <div class="metric-title">Storage</div>
                        <div class="metric-description">
                            <span>Total:</span>
                            <span class="value">{disk_info.get('total_gb', 0):.1f}GB</span>
                        </div>
                        <div class="metric-description">
                            <span>Free:</span>
                            <span class="value">{disk_info.get('free_gb', 0):.1f}GB</span>
                        </div>
                        <div class="progress-bar">
                            <div class="progress-fill" style="width: {100 - (disk_info.get('free_gb', 0) / disk_info.get('total_gb', 1) * 100)}%" data-value="{100 - (disk_info.get('free_gb', 0) / disk_info.get('total_gb', 1) * 100):.1f}%"></div>
                        </div>
                    </div>
                </div>
                
                <h3>GPU Information</h3>
                {gpu_info_html}
            </div>
            
            <div id="configuration" class="tab-content">
                <h3>Configuration Details</h3>
                <div class="metric-grid">
                    <div class="metric-card">
                        <div class="metric-title">Preset Configuration</div>
                        <div class="metric-description">
                            <span>Name:</span>
                            <span class="value">{config_info.get('preset_name', 'custom')}</span>
                        </div>
                        <div class="metric-description">
                            <span>Status:</span>
                            <span class="value">{config_info.get('validation_status', 'unknown')}</span>
                        </div>
                        <div class="metric-description">
                            <span>Config File:</span>
                            <span class="value">{config_info.get('config_file', 'unknown')}</span>
                        </div>
                    </div>
                    
                    <div class="metric-card">
                        <div class="metric-title">Available Presets</div>
                        <div class="metric-value">{len(config_info.get('available_presets', []))}</div>
                        <div class="metric-description">
                            {', '.join(config_info.get('available_presets', [])) or 'None'}
                        </div>
                    </div>
                </div>
                
                <h3>Model Variants</h3>
                <div class="metric-grid">
                    <div class="metric-card">
                        <div class="metric-title">Available Variants</div>
                        <div class="metric-value">{model_info.get('variants_available', 0)}</div>
                    </div>
                </div>
                
                <h3>Variant Status</h3>
                <table>
                    <tr><th>Variant</th><th>Status</th></tr>
                    {"".join([f"<tr><td>{name}</td><td><span class='{'status-success' if status == 'available' else 'status-error'}'>{'✅ Available' if status == 'available' else '❌ Missing'}</span></td></tr>" 
                             for name, status in model_info.get('variant_status', {}).items()])}
                </table>
            </div>
            
            <div id="performance" class="tab-content">
                <h3>Performance Metrics</h3>
                {performance_html}
            </div>
            
            <div id="dependencies" class="tab-content">
                <h3>Dependencies</h3>
                <div class="metric-grid">
                    <div class="metric-card">
                        <div class="metric-title">Core Framework</div>
                        <div class="metric-description">
                            <span>Python:</span>
                            <span class="value">{sys_info.get('python_version', 'unknown')}</span>
                        </div>
                        <div class="metric-description">
                            <span>PyTorch:</span>
                            <span class="value">{sys_info.get('pytorch_version', 'unknown')}</span>
                        </div>
                        <div class="metric-description">
                            <span>Platform:</span>
                            <span class="value">{sys_info.get('platform', 'unknown')}</span>
                        </div>
                    </div>
                </div>
                
                <h3>Core Dependencies</h3>
                <table>
                    <tr><th>Dependency</th><th>Version</th><th>Status</th></tr>
                    {core_deps_html}
                </table>
                
                <h3>Optional Dependencies</h3>
                <table>
                    <tr><th>Dependency</th><th>Status</th></tr>
                    {optional_deps_html}
                </table>
            </div>
            
            <!-- HISTOR TAB CONTENT -->
            {history_tab_content}
            
            <div id="reports" class="tab-content">
                <h3>Report Files</h3>
                <div class="metric-card">
                    <div class="metric-title">Consolidated Reports for {date_str}</div>
                    <div class="metric-description">
                        <span>Full Reports:</span>
                        <span class="value">{consolidated_files['report'].name} ({report_data['metadata']['total_reports']} entries)</span>
                    </div>
                    <div class="metric-description">
                        <span>Summary:</span>
                        <span class="value">{consolidated_files['summary'].name}</span>
                    </div>
                    <div class="metric-description">
                        <span>Status:</span>
                        <span class="value">{consolidated_files['status'].name} ({status_data['metadata']['total_entries']} entries)</span>
                    </div>
                    <div class="metric-description">
                        <span>Diagnostics:</span>
                        <span class="value">{consolidated_files['diagnostics'].name} ({diagnostics_data['metadata']['total_entries']} entries)</span>
                    </div>
                    <div class="metric-description">
                        <span>Dashboard Data:</span>
                        <span class="value">{consolidated_files['dashboard_json'].name} ({dashboard_json_file_data['metadata']['total_entries']} entries)</span>
                    </div>
                </div>
            </div>
        </div>
        
        <footer class="footer">
            <p><strong>Deep Learning System Dashboard</strong></p>
            <p>Last Updated: {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')} | Dashboard Version: 3.0</p>
        </footer>
    </div>
</body>
</html>"""
        
        with open(consolidated_files['dashboard_html'], 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        # 7. Update index file
        index_path = report_dir / "deep_initialization_reports.txt"
        report_entry = (
            f"{timestamp}: {compact_status['status']} "
            f"({compact_status['duration_seconds']:.2f}s, "
            f"health: {system_health_score:.1f}%, "
            f"cuda: {compact_status['cuda_available']}, "
            f"models: {compact_status['model_variants']})\n"
        )
        
        with open(index_path, 'a', encoding='utf-8') as f:
            f.write(report_entry)
        
        # 8. Update latest symlinks
        try:
            symlinks = {
                'deep_latest_init_report.json': consolidated_files['report'],
                'deep_latest_init_summary.txt': consolidated_files['summary'],
                'deep_latest_init_status.json': consolidated_files['status'],
                'deep_latest_init_diagnostics.json': consolidated_files['diagnostics'],
                'deep_latest_system_dashboard.json': consolidated_files['dashboard_json'],
                'deep_latest_system_dashboard.html': consolidated_files['dashboard_html']
            }
            
            for symlink_name, target_file in symlinks.items():
                symlink_path = report_dir / symlink_name
                if symlink_path.exists() or symlink_path.is_symlink():
                    symlink_path.unlink()
                symlink_path.symlink_to(target_file.name)
            
            logger.debug("Latest report symlinks updated successfully")
            
        except (OSError, NotImplementedError):
            # Create a text file with paths instead
            with open(report_dir / "latest_reports.txt", 'w') as f:
                f.write("Latest consolidated reports:\n")
                for file_type, file_path in consolidated_files.items():
                    f.write(f"{file_type}: {file_path.name}\n")
        
        # Log successful report generation
        logger.info(f"Initialization report saved to consolidated daily files:")
        logger.info(f"  - Full reports: {consolidated_files['report'].name}")
        logger.info(f"  - Summary: {consolidated_files['summary'].name}")
        logger.info(f"  - Status: {consolidated_files['status'].name}")
        logger.info(f"  - Diagnostics: {consolidated_files['diagnostics'].name}")
        logger.info(f"  - Dashboard Data: {consolidated_files['dashboard_json'].name}")
        logger.info(f"  - Dashboard HTML: {consolidated_files['dashboard_html'].name}")
        logger.info(f"  - Total entries today: {report_data['metadata']['total_reports']}")
        
    except Exception as e:
        # Log the error but don't raise it - we don't want report saving to interrupt the initialization process
        error_msg = f"Failed to save initialization report: {str(e)}"
        logger.error(error_msg)
        logger.debug(f"Report save error details:", exc_info=True)
        
        # Try to save a minimal error report
        try:
            error_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            error_path = report_dir / f"deep_report_error_{error_timestamp}.txt"
            
            with open(error_path, 'w', encoding='utf-8') as f:
                f.write(f"INITIALIZATION REPORT SAVE FAILED\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Error: {str(e)}\n")
                f.write(f"Error Type: {type(e).__name__}\n")
                f.write(f"Original system_status keys: {list(system_status.keys()) if isinstance(system_status, dict) else 'Not a dict'}\n")
                
                # Include file information if available
                if consolidated_files:
                    f.write(f"Target files: {[str(f) for f in consolidated_files.values()]}\n")
                
                f.write(f"Python version: {sys.version}\n")
                f.write(f"Platform: {platform.platform()}\n")
                f.write(f"Working directory: {Path.cwd()}\n")
                f.write(f"Report directory: {report_dir}\n")
            
            logger.info(f"Minimal error report saved to: {error_path}")
            
        except Exception as fallback_error:
            logger.critical(f"Failed to save even minimal error report: {fallback_error}")

def _generate_inline_dashboard_fallback(enhanced_status: Dict, compact_status: Dict, 
                                     system_health_score: float, dashboard_json_file_data: Dict,
                                     consolidated_files: Dict, report_data: Dict, 
                                     status_data: Dict, diagnostics_data: Dict,
                                     timestamp_obj: datetime, date_str: str, time_str: str) -> str:
    """
    Generate inline HTML dashboard as fallback when template renderer is not available.
    
    This function provides a simplified version of the dashboard with embedded styles
    and JavaScript, maintaining core functionality without template dependencies.
    """
    # Extract data safely
    init_info = enhanced_status.get('initialization', {})
    sys_info = enhanced_status.get('system', {})
    detailed_hw = enhanced_status.get('detailed_hardware', {})
    config_info = enhanced_status.get('config', {})
    model_info = enhanced_status.get('models', {})
    cuda_info = detailed_hw.get('cuda', {})
    cpu_info = detailed_hw.get('cpu_cores', {})
    ram_info = detailed_hw.get('system_ram', {})
    
    # Determine health score class
    if system_health_score > 80:
        health_class = 'excellent'
    elif system_health_score > 60:
        health_class = 'good'
    elif system_health_score > 40:
        health_class = 'fair'
    else:
        health_class = 'poor'
    
    # Generate basic GPU info
    gpu_info_html = ""
    if cuda_info.get('available'):
        gpu_info_html = f"""
        <div class="metric-card">
            <div class="metric-title">CUDA Information</div>
            <div class="metric-value">Available</div>
            <div class="metric-description">
                Version: {cuda_info.get('cuda_version', 'unknown')}<br>
                GPU Count: {cuda_info.get('gpu_count', 0)}<br>
                cuDNN: {cuda_info.get('cudnn_version', 'unknown')}
            </div>
        </div>
        """
    else:
        gpu_info_html = """
        <div class="metric-card">
            <div class="metric-title">CUDA Status</div>
            <div class="metric-value">Not Available</div>
            <div class="metric-description">CUDA is not available on this system</div>
        </div>
        """
    
    return f"""<!DOCTYPE html>
<html>
<head>
    <title>Deep Learning System Dashboard - {date_str} (Fallback)</title>
    <style>
        body {{ 
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
            margin: 0; 
            padding: 20px; 
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            color: #333;
        }}
        .container {{ 
            max-width: 1200px; 
            margin: 0 auto; 
            background: white; 
            padding: 30px; 
            border-radius: 12px; 
            box-shadow: 0 15px 35px rgba(0,0,0,0.1);
        }}
        .header {{ 
            text-align: center; 
            margin-bottom: 30px; 
            padding: 30px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 8px;
            margin: -30px -30px 30px -30px;
        }}
        .header h1 {{ margin: 0; font-size: 2.5em; }}
        .header .subtitle {{ font-size: 1.1em; margin-top: 10px; opacity: 0.9; }}
        .fallback-notice {{
            background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
            border-left: 5px solid #ff9800;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }}
        .health-score {{
            font-size: 3em; 
            font-weight: bold; 
            text-align: center; 
            padding: 30px; 
            border-radius: 10px; 
            margin: 20px 0;
            color: white;
        }}
        .health-score.excellent {{ background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%); }}
        .health-score.good {{ background: linear-gradient(135deg, #8BC34A 0%, #7CB342 100%); }}
        .health-score.fair {{ background: linear-gradient(135deg, #FF9800 0%, #F57C00 100%); }}
        .health-score.poor {{ background: linear-gradient(135deg, #F44336 0%, #D32F2F 100%); }}
        .metric-grid {{ 
            display: grid; 
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); 
            gap: 20px; 
            margin: 20px 0; 
        }}
        .metric-card {{ 
            background: #f9f9f9;
            padding: 20px; 
            border-radius: 8px; 
            border-left: 4px solid #2196f3;
            transition: transform 0.2s ease;
        }}
        .metric-card:hover {{ transform: translateY(-2px); }}
        .metric-title {{ font-weight: bold; color: #2196f3; margin-bottom: 10px; }}
        .metric-value {{ font-size: 1.8em; font-weight: bold; color: #333; margin-bottom: 5px; }}
        .metric-description {{ font-size: 0.9em; color: #666; }}
        .status-success {{ color: #4CAF50; }}
        .status-warning {{ color: #FF9800; }}
        .status-error {{ color: #F44336; }}
        .footer {{ 
            text-align: center; 
            margin-top: 40px; 
            padding: 20px; 
            border-top: 2px solid #eee; 
            color: #666; 
        }}
    </style>
    <script>
        const dashboardData = {json.dumps(dashboard_json_file_data, default=str, indent=2)};
        const currentSystemStatus = {json.dumps(compact_status, default=str, indent=2)};
        
        function exportDashboardData() {{
            const exportData = {{
                generated_at: new Date().toISOString(),
                dashboard_metadata: dashboardData.metadata,
                current_status: currentSystemStatus,
                all_entries: dashboardData.entries,
                export_type: 'fallback_dashboard_export',
                export_version: '1.0'
            }};
            
            const dataStr = "data:text/json;charset=utf-8," + encodeURIComponent(JSON.stringify(exportData, null, 2));
            const downloadAnchorNode = document.createElement('a');
            downloadAnchorNode.setAttribute("href", dataStr);
            downloadAnchorNode.setAttribute("download", "fallback_dashboard_export_" + new Date().toISOString().replace(/[:.]/g, '-') + ".json");
            document.body.appendChild(downloadAnchorNode);
            downloadAnchorNode.click();
            downloadAnchorNode.remove();
            alert('Dashboard data exported successfully!');
        }}
    </script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Deep Learning System Dashboard</h1>
            <div class="subtitle">Initialization Report - {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')}</div>
        </div>
        
        <div class="fallback-notice">
            <strong>⚠️ Fallback Mode:</strong> Template-based rendering is unavailable. 
            Using simplified inline HTML generation. Some advanced features may be limited.
        </div>
        
        <div class="health-score {health_class}">
            {system_health_score:.1f}%
            <div style="font-size: 0.4em; margin-top: 10px;">System Health Score</div>
        </div>
        
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-title">Initialization Status</div>
                <div class="metric-value {'status-success' if compact_status['status'] == 'success' else 'status-error'}">{compact_status['status'].upper()}</div>
                <div class="metric-description">Duration: {compact_status['duration_seconds']:.2f} seconds</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">CUDA GPU</div>
                <div class="metric-value {'status-success' if compact_status['cuda_available'] else 'status-error'}">
                    {'Available' if compact_status['cuda_available'] else 'Not Available'}
                </div>
                <div class="metric-description">GPU Count: {cuda_info.get('gpu_count', 0) if cuda_info.get('available') else 0}</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Model Variants</div>
                <div class="metric-value">{compact_status['model_variants']}</div>
                <div class="metric-description">Available model configurations</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">System Resources</div>
                <div class="metric-value">{cpu_info.get('logical_cores', '?')}</div>
                <div class="metric-description">
                    CPU Cores: {cpu_info.get('logical_cores', '?')} logical, {cpu_info.get('physical_cores', '?')} physical<br>
                    RAM: {ram_info.get('ram_total_gb', 0):.1f}GB total, {ram_info.get('ram_available_gb', 0):.1f}GB available
                </div>
            </div>
        </div>
        
        <h3>Configuration Scores</h3>
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-title">Reproducibility</div>
                <div class="metric-value">{compact_status.get('reproducibility_score', 0):.1f}%</div>
                <div class="metric-description">Seed configuration compliance</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Logging Compliance</div>
                <div class="metric-value">{compact_status.get('logging_compliance', 0):.1f}%</div>
                <div class="metric-description">Logging setup quality</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Performance Monitoring</div>
                <div class="metric-value {'status-success' if compact_status.get('performance_monitoring') else 'status-error'}">
                    {'Available' if compact_status.get('performance_monitoring') else 'Not Available'}
                </div>
                <div class="metric-description">System performance tracking</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Memory Management</div>
                <div class="metric-value {'status-success' if compact_status.get('memory_management') else 'status-warning'}">
                    {'Comprehensive' if compact_status.get('memory_management') else 'Basic'}
                </div>
                <div class="metric-description">Memory handling capabilities</div>
            </div>
        </div>
        
        <h3>Hardware Details</h3>
        <div class="metric-grid">
            {gpu_info_html}
            
            <div class="metric-card">
                <div class="metric-title">CPU Information</div>
                <div class="metric-value">{cpu_info.get('logical_cores', '?')}</div>
                <div class="metric-description">
                    Logical Cores: {cpu_info.get('logical_cores', 'Unknown')}<br>
                    Physical Cores: {cpu_info.get('physical_cores', 'Unknown')}<br>
                    Frequency: {cpu_info.get('capacity', {}).get('frequency_ghz', 'Unknown')} GHz
                </div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Total Initializations</div>
                <div class="metric-value">{dashboard_json_file_data['metadata']['total_entries']}</div>
                <div class="metric-description">Entries recorded today</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Export Data</div>
                <div class="metric-value">
                    <button onclick="exportDashboardData()" style="background: #2196f3; color: white; border: none; padding: 10px 20px; border-radius: 4px; cursor: pointer;">
                        Export JSON
                    </button>
                </div>
                <div class="metric-description">Download complete system data</div>
            </div>
        </div>
        
        <div class="footer">
            <p><strong>Deep Learning System Dashboard - Fallback Mode</strong></p>
            <p>Generated: {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')} | Version: 3.0 (Inline Fallback)</p>
            <p>⚠️ Limited functionality - Template renderer unavailable</p>
        </div>
    </div>
</body>
</html>"""



def _display_text_report(file_path, title=None):
    """Display a text report with pagination."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if not title:
            title = f"Text Report: {file_path.name}"
        
        console.print(
            Panel.fit(
                f"[bold green]{title}[/bold green]\n"
                f"File: [cyan]{file_path.name}[/cyan]\n"
                f"Size: [cyan]{len(content)} characters[/cyan]",
                title="TEXT REPORT",
                border_style="green",
                padding=(1, 2)
            )
        )
        
        # Split into pages for large files
        lines = content.split('\n')
        page_size = 50
        
        if len(lines) <= page_size:
            print(f"\n{content}")
        else:
            current_page = 0
            while current_page * page_size < len(lines):
                start_idx = current_page * page_size
                end_idx = min((current_page + 1) * page_size, len(lines))
                
                page_content = '\n'.join(lines[start_idx:end_idx])
                print(f"\n{Fore.CYAN}--- Page {current_page + 1} of {(len(lines) - 1) // page_size + 1} ---{Style.RESET_ALL}")
                print(page_content)
                
                if end_idx < len(lines):
                    try:
                        user_input = input(f"\n{Fore.YELLOW}Press Enter for next page, 'q' to quit, or 'a' for all: {Style.RESET_ALL}").strip().lower()
                        if user_input == 'q':
                            break
                        elif user_input == 'a':
                            print('\n'.join(lines[end_idx:]))
                            break
                    except (EOFError, KeyboardInterrupt):
                        break
                
                current_page += 1
        
    except Exception as e:
        print(f"{Fore.RED}Error reading file: {str(e)}{Style.RESET_ALL}")

def _display_json_report(file_path):
    """Display a JSON report with formatted output."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        console.print(
            Panel.fit(
                f"[bold green]JSON Report: {file_path.name}[/bold green]\n"
                f"Type: [cyan]{data.get('metadata', {}).get('report_type', 'unknown')}[/cyan]\n"
                f"Version: [cyan]{data.get('metadata', {}).get('version', 'unknown')}[/cyan]\n"
                f"Entries: [cyan]{len(data.get('reports', []))}[/cyan]",
                title="JSON REPORT",
                border_style="green",
                padding=(1, 2)
            )
        )
        
        # Display summary of entries
        reports = data.get('reports', [])
        if reports:
            print(f"\n{Fore.CYAN}Report Entries:{Style.RESET_ALL}")
            # Show first 10
            for i, report in enumerate(reports[:10], 1):
                timestamp = report.get('timestamp', 'Unknown')
                status = report.get('data', {}).get('initialization', {}).get('status', 'unknown')
                duration = report.get('data', {}).get('initialization', {}).get('duration_seconds', 0)
                print(f"  {i}. {timestamp} - Status: {status}, Duration: {duration:.2f}s")
            
            if len(reports) > 10:
                print(f"  ... and {len(reports) - 10} more entries")
        
        # Ask if user wants to see full JSON
        try:
            show_full = input(f"\n{Fore.YELLOW}Show full JSON content? (y/N): {Style.RESET_ALL}").strip().lower()
            if show_full in ('y', 'yes'):
                formatted_json = json.dumps(data, indent=2, default=str)
                print(f"\n{formatted_json}")
        except (EOFError, KeyboardInterrupt):
            pass
            
    except Exception as e:
        print(f"{Fore.RED}Error reading JSON file: {str(e)}{Style.RESET_ALL}")

def _display_status_report(file_path):
    """Display a status report with compact formatting."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        entries = data.get('entries', [])
        metadata = data.get('metadata', {})
        
        console.print(
            Panel.fit(
                f"[bold green]Status Report: {file_path.name}[/bold green]\n"
                f"Date: [cyan]{data.get('date', 'unknown')}[/cyan]\n"
                f"Entries: [cyan]{len(entries)}[/cyan]\n"
                f"Last Updated: [cyan]{metadata.get('last_updated', 'unknown')}[/cyan]",
                title="STATUS REPORT",
                border_style="green",
                padding=(1, 2)
            )
        )
        
        if entries:
            print(f"\n{Fore.CYAN}Initialization Status Summary:{Style.RESET_ALL}")
            print(f"{'Time':<10} {'Status':<10} {'Health':<8} {'Duration':<10} {'CUDA':<6} {'Models':<7}")
            print("-" * 60)
            # Show last 20 entries
            for entry in entries[-20:]:
                entry_data = entry.get('data', {})
                time_str = entry.get('time_str', 'unknown')[:8]
                status = entry_data.get('status', 'unknown')[:9]
                health = f"{entry_data.get('system_health_score', 0):.1f}%"
                duration = f"{entry_data.get('duration_seconds', 0):.2f}s"
                cuda = "Yes" if entry_data.get('cuda_available', False) else "No"
                models = str(entry_data.get('model_variants', 0))
                
                print(f"{time_str:<10} {status:<10} {health:<8} {duration:<10} {cuda:<6} {models:<7}")
                
    except Exception as e:
        print(f"{Fore.RED}Error reading status file: {str(e)}{Style.RESET_ALL}")

def _display_diagnostic_report(file_path):
    """Display a diagnostic report with technical details."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        entries = data.get('entries', [])
        metadata = data.get('metadata', {})
        
        console.print(
            Panel.fit(
                f"[bold green]Diagnostic Report: {file_path.name}[/bold green]\n"
                f"Date: [cyan]{data.get('date', 'unknown')}[/cyan]\n"
                f"Entries: [cyan]{len(entries)}[/cyan]\n"
                f"Framework Version: [cyan]{metadata.get('version', 'unknown')}[/cyan]",
                title="DIAGNOSTIC REPORT",
                border_style="green",
                padding=(1, 2)
            )
        )
        
        if entries:
            # Show latest entry details
            latest_entry = entries[-1]
            entry_data = latest_entry.get('data', {})
            
            print(f"\n{Fore.CYAN}Latest Diagnostic Entry ({latest_entry.get('timestamp', 'unknown')}):{Style.RESET_ALL}")
            
            # Hardware analysis
            hw_analysis = entry_data.get('hardware_analysis', {})
            if hw_analysis:
                print(f"\n  {Fore.GREEN}Hardware Analysis:{Style.RESET_ALL}")
                cuda_info = hw_analysis.get('cuda', {})
                if cuda_info:
                    print(f"    CUDA: {cuda_info.get('status', 'unknown')} - {cuda_info.get('gpu_count', 0)} GPUs")
                cpu_info = hw_analysis.get('cpu_cores', {})
                if cpu_info:
                    print(f"    CPU: {cpu_info.get('logical_cores', '?')} logical cores")
            
            # Version validation
            version_validation = entry_data.get('version_validation', {})
            if version_validation:
                print(f"\n  {Fore.GREEN}Version Validation:{Style.RESET_ALL}")
                compatible_count = sum(1 for v in version_validation.values() if v.get('compatible', False))
                print(f"    Compatible dependencies: {compatible_count}/{len(version_validation)}")
            
            # Reproducibility config
            repro_config = entry_data.get('reproducibility_config', {})
            if repro_config:
                print(f"\n  {Fore.GREEN}Reproducibility:{Style.RESET_ALL}")
                print(f"    Seed config passed: {repro_config.get('seed_config_passed', False)}")
                print(f"    Compliance score: {repro_config.get('compliance_score', 0):.1f}%")
                
    except Exception as e:
        print(f"{Fore.RED}Error reading diagnostic file: {str(e)}{Style.RESET_ALL}")

def _display_dashboard_data(file_path):
    """Display dashboard data with visualization options."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        entries = data.get('entries', [])
        metadata = data.get('metadata', {})
        
        console.print(
            Panel.fit(
                f"[bold green]Dashboard Data: {file_path.name}[/bold green]\n"
                f"Date: [cyan]{data.get('date', 'unknown')}[/cyan]\n"
                f"Entries: [cyan]{len(entries)}[/cyan]\n"
                f"Last Updated: [cyan]{metadata.get('last_updated', 'unknown')}[/cyan]",
                title="DASHBOARD DATA",
                border_style="green",
                padding=(1, 2)
            )
        )
        
        if entries:
            # Calculate statistics
            health_scores = [e.get('data', {}).get('health_score', 0) for e in entries]
            durations = [e.get('data', {}).get('duration_seconds', 0) for e in entries]
            
            avg_health = sum(health_scores) / len(health_scores)
            avg_duration = sum(durations) / len(durations)
            success_count = sum(1 for e in entries if e.get('data', {}).get('status', '').lower() == 'success')
            
            print(f"\n{Fore.CYAN}Statistics Summary:{Style.RESET_ALL}")
            print(f"  Average Health Score: {avg_health:.1f}%")
            print(f"  Average Duration: {avg_duration:.2f}s")
            print(f"  Success Rate: {success_count}/{len(entries)} ({success_count/len(entries)*100:.1f}%)")
            
            # Show trend
            if len(entries) >= 2:
                recent_health = sum(e.get('data', {}).get('health_score', 0) for e in entries[-5:]) / min(5, len(entries))
                overall_health = sum(health_scores) / len(health_scores)
                trend = "↑" if recent_health > overall_health else "↓" if recent_health < overall_health else "→"
                print(f"  Health Trend: {trend} Recent avg: {recent_health:.1f}%")
                
    except Exception as e:
        print(f"{Fore.RED}Error reading dashboard data: {str(e)}{Style.RESET_ALL}")

def _show_all_report_files(report_dir, report_files):
    """Show all available report files with details."""
    print(f"\n{Fore.CYAN}All Available Report Files:{Style.RESET_ALL}")
    print(f"Location: {report_dir}\n")
    
    all_files = []
    for category, files in report_files.items():
        for file_path in files:
            if file_path.exists():
                file_size = file_path.stat().st_size / 1024
                mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                all_files.append((file_path, file_size, mod_time, category))
    
    # Sort by modification time (newest first)
    all_files.sort(key=lambda x: x[2], reverse=True)
    
    if all_files:
        print(f"{'File Name':<40} {'Size':<10} {'Modified':<20} {'Type':<15}")
        print("-" * 90)
        
        for file_path, file_size, mod_time, category in all_files:
            print(f"{file_path.name:<40} {file_size:>7.1f}KB {mod_time.strftime('%Y-%m-%d %H:%M'):<20} {category:<15}")
    else:
        print("No files found.")

# Model architecture comparison
def initialize_model_variants(silent: bool = False) -> None:
    """Initialize MODEL_VARIANTS dictionary with validation and error recovery.
    
    This function has been updated to leverage the model_instantiation_with_validation() helper
    for comprehensive model initialization with built-in validation and error handling.
    
    Args:
        silent: If True, suppress detailed logging messages and progress bars during system checks
    """
    global MODEL_VARIANTS
    
    if not silent:
        logger.info("Initializing model variants using helper functions")
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'successful_models': 0,
        'failed_models': 0,
        'current_model': None,
        'instantiation_method': None
    }
    
    # Calculate total work units for progress bar
    total_stages = 6  # System, Config, Params, Definitions, Models, Finalization
    total_models = 0  # Will be set after model_definitions is created
    
    try:
        # STAGE 1: System Preparation
        progress_data['current_stage'] = "System Preparation"
        
        hardware_data = None
        total_ram_gb = 8.0
        
        with alive_bar(total_stages, title='Initializing Model Variants\t', unit='stages') as bar:
            
            try:
                bar.text = "Checking system hardware..."
                hardware_data = check_hardware(include_memory_usage=True)
                total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
                
                # Initial memory cleanup for memory-constrained systems
                _optimize_memory_if_needed(
                    condition=total_ram_gb < 8,
                    hardware_data=hardware_data,
                    aggressive=total_ram_gb < 4,
                    silent=silent
                )
                bar.text = "System check complete"
            except Exception as e:
                if not silent:
                    logger.debug(f"Hardware detection failed: {e}")
                hardware_data = {}
                bar.text = "System check (using defaults)"
            
            # Clear existing variants
            MODEL_VARIANTS = {}
            bar()
            
            # STAGE 2: Configuration Loading
            progress_data['current_stage'] = "Loading Configuration"
            bar.text = "Loading configuration..."
            
            # Get current configuration with fallbacks
            try:
                current_config = get_current_config()
                if not isinstance(current_config, dict):
                    current_config = {}
                
                model_config = current_config.get('model', {})
                data_config = current_config.get('data', {})
                training_config = current_config.get('training', {})
                hardware_config = current_config.get('hardware', {})
                system_config = current_config.get('system', {})
                
                if not silent:
                    logger.debug("Loaded configuration for model variant initialization")
                bar.text = "Configuration loaded"
            except Exception as e:
                if not silent:
                    logger.warning(f"Could not load current config, using defaults: {e}")
                current_config = {}
                model_config = {}
                data_config = {}
                training_config = {}
                hardware_config = {}
                system_config = {}
                bar.text = "Configuration (using defaults)"
            
            # MEMORY OPTIMIZATION CHECKPOINT - Clear memory after config processing for large configs
            config_size_estimate = len(str(current_config))
            _optimize_memory_if_needed(
                condition=config_size_estimate > 50000 and total_ram_gb < 16,
                hardware_data=hardware_data,
                aggressive=config_size_estimate > 100000,
                silent=silent
            )
            
            bar()
            
            # STAGE 3: Parameter Extraction & Validation
            progress_data['current_stage'] = "Validating Parameters"
            bar.text = "Extracting parameters..."
            
            # Extract configuration values using helper function
            test_input_dim = _extract_and_validate_config_param(
                data_config, 'features', 20, 'FEATURES',
                lambda x: isinstance(x, int) and x > 0,
                "input feature dimension", silent
            )
            
            base_encoding_dim = _extract_and_validate_config_param(
                model_config, 'encoding_dim', 16, 'DEFAULT_ENCODING_DIM',
                lambda x: isinstance(x, int) and x > 0,
                "latent encoding dimension", silent
            )
            
            base_hidden_dims = _extract_and_validate_config_param(
                model_config, 'hidden_dims', [128, 64], 'HIDDEN_LAYER_SIZES',
                lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(d, int) and d > 0 for d in x),
                "hidden layer dimensions", silent
            )
            
            base_dropout_rates = _extract_and_validate_config_param(
                model_config, 'dropout_rates', [0.2, 0.15], 'DROPOUT_RATES',
                lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(r, (int, float)) and 0 <= r < 1 for r in x),
                "dropout rates", silent
            )
            
            # Activation and normalization
            activation = _extract_and_validate_config_param(
                model_config, 'activation', 'leaky_relu', 'ACTIVATION',
                lambda x: x in ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu', 'prelu'],
                "activation function", silent
            )
            
            activation_param = _extract_and_validate_config_param(
                model_config, 'activation_param', 0.2, 'ACTIVATION_PARAM',
                lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
                "activation parameter", silent
            )
            
            normalization = _extract_and_validate_config_param(
                model_config, 'normalization', 'batch', 'NORMALIZATION',
                lambda x: x in ['batch', 'layer', 'instance', 'group', 'none', None],
                "normalization type", silent
            )
            
            # Architecture features
            use_batch_norm = _extract_and_validate_config_param(
                model_config, 'use_batch_norm', True, 'USE_BATCH_NORM',
                lambda x: isinstance(x, bool),
                "batch normalization flag", silent
            )
            
            use_layer_norm = _extract_and_validate_config_param(
                model_config, 'use_layer_norm', False, 'USE_LAYER_NORM',
                lambda x: isinstance(x, bool),
                "layer normalization flag", silent
            )
            
            skip_connection = _extract_and_validate_config_param(
                model_config, 'skip_connection', True, 'SKIP_CONNECTION',
                lambda x: isinstance(x, bool),
                "skip connections flag", silent
            )
            
            residual_blocks = _extract_and_validate_config_param(
                model_config, 'residual_blocks', False, 'RESIDUAL_BLOCKS',
                lambda x: isinstance(x, bool),
                "residual blocks flag", silent
            )
            
            use_attention = _extract_and_validate_config_param(
                model_config, 'use_attention', False, 'USE_ATTENTION',
                lambda x: isinstance(x, bool),
                "attention mechanism flag", silent
            )
            
            # Ensemble parameters
            num_models = _extract_and_validate_config_param(
                model_config, 'num_models', 3, 'NUM_MODELS',
                lambda x: isinstance(x, int) and 1 <= x <= 10,
                "ensemble size", silent
            )
            
            diversity_factor = _extract_and_validate_config_param(
                model_config, 'diversity_factor', 0.2, 'DIVERSITY_FACTOR',
                lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
                "ensemble diversity factor", silent
            )
            
            # Training parameters
            mixed_precision = _extract_and_validate_config_param(
                training_config, 'mixed_precision', False, 'MIXED_PRECISION',
                lambda x: isinstance(x, bool),
                "mixed precision training", silent
            )
            
            learning_rate = _extract_and_validate_config_param(
                training_config, 'learning_rate', 0.001, 'LEARNING_RATE',
                lambda x: isinstance(x, (int, float)) and x > 0,
                "learning rate", silent
            )
            
            batch_size = _extract_and_validate_config_param(
                training_config, 'batch_size', 32, 'BATCH_SIZE',
                lambda x: isinstance(x, int) and x > 0,
                "batch size", silent
            )
            
            # Hardware parameters
            device = _extract_and_validate_config_param(
                hardware_config, 'device', 'auto', 'DEVICE',
                lambda x: isinstance(x, str) and x in ['auto', 'cpu', 'cuda'] or x.startswith('cuda:'),
                "compute device", silent
            )
            
            # System parameters
            random_seed = _extract_and_validate_config_param(
                system_config, 'random_seed', 42, 'RANDOM_SEED',
                lambda x: isinstance(x, int),
                "random seed", silent
            )
            
            legacy_mode = _extract_and_validate_config_param(
                model_config, 'legacy_mode', False, 'LEGACY_MODE',
                lambda x: isinstance(x, bool),
                "legacy compatibility mode", silent
            )
            
            # Validate and adjust parameters using helper function
            base_hidden_dims, base_dropout_rates = _validate_and_adjust_parameters(
                base_hidden_dims, base_dropout_rates, silent
            )
            
            # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before intensive model testing
            _optimize_memory_if_needed(
                condition=len(str(base_hidden_dims) + str(base_dropout_rates)) > 1000 or total_ram_gb < 8,
                hardware_data=hardware_data,
                aggressive=total_ram_gb < 4,
                silent=silent
            )
            
            bar.text = "Parameters validated"
            bar()
            
            # STAGE 4: Model Definitions
            progress_data['current_stage'] = "Creating Model Definitions"
            bar.text = "Creating model definitions..."
            
            # Create model test definitions using helper function with ALL required parameters
            model_definitions = _create_model_test_definition(
                encoding_dim=base_encoding_dim,
                hidden_dims=base_hidden_dims,
                dropout_rates=base_dropout_rates,
                use_attention=use_attention,
                residual_blocks=residual_blocks,
                skip_connection=skip_connection,
                legacy_mode=legacy_mode,
                num_models=num_models,
                diversity_factor=diversity_factor,
                mixed_precision=mixed_precision,
                input_dim=test_input_dim,
                activation=activation,
                activation_param=activation_param,
                normalization=normalization,
                use_batch_norm=use_batch_norm,
                use_layer_norm=use_layer_norm
            )
            
            total_models = len(model_definitions)
            bar.text = f"Created {total_models} model definitions"
            bar()
            
            # Close the main progress bar and start model initialization bar
            bar.text = "Starting model initialization..."
        
        # STAGE 5: Model Initialization with detailed progress
        progress_data['current_stage'] = "Model Initialization"
        
        # Track initialization statistics
        initialization_stats = {
            'attempted': 0,
            'successful': [],
            'failed': [],
            'skipped': [],
            'fallback_used': [],
            'minimal_used': [],
            'adaptive_used': [],
            'validation_passed': 0,
            'errors': [],
            'config_validation_passed': 0,
            'architecture_tests_passed': 0,
            'performance_tests_passed': 0,
            'memory_optimizations_performed': 0,
            'detailed_metrics': {}
        }
        
        # Status symbols for visual feedback
        status_symbols = {
            'success': '[OK]',
            'failure': '[FAIL]',
            'skip': '[SKIP]'
        }
        
        method_symbols = {
            'primary': '[PRIMARY]',
            'fallback': '[FALLBACK]',
            'minimal': '[MINIMAL]',
            'adaptive': '[ADAPTIVE]'
        }
        
        with alive_bar(total_models, title='Initializing Models\t\t', unit='models') as model_bar:
            
            # Initialize each model variant using the helper function
            for name, definition in model_definitions.items():
                initialization_stats['attempted'] += 1
                progress_data['current_model'] = name
                
                # Update progress bar with current model info
                successful_count = len(initialization_stats['successful'])
                failed_count = len(initialization_stats['failed'])
                model_bar.text = f"Testing {name}... ({successful_count} passed, {failed_count} failed)"
                
                try:
                    if not silent:
                        logger.debug(f"Attempting to initialize {name} using helper function")
                    
                    # Check if model class exists and is callable
                    if not definition['class_check']():
                        error_msg = f"Class not available or not callable"
                        if not silent:
                            logger.warning(f"{name}: {error_msg}")
                        initialization_stats['errors'].append(f"{name}: {error_msg}")
                        initialization_stats['skipped'].append(name)
                        model_bar.text = f"{status_symbols['skip']} {name} skipped"
                        progress_data['failed_models'] += 1
                        model_bar()
                        continue
                    
                    # Get the model class
                    model_class = definition['class_getter']()
                    
                    # Use model_instantiation_with_validation for initialization
                    test_model, validation_results, performance_metrics, instantiation_details = model_instantiation_with_validation(
                        variant_class=model_class,
                        variant_name=name,
                        input_dim=test_input_dim,
                        base_config=definition['primary_config'],
                        fallback_config=definition['fallback_config'],
                        minimal_config=definition['minimal_config'],
                        validation_tests=['basic', 'forward_pass', 'parameters', 'config_methods', 'training_mode'],
                        comprehensive_validation=True,
                        hardware_data=hardware_data,
                        silent=silent,
                        logger=logger
                    )
                    
                    # Track instantiation method used
                    instantiation_method = instantiation_details.get('method', 'failed')
                    progress_data['instantiation_method'] = instantiation_method
                    
                    if instantiation_method == 'fallback':
                        initialization_stats['fallback_used'].append(name)
                    elif instantiation_method == 'minimal':
                        initialization_stats['minimal_used'].append(name)
                    elif instantiation_method == 'adaptive':
                        initialization_stats['adaptive_used'].append(name)
                    
                    # Process instantiation results
                    if test_model is not None:
                        try:
                            # Store detailed metrics
                            initialization_stats['detailed_metrics'][name] = {
                                'instantiation_method': instantiation_method,
                                'performance_metrics': performance_metrics,
                                'validation_results': validation_results,
                                'config_used': instantiation_details.get('config_used', {})
                            }
                            
                            # Record validation statistics from helper function
                            test_results = validation_results.get('test_results', {})
                            
                            # Test 1: Configuration validation (handled by helper)
                            if test_results.get('config_methods') == 'passed':
                                initialization_stats['config_validation_passed'] += 1
                            
                            # Test 2: Basic forward pass (handled by helper)
                            if test_results.get('forward_pass') == 'passed':
                                initialization_stats['architecture_tests_passed'] += 1
                            
                            # Test 3: Training mode compatibility (handled by helper)
                            if test_results.get('training_mode') == 'passed':
                                initialization_stats['performance_tests_passed'] += 1
                            
                            # Additional validation
                            if hasattr(test_model, 'encode') and hasattr(test_model, 'decode'):
                                try:
                                    test_model.eval()
                                    test_input_encode = torch.randn(2, test_input_dim)
                                    if hasattr(test_model, 'device'):
                                        test_input_encode = test_input_encode.to(test_model.device)
                                    
                                    encoded = test_model.encode(test_input_encode)
                                    decoded = test_model.decode(encoded)
                                    
                                    if decoded.shape == test_input_encode.shape:
                                        initialization_stats['performance_tests_passed'] += 1
                                except Exception:
                                    pass  # Encode/decode is optional
                            
                            # Test parameter counting (already done by helper)
                            total_params = performance_metrics.get('total_parameters', 0)
                            if total_params > 0:
                                initialization_stats['performance_tests_passed'] += 1
                            
                            # Test device compatibility
                            device_str = str(test_model.device if hasattr(test_model, 'device') else 'cpu')
                            if 'cuda' in device_str and not torch.cuda.is_available():
                                if not silent:
                                    logger.warning(f"{name}: Model on CUDA device but CUDA not available")
                            
                            # Success - register the model variant
                            MODEL_VARIANTS[name] = model_class
                            initialization_stats['successful'].append(name)
                            initialization_stats['validation_passed'] += 1
                            progress_data['successful_models'] += 1
                            
                            method_symbol = method_symbols.get(instantiation_method, '[UNKNOWN]')
                            model_bar.text = f"{status_symbols['success']} {name} {method_symbol} ({total_params:,} params)"
                            
                            if not silent:
                                logger.info(f"{status_symbols['success']} {name}: Successfully initialized and validated "
                                          f"({instantiation_method}, {total_params:,} params, "
                                          f"score: {validation_results.get('overall_score', 0):.1%})")
                            
                        except Exception as validation_error:
                            error_msg = f"Post-instantiation validation failed: {str(validation_error)}"
                            if not silent:
                                logger.error(f"{status_symbols['failure']} {name}: {error_msg}")
                            initialization_stats['errors'].append(f"{name}: {error_msg}")
                            initialization_stats['failed'].append(name)
                            progress_data['failed_models'] += 1
                            model_bar.text = f"{status_symbols['failure']} {name} validation failed"
                    else:
                        # Instantiation failed
                        error_msg = f"Instantiation failed: {', '.join(validation_results.get('errors', []))}"
                        if not silent:
                            logger.error(f"{status_symbols['failure']} {name}: {error_msg}")
                        initialization_stats['errors'].append(f"{name}: {error_msg}")
                        initialization_stats['failed'].append(name)
                        progress_data['failed_models'] += 1
                        model_bar.text = f"{status_symbols['failure']} {name} instantiation failed"
                    
                    # Cleanup test model and free memory
                    if test_model is not None:
                        del test_model
                    
                    # MEMORY OPTIMIZATION CHECKPOINT - Cleanup between model tests
                    if _optimize_memory_if_needed(
                        condition=True,  # Always aggressive between models
                        hardware_data=hardware_data,
                        aggressive=True,
                        silent=silent
                    ):
                        initialization_stats['memory_optimizations_performed'] += 1
                    
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
                    
                except Exception as e:
                    if not silent:
                        logger.error(f"{status_symbols['failure']} Failed to initialize model variant {name}: {e}")
                    initialization_stats['errors'].append(f"{name}: Unexpected error: {str(e)}")
                    initialization_stats['failed'].append(name)
                    progress_data['failed_models'] += 1
                    model_bar.text = f"{status_symbols['failure']} {name} unexpected error"
                
                # Update progress bar
                model_bar()
            
            # Final update for model initialization
            successful_count = len(initialization_stats['successful'])
            failed_count = len(initialization_stats['failed'])
            model_bar.text = f"Models: {successful_count} passed, {failed_count} failed"
        
        # STAGE 6: Finalization
        progress_data['current_stage'] = "Finalizing"
        
        with alive_bar(1, title='Finalizing\t\t\t') as final_bar:
            
            final_bar.text = "Finalizing setup..."
            
            # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before post-processing
            _optimize_memory_if_needed(
                condition=len(MODEL_VARIANTS) > 2,
                hardware_data=hardware_data,
                aggressive=len(MODEL_VARIANTS) > 2,
                silent=silent
            )
            
            # Log initialization summary
            total_attempted = initialization_stats['attempted']
            successful_count = len(initialization_stats['successful'])
            failed_count = len(initialization_stats['failed'])
            fallback_count = len(initialization_stats['fallback_used'])
            minimal_count = len(initialization_stats['minimal_used'])
            adaptive_count = len(initialization_stats['adaptive_used'])
            memory_opts = initialization_stats['memory_optimizations_performed']
            
            if not silent:
                logger.info("Model variants initialization completed using helper functions:")
                logger.info(f"  - Total attempted: {total_attempted}")
                logger.info(f"  - Successful: {successful_count}")
                logger.info(f"  - Failed: {failed_count}")
                logger.info(f"  - Using primary config: {successful_count - fallback_count - minimal_count - adaptive_count}")
                logger.info(f"  - Using fallback config: {fallback_count}")
                logger.info(f"  - Using minimal config: {minimal_count}")
                logger.info(f"  - Using adaptive config: {adaptive_count}")
                logger.info(f"  - Configuration validation passed: {initialization_stats['config_validation_passed']}")
                logger.info(f"  - Architecture tests passed: {initialization_stats['architecture_tests_passed']}")
                logger.info(f"  - Performance tests passed: {initialization_stats['performance_tests_passed']}")
                logger.info(f"  - Memory optimizations performed: {memory_opts}")
                
                # Log detailed metrics for successful models
                if initialization_stats['detailed_metrics']:
                    logger.info("  - Detailed metrics available for successful models")
                    for model_name, metrics in initialization_stats['detailed_metrics'].items():
                        score = metrics['validation_results'].get('overall_score', 0)
                        params = metrics['performance_metrics'].get('total_parameters', 0)
                        logger.debug(f"    {model_name}: {score:.1%} score, {params:,} params, {metrics['instantiation_method']} config")
                
                if initialization_stats['successful']:
                    logger.info(f"  - Available models: {', '.join(initialization_stats['successful'])}")
                
                if initialization_stats['failed']:
                    logger.warning(f"  - Failed models: {', '.join(initialization_stats['failed'])}")
                
                # Log critical errors for debugging
                if initialization_stats['errors']:
                    logger.warning("Initialization errors encountered (showing first 5):")
                    for error in initialization_stats['errors'][:5]:
                        logger.warning(f"  - {error}")
                    if len(initialization_stats['errors']) > 5:
                        logger.warning(f"  ... and {len(initialization_stats['errors']) - 5} more errors")
            
            # Ensure at least one model variant is available
            if not MODEL_VARIANTS:
                error_msg = "No model variants could be initialized"
                if not silent:
                    logger.error(error_msg)
                    logger.error("This indicates a serious configuration or dependency issue")
                
                final_bar.text = "No models - emergency fallback"
                
                # Emergency fallback using the helper function
                try:
                    if SimpleAutoencoder is not None:
                        emergency_model, emergency_results, emergency_metrics, emergency_details = model_instantiation_with_validation(
                            variant_class=SimpleAutoencoder,
                            variant_name='SimpleAutoencoder',
                            input_dim=10,
                            base_config=_create_adaptive_config(
                                model_name='SimpleAutoencoder',
                                model_class=SimpleAutoencoder,
                                system_class='emergency',
                                input_dim=10,
                                encoding_dim=4,
                                hidden_dims=[16],
                                dropout_rates=[0.1],
                                activation='relu',
                                activation_param=0.0,
                                normalization=None,
                                use_batch_norm=False,
                                use_layer_norm=False,
                                skip_connection=False,
                                residual_blocks=False,
                                use_attention=False,
                                legacy_mode=True,
                                num_models=1,
                                diversity_factor=0.0,
                                learning_rate=0.001,
                                batch_size=32,
                                mixed_precision=False,
                                optimizer_type='Adam',
                                device_setting='cpu',
                                random_seed=42,
                                hardware_data=hardware_data
                            ),
                            validation_tests=['basic', 'forward_pass'],
                            comprehensive_validation=False,
                            hardware_data=hardware_data,
                            silent=silent,
                            logger=logger
                        )
                        
                        if emergency_model is not None:
                            MODEL_VARIANTS['SimpleAutoencoder'] = SimpleAutoencoder
                            initialization_stats['memory_optimizations_performed'] += 1
                            initialization_stats['successful'].append('SimpleAutoencoder')
                            progress_data['successful_models'] += 1
                            final_bar.text = "Emergency fallback activated"
                            if not silent:
                                logger.warning("Emergency fallback: Ultra-minimal SimpleAutoencoder available")
                        else:
                            raise RuntimeError("Emergency fallback instantiation failed")
                    else:
                        raise RuntimeError(error_msg)
                        
                except Exception as e:
                    if not silent:
                        logger.critical(f"Emergency fallback failed: {e}")
                    raise RuntimeError(f"{error_msg}: {str(e)}")
            
            # Run post-initialization validation if available
            try:
                if not silent:
                    final_bar.text = "Running final validation"
                
                # Use the validation function if available
                if 'validate_model_variants' in globals():
                    variant_validation_results = validate_model_variants(logger, silent=silent)
                    
                    fully_validated = [
                        name for name, status in variant_validation_results.items() 
                        if status == 'available'
                    ]
                    
                    if fully_validated:
                        if not silent:
                            logger.info(f"-SUCCESS- Fully validated model variants: {', '.join(fully_validated)}")
                    else:
                        if not silent:
                            logger.warning("-WARN- No model variants passed comprehensive post-validation")
                    final_bar.text = "Final validation complete"
                else:
                    if not silent:
                        logger.debug("Post-initialization validation function not available")
                    
            except Exception as validation_error:
                if not silent:
                    logger.error(f"Post-initialization validation failed: {validation_error}")
            
            # FINAL MEMORY OPTIMIZATION
            _optimize_memory_if_needed(
                condition=True,  # Always aggressive final cleanup
                hardware_data=hardware_data,
                aggressive=True,
                silent=silent
            )
            
            final_bar.text = "Initialization complete!"
            final_bar()
    
    except Exception as e:
        # If we were using progress bars, they're already closed by the context managers
        raise e
    
    if not silent:
        logger.info(f"Model variants initialization completed successfully with "
                   f"{len(MODEL_VARIANTS)} available variants")
        logger.info(f"Memory optimizations: {initialization_stats['memory_optimizations_performed']} operations performed")
        
        # Log final summary of available capabilities
        capabilities_summary = []
        for name in MODEL_VARIANTS.keys():
            if name == 'SimpleAutoencoder':
                capabilities_summary.append("Simple: Basic encoder-decoder")
            elif name == 'EnhancedAutoencoder':
                capabilities_summary.append("Enhanced: Advanced features")
            elif name == 'AutoencoderEnsemble':
                capabilities_summary.append("Ensemble: Multiple models")
        
        if capabilities_summary:
            logger.info(f"Available capabilities: {', '.join(capabilities_summary)}")

def validate_model_variants(logger: logging.Logger, silent: bool = False) -> Dict[str, str]:
    """
    Comprehensive validation of all registered model variants with enhanced testing.
    
    This function has been updated to fully leverage the existing helper functions while
    preserving ALL original validation capabilities including advanced scenario testing,
    memory analysis, model-specific feature validation, and comprehensive diagnostics.
    
    Args:
        logger: Logger instance for reporting validation results
        silent: If True, suppress detailed logging messages and progress bars during validation
        
    Returns:
        Dictionary mapping model names to their validation status
    """
    variant_status = {}
    validation_start_time = time.time()
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'models_tested': 0,
        'models_passed': 0,
        'models_failed': 0,
        'current_model': None,
        'current_test': None
    }
    
    # Phase 1: Check if MODEL_VARIANTS exists and initialize if needed
    progress_data['current_stage'] = "Initial Setup"
    
    with alive_bar(1, title='Validation Setup\t\t') as setup_bar:
        
        setup_bar.text = "Checking model variants..."
        
        if not MODEL_VARIANTS:
            if not silent:
                logger.warning("MODEL_VARIANTS is empty, attempting initialization")
            try:
                initialize_model_variants(silent=silent)
                setup_bar.text = "Model variants initialized"
            except Exception as e:
                error_msg = f'initialization_failed: {str(e)}'
                if not silent:
                    logger.error(f"Failed to initialize model variants for validation: {e}")
                setup_bar.text = "Initialization failed"
                return {'error': error_msg}
        
        if not MODEL_VARIANTS:
            error_msg = 'no_models_available'
            if not silent:
                logger.error("No model variants available after initialization attempt")
            setup_bar.text = "No models available"
            return {'error': error_msg}
        
        setup_bar.text = f"Found {len(MODEL_VARIANTS)} model variants"
        setup_bar()
    
    # Phase 2: Get hardware context for memory optimization
    progress_data['current_stage'] = "Hardware Check"
    
    with alive_bar(1, title='Hardware Check\t\t\t') as hardware_bar:
        
        hardware_bar.text = "Checking hardware..."
        
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
            
            # Initial memory cleanup for memory-constrained systems
            _optimize_memory_if_needed(
                condition=total_ram_gb < 8,
                hardware_data=hardware_data,
                aggressive=total_ram_gb < 4,
                silent=silent
            )
            hardware_bar.text = f"Hardware check complete ({total_ram_gb}GB RAM)"
        except Exception as e:
            if not silent:
                logger.debug(f"Hardware detection failed: {e}")
            hardware_data = {}
            total_ram_gb = 8.0
            hardware_bar.text = "Hardware check (using defaults)"
        
        hardware_bar()
    
    # Phase 3: Load configuration
    progress_data['current_stage'] = "Configuration Loading"
    
    with alive_bar(1, title='Loading Configuration\t\t') as config_bar:
        
        config_bar.text = "Loading configuration..."
        
        try:
            current_config = get_current_config()
            if not isinstance(current_config, dict):
                current_config = {}
            
            model_config = current_config.get('model', {})
            data_config = current_config.get('data', {})
            training_config = current_config.get('training', {})
            hardware_config = current_config.get('hardware', {})
            system_config = current_config.get('system', {})
            
            if not silent:
                logger.debug("Loaded configuration for validation testing")
            config_bar.text = "Configuration loaded"
                
        except Exception as e:
            if not silent:
                logger.warning(f"Could not load configuration for validation, using defaults: {e}")
            current_config = {}
            model_config = {}
            data_config = {}
            training_config = {}
            hardware_config = {}
            system_config = {}
            config_bar.text = "Configuration (using defaults)"
        
        config_bar()
    
    # Phase 4: Extract configuration parameters using helper functions
    progress_data['current_stage'] = "Parameter Extraction"
    
    with alive_bar(1, title='Extracting Parameters\t\t') as param_bar:
        
        param_bar.text = "Extracting parameters..."
        
        test_input_dim = _extract_and_validate_config_param(
            data_config, 'features', 20, 'FEATURES',
            lambda x: isinstance(x, int) and x > 0,
            "input feature dimension", silent
        )
        
        base_encoding_dim = _extract_and_validate_config_param(
            model_config, 'encoding_dim', 16, 'DEFAULT_ENCODING_DIM',
            lambda x: isinstance(x, int) and x > 0,
            "latent encoding dimension", silent
        )
        
        base_hidden_dims = _extract_and_validate_config_param(
            model_config, 'hidden_dims', [128, 64], 'HIDDEN_LAYER_SIZES',
            lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(d, int) and d > 0 for d in x),
            "hidden layer dimensions", silent
        )
        
        base_dropout_rates = _extract_and_validate_config_param(
            model_config, 'dropout_rates', [0.2, 0.15], 'DROPOUT_RATES',
            lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(r, (int, float)) and 0 <= r < 1 for r in x),
            "dropout rates", silent
        )
        
        # Additional parameters for validation
        activation = _extract_and_validate_config_param(
            model_config, 'activation', 'leaky_relu', 'ACTIVATION',
            lambda x: x in ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu', 'prelu'],
            "activation function", silent
        )
        
        activation_param = _extract_and_validate_config_param(
            model_config, 'activation_param', 0.2, 'ACTIVATION_PARAM',
            lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
            "activation parameter", silent
        )
        
        normalization = _extract_and_validate_config_param(
            model_config, 'normalization', 'batch', 'NORMALIZATION',
            lambda x: x in ['batch', 'layer', 'instance', 'group', 'none', None],
            "normalization type", silent
        )
        
        use_batch_norm = _extract_and_validate_config_param(
            model_config, 'use_batch_norm', True, 'USE_BATCH_NORM',
            lambda x: isinstance(x, bool),
            "batch normalization flag", silent
        )
        
        use_layer_norm = _extract_and_validate_config_param(
            model_config, 'use_layer_norm', False, 'USE_LAYER_NORM',
            lambda x: isinstance(x, bool),
            "layer normalization flag", silent
        )
        
        skip_connection = _extract_and_validate_config_param(
            model_config, 'skip_connection', True, 'SKIP_CONNECTION',
            lambda x: isinstance(x, bool),
            "skip connections flag", silent
        )
        
        residual_blocks = _extract_and_validate_config_param(
            model_config, 'residual_blocks', False, 'RESIDUAL_BLOCKS',
            lambda x: isinstance(x, bool),
            "residual blocks flag", silent
        )
        
        use_attention = _extract_and_validate_config_param(
            model_config, 'use_attention', False, 'USE_ATTENTION',
            lambda x: isinstance(x, bool),
            "attention mechanism flag", silent
        )
        
        num_models = _extract_and_validate_config_param(
            model_config, 'num_models', 3, 'NUM_MODELS',
            lambda x: isinstance(x, int) and 1 <= x <= 10,
            "ensemble size", silent
        )
        
        diversity_factor = _extract_and_validate_config_param(
            model_config, 'diversity_factor', 0.2, 'DIVERSITY_FACTOR',
            lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
            "ensemble diversity factor", silent
        )
        
        mixed_precision = _extract_and_validate_config_param(
            training_config, 'mixed_precision', False, 'MIXED_PRECISION',
            lambda x: isinstance(x, bool),
            "mixed precision training", silent
        )
        
        learning_rate = _extract_and_validate_config_param(
            training_config, 'learning_rate', 0.001, 'LEARNING_RATE',
            lambda x: isinstance(x, (int, float)) and x > 0,
            "learning rate", silent
        )
        
        device_setting = _extract_and_validate_config_param(
            hardware_config, 'device', 'auto', 'DEVICE',
            lambda x: isinstance(x, str) and x in ['auto', 'cpu', 'cuda'] or x.startswith('cuda:'),
            "compute device", silent
        )
        
        random_seed = _extract_and_validate_config_param(
            system_config, 'random_seed', 42, 'RANDOM_SEED',
            lambda x: isinstance(x, int),
            "random seed", silent
        )
        
        legacy_mode = _extract_and_validate_config_param(
            model_config, 'legacy_mode', False, 'LEGACY_MODE',
            lambda x: isinstance(x, bool),
            "legacy compatibility mode", silent
        )
        
        # Phase 5: Validate and adjust parameters
        base_hidden_dims, base_dropout_rates = _validate_and_adjust_parameters(
            base_hidden_dims, base_dropout_rates, silent
        )
        
        param_bar.text = "Parameters validated"
        param_bar()
    
    # Phase 6: Create model test definitions with ALL required parameters
    progress_data['current_stage'] = "Creating Test Definitions"
    
    with alive_bar(1, title='Creating Test Definitions\t') as def_bar:
        
        def_bar.text = "Creating model definitions..."
        
        # Create model test definitions with ALL required parameters
        model_definitions = _create_model_test_definition(
            encoding_dim=base_encoding_dim,
            hidden_dims=base_hidden_dims,
            dropout_rates=base_dropout_rates,
            use_attention=use_attention,
            residual_blocks=residual_blocks,
            skip_connection=skip_connection,
            legacy_mode=legacy_mode,
            num_models=num_models,
            diversity_factor=diversity_factor,
            mixed_precision=mixed_precision,
            # ADDED: Pass all the missing parameters that are now required
            input_dim=test_input_dim,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm
        )
        
        # Update configurations with validation-specific input dimension (redundant but safe)
        for definition in model_definitions.values():
            for config_type in ['primary_config', 'fallback_config', 'minimal_config']:
                if config_type in definition:
                    definition[config_type]['data']['features'] = test_input_dim
                    definition[config_type]['model']['input_dim'] = test_input_dim
        
        def_bar.text = f"Created {len(model_definitions)} test definitions"
        def_bar()
    
    # Define validation test scenarios
    validation_scenarios = [
        {
            'name': 'batch_norm_compatible',
            'batch_size': 4,
            'description': 'Batch normalization compatibility test'
        },
        {
            'name': 'single_sample',
            'batch_size': 1,
            'description': 'Single sample inference test'
        },
        {
            'name': 'small_batch',
            'batch_size': 2,
            'description': 'Small batch processing test'
        },
        {
            'name': 'medium_batch',
            'batch_size': 8,
            'description': 'Medium batch processing test'
        },
        {
            'name': 'large_batch',
            'batch_size': 16,
            'description': 'Large batch processing test'
        }
    ]
    
    # Phase 7: Initialize validation statistics
    validation_stats = {
        'models_attempted': 0,
        'models_successful': 0,
        'models_failed': 0,
        'models_warning': 0,
        'total_tests_performed': 0,
        'total_tests_passed': 0,
        'total_tests_failed': 0,
        'configuration_tests_passed': 0,
        'architecture_tests_passed': 0,
        'functionality_tests_passed': 0,
        'performance_tests_passed': 0,
        'robustness_tests_passed': 0,
        'memory_tests_passed': 0,
        'device_compatibility_tests': 0,
        'memory_optimizations_performed': 0,
        'detailed_metrics': {},
        'available_variants': [],
        'warning_variants': [],
        'failed_variants': []
    }
    
    # Status symbols for visual feedback
    status_symbols = {
        'success': '[OK]',
        'failure': '[FAIL]',
        'warning': '[WARN]',
        'skip': '[SKIP]'
    }
    
    # Phase 8: Validate each model variant
    progress_data['current_stage'] = "Model Validation"
    total_models = len(MODEL_VARIANTS)
    
    with alive_bar(total_models, title='Validating Models\t') as model_bar:
        
        for variant_name, variant_class in MODEL_VARIANTS.items():
            model_validation_start = time.time()
            validation_stats['models_attempted'] += 1
            progress_data['current_model'] = variant_name
            
            # Update progress bar with current model info
            passed_count = validation_stats['models_successful']
            failed_count = validation_stats['models_failed']
            model_bar.text = f"Testing {variant_name}... ({passed_count} passed, {failed_count} failed)"
            
            # Initialize detailed tracking for this variant
            overall_status = 'available'
            variant_details = {
                'class_name': variant_class.__name__ if variant_class else 'None',
                'tests_performed': [],
                'tests_passed': [],
                'tests_failed': [],
                'errors': [],
                'warnings': [],
                'performance_metrics': {},
                'compatibility_results': {},
                'configuration_validation': {},
                'memory_usage': {}
            }
            
            try:
                if not silent:
                    logger.debug(f"Starting validation for {variant_name} using helper functions")
                
                # Test 1: Class Availability and Callability
                if variant_class is None:
                    variant_status[variant_name] = 'class_not_found'
                    variant_details['errors'].append('Model class is None')
                    validation_stats['failed_variants'].append(variant_name)
                    validation_stats['models_failed'] += 1
                    model_bar.text = f"{status_symbols['failure']} {variant_name} class not found"
                    model_bar()
                    continue
                
                if not callable(variant_class):
                    variant_status[variant_name] = 'class_not_callable'
                    variant_details['errors'].append('Model class is not callable')
                    validation_stats['failed_variants'].append(variant_name)
                    validation_stats['models_failed'] += 1
                    model_bar.text = f"{status_symbols['failure']} {variant_name} not callable"
                    model_bar()
                    continue
                
                variant_details['tests_performed'].append('class_availability')
                variant_details['tests_passed'].append('class_availability')
                validation_stats['total_tests_performed'] += 1
                validation_stats['total_tests_passed'] += 1
                
                # Get the appropriate test definition for this model
                if variant_name in model_definitions:
                    definition = model_definitions[variant_name]
                else:
                    # Create adaptive definition for unknown model types
                    if not silent:
                        logger.debug(f"Creating adaptive configuration for unknown model type: {variant_name}")
                    definition = {
                        'primary_config': _create_adaptive_config(
                            model_name=variant_name,
                            model_class=variant_class,
                            system_class='validation',
                            input_dim=test_input_dim,
                            encoding_dim=base_encoding_dim,
                            hidden_dims=base_hidden_dims,
                            dropout_rates=base_dropout_rates,
                            activation=activation,
                            activation_param=activation_param,
                            normalization=normalization,
                            use_batch_norm=use_batch_norm,
                            use_layer_norm=use_layer_norm,
                            skip_connection=skip_connection,
                            residual_blocks=residual_blocks,
                            use_attention=use_attention,
                            legacy_mode=legacy_mode,
                            num_models=num_models,
                            diversity_factor=diversity_factor,
                            learning_rate=learning_rate,
                            batch_size=32,
                            mixed_precision=mixed_precision,
                            optimizer_type='AdamW',
                            device_setting=device_setting,
                            random_seed=random_seed,
                            hardware_data=hardware_data
                        ),
                        'fallback_config': _create_adaptive_config(
                            model_name=variant_name,
                            model_class=variant_class,
                            system_class='fallback',
                            input_dim=test_input_dim,
                            encoding_dim=max(4, base_encoding_dim // 2),
                            hidden_dims=[64],
                            dropout_rates=[0.2],
                            activation='relu',
                            activation_param=0.0,
                            normalization=None,
                            use_batch_norm=False,
                            use_layer_norm=False,
                            skip_connection=False,
                            residual_blocks=False,
                            use_attention=False,
                            legacy_mode=True,
                            num_models=1,
                            diversity_factor=0.0,
                            learning_rate=0.001,
                            batch_size=32,
                            mixed_precision=False,
                            optimizer_type='Adam',
                            device_setting='cpu',
                            random_seed=42,
                            hardware_data=hardware_data
                        ),
                        'minimal_config': _create_adaptive_config(
                            model_name=variant_name,
                            model_class=variant_class,
                            system_class='minimal',
                            input_dim=test_input_dim,
                            encoding_dim=8,
                            hidden_dims=[32],
                            dropout_rates=[0.1],
                            activation='relu',
                            activation_param=0.0,
                            normalization=None,
                            use_batch_norm=False,
                            use_layer_norm=False,
                            skip_connection=False,
                            residual_blocks=False,
                            use_attention=False,
                            legacy_mode=True,
                            num_models=1,
                            diversity_factor=0.0,
                            learning_rate=0.001,
                            batch_size=32,
                            mixed_precision=False,
                            optimizer_type='Adam',
                            device_setting='cpu',
                            random_seed=42,
                            hardware_data=hardware_data
                        )
                    }
                
                # Test 2: Configuration Creation
                variant_details['configuration_validation']['config_created'] = True
                variant_details['tests_performed'].append('configuration_creation')
                variant_details['tests_passed'].append('configuration_creation')
                validation_stats['total_tests_performed'] += 1
                validation_stats['total_tests_passed'] += 1
                validation_stats['configuration_tests_passed'] += 1
                
                # Use the model_instantiation_with_validation helper function
                test_instance, validation_results, performance_metrics, instantiation_details = model_instantiation_with_validation(
                    variant_class=variant_class,
                    variant_name=variant_name,
                    input_dim=test_input_dim,
                    base_config=definition['primary_config'],
                    fallback_config=definition.get('fallback_config'),
                    minimal_config=definition.get('minimal_config'),
                    validation_tests=['basic', 'forward_pass', 'parameters', 'config_methods', 'training_mode'],
                    comprehensive_validation=True,
                    hardware_data=hardware_data,
                    silent=silent,
                    logger=logger
                )
                
                # Process results
                instantiation_method = instantiation_details.get('method', 'failed')
                validation_score = validation_results.get('overall_score', 0)
                test_results = validation_results.get('test_results', {})
                warnings = validation_results.get('warnings', [])
                errors = validation_results.get('errors', [])
                
                # Track instantiation method
                if instantiation_method == 'fallback':
                    variant_details['warnings'].append('Used fallback configuration')
                elif instantiation_method == 'minimal':
                    variant_details['warnings'].append('Used minimal configuration')
                elif instantiation_method == 'adaptive':
                    variant_details['warnings'].append('Used adaptive configuration')
                
                # Add all results
                variant_details['warnings'].extend(warnings)
                variant_details['errors'].extend(errors)
                variant_details['performance_metrics'].update(performance_metrics)
                variant_details['configuration_validation'].update(performance_metrics.get('config_validation', {}))
                variant_details['instantiation_method'] = instantiation_method
                
                # Track models for cleanup
                test_models = []
                if test_instance is not None:
                    test_models.append(test_instance)
                    
                    # Record instantiation success
                    variant_details['tests_performed'].append(f'instantiation_{instantiation_method}')
                    variant_details['tests_passed'].append(f'instantiation_{instantiation_method}')
                    validation_stats['total_tests_performed'] += 1
                    validation_stats['total_tests_passed'] += 1
                    
                    # Process validation results
                    for test_name, test_result in test_results.items():
                        variant_details['tests_performed'].append(test_name)
                        if test_result == 'passed':
                            variant_details['tests_passed'].append(test_name)
                            validation_stats['total_tests_passed'] += 1
                        else:
                            variant_details['tests_failed'].append(test_name)
                            validation_stats['total_tests_failed'] += 1
                        validation_stats['total_tests_performed'] += 1
                    
                    # Update validation statistics based on helper results
                    if test_results.get('basic') == 'passed':
                        validation_stats['architecture_tests_passed'] += 1
                    if test_results.get('forward_pass') == 'passed':
                        validation_stats['functionality_tests_passed'] += 1
                    if test_results.get('parameters') == 'passed':
                        validation_stats['performance_tests_passed'] += 1
                    if test_results.get('config_methods') == 'passed':
                        validation_stats['configuration_tests_passed'] += 1
                    if test_results.get('comprehensive') == 'passed':
                        validation_stats['robustness_tests_passed'] += 1
                    
                    if not silent:
                        logger.debug(f"{status_symbols['success']} {variant_name}: Comprehensive validation completed with {instantiation_method} configuration")
                        
                else:
                    # Instantiation failed
                    variant_details['tests_performed'].append('instantiation')
                    variant_details['tests_failed'].append('instantiation')
                    validation_stats['total_tests_performed'] += 1
                    validation_stats['total_tests_failed'] += 1
                    validation_stats['models_failed'] += 1
                    
                    if not silent:
                        logger.error(f"{status_symbols['failure']} {variant_name}: Instantiation failed")
                    model_bar.text = f"{status_symbols['failure']} {variant_name} instantiation failed"
                    model_bar()
                    continue
                
                # Memory optimization after model instantiation
                _optimize_memory_if_needed(
                    condition=True,
                    hardware_data=hardware_data,
                    aggressive=total_ram_gb < 4,
                    silent=silent
                )
                
                # Additional specialized tests beyond the helper function
                if test_instance is not None:
                    # Test: Advanced Forward Pass Scenarios
                    try:
                        scenario_results = {}
                        for scenario in validation_scenarios:
                            try:
                                test_instance.eval()
                                batch_size = scenario['batch_size']
                                
                                # Adjust for batch norm requirements
                                test_config = definition['primary_config']
                                if ((test_config['model'].get('use_batch_norm', False) or 
                                     test_config['model'].get('normalization') == 'batch') and 
                                     batch_size == 1):
                                    batch_size = 2
                                
                                test_input = torch.randn(batch_size, test_input_dim)
                                if hasattr(test_instance, 'device'):
                                    test_input = test_input.to(test_instance.device)
                                
                                with torch.no_grad():
                                    output = test_instance(test_input)
                                
                                expected_shape = (batch_size, test_input_dim)
                                if output.shape == expected_shape:
                                    scenario_results[scenario['name']] = 'passed'
                                else:
                                    scenario_results[scenario['name']] = 'failed'
                                    variant_details['warnings'].append(f"Shape mismatch in {scenario['name']}")
                            except Exception as scenario_error:
                                scenario_results[scenario['name']] = 'failed'
                                variant_details['warnings'].append(f"{scenario['name']} failed: {str(scenario_error)}")
                        
                        # Record scenario results
                        passed_scenarios = sum(1 for result in scenario_results.values() if result == 'passed')
                        total_scenarios = len(scenario_results)
                        
                        variant_details['performance_metrics']['scenario_testing'] = scenario_results
                        variant_details['tests_performed'].append('advanced_scenarios')
                        
                        if passed_scenarios == total_scenarios:
                            variant_details['tests_passed'].append('advanced_scenarios')
                            validation_stats['total_tests_passed'] += 1
                        else:
                            variant_details['tests_failed'].append('advanced_scenarios')
                            validation_stats['total_tests_failed'] += 1
                        validation_stats['total_tests_performed'] += 1
                        
                    except Exception as scenario_test_error:
                        variant_details['warnings'].append(f"Advanced scenario testing failed: {str(scenario_test_error)}")
                    
                    # Test: Memory Usage Analysis
                    try:
                        # Use psutil if available for memory monitoring
                        try:
                            process = psutil.Process()
                            memory_before = process.memory_info().rss
                            
                            memory_test_input = torch.randn(32, test_input_dim)
                            if hasattr(test_instance, 'device'):
                                memory_test_input = memory_test_input.to(test_instance.device)
                            
                            test_instance.eval()
                            with torch.no_grad():
                                _ = test_instance(memory_test_input)
                            
                            memory_after = process.memory_info().rss
                            memory_used_mb = (memory_after - memory_before) / (1024 * 1024)
                            
                            variant_details['memory_usage']['inference_memory_mb'] = memory_used_mb
                            
                            if memory_used_mb > 1000:
                                variant_details['warnings'].append(f'High memory usage: {memory_used_mb:.1f} MB')
                            
                            variant_details['tests_performed'].append('memory_analysis')
                            variant_details['tests_passed'].append('memory_analysis')
                            validation_stats['total_tests_performed'] += 1
                            validation_stats['total_tests_passed'] += 1
                            validation_stats['memory_tests_passed'] += 1
                            
                        except ImportError:
                            variant_details['memory_usage']['psutil_available'] = False
                            variant_details['warnings'].append('psutil not available for detailed memory analysis')
                        
                    except Exception as memory_error:
                        variant_details['warnings'].append(f'Memory analysis failed: {str(memory_error)}')
                    
                    # Test: Model-Specific Feature Validation
                    if variant_name == 'EnhancedAutoencoder' and test_instance is not None:
                        try:
                            enhanced_features = []
                            if hasattr(test_instance, 'attention') and test_instance.attention is not None:
                                enhanced_features.append('attention')
                            if hasattr(test_instance, 'skip_layers'):
                                enhanced_features.append('skip_connections')
                            test_config = definition.get('primary_config', {})
                            if test_config.get('model', {}).get('residual_blocks', False):
                                enhanced_features.append('residual_blocks')
                            
                            variant_details['compatibility_results']['enhanced_features'] = enhanced_features
                            variant_details['tests_performed'].append('enhanced_features')
                            variant_details['tests_passed'].append('enhanced_features')
                            validation_stats['total_tests_performed'] += 1
                            validation_stats['total_tests_passed'] += 1
                            
                        except Exception as enhanced_error:
                            variant_details['warnings'].append(f'Enhanced features validation failed: {str(enhanced_error)}')
                    
                    elif variant_name == 'AutoencoderEnsemble' and test_instance is not None:
                        try:
                            if hasattr(test_instance, 'models'):
                                ensemble_size = len(test_instance.models)
                                variant_details['compatibility_results']['ensemble_size'] = ensemble_size
                                
                                test_config = definition.get('primary_config', {})
                                expected_models = test_config.get('model', {}).get('num_models', 3)
                                
                                if ensemble_size == 0:
                                    variant_details['errors'].append('Empty ensemble')
                                elif ensemble_size < expected_models:
                                    variant_details['warnings'].append(f'Partial ensemble: {ensemble_size} models')
                                
                                variant_details['tests_performed'].append('ensemble_features')
                                variant_details['tests_passed'].append('ensemble_features')
                                validation_stats['total_tests_performed'] += 1
                                validation_stats['total_tests_passed'] += 1
                            
                        except Exception as ensemble_error:
                            variant_details['warnings'].append(f'Ensemble features validation failed: {str(ensemble_error)}')
                
                # Calculate model validation time
                model_validation_time = time.time() - model_validation_start
                variant_details['performance_metrics']['validation_time_seconds'] = model_validation_time
                
                # Determine final status based on results
                error_count = len(variant_details['errors'])
                warning_count = len(variant_details['warnings'])
                tests_passed = len(variant_details['tests_passed'])
                tests_total = len(variant_details['tests_performed'])
                
                if error_count > 0:
                    overall_status = 'error'
                    validation_stats['models_failed'] += 1
                    validation_stats['failed_variants'].append(variant_name)
                    model_bar.text = f"{status_symbols['failure']} {variant_name} ({error_count} errors)"
                elif warning_count > 0 or validation_score < 0.8:
                    overall_status = 'warning'
                    validation_stats['models_warning'] += 1
                    validation_stats['warning_variants'].append(variant_name)
                    model_bar.text = f"{status_symbols['warning']} {variant_name} ({warning_count} warnings)"
                else:
                    overall_status = 'available'
                    validation_stats['models_successful'] += 1
                    validation_stats['available_variants'].append(variant_name)
                    model_bar.text = f"{status_symbols['success']} {variant_name} ({tests_passed}/{tests_total} tests)"
                
                # Create detailed status message
                status_msg = overall_status
                if overall_status != 'available':
                    details = []
                    if error_count > 0:
                        details.append(f"{error_count} errors")
                    if warning_count > 0:
                        details.append(f"{warning_count} warnings")
                    if details:
                        status_msg += f": {', '.join(details)}"
                    status_msg += f" ({validation_score:.1%} score)"
                
                variant_status[variant_name] = status_msg
                
                # Store detailed metrics for reporting
                validation_stats['detailed_metrics'][variant_name] = {
                    'instantiation_method': instantiation_method,
                    'validation_score': validation_score,
                    'tests_passed': tests_passed,
                    'tests_total': tests_total,
                    'performance_metrics': variant_details['performance_metrics'],
                    'validation_time_seconds': model_validation_time
                }
                
                # Cleanup test model and apply memory optimization
                if test_instance is not None:
                    del test_instance
                    
                # Memory optimization after each model test
                if _optimize_memory_if_needed(
                    condition=True,
                    hardware_data=hardware_data,
                    aggressive=total_ram_gb < 4,
                    silent=silent
                ):
                    validation_stats['memory_optimizations_performed'] += 1
                
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
                if not silent:
                    logger.info(f"Model {variant_name} validation completed: {status_msg} "
                               f"({tests_passed}/{tests_total} tests passed, "
                               f"{instantiation_method} config, "
                               f"{model_validation_time:.2f}s)")
                
            except Exception as e:
                error_msg = f'validation_error: {str(e)}'
                variant_status[variant_name] = error_msg
                validation_stats['failed_variants'].append(variant_name)
                validation_stats['models_failed'] += 1
                model_bar.text = f"{status_symbols['failure']} {variant_name} unexpected error"
                
                if not silent:
                    logger.error(f"Model variant {variant_name} validation failed with unexpected error: {e}")
            
            finally:
                # Store detailed results
                validation_stats[variant_name] = variant_details
                # Additional cleanup
                try:
                    if 'test_models' in locals():
                        for model in test_models:
                            del model
                    if 'test_instance' in locals():
                        del test_instance
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
                except Exception as cleanup_error:
                    if not silent:
                        logger.debug(f"Cleanup warning for {variant_name}: {cleanup_error}")
            
            # Update progress bar
            model_bar()
        
        # Final update for model validation
        passed_count = validation_stats['models_successful']
        failed_count = validation_stats['models_failed']
        model_bar.text = f"Validation complete: {passed_count} passed, {failed_count} failed"
    
    # Phase 9: Finalization
    progress_data['current_stage'] = "Finalizing"
    
    with alive_bar(1, title='Finalizing\t\t') as final_bar:
        
        final_bar.text = "Generating summary..."
        
        total_validation_time = time.time() - validation_start_time
        
        # Final memory cleanup
        _optimize_memory_if_needed(
            condition=True,
            hardware_data=hardware_data,
            aggressive=True,
            silent=silent
        )
        
        # Phase 10: Log validation summary
        if not silent:
            logger.info("="*70)
            logger.info("MODEL VARIANTS VALIDATION SUMMARY")
            logger.info("="*70)
            logger.info(f"Total Validation Time: {total_validation_time:.2f} seconds")
            logger.info(f"Models Attempted: {validation_stats['models_attempted']}")
            logger.info(f"Models Available: {validation_stats['models_successful']}")
            logger.info(f"Models with Warnings: {validation_stats['models_warning']}")
            logger.info(f"Models Failed: {validation_stats['models_failed']}")
            logger.info("-"*70)
            logger.info(f"Total Tests Performed: {validation_stats['total_tests_performed']}")
            logger.info(f"Total Tests Passed: {validation_stats['total_tests_passed']}")
            logger.info(f"Total Tests Failed: {validation_stats['total_tests_failed']}")
            success_rate = validation_stats['total_tests_passed']/max(1, validation_stats['total_tests_performed'])*100
            logger.info(f"Test Success Rate: {success_rate:.1f}%")
            logger.info("-"*70)
            logger.info(f"Configuration Tests Passed: {validation_stats['configuration_tests_passed']}")
            logger.info(f"Architecture Tests Passed: {validation_stats['architecture_tests_passed']}")
            logger.info(f"Functionality Tests Passed: {validation_stats['functionality_tests_passed']}")
            logger.info(f"Performance Tests Passed: {validation_stats['performance_tests_passed']}")
            logger.info(f"Robustness Tests Passed: {validation_stats['robustness_tests_passed']}")
            logger.info(f"Memory Tests Passed: {validation_stats['memory_tests_passed']}")
            logger.info(f"Device Compatibility Tests: {validation_stats['device_compatibility_tests']}")
            logger.info(f"Memory Optimizations Performed: {validation_stats['memory_optimizations_performed']}")
            
            # Log detailed metrics
            if validation_stats['detailed_metrics']:
                logger.info("-"*70)
                logger.info("DETAILED METRICS:")
                for model_name, metrics in validation_stats['detailed_metrics'].items():
                    score = metrics['validation_score']
                    tests_passed = metrics['tests_passed']
                    tests_total = metrics['tests_total']
                    method = metrics['instantiation_method']
                    logger.info(f"  {model_name}: {score:.1%} score, {tests_passed}/{tests_total} tests, {method} config")
            
            # Log individual model results
            logger.info("="*70)
            for model_name, status in variant_status.items():
                status_icon = "OK" if status == "available" else "WARN" if status.startswith("warning") else "FAIL"
                logger.info(f"{status_icon} {model_name}: {status}")
            
            # Log recommendations
            logger.info("="*70)
            available_models = [name for name, status in variant_status.items() if status == 'available']
            warning_models = [name for name, status in variant_status.items() if status.startswith('warning')]
            failed_models = [name for name, status in variant_status.items() if status.startswith('error') or 'failed' in status]
            
            if available_models:
                logger.info(f"RECOMMENDED MODELS: {', '.join(available_models)}")
            if warning_models:
                logger.info(f"MODELS WITH WARNINGS: {', '.join(warning_models)}")
            if failed_models:
                logger.info(f"FAILED MODELS: {', '.join(failed_models)}")
            
            logger.info("="*70)
        
        final_bar.text = "Validation complete!"
        final_bar()
    
    return variant_status

def model_instantiation(
    variant_class: Any,
    variant_name: str,
    input_dim: int,
    base_config: Dict[str, Any],
    fallback_config: Dict[str, Any] = None,
    minimal_config: Dict[str, Any] = None,
    hardware_data: Dict[str, Any] = None,
    silent: bool = False,
    logger: logging.Logger = None
) -> Tuple[Any, str, List[str], List[str], Dict[str, Any]]:
    """
    Enhanced helper function for model instantiation with comprehensive error handling and fallback support.
    
    This function can be used by validate_model_variants(), initialize_model_variants() and other functions 
    that need to instantiate model variants with robust error handling and configuration fallbacks.
    
    Args:
        variant_class: The model class to instantiate
        variant_name: Name of the model variant for logging
        input_dim: Input dimension for the model
        base_config: Primary configuration to try first
        fallback_config: Fallback configuration if primary fails (optional)
        minimal_config: Minimal configuration as last resort (optional)
        hardware_data: Hardware information for adaptive configuration (optional)
        silent: If True, suppress detailed logging
        logger: Logger instance for reporting results
        
    Returns:
        Tuple containing:
        - model_instance: Instantiated model or None if failed
        - instantiation_method: 'primary', 'fallback', 'minimal', 'adaptive', or 'failed'
        - warnings: List of warning messages
        - errors: List of error messages
        - config_used: The configuration that was successfully used
    """
    test_models = []
    warnings = []
    errors = []
    instantiation_method = 'failed'
    model_instance = None
    config_used = None
    
    if logger is None:
        # Create a basic logger if none provided
        logger = logging.getLogger(__name__)
    
    if hardware_data is None:
        hardware_data = {}
    
    # Try primary configuration first
    try:
        model_instance = variant_class(
            input_dim=input_dim,
            config=base_config
        )
        test_models.append(model_instance)
        instantiation_method = 'primary'
        config_used = base_config
        
        if not silent:
            logger.debug(f"-OK- {variant_name}: Instantiated with primary configuration")
            
    except Exception as primary_error:
        primary_error_msg = str(primary_error)
        if not silent:
            logger.debug(f"{variant_name}: Primary configuration failed: {primary_error_msg}")
        
        # Try fallback configuration if provided
        if fallback_config is not None:
            try:
                model_instance = variant_class(
                    input_dim=input_dim,
                    config=fallback_config
                )
                test_models.append(model_instance)
                instantiation_method = 'fallback'
                config_used = fallback_config
                warnings.append(f'Required fallback configuration: {primary_error_msg}')
                
                if not silent:
                    logger.info(f"[WARN] {variant_name}: Required fallback configuration")
                    
            except Exception as fallback_error:
                fallback_error_msg = str(fallback_error)
                if not silent:
                    logger.debug(f"{variant_name}: Fallback configuration failed: {fallback_error_msg}")
        
        # Try minimal configuration if provided and fallback failed
        if model_instance is None and minimal_config is not None:
            try:
                model_instance = variant_class(
                    input_dim=input_dim,
                    config=minimal_config
                )
                test_models.append(model_instance)
                instantiation_method = 'minimal'
                config_used = minimal_config
                warnings.append(f'Required minimal configuration: {primary_error_msg}')
                
                if not silent:
                    logger.info(f"[WARN] {variant_name}: Required minimal configuration")
                    
            except Exception as minimal_error:
                minimal_error_msg = str(minimal_error)
                if not silent:
                    logger.debug(f"{variant_name}: Minimal configuration failed: {minimal_error_msg}")
        
        # If all provided configurations failed, try adaptive configuration
        if model_instance is None:
            try:
                adaptive_config = _create_adaptive_config(
                    model_name=variant_name,
                    model_class=variant_class,
                    system_class='emergency',
                    input_dim=input_dim,
                    encoding_dim=max(4, base_config.get('model', {}).get('encoding_dim', 16) // 2),
                    hidden_dims=base_config.get('model', {}).get('hidden_dims', [64]),
                    dropout_rates=base_config.get('model', {}).get('dropout_rates', [0.2]),
                    activation=base_config.get('model', {}).get('activation', 'relu'),
                    activation_param=base_config.get('model', {}).get('activation_param', 0.0),
                    normalization=base_config.get('model', {}).get('normalization', None),
                    use_batch_norm=False,
                    use_layer_norm=False,
                    skip_connection=False,
                    residual_blocks=False,
                    use_attention=False,
                    legacy_mode=True,
                    num_models=1,
                    diversity_factor=0.0,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=False,
                    optimizer_type='Adam',
                    device_setting='cpu',
                    random_seed=42,
                    hardware_data=hardware_data
                )
                
                model_instance = variant_class(
                    input_dim=input_dim,
                    config=adaptive_config
                )
                test_models.append(model_instance)
                instantiation_method = 'adaptive'
                config_used = adaptive_config
                warnings.append(f'Required adaptive configuration: {primary_error_msg}')
                
                if not silent:
                    logger.info(f"[WARN] {variant_name}: Required adaptive configuration")
                    
            except Exception as adaptive_error:
                adaptive_error_msg = str(adaptive_error)
                error_details = [f'Primary: {primary_error_msg}']
                if fallback_config is not None:
                    error_details.append(f'Fallback: {fallback_error_msg}')
                if minimal_config is not None:
                    error_details.append(f'Minimal: {minimal_error_msg}')
                error_details.append(f'Adaptive: {adaptive_error_msg}')
                
                errors.append(', '.join(error_details))
                
                if not silent:
                    logger.error(f"[FAIL] {variant_name}: All instantiation attempts failed")
    
    return model_instance, instantiation_method, warnings, errors, config_used

def model_instantiation_with_validation(
    variant_class: Any,
    variant_name: str,
    input_dim: int,
    base_config: Dict[str, Any],
    fallback_config: Dict[str, Any] = None,
    minimal_config: Dict[str, Any] = None,
    validation_tests: List[str] = None,
    comprehensive_validation: bool = False,
    hardware_data: Dict[str, Any] = None,
    silent: bool = False,
    logger: logging.Logger = None
) -> Tuple[Any, Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    """
    Enhanced model instantiation with built-in validation tests optimized for initialize_model_variants().
    
    This function combines instantiation with comprehensive validation and is useful for
    functions that need both instantiation and immediate validation, especially initialize_model_variants().
    
    Args:
        variant_class: The model class to instantiate
        variant_name: Name of the model variant for logging
        input_dim: Input dimension for the model
        base_config: Primary configuration to try
        fallback_config: Fallback configuration if primary fails (optional)
        minimal_config: Minimal configuration as last resort (optional)
        validation_tests: List of validation tests to perform
        comprehensive_validation: If True, perform comprehensive validation similar to validate_model_variants()
        hardware_data: Hardware information for adaptive configuration
        silent: If True, suppress detailed logging
        logger: Logger instance for reporting results
        
    Returns:
        Tuple containing:
        - model_instance: Instantiated model or None if failed
        - validation_results: Dictionary with test results
        - performance_metrics: Dictionary with performance metrics
        - instantiation_details: Dictionary with instantiation details
    """
    if validation_tests is None:
        validation_tests = ['basic', 'forward_pass', 'parameters', 'config_methods', 'training_mode']
    
    if logger is None:
        logger = logging.getLogger(__name__)
    
    validation_results = {}
    performance_metrics = {}
    instantiation_details = {}
    
    # Instantiate the model with enhanced error handling
    model_instance, method, warnings, errors, config_used = model_instantiation(
        variant_class=variant_class,
        variant_name=variant_name,
        input_dim=input_dim,
        base_config=base_config,
        fallback_config=fallback_config,
        minimal_config=minimal_config,
        hardware_data=hardware_data,
        silent=silent,
        logger=logger
    )
    
    instantiation_details['method'] = method
    instantiation_details['warnings'] = warnings
    instantiation_details['errors'] = errors
    instantiation_details['config_used'] = config_used
    instantiation_details['successful'] = model_instance is not None
    
    validation_results['instantiation_method'] = method
    validation_results['warnings'] = warnings
    validation_results['errors'] = errors
    validation_results['instantiation_successful'] = model_instance is not None
    
    if model_instance is None:
        return None, validation_results, performance_metrics, instantiation_details
    
    # Perform validation tests
    test_results = {}
    
    if 'basic' in validation_tests:
        try:
            # Basic model structure validation
            total_params = sum(p.numel() for p in model_instance.parameters())
            trainable_params = sum(p.numel() for p in model_instance.parameters() if p.requires_grad)
            
            performance_metrics['total_parameters'] = total_params
            performance_metrics['trainable_parameters'] = trainable_params
            
            if total_params == 0:
                validation_results['errors'].append('Model has no parameters')
                test_results['basic'] = 'failed'
            elif trainable_params == 0:
                validation_results['warnings'].append('Model has no trainable parameters')
                test_results['basic'] = 'warning'
            else:
                test_results['basic'] = 'passed'
            
        except Exception as e:
            validation_results['errors'].append(f'Basic validation failed: {str(e)}')
            test_results['basic'] = 'failed'
    
    if 'forward_pass' in validation_tests and model_instance is not None:
        try:
            # Basic forward pass test with different batch sizes
            model_instance.eval()
            
            # Test with different batch sizes for robustness
            batch_sizes = [1, 2, 4] if not comprehensive_validation else [1, 2, 4, 8, 16]
            forward_pass_results = {}
            
            for batch_size in batch_sizes:
                try:
                    test_input = torch.randn(batch_size, input_dim)
                    
                    if hasattr(model_instance, 'device'):
                        test_input = test_input.to(model_instance.device)
                    
                    with torch.no_grad():
                        output = model_instance(test_input)
                    
                    expected_shape = (batch_size, input_dim)
                    if output.shape == expected_shape:
                        forward_pass_results[f'batch_{batch_size}'] = 'passed'
                        
                        # Check for numerical issues
                        if torch.isnan(output).any():
                            validation_results['warnings'].append(f'NaN values in output (batch_size={batch_size})')
                            forward_pass_results[f'batch_{batch_size}'] = 'warning'
                        if torch.isinf(output).any():
                            validation_results['errors'].append(f'Infinite values in output (batch_size={batch_size})')
                            forward_pass_results[f'batch_{batch_size}'] = 'failed'
                    else:
                        validation_results['errors'].append(f'Shape mismatch: {output.shape} vs {expected_shape} (batch_size={batch_size})')
                        forward_pass_results[f'batch_{batch_size}'] = 'failed'
                        
                except Exception as batch_error:
                    validation_results['warnings'].append(f'Forward pass failed for batch_size={batch_size}: {str(batch_error)}')
                    forward_pass_results[f'batch_{batch_size}'] = 'failed'
            
            # Determine overall forward pass result
            if all(result == 'passed' for result in forward_pass_results.values()):
                test_results['forward_pass'] = 'passed'
            elif any(result == 'failed' for result in forward_pass_results.values()):
                test_results['forward_pass'] = 'failed'
            else:
                test_results['forward_pass'] = 'warning'
                
            performance_metrics['forward_pass_results'] = forward_pass_results
                
        except Exception as e:
            validation_results['errors'].append(f'Forward pass test failed: {str(e)}')
            test_results['forward_pass'] = 'failed'
    
    if 'parameters' in validation_tests and model_instance is not None:
        try:
            # Parameter validation with detailed statistics
            param_stats = {}
            total_params = 0
            has_nan_params = False
            has_inf_params = False
            
            for name, param in model_instance.named_parameters():
                param_stats[name] = {
                    'shape': list(param.shape),
                    'numel': param.numel(),
                    'requires_grad': param.requires_grad,
                    'has_nan': torch.isnan(param).any().item(),
                    'has_inf': torch.isinf(param).any().item(),
                    'mean': param.mean().item() if param.numel() > 0 else 0,
                    'std': param.std().item() if param.numel() > 0 else 0
                }
                
                total_params += param.numel()
                if param_stats[name]['has_nan']:
                    has_nan_params = True
                if param_stats[name]['has_inf']:
                    has_inf_params = True
            
            performance_metrics['parameter_stats'] = param_stats
            performance_metrics['total_parameters_detailed'] = total_params
            
            if has_nan_params:
                validation_results['errors'].append('Parameters contain NaN values')
                test_results['parameters'] = 'failed'
            elif has_inf_params:
                validation_results['errors'].append('Parameters contain infinite values')
                test_results['parameters'] = 'failed'
            else:
                test_results['parameters'] = 'passed'
            
        except Exception as e:
            validation_results['errors'].append(f'Parameter validation failed: {str(e)}')
            test_results['parameters'] = 'failed'
    
    if 'config_methods' in validation_tests and model_instance is not None:
        try:
            # Configuration methods validation
            config_validation = {}
            
            if hasattr(model_instance, 'get_config'):
                config_dict = model_instance.get_config()
                if not isinstance(config_dict, dict):
                    validation_results['warnings'].append('get_config() does not return a dictionary')
                    config_validation['get_config'] = 'warning'
                else:
                    config_validation['get_config'] = 'passed'
                    performance_metrics['config_structure'] = list(config_dict.keys()) if config_dict else []
            else:
                config_validation['get_config'] = 'not_available'
            
            if hasattr(model_instance, 'update_from_config'):
                # Test configuration update capability
                try:
                    test_update = {'model': {'test_update': True}}
                    model_instance.update_from_config(test_update)
                    config_validation['update_from_config'] = 'passed'
                except Exception as update_error:
                    validation_results['warnings'].append(f'update_from_config failed: {str(update_error)}')
                    config_validation['update_from_config'] = 'warning'
            else:
                config_validation['update_from_config'] = 'not_available'
            
            test_results['config_methods'] = 'passed' if all(
                result in ['passed', 'not_available'] for result in config_validation.values()
            ) else 'warning'
            
            performance_metrics['config_validation'] = config_validation
            
        except Exception as e:
            validation_results['errors'].append(f'Configuration methods test failed: {str(e)}')
            test_results['config_methods'] = 'failed'
    
    if 'training_mode' in validation_tests and model_instance is not None:
        try:
            # Training mode compatibility test
            model_instance.train()
            training_batch_size = 2  # Conservative batch size for training mode
            
            training_input = torch.randn(training_batch_size, input_dim)
            if hasattr(model_instance, 'device'):
                training_input = training_input.to(model_instance.device)
            
            with torch.no_grad():
                training_output = model_instance(training_input)
            
            expected_shape = (training_batch_size, input_dim)
            if training_output.shape == expected_shape:
                test_results['training_mode'] = 'passed'
            else:
                validation_results['warnings'].append('Training mode output shape mismatch')
                test_results['training_mode'] = 'warning'
                
            # Return to eval mode
            model_instance.eval()
            
        except Exception as e:
            validation_results['warnings'].append(f'Training mode test failed: {str(e)}')
            test_results['training_mode'] = 'failed'
    
    if comprehensive_validation and model_instance is not None:
        try:
            # Additional comprehensive validation tests
            comprehensive_results = {}
            
            # Encode/decode functionality if available
            if hasattr(model_instance, 'encode') and hasattr(model_instance, 'decode'):
                try:
                    model_instance.eval()
                    test_input_encode = torch.randn(2, input_dim)
                    if hasattr(model_instance, 'device'):
                        test_input_encode = test_input_encode.to(model_instance.device)
                    
                    encoded = model_instance.encode(test_input_encode)
                    decoded = model_instance.decode(encoded)
                    
                    if decoded.shape == test_input_encode.shape:
                        reconstruction_error = torch.mean((test_input_encode - decoded) ** 2).item()
                        comprehensive_results['encode_decode'] = 'passed'
                        performance_metrics['reconstruction_error'] = reconstruction_error
                    else:
                        comprehensive_results['encode_decode'] = 'failed'
                        validation_results['errors'].append('Encode/decode shape mismatch')
                except Exception as encode_error:
                    comprehensive_results['encode_decode'] = 'failed'
                    validation_results['warnings'].append(f'Encode/decode test failed: {str(encode_error)}')
            
            # Device compatibility if CUDA available
            if torch.cuda.is_available():
                try:
                    cuda_device = torch.device('cuda')
                    model_cuda = model_instance.to(cuda_device)
                    test_input_cuda = torch.randn(2, input_dim, device=cuda_device)
                    
                    with torch.no_grad():
                        cuda_output = model_cuda(test_input_cuda)
                    
                    comprehensive_results['cuda_compatibility'] = 'passed'
                    # Move back to original device
                    model_instance = model_cuda.cpu() if hasattr(model_instance, 'device') else model_cuda
                except Exception as cuda_error:
                    comprehensive_results['cuda_compatibility'] = 'failed'
                    validation_results['warnings'].append(f'CUDA compatibility test failed: {str(cuda_error)}')
            
            test_results['comprehensive'] = 'passed' if all(
                result == 'passed' for result in comprehensive_results.values()
            ) else 'warning'
            
        except Exception as e:
            validation_results['warnings'].append(f'Comprehensive validation failed: {str(e)}')
            test_results['comprehensive'] = 'failed'
    
    # Calculate overall validation score
    passed_tests = sum(1 for result in test_results.values() if result == 'passed')
    total_tests = len(test_results)
    
    performance_metrics['validation_score'] = passed_tests / total_tests if total_tests > 0 else 0
    performance_metrics['tests_performed'] = test_results
    
    validation_results['test_results'] = test_results
    validation_results['overall_score'] = performance_metrics['validation_score']
    
    return model_instance, validation_results, performance_metrics, instantiation_details

def _get_system_context(silent: bool = False) -> Dict[str, Any]:
    """
    Comprehensive system analysis and hardware context collection.
    
    This function gathers detailed system information including hardware capabilities,
    memory usage, performance baselines, and applies memory optimization if needed.
    
    Args:
        silent (bool): Whether to suppress logging output
        
    Returns:
        Dict[str, Any]: Comprehensive system context including hardware data, 
                       system class, and collection status
    """
    # Initialize context structure
    system_context = {
        'hardware_data': None,
        'total_ram_gb': 8.0,
        'system_class': 'unknown',
        'collection_success': False,
        'collection_error': None
    }
    
    try:
        # Comprehensive system information collection
        system_info = get_system_info(
            include_versions=True,
            include_hardware=True,
            include_memory_usage=True,
            include_detailed_analysis=True,
            include_performance_baseline=True
        )
        
        hardware_analysis = system_info.get('hardware_analysis', {})
        capabilities = hardware_analysis.get('capabilities', {})
        system_class = hardware_analysis.get('system_class', 'unknown')
        baseline_results = system_info.get('performance_baseline', {})
        
        # Extract comprehensive hardware information
        gpu_available = capabilities.get('gpu', {}).get('available', False)
        gpu_memory_gb = capabilities.get('gpu', {}).get('total_memory_gb', 0)
        cpu_count = capabilities.get('cpu', {}).get('logical_cores', os.cpu_count() or 1)
        system_memory_gb = capabilities.get('memory', {}).get('total_gb', 8)
        memory_usage_percent = capabilities.get('memory', {}).get('usage_percent', 0)
        
        # Create comprehensive hardware_data structure
        hardware_data = {
            'gpu_available': gpu_available,
            'gpu_memory_gb': gpu_memory_gb,
            'cpu_count': cpu_count,
            'system_memory_gb': system_memory_gb,
            'system_class': system_class,
            'memory_usage_percent': memory_usage_percent,
            'hardware_analysis': hardware_analysis,
            'capabilities': capabilities,
            'performance_baseline': baseline_results,
            'collection_method': 'comprehensive_system_info'
        }
        
        # Additional detailed information from system_info
        if 'cuda' in system_info:
            hardware_data['cuda'] = system_info['cuda']
        if 'cpu_cores' in system_info:
            hardware_data['cpu_cores'] = system_info['cpu_cores']
        if 'system_ram' in system_info:
            hardware_data['system_ram'] = system_info['system_ram']
        
        total_ram_gb = system_memory_gb
        
        if not silent:
            logger.debug(f"Comprehensive system analysis: "
                    f"Class={system_class}, "
                    f"GPU={gpu_available} ({gpu_memory_gb:.1f}GB), "
                    f"CPU={cpu_count} cores, "
                    f"RAM={system_memory_gb:.1f}GB ({memory_usage_percent:.1f}% used)")
        
        # Update system context with successful collection
        system_context.update({
            'hardware_data': hardware_data,
            'total_ram_gb': total_ram_gb,
            'system_class': system_class,
            'collection_success': True,
            'collection_error': None
        })
        
    except Exception as e:
        if not silent:
            logger.warning(f"Comprehensive system analysis failed, using fallbacks: {e}")
        
        # Fallback to basic hardware detection
        hardware_data = {
            'gpu_available': torch.cuda.is_available(),
            'gpu_memory_gb': 0,
            'cpu_count': os.cpu_count() or 1,
            'system_memory_gb': 8,
            'system_class': 'unknown',
            'memory_usage_percent': 0,
            'collection_error': str(e),
            'partial_data': True,
            'collection_method': 'fallback_basic'
        }
        
        # Try to get basic GPU information if available
        if torch.cuda.is_available():
            try:
                gpu_props = torch.cuda.get_device_properties(0)
                hardware_data['gpu_memory_gb'] = gpu_props.total_memory / (1024**3)
                
                # Basic CUDA information
                hardware_data['cuda'] = {
                    'available': True,
                    'device_count': torch.cuda.device_count(),
                    'current_device': torch.cuda.current_device(),
                    'device_name': gpu_props.name,
                    'memory_gb': hardware_data['gpu_memory_gb']
                }
            except Exception as gpu_error:
                hardware_data['gpu_memory_gb'] = 4
                hardware_data['cuda'] = {'available': False, 'error': str(gpu_error)}
        
        # Update system context with fallback values
        system_context.update({
            'hardware_data': hardware_data,
            'total_ram_gb': hardware_data['system_memory_gb'],
            'system_class': hardware_data['system_class'],
            'collection_success': False,
            'collection_error': str(e)
        })
    
    return system_context

def _optimize_memory_if_needed(condition: bool, hardware_data: Dict[str, Any], aggressive: bool = False, silent: bool = False) -> bool:
    """Shared memory optimization routine."""
    if condition:
        try:
            results = enhanced_clear_memory(
                aggressive=aggressive,
                hardware_data=hardware_data
            )
            if results.get('success') and not silent:
                logger.debug("Memory optimization completed")
            return results.get('success', False)
        except Exception as e:
            if not silent:
                logger.debug(f"Memory optimization failed: {e}")
    return False

def _create_model_test_config(
    encoding_dim: int,
    hidden_dims: list,
    dropout_rates: list,
    activation: str,
    activation_param: float,
    normalization: str,
    use_batch_norm: bool,
    use_layer_norm: bool,
    skip_connection: bool,
    residual_blocks: bool,
    use_attention: bool,
    legacy_mode: bool,
    num_models: int,
    diversity_factor: float,
    learning_rate: float,
    batch_size: int,
    mixed_precision: bool,
    optimizer_type: str,
    device_setting: str,
    random_seed: int,
    input_dim: int,
    hardware_data: Dict[str, Any],
    model_specific_overrides: Dict[str, Any] = None
) -> Dict[str, Any]:
    """Creates a test configuration for model initialization."""
    # Get hardware data if not provided
    if not hardware_data:
        try:
            hardware_data = check_hardware(include_memory_usage=False) if 'check_hardware' in globals() else {}
        except:
            hardware_data = {}
    
    # Ensure normalization consistency
    if normalization == 'batch':
        effective_use_batch_norm = use_batch_norm or True
        effective_use_layer_norm = False
    elif normalization == 'layer':
        effective_use_batch_norm = False
        effective_use_layer_norm = use_layer_norm or True
    else:
        effective_use_batch_norm = use_batch_norm
        effective_use_layer_norm = use_layer_norm
    
    # Base configuration structure
    config = {
        'model': {
            'model_type': 'SimpleAutoencoder',
            'input_dim': input_dim,  # Ensure input_dim is included
            'encoding_dim': encoding_dim,
            'hidden_dims': hidden_dims.copy(),
            'dropout_rates': dropout_rates.copy(),
            'activation': activation,
            'activation_param': activation_param,
            'normalization': normalization,
            'use_batch_norm': effective_use_batch_norm,
            'use_layer_norm': effective_use_layer_norm,
            'skip_connection': skip_connection,
            'residual_blocks': residual_blocks,
            'use_attention': use_attention,
            'bias': True,
            'weight_init': 'xavier_uniform',
            'legacy_mode': legacy_mode,
            'num_models': num_models,
            'diversity_factor': diversity_factor,
            'min_features': 5
        },
        'training': {
            'learning_rate': learning_rate,
            'batch_size': batch_size,
            'mixed_precision': mixed_precision and hardware_data.get('gpu_available', False),
            'optimizer': optimizer_type,
            'weight_decay': 1e-4,
            'scheduler': 'ReduceLROnPlateau',
            'scheduler_params': {'patience': 10, 'factor': 0.5}
        },
        'data': {
            'features': input_dim,
            'normalization': 'standard'
        },
        'hardware': {
            'device': device_setting,
            'cuda_optimizations': hardware_data.get('gpu_available', False)
        },
        'system': {
            'random_seed': random_seed,
            'reproducible': True,
            'debug': False,
            'verbose': False
        },
        'monitoring': {
            'metrics_to_track': ['loss', 'reconstruction_error'],
            'save_best_model': False,
            'save_checkpoints': False
        }
    }
    
    # Apply model-specific overrides if provided
    if model_specific_overrides:
        for section, params in model_specific_overrides.items():
            if section in config:
                config[section].update(params)
            else:
                config[section] = params
    
    return config

def _create_model_test_definition(
    encoding_dim: int,
    hidden_dims: list,
    dropout_rates: list,
    use_attention: bool,
    residual_blocks: bool,
    skip_connection: bool,
    legacy_mode: bool,
    num_models: int,
    diversity_factor: float,
    mixed_precision: bool,
    input_dim: int,
    activation: str = 'leaky_relu',
    activation_param: float = 0.2,
    normalization: str = 'batch',
    use_batch_norm: bool = True,
    use_layer_norm: bool = False
) -> Dict[str, Any]:
    """
    Creates a model test definition dictionary with class checking.
    
    This function now performs upfront validation of class availability before
    creating lambda functions, preventing runtime errors and providing clear
    feedback about missing classes.
        
    Returns:
        Dictionary mapping model names to their test definitions
        
    Raises:
        RuntimeError: If no model classes are available for definition creation
    """
    # Common hardware data for all configs
    hardware_data = {'gpu_available': torch.cuda.is_available()}
    
    # Initialize model definitions dictionary
    model_definitions = {}
    
    # Track unavailable models for reporting
    unavailable_models = []
    availability_warnings = []
    
    # SimpleAutoencoder Class Checking
    try:
        # Verify class exists
        if 'SimpleAutoencoder' in globals() and callable(globals()['SimpleAutoencoder']):
            model_definitions['SimpleAutoencoder'] = {
                'class_check': lambda: SimpleAutoencoder is not None and callable(SimpleAutoencoder),
                'class_getter': lambda: SimpleAutoencoder,
                'primary_config': _create_model_test_config(
                    encoding_dim=encoding_dim,
                    hidden_dims=[hidden_dims[0]] if hidden_dims else [128],
                    dropout_rates=[dropout_rates[0]] if dropout_rates else [0.2],
                    activation=activation,
                    activation_param=activation_param,
                    normalization=normalization,
                    use_batch_norm=use_batch_norm,
                    use_layer_norm=use_layer_norm,
                    skip_connection=False,
                    residual_blocks=False,
                    use_attention=False,
                    legacy_mode=legacy_mode,
                    num_models=1,
                    diversity_factor=0.0,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=mixed_precision,
                    optimizer_type='adam',
                    device_setting='auto',
                    random_seed=42,
                    input_dim=input_dim,
                    hardware_data=hardware_data,
                    model_specific_overrides={
                        'model': {
                            'model_type': 'SimpleAutoencoder',
                            'use_attention': False,
                            'residual_blocks': False,
                            'skip_connection': False
                        }
                    }
                ),
                'fallback_config': _create_model_test_config(
                    encoding_dim=max(4, encoding_dim // 2),
                    hidden_dims=[64],
                    dropout_rates=[0.2],
                    activation='relu',
                    activation_param=0.0,
                    normalization=None,
                    use_batch_norm=False,
                    use_layer_norm=False,
                    skip_connection=False,
                    residual_blocks=False,
                    use_attention=False,
                    legacy_mode=True,
                    num_models=1,
                    diversity_factor=0.0,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=False,
                    optimizer_type='adam',
                    device_setting='auto',
                    random_seed=42,
                    input_dim=input_dim,
                    hardware_data=hardware_data,
                    model_specific_overrides={
                        'model': {
                            'model_type': 'SimpleAutoencoder',
                            'legacy_mode': True,
                            'use_attention': False,
                            'residual_blocks': False,
                            'skip_connection': False
                        }
                    }
                ),
                'minimal_config': _create_model_test_config(
                    encoding_dim=8,
                    hidden_dims=[32],
                    dropout_rates=[0.1],
                    activation='relu',
                    activation_param=0.0,
                    normalization=None,
                    use_batch_norm=False,
                    use_layer_norm=False,
                    skip_connection=False,
                    residual_blocks=False,
                    use_attention=False,
                    legacy_mode=True,
                    num_models=1,
                    diversity_factor=0.0,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=False,
                    optimizer_type='adam',
                    device_setting='auto',
                    random_seed=42,
                    input_dim=max(20, input_dim // 2),
                    hardware_data=hardware_data,
                    model_specific_overrides={
                        'model': {
                            'model_type': 'SimpleAutoencoder',
                            'use_attention': False,
                            'residual_blocks': False,
                            'skip_connection': False
                        },
                        'data': {
                            'features': max(20, input_dim // 2)
                        }
                    }
                ),
                'required_params': ['input_dim'],
                'description': 'Basic autoencoder with simple encoder-decoder architecture'
            }
            logger.debug("SimpleAutoencoder class available and registered")
        else:
            warning_msg = "SimpleAutoencoder class not available - skipping"
            logger.warning(warning_msg)
            unavailable_models.append('SimpleAutoencoder')
            availability_warnings.append(warning_msg)
    except NameError as e:
        error_msg = f"SimpleAutoencoder not defined: {e}"
        logger.error(error_msg)
        unavailable_models.append('SimpleAutoencoder')
        availability_warnings.append(error_msg)
    except Exception as e:
        error_msg = f"Unexpected error checking SimpleAutoencoder: {e}"
        logger.error(error_msg)
        unavailable_models.append('SimpleAutoencoder')
        availability_warnings.append(error_msg)
    
    # EnhancedAutoencoder Class Checking
    try:
        if 'EnhancedAutoencoder' in globals() and callable(globals()['EnhancedAutoencoder']):
            model_definitions['EnhancedAutoencoder'] = {
                'class_check': lambda: EnhancedAutoencoder is not None and callable(EnhancedAutoencoder),
                'class_getter': lambda: EnhancedAutoencoder,
                'primary_config': _create_model_test_config(
                    encoding_dim=encoding_dim,
                    hidden_dims=hidden_dims.copy(),
                    dropout_rates=dropout_rates.copy(),
                    activation=activation,
                    activation_param=activation_param,
                    normalization=normalization,
                    use_batch_norm=use_batch_norm,
                    use_layer_norm=use_layer_norm,
                    skip_connection=skip_connection,
                    residual_blocks=residual_blocks,
                    use_attention=use_attention and encoding_dim >= 32,
                    legacy_mode=legacy_mode,
                    num_models=1,
                    diversity_factor=0.0,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=mixed_precision,
                    optimizer_type='adam',
                    device_setting='auto',
                    random_seed=42,
                    input_dim=input_dim,
                    hardware_data=hardware_data,
                    model_specific_overrides={
                        'model': {
                            'model_type': 'EnhancedAutoencoder'
                        }
                    }
                ),
                'fallback_config': _create_model_test_config(
                    encoding_dim=max(8, encoding_dim // 2),
                    hidden_dims=[96, 48] if len(hidden_dims) > 1 else [64],
                    dropout_rates=[0.2, 0.15] if len(dropout_rates) > 1 else [0.2],
                    activation=activation,
                    activation_param=activation_param,
                    normalization=normalization,
                    use_batch_norm=use_batch_norm,
                    use_layer_norm=use_layer_norm,
                    skip_connection=True,
                    residual_blocks=False,
                    use_attention=False,
                    legacy_mode=False,
                    num_models=1,
                    diversity_factor=0.0,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=mixed_precision and torch.cuda.is_available(),
                    optimizer_type='adam',
                    device_setting='auto',
                    random_seed=42,
                    input_dim=input_dim,
                    hardware_data=hardware_data,
                    model_specific_overrides={
                        'model': {
                            'model_type': 'EnhancedAutoencoder',
                            'use_attention': False
                        }
                    }
                ),
                'minimal_config': _create_model_test_config(
                    encoding_dim=12,
                    hidden_dims=[48],
                    dropout_rates=[0.15],
                    activation='relu',
                    activation_param=0.0,
                    normalization=None,
                    use_batch_norm=False,
                    use_layer_norm=False,
                    skip_connection=False,
                    residual_blocks=False,
                    use_attention=False,
                    legacy_mode=True,
                    num_models=1,
                    diversity_factor=0.0,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=False,
                    optimizer_type='adam',
                    device_setting='auto',
                    random_seed=42,
                    input_dim=input_dim,
                    hardware_data=hardware_data,
                    model_specific_overrides={
                        'model': {
                            'model_type': 'EnhancedAutoencoder',
                            'use_attention': False,
                            'residual_blocks': False,
                            'skip_connection': False
                        }
                    }
                ),
                'required_params': ['input_dim'],
                'description': 'Advanced autoencoder with enhanced features and configurable architecture'
            }
            logger.debug("EnhancedAutoencoder class available and registered")
        else:
            warning_msg = "EnhancedAutoencoder class not available - skipping"
            logger.warning(warning_msg)
            unavailable_models.append('EnhancedAutoencoder')
            availability_warnings.append(warning_msg)
    except NameError as e:
        error_msg = f"EnhancedAutoencoder not defined: {e}"
        logger.error(error_msg)
        unavailable_models.append('EnhancedAutoencoder')
        availability_warnings.append(error_msg)
    except Exception as e:
        error_msg = f"Unexpected error checking EnhancedAutoencoder: {e}"
        logger.error(error_msg)
        unavailable_models.append('EnhancedAutoencoder')
        availability_warnings.append(error_msg)
    
    # AutoencoderEnsemble Class Checking
    try:
        if 'AutoencoderEnsemble' in globals() and callable(globals()['AutoencoderEnsemble']):
            model_definitions['AutoencoderEnsemble'] = {
                'class_check': lambda: AutoencoderEnsemble is not None and callable(AutoencoderEnsemble),
                'class_getter': lambda: AutoencoderEnsemble,
                'primary_config': _create_model_test_config(
                    encoding_dim=encoding_dim,
                    hidden_dims=hidden_dims.copy(),
                    dropout_rates=dropout_rates.copy(),
                    activation=activation,
                    activation_param=activation_param,
                    normalization=normalization,
                    use_batch_norm=use_batch_norm,
                    use_layer_norm=use_layer_norm,
                    skip_connection=skip_connection,
                    residual_blocks=residual_blocks and not legacy_mode,
                    use_attention=use_attention and not legacy_mode,
                    legacy_mode=legacy_mode,
                    num_models=min(3, num_models),
                    diversity_factor=diversity_factor,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=mixed_precision,
                    optimizer_type='adam',
                    device_setting='auto',
                    random_seed=42,
                    input_dim=input_dim,
                    hardware_data=hardware_data,
                    model_specific_overrides={
                        'model': {
                            'model_type': 'AutoencoderEnsemble'
                        }
                    }
                ),
                'fallback_config': _create_model_test_config(
                    encoding_dim=max(8, encoding_dim // 2),
                    hidden_dims=[80, 40] if len(hidden_dims) > 1 else [60],
                    dropout_rates=[0.2, 0.15] if len(dropout_rates) > 1 else [0.2],
                    activation=activation,
                    activation_param=activation_param,
                    normalization=normalization,
                    use_batch_norm=use_batch_norm,
                    use_layer_norm=use_layer_norm,
                    skip_connection=False,
                    residual_blocks=False,
                    use_attention=False,
                    legacy_mode=False,
                    num_models=2,
                    diversity_factor=0.1,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=False,
                    optimizer_type='adam',
                    device_setting='auto',
                    random_seed=42,
                    input_dim=input_dim,
                    hardware_data=hardware_data,
                    model_specific_overrides={
                        'model': {
                            'model_type': 'AutoencoderEnsemble',
                            'use_attention': False,
                            'residual_blocks': False
                        }
                    }
                ),
                'minimal_config': _create_model_test_config(
                    encoding_dim=8,
                    hidden_dims=[40],
                    dropout_rates=[0.15],
                    activation='relu',
                    activation_param=0.0,
                    normalization=None,
                    use_batch_norm=False,
                    use_layer_norm=False,
                    skip_connection=False,
                    residual_blocks=False,
                    use_attention=False,
                    legacy_mode=True,
                    num_models=2,
                    diversity_factor=0.05,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=False,
                    optimizer_type='adam',
                    device_setting='auto',
                    random_seed=42,
                    input_dim=input_dim,
                    hardware_data=hardware_data,
                    model_specific_overrides={
                        'model': {
                            'model_type': 'AutoencoderEnsemble',
                            'use_attention': False,
                            'residual_blocks': False
                        }
                    }
                ),
                'required_params': ['input_dim'],
                'description': 'Ensemble of diverse autoencoders for improved robustness and performance'
            }
            logger.debug("AutoencoderEnsemble class available and registered")
        else:
            warning_msg = "AutoencoderEnsemble class not available - skipping"
            logger.warning(warning_msg)
            unavailable_models.append('AutoencoderEnsemble')
            availability_warnings.append(warning_msg)
    except NameError as e:
        error_msg = f"AutoencoderEnsemble not defined: {e}"
        logger.error(error_msg)
        unavailable_models.append('AutoencoderEnsemble')
        availability_warnings.append(error_msg)
    except Exception as e:
        error_msg = f"Unexpected error checking AutoencoderEnsemble: {e}"
        logger.error(error_msg)
        unavailable_models.append('AutoencoderEnsemble')
        availability_warnings.append(error_msg)
    
    # Final validation and error handling
    if not model_definitions:
        # No models are available - this is a critical error
        error_details = {
            'unavailable_models': unavailable_models,
            'warnings': availability_warnings,
            'globals_check': {
                'SimpleAutoencoder': 'SimpleAutoencoder' in globals(),
                'EnhancedAutoencoder': 'EnhancedAutoencoder' in globals(),
                'AutoencoderEnsemble': 'AutoencoderEnsemble' in globals()
            },
            'callable_check': {
                'SimpleAutoencoder': callable(globals().get('SimpleAutoencoder', None)),
                'EnhancedAutoencoder': callable(globals().get('EnhancedAutoencoder', None)),
                'AutoencoderEnsemble': callable(globals().get('AutoencoderEnsemble', None))
            }
        }
        
        error_message = (
            f"No model classes available for test definition creation. "
            f"Unavailable models: {', '.join(unavailable_models)}. "
            f"This indicates that model classes have not been defined or imported properly. "
            f"Details: {error_details}"
        )
        
        logger.error(error_message)
        raise RuntimeError(error_message)
    
    # Log summary of available models
    available_count = len(model_definitions)
    unavailable_count = len(unavailable_models)
    
    logger.info(
        f"Model test definitions created successfully: "
        f"{available_count} available, {unavailable_count} unavailable"
    )
    
    if unavailable_models:
        logger.warning(
            f"The following models were not available and were skipped: "
            f"{', '.join(unavailable_models)}"
        )
    
    # Add metadata to the definitions dictionary
    model_definitions['_metadata'] = {
        'creation_timestamp': datetime.now().isoformat(),
        'total_definitions': available_count,
        'unavailable_models': unavailable_models,
        'availability_warnings': availability_warnings,
        'input_dim': input_dim,
        'encoding_dim': encoding_dim,
        'parameters': {
            'activation': activation,
            'normalization': normalization,
            'use_attention': use_attention,
            'residual_blocks': residual_blocks,
            'skip_connection': skip_connection,
            'legacy_mode': legacy_mode,
            'num_models': num_models,
            'diversity_factor': diversity_factor,
            'mixed_precision': mixed_precision
        }
    }
    
    return model_definitions

def _create_adaptive_config(
    model_name,
    model_class,
    system_class,
    input_dim,
    encoding_dim,
    hidden_dims,
    dropout_rates,
    activation,
    activation_param,
    normalization,
    use_batch_norm,
    use_layer_norm,
    skip_connection,
    residual_blocks,
    use_attention,
    legacy_mode,
    num_models,
    diversity_factor,
    learning_rate,
    batch_size,
    mixed_precision,
    optimizer_type,
    device_setting,
    random_seed,
    hardware_data
) -> Dict[str, Any]:
    """
    Create adaptive configuration for a specific model type when no predefined test definitions are available.
    
    This function generates model-specific configurations using _create_model_test_config
    with appropriate parameters tailored to each model type's characteristics.
    
    Returns:
        dict: Adaptive configuration for the specified model
    """
    
    # Create model-specific configuration using _create_model_test_config
    if model_name == 'SimpleAutoencoder':
        adaptive_config = _create_model_test_config(
            encoding_dim=max(4, encoding_dim // 2),
            hidden_dims=[hidden_dims[0]] if hidden_dims else [64],
            dropout_rates=[dropout_rates[0]] if dropout_rates else [0.2],
            activation=activation,
            activation_param=activation_param,
            normalization=None,
            use_batch_norm=False,
            use_layer_norm=False,
            skip_connection=False,
            residual_blocks=False,
            use_attention=False,
            legacy_mode=True,
            num_models=num_models,
            diversity_factor=diversity_factor,
            learning_rate=learning_rate,
            batch_size=batch_size,
            mixed_precision=False,
            optimizer_type='Adam',
            device_setting=device_setting,
            random_seed=random_seed,
            input_dim=input_dim,
            hardware_data=hardware_data,
            model_specific_overrides={
                'model': {
                    'model_type': 'SimpleAutoencoder',
                    'use_batch_norm': False,
                    'use_layer_norm': False
                },
                'training': {
                    'mixed_precision': False,
                    'optimizer': 'Adam'
                }
            }
        )
    elif model_name == 'EnhancedAutoencoder':
        adaptive_config = _create_model_test_config(
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention and encoding_dim >= 32,
            legacy_mode=legacy_mode,
            num_models=num_models,
            diversity_factor=diversity_factor,
            learning_rate=learning_rate,
            batch_size=batch_size,
            mixed_precision=mixed_precision,
            optimizer_type=optimizer_type,
            device_setting=device_setting,
            random_seed=random_seed,
            input_dim=input_dim,
            hardware_data=hardware_data,
            model_specific_overrides={
                'model': {
                    'model_type': 'EnhancedAutoencoder',
                    'hardware_adaptive': True,
                    'system_class': system_class
                }
            }
        )
    elif model_name == 'AutoencoderEnsemble':
        adaptive_config = _create_model_test_config(
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention and encoding_dim >= 32,
            legacy_mode=legacy_mode,
            num_models=min(5, num_models),
            diversity_factor=diversity_factor,
            learning_rate=learning_rate * 0.8,
            batch_size=max(8, batch_size // 2),
            mixed_precision=mixed_precision,
            optimizer_type=optimizer_type,
            device_setting=device_setting,
            random_seed=random_seed,
            input_dim=input_dim,
            hardware_data=hardware_data,
            model_specific_overrides={
                'model': {
                    'model_type': 'AutoencoderEnsemble',
                    'num_models': min(5, num_models),
                    'diversity_factor': diversity_factor,
                    'hardware_adaptive': True,
                    'system_class': system_class
                }
            }
        )
    else:
        # Generic configuration for unknown model types
        adaptive_config = _create_model_test_config(
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention,
            legacy_mode=legacy_mode,
            num_models=num_models,
            diversity_factor=diversity_factor,
            learning_rate=learning_rate,
            batch_size=batch_size,
            mixed_precision=mixed_precision,
            optimizer_type=optimizer_type,
            device_setting=device_setting,
            random_seed=random_seed,
            input_dim=input_dim,
            hardware_data=hardware_data,
            model_specific_overrides={'model': {'model_type': model_name}}
        )
    
    return adaptive_config

def _extract_and_validate_config_param(
    config: Dict,
    key: str,
    default: Any,
    global_name: str = None,
    validator=None,
    description: str = "",
    silent: bool = False
) -> Any:
    """Extract and validate configuration parameters from multiple config sections with fallbacks and validation."""
    try:
        # Try configuration first
        if key in config:
            value = config[key]
            if value is not None:
                if validator is None or validator(value):
                    return value
                else:
                    if not silent:
                        logger.debug(f"Invalid config value for {key} ({description}): {value}, using default")
        
        # Try global variable
        if global_name and global_name in globals():
            global_value = globals()[global_name]
            if validator is None or validator(global_value):
                return global_value
            else:
                if not silent:
                    logger.debug(f"Invalid global value for {key} ({global_name}): {global_value}, using default")
        
        # Use default
        if validator and not validator(default):
            return default
        else:
            if not silent:
                logger.warning(f"Default value for {key} failed validation, using safe fallback")
            # Return safe fallback based on type
            if isinstance(default, int):
                return max(1, default)
            elif isinstance(default, list) and default:
                return [max(1, x) if isinstance(x, int) else x for x in default]
            elif isinstance(default, float):
                return max(0.0, min(1.0, default))
            else:
                return default
        
        return default
        
    except Exception as e:
        if not silent:
            logger.debug(f"Error extracting parameter {key}: {e}, using default: {default}")
        return default

def _validate_and_adjust_parameters(hidden_dims: list, dropout_rates: list, silent: bool = False) -> tuple:
    """Validates and adjusts parameters for consistency."""
    
    # Ensure hidden_dims is a valid list
    if not isinstance(hidden_dims, list) or not hidden_dims:
        if isinstance(hidden_dims, (int, float)) and hidden_dims > 0:
            hidden_dims = [int(hidden_dims)]
            if not silent:
                logger.warning(f"Converted hidden_dims to list: {hidden_dims}")
        else:
            hidden_dims = [128, 64]
            if not silent:
                logger.warning(f"Invalid hidden_dims, using default: {hidden_dims}")
    
    # Remove any invalid dimensions
    valid_dims = [dim for dim in hidden_dims if isinstance(dim, (int, float)) and dim > 0]
    if len(valid_dims) != len(hidden_dims):
        if valid_dims:
            hidden_dims = [int(dim) for dim in valid_dims]
            if not silent:
                logger.warning(f"Filtered invalid hidden dimensions: {hidden_dims}")
        else:
            hidden_dims = [64]
            if not silent:
                logger.warning(f"Invalid hidden_dims, using default: {hidden_dims}")
    
    # Ensure dropout_rates is a valid list
    if not isinstance(dropout_rates, list) or not dropout_rates:
        if isinstance(dropout_rates, (int, float)) and 0 <= dropout_rates < 1:
            dropout_rates = [float(dropout_rates)]
            if not silent:
                logger.info(f"Converted dropout_rates to list: {dropout_rates}")
        else:
            dropout_rates = [0.2, 0.15]
            if not silent:
                logger.warning(f"Invalid dropout_rates, using default: {dropout_rates}")
    
    # Remove any invalid rates
    valid_rates = [rate for rate in dropout_rates if isinstance(rate, (int, float)) and 0 <= rate < 1]
    if len(valid_rates) != len(dropout_rates):
        if valid_rates:
            dropout_rates = valid_rates
            if not silent:
                logger.warning(f"Filtered invalid dropout rates: {dropout_rates}")
        else:
            dropout_rates = [0.2]
            if not silent:
                logger.warning(f"Invalid dropout_rates, using default: {dropout_rates}")
    
    # Ensure matching lengths (allow auto-adjustment in classes)
    if len(hidden_dims) != len(dropout_rates):
        if not silent:
            logger.warning(f"Mismatch in lengths of hidden_dims and dropout_rates, proceeding with auto-adjustment in model classes")
    else:
        # Adjust lengths to match
        max_length = max(len(hidden_dims), len(dropout_rates))
        
        # Extend hidden_dims if needed
        while len(hidden_dims) < max_length:
            hidden_dims.append(hidden_dims[-1] // 2 if hidden_dims[-1] > 32 else 32)
        
        # Extend dropout_rates if needed
        while len(dropout_rates) < max_length:
            last_rate = dropout_rates[-1] * 0.8 if dropout_rates[-1] > 0.05 else 0.05
            dropout_rates.append(min(0.5, last_rate))
        
        # Truncate to matching length
        min_length = min(len(hidden_dims), len(dropout_rates))
        hidden_dims = hidden_dims[:min_length]
        dropout_rates = dropout_rates[:min_length]
        
        if not silent:
            logger.debug(f"Adjusted parameter lengths: hidden_dims={hidden_dims}, dropout_rates={dropout_rates}")
    
    return hidden_dims, dropout_rates

def compare_model_architectures(input_dim: int = None, silent: bool = False) -> Dict[str, Any]:
    """
    Compare parameter counts and complexity of different model architectures with comprehensive analysis.
    
    This function performs detailed architectural comparison, performance analysis, and resource requirements
    for all preset configurations and model variants following the initialization and validation of all the
    model variants registered/initialized in the MODEL_VARIANTS dictionary.
    
    Args:
        input_dim: Input dimension for comparison (uses config/default if None)
        silent: If True, suppress detailed logging messages during validation
        
    Returns:
        Dictionary containing detailed comparison results, recommendations, and optimization guidance
    """
    comparison_start_time = time.time()
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'successful_analyses': 0,
        'failed_analyses': 0,
        'current_model': None,
        'analysis_method': None,
        'memory_optimizations': 0
    }
    
    try:
        if not silent:
            logger.debug("Starting model architecture comparison using helper functions")
        
        # Calculate total work units for progress tracking
        total_stages = 8  # System, Config, Validation, Parameters, Configs, Analysis, Summary, Finalization
        total_models = len(MODEL_VARIANTS) if MODEL_VARIANTS else 0
        
        # Initialize results structure
        results = {
            '_metadata': {
                'comparison_timestamp': datetime.now().isoformat(),
                'comparison_version': '3.2',
                'input_dimension': None,
                'config_source': 'unknown',
                'available_variants': 0,
                'successful_comparisons': 0,
                'failed_comparisons': 0,
                'hardware_context': None,
                'comparison_duration_seconds': 0,
                'helper_functions_utilized': [
                    '_get_system_context()',
                    '_optimize_memory_if_needed()', 
                    '_extract_and_validate_config_param()',
                    '_validate_and_adjust_parameters()',
                    '_create_model_test_definition()',
                    '_create_adaptive_config()',
                    'model_instantiation_with_validation()',
                    'analyze_model_layers()',
                    'calculate_architecture_efficiency()',
                    'execute_performance_testing()',
                    'estimate_flops()',
                    'analyze_scaling_behavior()',
                    'analyze_memory_usage()',
                    'estimate_training_resources()',
                    'model_specific_feature_analysis()',
                    'generate_model_recommendations()',
                    'generate_comparative_summary()'
                ],
                'memory_optimization_summary': {
                    'optimizations_performed': 0,
                    'hardware_aware': True,
                    'total_cleanup_actions': 0
                }
            },
            '_summary': {
                'recommendations': [],
                'warnings': [],
                'optimal_choices': {},
                'performance_ranking': {},
                'resource_efficiency': {},
                'use_case_recommendations': {},
                'compatibility_matrix': {},
                'optimization_suggestions': []
            },
            '_analysis_results': {
                'architectural_complexity': {},
                'computational_efficiency': {},
                'memory_utilization': {},
                'training_requirements': {},
                'inference_performance': {},
                'scalability_metrics': {}
            }
        }

        with alive_bar(total_stages, title='Model Architecture Comparison\t', unit='stages') as bar:
            
            # STAGE 1: System Analysis
            progress_data['current_stage'] = "System Analysis"
            bar.text = "Analyzing system hardware..."
            
            try:
                system_context = _get_system_context(silent=silent)
                hardware_data = system_context['hardware_data']
                total_ram_gb = system_context['total_ram_gb']
                system_class = system_context['system_class']
                collection_success = system_context['collection_success']
                
                # Extract nested hardware information with fallbacks
                baseline_results = hardware_data.get('performance_baseline', {}) if hardware_data else {}
                gpu_available = hardware_data.get('gpu_available', False) if hardware_data else False
                capabilities = hardware_data.get('capabilities', {}) if hardware_data else {}
                system_memory_gb = hardware_data.get('system_memory_gb', 8.0) if hardware_data else 8.0
                memory_usage_percent = hardware_data.get('memory_usage_percent', 0) if hardware_data else 0
                memory_pressure = memory_usage_percent > 70 or system_memory_gb < 8
                
                results['_metadata']['hardware_context'] = hardware_data
                bar.text = "System analysis complete"
            except Exception as e:
                if not silent:
                    logger.debug(f"System analysis failed: {e}")
                hardware_data = {}
                total_ram_gb = 8.0
                system_class = 'unknown'
                bar.text = "System analysis (using defaults)"
            
            bar()
            
            # STAGE 2: Configuration Loading
            progress_data['current_stage'] = "Loading Configuration"
            bar.text = "Loading configuration..."
            
            try:
                current_config = get_current_config()
                if not isinstance(current_config, dict):
                    current_config = {}
                
                model_config = current_config.get('model', {})
                data_config = current_config.get('data', {})
                training_config = current_config.get('training', {})
                hardware_config = current_config.get('hardware', {})
                system_config = current_config.get('system', {})
                
                results['_metadata']['config_source'] = 'current_config'
                if not silent:
                    logger.debug("Loaded configuration for architectural comparison")
                bar.text = "Configuration loaded"
            except Exception as e:
                if not silent:
                    logger.warning(f"Could not load current config, using defaults: {e}")
                current_config = get_default_config()
                if not isinstance(current_config, dict):
                    current_config = {}
                
                model_config = current_config.get('model', {})
                data_config = current_config.get('data', {})
                training_config = current_config.get('training', {})
                hardware_config = current_config.get('hardware', {})
                system_config = current_config.get('system', {})
                results['_metadata']['config_source'] = 'intelligent_defaults'
                if not silent:
                    logger.debug("Loaded default configuration for architectural comparison")
                bar.text = "Configuration (using defaults)"
            
            # Memory optimization after config processing
            config_size_estimate = len(str(current_config))
            config_memory_optimized = _optimize_memory_if_needed(
                condition=config_size_estimate > 50000 and total_ram_gb < 16,
                hardware_data=hardware_data,
                aggressive=config_size_estimate > 100000,
                silent=silent
            )
            if config_memory_optimized:
                results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                progress_data['memory_optimizations'] += 1
            
            bar()
            
            # STAGE 3: Model Variants Validation
            progress_data['current_stage'] = "Validating Model Variants"
            bar.text = "Validating model variants..."
            
            if not MODEL_VARIANTS:
                if not silent:
                    logger.info("MODEL_VARIANTS empty, performing initialization and validation")
                try:
                    initialize_model_variants(silent=silent)
                    if MODEL_VARIANTS:
                        validation_results = validate_model_variants(logger, silent=silent)
                        valid_variants = [name for name, status in validation_results.items() 
                                        if name != '_validation_summary' and (status == 'available' or status.startswith('warning'))]
                        results['_metadata']['validation_checks_performed'] = ['model_variants_validation']
                        results['_metadata']['valid_variants'] = valid_variants
                        if not silent:
                            logger.info(f"Validated {len(valid_variants)} model variants for comparison")
                        bar.text = f"Validated {len(valid_variants)} model variants"
                    else:
                        raise RuntimeError("No model variants available after initialization")
                except Exception as e:
                    error_msg = f"Model initialization and validation failed: {str(e)}"
                    results['initialization_error'] = error_msg
                    results['_metadata']['initialization_failure'] = True
                    if not silent:
                        logger.error(error_msg)
                    bar.text = "Model validation failed"
                    # Continue to allow partial analysis if possible
            else:
                bar.text = f"Using {len(MODEL_VARIANTS)} pre-validated variants"
            
            results['_metadata']['available_variants'] = len(MODEL_VARIANTS)
            total_models = len(MODEL_VARIANTS)
            
            bar()
            
            # STAGE 4: Parameter Extraction
            progress_data['current_stage'] = "Extracting Parameters"
            bar.text = "Extracting configuration parameters..."
            
            # Validate input dimension
            if input_dim is None:
                input_dim = _extract_and_validate_config_param(
                    data_config, 'features', 20, 'FEATURES',
                    lambda x: isinstance(x, int) and x > 0,
                    "input feature dimension", silent
                )
            
            if not isinstance(input_dim, int) or input_dim < 1:
                current_input_dim = input_dim
                input_dim = 20
                if not silent:
                    logger.warning(f"Invalid input_dim {current_input_dim}, using validated default {input_dim}")
            
            if input_dim > 10000:
                results['_metadata']['large_input_warning'] = True
                if not silent:
                    logger.warning(f"Very large input dimension ({input_dim}) detected - analysis may be memory intensive")
            
            results['_metadata']['input_dimension'] = input_dim
            
            # Extract all configuration parameters
            encoding_dim = _extract_and_validate_config_param(
                model_config, 'encoding_dim', 16, 'DEFAULT_ENCODING_DIM',
                lambda x: isinstance(x, int) and x > 0,
                "latent encoding dimension", silent
            )
            
            hidden_dims = _extract_and_validate_config_param(
                model_config, 'hidden_dims', [128, 64], 'HIDDEN_LAYER_SIZES',
                lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(d, int) and d > 0 for d in x),
                "hidden layer dimensions", silent
            )
            
            dropout_rates = _extract_and_validate_config_param(
                model_config, 'dropout_rates', [0.2, 0.15], 'DROPOUT_RATES',
                lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(r, (int, float)) and 0 <= r < 1 for r in x),
                "dropout rates", silent
            )
            
            activation = _extract_and_validate_config_param(
                model_config, 'activation', 'leaky_relu', 'ACTIVATION',
                lambda x: x in ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu', 'prelu'],
                "activation function", silent
            )
            
            activation_param = _extract_and_validate_config_param(
                model_config, 'activation_param', 0.2, 'ACTIVATION_PARAM',
                lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
                "activation parameter", silent
            )
            
            normalization = _extract_and_validate_config_param(
                model_config, 'normalization', 'batch', 'NORMALIZATION',
                lambda x: x in ['batch', 'layer', 'instance', 'group', 'none', None],
                "normalization type", silent
            )
            
            use_batch_norm = _extract_and_validate_config_param(
                model_config, 'use_batch_norm', True, 'USE_BATCH_NORM',
                lambda x: isinstance(x, bool),
                "batch normalization flag", silent
            )
            
            use_layer_norm = _extract_and_validate_config_param(
                model_config, 'use_layer_norm', False, 'USE_LAYER_NORM',
                lambda x: isinstance(x, bool),
                "layer normalization flag", silent
            )
            
            use_attention = _extract_and_validate_config_param(
                model_config, 'use_attention', True, 'USE_ATTENTION',
                lambda x: isinstance(x, bool),
                "attention mechanism flag", silent
            )
            
            residual_blocks = _extract_and_validate_config_param(
                model_config, 'residual_blocks', True, 'RESIDUAL_BLOCKS',
                lambda x: isinstance(x, bool),
                "residual blocks flag", silent
            )
            
            skip_connection = _extract_and_validate_config_param(
                model_config, 'skip_connection', True, 'SKIP_CONNECTION',
                lambda x: isinstance(x, bool),
                "skip connections flag", silent
            )
            
            num_models = _extract_and_validate_config_param(
                model_config, 'num_models', 3, 'NUM_MODELS',
                lambda x: isinstance(x, int) and 1 <= x <= 20,
                "ensemble size", silent
            )
            
            diversity_factor = _extract_and_validate_config_param(
                model_config, 'diversity_factor', 0.3, 'DIVERSITY_FACTOR',
                lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
                "ensemble diversity factor", silent
            )
            
            batch_size = _extract_and_validate_config_param(
                training_config, 'batch_size', 32, 'DEFAULT_BATCH_SIZE',
                lambda x: isinstance(x, int) and x > 0,
                "training batch size", silent
            )
            
            mixed_precision = _extract_and_validate_config_param(
                training_config, 'mixed_precision', True, 'MIXED_PRECISION',
                lambda x: isinstance(x, bool),
                "mixed precision training", silent
            )
            
            learning_rate = _extract_and_validate_config_param(
                training_config, 'learning_rate', 0.001, 'LEARNING_RATE',
                lambda x: isinstance(x, (int, float)) and x > 0,
                "learning rate", silent
            )
            
            optimizer_type = _extract_and_validate_config_param(
                training_config, 'optimizer', 'AdamW', 'OPTIMIZER',
                lambda x: x in ['Adam', 'AdamW', 'SGD', 'RMSprop', 'Adagrad'],
                "optimizer type", silent
            )
            
            device_setting = _extract_and_validate_config_param(
                hardware_config, 'device', 'auto', 'DEVICE',
                lambda x: isinstance(x, str) and x in ['auto', 'cpu', 'cuda'] or x.startswith('cuda:'),
                "compute device", silent
            )
            
            random_seed = _extract_and_validate_config_param(
                system_config, 'random_seed', 42, 'RANDOM_SEED',
                lambda x: isinstance(x, int),
                "random seed", silent
            )
            
            legacy_mode = _extract_and_validate_config_param(
                model_config, 'legacy_mode', False, 'LEGACY_MODE',
                lambda x: isinstance(x, bool),
                "legacy compatibility mode", silent
            )
            
            # Validate and adjust parameters
            hidden_dims, dropout_rates = _validate_and_adjust_parameters(hidden_dims, dropout_rates, silent)
            
            bar.text = "Parameters extracted and validated"
            bar()
            
            # STAGE 5: Test Configuration Creation with ALL required parameters
            progress_data['current_stage'] = "Creating Test Configurations"
            bar.text = "Creating test configurations..."
            
            # Generate test definitions with ALL required parameters
            model_test_definitions = _create_model_test_definition(
                encoding_dim=encoding_dim,
                hidden_dims=hidden_dims,
                dropout_rates=dropout_rates,
                use_attention=use_attention,
                residual_blocks=residual_blocks,
                skip_connection=skip_connection,
                legacy_mode=legacy_mode,
                num_models=num_models,
                diversity_factor=diversity_factor,
                mixed_precision=mixed_precision,
                # ADDED: Pass all the missing parameters that are now required
                input_dim=input_dim,
                activation=activation,
                activation_param=activation_param,
                normalization=normalization,
                use_batch_norm=use_batch_norm,
                use_layer_norm=use_layer_norm
            )
            
            # Create test configurations for each model variant
            test_configurations = {}
            for model_name, model_class in MODEL_VARIANTS.items():
                if model_name in model_test_definitions:
                    # Use test definition from helper function
                    test_def = model_test_definitions[model_name]
                    primary_config = test_def.get('primary_config', {})
                    
                    # Update input dimension in the configuration (redundant but safe)
                    if 'data' in primary_config:
                        primary_config['data']['features'] = input_dim
                    else:
                        primary_config['data'] = {'features': input_dim}
                    
                    if 'model' in primary_config:
                        primary_config['model']['input_dim'] = input_dim
                    else:
                        primary_config['model'] = {'input_dim': input_dim}
                    
                    test_configurations[model_name] = {
                        'params': {
                            'input_dim': input_dim,
                            'config': primary_config
                        },
                        'description': test_def.get('description', 'No description available'),
                        'use_cases': ['general purpose', 'production ready'],
                        'complexity_level': 'standard',
                        'computational_class': 'optimized',
                        'memory_efficiency': 'good',
                        'training_speed': 'moderate'
                    }
                else:
                    # Create adaptive configuration for unknown models
                    adaptive_config = _create_adaptive_config(
                        model_name=model_name,
                        model_class=model_class,
                        system_class=system_class,
                        input_dim=input_dim,
                        encoding_dim=encoding_dim,
                        hidden_dims=hidden_dims,
                        dropout_rates=dropout_rates,
                        activation=activation,
                        activation_param=activation_param,
                        normalization=normalization,
                        use_batch_norm=use_batch_norm,
                        use_layer_norm=use_layer_norm,
                        skip_connection=skip_connection,
                        residual_blocks=residual_blocks,
                        use_attention=use_attention,
                        legacy_mode=legacy_mode,
                        num_models=num_models if 'ensemble' in model_name.lower() else 1,
                        diversity_factor=diversity_factor,
                        learning_rate=learning_rate,
                        batch_size=batch_size,
                        mixed_precision=mixed_precision,
                        optimizer_type=optimizer_type,
                        device_setting=device_setting,
                        random_seed=random_seed,
                        hardware_data=hardware_data
                    )
                    
                    test_configurations[model_name] = {
                        'params': {
                            'input_dim': input_dim,
                            'config': adaptive_config
                        },
                        'description': f'Adaptive configuration for {model_name}',
                        'use_cases': ['general purpose', 'adaptive deployment'],
                        'complexity_level': 'adaptive',
                        'computational_class': 'adaptive',
                        'memory_efficiency': 'variable',
                        'training_speed': 'moderate'
                    }
            
            bar.text = f"Created {len(test_configurations)} test configurations"
            bar()
            
            # Close main progress bar and start model analysis bar
            bar.text = "Starting model analysis..."
        
        # STAGE 6: Comprehensive Model Analysis
        progress_data['current_stage'] = "Model Analysis"
        
        # Status symbols for visual feedback
        status_symbols = {
            'success': '[OK]',
            'failure': '[FAIL]',
            'skip': '[SKIP]'
        }
        
        method_symbols = {
            'primary': '[PRIMARY]',
            'fallback': '[FALLBACK]',
            'minimal': '[MINIMAL]',
            'adaptive': '[ADAPTIVE]'
        }
        
        with alive_bar(total_models, title='Analyzing Models\t\t', unit='models') as model_bar:
            
            for model_name, model_class in MODEL_VARIANTS.items():
                progress_data['current_model'] = model_name
                
                # Update progress bar with current model info
                successful_count = progress_data['successful_analyses']
                failed_count = progress_data['failed_analyses']
                model_bar.text = f"Analyzing {model_name}... ({successful_count} passed, {failed_count} failed)"
                
                if model_name not in test_configurations:
                    if not silent:
                        logger.warning(f"No test configuration for {model_name}, skipping analysis")
                    progress_data['failed_analyses'] += 1
                    model_bar.text = f"{status_symbols['skip']} {model_name} skipped"
                    model_bar()
                    continue
                
                test_config = test_configurations[model_name]
                analysis_start_time = time.time()
                model_results = {
                    'analysis_status': 'starting',
                    'errors': [],
                    'warnings': [],
                    'test_definition_used': model_name in model_test_definitions,
                    'helper_functions_used': []
                }
                
                try:
                    if not silent:
                        logger.debug(f"Starting comprehensive analysis for {model_name} using helper functions")
                    
                    # Model instantiation
                    model_results['helper_functions_used'].append('model_instantiation_with_validation')
                    
                    model_instance, validation_results, performance_metrics, instantiation_details = model_instantiation_with_validation(
                        variant_class=model_class,
                        variant_name=model_name,
                        input_dim=input_dim,
                        base_config=test_config['params']['config'],
                        fallback_config=test_config.get('fallback_config'),
                        minimal_config=test_config.get('minimal_config'),
                        validation_tests=['basic', 'forward_pass', 'parameters', 'config_methods'],
                        comprehensive_validation=False,
                        hardware_data=hardware_data,
                        silent=silent,
                        logger=logger
                    )
                    
                    if model_instance is None:
                        model_results['errors'].append('Model instantiation failed')
                        model_results['analysis_status'] = 'instantiation_failed'
                        results[model_name] = model_results
                        results['_metadata']['failed_comparisons'] += 1
                        progress_data['failed_analyses'] += 1
                        if not silent:
                            logger.error(f"Skipping analysis for {model_name} due to instantiation failure")
                        model_bar.text = f"{status_symbols['failure']} {model_name} instantiation failed"
                        model_bar()
                        continue
                    
                    instantiation_method = instantiation_details.get('method', 'primary')
                    validation_score = validation_results.get('overall_score', 0)
                    progress_data['analysis_method'] = instantiation_method
                    
                    # Track instantiation method
                    if instantiation_method != 'primary':
                        model_results['warnings'].append(f'Used {instantiation_method} configuration instead of primary')
                    
                    # Memory optimization after instantiation
                    post_instantiation_optimized = _optimize_memory_if_needed(
                        condition=total_ram_gb < 8,
                        hardware_data=hardware_data,
                        aggressive=total_ram_gb < 4,
                        silent=silent
                    )
                    if post_instantiation_optimized:
                        results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                        progress_data['memory_optimizations'] += 1
                        model_results['helper_functions_used'].append('_optimize_memory_if_needed')
                    
                    # Architecture Analysis using helper functions
                    total_params = sum(p.numel() for p in model_instance.parameters())
                    trainable_params = sum(p.numel() for p in model_instance.parameters() if p.requires_grad)
                    non_trainable_params = total_params - trainable_params
                    model_size_mb = total_params * 4 / (1024 * 1024)
                    
                    # Layer analysis using helper function
                    model_results['helper_functions_used'].append('analyze_model_layers')
                    layer_analysis = analyze_model_layers(model=model_instance)
                    
                    # Architecture efficiency using helper function
                    model_results['helper_functions_used'].append('calculate_architecture_efficiency')
                    architecture_efficiency = calculate_architecture_efficiency(total_params, layer_analysis)
                    
                    model_results['architecture'] = {
                        'total_params': total_params,
                        'trainable_params': trainable_params,
                        'non_trainable_params': non_trainable_params,
                        'model_size_mb': model_size_mb,
                        'complexity_level': test_config['complexity_level'],
                        'computational_class': test_config['computational_class'],
                        'description': test_config['description'],
                        'layer_count': layer_analysis['total_layers'],
                        'layer_types': layer_analysis['layer_types'],
                        'parameter_distribution': layer_analysis['parameter_distribution'],
                        'architecture_efficiency': architecture_efficiency,
                        'test_definition_based': model_name in model_test_definitions,
                        'instantiation_method': instantiation_method,
                        'validation_score': validation_score
                    }
                    
                    # Performance Testing with metrics using helper function
                    model_results['helper_functions_used'].append('execute_performance_testing')
                    performance_test_metrics = execute_performance_testing(
                        batch_size, input_dim, model_name, model_instance, hardware_data,
                        config_used=test_config['params']['config']
                    )
                    model_results['performance'] = performance_test_metrics
                    
                    # Memory optimization after performance testing
                    post_performance_optimized = _optimize_memory_if_needed(
                        condition=len(str(performance_test_metrics)) > 20000 or total_ram_gb < 8,
                        hardware_data=hardware_data,
                        aggressive=len(str(performance_test_metrics)) > 50000 or total_ram_gb < 4,
                        silent=silent
                    )
                    if post_performance_optimized:
                        results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                        progress_data['memory_optimizations'] += 1
                    
                    # FLOP Analysis using helper function
                    model_results['helper_functions_used'].append('estimate_flops')
                    flop_metrics = estimate_flops(input_dim, batch_size, model_instance)
                    model_results['computational_complexity'] = flop_metrics
                    
                    # Scaling Analysis using helper function
                    model_results['helper_functions_used'].append('analyze_scaling_behavior')
                    scaling_metrics = analyze_scaling_behavior(
                        model_class, test_config['params'], input_dim, model_name
                    )
                    model_results['scaling'] = scaling_metrics
                    
                    # Memory optimization after scaling analysis
                    post_scaling_optimized = _optimize_memory_if_needed(
                        condition=len(str(scaling_metrics)) > 30000 or total_ram_gb < 8,
                        hardware_data=hardware_data,
                        aggressive=len(str(scaling_metrics)) > 75000 or total_ram_gb < 4,
                        silent=silent
                    )
                    if post_scaling_optimized:
                        results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                        progress_data['memory_optimizations'] += 1
                    
                    # Memory Analysis with system awareness using helper function
                    model_results['helper_functions_used'].append('analyze_memory_usage')
                    memory_metrics = analyze_memory_usage(
                        batch_size, input_dim, model_name, model_instance, hardware_data
                    )
                    model_results['memory_analysis'] = memory_metrics
                    
                    # Training Resource Estimation using helper function
                    model_results['helper_functions_used'].append('estimate_training_resources')
                    training_resources = estimate_training_resources(
                        total_params, batch_size, input_dim, test_config['params']['config'],
                        model_name, hardware_data
                    )
                    model_results['resource_requirements'] = training_resources
                    
                    # Feature Analysis using helper function
                    model_results['helper_functions_used'].append('model_specific_feature_analysis')
                    feature_analysis = model_specific_feature_analysis(model_name, test_config, model_instance)
                    model_results['feature_analysis'] = feature_analysis
                    
                    # Generate Recommendations using helper function
                    model_results['helper_functions_used'].append('generate_model_recommendations')
                    recommendations = generate_model_recommendations(
                        model_name, total_params, hardware_data, performance_test_metrics
                    )
                    model_results['recommendations'] = recommendations
                    
                    # Finalize model results
                    model_results.update({
                        'use_cases': test_config['use_cases'],
                        'configuration_used': test_config['params'],
                        'analysis_metadata': {
                            'analysis_time_seconds': time.time() - analysis_start_time,
                            'test_batch_size': batch_size,
                            'analysis_version': '3.2',
                            'hardware_context_available': bool(hardware_data),
                            'system_class': system_class,
                            'memory_pressure': memory_pressure,
                            'validation_checks_passed': ['instantiation', 'forward_pass', 'parameter_count', 'memory_analysis'],
                            'test_definition_utilized': model_name in model_test_definitions,
                            'adaptive_configuration_used': model_name not in model_test_definitions,
                            'instantiation_method': instantiation_method,
                            'helper_functions_utilized': model_results['helper_functions_used']
                        },
                        'analysis_status': 'completed'
                    })
                    
                    results[model_name] = model_results
                    results['_metadata']['successful_comparisons'] += 1
                    progress_data['successful_analyses'] += 1
                    
                    method_symbol = method_symbols.get(instantiation_method, '[UNKNOWN]')
                    test_def_status = "TestDef" if model_name in model_test_definitions else "Adaptive"
                    model_bar.text = f"{status_symbols['success']} {model_name} {method_symbol} ({total_params:,} params)"
                    
                    if not silent:
                        logger.info(f"{status_symbols['success']} {model_name}: Comprehensive analysis completed "
                                  f"({test_def_status}, {instantiation_method}, {total_params:,} params, "
                                  f"score: {validation_score:.1%})")
                    
                except Exception as e:
                    error_msg = f"Analysis failed: {str(e)}"
                    if not silent:
                        logger.error(f"{model_name}: {error_msg}")
                    model_results['errors'].append(error_msg)
                    model_results['analysis_status'] = 'analysis_failed'
                    model_results['analysis_metadata'] = {
                        'analysis_time_seconds': time.time() - analysis_start_time,
                        'error_type': type(e).__name__,
                        'test_definition_utilized': model_name in model_test_definitions,
                        'helper_functions_utilized': model_results['helper_functions_used']
                    }
                    results[model_name] = model_results
                    results['_metadata']['failed_comparisons'] += 1
                    progress_data['failed_analyses'] += 1
                    model_bar.text = f"{status_symbols['failure']} {model_name} analysis failed"
                
                finally:
                    # Cleanup and memory optimization between models
                    try:
                        if 'model_instance' in locals() and model_instance is not None:
                            del model_instance
                        
                        inter_model_optimized = _optimize_memory_if_needed(
                            condition=True,
                            hardware_data=hardware_data,
                            aggressive=True,
                            silent=silent
                        )
                        if inter_model_optimized:
                            results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                            progress_data['memory_optimizations'] += 1
                        
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None
                    except Exception as cleanup_error:
                        if not silent:
                            logger.debug(f"Cleanup warning for {model_name}: {cleanup_error}")
                
                # Update progress bar
                model_bar()
            
            # Final update for model analysis
            successful_count = progress_data['successful_analyses']
            failed_count = progress_data['failed_analyses']
            model_bar.text = f"Analysis: {successful_count} passed, {failed_count} failed"
        
        # STAGE 7: Comparative Summary
        progress_data['current_stage'] = "Generating Summary"
        
        with alive_bar(1, title='Generating Summary\t\t') as summary_bar:
            
            summary_bar.text = "Generating comparative analysis..."
            
            # Memory optimization before comparative summary
            pre_summary_optimized = _optimize_memory_if_needed(
                condition=len(str(results)) > 100000,
                hardware_data=hardware_data,
                aggressive=len(str(results)) > 500000,
                silent=silent
            )
            if pre_summary_optimized:
                results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                progress_data['memory_optimizations'] += 1
            
            # Generate comparative analysis using helper function
            results['_summary'] = generate_comparative_summary(results, hardware_data)
            
            summary_bar.text = "Summary generated"
            summary_bar()
        
        # STAGE 8: Finalization
        progress_data['current_stage'] = "Finalizing"
        
        with alive_bar(1, title='Finalizing\t\t\t') as final_bar:
            
            final_bar.text = "Finalizing comparison..."
            
            # Final Metadata and Timing
            results['_metadata']['comparison_duration_seconds'] = time.time() - comparison_start_time
            results['_metadata']['analysis_completion_timestamp'] = datetime.now().isoformat()
            
            # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
            final_optimized = _optimize_memory_if_needed(
                condition=True,
                hardware_data=hardware_data,
                aggressive=True,
                silent=silent
            )
            if final_optimized:
                results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                progress_data['memory_optimizations'] += 1
                results['_metadata']['memory_optimization_summary']['final_cleanup'] = True
            
            # Add helper function utilization summary
            total_optimizations = results['_metadata']['memory_optimization_summary']['optimizations_performed']
            results['_metadata']['helper_function_summary'] = {
                'total_helper_functions_used': len(results['_metadata']['helper_functions_utilized']),
                'memory_optimizations_performed': total_optimizations,
                'system_context_method': 'comprehensive_system_analysis',
                'parameter_extraction_method': 'helper_function_based',
                'test_definition_method': 'helper_function_generated',
                'instantiation_method': 'helper_function_validation',
                'configuration_validation': 'helper_function_based',
                'adaptive_configuration_support': True,
                'integration_level': 'full'
            }
            
            # Log comparison summary
            successful = results['_metadata']['successful_comparisons']
            total = results['_metadata']['available_variants']
            duration = results['_metadata']['comparison_duration_seconds']
            helper_count = len(results['_metadata']['helper_functions_utilized'])
            test_def_count = sum(1 for m in MODEL_VARIANTS if m in model_test_definitions)
            
            if not silent:
                logger.debug("Model architecture comparison completed using enhanced helper functions:")
                logger.debug(f"  - Total models analyzed: {successful}/{total}")
                logger.debug(f"  - Comparison duration: {duration:.2f} seconds")
                logger.debug(f"  - Helper functions utilized: {helper_count}")
                logger.debug(f"  - Test definition based: {test_def_count}")
                logger.debug(f"  - Adaptive configurations: {total - test_def_count}")
                logger.debug(f"  - Memory optimizations: {progress_data['memory_optimizations']}")
                
                # Log available models and their key metrics
                if results['_metadata']['successful_comparisons'] > 0:
                    logger.debug("  - Successfully analyzed models:")
                    for model_name in MODEL_VARIANTS:
                        if model_name in results and results[model_name].get('analysis_status') == 'completed':
                            arch_data = results[model_name].get('architecture', {})
                            params = arch_data.get('total_params', 0)
                            method = arch_data.get('instantiation_method', 'unknown')
                            logger.debug(f"    {model_name}: {params:,} params ({method})")
                
                if results['_metadata']['failed_comparisons'] > 0:
                    logger.warning(f"  - Failed analyses: {results['_metadata']['failed_comparisons']}")
            
            final_bar.text = "Comparison complete!"
            final_bar()
    
    except Exception as e:
        if not silent:
            logger.error(f"Critical failure in model architecture comparison: {str(e)}", exc_info=True)
        
        # Emergency memory cleanup on critical failure
        emergency_optimized = _optimize_memory_if_needed(
            condition=True,
            hardware_data=hardware_data if 'hardware_data' in locals() else {},
            aggressive=True,
            silent=True
        )
        
        error_result = {
            'error': f'Critical comparison failure: {str(e)}',
            'timestamp': datetime.now().isoformat(),
            'error_type': type(e).__name__,
            'partial_results': results if 'results' in locals() else {},
            'recovery_suggestions': [
                'Check MODEL_VARIANTS initialization',
                'Verify configuration validity',
                'Ensure sufficient system resources',
                'Try with default parameters',
                'Review helper function implementations'
            ],
            'memory_optimization_attempted': True,
            'emergency_cleanup_performed': emergency_optimized,
            'helper_functions_utilized': [
                '_get_system_context()',
                '_optimize_memory_if_needed()', 
                '_create_model_test_definition()',
                '_create_adaptive_config()',
                '_extract_and_validate_config_param()',
                '_validate_and_adjust_parameters()',
                'model_instantiation_with_validation()',
                'analyze_model_layers()',
                'calculate_architecture_efficiency()',
                'execute_performance_testing()',
                'estimate_flops()',
                'analyze_scaling_behavior()',
                'analyze_memory_usage()',
                'estimate_training_resources()',
                'model_specific_feature_analysis()',
                'generate_model_recommendations()',
                'generate_comparative_summary()'
            ],
            'integration_status': 'failed_but_helper_functions_available'
        }
        return error_result

    return results

def analyze_model_layers(model: torch.nn.Module) -> Dict[str, Any]:
    """
    Analyze model layers and return comprehensive layer information.
    
    This function provides detailed layer analysis that harmonizes with the
    compare_model_architectures() function, focusing on parameter distribution,
    layer types, and architectural characteristics.
    
    Args:
        model: PyTorch model to analyze
        
    Returns:
        Dictionary containing layer analysis results
    """
    try:
        analysis = {
            'total_layers': 0,
            'layer_types': {},
            'parameter_distribution': {},
            'layer_details': [],
            'analysis_status': 'completed'
        }
        
        # Count layers and categorize by type
        layer_type_counts = {}
        layer_param_distribution = {}
        total_layers = 0
        layer_details = []
        
        for name, module in model.named_modules():
            if name == '':  # Skip root module
                continue
                
            total_layers += 1
            module_type = type(module).__name__
            
            # Count layer types
            layer_type_counts[module_type] = layer_type_counts.get(module_type, 0) + 1
            
            # Calculate parameters for this layer
            layer_params = sum(p.numel() for p in module.parameters(recurse=False))
            
            # Track parameter distribution by layer type
            if module_type not in layer_param_distribution:
                layer_param_distribution[module_type] = {
                    'total_params': 0,
                    'layer_count': 0,
                    'avg_params_per_layer': 0
                }
            
            layer_param_distribution[module_type]['total_params'] += layer_params
            layer_param_distribution[module_type]['layer_count'] += 1
            
            # Store layer details
            layer_info = {
                'name': name,
                'type': module_type,
                'parameters': layer_params,
                'trainable_params': sum(p.numel() for p in module.parameters(recurse=False) if p.requires_grad)
            }
            layer_details.append(layer_info)
        
        # Calculate average parameters per layer for each type
        for layer_type, info in layer_param_distribution.items():
            if info['layer_count'] > 0:
                info['avg_params_per_layer'] = info['total_params'] / info['layer_count']
        
        # Update analysis results
        analysis.update({
            'total_layers': total_layers,
            'layer_types': layer_type_counts,
            'parameter_distribution': layer_param_distribution,
            'layer_details': layer_details
        })
        
        return analysis
        
    except Exception as e:
        logger.debug(f"Layer analysis failed: {e}")
        return {
            'total_layers': 0,
            'layer_types': {},
            'parameter_distribution': {},
            'layer_details': [],
            'analysis_status': 'failed',
            'error': str(e)
        }

def calculate_architecture_efficiency(total_params: int, layer_analysis: Dict[str, Any]) -> float:
    """
    Calculate architecture efficiency score based on parameter count and layer analysis.
    
    This function provides an efficiency metric that harmonizes with the
    compare_model_architectures() function's architecture analysis, considering
    parameter distribution and layer structure.
    
    Args:
        total_params: Total number of model parameters
        layer_analysis: Layer analysis results from analyze_model_layers()
        
    Returns:
        Float efficiency score (0-100, higher is better)
    """
    try:
        if total_params <= 0 or not isinstance(layer_analysis, dict):
            return 0.0
        
        efficiency_score = 50.0  # Base score
        
        # Factor 1: Parameter distribution efficiency
        param_dist = layer_analysis.get('parameter_distribution', {})
        if param_dist:
            # Check if parameters are well distributed across layer types
            total_layers = layer_analysis.get('total_layers', 1)
            if total_layers > 0:
                # Reward balanced parameter distribution
                param_variance = 0
                layer_type_count = len(param_dist)
                
                if layer_type_count > 1:
                    avg_params_per_type = total_params / layer_type_count
                    for layer_type, info in param_dist.items():
                        type_params = info.get('total_params', 0)
                        param_variance += (type_params - avg_params_per_type) ** 2
                    
                    # Lower variance = better distribution = higher efficiency
                    param_variance = param_variance / layer_type_count
                    distribution_score = max(0, 20 - (param_variance / total_params) * 100)
                    efficiency_score += distribution_score
        
        # Factor 2: Layer structure efficiency
        layer_types = layer_analysis.get('layer_types', {})
        total_layers = layer_analysis.get('total_layers', 1)
        
        if total_layers > 0 and layer_types:
            # Reward efficient layer usage (not too many tiny layers)
            avg_params_per_layer = total_params / total_layers
            # Good parameter density
            if avg_params_per_layer > 1000:
                efficiency_score += 10
            elif avg_params_per_layer > 100:
                efficiency_score += 5
            
            # Reward diverse layer types (up to a point)
            unique_layer_types = len(layer_types)
            # Sweet spot for variety
            if 2 <= unique_layer_types <= 6:
                efficiency_score += min(15, unique_layer_types * 3)
        
        # Factor 3: Parameter efficiency relative to model size
        # Small model
        if total_params < 10000:
            # Bonus for efficiency
            efficiency_score += 15
        # Medium model
        elif total_params < 100000:
            efficiency_score += 10
        # Large model
        elif total_params < 1000000:
            efficiency_score += 5
        # Very large models get no bonus
        
        # Factor 4: Check for obviously inefficient patterns
        if layer_analysis.get('analysis_status') == 'failed':
            efficiency_score -= 20
        
        # Ensure score is within bounds
        efficiency_score = max(0.0, min(100.0, efficiency_score))
        
        return round(efficiency_score, 2)
        
    except Exception as e:
        logger.debug(f"Architecture efficiency calculation failed: {e}")
        # Default moderate efficiency score
        return 25.0

def execute_performance_testing(
    batch_size: int,
    input_dim: int,
    model_name: str,
    model: torch.nn.Module,
    hardware_data: Dict[str, Any],
    config_used: Dict[str, Any] = None,
    validation_mode: bool = False,
    scenario_filter: List[str] = None
) -> Dict[str, Any]:
    """
    Execute comprehensive performance testing leveraging full system analysis.
    
    This function is harmonized with compare_model_architectures() and uses
    the same parameter structure as called in the main comparison function.
    
    Args:
        batch_size: Base batch size for testing
        input_dim: Input dimension
        model_name: Name of the model for logging
        model: Model instance to test
        hardware_data: Hardware information from system context
        config_used: Configuration used for model creation
        validation_mode: If True, includes additional validation-specific tests
        scenario_filter: Optional list of scenario names to run (for targeted testing)
    
    Returns:
        Dictionary containing comprehensive performance metrics
    """
    performance_metrics = {}
    
    try:
        # Extract hardware information from the provided hardware_data
        gpu_available = hardware_data.get('gpu_available', False)
        system_memory_gb = hardware_data.get('system_memory_gb', 8.0)
        cpu_count = hardware_data.get('cpu_count', 1)
        
        # Determine system class based on hardware data
        if system_memory_gb >= 16 and gpu_available:
            system_class = 'high_performance'
        elif system_memory_gb < 8 or not gpu_available:
            system_class = 'limited'
        else:
            system_class = 'standard'
        
        # ADAPTIVE TEST SCENARIOS based on system capabilities and mode
        if system_class == 'high_performance':
            base_scenarios = [
                {'batch_size': 1, 'name': 'single_sample', 'description': 'Single sample inference test'},
                {'batch_size': 4, 'name': 'minimal_batch', 'description': 'Minimal batch processing'},
                {'batch_size': 16, 'name': 'small_batch', 'description': 'Small batch processing test'},
                {'batch_size': batch_size, 'name': 'standard_batch', 'description': 'Standard batch processing test'},
                {'batch_size': min(512, batch_size * 8), 'name': 'large_batch', 'description': 'Large batch processing test'},
                {'batch_size': min(1024, batch_size * 16), 'name': 'stress_test', 'description': 'Stress test with larger batch'}
            ]
        elif system_class == 'limited':
            base_scenarios = [
                {'batch_size': 1, 'name': 'single_sample', 'description': 'Single sample inference test'},
                {'batch_size': max(2, batch_size // 2), 'name': 'small_batch', 'description': 'Small batch processing test'},
                {'batch_size': batch_size, 'name': 'standard_batch', 'description': 'Standard batch processing test'}
            ]
        else:  # standard
            base_scenarios = [
                {'batch_size': 1, 'name': 'single_sample', 'description': 'Single sample inference test'},
                {'batch_size': max(2, batch_size // 2), 'name': 'small_batch', 'description': 'Small batch processing test'},
                {'batch_size': batch_size, 'name': 'standard_batch', 'description': 'Standard batch processing test'},
                {'batch_size': min(256, batch_size * 4), 'name': 'large_batch', 'description': 'Large batch processing test'}
            ]
        
        # Add validation-specific scenarios if in validation mode
        if validation_mode:
            # Add batch norm compatibility test
            if config_used and config_used.get('model', {}).get('use_batch_norm', False):
                base_scenarios.append({
                    'batch_size': max(4, batch_size), 
                    'name': 'batch_norm_compatible', 
                    'description': 'Batch normalization compatibility test'
                })
            
            # Add edge case scenarios for validation
            base_scenarios.extend([
                {'batch_size': 2, 'name': 'edge_case_small', 'description': 'Edge case: very small batch'},
                {'batch_size': max(8, batch_size * 2), 'name': 'medium_batch', 'description': 'Medium batch processing test'}
            ])
        
        # Filter scenarios if specified
        if scenario_filter:
            test_scenarios = [s for s in base_scenarios if s['name'] in scenario_filter]
        else:
            test_scenarios = base_scenarios
        
        # Remove duplicate batch sizes while preserving scenario names
        seen_batches = {}
        filtered_scenarios = []
        for scenario in test_scenarios:
            batch = scenario['batch_size']
            if batch not in seen_batches or scenario['name'] in ['batch_norm_compatible', 'single_sample']:
                seen_batches[batch] = True
                filtered_scenarios.append(scenario)
        
        test_scenarios = filtered_scenarios
        
        # SYSTEM-AWARE PERFORMANCE ANALYSIS
        performance_metrics.update({
            'system_class': system_class,
            'cpu_count': cpu_count,
            'system_memory_gb': system_memory_gb,
            'gpu_available': gpu_available,
            'model_name': model_name,
            'validation_mode': validation_mode,
            'scenarios_tested': len(test_scenarios),
            'test_configuration': {
                'batch_size': batch_size,
                'input_dim': input_dim,
                'config_used': config_used is not None
            }
        })
        
        # Execute performance testing for each scenario
        scenario_results = {}
        validation_results = {}
        
        for scenario in test_scenarios:
            scenario_name = scenario['name']
            scenario_batch_size = scenario['batch_size']
            scenario_description = scenario['description']
            
            try:
                # SYSTEM-AWARE BATCH SIZE ADJUSTMENT
                test_batch_size = scenario_batch_size
                
                # Adjust based on available memory
                if system_memory_gb < 8 and test_batch_size > 64:
                    original_batch_size = test_batch_size
                    test_batch_size = min(64, test_batch_size)
                    performance_metrics[f'{scenario_name}_batch_size_adjusted'] = {
                        'original': original_batch_size,
                        'adjusted': test_batch_size,
                        'reason': 'limited_system_memory'
                    }
                
                # Adjust for batch normalization requirements
                if ((config_used and config_used.get('model', {}).get('use_batch_norm', False)) or 
                    (config_used and config_used.get('model', {}).get('normalization') == 'batch')) and test_batch_size == 1:
                    test_batch_size = 2
                    performance_metrics[f'{scenario_name}_adjusted_batch_size'] = test_batch_size
                
                # Create test input with optimal device placement
                test_input = torch.randn(test_batch_size, input_dim)
                
                # ENHANCED DEVICE MANAGEMENT
                if hasattr(model, 'device'):
                    test_input = test_input.to(model.device)
                    device = model.device
                elif hasattr(next(model.parameters(), None), 'device'):
                    device = next(model.parameters()).device
                    test_input = test_input.to(device)
                else:
                    device = torch.device('cuda' if gpu_available else 'cpu')
                    test_input = test_input.to(device)
                    model = model.to(device)
                
                # SYSTEM-AWARE WARMUP
                warmup_runs = 10 if system_class == 'high_performance' else 5
                timing_runs = 30 if system_class == 'high_performance' else 20
                
                # Warmup with memory monitoring
                memory_before_warmup = None
                if torch.cuda.is_available() and device.type == 'cuda':
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    memory_before_warmup = torch.cuda.memory_allocated(device) / 1024**2
                
                for _ in range(warmup_runs):
                    with torch.no_grad():
                        _ = model(test_input)
                        if torch.cuda.is_available() and device.type == 'cuda':
                            torch.cuda.synchronize()
                
                # ENHANCED TIMING with system-aware statistics
                times = []
                outputs = []
                memory_usage = []
                
                for _ in range(timing_runs):
                    # Memory monitoring
                    if torch.cuda.is_available() and device.type == 'cuda':
                        torch.cuda.synchronize()
                        mem_before = torch.cuda.memory_allocated(device) / 1024**2
                    
                    #start_time = time.time()
                    start_time = time.perf_counter()  # Use perf_counter for better precision
                    with torch.no_grad():
                        output = model(test_input)
                        if torch.cuda.is_available() and device.type == 'cuda':
                            torch.cuda.synchronize()
                    #end_time = time.time()
                    end_time = time.perf_counter()
                    
                    inference_time = end_time - start_time
                    
                    #times.append(end_time - start_time)
                    times.append(max(inference_time, 1e-6))  # Ensure minimum time to prevent division by zero
                    outputs.append(output)
                    
                    if torch.cuda.is_available() and device.type == 'cuda':
                        mem_after = torch.cuda.memory_allocated(device) / 1024**2
                        memory_usage.append(mem_after - mem_before)
                
                # ENHANCED STATISTICS with system context
                #avg_time = sum(times) / len(times)
                avg_time = max(sum(times) / len(times), 1e-6)  # Minimum 1 microsecond
                #std_time = (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5
                std_time = max((sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5, 1e-6)
                #min_time = min(times)
                min_time = max(min(times), 1e-6)
                #max_time = max(times)
                max_time = max(max(times), 1e-6)
                #throughput = test_batch_size / avg_time
                # Safe throughput calculation with bounds checking
                if avg_time > 0 and test_batch_size > 0:
                    throughput = test_batch_size / avg_time
                    time_per_sample = avg_time / test_batch_size
                else:
                    # Fallback values for extremely fast inference
                    throughput = 1000000.0  # 1M samples/sec as fallback for very fast models
                    time_per_sample = 1e-6  # 1 microsecond per sample as fallback
                    performance_metrics[f'{scenario_name}_timing_warning'] = "Inference too fast for accurate measurement"

                # Ensure throughput is reasonable (not infinite)
                throughput = min(throughput, 1e8)  # Cap at 100M samples/sec
                time_per_sample = max(time_per_sample, 1e-9)  # Minimum 1 nanosecond
                
                # Rate performance based on throughput and system class
                if system_class == 'high_performance':
                    if throughput > 10000: 
                        performance_rating = 'excellent'
                    elif throughput > 5000: 
                        performance_rating = 'good'
                    elif throughput > 1000: 
                        performance_rating = 'fair'
                    else: 
                        performance_rating = 'poor'
                elif system_class == 'limited':
                    if throughput > 500: 
                        performance_rating = 'excellent'
                    elif throughput > 100: 
                        performance_rating = 'good'
                    elif throughput > 50: 
                        performance_rating = 'fair'
                    else: 
                        performance_rating = 'poor'
                else:  # standard
                    if throughput > 2000: 
                        performance_rating = 'excellent'
                    elif throughput > 1000: 
                        performance_rating = 'good'
                    elif throughput > 500: 
                        performance_rating = 'fair'
                    else: 
                        performance_rating = 'poor'
                
                # Store comprehensive performance metrics
                scenario_metrics = {
                    f'{scenario_name}_inference_time_ms': avg_time * 1000,
                    f'{scenario_name}_inference_std_ms': std_time * 1000,
                    f'{scenario_name}_inference_min_ms': min_time * 1000,
                    f'{scenario_name}_inference_max_ms': max_time * 1000,
                    f'{scenario_name}_throughput_samples_per_sec': throughput,
                    f'{scenario_name}_batch_size': test_batch_size,
                    f'{scenario_name}_device_type': device.type,
                    f'{scenario_name}_performance_rating': performance_rating,
                }
                
                # Add memory metrics if available
                if memory_usage:
                    scenario_metrics.update({
                        f'{scenario_name}_avg_memory_usage_mb': sum(memory_usage) / len(memory_usage),
                        f'{scenario_name}_max_memory_usage_mb': max(memory_usage),
                        f'{scenario_name}_memory_efficiency': test_batch_size / max(memory_usage) if max(memory_usage) > 0 else 0
                    })
                
                performance_metrics.update(scenario_metrics)
                
                # ENHANCED OUTPUT QUALITY ANALYSIS for validation mode
                output = outputs[-1]
                quality_metrics = {}
                validation_status = 'passed'
                validation_errors = []
                validation_warnings = []
                
                # Basic quality checks
                has_nan = torch.isnan(output).any().item()
                has_inf = torch.isinf(output).any().item()
                
                quality_metrics[f'{scenario_name}_has_nan'] = has_nan
                quality_metrics[f'{scenario_name}_has_inf'] = has_inf
                
                if validation_mode:
                    # Validation-specific checks
                    expected_shape = (test_batch_size, input_dim)
                    if output.shape != expected_shape:
                        validation_errors.append(f'Shape mismatch - expected {expected_shape}, got {output.shape}')
                        validation_status = 'failed'
                    
                    if has_nan:
                        validation_errors.append('NaN values detected in output')
                        validation_status = 'failed'
                    
                    if has_inf:
                        validation_errors.append('Infinite values detected in output')
                        validation_status = 'failed'
                
                if not has_nan and not has_inf:
                    # Enhanced statistics
                    output_range = (output.max() - output.min()).item()
                    output_mean = output.mean().item()
                    output_std = output.std().item()
                    output_median = output.median().item()
                    
                    # Detect potential issues
                    quality_issues = []
                    if output_std < 1e-6:
                        quality_issues.append('very_low_variance')
                        if validation_mode:
                            validation_warnings.append('Output has very small dynamic range')
                    
                    if abs(output_mean) > 100:
                        quality_issues.append('high_magnitude_values')
                        if validation_mode:
                            validation_warnings.append('Extreme output values detected')
                    
                    if output_range > 1000:
                        quality_issues.append('extreme_value_range')
                        if validation_mode:
                            validation_warnings.append('Output has very large value range')
                    
                    # Calculate quality score directly
                    quality_score = 1.0
                    
                    # Penalize extreme values
                    if abs(output_mean) > 10:
                        quality_score *= 0.8
                    if output_range > 100:
                        quality_score *= 0.7
                    if output_std < 1e-6:
                        quality_score *= 0.5
                    
                    # Ensure score is within valid range
                    quality_score = max(0.0, min(1.0, quality_score))
                    
                    quality_metrics.update({
                        f'{scenario_name}_output_range': output_range,
                        f'{scenario_name}_output_mean': output_mean,
                        f'{scenario_name}_output_std': output_std,
                        f'{scenario_name}_output_median': output_median,
                        f'{scenario_name}_quality_issues': quality_issues,
                        f'{scenario_name}_quality_score': quality_score
                    })
                
                performance_metrics.update(quality_metrics)
                
                # Store validation results if in validation mode
                if validation_mode:
                    validation_results[scenario_name] = {
                        'status': validation_status,
                        'errors': validation_errors,
                        'warnings': validation_warnings,
                        'batch_size_used': test_batch_size,
                        'output_shape': list(output.shape),
                        'device_used': str(device),
                        'scenario_description': scenario_description
                    }
                
                scenario_results[scenario_name] = {
                    'throughput': throughput,
                    'latency_ms': avg_time * 1000,
                    'performance_rating': performance_rating,
                    'quality_score': quality_metrics.get(f'{scenario_name}_quality_score', 0),
                    'validation_status': validation_status if validation_mode else 'not_tested'
                }
                
            except Exception as scenario_error:
                error_msg = f"{scenario_name}: {str(scenario_error)}"
                logger.warning(f"{model_name} performance test failed for {scenario_name}: {scenario_error}")
                performance_metrics[f'{scenario_name}_error'] = error_msg
                performance_metrics[f'{scenario_name}_inference_time_ms'] = float('nan')
                performance_metrics[f'{scenario_name}_throughput_samples_per_sec'] = float('nan')
                
                if validation_mode:
                    validation_results[scenario_name] = {
                        'status': 'failed',
                        'errors': [error_msg],
                        'warnings': [],
                        'scenario_description': scenario_description
                    }
                
                scenario_results[scenario_name] = {
                    'error': error_msg,
                    'validation_status': 'failed' if validation_mode else 'error'
                }
        
        # Generate comprehensive recommendations
        recommendations = []
        
        # Get average throughput
        throughputs = [v for k, v in performance_metrics.items() if k.endswith('_throughput_samples_per_sec') and not math.isnan(v)]
        if throughputs:
            avg_throughput = sum(throughputs) / len(throughputs)
            
            if avg_throughput < 100 and system_class != 'limited':
                recommendations.append("Performance is below expected for system class - consider model optimization")
            
            if system_class == 'high_performance' and avg_throughput < 1000:
                recommendations.append("High-performance system underutilized - enable mixed precision or larger batch sizes")
        
        # GPU-specific recommendations
        if gpu_available:
            gpu_memory_gb = hardware_data.get('gpu_memory_gb', 0)
            if gpu_memory_gb > 8:
                recommendations.append("Large GPU memory available - consider using larger batch sizes")
            elif gpu_memory_gb < 4:
                recommendations.append("Limited GPU memory - use gradient accumulation instead of large batch sizes")
        
        performance_metrics['recommendations'] = recommendations
        
        # Add validation-specific summary if in validation mode
        if validation_mode:
            passed_scenarios = [name for name, result in validation_results.items() if result['status'] == 'passed']
            failed_scenarios = [name for name, result in validation_results.items() if result['status'] == 'failed']
            
            performance_metrics['validation_summary'] = {
                'total_scenarios': len(validation_results),
                'passed_scenarios': len(passed_scenarios),
                'failed_scenarios': len(failed_scenarios),
                'success_rate': len(passed_scenarios) / max(len(validation_results), 1) * 100,
                'validation_results': validation_results
            }
        
        # Add scenario summary
        performance_metrics['scenario_summary'] = scenario_results
        
        return performance_metrics
        
    except Exception as e:
        logger.error(f"Performance testing failed for {model_name}: {e}")
        return {
            'error': f'Performance testing failed: {str(e)}',
            'model_name': model_name,
            'analysis_status': 'failed'
        }

def estimate_flops(
    input_dim: int,
    batch_size: int,
    model: torch.nn.Module
) -> Dict[str, Any]:
    """
    Comprehensive FLOP estimation with detailed analysis and breakdown.
    
    This function provides enhanced FLOP estimation including forward pass,
    backward pass, layer-wise breakdown, and computational complexity classification.
    Harmonized with compare_model_architectures() parameter structure.
    
    Args:
        input_dim: Input dimension size
        batch_size: Batch size for calculation
        model: PyTorch model to analyze
        
    Returns:
        Dictionary containing comprehensive FLOP analysis
    """
    flop_start_time = time.time()
    
    try:
        flop_analysis = {
            'forward_pass': {
                'total_flops': 0,
                'layer_breakdown': [],
                'operation_breakdown': {},
                'per_sample_flops': 0
            },
            'backward_pass': {
                'estimated_flops': 0,
                'gradient_computation_flops': 0,
                'parameter_update_flops': 0
            },
            'training_flops': {
                'total_per_step': 0,
                'per_epoch_estimate': 0,
                'optimizer_overhead': 0
            },
            'complexity_metrics': {
                'flops_per_parameter': 0,
                'computational_density': 0,
                'complexity_class': 'unknown',
                'efficiency_rating': 'unknown'
            },
            'comparative_metrics': {
                'flops_vs_mobilenet': 0,
                'flops_vs_resnet18': 0,
                'relative_complexity': 'unknown'
            },
            'optimization_analysis': {
                'bottleneck_layers': [],
                'optimization_opportunities': [],
                'memory_compute_ratio': 0
            },
            'analysis_metadata': {
                'estimation_method': 'comprehensive_layer_analysis',
                'accuracy_level': 'high',
                'analysis_time_seconds': 0,
                'warnings': []
            }
        }
        
        logger.debug(f"Starting comprehensive FLOP estimation for model with {sum(p.numel() for p in model.parameters())} parameters")
        
        # Phase 1: Detailed Layer-by-Layer Analysis
        model.eval()
        layer_details = []
        total_forward_flops = 0
        operation_counts = {
            'linear': 0,
            'convolution': 0,
            'activation': 0,
            'normalization': 0,
            'attention': 0,
            'dropout': 0,
            'other': 0
        }
        
        # Track tensor dimensions through the network
        current_shape = (batch_size, input_dim)
        
        for name, module in model.named_modules():
            if name == '':  # Skip root module
                continue
            
            layer_flops = 0
            layer_info = {
                'name': name,
                'type': type(module).__name__,
                'input_shape': current_shape,
                'output_shape': None,
                'flops': 0,
                'parameters': sum(p.numel() for p in module.parameters()),
                'trainable_params': sum(p.numel() for p in module.parameters() if p.requires_grad),
                'operation_class': 'other',
                'computational_intensity': 'low',
                'memory_access': 0,
                'arithmetic_intensity': 0
            }
            
            try:
                if isinstance(module, torch.nn.Linear):
                    # Linear layer: input_features × output_features × batch_size (MAC operations)
                    in_features = module.in_features
                    out_features = module.out_features
                    
                    # Multiply-accumulate operations (2 FLOPs each: multiply + add)
                    mac_operations = in_features * out_features * batch_size
                    layer_flops = mac_operations * 2
                    
                    # Bias addition if present
                    if module.bias is not None:
                        layer_flops += out_features * batch_size
                    
                    # Update shape tracking
                    current_shape = (batch_size, out_features)
                    layer_info['output_shape'] = current_shape
                    layer_info['operation_class'] = 'linear'
                    layer_info['computational_intensity'] = 'high'
                    
                    # Memory access estimation (weights + input + output)
                    layer_info['memory_access'] = (in_features * out_features + 
                                                 in_features * batch_size + 
                                                 out_features * batch_size) * 4  # bytes
                    
                    # Arithmetic intensity (FLOPs per byte)
                    layer_info['arithmetic_intensity'] = layer_flops / max(layer_info['memory_access'], 1)
                    
                    operation_counts['linear'] += layer_flops
                    
                    layer_info.update({
                        'in_features': in_features,
                        'out_features': out_features,
                        'has_bias': module.bias is not None,
                        'weight_flops': mac_operations * 2,
                        'bias_flops': out_features * batch_size if module.bias is not None else 0
                    })
                    
                elif isinstance(module, torch.nn.Conv1d):
                    # 1D Convolution analysis
                    kernel_size = module.kernel_size[0] if isinstance(module.kernel_size, tuple) else module.kernel_size
                    in_channels = module.in_channels
                    out_channels = module.out_channels
                    padding = module.padding[0] if isinstance(module.padding, tuple) else module.padding
                    stride = module.stride[0] if isinstance(module.stride, tuple) else module.stride
                    
                    # Calculate output length
                    input_length = current_shape[-1] if len(current_shape) > 2 else input_dim
                    output_length = (input_length + 2 * padding - kernel_size) // stride + 1
                    
                    # Convolution FLOPs: kernel_size × in_channels × out_channels × output_length × batch_size
                    conv_operations = kernel_size * in_channels * out_channels * output_length * batch_size
                    layer_flops = conv_operations * 2  # MAC operations
                    
                    if module.bias is not None:
                        layer_flops += out_channels * output_length * batch_size
                    
                    current_shape = (batch_size, out_channels, output_length)
                    layer_info['output_shape'] = current_shape
                    layer_info['operation_class'] = 'convolution'
                    layer_info['computational_intensity'] = 'high'
                    
                    operation_counts['convolution'] += layer_flops
                    
                elif isinstance(module, torch.nn.Conv2d):
                    # 2D Convolution analysis
                    kernel_h, kernel_w = (module.kernel_size if isinstance(module.kernel_size, tuple) 
                                        else (module.kernel_size, module.kernel_size))
                    in_channels = module.in_channels
                    out_channels = module.out_channels
                    
                    # Estimate spatial dimensions (simplified)
                    spatial_dim = int(input_dim ** 0.5)
                    output_h = output_w = spatial_dim  # Simplified
                    
                    conv_operations = (kernel_h * kernel_w * in_channels * out_channels * 
                                     output_h * output_w * batch_size)
                    layer_flops = conv_operations * 2
                    
                    if module.bias is not None:
                        layer_flops += out_channels * output_h * output_w * batch_size
                    
                    layer_info['operation_class'] = 'convolution'
                    layer_info['computational_intensity'] = 'very_high'
                    operation_counts['convolution'] += layer_flops
                    
                elif isinstance(module, (torch.nn.BatchNorm1d, torch.nn.LayerNorm)):
                    # Normalization layers
                    if hasattr(module, 'num_features'):
                        num_features = module.num_features
                    elif hasattr(module, 'normalized_shape'):
                        num_features = (module.normalized_shape[0] if isinstance(module.normalized_shape, tuple) 
                                      else module.normalized_shape)
                    else:
                        num_features = current_shape[-1] if current_shape else input_dim
                    
                    # Normalization operations:
                    # 1. Mean calculation: num_features * batch_size
                    # 2. Variance calculation: num_features * batch_size
                    # 3. Normalization: 4 * num_features * batch_size (subtract, divide, scale, shift)
                    layer_flops = 6 * num_features * batch_size
                    
                    layer_info['operation_class'] = 'normalization'
                    layer_info['computational_intensity'] = 'medium'
                    operation_counts['normalization'] += layer_flops
                    
                    layer_info.update({
                        'num_features': num_features,
                        'affine': getattr(module, 'affine', True)
                    })
                    
                elif isinstance(module, torch.nn.GroupNorm):
                    # Group normalization
                    num_groups = module.num_groups
                    num_channels = module.num_channels
                    
                    # Group normalization is more complex than batch norm
                    layer_flops = 8 * num_channels * batch_size
                    
                    layer_info['operation_class'] = 'normalization'
                    layer_info['computational_intensity'] = 'medium'
                    operation_counts['normalization'] += layer_flops
                    
                elif isinstance(module, torch.nn.Dropout):
                    # Dropout: minimal cost during inference (0 in eval mode)
                    layer_flops = 0
                    layer_info['operation_class'] = 'dropout'
                    layer_info['computational_intensity'] = 'none'
                    operation_counts['dropout'] += layer_flops
                    
                elif isinstance(module, (torch.nn.ReLU, torch.nn.LeakyReLU, torch.nn.ELU, 
                                        torch.nn.GELU, torch.nn.Sigmoid, torch.nn.Tanh)):
                    # Activation functions
                    estimated_neurons = current_shape[-1] if current_shape else input_dim
                    
                    if isinstance(module, torch.nn.ReLU):
                        layer_flops = estimated_neurons * batch_size  # Comparison operation
                    elif isinstance(module, torch.nn.LeakyReLU):
                        layer_flops = estimated_neurons * batch_size * 2  # Comparison + multiplication
                    elif isinstance(module, torch.nn.ELU):
                        layer_flops = estimated_neurons * batch_size * 4  # Exponential + conditional
                    elif isinstance(module, torch.nn.GELU):
                        layer_flops = estimated_neurons * batch_size * 8  # Complex approximation
                    elif isinstance(module, torch.nn.Sigmoid):
                        layer_flops = estimated_neurons * batch_size * 4  # Exponential + division
                    elif isinstance(module, torch.nn.Tanh):
                        layer_flops = estimated_neurons * batch_size * 6  # Hyperbolic function
                    else:
                        layer_flops = estimated_neurons * batch_size  # Default
                    
                    layer_info['operation_class'] = 'activation'
                    layer_info['computational_intensity'] = 'low' if layer_flops <= estimated_neurons * batch_size else 'medium'
                    operation_counts['activation'] += layer_flops
                    
                    layer_info['activation_type'] = type(module).__name__
                    
                elif hasattr(module, '__class__') and 'Attention' in module.__class__.__name__:
                    # Multi-head attention (custom implementation)
                    if hasattr(module, 'dim'):
                        dim = module.dim
                        num_heads = getattr(module, 'num_heads', 8)
                        seq_len = 1  # Assuming single token for autoencoder
                        
                        # Attention FLOPs: Q, K, V projections + attention computation + output projection
                        projection_flops = 3 * (dim * dim * batch_size)  # Q, K, V
                        attention_flops = (dim * seq_len * seq_len * batch_size * num_heads +  # Attention scores
                                         dim * seq_len * seq_len * batch_size * num_heads)  # Value combination
                        output_projection_flops = dim * dim * batch_size
                        
                        layer_flops = projection_flops + attention_flops + output_projection_flops
                        
                        layer_info['operation_class'] = 'attention'
                        layer_info['computational_intensity'] = 'very_high'
                        operation_counts['attention'] += layer_flops
                        
                        layer_info.update({
                            'attention_dim': dim,
                            'num_heads': num_heads,
                            'sequence_length': seq_len
                        })
                
                # Handle custom blocks (ResidualBlock, etc.)
                elif 'Block' in type(module).__name__ or 'Residual' in type(module).__name__:
                    # For custom blocks, estimate based on constituent operations
                    block_params = sum(p.numel() for p in module.parameters())
                    # Rough estimate: 2 FLOPs per parameter for complex blocks
                    layer_flops = block_params * 2 * batch_size
                    
                    layer_info['operation_class'] = 'other'
                    layer_info['computational_intensity'] = 'high'
                    operation_counts['other'] += layer_flops
                
                # Update layer information
                layer_info['flops'] = layer_flops
                layer_details.append(layer_info)
                total_forward_flops += layer_flops
                
                # Track computational bottlenecks
                if layer_flops > total_forward_flops * 0.1:  # More than 10% of total
                    flop_analysis['optimization_analysis']['bottleneck_layers'].append({
                        'layer_name': name,
                        'layer_type': type(module).__name__,
                        'flops': layer_flops,
                        'percentage_of_total': (layer_flops / max(total_forward_flops, 1)) * 100
                    })
                
            except Exception as e:
                logger.debug(f"FLOP estimation failed for layer {name}: {e}")
                layer_info['estimation_error'] = str(e)
                layer_details.append(layer_info)
        
        # Phase 2: Forward Pass Analysis
        flop_analysis['forward_pass'].update({
            'total_flops': total_forward_flops,
            'layer_breakdown': layer_details,
            'operation_breakdown': operation_counts,
            'per_sample_flops': total_forward_flops / max(batch_size, 1)
        })
        
        # Phase 3: Backward Pass Estimation
        # Backward pass typically requires 2x forward pass FLOPs for gradients
        gradient_flops = total_forward_flops * 2
        
        # Parameter update FLOPs (optimizer dependent)
        total_params = sum(p.numel() for p in model.parameters())
        # Adam optimizer: ~5 operations per parameter (momentum, variance, bias correction, update)
        optimizer_flops = total_params * 5
        
        flop_analysis['backward_pass'].update({
            'estimated_flops': gradient_flops,
            'gradient_computation_flops': gradient_flops,
            'parameter_update_flops': optimizer_flops
        })
        
        # Phase 4: Training FLOPs
        total_training_flops = total_forward_flops + gradient_flops + optimizer_flops
        
        # Estimate per-epoch FLOPs (assuming 1000 samples per epoch)
        samples_per_epoch = 1000
        steps_per_epoch = max(1, samples_per_epoch // batch_size)
        epoch_flops = total_training_flops * steps_per_epoch
        
        flop_analysis['training_flops'].update({
            'total_per_step': total_training_flops,
            'per_epoch_estimate': epoch_flops,
            'optimizer_overhead': optimizer_flops
        })
        
        # Phase 5: Complexity Metrics
        flops_per_param = total_forward_flops / max(total_params, 1)
        
        # Computational density (FLOPs per MB of model)
        model_size_mb = total_params * 4 / (1024 * 1024)
        computational_density = total_forward_flops / max(model_size_mb, 0.001)
        
        # Classify computational complexity
        def _classify_computational_complexity(flops: int) -> str:
            """Classify computational complexity based on FLOP count."""
            if flops < 1e3:
                return 'micro'
            elif flops < 1e4:
                return 'ultra_low'
            elif flops < 1e5:
                return 'very_low'
            elif flops < 1e6:
                return 'low'
            elif flops < 1e7:
                return 'low_medium'
            elif flops < 1e8:
                return 'medium'
            elif flops < 5e8:
                return 'medium_high'
            elif flops < 1e9:
                return 'high'
            elif flops < 5e9:
                return 'very_high'
            elif flops < 1e10:
                return 'ultra_high'
            elif flops < 1e11:
                return 'extreme'
            elif flops < 1e12:
                return 'super_extreme'
            else:
                return 'massive'
        
        complexity_class = _classify_computational_complexity(total_forward_flops)
        
        # Efficiency rating based on FLOPs per parameter
        if flops_per_param < 10:
            efficiency_rating = 'excellent'
        elif flops_per_param < 50:
            efficiency_rating = 'good'
        elif flops_per_param < 200:
            efficiency_rating = 'moderate'
        else:
            efficiency_rating = 'poor'
        
        flop_analysis['complexity_metrics'].update({
            'flops_per_parameter': flops_per_param,
            'computational_density': computational_density,
            'complexity_class': complexity_class,
            'efficiency_rating': efficiency_rating
        })
        
        # Phase 6: Comparative Analysis
        # Compare with standard architectures (rough estimates)
        mobilenet_flops = 569_000_000  # MobileNetV2
        resnet18_flops = 1_814_000_000  # ResNet-18
        
        flop_analysis['comparative_metrics'].update({
            'flops_vs_mobilenet': total_forward_flops / mobilenet_flops,
            'flops_vs_resnet18': total_forward_flops / resnet18_flops,
            'relative_complexity': 'lightweight' if total_forward_flops < mobilenet_flops else 
                                  'moderate' if total_forward_flops < resnet18_flops else 'heavy'
        })
        
        # Phase 7: Optimization Analysis
        optimization_opportunities = []
        
        # Check for inefficient operations
        if operation_counts['activation'] > total_forward_flops * 0.1:
            optimization_opportunities.append('Consider using more efficient activation functions')
        
        if operation_counts['normalization'] > total_forward_flops * 0.2:
            optimization_opportunities.append('Normalization overhead is significant - consider layer normalization')
        
        if len(flop_analysis['optimization_analysis']['bottleneck_layers']) > 0:
            optimization_opportunities.append('Focus optimization on identified bottleneck layers')
        
        # Memory-compute ratio
        total_memory_accesses = sum(layer['memory_access'] for layer in layer_details 
                                  if 'memory_access' in layer and layer['memory_access'] > 0)
        memory_compute_ratio = total_memory_accesses / max(total_forward_flops, 1)
        
        flop_analysis['optimization_analysis'].update({
            'optimization_opportunities': optimization_opportunities,
            'memory_compute_ratio': memory_compute_ratio
        })
        
        # Phase 8: Validation and Warnings
        warnings = []
        
        if total_forward_flops == 0:
            warnings.append('No FLOPs computed - model may be empty or estimation failed')
            # Fallback estimation
            total_forward_flops = total_params * 2 * batch_size
            flop_analysis['forward_pass']['total_flops'] = total_forward_flops
            warnings.append(f'Used fallback estimation: {total_forward_flops} FLOPs')
        
        if len(layer_details) == 0:
            warnings.append('No layers analyzed - check model structure')
        
        if total_forward_flops < 1000:
            warnings.append('Very low FLOP count - verify model complexity')
        
        # Final metadata
        flop_analysis['analysis_metadata'].update({
            'analysis_time_seconds': time.time() - flop_start_time,
            'total_layers_analyzed': len(layer_details),
            'total_parameters': total_params,
            'batch_size_used': batch_size,
            'warnings': warnings
        })
        
        logger.debug(f"Comprehensive FLOP estimation completed: {total_forward_flops:,} FLOPs "
                    f"for {len(layer_details)} layers in {time.time() - flop_start_time:.3f}s")
        
        return flop_analysis
        
    except Exception as e:
        logger.error(f"Comprehensive FLOP estimation failed: {e}")
        
        # Fallback estimation
        try:
            total_params = sum(p.numel() for p in model.parameters())
            fallback_flops = total_params * 2 * batch_size
            
            return {
                'forward_pass': {
                    'total_flops': fallback_flops,
                    'estimation_method': 'fallback'
                },
                'error': str(e),
                'fallback_used': True,
                'analysis_metadata': {
                    'estimation_method': 'parameter_based_fallback',
                    'accuracy_level': 'low',
                    'warnings': [f'Main estimation failed: {str(e)}', 'Using parameter-based fallback']
                }
            }
        except Exception as fallback_error:
            return {
                'error': f'Both main and fallback FLOP estimation failed: {str(e)}, {str(fallback_error)}',
                'forward_pass': {'total_flops': 10000},  # Minimal fallback
                'analysis_metadata': {
                    'estimation_method': 'minimal_fallback',
                    'accuracy_level': 'none',
                    'warnings': ['Complete estimation failure']
                }
            }

def analyze_scaling_behavior(
    model_class: type, 
    base_params: Dict[str, Any], 
    input_dim: int, 
    model_name: str
) -> Dict[str, Any]:
    """
    Comprehensive analysis of model scaling behavior with enhanced metrics and predictions.
    
    This function analyzes how models scale with different input dimensions, batch sizes,
    and architectural parameters, providing detailed scaling curves, performance predictions,
    and resource requirement forecasts. Harmonized with compare_model_architectures().
    
    Args:
        model_class: Model class to analyze
        base_params: Base parameters for model instantiation
        input_dim: Base input dimension
        model_name: Name of the model for logging
        
    Returns:
        Dictionary containing comprehensive scaling analysis
    """
    scaling_start_time = time.time()
    
    try:
        scaling_analysis = {
            'input_dimension_scaling': {
                'data_points': [],
                'scaling_coefficients': {},
                'performance_curves': {},
                'memory_curves': {},
                'recommendations': []
            },
            'batch_size_scaling': {
                'throughput_analysis': [],
                'memory_scaling': [],
                'efficiency_analysis': {},
                'optimal_batch_sizes': {}
            },
            'parameter_scaling': {
                'complexity_growth': 'unknown',
                'efficiency_trends': {},
                'scalability_limits': {},
                'architectural_impact': {}
            },
            'performance_predictions': {
                'inference_time_model': {},
                'memory_usage_model': {},
                'training_time_model': {},
                'confidence_intervals': {}
            },
            'resource_scaling': {
                'cpu_scaling': {},
                'memory_scaling': {},
                'gpu_utilization': {},
                'power_consumption_estimates': {}
            },
            'optimization_recommendations': {
                'scaling_efficiency': [],
                'bottleneck_predictions': [],
                'hardware_recommendations': [],
                'configuration_suggestions': []
            },
            'analysis_metadata': {
                'total_models_tested': 0,
                'successful_tests': 0,
                'failed_tests': 0,
                'analysis_time_seconds': 0,
                'scaling_dimensions_tested': [],
                'warnings': []
            }
        }
        
        logger.debug(f"Starting comprehensive scaling analysis for {model_name}")
        
        # Phase 1: Input Dimension Scaling Analysis
        logger.debug("Phase 1: Input dimension scaling analysis")
        
        # Define comprehensive test dimensions
        dimension_multipliers = [0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0, 4.0]
        test_input_dims = [max(5, int(input_dim * mult)) for mult in dimension_multipliers]
        
        # Add some specific interesting dimensions
        test_input_dims.extend([10, 20, 50, 100, 200, 500])
        test_input_dims = sorted(list(set(test_input_dims)))  # Remove duplicates and sort
        
        # Limit test dimensions to prevent excessive testing
        if len(test_input_dims) > 10:
            test_input_dims = test_input_dims[::max(1, len(test_input_dims)//10)]
        
        input_scaling_data = []
        successful_dim_tests = 0
        
        for test_dim in test_input_dims:
            dim_start_time = time.time()
            try:
                # Create test parameters with appropriate scaling
                test_params = base_params.copy()
                if 'config' in test_params and 'model' in test_params['config']:
                    test_model_config = test_params['config']['model'].copy()
                    
                    # Scale encoding dimension proportionally
                    original_encoding = test_model_config.get('encoding_dim', 16)
                    scaled_encoding = max(4, min(original_encoding, test_dim // 3))
                    test_model_config['encoding_dim'] = scaled_encoding
                    
                    # Scale hidden dimensions proportionally
                    if 'hidden_dims' in test_model_config:
                        original_hidden = test_model_config['hidden_dims']
                        if isinstance(original_hidden, list):
                            scale_factor = min(2.0, test_dim / max(input_dim, 1))
                            scaled_hidden = [max(8, int(dim * scale_factor)) for dim in original_hidden]
                            test_model_config['hidden_dims'] = scaled_hidden
                    
                    test_params['config']['model'] = test_model_config
                    test_params['input_dim'] = test_dim
                
                # Create and analyze test model
                test_model = model_class(**test_params)
                test_model.eval()
                
                # Basic metrics
                param_count = sum(p.numel() for p in test_model.parameters())
                model_size_mb = param_count * 4 / (1024 * 1024)
                
                # Performance testing with multiple batch sizes
                performance_results = []
                test_batch_sizes = [1, 4, 16, 32] if test_dim <= 1000 else [1, 4, 16]
                
                for test_batch in test_batch_sizes:
                    try:
                        test_input = torch.randn(test_batch, test_dim)
                        
                        # Warmup
                        with torch.no_grad():
                            _ = test_model(test_input)
                        
                        # Timing test
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        batch_start_time = time.time()
                        
                        with torch.no_grad():
                            _ = test_model(test_input)
                        
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        batch_time = time.time() - batch_start_time
                        
                        performance_results.append({
                            'batch_size': test_batch,
                            'inference_time_ms': batch_time * 1000,
                            'samples_per_second': test_batch / batch_time,
                            'time_per_sample_ms': (batch_time / test_batch) * 1000
                        })
                        
                    except Exception as batch_error:
                        logger.debug(f"Batch test failed for dim={test_dim}, batch={test_batch}: {batch_error}")
                
                # Memory analysis
                memory_metrics = {}
                try:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        memory_before = torch.cuda.memory_allocated()
                        
                        test_model_gpu = test_model.cuda()
                        test_input_gpu = torch.randn(16, test_dim).cuda()
                        
                        with torch.no_grad():
                            _ = test_model_gpu(test_input_gpu)
                        
                        memory_after = torch.cuda.memory_allocated()
                        gpu_memory_mb = (memory_after - memory_before) / (1024 * 1024)
                        
                        memory_metrics['gpu_memory_mb'] = gpu_memory_mb
                        memory_metrics['memory_per_param'] = gpu_memory_mb / max(param_count, 1) * 1024 * 1024
                        
                        # Clean up
                        del test_model_gpu, test_input_gpu
                        torch.cuda.empty_cache()
                        
                except Exception as memory_error:
                    logger.debug(f"GPU memory analysis failed for dim={test_dim}: {memory_error}")
                
                # FLOP estimation
                try:
                    flop_result = estimate_flops(test_dim, 16, test_model)
                    if isinstance(flop_result, dict):
                        total_flops = flop_result.get('forward_pass', {}).get('total_flops', 0)
                    else:
                        total_flops = flop_result
                    flop_efficiency = total_flops / max(param_count, 1)
                except Exception as flop_error:
                    logger.debug(f"FLOP estimation failed for dim={test_dim}: {flop_error}")
                    total_flops = param_count * 2  # Fallback
                    flop_efficiency = 2.0
                
                # Compile dimension scaling data point
                dim_data_point = {
                    'input_dimension': test_dim,
                    'parameter_count': param_count,
                    'model_size_mb': model_size_mb,
                    'total_flops': total_flops,
                    'flops_per_param': flop_efficiency,
                    'performance_results': performance_results,
                    'memory_metrics': memory_metrics,
                    'analysis_time_ms': (time.time() - dim_start_time) * 1000,
                    'scaling_factor': test_dim / input_dim
                }
                
                # Add best performance metrics for easier analysis
                if performance_results:
                    best_perf = max(performance_results, key=lambda x: x['samples_per_second'])
                    dim_data_point.update({
                        'best_throughput_sps': best_perf['samples_per_second'],
                        'best_batch_size': best_perf['batch_size'],
                        'fastest_inference_ms': min(p['time_per_sample_ms'] for p in performance_results)
                    })
                
                input_scaling_data.append(dim_data_point)
                successful_dim_tests += 1
                scaling_analysis['analysis_metadata']['total_models_tested'] += 1
                
                # Clean up
                del test_model
                
            except Exception as dim_error:
                logger.debug(f"Dimension scaling test failed for dim={test_dim}: {dim_error}")
                scaling_analysis['analysis_metadata']['failed_tests'] += 1
                continue
        
        scaling_analysis['input_dimension_scaling']['data_points'] = input_scaling_data
        scaling_analysis['analysis_metadata']['successful_tests'] += successful_dim_tests
        
        # Phase 2: Analyze Input Dimension Scaling Patterns
        if len(input_scaling_data) >= 3:  # Need minimum data points for analysis
            try:
                # Extract scaling metrics
                dims = [dp['input_dimension'] for dp in input_scaling_data]
                params = [dp['parameter_count'] for dp in input_scaling_data]
                flops = [dp['total_flops'] for dp in input_scaling_data]
                throughputs = [dp.get('best_throughput_sps', 0) for dp in input_scaling_data]
                
                # Calculate scaling coefficients using simple regression
                # Parameter scaling: params = a * dims^b
                log_dims = np.log(np.array(dims))
                log_params = np.log(np.array(params))
                param_coeffs = np.polyfit(log_dims, log_params, 1)
                param_scaling_exp = param_coeffs[0]  # Exponent
                
                # FLOP scaling
                log_flops = np.log(np.array(flops))
                flop_coeffs = np.polyfit(log_dims, log_flops, 1)
                flop_scaling_exp = flop_coeffs[0]
                
                # Performance scaling (inverse relationship expected)
                valid_throughputs = [t for t in throughputs if t > 0]
                if len(valid_throughputs) >= 3:
                    log_throughputs = np.log(np.array(valid_throughputs))
                    perf_dims = [dims[i] for i, t in enumerate(throughputs) if t > 0]
                    log_perf_dims = np.log(np.array(perf_dims))
                    perf_coeffs = np.polyfit(log_perf_dims, log_throughputs, 1)
                    perf_scaling_exp = perf_coeffs[0]
                else:
                    perf_scaling_exp = -1.0  # Default assumption
                
                # Classify scaling complexity directly
                def _classify_scaling_complexity(scaling_exponent: float) -> str:
                    """Classify scaling complexity based on scaling exponent."""
                    try:
                        if scaling_exponent <= 1.2:
                            return 'linear'
                        elif scaling_exponent <= 1.8:
                            return 'sub_quadratic'
                        elif scaling_exponent <= 2.5:
                            return 'quadratic'
                        elif scaling_exponent <= 3.5:
                            return 'cubic'
                        else:
                            return 'super_cubic'
                    except:
                        return 'unknown'
                
                scaling_analysis['input_dimension_scaling']['scaling_coefficients'] = {
                    'parameter_scaling_exponent': float(param_scaling_exp),
                    'flop_scaling_exponent': float(flop_scaling_exp),
                    'performance_scaling_exponent': float(perf_scaling_exp),
                    'parameter_complexity_class': _classify_scaling_complexity(param_scaling_exp),
                    'computational_complexity_class': _classify_scaling_complexity(flop_scaling_exp)
                }
                
                # Generate performance curves for prediction
                scaling_analysis['input_dimension_scaling']['performance_curves'] = {
                    'parameter_growth': f"O(n^{param_scaling_exp:.2f})",
                    'flop_growth': f"O(n^{flop_scaling_exp:.2f})",
                    'throughput_decay': f"O(n^{perf_scaling_exp:.2f})",
                    'memory_growth_estimate': f"O(n^{max(1.0, param_scaling_exp):.2f})"
                }
                
                # Memory scaling analysis
                gpu_memories = [dp['memory_metrics'].get('gpu_memory_mb', 0) for dp in input_scaling_data]
                valid_memories = [(dims[i], mem) for i, mem in enumerate(gpu_memories) if mem > 0]
                
                if len(valid_memories) >= 3:
                    mem_dims, mem_values = zip(*valid_memories)
                    log_mem_dims = np.log(np.array(mem_dims))
                    log_mem_values = np.log(np.array(mem_values))
                    mem_coeffs = np.polyfit(log_mem_dims, log_mem_values, 1)
                    memory_scaling_exp = mem_coeffs[0]
                    
                    scaling_analysis['input_dimension_scaling']['memory_curves'] = {
                        'memory_scaling_exponent': float(memory_scaling_exp),
                        'memory_growth': f"O(n^{memory_scaling_exp:.2f})"
                    }
                
            except Exception as e:
                logger.debug(f"Scaling coefficient analysis failed: {e}")
                scaling_analysis['analysis_metadata']['warnings'].append(f"Scaling analysis failed: {str(e)}")
        
        # Phase 3: Batch Size Scaling Analysis
        logger.debug("Phase 3: Batch size scaling analysis")
        
        try:
            # Test different batch sizes with the base input dimension
            test_batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]
            batch_scaling_data = []
            
            # Create base model for batch testing
            base_test_model = model_class(**base_params)
            base_test_model.eval()
            
            for batch_size in test_batch_sizes:
                try:
                    # Memory constraint check - skip large batches if input is very large
                    if input_dim * batch_size > 100000:  # Arbitrary threshold
                        continue
                    
                    test_input = torch.randn(batch_size, input_dim)
                    
                    # Warmup
                    with torch.no_grad():
                        _ = base_test_model(test_input)
                    
                    # Timing test
                    num_runs = max(3, 20 // batch_size)  # More runs for smaller batches
                    times = []
                    
                    for _ in range(num_runs):
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        start_time = time.time()
                        
                        with torch.no_grad():
                            _ = base_test_model(test_input)
                        
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        times.append(time.time() - start_time)
                    
                    avg_time = sum(times) / len(times)
                    throughput = batch_size / avg_time
                    time_per_sample = avg_time / batch_size
                    
                    # Memory measurement
                    memory_usage = 0
                    if torch.cuda.is_available():
                        try:
                            torch.cuda.empty_cache()
                            mem_before = torch.cuda.memory_allocated()
                            
                            test_model_gpu = base_test_model.cuda() if not next(base_test_model.parameters()).is_cuda else base_test_model
                            test_input_gpu = test_input.cuda()
                            
                            with torch.no_grad():
                                _ = test_model_gpu(test_input_gpu)
                            
                            mem_after = torch.cuda.memory_allocated()
                            memory_usage = (mem_after - mem_before) / (1024 * 1024)  # MB
                            
                            del test_input_gpu
                            torch.cuda.empty_cache()
                            
                        except Exception as mem_error:
                            logger.debug(f"GPU memory measurement failed for batch={batch_size}: {mem_error}")
                    
                    batch_data = {
                        'batch_size': batch_size,
                        'avg_inference_time_ms': avg_time * 1000,
                        'throughput_sps': throughput,
                        'time_per_sample_ms': time_per_sample * 1000,
                        'memory_usage_mb': memory_usage,
                        'memory_per_sample_mb': memory_usage / batch_size if batch_size > 0 else 0,
                        'efficiency_score': throughput / max(memory_usage, 1),
                        'std_time_ms': (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5 * 1000
                    }
                    
                    batch_scaling_data.append(batch_data)
                    
                except Exception as batch_error:
                    logger.debug(f"Batch scaling test failed for batch={batch_size}: {batch_error}")
                    continue
            
            scaling_analysis['batch_size_scaling']['throughput_analysis'] = batch_scaling_data
            
            # Analyze batch scaling patterns
            if len(batch_scaling_data) >= 3:
                try:
                    batch_sizes = [bd['batch_size'] for bd in batch_scaling_data]
                    throughputs = [bd['throughput_sps'] for bd in batch_scaling_data]
                    memories = [bd['memory_usage_mb'] for bd in batch_scaling_data]
                    efficiencies = [bd['efficiency_score'] for bd in batch_scaling_data]
                    
                    # Find optimal batch size
                    max_throughput_idx = throughputs.index(max(throughputs))
                    max_efficiency_idx = efficiencies.index(max(efficiencies))
                    
                    scaling_analysis['batch_size_scaling']['optimal_batch_sizes'] = {
                        'max_throughput': batch_sizes[max_throughput_idx],
                        'max_efficiency': batch_sizes[max_efficiency_idx],
                        'recommended': batch_sizes[max_efficiency_idx]  # Prefer efficiency
                    }
                    
                    # Batch efficiency analysis
                    scaling_analysis['batch_size_scaling']['efficiency_analysis'] = {
                        'linear_scaling_achieved': throughputs[-1] / throughputs[0] > len(batch_sizes) * 0.7,
                        'memory_scaling_linear': memories[-1] / memories[0] < len(batch_sizes) * 1.3,
                        'optimal_efficiency_range': [
                            batch_sizes[max(0, max_efficiency_idx - 1)],
                            batch_sizes[min(len(batch_sizes) - 1, max_efficiency_idx + 1)]
                        ]
                    }
                    
                except Exception as analysis_error:
                    logger.debug(f"Batch scaling analysis failed: {analysis_error}")
            
            # Clean up
            del base_test_model
            
        except Exception as batch_error:
            logger.debug(f"Batch size scaling analysis failed: {batch_error}")
            scaling_analysis['analysis_metadata']['warnings'].append(f"Batch scaling analysis failed: {str(batch_error)}")
        
        # Phase 4: Parameter Scaling Analysis
        logger.debug("Phase 4: Parameter scaling analysis")
        
        try:
            # Analyze how different architectural parameters affect scaling
            if model_name == 'EnhancedAutoencoder':
                # Test different encoding dimensions
                encoding_variants = [8, 16, 32, 64] if input_dim >= 32 else [4, 8, 16]
                encoding_scaling = []
                
                for enc_dim in encoding_variants:
                    try:
                        variant_params = base_params.copy()
                        if 'config' in variant_params and 'model' in variant_params['config']:
                            variant_params['config']['model']['encoding_dim'] = enc_dim
                        
                        variant_model = model_class(**variant_params)
                        variant_params_count = sum(p.numel() for p in variant_model.parameters())
                        
                        # Quick performance test
                        test_input = torch.randn(16, input_dim)
                        with torch.no_grad():
                            start_time = time.time()
                            _ = variant_model(test_input)
                            inference_time = time.time() - start_time
                        
                        encoding_scaling.append({
                            'encoding_dim': enc_dim,
                            'parameter_count': variant_params_count,
                            'inference_time_ms': inference_time * 1000,
                            'params_per_encoding_dim': variant_params_count / enc_dim
                        })
                        
                        del variant_model
                        
                    except Exception as enc_error:
                        logger.debug(f"Encoding dimension test failed for dim={enc_dim}: {enc_error}")
                
                if encoding_scaling:
                    scaling_analysis['parameter_scaling']['architectural_impact']['encoding_dimension'] = encoding_scaling
            
            elif model_name == 'AutoencoderEnsemble':
                # Test different ensemble sizes
                ensemble_sizes = [2, 3, 5] if base_params.get('config', {}).get('model', {}).get('num_models', 3) >= 3 else [2]
                ensemble_scaling = []
                
                for ensemble_size in ensemble_sizes:
                    try:
                        variant_params = base_params.copy()
                        if 'config' in variant_params and 'model' in variant_params['config']:
                            variant_params['config']['model']['num_models'] = ensemble_size
                        
                        variant_model = model_class(**variant_params)
                        variant_params_count = sum(p.numel() for p in variant_model.parameters())
                        
                        # Quick performance test
                        test_input = torch.randn(8, input_dim)  # Smaller batch for ensemble
                        with torch.no_grad():
                            start_time = time.time()
                            _ = variant_model(test_input)
                            inference_time = time.time() - start_time
                        
                        ensemble_scaling.append({
                            'ensemble_size': ensemble_size,
                            'parameter_count': variant_params_count,
                            'inference_time_ms': inference_time * 1000,
                            'params_per_model': variant_params_count / ensemble_size,
                            'time_scaling_factor': inference_time * 1000 / ensemble_size
                        })
                        
                        del variant_model
                        
                    except Exception as ens_error:
                        logger.debug(f"Ensemble size test failed for size={ensemble_size}: {ens_error}")
                
                if ensemble_scaling:
                    scaling_analysis['parameter_scaling']['architectural_impact']['ensemble_size'] = ensemble_scaling
            
            # Complexity growth analysis
            base_param_count = sum(p.numel() for p in model_class(**base_params).parameters())
            
            if input_scaling_data:
                max_params = max(dp['parameter_count'] for dp in input_scaling_data)
                min_params = min(dp['parameter_count'] for dp in input_scaling_data)
                param_growth_ratio = max_params / max(min_params, 1)
                
                if param_growth_ratio < 2:
                    complexity_growth = 'linear'
                elif param_growth_ratio < 10:
                    complexity_growth = 'polynomial'
                else:
                    complexity_growth = 'exponential'
                
                scaling_analysis['parameter_scaling']['complexity_growth'] = complexity_growth
                scaling_analysis['parameter_scaling']['parameter_growth_ratio'] = param_growth_ratio
            
        except Exception as param_error:
            logger.debug(f"Parameter scaling analysis failed: {param_error}")
            scaling_analysis['analysis_metadata']['warnings'].append(f"Parameter scaling analysis failed: {str(param_error)}")
        
        # Phase 5: Performance Predictions and Modeling
        logger.debug("Phase 5: Performance prediction modeling")
        
        try:
            if len(input_scaling_data) >= 3:
                # Simple polynomial fitting for predictions
                dims = np.array([dp['input_dimension'] for dp in input_scaling_data])
                
                # Inference time prediction model
                inference_times = np.array([dp.get('fastest_inference_ms', 0) for dp in input_scaling_data])
                valid_times = inference_times > 0
                
                if np.any(valid_times):
                    valid_dims = dims[valid_times]
                    valid_inference = inference_times[valid_times]
                    
                    try:
                        # Fit polynomial (degree 2)
                        time_coeffs = np.polyfit(valid_dims, valid_inference, 2)
                        
                        # Prediction for common input sizes
                        prediction_dims = [50, 100, 200, 500, 1000]
                        time_predictions = {}
                        
                        for pred_dim in prediction_dims:
                            if pred_dim not in dims:  # Don't predict for tested dimensions
                                pred_time = np.polyval(time_coeffs, pred_dim)
                                time_predictions[pred_dim] = max(0.1, pred_time)  # Minimum realistic time
                        
                        scaling_analysis['performance_predictions']['inference_time_model'] = {
                            'coefficients': time_coeffs.tolist(),
                            'predictions_ms': time_predictions,
                            'model_type': 'polynomial_degree_2'
                        }
                        
                    except Exception as pred_error:
                        logger.debug(f"Time prediction modeling failed: {pred_error}")
                
                # Memory usage prediction model
                mem_usages = np.array([dp['memory_metrics'].get('gpu_memory_mb', 0) for dp in input_scaling_data])
                valid_mem = mem_usages > 0
                
                if np.any(valid_mem):
                    valid_dims_mem = dims[valid_mem]
                    valid_memories = mem_usages[valid_mem]
                    
                    try:
                        mem_coeffs = np.polyfit(valid_dims_mem, valid_memories, 2)
                        
                        mem_predictions = {}
                        for pred_dim in prediction_dims:
                            if pred_dim not in dims:
                                pred_mem = np.polyval(mem_coeffs, pred_dim)
                                mem_predictions[pred_dim] = max(1.0, pred_mem)  # Minimum 1MB
                        
                        scaling_analysis['performance_predictions']['memory_usage_model'] = {
                            'coefficients': mem_coeffs.tolist(),
                            'predictions_mb': mem_predictions,
                            'model_type': 'polynomial_degree_2'
                        }
                        
                    except Exception as mem_pred_error:
                        logger.debug(f"Memory prediction modeling failed: {mem_pred_error}")
        
        except Exception as prediction_error:
            logger.debug(f"Performance prediction phase failed: {prediction_error}")
        
        # Phase 6: Optimization Recommendations
        logger.debug("Phase 6: Generating optimization recommendations")
        
        try:
            recommendations = {
                'scaling_efficiency': [],
                'bottleneck_predictions': [],
                'hardware_recommendations': [],
                'configuration_suggestions': []
            }
            
            # Scaling efficiency recommendations
            if scaling_analysis['input_dimension_scaling']['scaling_coefficients']:
                param_exp = scaling_analysis['input_dimension_scaling']['scaling_coefficients'].get('parameter_scaling_exponent', 2.0)
                flop_exp = scaling_analysis['input_dimension_scaling']['scaling_coefficients'].get('flop_scaling_exponent', 2.0)
                
                if param_exp > 3.0:
                    recommendations['scaling_efficiency'].append("High parameter growth - consider dimensionality reduction")
                elif param_exp < 1.5:
                    recommendations['scaling_efficiency'].append("Efficient parameter scaling - good for large inputs")
                
                if flop_exp > 3.0:
                    recommendations['scaling_efficiency'].append("High computational growth - optimize for smaller inputs")
                elif flop_exp < 2.0:
                    recommendations['scaling_efficiency'].append("Good computational scaling")
            
            # Bottleneck predictions
            if input_scaling_data:
                large_dim_data = [dp for dp in input_scaling_data if dp['input_dimension'] > input_dim * 2]
                if large_dim_data:
                    avg_large_throughput = sum(dp.get('best_throughput_sps', 0) for dp in large_dim_data) / len(large_dim_data)
                    small_dim_data = [dp for dp in input_scaling_data if dp['input_dimension'] <= input_dim]
                    if small_dim_data:
                        avg_small_throughput = sum(dp.get('best_throughput_sps', 0) for dp in small_dim_data) / len(small_dim_data)
                        
                        if avg_large_throughput < avg_small_throughput * 0.3:  # >70% performance drop
                            recommendations['bottleneck_predictions'].append("Severe performance degradation at large input sizes")
                        elif avg_large_throughput < avg_small_throughput * 0.6:  # >40% performance drop
                            recommendations['bottleneck_predictions'].append("Moderate performance degradation at scale")
            
            # Hardware recommendations
            max_memory_mb = 0
            if input_scaling_data:
                memory_usages = [dp['memory_metrics'].get('gpu_memory_mb', 0) for dp in input_scaling_data]
                max_memory_mb = max(memory_usages) if memory_usages else 0
            
            if max_memory_mb > 8000:  # >8GB
                recommendations['hardware_recommendations'].append("High-end GPU recommended for large inputs")
            elif max_memory_mb > 4000:  # >4GB
                recommendations['hardware_recommendations'].append("Mid-range GPU sufficient")
            else:
                recommendations['hardware_recommendations'].append("Entry-level GPU adequate")
            
            # Configuration suggestions
            if scaling_analysis['batch_size_scaling']['optimal_batch_sizes']:
                optimal_batch = scaling_analysis['batch_size_scaling']['optimal_batch_sizes'].get('recommended', 32)
                recommendations['configuration_suggestions'].append(f"Optimal batch size: {optimal_batch}")
            
            if model_name == 'AutoencoderEnsemble':
                recommendations['configuration_suggestions'].append("Consider ensemble size vs. resource trade-off")
            
            if len(input_scaling_data) > 0:
                param_counts = [dp['parameter_count'] for dp in input_scaling_data]
                if max(param_counts) > 1000000:  # >1M parameters
                    recommendations['configuration_suggestions'].append("Large model - consider mixed precision training")
            
            scaling_analysis['optimization_recommendations'] = recommendations
            
        except Exception as rec_error:
            logger.debug(f"Optimization recommendations generation failed: {rec_error}")
        
        # Final metadata and cleanup
        scaling_analysis['analysis_metadata'].update({
            'analysis_time_seconds': time.time() - scaling_start_time,
            'scaling_dimensions_tested': [dp['input_dimension'] for dp in input_scaling_data],
            'batch_sizes_tested': [bd['batch_size'] for bd in scaling_analysis['batch_size_scaling']['throughput_analysis']],
            'total_successful_tests': len(input_scaling_data) + len(scaling_analysis['batch_size_scaling']['throughput_analysis'])
        })
        
        # Clean up any remaining tensors
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.debug(f"Comprehensive scaling analysis completed for {model_name} in {time.time() - scaling_start_time:.2f}s")
        
        return scaling_analysis
        
    except Exception as e:
        logger.error(f"Comprehensive scaling analysis failed for {model_name}: {e}")
        
        return {
            'error': f'Scaling analysis failed: {str(e)}',
            'analysis_metadata': {
                'analysis_time_seconds': time.time() - scaling_start_time if 'scaling_start_time' in locals() else 0,
                'error_type': type(e).__name__,
                'warnings': [f'Complete analysis failure: {str(e)}']
            }
        }

def analyze_memory_usage(
    batch_size: int,
    input_dim: int,
    model_name: str,
    model: torch.nn.Module,
    hardware_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Comprehensive memory usage analysis with system awareness.
    
    This function provides detailed memory analysis including GPU memory usage,
    CPU memory estimation, and system-aware recommendations. Harmonized with
    compare_model_architectures() parameter structure.
    
    Args:
        batch_size: Batch size for memory analysis
        input_dim: Input dimension size
        model_name: Name of the model for logging
        model: PyTorch model to analyze
        hardware_data: Hardware information from system context
        
    Returns:
        Dictionary containing comprehensive memory analysis
    """
    try:
        memory_analysis = {
            'system_context': {
                'system_class': 'unknown',
                'total_system_memory_gb': 0,
                'memory_performance_class': 'unknown',
                'current_memory_usage_percent': 0,
                'gpu_available': False,
                'total_gpu_memory_gb': 0
            },
            'cpu_memory_analysis': {
                'parameter_memory_mb': 0,
                'estimated_activation_memory_mb': 0,
                'estimated_total_cpu_memory_mb': 0,
                'memory_pressure_score': 0,
                'system_memory_adequate': False
            },
            'gpu_memory_detailed': {
                'allocated_mb': 0,
                'peak_mb': 0,
                'per_sample_kb': 0,
                'memory_efficiency_score': 0,
                'snapshots': []
            },
            'model_complexity': {
                'total_parameters': 0,
                'model_size_mb': 0,
                'complexity_class': 'unknown',
                'memory_efficiency_score': 0
            },
            'recommendations': [],
            'analysis_metadata': {
                'analysis_method': 'comprehensive_system_aware',
                'gpu_analysis_performed': False,
                'warnings': []
            }
        }
        
        logger.debug(f"Starting comprehensive memory analysis for {model_name}")
        
        # Extract hardware information from the provided hardware_data
        gpu_available = hardware_data.get('gpu_available', False)
        system_memory_gb = hardware_data.get('system_memory_gb', 8.0)
        memory_usage_percent = hardware_data.get('memory_usage_percent', 0)
        gpu_memory_gb = hardware_data.get('gpu_memory_gb', 0)
        system_class = hardware_data.get('system_class', 'unknown')
        
        # SYSTEM CONTEXT INTEGRATION
        memory_analysis['system_context'].update({
            'system_class': system_class,
            'total_system_memory_gb': system_memory_gb,
            'current_memory_usage_percent': memory_usage_percent,
            'gpu_available': gpu_available,
            'total_gpu_memory_gb': gpu_memory_gb
        })
        
        # ENHANCED GPU MEMORY ANALYSIS with system awareness
        if gpu_available and torch.cuda.is_available():
            try:
                # Enhanced memory monitoring with system context
                torch.cuda.empty_cache()
                memory_before = torch.cuda.memory_allocated()
                
                # Move model and create input with optimal memory management
                model_gpu = model.cuda() if not next(model.parameters()).is_cuda else model
                
                # ADAPTIVE BATCH SIZE based on system memory
                test_batch_size = batch_size
                if system_memory_gb < 8:
                    test_batch_size = min(batch_size, 32)
                    memory_analysis['batch_size_adjusted'] = {
                        'original': batch_size,
                        'adjusted': test_batch_size,
                        'reason': 'limited_system_memory'
                    }
                
                test_input_gpu = torch.randn(test_batch_size, input_dim).cuda()
                
                # COMPREHENSIVE MEMORY PROFILING
                memory_snapshots = []
                
                # Baseline measurement
                torch.cuda.synchronize()
                baseline_memory = torch.cuda.memory_allocated()
                memory_snapshots.append(('baseline', baseline_memory))
                
                # Forward pass with detailed tracking
                with torch.no_grad():
                    torch.cuda.reset_peak_memory_stats()
                    output = model_gpu(test_input_gpu)
                    torch.cuda.synchronize()
                    
                    forward_memory = torch.cuda.memory_allocated()
                    peak_memory = torch.cuda.max_memory_allocated()
                    memory_snapshots.append(('forward', forward_memory))
                    memory_snapshots.append(('peak', peak_memory))
                
                # Calculate comprehensive memory metrics
                gpu_memory_mb = (forward_memory - memory_before) / (1024 * 1024)
                peak_memory_mb = peak_memory / (1024 * 1024)
                
                memory_analysis['gpu_memory_detailed'] = {
                    'allocated_mb': gpu_memory_mb,
                    'peak_mb': peak_memory_mb,
                    'per_sample_kb': (gpu_memory_mb * 1024) / test_batch_size,
                    'memory_efficiency_score': test_batch_size / max(gpu_memory_mb, 1),
                    'snapshots': [(name, mem / (1024 * 1024)) for name, mem in memory_snapshots]
                }
                
                # SYSTEM-AWARE MEMORY RECOMMENDATIONS
                memory_usage_percent = (peak_memory_mb / 1024) / max(gpu_memory_gb, 1) * 100 if gpu_memory_gb > 0 else 0
                
                memory_analysis['gpu_memory_recommendations'] = []
                
                if memory_usage_percent > 80:
                    memory_analysis['gpu_memory_recommendations'].append(
                        f"High GPU memory usage ({memory_usage_percent:.1f}%) - consider smaller batch sizes"
                    )
                elif memory_usage_percent < 30 and gpu_memory_gb > 4:
                    memory_analysis['gpu_memory_recommendations'].append(
                        f"Low GPU memory usage ({memory_usage_percent:.1f}%) - can use larger batch sizes"
                    )
                
                # Clean up GPU memory
                del test_input_gpu, output
                if model_gpu is not model:
                    del model_gpu
                
                torch.cuda.empty_cache()
                memory_analysis['analysis_metadata']['gpu_analysis_performed'] = True
                
            except Exception as gpu_error:
                logger.debug(f"GPU memory analysis failed: {gpu_error}")
                memory_analysis['analysis_metadata']['warnings'].append(f"GPU analysis failed: {str(gpu_error)}")
        
        # ENHANCED CPU MEMORY ESTIMATION with system awareness
        total_params = sum(p.numel() for p in model.parameters())
        param_memory_mb = total_params * 4 / (1024 * 1024)  # float32
        
        # SYSTEM-AWARE ACTIVATION MEMORY ESTIMATION
        # Try to estimate hidden dimensions from model structure
        estimated_hidden = input_dim * 2  # Conservative fallback
        
        # Check for common model attributes to improve estimation
        if hasattr(model, 'encoding_dim'):
            estimated_hidden = model.encoding_dim * 4
        elif hasattr(model, 'hidden_dims'):
            if hasattr(model.hidden_dims, '__len__') and len(model.hidden_dims) > 0:
                estimated_hidden = sum(model.hidden_dims) / len(model.hidden_dims)
        
        activation_memory_mb = estimated_hidden * batch_size * 4 / (1024 * 1024)
        estimated_cpu_memory_mb = param_memory_mb + activation_memory_mb
        
        # SYSTEM CONTEXT ANALYSIS
        memory_pressure_score = 0.0
        if system_memory_gb > 0:
            estimated_additional_percent = (estimated_cpu_memory_mb / 1024) / system_memory_gb * 100
            total_projected_usage = memory_usage_percent + estimated_additional_percent
            
            if total_projected_usage > 90:
                memory_pressure_score = 1.0  # High pressure
            elif total_projected_usage > 70:
                memory_pressure_score = 0.5  # Medium pressure
            else:
                memory_pressure_score = 0.0  # Low pressure
        
        # Classify model complexity directly
        if total_params < 10000:
            complexity_class = 'very_simple'
        elif total_params < 100000:
            complexity_class = 'simple'
        elif total_params < 1000000:
            complexity_class = 'medium'
        elif total_params < 10000000:
            complexity_class = 'complex'
        else:
            complexity_class = 'very_complex'
        
        memory_analysis['cpu_memory_analysis'].update({
            'parameter_memory_mb': param_memory_mb,
            'estimated_activation_memory_mb': activation_memory_mb,
            'estimated_total_cpu_memory_mb': estimated_cpu_memory_mb,
            'memory_pressure_score': memory_pressure_score,
            'system_memory_adequate': estimated_cpu_memory_mb < (system_memory_gb * 1024 * 0.3) if system_memory_gb > 0 else False
        })
        
        memory_analysis['model_complexity'].update({
            'total_parameters': total_params,
            'model_size_mb': param_memory_mb,
            'complexity_class': complexity_class,
            'memory_efficiency_score': 0  # Will be updated if throughput data is available
        })
        
        # COMPREHENSIVE RECOMMENDATIONS using system analysis
        recommendations = []
        
        # System class based recommendations
        if system_class == 'limited':
            recommendations.append("Limited system detected - consider using gradient checkpointing to reduce memory usage")
            if estimated_cpu_memory_mb > 500:
                recommendations.append("Model may be too large for system - consider using a smaller architecture")
        elif system_class == 'high_performance':
            if estimated_cpu_memory_mb < 1000 and system_memory_gb > 16:
                recommendations.append("High-performance system with ample memory - can handle larger models or batch sizes")
        
        # Memory pressure recommendations
        if memory_pressure_score > 0.5:
            recommendations.extend([
                "High memory pressure detected - consider reducing batch size",
                "Enable gradient accumulation to maintain effective batch size with lower memory usage"
            ])
        
        # GPU-specific recommendations
        if gpu_available:
            if gpu_memory_gb < 4:
                recommendations.append("Limited GPU memory - use mixed precision training to reduce memory usage")
            elif gpu_memory_gb > 8:
                recommendations.append("Ample GPU memory available - can use larger batch sizes or more complex models")
        
        memory_analysis['recommendations'] = recommendations
        
        logger.debug(f"Comprehensive memory analysis completed for {model_name}")
        
        return memory_analysis
        
    except Exception as e:
        logger.error(f"Comprehensive memory analysis failed for {model_name}: {e}")
        
        return {
            'error': f'Memory analysis failed: {str(e)}',
            'analysis_metadata': {
                'analysis_method': 'failed',
                'warnings': [f'Complete analysis failure: {str(e)}']
            }
        }

def estimate_training_resources(
    total_params: int,
    batch_size: int,
    input_dim: int,
    config: Dict[str, Any],
    model_name: str,
    hardware_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Comprehensive training resource estimation with detailed analysis and recommendations.
    
    This function provides enhanced resource estimation including memory requirements,
    computational needs, time estimates, hardware recommendations, and optimization
    strategies for different training scenarios. Harmonized with compare_model_architectures().
    
    Args:
        total_params: Number of model parameters
        batch_size: Training batch size
        input_dim: Input dimension size
        config: Configuration dictionary for detailed analysis
        model_name: Model name for logging and recommendations
        hardware_data: Hardware information from system context
        
    Returns:
        Dictionary containing comprehensive resource requirement estimates
    """
    estimation_start_time = time.time()
    
    try:
        logger.debug(f"Starting comprehensive training resource estimation for {model_name}")
        
        resources = {
            'memory': {
                'training_memory': {},
                'inference_memory': {},
                'optimization_memory': {},
                'total_estimates': {},
                'memory_breakdown': {},
                'memory_optimization': {}
            },
            'compute': {
                'training_compute': {},
                'inference_compute': {},
                'flops_analysis': {},
                'computational_intensity': {},
                'compute_optimization': {}
            },
            'time_estimates': {
                'training_time': {},
                'inference_time': {},
                'epoch_estimates': {},
                'convergence_estimates': {},
                'hardware_scaling': {}
            },
            'hardware_requirements': {
                'minimum_requirements': {},
                'recommended_requirements': {},
                'optimal_requirements': {},
                'hardware_compatibility': {},
                'upgrade_recommendations': {}
            },
            'optimization_strategies': {
                'memory_optimization': [],
                'compute_optimization': [],
                'training_optimization': [],
                'hardware_optimization': [],
                'cost_optimization': []
            },
            'scaling_analysis': {
                'batch_size_scaling': {},
                'parameter_scaling': {},
                'data_scaling': {},
                'hardware_scaling': {}
            },
            'cost_analysis': {
                'training_cost_estimates': {},
                'inference_cost_estimates': {},
                'hardware_cost_estimates': {},
                'optimization_savings': {}
            },
            'analysis_metadata': {
                'model_name': model_name,
                'param_count': total_params,
                'batch_size': batch_size,
                'input_dim': input_dim,
                'estimation_timestamp': datetime.now().isoformat(),
                'estimation_duration_seconds': 0,
                'hardware_analysis_available': bool(hardware_data),
                'config_analysis_available': bool(config),
                'warnings': [],
                'assumptions': [],
                'validation_checks': []
            }
        }
        
        # Extract configuration details if available
        model_config = config.get('model', {}) if config else {}
        training_config = config.get('training', {}) if config else {}
        
        # Basic validation and assumptions
        if total_params <= 0:
            resources['analysis_metadata']['warnings'].append("Invalid parameter count, using default estimate")
            total_params = 100000
        
        if batch_size <= 0:
            resources['analysis_metadata']['warnings'].append("Invalid batch size, using default")
            batch_size = 32
        
        if input_dim <= 0:
            resources['analysis_metadata']['warnings'].append("Invalid input dimension, using default")
            input_dim = 20
        
        # Phase 1: Comprehensive Memory Analysis
        try:
            logger.debug(f"Phase 1: Memory analysis for {model_name}")
            
            # Memory constants
            bytes_per_float32 = 4
            bytes_per_float16 = 2
            mixed_precision = training_config.get('mixed_precision', False)
            precision_bytes = bytes_per_float16 if mixed_precision else bytes_per_float32
            
            # Model parameters memory
            param_memory_bytes = total_params * bytes_per_float32  # Always FP32
            
            # Gradient memory (same size as parameters)
            gradient_memory_bytes = param_memory_bytes
            
            # Optimizer state memory
            optimizer_type = training_config.get('optimizer', 'AdamW').lower()
            if optimizer_type in ['adam', 'adamw']:
                # Adam/AdamW: momentum (FP32) + variance (FP32) + possibly more
                optimizer_state_multiplier = 2.0
                optimizer_memory_bytes = param_memory_bytes * optimizer_state_multiplier
            elif optimizer_type == 'sgd':
                # SGD with momentum: momentum buffer
                optimizer_state_multiplier = 1.0
                optimizer_memory_bytes = param_memory_bytes * optimizer_state_multiplier
            else:
                # Conservative estimate
                optimizer_state_multiplier = 1.5
                optimizer_memory_bytes = param_memory_bytes * optimizer_state_multiplier
            
            # Activation memory estimation
            # Estimate based on model architecture
            hidden_dims = model_config.get('hidden_dims', [128, 64])
            encoding_dim = model_config.get('encoding_dim', 32)
            
            # Calculate activation memory for forward pass
            activation_sizes = [input_dim] + hidden_dims + [encoding_dim] + list(reversed(hidden_dims)) + [input_dim]
            total_activation_elements = sum(activation_sizes) * batch_size
            activation_memory_bytes = total_activation_elements * precision_bytes
            
            # Additional memory for intermediate computations
            intermediate_memory_bytes = activation_memory_bytes * 0.5
            
            # Data batch memory (input + target)
            data_memory_bytes = input_dim * batch_size * precision_bytes * 2
            
            # Total training memory
            total_training_memory_bytes = (
                param_memory_bytes + 
                gradient_memory_bytes + 
                optimizer_memory_bytes + 
                activation_memory_bytes + 
                intermediate_memory_bytes + 
                data_memory_bytes
            )
            
            # System overhead and fragmentation
            overhead_factor = 1.5  # 50% overhead
            total_memory_with_overhead = total_training_memory_bytes * overhead_factor
            
            # Convert to more readable units
            def bytes_to_units(bytes_val):
                mb = bytes_val / (1024 ** 2)
                gb = bytes_val / (1024 ** 3)
                return {'bytes': bytes_val, 'mb': mb, 'gb': gb}
            
            resources['memory']['training_memory'] = {
                'parameters': bytes_to_units(param_memory_bytes),
                'gradients': bytes_to_units(gradient_memory_bytes),
                'optimizer_state': bytes_to_units(optimizer_memory_bytes),
                'activations': bytes_to_units(activation_memory_bytes),
                'intermediate_computations': bytes_to_units(intermediate_memory_bytes),
                'data_batch': bytes_to_units(data_memory_bytes),
                'total_training': bytes_to_units(total_training_memory_bytes),
                'total_with_overhead': bytes_to_units(total_memory_with_overhead),
                'optimizer_state_multiplier': optimizer_state_multiplier,
                'precision_used': 'FP16' if mixed_precision else 'FP32'
            }
            
            # Inference memory (much smaller)
            inference_memory_bytes = param_memory_bytes + activation_memory_bytes + data_memory_bytes
            resources['memory']['inference_memory'] = {
                'total_inference': bytes_to_units(inference_memory_bytes),
                'parameters': bytes_to_units(param_memory_bytes),
                'activations': bytes_to_units(activation_memory_bytes),
                'data_batch': bytes_to_units(data_memory_bytes)
            }
            
            # Memory breakdown percentages
            total_mem = total_memory_with_overhead
            resources['memory']['memory_breakdown'] = {
                'parameters_percent': (param_memory_bytes / total_mem) * 100,
                'gradients_percent': (gradient_memory_bytes / total_mem) * 100,
                'optimizer_percent': (optimizer_memory_bytes / total_mem) * 100,
                'activations_percent': (activation_memory_bytes / total_mem) * 100,
                'data_percent': (data_memory_bytes / total_mem) * 100,
                'overhead_percent': ((total_mem - total_training_memory_bytes) / total_mem) * 100
            }
            
            # Memory optimization suggestions
            memory_optimizations = []
            if mixed_precision:
                memory_optimizations.append("Using mixed precision (FP16) - saves ~50% activation memory")
            else:
                memory_optimizations.append("Consider enabling mixed precision to reduce memory usage")
            
            if batch_size > 64:
                memory_optimizations.append("Large batch size detected - consider gradient accumulation")
            
            if total_params > 1000000:
                memory_optimizations.append("Large model - consider gradient checkpointing")
            
            resources['memory']['memory_optimization'] = {
                'suggestions': memory_optimizations,
                'potential_savings': {
                    'mixed_precision_savings_mb': (activation_memory_bytes * 0.5) / (1024**2) if not mixed_precision else 0,
                    'gradient_checkpointing_savings_mb': (activation_memory_bytes * 0.7) / (1024**2),
                    'smaller_batch_savings_mb': ((batch_size - 16) * input_dim * precision_bytes * 2) / (1024**2) if batch_size > 16 else 0
                }
            }
            
            resources['analysis_metadata']['validation_checks'].append('memory_analysis_completed')
            
        except Exception as e:
            logger.error(f"Memory analysis failed for {model_name}: {e}")
            resources['memory']['error'] = str(e)
            resources['analysis_metadata']['warnings'].append(f"Memory analysis failed: {str(e)}")
        
        # Phase 2: Computational Requirements Analysis
        try:
            logger.debug(f"Phase 2: Computational analysis for {model_name}")
            
            # FLOP estimation for forward pass
            # Rough estimate based on matrix multiplications
            forward_flops = 0
            layer_dims = [input_dim] + hidden_dims + [encoding_dim]
            
            # Encoder FLOPs
            for i in range(len(layer_dims) - 1):
                # Matrix multiplication: input_dim × output_dim × batch_size × 2 (multiply-add)
                layer_flops = layer_dims[i] * layer_dims[i + 1] * batch_size * 2
                forward_flops += layer_flops
            
            # Decoder FLOPs (reverse)
            decoder_dims = [encoding_dim] + list(reversed(hidden_dims)) + [input_dim]
            for i in range(len(decoder_dims) - 1):
                layer_flops = decoder_dims[i] * decoder_dims[i + 1] * batch_size * 2
                forward_flops += layer_flops
            
            # Activation function FLOPs (approximate)
            activation_flops = sum(layer_dims[1:]) * batch_size  # One FLOP per activation
            activation_flops += sum(decoder_dims[1:-1]) * batch_size
            forward_flops += activation_flops
            
            # Backward pass FLOPs (typically 2x forward pass)
            backward_flops = forward_flops * 2
            
            # Optimizer FLOPs
            if optimizer_type in ['adam', 'adamw']:
                # Adam: momentum update, variance update, bias correction, parameter update
                optimizer_flops = total_params * 5
            elif optimizer_type == 'sgd':
                # SGD: momentum update, parameter update
                optimizer_flops = total_params * 2
            else:
                optimizer_flops = total_params * 3  # Conservative estimate
            
            total_training_flops = forward_flops + backward_flops + optimizer_flops
            
            # Classify computational intensity directly
            def _classify_computational_intensity(flops_per_step: int) -> str:
                """Classify computational intensity for training steps."""
                if flops_per_step < 1e3:
                    return 'negligible'
                elif flops_per_step < 1e4:
                    return 'minimal'
                elif flops_per_step < 1e5:
                    return 'very_light'
                elif flops_per_step < 1e6:
                    return 'light'
                elif flops_per_step < 1e7:
                    return 'light_moderate'
                elif flops_per_step < 1e8:
                    return 'moderate'
                elif flops_per_step < 5e8:
                    return 'moderate_high'
                elif flops_per_step < 1e9:
                    return 'high'
                elif flops_per_step < 5e9:
                    return 'very_high'
                elif flops_per_step < 1e10:
                    return 'intensive'
                elif flops_per_step < 5e10:
                    return 'very_intensive'
                elif flops_per_step < 1e11:
                    return 'ultra_intensive'
                elif flops_per_step < 1e12:
                    return 'extreme'
                else:
                    return 'massive'
            
            computational_intensity = _classify_computational_intensity(total_training_flops)
            
            resources['compute']['training_compute'] = {
                'forward_pass_flops': forward_flops,
                'backward_pass_flops': backward_flops,
                'optimizer_flops': optimizer_flops,
                'total_training_step_flops': total_training_flops,
                'flops_per_parameter': total_training_flops / total_params,
                'computational_intensity': computational_intensity
            }
            
            resources['compute']['inference_compute'] = {
                'inference_flops': forward_flops,
                'flops_per_sample': forward_flops / batch_size,
                'computational_efficiency': forward_flops / total_params
            }
            
            # FLOP analysis by operation type
            resources['compute']['flops_analysis'] = {
                'matrix_multiplication_percent': ((forward_flops + backward_flops - activation_flops * 3) / total_training_flops) * 100,
                'activation_function_percent': ((activation_flops * 3) / total_training_flops) * 100,
                'optimizer_percent': (optimizer_flops / total_training_flops) * 100,
                'floating_point_intensity': 'high' if total_training_flops > 1e9 else 'medium' if total_training_flops > 1e6 else 'low'
            }
            
            resources['analysis_metadata']['validation_checks'].append('compute_analysis_completed')
            
        except Exception as e:
            logger.error(f"Computational analysis failed for {model_name}: {e}")
            resources['compute']['error'] = str(e)
            resources['analysis_metadata']['warnings'].append(f"Computational analysis failed: {str(e)}")
        
        # Phase 3: Time Estimation Analysis
        try:
            logger.debug(f"Phase 3: Time estimation analysis for {model_name}")
            
            # Extract hardware information
            gpu_available = hardware_data.get('gpu_available', False)
            gpu_memory_gb = hardware_data.get('gpu_memory_gb', 0)
            cpu_count = hardware_data.get('cpu_count', os.cpu_count() or 1)
            
            # Estimate computational capacity
            if gpu_available and gpu_memory_gb > 0:
                # GPU performance estimates (FLOPS/second)
                if gpu_memory_gb >= 24:  # High-end GPU
                    gpu_flops_per_second = 15e12  # 15 TFLOPS
                elif gpu_memory_gb >= 8:   # Mid-range GPU
                    gpu_flops_per_second = 8e12   # 8 TFLOPS
                elif gpu_memory_gb >= 4:   # Entry-level GPU
                    gpu_flops_per_second = 4e12   # 4 TFLOPS
                else:
                    gpu_flops_per_second = 2e12   # Basic GPU
                
                # Memory constraint check
                required_memory_gb = resources['memory']['training_memory']['total_with_overhead']['gb']
                
                if required_memory_gb <= gpu_memory_gb:
                    computational_capacity = gpu_flops_per_second
                    device_type = 'GPU (unconstrained)'
                    memory_constraint_factor = 1.0
                else:
                    # Memory constrained - performance degradation
                    constraint_ratio = gpu_memory_gb / required_memory_gb
                    memory_constraint_factor = constraint_ratio * 0.7  # Significant degradation
                    computational_capacity = gpu_flops_per_second * memory_constraint_factor
                    device_type = 'GPU (memory constrained)'
            else:
                # CPU performance estimates
                cpu_flops_per_second = 1e10 * min(cpu_count, 16)  # ~10 GFLOPS per core, diminishing returns after 16
                computational_capacity = cpu_flops_per_second
                device_type = 'CPU'
                memory_constraint_factor = 1.0
            
            # Time per training step
            if 'total_training_step_flops' in resources['compute'].get('training_compute', {}):
                flops_per_step = resources['compute']['training_compute']['total_training_step_flops']
                time_per_step_seconds = flops_per_step / computational_capacity
                
                # Estimate samples per epoch and steps per epoch
                estimated_samples_per_epoch = training_config.get('samples_per_epoch', 10000)
                steps_per_epoch = max(1, estimated_samples_per_epoch // batch_size)
                
                time_per_epoch_seconds = time_per_step_seconds * steps_per_epoch
                time_per_epoch_minutes = time_per_epoch_seconds / 60
                time_per_epoch_hours = time_per_epoch_minutes / 60
                
                # Convergence estimates
                estimated_epochs_to_converge = training_config.get('epochs', 100)
                total_training_time_hours = time_per_epoch_hours * estimated_epochs_to_converge
                
                resources['time_estimates']['training_time'] = {
                    'time_per_step_ms': time_per_step_seconds * 1000,
                    'time_per_epoch_minutes': time_per_epoch_minutes,
                    'time_per_epoch_hours': time_per_epoch_hours,
                    'steps_per_epoch': steps_per_epoch,
                    'computational_capacity_flops': computational_capacity,
                    'device_type': device_type,
                    'memory_constraint_factor': memory_constraint_factor
                }
                
                resources['time_estimates']['convergence_estimates'] = {
                    'estimated_epochs': estimated_epochs_to_converge,
                    'total_training_time_hours': total_training_time_hours,
                    'total_training_time_days': total_training_time_hours / 24,
                    'epochs_per_day': 24 / max(time_per_epoch_hours, 0.001),
                    'training_throughput_samples_per_hour': (3600 / max(time_per_step_seconds, 0.001)) * batch_size
                }
            
            # Inference time estimates
            if 'inference_flops' in resources['compute'].get('inference_compute', {}):
                inference_flops = resources['compute']['inference_compute']['inference_flops']
                inference_time_seconds = inference_flops / computational_capacity
                
                resources['time_estimates']['inference_time'] = {
                    'inference_time_ms': inference_time_seconds * 1000,
                    'inference_throughput_samples_per_second': batch_size / inference_time_seconds,
                    'single_sample_latency_ms': (inference_time_seconds * 1000) / batch_size
                }
            
            # Hardware scaling analysis
            scaling_factors = {
                'double_gpu_memory': 1.8 if gpu_available else 1.0,
                'high_end_gpu': 2.5 if gpu_available else 3.0,
                'multi_gpu_2x': 1.7 if gpu_available else 1.0,
                'multi_gpu_4x': 3.2 if gpu_available else 1.0
            }
            
            resources['time_estimates']['hardware_scaling'] = {}
            for scenario, factor in scaling_factors.items():
                if 'time_per_epoch_hours' in resources['time_estimates'].get('training_time', {}):
                    original_time = resources['time_estimates']['training_time']['time_per_epoch_hours']
                    scaled_time = original_time / factor
                    resources['time_estimates']['hardware_scaling'][scenario] = {
                        'time_per_epoch_hours': scaled_time,
                        'speedup_factor': factor,
                        'total_training_time_reduction_hours': original_time - scaled_time
                    }
            
            resources['analysis_metadata']['validation_checks'].append('time_estimation_completed')
            
        except Exception as e:
            logger.error(f"Time estimation analysis failed for {model_name}: {e}")
            resources['time_estimates']['error'] = str(e)
            resources['analysis_metadata']['warnings'].append(f"Time estimation failed: {str(e)}")
        
        # Phase 4: Hardware Requirements and Recommendations
        try:
            logger.debug(f"Phase 4: Hardware requirements analysis for {model_name}")
            
            # Memory requirements
            required_memory_gb = resources['memory']['training_memory']['total_with_overhead']['gb']
            inference_memory_gb = resources['memory']['inference_memory']['total_inference']['gb']
            
            # Minimum requirements
            resources['hardware_requirements']['minimum_requirements'] = {
                'system_memory_gb': max(8, int(required_memory_gb * 1.5)),
                'gpu_memory_gb': max(4, int(required_memory_gb)) if gpu_available else None,
                'cpu_cores': max(4, cpu_count),
                'storage_gb': max(10, total_params * 8 / (1024**3)),  # Model + checkpoints + data
                'description': 'Minimum viable configuration for training'
            }
            
            # Recommended requirements
            resources['hardware_requirements']['recommended_requirements'] = {
                'system_memory_gb': max(16, int(required_memory_gb * 2)),
                'gpu_memory_gb': max(8, int(required_memory_gb * 1.5)) if gpu_available else None,
                'cpu_cores': max(8, cpu_count * 2),
                'storage_gb': max(50, total_params * 16 / (1024**3)),
                'gpu_compute_capability': 7.0 if gpu_available else None,
                'description': 'Recommended configuration for efficient training'
            }
            
            # Optimal requirements
            resources['hardware_requirements']['optimal_requirements'] = {
                'system_memory_gb': max(32, int(required_memory_gb * 3)),
                'gpu_memory_gb': max(16, int(required_memory_gb * 2)) if gpu_available else None,
                'cpu_cores': max(16, cpu_count * 4),
                'storage_gb': max(100, total_params * 32 / (1024**3)),
                'gpu_compute_capability': 8.0 if gpu_available else None,
                'nvme_storage': True,
                'description': 'Optimal configuration for maximum performance'
            }
            
            # Hardware compatibility analysis
            current_gpu_memory = hardware_data.get('gpu_memory_gb', 0)
            current_system_memory = hardware_data.get('system_memory_gb', 8)
            
            compatibility_status = 'unknown'
            compatibility_issues = []
            
            if required_memory_gb > current_gpu_memory and gpu_available:
                compatibility_issues.append(f"Insufficient GPU memory: need {required_memory_gb:.1f}GB, have {current_gpu_memory:.1f}GB")
                compatibility_status = 'insufficient_gpu_memory'
            elif required_memory_gb * 1.5 > current_system_memory:
                compatibility_issues.append(f"Insufficient system memory: need {required_memory_gb*1.5:.1f}GB, have {current_system_memory:.1f}GB")
                compatibility_status = 'insufficient_system_memory'
            elif not gpu_available and total_params > 1000000:
                compatibility_issues.append("Large model without GPU acceleration - training will be very slow")
                compatibility_status = 'cpu_only_large_model'
            else:
                compatibility_status = 'compatible'
            
            resources['hardware_requirements']['hardware_compatibility'] = {
                'status': compatibility_status,
                'issues': compatibility_issues,
                'current_gpu_memory_gb': current_gpu_memory,
                'current_system_memory_gb': current_system_memory,
                'memory_utilization_percent': (required_memory_gb / max(current_gpu_memory, current_system_memory)) * 100
            }
            
            # Upgrade recommendations
            upgrade_recommendations = []
            
            if compatibility_status != 'compatible':
                if 'gpu_memory' in compatibility_status:
                    upgrade_recommendations.append(f"Upgrade to GPU with at least {int(required_memory_gb * 1.5)}GB VRAM")
                if 'system_memory' in compatibility_status:
                    upgrade_recommendations.append(f"Increase system RAM to at least {int(required_memory_gb * 2)}GB")
                if not gpu_available and total_params > 500000:
                    upgrade_recommendations.append("Consider adding GPU acceleration for reasonable training times")
            
            # Performance improvement recommendations
            if gpu_available and current_gpu_memory < 16:
                upgrade_recommendations.append("GPU with more VRAM would allow larger batch sizes")
            
            if cpu_count < 8:
                upgrade_recommendations.append("More CPU cores would improve data loading performance")
            
            resources['hardware_requirements']['upgrade_recommendations'] = upgrade_recommendations
            
            resources['analysis_metadata']['validation_checks'].append('hardware_analysis_completed')
            
        except Exception as e:
            logger.error(f"Hardware requirements analysis failed for {model_name}: {e}")
            resources['hardware_requirements']['error'] = str(e)
            resources['analysis_metadata']['warnings'].append(f"Hardware analysis failed: {str(e)}")
        
        # Phase 5: Optimization Strategies
        try:
            logger.debug(f"Phase 5: Generating optimization strategies for {model_name}")
            
            # Memory optimization strategies
            memory_opts = []
            
            if not training_config.get('mixed_precision', False) and gpu_available:
                memory_opts.append("Enable mixed precision (FP16) training to reduce memory usage by ~50%")
            
            if batch_size > 32:
                memory_opts.append("Consider gradient accumulation with smaller batch sizes")
            
            if total_params > 1000000:
                memory_opts.append("Enable gradient checkpointing to trade compute for memory")
            
            if len(hidden_dims) > 3:
                memory_opts.append("Consider reducing model depth to decrease activation memory")
            
            resources['optimization_strategies']['memory_optimization'] = memory_opts
            
            # Compute optimization strategies
            compute_opts = []
            
            if not gpu_available and total_params > 100000:
                compute_opts.append("GPU acceleration strongly recommended for reasonable training times")
            
            if gpu_available and batch_size < 32:
                compute_opts.append("Increase batch size to better utilize GPU parallelism")
            
            if training_config.get('optimizer', '').lower() == 'sgd':
                compute_opts.append("Consider AdamW optimizer for potentially faster convergence")
            
            resources['optimization_strategies']['compute_optimization'] = compute_opts
            
            # Training optimization strategies
            training_opts = []
            
            training_opts.append("Implement early stopping to avoid overtraining")
            training_opts.append("Use learning rate scheduling for better convergence")
            
            if not training_config.get('gradient_clip'):
                training_opts.append("Consider gradient clipping for training stability")
            
            resources['optimization_strategies']['training_optimization'] = training_opts
            
            # Hardware optimization strategies
            hardware_opts = []
            
            if compatibility_status != 'compatible':
                hardware_opts.extend(resources['hardware_requirements']['upgrade_recommendations'])
            else:
                hardware_opts.append("Current hardware is compatible with training requirements")
                
                if gpu_available and current_gpu_memory > required_memory_gb * 2:
                    hardware_opts.append("GPU memory allows for larger batch sizes or model complexity")
            
            resources['optimization_strategies']['hardware_optimization'] = hardware_opts
            
            # Cost optimization strategies
            cost_opts = []
            
            if 'total_training_time_hours' in resources['time_estimates'].get('convergence_estimates', {}):
                training_hours = resources['time_estimates']['convergence_estimates']['total_training_time_hours']
                
                if training_hours > 24:
                    cost_opts.append("Long training time - consider cloud GPU instances for cost efficiency")
                
                if training_hours > 168:  # 1 week
                    cost_opts.append("Very long training - investigate model compression or architecture optimization")
            
            cost_opts.append("Monitor training progress to stop early if not improving")
            cost_opts.append("Use model checkpointing to resume training if interrupted")
            
            resources['optimization_strategies']['cost_optimization'] = cost_opts
            
            resources['analysis_metadata']['validation_checks'].append('optimization_strategies_generated')
            
        except Exception as e:
            logger.error(f"Optimization strategy generation failed for {model_name}: {e}")
            resources['optimization_strategies']['error'] = str(e)
            resources['analysis_metadata']['warnings'].append(f"Optimization strategy generation failed: {str(e)}")
        
        # Finalize analysis metadata
        resources['analysis_metadata']['estimation_duration_seconds'] = time.time() - estimation_start_time
        resources['analysis_metadata']['total_validation_checks'] = len(resources['analysis_metadata']['validation_checks'])
        resources['analysis_metadata']['total_warnings'] = len(resources['analysis_metadata']['warnings'])
        
        # Add summary statistics
        resources['analysis_metadata']['summary'] = {
            'parameter_count_formatted': f"{total_params:,}",
            'estimated_training_memory_gb': resources['memory']['training_memory']['total_with_overhead']['gb'],
            'estimated_training_time_hours': resources['time_estimates'].get('convergence_estimates', {}).get('total_training_time_hours', 0),
            'hardware_compatibility': resources['hardware_requirements']['hardware_compatibility']['status'],
            'primary_optimization_recommendation': resources['optimization_strategies']['memory_optimization'][0] if resources['optimization_strategies']['memory_optimization'] else "No specific recommendations",
            'analysis_completeness_percent': (len(resources['analysis_metadata']['validation_checks']) / 5) * 100  # 5 total phases
        }
        
        logger.debug(f"Training resource estimation completed for {model_name}: "
                    f"{resources['analysis_metadata']['total_validation_checks']} checks passed, "
                    f"{resources['analysis_metadata']['total_warnings']} warnings, "
                    f"duration: {resources['analysis_metadata']['estimation_duration_seconds']:.2f}s")
        
        return resources
        
    except Exception as e:
        logger.error(f"Critical training resource estimation failure for {model_name}: {e}")
        return {
            'error': f'Training resource estimation failed: {str(e)}',
            'model_name': model_name,
            'estimation_timestamp': datetime.now().isoformat(),
            'fallback_estimates': {
                'memory_mb': total_params * 16 / (1024**2),  # Very rough fallback
                'training_time_estimate': 'Unable to estimate due to analysis failure'
            },
            'recovery_suggestions': [
                'Check parameter validity (total_params, batch_size, input_dim)',
                'Verify hardware_data format',
                'Ensure configuration dictionary is properly formatted',
                'Try with default parameters'
            ]
        }
    
    finally:
        # Memory cleanup
        try:
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
        except Exception:
            pass

def model_specific_feature_analysis(
    model_name: str,
    test_config: Dict[str, Any],
    model: torch.nn.Module
) -> Dict[str, Any]:
    """
    Analyze model-specific features and capabilities.
    
    This function provides detailed analysis of model-specific features,
    architectural characteristics, and implementation details. Harmonized
    with compare_model_architectures() parameter structure.
    
    Args:
        model_name: Name of the model for feature analysis
        test_config: Configuration dictionary containing model parameters
        model: The model instance to analyze
        
    Returns:
        Dictionary containing model-specific feature analysis
    """
    try:
        logger.debug(f"Starting model-specific feature analysis for {model_name}")
        
        # Extract model configuration
        model_config = test_config.get('params', {}).get('config', {}).get('model', {})
        
        # Base feature analysis
        feature_analysis = {
            'supports_attention': getattr(model, 'use_attention', False) and hasattr(model, 'attention'),
            'supports_residual_blocks': getattr(model, 'residual_blocks', False),
            'supports_skip_connections': getattr(model, 'skip_connection', False) and hasattr(model, 'skip_layers'),
            'normalization_type': getattr(model, 'normalization', 'none'),
            'activation_function': getattr(model, 'activation', 'unknown'),
            'mixed_precision_compatible': getattr(model, 'mixed_precision', False),
            'ensemble_size': getattr(model, 'num_models', 1) if model_name == 'AutoencoderEnsemble' else 1,
            'model_type': model_name,
            'encoding_dim': getattr(model, 'encoding_dim', 0),
            'has_batch_norm': getattr(model, 'use_batch_norm', False),
            'has_layer_norm': getattr(model, 'use_layer_norm', False),
            'legacy_mode': getattr(model, 'legacy_mode', False),
            'feature_status': 'completed'
        }
        
        # Model-specific additional analysis
        if model_name == 'AutoencoderEnsemble':
            # Ensemble-specific features
            feature_analysis.update({
                'ensemble_diversity': getattr(model, 'diversity_factor', 0.0),
                'ensemble_voting': getattr(model, 'voting_method', 'average'),
                'individual_model_compatibility': all(hasattr(m, 'forward') for m in getattr(model, 'models', []) if hasattr(model, 'models'))
            })
        
        elif model_name == 'EnhancedAutoencoder':
            # Enhanced autoencoder specific features
            feature_analysis.update({
                'advanced_activations': hasattr(model, 'advanced_activation') or hasattr(model, 'activation_param'),
                'configurable_normalization': feature_analysis['normalization_type'] != 'none',
                'optimization_features': hasattr(model, 'cuda_optimizations') or hasattr(model, 'optimized_layers')
            })
        
        elif model_name == 'SimpleAutoencoder':
            # Simple autoencoder specific features
            feature_analysis.update({
                'minimal_architecture': True,
                'fast_inference': True,
                'low_memory_footprint': True
            })
        
        # Check if features are actually implemented (not just configured)
        feature_analysis.update({
            'attention_implemented': feature_analysis['supports_attention'] and hasattr(model, 'attention_layers'),
            'residual_implemented': feature_analysis['supports_residual_blocks'] and hasattr(model, 'residual_layers'),
            'skip_connections_implemented': feature_analysis['supports_skip_connections'] and hasattr(model, 'skip_connections')
        })
        
        # Analyze activation function specifics
        activation = feature_analysis['activation_function']
        if hasattr(model, 'activation_param'):
            feature_analysis['activation_parameters'] = getattr(model, 'activation_param', {})
        elif hasattr(model, 'activation_fn'):
            feature_analysis['activation_function'] = str(type(getattr(model, 'activation_fn', lambda x: x)).__name__)
        
        # Count normalization layers directly
        if feature_analysis['normalization_type'] == 'batch':
            batch_norm_count = 0
            for module in model.modules():
                if isinstance(module, torch.nn.BatchNorm1d):
                    batch_norm_count += 1
            feature_analysis['batch_norm_layers'] = batch_norm_count
        elif feature_analysis['normalization_type'] == 'layer':
            layer_norm_count = 0
            for module in model.modules():
                if isinstance(module, torch.nn.LayerNorm):
                    layer_norm_count += 1
            feature_analysis['layer_norm_layers'] = layer_norm_count
        
        # Calculate complexity score directly
        complexity = 0
        
        # Base complexity
        if feature_analysis.get('supports_attention', False):
            complexity += 2
        if feature_analysis.get('supports_residual_blocks', False):
            complexity += 2
        if feature_analysis.get('supports_skip_connections', False):
            complexity += 1
        if feature_analysis.get('has_batch_norm', False) or feature_analysis.get('has_layer_norm', False):
            complexity += 1
        if feature_analysis.get('ensemble_size', 1) > 1:
            complexity += 3
        if feature_analysis.get('mixed_precision_compatible', False):
            complexity += 1
        
        # Additional complexity factors
        norm_type = feature_analysis.get('normalization_type', 'none')
        if norm_type != 'none':
            complexity += 1
        
        # Model type specific complexity
        model_type = feature_analysis.get('model_type', '')
        if model_type == 'EnhancedAutoencoder':
            complexity += 2
        elif model_type == 'AutoencoderEnsemble':
            complexity += 3
        
        feature_analysis['complexity_score'] = complexity
        feature_analysis['complexity_level'] = (
            'high' if complexity > 7 else
            'medium' if complexity > 3 else
            'low'
        )
        
        logger.debug(f"Completed feature analysis for {model_name}")
        return feature_analysis
        
    except Exception as e:
        logger.error(f"Feature analysis failed for {model_name}: {str(e)}")
        # Return basic feature analysis with error information
        return {
            'feature_status': 'failed',
            'error': str(e),
            'model_type': model_name,
            'supports_attention': False,
            'supports_residual_blocks': False,
            'supports_skip_connections': False,
            'normalization_type': 'unknown',
            'activation_function': 'unknown',
            'mixed_precision_compatible': False,
            'ensemble_size': 1
        }

def generate_model_recommendations(
    model_name: str,
    total_params: int,
    hardware_data: Dict[str, Any],
    performance_metrics: Dict[str, Any]
) -> List[str]:
    """
    Generate specific recommendations for a model based on its characteristics.
    
    This function provides tailored recommendations based on model architecture,
    performance metrics, and hardware context. Harmonized with compare_model_architectures().
    
    Args:
        model_name: Name of the model
        total_params: Number of parameters
        hardware_data: Hardware information from system context
        performance_metrics: Performance measurement results
        
    Returns:
        List of recommendation strings
    """
    recommendations = []
    
    try:
        # Parameter-based recommendations
        if total_params < 10000:
            recommendations.extend([
                "Excellent for prototyping and development",
                "Minimal resource requirements",
                "Fast training and inference"
            ])
        elif total_params < 100000:
            recommendations.extend([
                "Good balance of complexity and efficiency",
                "Suitable for most production environments",
                "Reasonable training times"
            ])
        elif total_params < 1000000:
            recommendations.extend([
                "High capacity model for complex tasks",
                "Requires adequate hardware resources",
                "Consider mixed precision training"
            ])
        else:
            recommendations.extend([
                "Large model requiring significant resources",
                "Best suited for specialized hardware",
                "Distributed training may be beneficial"
            ])
        
        # Performance-based recommendations
        inference_fps = performance_metrics.get('inference_fps', 0)
        if inference_fps > 1000:
            recommendations.append("Excellent for real-time applications")
        elif inference_fps > 100:
            recommendations.append("Suitable for interactive applications")
        elif inference_fps < 10:
            recommendations.append("May not be suitable for real-time use")
        
        # Memory-based recommendations
        gpu_memory_mb = performance_metrics.get('gpu_memory_mb', 0)
        if gpu_memory_mb > 0:
            if gpu_memory_mb < 100:
                recommendations.append("Low GPU memory footprint")
            elif gpu_memory_mb < 1000:
                recommendations.append("Moderate GPU memory requirements")
            else:
                recommendations.append("High GPU memory requirements")
        
        # Hardware-specific recommendations
        gpu_available = hardware_data.get('gpu_available', False)
        gpu_memory_gb = hardware_data.get('gpu_memory_gb', 0)
        
        if not gpu_available:
            if total_params < 50000:
                recommendations.append("Suitable for CPU-only environments")
            else:
                recommendations.append("GPU strongly recommended for reasonable performance")
        elif gpu_memory_gb > 0:
            # Using >80% of GPU memory
            if gpu_memory_mb / 1024 > gpu_memory_gb * 0.8:
                recommendations.append("Consider reducing batch size or model complexity")
            # Using <30% of GPU memory
            elif gpu_memory_mb / 1024 < gpu_memory_gb * 0.3:
                recommendations.append("Could increase batch size for better GPU utilization")
        
        # Model-specific recommendations
        if model_name == 'SimpleAutoencoder':
            recommendations.extend([
                "Ideal for baseline comparisons",
                "Good starting point for hyperparameter tuning",
                "Limited capacity for complex patterns"
            ])
        elif model_name == 'EnhancedAutoencoder':
            recommendations.extend([
                "Configurable complexity for different requirements",
                "Production-ready with good performance",
                "Supports advanced training techniques"
            ])
        elif model_name == 'AutoencoderEnsemble':
            recommendations.extend([
                "Maximum accuracy for critical applications",
                "Requires more resources but provides robustness",
                "Consider ensemble size vs. resource trade-offs"
            ])
        
        # Training time recommendations
        avg_inference_time = performance_metrics.get('avg_inference_time_ms', 0)
        # >100ms inference time is considered long
        if avg_inference_time > 100:
            recommendations.append("Long inference time - consider model optimization")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_recommendations = []
        for rec in recommendations:
            if rec not in seen:
                seen.add(rec)
                unique_recommendations.append(rec)
        
        # Limit to top 10 recommendations
        return unique_recommendations[:10]
        
    except Exception as e:
        logger.debug(f"Error generating recommendations for {model_name}: {e}")
        return [f"Analysis completed for {model_name}", "Review performance metrics for details"]

def generate_comparative_summary(
    results: Dict[str, Any],
    hardware_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Generate comprehensive comparative summary and recommendations.
    
    This function analyzes all model results and provides comparative analysis,
    rankings, and recommendations. Harmonized with compare_model_architectures().
    
    Args:
        results: Complete results dictionary from model comparison
        hardware_data: Hardware context information from system analysis
        
    Returns:
        Dictionary containing comparative analysis and recommendations
    """
    try:
        summary = {
            'recommendations': [],
            'warnings': [],
            'optimal_choices': {},
            'performance_ranking': {},
            'resource_efficiency': {},
            'use_case_recommendations': {},
            'compatibility_matrix': {},
            'optimization_suggestions': []
        }
        
        # Extract successful model results
        model_results = {}
        for key, value in results.items():
            if not key.startswith('_') and isinstance(value, dict) and 'error' not in value:
                model_results[key] = value
        
        if not model_results:
            summary['warnings'].append("No models were successfully analyzed")
            return summary
        
        # Performance ranking
        performance_metrics = {}
        
        for model_name, data in model_results.items():
            arch = data.get('architecture', {})
            perf = data.get('performance', {})
            resources = data.get('resource_requirements', {})
            
            # Collect metrics for ranking
            performance_metrics[model_name] = {
                'param_count': arch.get('total_params', 0),
                'inference_fps': perf.get('inference_fps', 0),
                'model_size_mb': arch.get('model_size_mb', 0),
                'memory_requirement_mb': resources.get('memory', {}).get('total_with_overhead_mb', 0)
            }
        
        # Rank by different criteria
        rankings = {}
        
        # Speed ranking (highest FPS first)
        speed_ranking = sorted(
            performance_metrics.items(),
            key=lambda x: x[1]['inference_fps'],
            reverse=True
        )
        rankings['speed'] = [name for name, _ in speed_ranking]
        
        # Efficiency ranking (highest FPS per parameter)
        efficiency_ranking = sorted(
            performance_metrics.items(),
            key=lambda x: x[1]['inference_fps'] / max(x[1]['param_count'], 1),
            reverse=True
        )
        rankings['efficiency'] = [name for name, _ in efficiency_ranking]
        
        # Memory efficiency ranking (lowest memory usage)
        memory_ranking = sorted(
            performance_metrics.items(),
            key=lambda x: x[1]['memory_requirement_mb']
        )
        rankings['memory_efficiency'] = [name for name, _ in memory_ranking]
        
        # Size ranking (smallest model first)
        size_ranking = sorted(
            performance_metrics.items(),
            key=lambda x: x[1]['param_count']
        )
        rankings['size'] = [name for name, _ in size_ranking]
        
        summary['performance_ranking'] = rankings
        
        # Optimal choices for different scenarios
        optimal_choices = {}
        
        if speed_ranking:
            optimal_choices['fastest_inference'] = speed_ranking[0][0]
        
        if efficiency_ranking:
            optimal_choices['most_efficient'] = efficiency_ranking[0][0]
        
        if memory_ranking:
            optimal_choices['lowest_memory'] = memory_ranking[0][0]
        
        if size_ranking:
            optimal_choices['smallest_model'] = size_ranking[0][0]
        
        # Balanced recommendation (consider multiple factors)
        balanced_scores = {}
        for model_name, metrics in performance_metrics.items():
            # Normalize metrics (0-1 scale)
            max_fps = max(m['inference_fps'] for m in performance_metrics.values())
            max_params = max(m['param_count'] for m in performance_metrics.values())
            max_memory = max(m['memory_requirement_mb'] for m in performance_metrics.values())
            
            if max_fps > 0 and max_params > 0 and max_memory > 0:
                speed_score = metrics['inference_fps'] / max_fps
                # Smaller is better
                size_score = 1.0 - (metrics['param_count'] / max_params)
                # Less is better
                memory_score = 1.0 - (metrics['memory_requirement_mb'] / max_memory)
                
                # Weighted combination
                balanced_scores[model_name] = (speed_score * 0.4 + size_score * 0.3 + memory_score * 0.3)
        
        if balanced_scores:
            best_balanced = max(balanced_scores.items(), key=lambda x: x[1])
            optimal_choices['best_balanced'] = best_balanced[0]
        
        summary['optimal_choices'] = optimal_choices
        
        # Use case recommendations
        use_case_recs = {
            'prototyping_development': [],
            'production_deployment': [],
            'resource_constrained': [],
            'high_performance': [],
            'research_experimentation': []
        }
        
        for model_name, data in model_results.items():
            arch = data.get('architecture', {})
            param_count = arch.get('total_params', 0)
            complexity = arch.get('complexity_level', 'unknown')
            use_cases = data.get('use_cases', [])
            
            # Categorize by parameter count and complexity
            if param_count < 50000 or complexity == 'low':
                use_case_recs['prototyping_development'].append(model_name)
                use_case_recs['resource_constrained'].append(model_name)
            
            if 10000 < param_count < 500000 or complexity == 'medium':
                use_case_recs['production_deployment'].append(model_name)
            
            if param_count > 100000 or complexity in ['high', 'very_high']:
                use_case_recs['high_performance'].append(model_name)
                use_case_recs['research_experimentation'].append(model_name)
        
        summary['use_case_recommendations'] = use_case_recs
        
        # Generate overall recommendations
        recommendations = []
        
        # Hardware-based recommendations
        gpu_available = hardware_data.get('gpu_available', False)
        gpu_memory_gb = hardware_data.get('gpu_memory_gb', 0)
        
        if not gpu_available:
            recommendations.append("GPU acceleration not available - consider smallest models for reasonable performance")
            if use_case_recs['resource_constrained']:
                recommendations.append(f"For CPU-only: Recommended models: {', '.join(use_case_recs['resource_constrained'][:2])}")
        elif gpu_memory_gb < 4:
            recommendations.append("Limited GPU memory - avoid largest models or reduce batch sizes")
        elif gpu_memory_gb >= 8:
            recommendations.append("Adequate GPU memory - all models should run efficiently")
        
        # Performance-based recommendations
        if speed_ranking:
            fastest_model = speed_ranking[0][0]
            fastest_fps = speed_ranking[0][1]['inference_fps']
            recommendations.append(f"Fastest inference: {fastest_model} ({fastest_fps:.1f} samples/sec)")
        
        if optimal_choices.get('best_balanced'):
            recommendations.append(f"Best overall balance: {optimal_choices['best_balanced']}")
        
        # Resource efficiency recommendations
        if efficiency_ranking:
            most_efficient = efficiency_ranking[0][0]
            recommendations.append(f"Most parameter-efficient: {most_efficient}")
        
        summary['recommendations'] = recommendations
        
        # Generate warnings
        warnings = []
        
        # Check for potential issues
        for model_name, data in model_results.items():
            resources = data.get('resource_requirements', {})
            memory_req = resources.get('memory', {}).get('recommended_system_memory_gb', 0)
            
            if memory_req > 32:
                warnings.append(f"{model_name} requires >32GB RAM - ensure adequate system memory")
            
            time_estimates = resources.get('time_estimates', {})
            minutes_per_epoch = time_estimates.get('minutes_per_epoch', 0)
            
            # >1 hour per epoch
            if minutes_per_epoch > 60:
                warnings.append(f"{model_name} estimated >1 hour per training epoch")
        
        # Hardware warnings
        if not gpu_available and any(metrics['param_count'] > 100000 for metrics in performance_metrics.values()):
            warnings.append("Large models detected but no GPU available - training will be very slow")
        
        summary['warnings'] = warnings
        
        # Resource efficiency summary with integrated efficiency classification
        efficiency_summary = {}
        for model_name, metrics in performance_metrics.items():
            if metrics['param_count'] > 0:
                fps_per_param = metrics['inference_fps'] / metrics['param_count']
                fps_per_mb = metrics['inference_fps'] / max(metrics['memory_requirement_mb'], 1)
                
                # Classify efficiency directly
                try:
                    # Normalize and combine metrics
                    # Scale and cap at 10
                    param_score = min(fps_per_param * 1000, 10)
                    # Scale and cap at 10
                    memory_score = min(fps_per_mb / 10, 10)
                    
                    combined_score = (param_score + memory_score) / 2
                    
                    if combined_score >= 7:
                        efficiency_class = 'excellent'
                    elif combined_score >= 5:
                        efficiency_class = 'good'
                    elif combined_score >= 3:
                        efficiency_class = 'moderate'
                    elif combined_score >= 1:
                        efficiency_class = 'poor'
                    else:
                        efficiency_class = 'very_poor'
                except:
                    efficiency_class = 'unknown'
                
                efficiency_summary[model_name] = {
                    'fps_per_parameter': fps_per_param,
                    'fps_per_mb_memory': fps_per_mb,
                    'efficiency_class': efficiency_class
                }
        
        summary['resource_efficiency'] = efficiency_summary
        
        logger.debug(f"Comparative summary generated: {len(recommendations)} recommendations, {len(warnings)} warnings")
        
        return summary
        
    except Exception as e:
        logger.warning(f"Error generating comparative summary: {e}")
        return {
            'recommendations': ["Comparison completed with errors - review individual model results"],
            'warnings': [f"Summary generation failed: {str(e)}"],
            'optimal_choices': {},
            'performance_ranking': {},
            'resource_efficiency': {},
            'use_case_recommendations': {}
        }

# Display model comparison
def display_model_comparison() -> None:
    """
    Display available initialization reports and provide options to view them.
    Supports multiple report formats: HTML dashboard, JSON, TXT summary, and diagnostics.
    """
    try:
        # Clear screen and show banner with configuration
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Configuration context using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu display with context
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "="*40)
        print(Fore.CYAN + Style.BRIGHT + "MODEL ARCHITECTURE COMPARISON")
        print(Fore.YELLOW + Style.BRIGHT + "="*40)
        print(Fore.GREEN + Style.BRIGHT + f"Active Comparison Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}\n")
        
        # Get comparison results with error handling
        try:
            results = compare_model_architectures()
        except Exception as e:
            logger.error(f"Failed to generate model comparison: {e}")
            
            # Error display with context
            message = (
                f"Failed to generate model architecture comparison: {str(e)}\n"
                f"Context:\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could be due to:\n"
                f"- Model variants not properly initialized\n"
                f"- Configuration parameter issues\n"
                f"- System resource constraints\n"
                f"- Missing dependencies or compatibility problems\n\n"
                f"Troubleshooting Steps:\n"
                f"1. Run 'initialize_model_variants()' to refresh model registry\n"
                f"2. Check configuration with 'get_current_config()'\n"
                f"3. Validate models with 'validate_model_variants(logger)'\n"
                f"4. Check system resources and dependencies"
            )
            console.print(
                Panel.fit(
                    f"[bold red]{message}[/bold red]",
                    title="COMPARISON FAILED",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            return
        
        # Handle critical errors with error analysis
        if isinstance(results, dict):
            if 'error' in results or 'critical_validation_failure' in results:
                error_msg = results.get('error') or results.get('critical_validation_failure', 'Unknown error')
                
                message = (
                    f"Analysis Error: {error_msg}\n"
                    f"Context:\n"
                    f"- Current Preset: {preset_name}\n"
                    f"- Model Type: {model_type}\n\n"
                )
                
                # Error-specific guidance based on current implementation
                if 'initialization_failed' in error_msg.lower() or 'MODEL_VARIANTS' in error_msg:
                    message += (
                        f"Model Initialization Issues:\n"
                        f"1. Check if model classes are properly imported\n"
                        f"2. Verify PyTorch installation and dependencies\n"
                        f"3. Run: initialize_model_variants(silent=False)\n"
                        f"4. Check for configuration compatibility issues\n"
                    )
                elif 'memory' in error_msg.lower() or 'resource' in error_msg.lower():
                    message += (
                        f"Resource Issues:\n"
                        f"1. Close other applications to free memory\n"
                        f"2. Reduce batch size or model complexity\n"
                        f"3. Enable memory optimization: enhanced_clear_memory()\n"
                        f"4. Check available system and GPU memory\n"
                    )
                elif 'validation' in error_msg.lower():
                    message += (
                        f"Model Validation Issues:\n"
                        f"1. Run: validate_model_variants(logger, silent=False)\n"
                        f"2. Check individual model instantiation\n"
                        f"3. Verify configuration parameters\n"
                    )
                else:
                    message += (
                        f"General Analysis Issues:\n"
                        f"1. Check system resources and dependencies\n"
                        f"2. Verify configuration file integrity\n"
                        f"3. Ensure all required packages are installed\n"
                    )
                
                # Display partial results if available
                partial_results = results.get('partial_results', {})
                if partial_results and isinstance(partial_results, dict):
                    message += f"\nPartial Results Available:\n"
                    for key, value in partial_results.items():
                        if not key.startswith('_'):
                            status = value.get('analysis_status', 'unknown') if isinstance(value, dict) else 'unknown'
                            message += f"  - {key}: {status}\n"
                
                console.print(
                    Panel.fit(
                        f"[bold red]{message}[/bold red]",
                        title="ANALYSIS ERROR",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                return
            
            if 'initialization_error' in results:
                message = (
                    f"Initialization Error: {results['initialization_error']}\n"
                    f"Context:\n"
                    f"- Current Preset: {preset_name}\n"
                    f"- Model Type: {model_type}\n\n"
                    f"This indicates:\n"
                    f"1. Model variants could not be initialized properly\n"
                    f"2. Configuration parameters are invalid or incompatible\n"
                    f"3. Required dependencies may be missing\n"
                    f"4. System resources may be insufficient"
                )
                console.print(
                    Panel.fit(
                        f"[bold red]{message}[/bold red]",
                        title="INITIALIZATION ERROR",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                return
        
        # Extract metadata and summary with validation
        metadata = results.get('_metadata', {})
        summary = results.get('_summary', {})
        analysis_results = results.get('_analysis_results', {})
        
        # Validate metadata
        if not metadata:
            logger.warning("No metadata found in comparison results")
            metadata = {
                'comparison_timestamp': datetime.now().isoformat(),
                'comparison_version': '3.2',
                'available_variants': 0,
                'successful_comparisons': 0,
                'failed_comparisons': 0,
                'hardware_context': {},
                'input_dimension': 'unknown',
                'config_source': 'unknown',
                'validation_checks_performed': [],
                'memory_optimization_summary': {'optimizations_performed': 0},
                'helper_functions_utilized': []
            }
        
        # Filter model results (exclude metadata and analysis results)
        model_results = {}
        for key, value in results.items():
            if not key.startswith('_') and isinstance(value, dict):
                model_results[key] = value
        
        if not model_results:
            message = (
                f"No Model Results Available\n"
                f"Context:\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n\n"
                f"This could indicate:\n"
                f"1. No model variants are initialized in MODEL_VARIANTS\n"
                f"2. All model analyses failed during execution\n"
                f"3. Configuration issues prevent proper analysis\n"
                f"4. System resource constraints blocked analysis\n\n"
                f"Try running: [bold cyan]initialize_model_variants(silent=False)[/bold cyan]"
            )
            console.print(
                Panel.fit(
                    f"[bold yellow]{message}[/bold yellow]",
                    title="NO RESULTS AVAILABLE",
                    style="bold yellow",
                    border_style="yellow",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            return
        
        # ENHANCED MAIN HEADER SECTION
        comparison_version = metadata.get('comparison_version', '3.2')
        hardware_ctx = metadata.get('hardware_context', {})
        gpu_available = hardware_ctx.get('gpu_available', False)
        system_class = hardware_ctx.get('system_class', 'unknown')
        memory_optimizations = metadata.get('memory_optimization_summary', {}).get('optimizations_performed', 0)
        helper_functions_used = len(metadata.get('helper_functions_utilized', []))
        
        header_panel = Panel.fit(
            f"[bold yellow]MODEL ARCHITECTURE COMPARISON REPORT v{comparison_version}[/bold yellow]\n"
            f"Generated: {metadata.get('comparison_timestamp', 'Unknown')[:19]} | "
            f"Duration: {metadata.get('comparison_duration_seconds', 0):.2f}s\n"
            f"Input Dimension: {metadata.get('input_dimension', 'Unknown')} | "
            f"Config Source: {metadata.get('config_source', 'Unknown')}\n"
            f"Models Available: {metadata.get('available_variants', 0)} | "
            f"Successfully Analyzed: {metadata.get('successful_comparisons', 0)} | "
            f"Failed: {metadata.get('failed_comparisons', 0)}\n"
            f"System Class: {system_class.title()} | "
            f"Hardware: {'GPU Available' if gpu_available else 'CPU Only'} | "
            f"Memory Optimizations: {memory_optimizations}\n"
            f"Helper Functions: {helper_functions_used} | "
            f"Analysis Completeness: {(metadata.get('successful_comparisons', 0) / max(metadata.get('available_variants', 1), 1) * 100):.1f}%",
            title="[bold yellow]COMPREHENSIVE ANALYSIS OVERVIEW[/bold yellow]",
            border_style="green",
            title_align="left",
            style="bold green",
            padding=(1, 2)
        )
        console.print()
        console.print(header_panel)
        
        # ENHANCED MAIN COMPARISON TABLE
        main_table = Table(
            title="\nPERFORMANCE & RESOURCE COMPARISON",
            box=box.ROUNDED,
            header_style="bold cyan",
            border_style="white",
            title_style="bold yellow",
            title_justify="left",
            show_lines=True,
            expand=True,
            width=min(180, console.width - 2)
        )
        
        # Configure main table columns
        main_table.add_column("Model", style="bold yellow", width=15, no_wrap=True)
        main_table.add_column("Parameters", style="bold cyan", width=10, justify="left")
        main_table.add_column("Size (MB)", style="bold blue", width=8, justify="left")
        main_table.add_column("Complexity", style="bold magenta", width=8, justify="left")
        main_table.add_column("Inference (ms)", style="bold green", width=10, justify="left")
        main_table.add_column("Throughput", style="bold", width=12, justify="left")
        main_table.add_column("Memory (MB)", style="bold", width=10, justify="left")
        main_table.add_column("Efficiency", style="bold", width=10, justify="left")
        main_table.add_column("Status", style="bold", width=10, justify="left")
        
        # Prepare and sort model data
        model_data_list = []
        successful_models = []
        failed_models = []
        
        for model_name, model_data in model_results.items():
            analysis_status = model_data.get('analysis_status', 'unknown')
            
            if analysis_status in ['failed', 'analysis_failed', 'instantiation_failed', 'validation_error'] or 'error' in model_data:
                failed_models.append((model_name, model_data))
            elif analysis_status == 'completed':
                param_count = model_data.get('architecture', {}).get('total_params', 0)
                model_data_list.append((param_count, model_name, model_data))
                successful_models.append((model_name, model_data))
            else:
                # Handle partial success cases
                param_count = model_data.get('architecture', {}).get('total_params', 0)
                if param_count > 0 and 'architecture' in model_data:
                    model_data_list.append((param_count, model_name, model_data))
                    successful_models.append((model_name, model_data))
                else:
                    failed_models.append((model_name, model_data))
        
        # Sort by parameter count for logical ordering
        model_data_list.sort(key=lambda x: x[0])
        
        # Add successful models to main table
        for param_count, model_name, model_data in model_data_list:
            arch = model_data.get('architecture', {})
            perf = model_data.get('performance', {})
            memory_analysis = model_data.get('memory_analysis', {})
            resource_req = model_data.get('resource_requirements', {})
            
            # Performance metrics extraction
            inference_time_ms = None
            throughput_sps = None
            
            # Extract from scenario_summary
            scenario_summary = perf.get('scenario_summary', {})
            if scenario_summary:
                # Try standard_batch first, then single_sample
                for scenario_name in ['standard_batch', 'single_sample', 'small_batch']:
                    scenario_data = scenario_summary.get(scenario_name, {})
                    if scenario_data and 'latency_ms' in scenario_data:
                        inference_time_ms = scenario_data.get('latency_ms', 0)
                        throughput_sps = scenario_data.get('throughput', 0)
                        break
            
            # Fallback to direct performance metrics
            if inference_time_ms is None:
                inference_time_ms = (
                    perf.get('standard_batch_inference_time_ms') or
                    perf.get('single_sample_inference_time_ms') or
                    perf.get('avg_inference_time_ms', 0)
                )
            
            if throughput_sps is None:
                throughput_sps = (
                    perf.get('standard_batch_throughput_samples_per_second') or
                    perf.get('single_sample_throughput_samples_per_second') or
                    perf.get('throughput_samples_per_second', 0)
                )
            
            # Memory usage from multiple sources
            memory_mb = 0
            
            # Try GPU memory from detailed analysis
            gpu_memory_detailed = memory_analysis.get('gpu_memory_detailed', {})
            if gpu_memory_detailed:
                memory_mb = gpu_memory_detailed.get('allocated_mb', 0)
            
            # Fallback to other memory sources
            if memory_mb == 0:
                memory_mb = (
                    memory_analysis.get('gpu_memory_mb', 0) or
                    memory_analysis.get('cpu_memory_analysis', {}).get('estimated_total_cpu_memory_mb', 0) or
                    arch.get('model_size_mb', 0) * 4  # Rough estimate
                )
            
            # Calculate efficiency score
            efficiency_score = arch.get('architecture_efficiency', 0)
            
            # Status determination based on current metrics
            if inference_time_ms and throughput_sps:
                if inference_time_ms < 5 and throughput_sps > 1000:
                    status_text = "EXCELLENT"
                    status_style = "bold green"
                elif inference_time_ms < 20 and throughput_sps > 500:
                    status_text = "VERY GOOD"
                    status_style = "bold cyan"
                elif inference_time_ms < 50 and throughput_sps > 100:
                    status_text = "GOOD"
                    status_style = "bold blue"
                elif inference_time_ms < 200:
                    status_text = "ACCEPTABLE"
                    status_style = "bold yellow"
                else:
                    status_text = "SLOW"
                    status_style = "bold red"
            else:
                status_text = "PARTIAL"
                status_style = "bold magenta"
            
            # Memory formatting
            if memory_mb < 50:
                memory_text = f"{memory_mb:.3f}"
                memory_style = "bold green"
            elif memory_mb < 200:
                memory_text = f"{memory_mb:.3f}"
                memory_style = "bold cyan"
            elif memory_mb < 1000:
                memory_text = f"{memory_mb:.3f}"
                memory_style = "yellow"
            else:
                memory_text = f"{memory_mb:.3f}"
                memory_style = "red"
            
            # Format efficiency score
            if efficiency_score >= 80:
                eff_text = f"{efficiency_score:.0f}%"
                eff_style = "bold green"
            elif efficiency_score >= 60:
                eff_text = f"{efficiency_score:.0f}%"
                eff_style = "bold cyan"
            elif efficiency_score >= 40:
                eff_text = f"{efficiency_score:.0f}%"
                eff_style = "bold yellow"
            else:
                eff_text = f"{efficiency_score:.0f}%" if efficiency_score > 0 else "N/A"
                eff_style = "bold red" if efficiency_score > 0 else "bold magenta"
            
            main_table.add_row(
                Text(model_name, style="bold"),
                f"{param_count:,}",
                f"{arch.get('model_size_mb', 0):.3f}",
                arch.get('complexity_level', 'Unknown').title(),
                f"{inference_time_ms:.1f}" if inference_time_ms and inference_time_ms > 0 else "N/A",
                f"{throughput_sps:.0f}/s" if throughput_sps and throughput_sps > 0 else "N/A",
                Text(memory_text, style=memory_style),
                Text(eff_text, style=eff_style),
                Text(status_text, style=status_style)
            )
        
        # Add failed models to table with error information
        for model_name, model_data in failed_models:
            errors = model_data.get('errors', [])
            if errors:
                error_msg = errors[0] if isinstance(errors, list) else str(errors)
            else:
                error_msg = model_data.get('error', 'Unknown error')
            
            # Categorize error types based on current implementation
            if 'memory' in error_msg.lower():
                error_type = "MEMORY"
                error_style = "bold red"
            elif 'parameter' in error_msg.lower() or 'config' in error_msg.lower():
                error_type = "CONFIG"
                error_style = "bold yellow"
            elif 'instantiation' in error_msg.lower():
                error_type = "INIT"
                error_style = "bold red"
            elif 'validation' in error_msg.lower():
                error_type = "VALID"
                error_style = "bold red"
            else:
                error_type = "ERROR"
                error_style = "bold red"
            
            main_table.add_row(
                Text(model_name, style="bold"),
                Text("--", style="bold"),
                Text("--", style="bold"),
                Text(error_type, style=error_style),
                Text("--", style="bold"),
                Text("--", style="bold"),
                Text("--", style="bold"),
                Text("--", style="bold"),
                Text("FAILED", style="bold red")
            )
        
        console.print(main_table)
        
        # ENHANCED DETAILED ARCHITECTURE ANALYSIS
        if successful_models:
            detail_table = Table(
                title="\nDETAILED ARCHITECTURE & FEATURE ANALYSIS",
                box=box.ROUNDED,
                header_style="bold cyan",
                border_style="cyan",
                title_style="bold magenta",
                title_justify="left",
                show_lines=True,
                expand=True,
                width=min(200, console.width - 2)
            )
            
            detail_table.add_column("Model", style="bold magenta", width=15)
            detail_table.add_column("Description", style="bold yellow", width=32, justify="left")
            detail_table.add_column("Layers", style="bold green", width=5, justify="left")
            detail_table.add_column("Features", style="bold", width=22, justify="left")
            detail_table.add_column("FLOPs", style="bold cyan", width=7, justify="left")
            detail_table.add_column("Complexity", style="bold cyan", width=8, justify="left")
            detail_table.add_column("Use Cases", style="bold yellow", width=30, justify="left")
            
            for model_name, model_data in successful_models:
                arch = model_data.get('architecture', {})
                flop_analysis = model_data.get('computational_complexity', {})
                feature_analysis = model_data.get('feature_analysis', {})
                use_cases = model_data.get('use_cases', [])
                
                # Feature detection
                features = []
                if feature_analysis.get('supports_attention', False):
                    features.append("Attention")
                if feature_analysis.get('supports_residual_blocks', False):
                    features.append("Residual")
                if feature_analysis.get('supports_skip_connections', False):
                    features.append("Skip")
                if feature_analysis.get('has_batch_norm', False):
                    features.append("BatchNorm")
                if feature_analysis.get('has_layer_norm', False):
                    features.append("LayerNorm")
                if feature_analysis.get('mixed_precision_compatible', False):
                    features.append("FP16")
                
                # Check normalization type
                norm_type = feature_analysis.get('normalization_type', 'none')
                if norm_type and norm_type != 'none' and norm_type not in [f.lower() for f in features]:
                    features.append(norm_type.title())
                
                # Ensemble information
                ensemble_size = feature_analysis.get('ensemble_size', 1)
                if ensemble_size > 1:
                    features.append(f"Ens({ensemble_size})")
                
                features_text = ", ".join(features) if features else "Basic"
                
                # FLOP formatting
                flops = flop_analysis.get('forward_pass', {}).get('total_flops', 0)
                if flops > 1e12:
                    flops_text = f"{flops/1e12:.1f}T"
                elif flops > 1e9:
                    flops_text = f"{flops/1e9:.1f}G"
                elif flops > 1e6:
                    flops_text = f"{flops/1e6:.1f}M"
                elif flops > 1e3:
                    flops_text = f"{flops/1e3:.1f}K"
                else:
                    flops_text = str(int(flops)) if flops > 0 else "N/A"
                
                # Use cases formatting
                use_cases_text = ', '.join(use_cases[:2]) + ('...' if len(use_cases) > 2 else '')
                if not use_cases_text:
                    use_cases_text = "General purpose"
                
                # Get complexity class from current implementation
                complexity_class = (
                    flop_analysis.get('complexity_metrics', {}).get('complexity_class') or
                    arch.get('computational_class', 'Unknown')
                )
                
                detail_table.add_row(
                    model_name,
                    arch.get('description', 'No description available')[:32],
                    str(arch.get('layer_count', 0)),
                    features_text,
                    flops_text,
                    complexity_class.replace('_', ' ').title() if complexity_class else "Unknown",
                    use_cases_text
                )
            
            console.print(detail_table)
        
        # ENHANCED HARDWARE CONTEXT & SYSTEM INFO
        gpu_memory_gb = hardware_ctx.get('gpu_memory_gb', 0)
        cpu_count = hardware_ctx.get('cpu_count', os.cpu_count() or 1)
        system_memory_gb = hardware_ctx.get('system_memory_gb', 8)
        
        # Hardware status with GPU info
        if gpu_available:
            cuda_info = hardware_ctx.get('cuda', {})
            if cuda_info.get('device_name'):
                gpu_name = cuda_info['device_name']
                gpu_status = f"[bold green]-OK- {gpu_name}[/bold green] ({gpu_memory_gb:.1f}GB VRAM)"
            elif 'devices' in cuda_info and isinstance(cuda_info['devices'], list) and cuda_info['devices']:
                gpu_name = cuda_info['devices'][0].get('name', 'Unknown GPU')
                gpu_status = f"[bold green]-OK- {gpu_name}[/bold green] ({gpu_memory_gb:.1f}GB VRAM)"
            else:
                gpu_status = f"[bold green]-OK- GPU Available[/bold green] ({gpu_memory_gb:.1f}GB VRAM)"
        else:
            gpu_status = "[bold yellow]-WARN- CPU Only[/bold yellow] - Consider GPU for better performance"
        
        # CPU status with performance class info
        cpu_perf_class = hardware_ctx.get('cpu_performance_class', 'unknown')
        if cpu_count >= 16:
            cpu_status = f"[bold green]-OK- {cpu_count} CPU Cores[/bold green] ({cpu_perf_class})"
        elif cpu_count >= 8:
            cpu_status = f"[bold green]-OK- {cpu_count} CPU Cores[/bold green] ({cpu_perf_class})"
        elif cpu_count >= 4:
            cpu_status = f"[bold yellow]-WARN- {cpu_count} CPU Cores[/bold yellow] ({cpu_perf_class})"
        else:
            cpu_status = f"[bold red]-WARN- {cpu_count} CPU Cores[/bold red] (Limited)"
        
        # System memory status with performance class
        memory_perf_class = hardware_ctx.get('memory_performance_class', 'unknown')
        if system_memory_gb >= 32:
            mem_status = f"[bold green]-OK- {system_memory_gb:.1f}GB RAM[/bold green] ({memory_perf_class})"
        elif system_memory_gb >= 16:
            mem_status = f"[bold green]-OK- {system_memory_gb:.1f}GB RAM[/bold green] ({memory_perf_class})"
        elif system_memory_gb >= 8:
            mem_status = f"[bold yellow]-WARN- {system_memory_gb:.1f}GB RAM[/bold yellow] ({memory_perf_class})"
        else:
            mem_status = f"[bold red]-WARN- {system_memory_gb:.1f}GB RAM[/bold red] (Limited)"
        
        hardware_text = f"{gpu_status}\n{cpu_status}\n{mem_status}"
        
        # Add system class
        if system_class != 'unknown':
            hardware_text += f"\n[bold cyan]-INFO- System Class: {system_class.title()}[/bold cyan]"
        
        # Add memory optimization
        if memory_optimizations > 0:
            hardware_text += f"\n[bold green]-INFO- Memory optimizations performed: {memory_optimizations}[/bold green]"
        
        # Add helper function utilization
        if helper_functions_used > 0:
            hardware_text += f"\n[bold cyan]-INFO- Helper functions utilized: {helper_functions_used}[/bold cyan]"
        
        # Add hardware analysis method if available
        collection_method = hardware_ctx.get('collection_method', '')
        if collection_method:
            hardware_text += f"\n[bold]-INFO- Detection method: {collection_method}[/bold]"
        
        hardware_panel = Panel.fit(
            hardware_text,
            title="[bold]SYSTEM CAPABILITIES & ANALYSIS CONTEXT[/bold]",
            border_style="bold blue",
            title_align="left",
            style="bold",
            padding=(1, 2)
        )
        console.print()
        console.print(hardware_panel)
        
        # ENHANCED RECOMMENDATIONS & OPTIMAL CHOICES
        if summary:
            # Overall recommendations
            overall_recs = summary.get('recommendations', [])
            optimization_suggestions = summary.get('optimization_suggestions', [])
            
            all_recommendations = overall_recs + optimization_suggestions
            
            if all_recommendations:
                # Categorize recommendations based on current implementation patterns
                performance_recs = [rec for rec in all_recommendations if any(word in rec.lower() for word in ['performance', 'speed', 'throughput', 'optimization', 'gpu', 'batch', 'precision'])]
                memory_recs = [rec for rec in all_recommendations if any(word in rec.lower() for word in ['memory', 'ram', 'vram', 'checkpointing', 'accumulation', 'clear'])]
                config_recs = [rec for rec in all_recommendations if any(word in rec.lower() for word in ['config', 'setting', 'parameter', 'dimension', 'encoding'])]
                general_recs = [rec for rec in all_recommendations if rec not in performance_recs + memory_recs + config_recs]
                
                rec_sections = [
                    ("Performance Optimization", performance_recs, "bold green"),
                    ("Memory Management", memory_recs, "bold yellow"),
                    ("Configuration Tuning", config_recs, "bold cyan"),
                    ("General Recommendations", general_recs, "bold white")
                ]
                
                for section_title, recs, color in rec_sections:
                    if recs:
                        # Limit to 5 per section and format consistently
                        rec_text = "\n".join([f"[{color}]+[/{color}] {rec}" for rec in recs[:5]])
                        
                        rec_panel = Panel.fit(
                            rec_text,
                            title=f"[bold {color}]{section_title.upper()}[/bold {color}]",
                            border_style=color,
                            title_align="left",
                            style="bold",
                            padding=(1, 2)
                        )
                        console.print(rec_panel)
            
            # Optimal choices table
            optimal = summary.get('optimal_choices', {})
            if optimal:
                opt_table = Table(
                    title="\n[bold yellow]OPTIMAL MODEL SELECTION GUIDE[/bold yellow]",
                    box=box.ROUNDED,
                    header_style="bold white",
                    title_style="bold green",
                    title_justify="left",
                    border_style="green",
                    show_lines=True,
                    expand=True
                )
                
                opt_table.add_column("Scenario", style="bold cyan", width=15)
                opt_table.add_column("Recommended Model", style="bold green", width=15)
                opt_table.add_column("Rationale", style="bold", width=35)
                opt_table.add_column("Performance", style="bold cyan", width=10, justify="left")
                
                # Rationale and performance metrics
                for scenario, choice in optimal.items():
                    scenario_display = scenario.replace('_', ' ').title()
                    
                    # Get performance data for the chosen model
                    choice_data = model_results.get(choice, {})
                    perf_data = choice_data.get('performance', {})
                    arch_data = choice_data.get('architecture', {})
                    memory_data = choice_data.get('memory_analysis', {})
                    
                    # Rationale based on actual metrics from current implementation
                    if scenario == 'fastest_inference':
                        rationale = "Minimizes latency for real-time applications"
                        # Get inference time from scenario summary
                        scenario_summary = perf_data.get('scenario_summary', {})
                        best_time = float('inf')
                        for sc_name, sc_data in scenario_summary.items():
                            if 'latency_ms' in sc_data:
                                best_time = min(best_time, sc_data['latency_ms'])
                        perf_metric = f"{best_time:.1f}ms" if best_time < float('inf') else "N/A"
                        
                    elif scenario == 'most_efficient':
                        rationale = "Best performance per parameter ratio"
                        eff_score = arch_data.get('architecture_efficiency', 0)
                        perf_metric = f"{eff_score:.0f}%" if eff_score > 0 else "N/A"
                        
                    elif scenario == 'lowest_memory':
                        rationale = "Suitable for memory-constrained environments"
                        # Get memory from detailed analysis
                        gpu_mem = memory_data.get('gpu_memory_detailed', {}).get('allocated_mb', 0)
                        cpu_mem = memory_data.get('cpu_memory_analysis', {}).get('estimated_total_cpu_memory_mb', 0)
                        mem_mb = gpu_mem or cpu_mem
                        perf_metric = f"{mem_mb:.3f}MB" if mem_mb > 0 else "N/A"
                        
                    elif scenario == 'smallest_model':
                        rationale = "Minimal resource footprint and fastest loading"
                        params = arch_data.get('total_params', 0)
                        perf_metric = f"{params:,}" if params > 0 else "N/A"
                        
                    elif scenario == 'best_balanced':
                        rationale = "Optimal trade-off across all performance metrics"
                        perf_metric = "Balanced"
                        
                    else:
                        rationale = "Best choice for this specific use case"
                        perf_metric = "Optimized"
                    
                    opt_table.add_row(
                        scenario_display,
                        Text(choice, style="bold"),
                        rationale,
                        perf_metric
                    )
                
                console.print(opt_table)
            
            # Performance rankings
            rankings = summary.get('performance_ranking', {})
            if rankings:
                rank_table = Table(
                    title="\n[bold yellow]PERFORMANCE RANKINGS BY CATEGORY[/bold yellow]",
                    box=box.ROUNDED,
                    header_style="bold white",
                    title_style="bold blue",
                    title_justify="left",
                    border_style="blue",
                    show_lines=True,
                    expand=True
                )
                
                rank_table.add_column("Category", style="bold cyan", width=15)
                rank_table.add_column("First Place", style="bold green", width=15)
                rank_table.add_column("Second Place", style="bold yellow", width=15)
                rank_table.add_column("Third Place", style="bold white", width=15)
                rank_table.add_column("Metric", width=10, justify="left")
                
                ranking_labels = {
                    'speed': 'Fastest Inference',
                    'efficiency': 'Most Efficient',
                    'memory_efficiency': 'Memory Efficient',
                    'size': 'Smallest Size'
                }
                
                for metric, models in rankings.items():
                    if models and metric in ranking_labels:
                        first = models[0] if len(models) > 0 else "N/A"
                        second = models[1] if len(models) > 1 else "N/A"
                        third = models[2] if len(models) > 2 else "N/A"
                        
                        # Get metric value for first place from current implementation data
                        if first != "N/A" and first in model_results:
                            first_data = model_results[first]
                            if metric == 'speed':
                                # Get best throughput from scenario summary
                                perf = first_data.get('performance', {})
                                scenario_summary = perf.get('scenario_summary', {})
                                best_throughput = 0
                                for sc_data in scenario_summary.values():
                                    if 'throughput' in sc_data:
                                        best_throughput = max(best_throughput, sc_data['throughput'])
                                metric_val = f"{best_throughput:.0f}/s" if best_throughput > 0 else "N/A"
                                
                            elif metric == 'efficiency':
                                arch = first_data.get('architecture', {})
                                metric_val = f"{arch.get('architecture_efficiency', 0):.0f}%"
                                
                            elif metric == 'memory_efficiency':
                                mem = first_data.get('memory_analysis', {})
                                gpu_mem = mem.get('gpu_memory_detailed', {}).get('allocated_mb', 0)
                                cpu_mem = mem.get('cpu_memory_analysis', {}).get('estimated_total_cpu_memory_mb', 0)
                                mem_mb = gpu_mem or cpu_mem
                                metric_val = f"{mem_mb:.3f}MB" if mem_mb > 0 else "N/A"
                                
                            elif metric == 'size':
                                arch = first_data.get('architecture', {})
                                metric_val = f"{arch.get('total_params', 0):,}"
                            else:
                                metric_val = "N/A"
                        else:
                            metric_val = "N/A"
                        
                        rank_table.add_row(
                            ranking_labels[metric],
                            first,
                            second,
                            third,
                            metric_val
                        )
                
                console.print(rank_table)
        
        # ENHANCED WARNINGS & ISSUES
        warnings = summary.get('warnings', []) if summary else []
        analysis_warnings = []
        for model_name, model_data in model_results.items():
            model_warnings = model_data.get('warnings', [])
            if model_warnings:
                analysis_warnings.extend([f"{model_name}: {w}" for w in model_warnings])
        
        all_warnings = warnings + analysis_warnings
        
        if all_warnings or failed_models:
            warn_items = []
            
            # Categorize warnings by type based on current implementation
            critical_warnings = []
            performance_warnings = []
            config_warnings = []
            memory_warnings = []
            
            for warning in all_warnings:
                if any(word in warning.lower() for word in ['critical', 'error', 'fail', 'crash']):
                    critical_warnings.append(f"[bold red]-CRITICAL-[/bold red] {warning}")
                elif any(word in warning.lower() for word in ['memory', 'ram', 'vram', 'oom', 'allocation']):
                    memory_warnings.append(f"[bold yellow]-MEMORY-[/bold yellow] {warning}")
                elif any(word in warning.lower() for word in ['performance', 'slow', 'degradation', 'throughput']):
                    performance_warnings.append(f"[bold cyan]-PERF-[/bold cyan] {warning}")
                else:
                    config_warnings.append(f"[bold magenta]-CONFIG-[/bold magenta] {warning}")
            
            # Add failed model warnings with error analysis from current implementation
            for model_name, model_data in failed_models:
                errors = model_data.get('errors', [])
                analysis_status = model_data.get('analysis_status', 'unknown')
                
                if errors:
                    error_msg = errors[0] if isinstance(errors, list) else str(errors)
                else:
                    error_msg = model_data.get('error', 'Unknown error')
                
                analysis_time = model_data.get('analysis_metadata', {}).get('analysis_time_seconds', 0)
                error_type = model_data.get('analysis_metadata', {}).get('error_type', 'Unknown')
                
                detailed_error = f"{model_name}: {error_msg[:60]}{'...' if len(error_msg) > 60 else ''}"
                if analysis_time > 0:
                    detailed_error += f" (failed after {analysis_time:.2f}s)"
                if error_type != 'Unknown':
                    detailed_error += f" [{error_type}]"
                
                critical_warnings.append(f"[bold red]-FAIL-[/boldred] {detailed_error}")
            
            # Display warnings by category with current implementation context
            all_warn_items = critical_warnings + memory_warnings + performance_warnings + config_warnings
            
            if all_warn_items:
                warn_text = "\n".join(all_warn_items[:12])  # Limit to 12 total warnings
                if len(all_warn_items) > 12:
                    warn_text += f"\n[bold]... and {len(all_warn_items) - 12} more issues[/bold]"
                
                warn_panel = Panel.fit(
                    warn_text,
                    title="[bold yellow]WARNINGS & ANALYSIS ISSUES[/bold yellow]",
                    border_style="yellow",
                    title_align="left",
                    padding=(1, 2)
                )
                console.print(warn_panel)
        
        # ENHANCED USE CASE RECOMMENDATIONS
        use_case_recs = summary.get('use_case_recommendations', {}) if summary else {}
        if use_case_recs:
            use_case_table = Table(
                title="[bold yellow]USE CASE SPECIFIC RECOMMENDATIONS[/bold yellow]",
                box=box.ROUNDED,
                header_style="bold white",
                title_style="bold magenta",
                title_justify="left",
                border_style="magenta",
                show_lines=True,
                expand=True
            )
            
            use_case_table.add_column("Use Case", style="bold magenta", width=15)
            use_case_table.add_column("Primary Choice", style="bold green", width=15)
            use_case_table.add_column("Alternative Options", width=25)
            use_case_table.add_column("Key Benefits", width=25)
            
            use_case_labels = {
                'prototyping_development': 'Prototyping & Development',
                'production_deployment': 'Production Deployment',
                'resource_constrained': 'Resource-Constrained',
                'high_performance': 'High Performance',
                'research_experimentation': 'Research & Experimentation'
            }
            
            use_case_benefits = {
                'prototyping_development': 'Fast iteration, low resource use',
                'production_deployment': 'Balanced performance & reliability',
                'resource_constrained': 'Minimal memory & compute needs',
                'high_performance': 'Maximum accuracy & capability',
                'research_experimentation': 'Advanced features & flexibility'
            }
            
            for use_case, models in use_case_recs.items():
                if models and use_case in use_case_labels:
                    primary = models[0] if models else "None"
                    alternatives = ", ".join(models[1:3]) if len(models) > 1 else "None"
                    if len(models) > 3:
                        alternatives += f" (+{len(models) - 3} more)"
                    
                    benefits = use_case_benefits.get(use_case, "Optimized for this scenario")
                    
                    use_case_table.add_row(
                        use_case_labels[use_case],
                        Text(primary, style="bold"),
                        alternatives,
                        benefits
                    )
            
            console.print(use_case_table)
        
        # ENHANCED CONFIGURATION GUIDANCE
        config_guidance = []
        
        # Hardware-specific guidance with recommendations from current system analysis
        if gpu_available:
            gpu_perf_class = hardware_ctx.get('gpu_performance_class', 'unknown')
            if gpu_memory_gb >= 16:
                config_guidance.extend([
                    f"[bold green]OK[/bold green] High-end GPU detected ({gpu_perf_class}) - All models fully supported",
                    "[bold green]OK[/bold green] Enable mixed precision (FP16) for 40-50% memory savings",
                    "[bold green]OK[/bold green] Use large batch sizes (64-128) for optimal throughput",
                    "[bold green]OK[/bold green] AutoencoderEnsemble recommended for maximum accuracy"
                ])
            elif gpu_memory_gb >= 8:
                config_guidance.extend([
                    f"[bold cyan]OK[/bold cyan] Mid-range GPU ({gpu_perf_class}) - Good for most models",
                    "[bold cyan]WARN[/bold cyan] Use moderate batch sizes (32-64) to avoid memory issues",
                    "[bold cyan]OK[/bold cyan] EnhancedAutoencoder recommended for balanced performance"
                ])
            elif gpu_memory_gb >= 4:
                config_guidance.extend([
                    f"[bold yellow]WARN[/bold yellow] Entry-level GPU ({gpu_perf_class}) - Reduce batch sizes to 16-32",
                    "[bold yellow]WARN[/bold yellow] SimpleAutoencoder or small EnhancedAutoencoder recommended",
                    "[bold yellow]WARN[/bold yellow] Monitor memory usage during training with nvidia-smi"
                ])
            else:
                config_guidance.extend([
                    "[bold red]WARN[/bold red] Very limited GPU memory - Use smallest models only",
                    "[bold red]WARN[/bold red] Batch size should be 8-16 maximum",
                    "[bold red]WARN[/bold red] Consider gradient accumulation instead of large batches"
                ])
        else:
            config_guidance.extend([
                "[bold yellow]WARN[/bold yellow] CPU-only training - Use SimpleAutoencoder for reasonable performance",
                "[bold yellow]WARN[/bold yellow] Reduce batch_size to 16-32 for CPU efficiency",
                "[bold yellow]WARN[/bold yellow] Set num_workers to match CPU core count for data loading",
                "[bold yellow]INFO[/bold yellow] Consider cloud GPU instances for significantly faster training"
            ])
        
        # System class specific guidance
        if system_class == 'high_performance':
            config_guidance.append("[bold green]-OK-[/bold green] High-performance system - Can handle complex models and large batches")
        elif system_class == 'limited':
            config_guidance.append("[bold yellow]-WARN-[/bold yellow] Limited system resources - Use conservative settings")
        elif system_class == 'standard':
            config_guidance.append("[bold cyan]-INFO-[/bold cyan] Standard system capabilities - Balanced configuration recommended")
        
        # Memory-specific guidance from current system analysis
        if system_memory_gb < 8:
            config_guidance.append("[bold red]-WARN-[/bold red] Low system RAM - Close other applications during training")
        elif system_memory_gb >= 32:
            config_guidance.append("[bold green]-OK-[/bold green] Excellent system RAM - Can handle large datasets and models")
        
        # Memory optimization guidance
        if memory_optimizations > 0:
            config_guidance.append(f"[bold cyan]-INFO-[/bold cyan] Memory optimizations already applied ({memory_optimizations} operations)")
        
        # Helper function integration guidance
        if helper_functions_used > 0:
            config_guidance.append(f"[bold cyan]-INFO-[/bold cyan] Comprehensive analysis using {helper_functions_used} helper functions")
        
        # Configuration optimization tips
        config_guidance.extend([
            "[bold cyan]-INFO-[/bold cyan] Use presets: select_preset_config('lightweight') or select_preset_config('performance')",
            "[bold cyan]-INFO-[/bold cyan] Enable tensorboard logging: config['training']['use_tensorboard'] = True",
            "[bold cyan]-INFO-[/bold cyan] Set up early stopping: config['training']['early_stopping'] = True",
            "[bold cyan]-INFO-[/bold cyan] Configure gradient clipping: config['training']['gradient_clip'] = 1.0",
            "[bold cyan]-INFO-[/bold cyan] Use enhanced_clear_memory() for memory optimization"
        ])
        
        # Model-specific configuration guidance based on successful analyses
        if successful_models:
            best_model = max(successful_models, key=lambda x: x[1].get('architecture', {}).get('architecture_efficiency', 0))
            config_guidance.append(f"[bold green]-RECOMMENDED-[/bold green] Start with {best_model[0]} for optimal balance")
        
        config_text = "\n".join(config_guidance)
        config_panel = Panel.fit(
            config_text,
            title="[bold yellow]CONFIGURATION GUIDANCE & OPTIMIZATION[/bold yellow]",
            border_style="bold cyan",
            title_align="left",
            padding=(1, 2)
        )
        console.print(config_panel)
        
        # ENHANCED RESOURCE REQUIREMENTS SUMMARY
        if successful_models:
            resource_table = Table(
                title="[bold yellow]RESOURCE REQUIREMENTS SUMMARY[/bold yellow]",
                box=box.ROUNDED,
                header_style="bold cyan",
                border_style="cyan",
                title_style="bold green",
                title_justify="left",
                show_lines=True,
                expand=True
            )
            
            resource_table.add_column("Model", style="bold cyan", width=15)
            resource_table.add_column("Training Memory", width=10, justify="left")
            resource_table.add_column("Training Time", width=10, justify="left")
            resource_table.add_column("Min Hardware", width=10)
            resource_table.add_column("Recommended", width=10)
            resource_table.add_column("Optimization", width=15)
            
            for model_name, model_data in successful_models[:5]:  # Limit to top 5
                resources_data = model_data.get('resource_requirements', {})
                
                # Extract resource information
                training_mem = resources_data.get('memory', {}).get('training_memory', {})
                time_estimates = resources_data.get('time_estimates', {})
                hw_reqs = resources_data.get('hardware_requirements', {})
                
                # Format memory requirement
                mem_with_overhead = training_mem.get('total_with_overhead', {})
                if isinstance(mem_with_overhead, dict):
                    mem_gb = mem_with_overhead.get('gb', 0)
                else:
                    mem_gb = training_mem.get('total_training', {}).get('gb', 0)
                mem_text = f"{mem_gb:.3f}GB" if mem_gb > 0 else "N/A"
                
                # Format training time
                convergence_estimates = time_estimates.get('convergence_estimates', {})
                if convergence_estimates:
                    epoch_time = convergence_estimates.get('total_training_time_hours', 0) / max(convergence_estimates.get('estimated_epochs', 100), 1)
                else:
                    epoch_time = time_estimates.get('training_time', {}).get('time_per_epoch_hours', 0)
                
                if epoch_time > 1:
                    time_text = f"{epoch_time:.3f}h/epoch"
                elif epoch_time > 0:
                    time_text = f"{epoch_time*60:.3f}m/epoch"
                else:
                    time_text = "N/A"
                
                # Hardware recommendations
                min_hw = hw_reqs.get('minimum_requirements', {})
                rec_hw = hw_reqs.get('recommended_requirements', {})
                
                min_gpu = min_hw.get('gpu_memory_gb', 0) or 0
                rec_gpu = rec_hw.get('gpu_memory_gb', 0) or 0
                
                min_text = f"{min_gpu:.3f}GB GPU" if min_gpu > 0 else "CPU OK"
                rec_text = f"{rec_gpu:.3f}GB GPU" if rec_gpu > 0 else "CPU"
                
                # Optimization suggestions
                opt_strategies = resources_data.get('optimization_strategies', {})
                mem_opts = opt_strategies.get('memory_optimization', [])
                comp_opts = opt_strategies.get('compute_optimization', [])
                
                if mem_opts and 'mixed precision' in str(mem_opts[0]).lower():
                    opt_text = "Mixed Precision"
                elif mem_opts and 'batch' in str(mem_opts[0]).lower():
                    opt_text = "Smaller Batches"
                elif mem_opts and 'checkpointing' in str(mem_opts[0]).lower():
                    opt_text = "Grad Checkpoint"
                elif comp_opts and 'gpu' in str(comp_opts[0]).lower():
                    opt_text = "GPU Required"
                else:
                    opt_text = "Standard"
                
                resource_table.add_row(
                    model_name,
                    mem_text,
                    time_text,
                    min_text,
                    rec_text,
                    opt_text
                )
            
            console.print(resource_table)
        
        # ENHANCED SCALING ANALYSIS SUMMARY
        if successful_models and any('scaling' in model_data for _, model_data in successful_models):
            scaling_table = Table(
                title="[bold yellow]MODEL SCALING ANALYSIS[/bold yellow]",
                box=box.ROUNDED,
                header_style="bold cyan",
                border_style="cyan",
                title_style="bold blue",
                title_justify="left",
                show_lines=True,
                expand=True
            )
            
            scaling_table.add_column("Model", style="bold cyan", width=15)
            scaling_table.add_column("Parameter Scaling", width=10)
            scaling_table.add_column("FLOP Scaling", width=10)
            scaling_table.add_column("Memory Scaling", width=10)
            scaling_table.add_column("Optimal Batch", width=10, justify="left")
            scaling_table.add_column("Complexity Growth", width=15)
            
            for model_name, model_data in successful_models:
                scaling_data = model_data.get('scaling', {})
                if scaling_data:
                    # Input dimension scaling
                    input_scaling = scaling_data.get('input_dimension_scaling', {})
                    scaling_coeffs = input_scaling.get('scaling_coefficients', {})
                    
                    param_exp = scaling_coeffs.get('parameter_scaling_exponent', 0)
                    flop_exp = scaling_coeffs.get('flop_scaling_exponent', 0)
                    
                    param_scaling_text = f"O(n^{param_exp:.3f})" if param_exp > 0 else "N/A"
                    flop_scaling_text = f"O(n^{flop_exp:.3f})" if flop_exp > 0 else "N/A"
                    
                    # Memory scaling
                    memory_curves = input_scaling.get('memory_curves', {})
                    mem_exp = memory_curves.get('memory_scaling_exponent', 0)
                    mem_scaling_text = f"O(n^{mem_exp:.3f})" if mem_exp > 0 else "Linear"
                    
                    # Batch size scaling
                    batch_scaling = scaling_data.get('batch_size_scaling', {})
                    optimal_batches = batch_scaling.get('optimal_batch_sizes', {})
                    opt_batch = optimal_batches.get('recommended', 'N/A')
                    
                    # Parameter scaling complexity growth
                    param_scaling = scaling_data.get('parameter_scaling', {})
                    complexity_growth = param_scaling.get('complexity_growth', 'unknown').replace('_', ' ').title()
                    
                    scaling_table.add_row(
                        model_name,
                        param_scaling_text,
                        flop_scaling_text,
                        mem_scaling_text,
                        str(opt_batch),
                        complexity_growth
                    )
            
            if scaling_table.rows:  # Only show if we have scaling data
                console.print(scaling_table)
        
        # TROUBLESHOOTING SECTION
        troubleshoot_sections = {
            "Essential Commands": [
                "[bold cyan]initialize_model_variants(silent=False)[/bold cyan] - Refresh model registry with detailed logs",
                "[bold cyan]validate_model_variants(logger, silent=False)[/bold cyan] - Test model functionality thoroughly",
                "[bold cyan]get_current_config()[/bold cyan] - View current configuration parameters",
                "[bold cyan]select_preset_config('performance')[/bold cyan] - Load optimized preset configuration",
                "[bold cyan]enhanced_clear_memory(aggressive=True)[/bold cyan] - Optimize memory usage"
            ],
            "Performance Optimization": [
                "Adjust batch_size based on available GPU memory and system class",
                "Use mixed precision (FP16) for 40-50% memory savings on modern GPUs",
                "Enable gradient checkpointing: config['training']['gradient_checkpointing'] = True",
                "Use learning rate scheduling for better convergence",
                "Enable early stopping to prevent overfitting and reduce training time"
            ],
            "Memory Management": [
                "Call enhanced_clear_memory() before training to optimize memory",
                "Reduce batch_size if encountering OOM errors",
                "Enable gradient accumulation for large effective batch sizes",
                "Use gradient checkpointing for memory-intensive models",
                "Monitor GPU memory usage with nvidia-smi during training"
            ],
            "Model Selection": [
                f"Start with {optimal.get('best_balanced', 'EnhancedAutoencoder')} for balanced performance" if optimal else "Use EnhancedAutoencoder for balanced performance",
                "Use SimpleAutoencoder for prototyping and debugging",
                "Use AutoencoderEnsemble only when maximum accuracy is required",
                f"Consider system class ({system_class}) when selecting models",
                "Test with default presets before custom configurations"
            ]
        }
        
        troubleshoot_text = ""
        for section, items in troubleshoot_sections.items():
            troubleshoot_text += f"[bold cyan]{section}:[/bold cyan]\n"
            # Limit to 4 items per section
            for item in items[:4]:
                troubleshoot_text += f"- {item}\n"
            troubleshoot_text += "\n"
        
        troubleshoot_panel = Panel.fit(
            troubleshoot_text.strip(),
            title="[bold yellow]TROUBLESHOOTING & OPTIMIZATION GUIDE[/bold yellow]",
            border_style="bold cyan",
            title_align="left",
            style="bold",
            padding=(1, 2)
        )
        console.print(troubleshoot_panel)
        
        # FOOTER WITH STATISTICS
        total_models = len(model_results)
        success_count = len(successful_models)
        fail_count = len(failed_models)
        success_rate = (success_count / total_models * 100) if total_models > 0 else 0
        
        # Analysis completeness metrics
        total_validation_checks = sum(
            len(model_data.get('analysis_metadata', {}).get('validation_checks_passed', []))
            for _, model_data in successful_models
        )
        avg_checks = total_validation_checks / max(success_count, 1)
        
        # Helper function utilization metrics
        total_helper_functions = sum(
            len(model_data.get('analysis_metadata', {}).get('helper_functions_utilized', []))
            for _, model_data in successful_models
        )
        avg_helper_functions = total_helper_functions / max(success_count, 1)
        
        # Performance summary
        if successful_models:
            avg_params = sum(model_data.get('architecture', {}).get('total_params', 0) 
                           for _, model_data in successful_models) / success_count
            avg_efficiency = sum(model_data.get('architecture', {}).get('architecture_efficiency', 0) 
                               for _, model_data in successful_models) / success_count
            
            perf_summary = f"Avg Parameters: {avg_params:,.0f} | Avg Efficiency: {avg_efficiency:.1f}%"
        else:
            perf_summary = "No performance data available"
        
        # Additional metrics
        total_memory_opts = metadata.get('memory_optimization_summary', {}).get('optimizations_performed', 0)
        total_cleanup_actions = metadata.get('memory_optimization_summary', {}).get('total_cleanup_actions', 0)
        
        footer_text = (
            f"[bold cyan]Analysis Completed Successfully[/bold cyan]\n"
            f"Models analyzed: {total_models} | Successful: {success_count} | Failed: {fail_count} | Success rate: {success_rate:.1f}%\n"
            f"System: {system_class.title()} | Hardware: {'GPU available' if gpu_available else 'CPU only'} | {perf_summary}\n"
            f"Analysis depth: {avg_checks:.1f} avg checks/model | Helper functions: {avg_helper_functions:.1f}/model\n"
            f"Memory optimizations: {total_memory_opts} operations, {total_cleanup_actions} cleanup actions\n"
            f"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Version: {comparison_version}"
        )
        
        footer_panel = Panel.fit(
            footer_text,
            title="[bold yellow]COMPREHENSIVE ANALYSIS SUMMARY[/bold yellow]",
            border_style="bold green",
            title_align="left",
            style="bold",
            padding=(0, 2)
        )
        console.print(footer_panel)
        
        # Logging with metrics
        logger.debug(f"Model comparison display completed successfully:")
        logger.debug(f"  - Total models analyzed: {total_models}")
        logger.debug(f"  - Successful comprehensive analyses: {success_count}")
        logger.debug(f"  - Failed analyses: {fail_count}")
        logger.debug(f"  - Overall success rate: {success_rate:.1f}%")
        logger.debug(f"  - Average validation checks per model: {avg_checks:.1f}")
        logger.debug(f"  - Average helper functions per model: {avg_helper_functions:.1f}")
        logger.debug(f"  - System class: {system_class}")
        logger.debug(f"  - Hardware context: {'GPU available' if gpu_available else 'CPU only'}")
        logger.debug(f"  - Memory optimizations: {total_memory_opts} operations")
        logger.debug(f"  - Comparison version: {comparison_version}")
        
        # Log individual model performance for debugging
        for model_name, model_data in successful_models:
            arch = model_data.get('architecture', {})
            analysis_status = model_data.get('analysis_status', 'unknown')
            validation_checks = len(model_data.get('analysis_metadata', {}).get('validation_checks_passed', []))
            helper_functions = len(model_data.get('analysis_metadata', {}).get('helper_functions_utilized', []))
            
            logger.debug(
                f"Model {model_name} [{analysis_status}]: "
                f"{arch.get('total_params', 0):,} params, "
                f"efficiency: {arch.get('architecture_efficiency', 0):.1f}%, "
                f"complexity: {arch.get('complexity_level', 'unknown')}, "
                f"checks: {validation_checks}, "
                f"helpers: {helper_functions}"
            )
        
        # Log error information for failed models
        for model_name, model_data in failed_models:
            errors = model_data.get('errors', [])
            analysis_status = model_data.get('analysis_status', 'unknown')
            error_type = model_data.get('analysis_metadata', {}).get('error_type', 'Unknown')
            
            if errors:
                error_msg = errors[0] if isinstance(errors, list) else str(errors)
            else:
                error_msg = model_data.get('error', 'Unknown error')
            
            logger.error(f"Model {model_name} analysis failed [{analysis_status}] [{error_type}]: {error_msg}")
        
    except Exception as e:
        error_msg = f"Critical failure in enhanced display_model_comparison: {str(e)}"
        logger.critical(error_msg, exc_info=True)
        
        # Display errors with context
        message = (
            f"Critical failure in model comparison display: {str(e)}\n"
            f"Context:\n"
            f"- Current Preset: [bold yellow]{preset_name if 'preset_name' in locals() else 'Unknown'}[/bold yellow]\n"
            f"- Model Type: [bold yellow]{model_type if 'model_type' in locals() else 'Unknown'}[/bold yellow]\n"
            f"- Config Source: [bold yellow]{config_source if 'config_source' in locals() else 'Unknown'}[/bold yellow]\n\n"
            f"Enhanced Recovery Actions:\n"
            f"1. [bold yellow]initialize_model_variants()[/bold yellow] - Reinitialize model registry\n"
            f"2. [bold yellow]check_hardware()[/bold yellow] - Verify system capabilities and resources\n"
            f"3. [bold yellow]get_current_config()[/bold yellow] - Validate configuration parameters\n"
            f"4. [bold yellow]validate_model_variants(logger)[/bold yellow] - Test model functionality\n"
            f"5. [bold yellow]select_preset_config('lightweight')[/bold yellow] - Use simplified configuration\n"
            f"6. Restart Python session if persistent issues occur"
        )
        console.print(
            Panel.fit(
                f"[bold red]{message}[/bold red]",
                title="CRITICAL DISPLAY ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )

# Model variants validation


def display_model_initialization_summary(
    model_instance,
    model_type: str,
    input_dim: int,
    architecture_info: Dict[str, Any],
    device: torch.device,
    mixed_precision: bool,
    preset_name: Optional[str] = None,
    training_config: Optional[Dict[str, Any]] = None,
    enhanced_features: Optional[Dict[str, bool]] = None,
    ensemble_info: Optional[Dict[str, Any]] = None,
    show_panels: Union[bool, Dict[str, bool]] = False
) -> None:
    """
    Display comprehensive model initialization summary with rich formatting.
    
    This function provides rich console output and logs detailed information to file
    for SimpleAutoencoder, EnhancedAutoencoder, and AutoencoderEnsemble classes.
    
    Args:
        model_instance: The model instance (for parameter counting)
        model_type: Type of model ('SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble')
        input_dim: Input dimension
        architecture_info: Dictionary containing architecture details
        device: PyTorch device
        mixed_precision: Whether mixed precision is enabled
        preset_name: Name of preset used (if any)
        training_config: Training configuration dictionary
        enhanced_features: Dictionary of enhanced features (for EnhancedAutoencoder)
        ensemble_info: Dictionary of ensemble information (for AutoencoderEnsemble)
        show_panels: Control which panels to display. Options:
            - True: Show all panels
            - False: Hide all panels
            - Dict: Selective control with keys:
                - "status": Status & recommendations panel
                - "quick_ref": Quick reference panel
                - "optimization": Optimization tips panel
    """
    try:
        # ENHANCED PRESET NAME DETECTION
        detected_preset_name = preset_name
        
        # Try multiple sources for preset information if not provided
        if not detected_preset_name and hasattr(model_instance, 'config'):
            config = model_instance.config
            # Check multiple locations for preset information
            detected_preset_name = (
                config.get('metadata', {}).get('preset_used') or
                config.get('presets', {}).get('current_preset') or
                config.get('runtime', {}).get('applied_preset') or
                config.get('runtime', {}).get('factory_preset_applied')
            )
        
        # Check model instance attributes
        if not detected_preset_name and hasattr(model_instance, 'preset_name'):
            detected_preset_name = model_instance.preset_name
        
        # Final fallback
        if not detected_preset_name:
            detected_preset_name = "Custom"
        
        # Calculate model parameters
        total_params = sum(p.numel() for p in model_instance.parameters())
        trainable_params = sum(p.numel() for p in model_instance.parameters() if p.requires_grad)
        
        # Parse panel display options
        if isinstance(show_panels, bool):
            show_status = show_panels
            show_quick_ref = show_panels
            show_optimization = show_panels
        else:
            show_status = show_panels.get("status", False)
            show_quick_ref = show_panels.get("quick_ref", False)
            show_optimization = show_panels.get("optimization", False)
        
        # CREATE COMPREHENSIVE LOG DATA DICTIONARY (for file logging only)
        log_data = {
            'model_initialization': {
                'model_type': model_type,
                'input_dim': input_dim,
                'total_parameters': total_params,
                'trainable_parameters': trainable_params,
                'device': str(device),
                'mixed_precision': mixed_precision,
                'preset_name': detected_preset_name,
                'initialization_timestamp': datetime.now().isoformat()
            },
            'architecture_details': {
                'input_dimension': input_dim,
                'hidden_dims': architecture_info.get('hidden_dims', []),
                'encoding_dim': architecture_info.get('encoding_dim', 'Unknown'),
                'architecture_summary': f"{input_dim} -> {architecture_info.get('hidden_dims', [])} -> {architecture_info.get('encoding_dim', 'Unknown')}"
            },
            'model_configuration': {},
            'enhanced_features': enhanced_features or {},
            'ensemble_configuration': ensemble_info or {},
            'training_configuration': training_config or {},
            'system_information': {
                'device': str(device),
                'device_type': device.type,
                'cuda_available': torch.cuda.is_available(),
                'mixed_precision_enabled': mixed_precision
            },
            'memory_estimates': {
                'parameter_memory_mb': total_params * 4 / (1024**2),
                'estimated_training_memory_mb': total_params * 16 / (1024**2),  # Rough estimate
                'model_size_mb': total_params * 4 / (1024**2)
            },
            'warnings_and_recommendations': []
        }
        
        # Add ensemble-specific information to log data
        if ensemble_info:
            log_data['ensemble_details'] = {
                'num_models': ensemble_info.get('num_models', 'Unknown'),
                'model_types': ensemble_info.get('model_types', {}),
                'diversity_factor': ensemble_info.get('diversity_factor', 'Unknown'),
                'ensemble_size': f"{ensemble_info.get('num_models', 'Unknown')} models"
            }
        
        # Add enhanced features details to log data
        if enhanced_features:
            features_enabled = []
            features_disabled = []
            for feature, enabled in enhanced_features.items():
                if enabled:
                    features_enabled.append(feature.replace('_', ' ').title())
                else:
                    features_disabled.append(feature.replace('_', ' ').title())
            
            log_data['enhanced_features_analysis'] = {
                'features_enabled': features_enabled,
                'features_disabled': features_disabled,
                'total_features': len(enhanced_features),
                'enabled_count': len(features_enabled),
                'disabled_count': len(features_disabled)
            }
        
        # Add training configuration details to log data
        if training_config:
            log_data['training_details'] = {
                'optimizer': training_config.get('optimizer', 'Unknown'),
                'learning_rate': training_config.get('learning_rate', 'Unknown'),
                'batch_size': training_config.get('batch_size', 'Unknown'),
                'configuration_complete': True
            }
        else:
            log_data['training_details'] = {
                'configuration_complete': False,
                'status': 'Training components not configured'
            }
        
        # Generate warnings and collect them in log data
        warnings = []
        if hasattr(model_instance, 'mixed_precision') and model_instance.mixed_precision and str(device) == 'cpu':
            warnings.append("Mixed precision enabled but using CPU device")
        
        if ensemble_info and ensemble_info.get('num_models', 0) > 10:
            warnings.append(f"Large ensemble with {ensemble_info.get('num_models')} models may be memory intensive")
        
        if model_type == 'EnhancedAutoencoder' and len(architecture_info.get('hidden_dims', [])) > 8:
            warnings.append(f"Very deep architecture with {len(architecture_info.get('hidden_dims', []))} hidden layers")
        
        if total_params > 10_000_000:  # 10M parameters
            warnings.append(f"Large model with {total_params:,} parameters may require significant memory")
        
        log_data['warnings_and_recommendations'] = warnings
        
        # Add performance analysis to log data
        memory_mb = total_params * 4 / (1024**2)
        log_data['performance_analysis'] = {
            'complexity_level': 'high' if total_params > 1_000_000 else 'medium' if total_params > 100_000 else 'low',
            'memory_classification': 'large' if memory_mb > 100 else 'medium' if memory_mb > 10 else 'small',
            'training_complexity': 'complex' if len(architecture_info.get('hidden_dims', [])) > 3 else 'moderate',
            'hardware_recommendations': []
        }
        
        # Add hardware recommendations to log data
        if torch.cuda.is_available() and "cpu" in str(device).lower():
            log_data['performance_analysis']['hardware_recommendations'].append("GPU available but using CPU - consider GPU acceleration")
        elif not torch.cuda.is_available():
            log_data['performance_analysis']['hardware_recommendations'].append("No GPU available - training will use CPU")
        
        if mixed_precision and torch.cuda.is_available():
            log_data['performance_analysis']['hardware_recommendations'].append("Mixed precision enabled for faster training")
        elif torch.cuda.is_available() and not mixed_precision:
            log_data['performance_analysis']['hardware_recommendations'].append("Consider enabling mixed precision for faster training")
        
        # LOG ALL DETAILED INFORMATION TO FILE ONLY (no console output)
        # Store the original logger level to suppress console handlers temporarily
        handlers_to_suppress = []
        for handler in logger.handlers:
            if isinstance(handler, logging.StreamHandler) and handler.stream.name in ['<stdout>', '<stderr>']:
                handlers_to_suppress.append(handler)
                handler.setLevel(logging.CRITICAL)  # Temporarily suppress console output
        
        try:
            # Log comprehensive information to file
            logger.info("=" * 80)
            logger.info(f"{model_type} INITIALIZATION SUMMARY")
            logger.info("=" * 80)
            
            # Basic information
            logger.info(f"Model Type: {model_type}")
            logger.info(f"Input Dimension: {input_dim}")
            logger.info(f"Architecture: {input_dim} -> {architecture_info.get('hidden_dims', [])} -> {architecture_info.get('encoding_dim', 'Unknown')}")
            logger.info(f"Parameters: {total_params:,} total, {trainable_params:,} trainable")
            logger.info(f"Model Size: {memory_mb:.3f} MB")
            logger.info(f"Device: {device}")
            logger.info(f"Mixed Precision: {mixed_precision}")
            logger.info(f"Preset Used: {detected_preset_name}")
            
            # Ensemble information
            if ensemble_info:
                logger.info(f"Ensemble Configuration:")
                logger.info(f"  - Size: {ensemble_info.get('num_models', 'Unknown')} models")
                logger.info(f"  - Model Types: {ensemble_info.get('model_types', {})}")
                logger.info(f"  - Diversity Factor: {ensemble_info.get('diversity_factor', 'Unknown')}")
            
            # Enhanced features
            if enhanced_features:
                features_list = [f"{k}={v}" for k, v in enhanced_features.items()]
                logger.info(f"Enhanced Features: {', '.join(features_list)}")
                logger.info(f"Features Enabled: {len([f for f, enabled in enhanced_features.items() if enabled])}")
                logger.info(f"Features Disabled: {len([f for f, enabled in enhanced_features.items() if not enabled])}")
            
            # Training configuration
            if training_config:
                logger.info(f"Training Configuration:")
                logger.info(f"  - Optimizer: {training_config.get('optimizer', 'Unknown')}")
                logger.info(f"  - Learning Rate: {training_config.get('learning_rate', 'Unknown')}")
                logger.info(f"  - Batch Size: {training_config.get('batch_size', 'Unknown')}")
            else:
                logger.info("Training Configuration: Not configured")
            
            # Performance analysis
            logger.info(f"Performance Analysis:")
            logger.info(f"  - Complexity Level: {log_data['performance_analysis']['complexity_level']}")
            logger.info(f"  - Memory Classification: {log_data['performance_analysis']['memory_classification']}")
            logger.info(f"  - Training Complexity: {log_data['performance_analysis']['training_complexity']}")
            
            # Hardware recommendations
            if log_data['performance_analysis']['hardware_recommendations']:
                logger.info(f"Hardware Recommendations:")
                for rec in log_data['performance_analysis']['hardware_recommendations']:
                    logger.info(f"  - {rec}")
            
            # Warnings
            if warnings:
                logger.info(f"Warnings:")
                for warning in warnings:
                    logger.warning(f"  - {warning}")
            
            # Memory estimates
            logger.info(f"Memory Estimates:")
            logger.info(f"  - Parameter Memory: {log_data['memory_estimates']['parameter_memory_mb']:.3f} MB")
            logger.info(f"  - Estimated Training Memory: {log_data['memory_estimates']['estimated_training_memory_mb']:.3f} MB")
            
            # Log the complete data structure for debugging
            logger.debug("Complete initialization data structure:")
            import json
            logger.debug(json.dumps(log_data, indent=2, default=str))
            
            logger.info("=" * 80)
            
        finally:
            # Restore original logger levels
            for handler in handlers_to_suppress:
                #handler.setLevel(logging.NOTSET)
                handler.setLevel(logging.ERROR)
        
        # RICH CONSOLE DISPLAY (only visual output)
        try:
            # Model type specific titles and colors
            if model_type == 'SimpleAutoencoder':
                title_color = "bold green"
                border_color = "green"
                header_color = "bold yellow"
                title = "SimpleAutoencoder Initialization Complete"
            elif model_type == 'EnhancedAutoencoder':
                title_color = "bold cyan"
                border_color = "cyan"
                header_color = "bold yellow"
                title = "EnhancedAutoencoder Initialization Complete"
            else:  # AutoencoderEnsemble
                title_color = "bold magenta"
                border_color = "magenta"
                header_color = "bold yellow"
                title = "AutoencoderEnsemble Initialization Complete"
            
            # Create main model information table
            model_table = Table(
                #title=f"[{title_color}]{title}[/{title_color}]",
                title=title,
                box=box.ROUNDED,
                header_style=header_color,
                border_style=border_color,
                #title_style=f"{title_color}",
                title_style=title_color,
                title_justify="left",
                show_lines=True,
                expand=False
            )
            
            model_table.add_column("Attribute", style="bold cyan", width=20)
            model_table.add_column("Value", style="bold white", width=35)
            model_table.add_column("Details", style=title_color, width=25)
            
            # Format memory usage
            if memory_mb > 1000:
                memory_text = f"{memory_mb/1024:.3f} GB"
                memory_style = "bold red" if memory_mb > 5000 else "bold yellow"
            else:
                memory_text = f"{memory_mb:.3f} MB"
                memory_style = "bold green" if memory_mb < 500 else "bold yellow"
            
            # Format parameter count
            if total_params > 1_000_000:
                param_display = f"{total_params/1_000_000:.1f}M"
                param_style = "bold red" if total_params > 100_000_000 else "bold yellow"
            elif total_params > 1_000:
                param_display = f"{total_params/1_000:.1f}K"
                param_style = "bold green"
            else:
                param_display = f"{total_params:,}"
                param_style = "bold green"
            
            # Device status
            device_style = "bold green" if "cuda" in str(device).lower() else "bold yellow"
            device_text = str(device).upper()
            if torch.cuda.is_available() and "cpu" in device_text.lower():
                device_detail = "GPU available but using CPU"
            elif torch.cuda.is_available():
                device_detail = "GPU accelerated"
            else:
                device_detail = "CPU only"
            
            # Mixed precision status
            mp_style = "bold green" if mixed_precision else "bold yellow"
            mp_text = "Enabled" if mixed_precision else "Disabled"
            mp_detail = "FP16 acceleration" if mixed_precision else "FP32 precision"
            
            # Architecture summary
            hidden_dims = architecture_info.get('hidden_dims', [])
            encoding_dim = architecture_info.get('encoding_dim', 'Unknown')
            arch_text = f"{input_dim} - {' - '.join(map(str, hidden_dims))} - {encoding_dim}"
            
            # Add basic rows to table
            model_table.add_row(
                "Model Type", 
                Text(model_type, style=title_color), 
                "Architecture class"
            )
            
            # Ensemble-specific information
            if ensemble_info:
                num_models = ensemble_info.get('num_models', 0)
                diversity = ensemble_info.get('diversity_factor', 0)
                model_types_dist = ensemble_info.get('model_types', {})
                
                model_table.add_row(
                    "Ensemble Size", 
                    Text(f"{num_models} models", style="bold white"), 
                    f"Diversity: {diversity}"
                )
                
                if isinstance(model_types_dist, dict):
                    dist_text = ", ".join([f"{count}x {name}" for name, count in model_types_dist.items()])
                else:
                    dist_text = str(model_types_dist)
                
                model_table.add_row(
                    "Model Distribution", 
                    Text(dist_text, style="bold white"), 
                    "Mixed architectures"
                )
            
            model_table.add_row(
                "Architecture", 
                Text(arch_text, style="bold white"), 
                "Layer dimensions"
            )
            
            model_table.add_row(
                "Parameters", 
                Text(param_display, style=param_style), 
                f"{trainable_params:,} trainable"
            )
            
            model_table.add_row(
                "Memory Usage", 
                Text(memory_text, style=memory_style), 
                "Estimated (FP32)"
            )
            
            model_table.add_row(
                "Device", 
                Text(device_text, style=device_style), 
                device_detail
            )
            
            model_table.add_row(
                "Mixed Precision", 
                Text(mp_text, style=mp_style), 
                mp_detail
            )
            
            # Enhanced features (for EnhancedAutoencoder)
            if enhanced_features:
                features_enabled = []
                features_disabled = []
                
                for feature, enabled in enhanced_features.items():
                    if enabled:
                        features_enabled.append(feature.replace('_', ' ').title())
                    else:
                        features_disabled.append(feature.replace('_', ' ').title())
                
                features_text = ", ".join(features_enabled) if features_enabled else "None"
                features_detail = f"Disabled: {', '.join(features_disabled)}" if features_disabled else "All enabled"
                
                model_table.add_row(
                    "Enhanced Features", 
                    Text(features_text, style="bold cyan"), 
                    features_detail
                )
            
            # Training configuration
            if training_config:
                optimizer = training_config.get('optimizer', 'Unknown')
                lr = training_config.get('learning_rate', 'Unknown')
                training_text = optimizer
                training_detail = f"LR: {lr}"
                training_style = "bold white"
            else:
                training_text = "Not configured"
                training_detail = "Training components not set up"
                training_style = "bold white"
            
            model_table.add_row(
                "Training Setup", 
                Text(training_text, style=training_style), 
                training_detail
            )
            
            model_table.add_row(
                "Preset Used", 
                Text(detected_preset_name, style="bold yellow" if detected_preset_name else "bold white"),
                "Configuration source"
            )
            
            # Display the main table
            console.print()
            console.print(model_table)
            
            # STATUS PANEL (only if enabled)
            if show_status:
                status_items = []
                
                # Performance status
                if total_params > 100_000_000:
                    status_items.append("[bold red]-WARNING-[/bold red] Very large model - may require significant memory and training time")
                elif total_params > 10_000_000:
                    status_items.append("[bold yellow]-NOTICE-[/bold yellow] Large model - monitor memory usage during training")
                else:
                    status_items.append("[bold green]-OK-[/bold green] Model size is reasonable for most hardware configurations")
                
                # Device optimization
                if torch.cuda.is_available() and "cpu" in str(device).lower():
                    status_items.append("[bold yellow]-OPTIMIZE-[/bold yellow] GPU available but using CPU - consider GPU acceleration for better performance")
                elif "cuda" in str(device).lower():
                    status_items.append("[bold green]-OPTIMIZED-[/bold green] Using GPU acceleration for optimal performance")
                
                # Mixed precision
                if mixed_precision:
                    status_items.append("[bold green]-OPTIMIZED-[/bold green] Mixed precision enabled - up to 50% faster training on compatible hardware")
                elif torch.cuda.is_available():
                    status_items.append("[bold yellow]-OPTIMIZE-[/bold yellow] Mixed precision available but disabled - enable for faster training")
                
                # Model-specific recommendations
                if model_type == 'AutoencoderEnsemble':
                    num_models = ensemble_info.get('num_models', 0) if ensemble_info else 0
                    if num_models > 5:
                        status_items.append("[bold yellow]-NOTICE-[/bold yellow] Large ensemble - excellent robustness but higher computational cost")
                    elif num_models >= 3:
                        status_items.append("[bold green]-BALANCED-[/bold green] Good ensemble size balancing robustness vs performance")
                    else:
                        status_items.append("[bold cyan]-INFO-[/bold cyan] Small ensemble - faster inference but potentially less robust")
                elif model_type == 'EnhancedAutoencoder':
                    if enhanced_features and enhanced_features.get('use_attention', False):
                        status_items.append("[bold green][ADVANCED][/bold green] Attention mechanisms enabled for enhanced feature learning")
                    if enhanced_features and enhanced_features.get('residual_blocks', False):
                        status_items.append("[bold green]-ADVANCED-[/bold green] Residual connections enabled for better gradient flow")
                else:  # SimpleAutoencoder
                    status_items.append("[bold green]-SIMPLE-[/bold green] Lightweight architecture - ideal for prototyping and resource-constrained environments")
                
                # Training configuration status
                if training_config:
                    status_items.append("[bold green]-CONFIGURED-[/bold green] Training components initialized and ready")
                else:
                    status_items.append("[bold cyan]-INFO-[/bold cyan] Training components not configured - call setup methods before training")
                
                # Architecture depth warnings
                if len(hidden_dims) > 8:
                    status_items.append("[bold yellow]-WARNING-[/bold yellow] Very deep architecture may be difficult to train - consider gradient clipping")
                elif len(hidden_dims) > 5:
                    status_items.append("[bold cyan]-INFO-[/bold cyan] Deep architecture detected - monitor for vanishing gradient issues")
                
                if status_items:
                    status_text = "\n".join(status_items)
                    status_panel = Panel.fit(
                        status_text,
                        #title="[bold yellow]Initialization Status & Recommendations[/bold yellow]",
                        title="Initialization Status & Recommendations",
                        border_style="bold yellow",
                        style="bold yellow",
                        title_align="left",
                        padding=(1, 2)
                    )
                    console.print()
                    console.print(status_panel)
            
            # QUICK REFERENCE PANEL (only if enabled)
            if show_quick_ref:
                if model_type == 'AutoencoderEnsemble':
                    quick_ref = [
                        "[bold cyan]Ensemble Quick Reference:[/bold cyan]",
                        f"Model summary: [bold green]ensemble.get_model_summary()[/bold green]",
                        f"Configuration: [bold green]ensemble.get_config()[/bold green]",
                        f"Forward pass: [bold green]output = ensemble(input_tensor)[/bold green]",
                        f"Individual models: [bold green]ensemble.models[i](input_tensor)[/bold green]",
                        f"Save ensemble: [bold green]ensemble.save_model('path.pth')[/bold green]"
                    ]
                elif model_type == 'EnhancedAutoencoder':
                    quick_ref = [
                        "[bold cyan]Enhanced Model Quick Reference:[/bold cyan]",
                        f"Model summary: [bold green]model.get_model_summary()[/bold green]",
                        f"Configuration: [bold green]model.get_config()[/bold green]",
                        f"Forward pass: [bold green]output = model(input_tensor)[/bold green]",
                        f"Encode only: [bold green]encoded = model.encode(input_tensor)[/bold green]",
                        f"Decode only: [bold green]decoded = model.decode(encoded)[/bold green]",
                        f"Save model: [bold green]model.save_model('path.pth')[/bold green]"
                    ]
                else:  # SimpleAutoencoder
                    quick_ref = [
                        "[bold cyan]Simple Model Quick Reference:[/bold cyan]",
                        f"Model summary: [bold green]model.get_model_summary()[/bold green]",
                        f"Configuration: [bold green]model.get_config()[/bold green]",
                        f"Forward pass: [bold green]output = model(input_tensor)[/bold green]",
                        f"Encode: [bold green]encoded = model.encode(input_tensor)[/bold green]",
                        f"Decode: [bold green]decoded = model.decode(encoded)[/bold green]",
                        f"Save model: [bold green]model.save_model('path.pth')[/bold green]"
                    ]
                
                quick_ref_text = "\n".join(quick_ref)
                quick_ref_panel = Panel.fit(
                    quick_ref_text,
                    #title="[bold cyan]Usage Guide[/bold cyan]",
                    title="Usage Guide",
                    border_style="bold cyan",
                    style="bold cyan",
                    title_align="left",
                    padding=(0, 2)
                )
                console.print()
                console.print(quick_ref_panel)
            
            # OPTIMIZATION TIPS PANEL (only if enabled)
            if show_optimization:
                optimization_tips = []
                
                if not mixed_precision and torch.cuda.is_available():
                    optimization_tips.append("Enable mixed precision: [bold green]config['training']['mixed_precision'] = True[/bold green]")
                
                if "cpu" in str(device).lower() and torch.cuda.is_available():
                    optimization_tips.append("Use GPU acceleration: [bold green]config['hardware']['device'] = 'cuda'[/bold green]")
                
                if total_params > 1_000_000:
                    optimization_tips.append("Consider gradient clipping: [bold green]config['training']['gradient_clip'] = 1.0[/bold green]")
                
                if model_type == 'AutoencoderEnsemble' and ensemble_info:
                    num_models = ensemble_info.get('num_models', 0)
                    if num_models > 5:
                        optimization_tips.append("For faster inference, consider reducing ensemble size")
                
                optimization_tips.extend([
                    "Monitor training: [bold green]config['monitoring']['tensorboard_logging'] = True[/bold green]",
                    "Enable early stopping: [bold green]config['training']['early_stopping'] = True[/bold green]",
                    "Use learning rate scheduling: [bold green]config['training']['scheduler'] = 'ReduceLROnPlateau'[/bold green]"
                ])
                
                if optimization_tips:
                    opt_text = "\n".join(optimization_tips)
                    opt_panel = Panel.fit(
                        opt_text,
                        #title="[bold green]Performance Optimization Tips[/bold green]",
                        title="Performance Optimization Tips",
                        border_style="green",
                        style="bold green",
                        title_align="left",
                        padding=(0, 2)
                    )
                    console.print()
                    console.print(opt_panel)
            
            console.print()
            
        except ImportError:
            # Fallback to simple formatted text if Rich is not available
            print("=" * 80)
            print(f"{model_type} Initialization Complete")
            print("=" * 80)
            print(f"Model Type: {model_type}")
            
            if ensemble_info:
                print(f"Ensemble Size: {ensemble_info.get('num_models', 'Unknown')} models")
                print(f"Model Types: {ensemble_info.get('model_types', {})}")
                print(f"Diversity Factor: {ensemble_info.get('diversity_factor', 'Unknown')}")
            
            print(f"Architecture: {input_dim} -> {architecture_info.get('hidden_dims', [])} -> {architecture_info.get('encoding_dim', 'Unknown')}")
            print(f"Parameters: {total_params:,} total, {trainable_params:,} trainable")
            print(f"Device: {device}")
            print(f"Mixed Precision: {mixed_precision}")
            print(f"Preset: {detected_preset_name}")
            
            if enhanced_features:
                features_list = [f"{k}={v}" for k, v in enhanced_features.items()]
                print(f"Enhanced Features: {', '.join(features_list)}")
            
            if training_config:
                print(f"Training: {training_config.get('optimizer', 'Unknown')} optimizer")
            
            print(f"Memory Usage: ~{memory_mb:.1f} MB (FP32)")
            
            if warnings:
                print("\nWarnings:")
                for warning in warnings:
                    print(f"  - {warning}")
            
            print("=" * 80)
            
        except Exception as display_error:
            logger.error(f"Failed to display rich initialization summary: {display_error}")
            # Minimal fallback
            print(f"{model_type} initialized: {total_params:,} parameters on {device}")
    
    except Exception as e:
        logger.error(f"Critical error in display_model_initialization_summary: {e}")
        print(f"Model initialization completed with errors. Check logs for details.")

def create_model_instance(
    model_type: str,
    input_dim: int,
    config: Dict[str, Any] = None,
    preset: str = None,
    **kwargs
) -> nn.Module:
    """
    Factory function that uses existing initialization/validation functions to create models.
    
    This eliminates the redundancy between SimpleAutoencoder's parameter processing and
    the functions initialize_model_variants() and validate_model_variants() by leveraging
    the existing comprehensive validation and instantiation infrastructure.
    
    Args:
        model_type: Type of model to create ('SimpleAutoencoder', 'EnhancedAutoencoder', etc.)
        input_dim: Input dimension for the model
        config: Configuration dictionary (optional, will use current config if None)
        preset: Preset name to apply (optional)
        **kwargs: Additional parameters that will be merged into config
        
    Returns:
        Instantiated and validated model instance
        
    Raises:
        ValueError: If model_type is not available or configuration is invalid
        RuntimeError: If model instantiation fails
    """
    try:
        # Ensure MODEL_VARIANTS is initialized
        if not MODEL_VARIANTS:
            logger.info("Initializing model variants for factory pattern")
            initialize_model_variants(silent=True)
        
        # Validate model type availability
        if model_type not in MODEL_VARIANTS:
            available_types = list(MODEL_VARIANTS.keys())
            raise ValueError(f"Model type '{model_type}' not available. Available types: {available_types}")
        
        # Get the model class
        model_class = MODEL_VARIANTS[model_type]
        
        # Get base configuration using existing helper
        if config is None:
            base_config = get_current_config()
        else:
            base_config = config.copy()
        
        # Apply preset if specified
        if preset and preset in PRESET_CONFIGS:
            preset_config = PRESET_CONFIGS[preset].copy()
            # Merge preset with base config (preset takes precedence)
            base_config = deep_update(base_config, preset_config)
            
            # EXPLICITLY SET PRESET INFORMATION IN MULTIPLE LOCATIONS
            if 'metadata' not in base_config:
                base_config['metadata'] = {}
            base_config['metadata']['preset_used'] = preset
            
            if 'presets' not in base_config:
                base_config['presets'] = {}
            base_config['presets']['current_preset'] = preset
            
            if 'runtime' not in base_config:
                base_config['runtime'] = {}
            base_config['runtime']['factory_preset_applied'] = preset
            
            logger.debug(f"Applied preset '{preset}' to configuration")
        
        # Merge any additional kwargs into config
        if kwargs:
            # Convert flat kwargs into structured config sections
            structured_kwargs = _structure_kwargs_into_config_sections(kwargs)
            base_config = deep_update(base_config, structured_kwargs)
            logger.debug(f"Merged {len(kwargs)} additional parameters into configuration")
        
        # Ensure input_dim is in the configuration
        if 'model' not in base_config:
            base_config['model'] = {}
        base_config['model']['input_dim'] = input_dim
        
        # Use existing model_instantiation_with_validation for comprehensive creation
        model_instance, validation_results, performance_metrics, instantiation_details = model_instantiation_with_validation(
            variant_class=model_class,
            variant_name=model_type,
            input_dim=input_dim,
            base_config=base_config,
            fallback_config=None,
            minimal_config=None,
            validation_tests=['basic', 'forward_pass', 'parameters', 'config_methods'],
            comprehensive_validation=True,
            hardware_data=None,
            silent=False,
            #silent=True,
            logger=logger
        )
        
        # Check instantiation success
        if model_instance is None:
            error_msg = f"Failed to instantiate {model_type}: {', '.join(validation_results.get('errors', []))}"
            raise RuntimeError(error_msg)
        
        # Log successful creation
        instantiation_method = instantiation_details.get('method', 'unknown')
        validation_score = validation_results.get('overall_score', 0)
        total_params = performance_metrics.get('total_parameters', 0)
        
        return model_instance
        
    except Exception as e:
        logger.error(f"Factory pattern model creation failed for {model_type}: {e}")
        raise RuntimeError(f"Model factory failed to create {model_type}: {str(e)}") from e

def _structure_kwargs_into_config_sections(kwargs: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convert flat kwargs into structured configuration sections.
    
    This helper function organizes flat parameters into the appropriate configuration
    sections, maintaining compatibility with all preset parameters.
    
    Args:
        kwargs: Flat dictionary of parameters
        
    Returns:
        Structured configuration dictionary with proper sections
    """
    # Parameter to section mapping (same as used in SimpleAutoencoder)
    param_sections = {
        'model': [
            'encoding_dim', 'hidden_dims', 'dropout_rates', 'activation', 'activation_param',
            'normalization', 'use_batch_norm', 'use_layer_norm', 'bias', 'weight_init',
            'skip_connection', 'residual_blocks', 'use_attention', 'model_type',
            'model_types', 'available_activations', 'available_normalizations',
            'available_initializers', 'legacy_mode', 'diversity_factor', 'min_features',
            'num_models'
        ],
        'training': [
            'batch_size', 'epochs', 'learning_rate', 'patience', 'weight_decay',
            'gradient_clip', 'gradient_accumulation_steps', 'mixed_precision',
            'num_workers', 'optimizer', 'scheduler', 'scheduler_params',
            'early_stopping', 'validation_split', 'shuffle', 'pin_memory',
            'persistent_workers', 'adam_betas', 'adam_eps', 'lr_patience',
            'lr_factor', 'min_lr'
        ],
        'data': [
            'normal_samples', 'attack_samples', 'features', 'use_real_data',
            'data_normalization', 'anomaly_factor', 'random_state', 'test_split',
            'stratified_split', 'data_path', 'artifacts_path', 'synthetic_generation',
            'preprocessing'
        ],
        'security': [
            'percentile', 'attack_threshold', 'false_negative_cost', 'enable_security_metrics',
            'anomaly_threshold_strategy', 'early_warning_threshold', 'adaptive_threshold',
            'confidence_interval', 'detection_methods', 'alert_levels', 'threshold_validation',
            'robust_detection', 'false_positive_tolerance', 'performance_optimized_detection',
            'real_time_monitoring', 'ensemble_voting', 'uncertainty_threshold'
        ],
        'monitoring': [
            'metrics_frequency', 'checkpoint_frequency', 'tensorboard_logging', 'progress_bar',
            'console_logging_level', 'save_best_model', 'save_model_history', 'silent',
            'metrics_to_track', 'early_stopping_metric', 'checkpoint_format',
            'log_model_summary', 'tensorboard_dir', 'log_frequency', 'save_checkpoints',
            'tensorboard', 'stability_metrics', 'performance_metrics', 'profiling_enabled'
        ],
        'hardware': [
            'device', 'recommended_gpu_memory', 'minimum_system_requirements',
            'optimal_system_requirements', 'memory_management', 'performance_optimization',
            'detected_gpu_memory', 'detected_system_memory', 'system_performance_class',
            'optimization_recommendations'
        ],
        'system': [
            'model_dir', 'log_dir', 'config_dir', 'data_dir', 'checkpoint_dir', 'results_dir',
            'debug', 'verbose', 'random_seed', 'reproducible', 'parallel_processing',
            'max_workers', 'export_onnx', 'non_interactive', 'cuda_optimizations',
            'onnx_export', 'distributed_training', 'python_executable',
            'working_directory', 'environment_health'
        ],
        'presets': [
            'available_presets', 'current_preset', 'current_override', 'override_rules',
            'preset_configs', 'custom_presets_available', 'auto_apply',
            'validate_compatibility', 'system_recommended_preset', 'preset_compatibility'
        ],
        'hyperparameter_optimization': [
            'hpo_enabled', 'hpo_strategy', 'study_name', 'direction', 'n_trials',
            'timeout', 'sampler', 'pruner', 'objective_metric', 'optimization_space',
            'hpo_early_stopping', 'timeout_seconds', 'trial_epochs', 'trial_patience',
            'cleanup_trials', 'generate_plots', 'search_space', 'hpo_sampler',
            'hpo_pruner', 'scoring', 'storage'
        ],
        'validation': [
            'cross_validation', 'metrics', 'validation_frequency', 'save_validation_results',
            'detailed_metrics', 'robustness_testing', 'performance_benchmarking',
            'confidence_intervals'
        ],
        'experimental': [
            'experimental_features', 'experimental_settings'
        ],
        'metadata': [
            'description', 'version', 'config_version', 'config_type', 'created',
            'last_modified', 'preset_used', 'recommended_hardware', 'compatibility',
            'system_info', 'validation_info'
        ],
        'runtime': [
            'config_loaded_at', 'config_source', 'runtime_id', 'process_id',
            'system_analysis_completed', 'system_performance_score', 'system_class',
            'optimizations_applied', 'resource_status', 'system_warnings',
            'recommendations', 'configuration_health'
        ]
    }
    
    structured_config = {}
    
    # Organize parameters into sections
    for section, param_list in param_sections.items():
        section_params = {}
        for param in param_list:
            if param in kwargs:
                # Handle special parameter name mappings
                config_param = param
                if param == 'data_normalization':
                    config_param = 'normalization'
                elif param.startswith('hpo_'):
                    config_param = param[4:]  # Remove 'hpo_' prefix
                
                section_params[config_param] = kwargs[param]
        
        if section_params:
            structured_config[section] = section_params
    
    return structured_config

def _initialize_autoencoder_config(
    model_class_name: str,
    input_dim: Optional[int] = None,
    config: Optional[Dict[str, Any]] = None,
    preset: Optional[str] = None,
    
    # Core Model Architecture Parameters
    encoding_dim: Optional[int] = None,
    hidden_dims: Optional[List[int]] = None,
    dropout_rates: Optional[List[float]] = None,
    activation: Optional[str] = None,
    activation_param: Optional[float] = None,
    normalization: Optional[str] = None,
    use_batch_norm: Optional[bool] = None,
    use_layer_norm: Optional[bool] = None,
    bias: Optional[bool] = None,
    weight_init: Optional[str] = None,
    skip_connection: Optional[bool] = None,
    residual_blocks: Optional[bool] = None,
    use_attention: Optional[bool] = None,
    
    # Model Type and Variants
    model_type: Optional[str] = None,
    model_types: Optional[List[str]] = None,
    available_activations: Optional[List[str]] = None,
    available_normalizations: Optional[List[str]] = None,
    available_initializers: Optional[List[str]] = None,
    legacy_mode: Optional[bool] = None,
    
    # Ensemble Parameters
    diversity_factor: Optional[float] = None,
    min_features: Optional[int] = None,
    num_models: Optional[int] = None,
    
    # Training Parameters
    batch_size: Optional[int] = None,
    epochs: Optional[int] = None,
    learning_rate: Optional[float] = None,
    patience: Optional[int] = None,
    weight_decay: Optional[float] = None,
    gradient_clip: Optional[float] = None,
    gradient_accumulation_steps: Optional[int] = None,
    mixed_precision: Optional[bool] = None,
    num_workers: Optional[int] = None,
    optimizer: Optional[str] = None,
    scheduler: Optional[str] = None,
    scheduler_params: Optional[Dict[str, Any]] = None,
    early_stopping: Optional[bool] = None,
    validation_split: Optional[float] = None,
    shuffle: Optional[bool] = None,
    pin_memory: Optional[bool] = None,
    persistent_workers: Optional[bool] = None,
    adam_betas: Optional[Tuple[float, float]] = None,
    adam_eps: Optional[float] = None,
    lr_patience: Optional[int] = None,
    lr_factor: Optional[float] = None,
    min_lr: Optional[float] = None,
    
    # Data Parameters
    normal_samples: Optional[int] = None,
    attack_samples: Optional[int] = None,
    features: Optional[int] = None,
    use_real_data: Optional[bool] = None,
    data_normalization: Optional[str] = None,
    anomaly_factor: Optional[float] = None,
    random_state: Optional[int] = None,
    test_split: Optional[float] = None,
    stratified_split: Optional[bool] = None,
    data_path: Optional[str] = None,
    artifacts_path: Optional[str] = None,
    synthetic_generation: Optional[Dict[str, Any]] = None,
    preprocessing: Optional[Dict[str, Any]] = None,
    
    # Security Parameters
    percentile: Optional[float] = None,
    attack_threshold: Optional[float] = None,
    false_negative_cost: Optional[float] = None,
    enable_security_metrics: Optional[bool] = None,
    anomaly_threshold_strategy: Optional[str] = None,
    early_warning_threshold: Optional[float] = None,
    adaptive_threshold: Optional[bool] = None,
    confidence_interval: Optional[float] = None,
    detection_methods: Optional[List[str]] = None,
    alert_levels: Optional[List[str]] = None,
    threshold_validation: Optional[bool] = None,
    robust_detection: Optional[bool] = None,
    false_positive_tolerance: Optional[float] = None,
    performance_optimized_detection: Optional[bool] = None,
    real_time_monitoring: Optional[bool] = None,
    ensemble_voting: Optional[str] = None,
    uncertainty_threshold: Optional[float] = None,
    
    # Monitoring Parameters
    metrics_frequency: Optional[int] = None,
    checkpoint_frequency: Optional[int] = None,
    tensorboard_logging: Optional[bool] = None,
    console_logging_level: Optional[str] = None,
    save_best_model: Optional[bool] = None,
    save_model_history: Optional[bool] = None,
    metrics_to_track: Optional[List[str]] = None,
    early_stopping_metric: Optional[str] = None,
    checkpoint_format: Optional[str] = None,
    log_model_summary: Optional[bool] = None,
    tensorboard_dir: Optional[str] = None,
    log_frequency: Optional[int] = None,
    save_checkpoints: Optional[bool] = None,
    tensorboard: Optional[Dict[str, Any]] = None,
    stability_metrics: Optional[bool] = None,
    performance_metrics: Optional[bool] = None,
    profiling_enabled: Optional[bool] = None,
    
    # Hardware Parameters
    device: Optional[str] = None,
    recommended_gpu_memory: Optional[float] = None,
    minimum_system_requirements: Optional[Dict[str, Any]] = None,
    optimal_system_requirements: Optional[Dict[str, Any]] = None,
    memory_management: Optional[Dict[str, Any]] = None,
    performance_optimization: Optional[Dict[str, Any]] = None,
    detected_gpu_memory: Optional[float] = None,
    detected_system_memory: Optional[float] = None,
    system_performance_class: Optional[str] = None,
    optimization_recommendations: Optional[List[str]] = None,
    
    # System Parameters
    model_dir: Optional[str] = None,
    log_dir: Optional[str] = None,
    config_dir: Optional[str] = None,
    data_dir: Optional[str] = None,
    checkpoint_dir: Optional[str] = None,
    results_dir: Optional[str] = None,
    debug: Optional[bool] = None,
    verbose: Optional[bool] = None,
    random_seed: Optional[int] = None,
    reproducible: Optional[bool] = None,
    parallel_processing: Optional[bool] = None,
    max_workers: Optional[int] = None,
    export_onnx: Optional[bool] = None,
    non_interactive: Optional[bool] = None,
    cuda_optimizations: Optional[bool] = None,
    onnx_export: Optional[Dict[str, Any]] = None,
    distributed_training: Optional[bool] = None,
    python_executable: Optional[str] = None,
    working_directory: Optional[str] = None,
    environment_health: Optional[str] = None,
    
    # Preset Parameters
    available_presets: Optional[List[str]] = None,
    current_preset: Optional[str] = None,
    current_override: Optional[str] = None,
    override_rules: Optional[Dict[str, bool]] = None,
    preset_configs: Optional[Dict[str, str]] = None,
    custom_presets_available: Optional[List[str]] = None,
    auto_apply: Optional[bool] = None,
    validate_compatibility: Optional[bool] = None,
    system_recommended_preset: Optional[str] = None,
    preset_compatibility: Optional[Dict[str, Any]] = None,
    
    # Hyperparameter Optimization Parameters
    hpo_enabled: Optional[bool] = None,
    hpo_strategy: Optional[str] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    n_trials: Optional[int] = None,
    timeout: Optional[int] = None,
    sampler: Optional[str] = None,
    pruner: Optional[str] = None,
    objective_metric: Optional[str] = None,
    optimization_space: Optional[Dict[str, Any]] = None,
    hpo_early_stopping: Optional[Dict[str, Any]] = None,
    timeout_seconds: Optional[int] = None,
    trial_epochs: Optional[int] = None,
    trial_patience: Optional[int] = None,
    cleanup_trials: Optional[bool] = None,
    generate_plots: Optional[bool] = None,
    search_space: Optional[Dict[str, Any]] = None,
    hpo_sampler: Optional[Dict[str, Any]] = None,
    hpo_pruner: Optional[Dict[str, Any]] = None,
    scoring: Optional[Dict[str, Any]] = None,
    storage: Optional[Dict[str, Any]] = None,
    
    # Validation Parameters
    cross_validation: Optional[Dict[str, Any]] = None,
    metrics: Optional[List[str]] = None,
    validation_frequency: Optional[int] = None,
    save_validation_results: Optional[bool] = None,
    detailed_metrics: Optional[bool] = None,
    robustness_testing: Optional[bool] = None,
    performance_benchmarking: Optional[bool] = None,
    confidence_intervals: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_features: Optional[Dict[str, bool]] = None,
    experimental_settings: Optional[Dict[str, bool]] = None,
    
    # Metadata Parameters
    description: Optional[str] = None,
    version: Optional[str] = None,
    config_version: Optional[str] = None,
    config_type: Optional[str] = None,
    created: Optional[str] = None,
    last_modified: Optional[str] = None,
    preset_used: Optional[str] = None,
    recommended_hardware: Optional[Dict[str, Any]] = None,
    compatibility: Optional[List[str]] = None,
    system_info: Optional[Dict[str, Any]] = None,
    validation_info: Optional[Dict[str, Any]] = None,
    
    # Runtime Parameters
    config_loaded_at: Optional[str] = None,
    config_source: Optional[str] = None,
    runtime_id: Optional[str] = None,
    process_id: Optional[int] = None,
    system_analysis_completed: Optional[bool] = None,
    system_performance_score: Optional[float] = None,
    system_class: Optional[str] = None,
    optimizations_applied: Optional[Dict[str, bool]] = None,
    resource_status: Optional[Dict[str, bool]] = None,
    system_warnings: Optional[List[str]] = None,
    recommendations: Optional[List[str]] = None,
    configuration_health: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """
    Centralized autoencoder configuration initialization helper function.
    
    This function handles all the parameter processing, validation, and configuration
    setup that was previously duplicated across SimpleAutoencoder, EnhancedAutoencoder,
    and AutoencoderEnsemble classes.
    
    Args:
        model_class_name: Name of the model class ('SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble')
        input_dim: Input dimension for the model
        config: Configuration dictionary (optional, will use current config if None)
        preset: Preset name to apply (optional)
        **all other parameters: All the parameters that were previously handled in each class
        
    Returns:
        Dictionary containing:
        - 'config': Final processed configuration
        - 'processed_params': Dictionary of extracted and validated parameters
        - 'device': PyTorch device object
        - 'mixed_precision': Boolean indicating if mixed precision is enabled
        - 'initialization_timestamp': ISO timestamp of initialization
        - 'preset_name': Name of preset used (if any)
    """
    from datetime import datetime
    
    # Store initialization metadata
    initialization_timestamp = datetime.now().isoformat()
    
    # PRESERVE PRESET NAME EARLY
    actual_preset_used = preset
    
    # Validate input_dim
    if input_dim is None:
        # Try to extract from config sources in priority order
        if config and 'model' in config and 'input_dim' in config['model']:
            input_dim = config['model']['input_dim']
            logger.debug("input_dim extracted from config['model']['input_dim']")
        elif config and 'data' in config and 'features' in config['data']:
            input_dim = config['data']['features']
            logger.debug("input_dim extracted from config['data']['features']")
        else:
            input_dim = 20
            logger.warning("input_dim not provided in config, using default: 20")
    
    # Enhanced tensor/array handling with better error messages
    original_input_dim = input_dim  # Keep track for logging
    
    # Handle different input_dim types using existing helper pattern
    if isinstance(input_dim, (torch.Tensor, np.ndarray)):
        if hasattr(input_dim, 'shape') and len(input_dim.shape) >= 2:
            # For 2D+ tensors, use the last dimension (feature dimension)
            input_dim = input_dim.shape[-1]
            logger.info(f"input_dim inferred from tensor shape: {original_input_dim.shape} -> {input_dim}")
        elif hasattr(input_dim, 'shape') and len(input_dim.shape) == 1:
            # For 1D tensors, use the length
            input_dim = len(input_dim)
            logger.info(f"input_dim inferred from 1D tensor length: {len(original_input_dim)} -> {input_dim}")
        else:
            try:
                # Try to get total elements for other tensor types
                input_dim = input_dim.numel() if hasattr(input_dim, 'numel') else len(input_dim)
                logger.info(f"input_dim inferred from tensor elements: {input_dim}")
            except Exception as e:
                logger.error(f"Could not infer input_dim from tensor: {e}")
                input_dim = 20
                logger.warning("Using fallback input_dim: 20")
    
    elif isinstance(input_dim, (list, tuple)):
        if len(input_dim) > 0:
            # For lists/tuples, check if it's data or just dimensions
            if all(isinstance(x, (int, float)) for x in input_dim):
                # Assume it's a feature vector, use length
                input_dim = len(input_dim)
                logger.info(f"input_dim inferred from feature vector length: {input_dim}")
            else:
                # Assume it's nested data, use last dimension of first element
                try:
                    if hasattr(input_dim[0], '__len__'):
                        input_dim = len(input_dim[0])
                    else:
                        input_dim = len(input_dim)
                    logger.info(f"input_dim inferred from nested structure: {input_dim}")
                except Exception:
                    input_dim = len(original_input_dim)
                    logger.warning(f"Using length of input structure: {input_dim}")
        else:
            logger.error("input_dim was empty list/tuple, using default")
            input_dim = 20
    
    elif isinstance(input_dim, (float, np.number)):
        # Convert float to int
        input_dim = int(input_dim)
        logger.debug(f"Converted input_dim from {type(original_input_dim).__name__} to int: {input_dim}")
    
    # Final validation
    if not isinstance(input_dim, int) or input_dim <= 0:
        logger.error(f"input_dim must be a positive integer, got {type(input_dim).__name__}: {input_dim}")
        raise ValueError(f"input_dim must be a positive integer, got {type(input_dim).__name__}: {input_dim}")
    
    # Log the final input_dim for debugging
    logger.debug(f"Final input_dim: {input_dim}")
    
    # Use existing configuration processing helper
    if config is None:
        config = get_current_config()
    
    # Apply preset if specified using existing helper
    if preset and preset in PRESET_CONFIGS:
        preset_config = deepcopy(PRESET_CONFIGS[preset])
        config = deep_update(preset_config, config)
        logger.debug(f"Applied preset '{preset}' to configuration")
        
        # Ensure input_dim consistency between model and data sections
        if 'model' in config:
            config['model']['input_dim'] = input_dim
        if 'data' in config:
            config['data']['features'] = input_dim
        
        # ENSURE PRESET NAME IS RECORDED IN MULTIPLE PLACES
        if 'metadata' not in config:
            config['metadata'] = {}
        config['metadata']['preset_used'] = preset
        
        if 'presets' not in config:
            config['presets'] = {}
        config['presets']['current_preset'] = preset
        
        # Also store in runtime for easy access
        if 'runtime' not in config:
            config['runtime'] = {}
        config['runtime']['applied_preset'] = preset
    
    # Ensure input_dim consistency across all relevant sections
    if 'model' not in config:
        config['model'] = {}
    config['model']['input_dim'] = input_dim
    
    if 'data' not in config:
        config['data'] = {}
    config['data']['features'] = input_dim
    
    #preset_name = preset if preset in PRESET_CONFIGS else None
    actual_preset_used = preset if preset in PRESET_CONFIGS else None
    
    
    # Collect all non-None parameters for processing
    local_params = locals().copy()
    params_to_remove = {'model_class_name', 'config', 'preset', 'kwargs', 'local_params', 'initialization_timestamp'}
    individual_params = {k: v for k, v in local_params.items() if k not in params_to_remove and v is not None}
    
    # Add kwargs
    if kwargs:
        individual_params.update(kwargs)
    
    # Structure individual parameters using existing helper
    if individual_params:
        structured_params = _structure_kwargs_into_config_sections(individual_params)
        config = deep_update(config, structured_params)
        logger.debug(f"Processed {len(individual_params)} individual parameters")
    
    # Extract and validate core parameters using existing helpers
    model_config = config.get('model', {})
    
    # Set model-specific defaults based on class
    if model_class_name == 'SimpleAutoencoder':
        default_encoding_dim = 12
        default_hidden_dims = [128, 64]
        default_dropout_rates = [0.2, 0.15]
        default_mixed_precision = False
        default_use_attention = False
        default_residual_blocks = False
        default_skip_connection = True
        default_legacy_mode = False
    elif model_class_name == 'EnhancedAutoencoder':
        default_encoding_dim = 32
        default_hidden_dims = [256, 128, 64]
        default_dropout_rates = [0.2, 0.15, 0.1]
        default_mixed_precision = True
        default_use_attention = True
        default_residual_blocks = True
        default_skip_connection = True
        default_legacy_mode = False
    elif model_class_name == 'AutoencoderEnsemble':
        default_encoding_dim = 24
        default_hidden_dims = [192, 96, 48]
        default_dropout_rates = [0.25, 0.2, 0.15]
        default_mixed_precision = True
        default_use_attention = True
        default_residual_blocks = True
        default_skip_connection = True
        default_legacy_mode = False
    else:
        # Fallback defaults
        default_encoding_dim = 16
        default_hidden_dims = [128, 64]
        default_dropout_rates = [0.2, 0.15]
        default_mixed_precision = False
        default_use_attention = False
        default_residual_blocks = False
        default_skip_connection = False
        default_legacy_mode = False
    
    # Extract and validate parameters using existing helper functions
    processed_params = {}
    
    processed_params['encoding_dim'] = _extract_and_validate_config_param(
        model_config, 'encoding_dim', default_encoding_dim, 'DEFAULT_ENCODING_DIM',
        lambda x: isinstance(x, int) and x > 0,
        "encoding dimension"
    )
    
    processed_params['hidden_dims'] = _extract_and_validate_config_param(
        model_config, 'hidden_dims', default_hidden_dims, 'HIDDEN_LAYER_SIZES',
        lambda x: isinstance(x, list) and len(x) > 0,
        "hidden dimensions"
    )
    
    processed_params['dropout_rates'] = _extract_and_validate_config_param(
        model_config, 'dropout_rates', default_dropout_rates, 'DROPOUT_RATES',
        lambda x: isinstance(x, list) and len(x) > 0,
        "dropout rates"
    )
    
    processed_params['activation'] = _extract_and_validate_config_param(
        model_config, 'activation', 'leaky_relu', 'ACTIVATION',
        lambda x: x in ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu', 'prelu'],
        "activation function"
    )
    
    processed_params['activation_param'] = _extract_and_validate_config_param(
        model_config, 'activation_param', 0.2, 'ACTIVATION_PARAM',
        lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
        "activation parameter"
    )
    
    processed_params['normalization'] = _extract_and_validate_config_param(
        model_config, 'normalization', 'batch', 'NORMALIZATION',
        lambda x: x in ['batch', 'layer', 'instance', 'group', 'none', None],
        "normalization type"
    )
    
    processed_params['use_batch_norm'] = _extract_and_validate_config_param(
        model_config, 'use_batch_norm', True, 'USE_BATCH_NORM',
        lambda x: isinstance(x, bool),
        "batch normalization flag"
    )
    
    processed_params['use_layer_norm'] = _extract_and_validate_config_param(
        model_config, 'use_layer_norm', False, 'USE_LAYER_NORM',
        lambda x: isinstance(x, bool),
        "layer normalization flag"
    )
    
    processed_params['bias'] = _extract_and_validate_config_param(
        model_config, 'bias', True, 'BIAS',
        lambda x: isinstance(x, bool),
        "bias flag"
    )
    
    processed_params['weight_init'] = _extract_and_validate_config_param(
        model_config, 'weight_init', 'xavier_uniform', 'WEIGHT_INIT',
        lambda x: x in ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'orthogonal', 'he_uniform', 'he_normal'],
        "weight initialization"
    )
    
    processed_params['skip_connection'] = _extract_and_validate_config_param(
        model_config, 'skip_connection', default_skip_connection, 'SKIP_CONNECTION',
        lambda x: isinstance(x, bool),
        "skip connection flag"
    )
    
    processed_params['residual_blocks'] = _extract_and_validate_config_param(
        model_config, 'residual_blocks', default_residual_blocks, 'RESIDUAL_BLOCKS',
        lambda x: isinstance(x, bool),
        "residual blocks flag"
    )
    
    processed_params['use_attention'] = _extract_and_validate_config_param(
        model_config, 'use_attention', default_use_attention, 'USE_ATTENTION',
        lambda x: isinstance(x, bool),
        "attention mechanism flag"
    )
    
    processed_params['model_type'] = model_config.get('model_type', model_class_name)
    processed_params['min_features'] = model_config.get('min_features', 5)
    processed_params['legacy_mode'] = _extract_and_validate_config_param(
        model_config, 'legacy_mode', default_legacy_mode, 'LEGACY_MODE',
        lambda x: isinstance(x, bool),
        "legacy compatibility mode"
    )
    
    # Ensemble-specific parameters
    if model_class_name == 'AutoencoderEnsemble':
        processed_params['num_models'] = _extract_and_validate_config_param(
            model_config, 'num_models', 3, 'NUM_MODELS',
            lambda x: isinstance(x, int) and 1 <= x <= 10,
            "ensemble size"
        )
        
        processed_params['diversity_factor'] = _extract_and_validate_config_param(
            model_config, 'diversity_factor', 0.3, 'DIVERSITY_FACTOR',
            lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
            "ensemble diversity factor"
        )
    else:
        processed_params['num_models'] = 1
        processed_params['diversity_factor'] = 0.0
    
    # Validate and adjust parameters using existing helper
    processed_params['hidden_dims'], processed_params['dropout_rates'] = _validate_and_adjust_parameters(
        processed_params['hidden_dims'], processed_params['dropout_rates']
    )
    
    # Model-specific adjustments
    if model_class_name == 'SimpleAutoencoder':
        # SimpleAutoencoder-specific adjustments
        if len(processed_params['hidden_dims']) > 1:
            processed_params['hidden_dims'] = [processed_params['hidden_dims'][0]]
            processed_params['dropout_rates'] = [processed_params['dropout_rates'][0]]
            logger.debug("Simplified architecture for SimpleAutoencoder")
        
        # Disable advanced features for SimpleAutoencoder
        processed_params['use_attention'] = False
        processed_params['residual_blocks'] = False
    
    elif model_class_name == 'EnhancedAutoencoder':
        # EnhancedAutoencoder-specific validations
        if input_dim < processed_params['min_features']:
            raise ValueError(f"Input dimension ({input_dim}) must be at least {processed_params['min_features']}")
            
        # Legacy mode adjustments
        if processed_params['legacy_mode']:
            processed_params['use_attention'] = False
            processed_params['residual_blocks'] = False
            logger.debug("Simplified architecture for legacy mode")
    
    elif model_class_name == 'AutoencoderEnsemble':
        # Ensemble-specific validations
        if input_dim < processed_params['min_features']:
            raise ValueError(f"Input dimension ({input_dim}) must be at least {processed_params['min_features']}")
        
        if not isinstance(processed_params['num_models'], int) or processed_params['num_models'] < 1:
            raise ValueError(f"num_models must be a positive integer, got {processed_params['num_models']}")
        
        if not isinstance(processed_params['diversity_factor'], (int, float)) or not 0 <= processed_params['diversity_factor'] <= 1:
            raise ValueError(f"diversity_factor must be between 0 and 1, got {processed_params['diversity_factor']}")
        
        # Legacy mode adjustments
        if processed_params['legacy_mode']:
            processed_params['use_attention'] = False
            processed_params['residual_blocks'] = False
            if len(processed_params['hidden_dims']) > 2:
                processed_params['hidden_dims'] = processed_params['hidden_dims'][:2]
                processed_params['dropout_rates'] = processed_params['dropout_rates'][:2]
            if processed_params['num_models'] > 3:
                processed_params['num_models'] = 3
            logger.debug("Simplified ensemble architecture for legacy mode")
    
    # Fix normalization consistency
    if processed_params['normalization'] == 'batch':
        processed_params['use_batch_norm'] = True
        processed_params['use_layer_norm'] = False
    elif processed_params['normalization'] == 'layer':
        processed_params['use_batch_norm'] = False
        processed_params['use_layer_norm'] = True
    elif processed_params['normalization'] in [None, 'none']:
        processed_params['normalization'] = None
        processed_params['use_batch_norm'] = False
        processed_params['use_layer_norm'] = False
    elif processed_params['normalization'] in ['instance', 'group'] and processed_params['legacy_mode']:
        logger.debug(f"Normalization '{processed_params['normalization']}' not supported in legacy mode, disabling")
        processed_params['normalization'] = None
        processed_params['use_batch_norm'] = False
        processed_params['use_layer_norm'] = False
    
    # Setup device using existing system context helper
    system_context = _get_system_context(silent=True)
    hardware_data = system_context.get('hardware_data', {})
    
    # Determine device
    device_setting = config.get('hardware', {}).get('device', 'auto')
    if device_setting == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    elif device_setting == 'cpu':
        device = torch.device('cpu')
    elif device_setting == 'cuda':
        if torch.cuda.is_available():
            device = torch.device('cuda')
        else:
            logger.warning("CUDA requested but not available, falling back to CPU")
            device = torch.device('cpu')
    else:
        try:
            device = torch.device(device_setting)
        except Exception as e:
            logger.error(f"Invalid device '{device_setting}': {e}, falling back to auto")
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Mixed precision settings
    training_config = config.get('training', {})
    mixed_precision_requested = training_config.get('mixed_precision', default_mixed_precision)
    mixed_precision = mixed_precision_requested and torch.cuda.is_available()
    
    if mixed_precision_requested and not torch.cuda.is_available():
        logger.warning("Mixed precision requested but CUDA not available")
    
    # Setup reproducibility
    system_config = config.get('system', {})
    random_seed = system_config.get('random_seed', 42)
    reproducible = system_config.get('reproducible', True)
    
    if reproducible:
        torch.manual_seed(random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(random_seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
    
    # Store processed input_dim
    processed_params['input_dim'] = input_dim
    
    # Ensure input_dim is in the configuration
    if 'model' not in config:
        config['model'] = {}
    config['model']['input_dim'] = input_dim
    
    # Return comprehensive initialization data
    return {
        'config': config,
        'processed_params': processed_params,
        'device': device,
        'mixed_precision': mixed_precision,
        'mixed_precision_requested': mixed_precision_requested,
        'initialization_timestamp': initialization_timestamp,
        #'preset_name': preset,
        'preset_name': actual_preset_used,
        #'preset_name': preset_name,
        'training_config': training_config,
        'hardware_data': hardware_data,
        'system_context': system_context
    }

class SimpleAutoencoder(nn.Module):
    """
    Streamlined SimpleAutoencoder using centralized configuration initialization.
    
    This class leverages the _initialize_autoencoder_config() helper function to eliminate
    redundancy and provide consistent parameter processing across all autoencoder variants.
    """
    
    def __init__(
        self,
        # Core parameter - always required
        input_dim: Optional[int] = None,
        
        # Configuration override - can contain any/all preset parameters
        config: Optional[Dict[str, Any]] = None,
        preset: Optional[str] = None,
        
        # ALL preset parameters accepted for backward compatibility
        # Core Model Architecture Parameters
        encoding_dim: Optional[int] = None,
        hidden_dims: Optional[List[int]] = None,
        dropout_rates: Optional[List[float]] = None,
        activation: Optional[str] = None,
        activation_param: Optional[float] = None,
        normalization: Optional[str] = None,
        use_batch_norm: Optional[bool] = None,
        use_layer_norm: Optional[bool] = None,
        bias: Optional[bool] = None,
        weight_init: Optional[str] = None,
        skip_connection: Optional[bool] = None,
        residual_blocks: Optional[bool] = None,
        use_attention: Optional[bool] = None,
        
        # Model Type and Variants
        model_type: Optional[str] = None,
        model_types: Optional[List[str]] = None,
        available_activations: Optional[List[str]] = None,
        available_normalizations: Optional[List[str]] = None,
        available_initializers: Optional[List[str]] = None,
        legacy_mode: Optional[bool] = None,
        
        # Ensemble Parameters (accepted but ignored for SimpleAutoencoder)
        diversity_factor: Optional[float] = None,
        min_features: Optional[int] = None,
        num_models: Optional[int] = None,
        
        # Training Parameters
        batch_size: Optional[int] = None,
        epochs: Optional[int] = None,
        learning_rate: Optional[float] = None,
        patience: Optional[int] = None,
        weight_decay: Optional[float] = None,
        gradient_clip: Optional[float] = None,
        gradient_accumulation_steps: Optional[int] = None,
        mixed_precision: Optional[bool] = None,
        num_workers: Optional[int] = None,
        optimizer: Optional[str] = None,
        scheduler: Optional[str] = None,
        scheduler_params: Optional[Dict[str, Any]] = None,
        early_stopping: Optional[bool] = None,
        validation_split: Optional[float] = None,
        shuffle: Optional[bool] = None,
        pin_memory: Optional[bool] = None,
        persistent_workers: Optional[bool] = None,
        adam_betas: Optional[Tuple[float, float]] = None,
        adam_eps: Optional[float] = None,
        lr_patience: Optional[int] = None,
        lr_factor: Optional[float] = None,
        min_lr: Optional[float] = None,
        
        # Data Parameters
        normal_samples: Optional[int] = None,
        attack_samples: Optional[int] = None,
        features: Optional[int] = None,
        use_real_data: Optional[bool] = None,
        data_normalization: Optional[str] = None,
        anomaly_factor: Optional[float] = None,
        random_state: Optional[int] = None,
        test_split: Optional[float] = None,
        stratified_split: Optional[bool] = None,
        data_path: Optional[str] = None,
        artifacts_path: Optional[str] = None,
        synthetic_generation: Optional[Dict[str, Any]] = None,
        preprocessing: Optional[Dict[str, Any]] = None,
        
        # Security Parameters
        percentile: Optional[float] = None,
        attack_threshold: Optional[float] = None,
        false_negative_cost: Optional[float] = None,
        enable_security_metrics: Optional[bool] = None,
        anomaly_threshold_strategy: Optional[str] = None,
        early_warning_threshold: Optional[float] = None,
        adaptive_threshold: Optional[bool] = None,
        confidence_interval: Optional[float] = None,
        detection_methods: Optional[List[str]] = None,
        alert_levels: Optional[List[str]] = None,
        threshold_validation: Optional[bool] = None,
        robust_detection: Optional[bool] = None,
        false_positive_tolerance: Optional[float] = None,
        performance_optimized_detection: Optional[bool] = None,
        real_time_monitoring: Optional[bool] = None,
        ensemble_voting: Optional[str] = None,
        uncertainty_threshold: Optional[float] = None,
        
        # Monitoring Parameters
        metrics_frequency: Optional[int] = None,
        checkpoint_frequency: Optional[int] = None,
        tensorboard_logging: Optional[bool] = None,
        console_logging_level: Optional[str] = None,
        save_best_model: Optional[bool] = None,
        save_model_history: Optional[bool] = None,
        metrics_to_track: Optional[List[str]] = None,
        early_stopping_metric: Optional[str] = None,
        checkpoint_format: Optional[str] = None,
        log_model_summary: Optional[bool] = None,
        tensorboard_dir: Optional[str] = None,
        log_frequency: Optional[int] = None,
        save_checkpoints: Optional[bool] = None,
        tensorboard: Optional[Dict[str, Any]] = None,
        stability_metrics: Optional[bool] = None,
        performance_metrics: Optional[bool] = None,
        profiling_enabled: Optional[bool] = None,
        
        # Hardware Parameters
        device: Optional[str] = None,
        recommended_gpu_memory: Optional[float] = None,
        minimum_system_requirements: Optional[Dict[str, Any]] = None,
        optimal_system_requirements: Optional[Dict[str, Any]] = None,
        memory_management: Optional[Dict[str, Any]] = None,
        performance_optimization: Optional[Dict[str, Any]] = None,
        detected_gpu_memory: Optional[float] = None,
        detected_system_memory: Optional[float] = None,
        system_performance_class: Optional[str] = None,
        optimization_recommendations: Optional[List[str]] = None,
        
        # System Parameters
        model_dir: Optional[str] = None,
        log_dir: Optional[str] = None,
        config_dir: Optional[str] = None,
        data_dir: Optional[str] = None,
        checkpoint_dir: Optional[str] = None,
        results_dir: Optional[str] = None,
        debug: Optional[bool] = None,
        verbose: Optional[bool] = None,
        random_seed: Optional[int] = None,
        reproducible: Optional[bool] = None,
        parallel_processing: Optional[bool] = None,
        max_workers: Optional[int] = None,
        export_onnx: Optional[bool] = None,
        non_interactive: Optional[bool] = None,
        cuda_optimizations: Optional[bool] = None,
        onnx_export: Optional[Dict[str, Any]] = None,
        distributed_training: Optional[bool] = None,
        python_executable: Optional[str] = None,
        working_directory: Optional[str] = None,
        environment_health: Optional[str] = None,
        
        # Preset Parameters
        available_presets: Optional[List[str]] = None,
        current_preset: Optional[str] = None,
        current_override: Optional[str] = None,
        override_rules: Optional[Dict[str, bool]] = None,
        preset_configs: Optional[Dict[str, str]] = None,
        custom_presets_available: Optional[List[str]] = None,
        auto_apply: Optional[bool] = None,
        validate_compatibility: Optional[bool] = None,
        system_recommended_preset: Optional[str] = None,
        preset_compatibility: Optional[Dict[str, Any]] = None,
        
        # Hyperparameter Optimization Parameters
        hpo_enabled: Optional[bool] = None,
        hpo_strategy: Optional[str] = None,
        study_name: Optional[str] = None,
        direction: Optional[str] = None,
        n_trials: Optional[int] = None,
        timeout: Optional[int] = None,
        sampler: Optional[str] = None,
        pruner: Optional[str] = None,
        objective_metric: Optional[str] = None,
        optimization_space: Optional[Dict[str, Any]] = None,
        hpo_early_stopping: Optional[Dict[str, Any]] = None,
        timeout_seconds: Optional[int] = None,
        trial_epochs: Optional[int] = None,
        trial_patience: Optional[int] = None,
        cleanup_trials: Optional[bool] = None,
        generate_plots: Optional[bool] = None,
        search_space: Optional[Dict[str, Any]] = None,
        hpo_sampler: Optional[Dict[str, Any]] = None,
        hpo_pruner: Optional[Dict[str, Any]] = None,
        scoring: Optional[Dict[str, Any]] = None,
        storage: Optional[Dict[str, Any]] = None,
        
        # Validation Parameters
        cross_validation: Optional[Dict[str, Any]] = None,
        metrics: Optional[List[str]] = None,
        validation_frequency: Optional[int] = None,
        save_validation_results: Optional[bool] = None,
        detailed_metrics: Optional[bool] = None,
        robustness_testing: Optional[bool] = None,
        performance_benchmarking: Optional[bool] = None,
        confidence_intervals: Optional[bool] = None,
        
        # Experimental Parameters
        experimental_features: Optional[Dict[str, bool]] = None,
        experimental_settings: Optional[Dict[str, bool]] = None,
        
        # Metadata Parameters
        description: Optional[str] = None,
        version: Optional[str] = None,
        config_version: Optional[str] = None,
        config_type: Optional[str] = None,
        created: Optional[str] = None,
        last_modified: Optional[str] = None,
        preset_used: Optional[str] = None,
        recommended_hardware: Optional[Dict[str, Any]] = None,
        compatibility: Optional[List[str]] = None,
        system_info: Optional[Dict[str, Any]] = None,
        validation_info: Optional[Dict[str, Any]] = None,
        
        # Runtime Parameters
        config_loaded_at: Optional[str] = None,
        config_source: Optional[str] = None,
        runtime_id: Optional[str] = None,
        process_id: Optional[int] = None,
        system_analysis_completed: Optional[bool] = None,
        system_performance_score: Optional[float] = None,
        system_class: Optional[str] = None,
        optimizations_applied: Optional[Dict[str, bool]] = None,
        resource_status: Optional[Dict[str, bool]] = None,
        system_warnings: Optional[List[str]] = None,
        recommendations: Optional[List[str]] = None,
        configuration_health: Optional[Dict[str, Any]] = None,
        
        **kwargs  # Catch any additional parameters
    ):
        super(SimpleAutoencoder, self).__init__()
        
        # Use centralized initialization helper
        init_data = _initialize_autoencoder_config(
            model_class_name='SimpleAutoencoder',
            input_dim=input_dim,
            config=config,
            preset=preset,
            # Pass through all parameters
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            bias=bias,
            weight_init=weight_init,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention,
            model_type=model_type,
            model_types=model_types,
            available_activations=available_activations,
            available_normalizations=available_normalizations,
            available_initializers=available_initializers,
            legacy_mode=legacy_mode,
            diversity_factor=diversity_factor,
            min_features=min_features,
            num_models=num_models,
            batch_size=batch_size,
            epochs=epochs,
            learning_rate=learning_rate,
            patience=patience,
            weight_decay=weight_decay,
            gradient_clip=gradient_clip,
            gradient_accumulation_steps=gradient_accumulation_steps,
            mixed_precision=mixed_precision,
            num_workers=num_workers,
            optimizer=optimizer,
            scheduler=scheduler,
            scheduler_params=scheduler_params,
            early_stopping=early_stopping,
            validation_split=validation_split,
            shuffle=shuffle,
            pin_memory=pin_memory,
            persistent_workers=persistent_workers,
            adam_betas=adam_betas,
            adam_eps=adam_eps,
            lr_patience=lr_patience,
            lr_factor=lr_factor,
            min_lr=min_lr,
            normal_samples=normal_samples,
            attack_samples=attack_samples,
            features=features,
            use_real_data=use_real_data,
            data_normalization=data_normalization,
            anomaly_factor=anomaly_factor,
            random_state=random_state,
            test_split=test_split,
            stratified_split=stratified_split,
            data_path=data_path,
            artifacts_path=artifacts_path,
            synthetic_generation=synthetic_generation,
            preprocessing=preprocessing,
            percentile=percentile,
            attack_threshold=attack_threshold,
            false_negative_cost=false_negative_cost,
            enable_security_metrics=enable_security_metrics,
            anomaly_threshold_strategy=anomaly_threshold_strategy,
            early_warning_threshold=early_warning_threshold,
            adaptive_threshold=adaptive_threshold,
            confidence_interval=confidence_interval,
            detection_methods=detection_methods,
            alert_levels=alert_levels,
            threshold_validation=threshold_validation,
            robust_detection=robust_detection,
            false_positive_tolerance=false_positive_tolerance,
            performance_optimized_detection=performance_optimized_detection,
            real_time_monitoring=real_time_monitoring,
            ensemble_voting=ensemble_voting,
            uncertainty_threshold=uncertainty_threshold,
            metrics_frequency=metrics_frequency,
            checkpoint_frequency=checkpoint_frequency,
            tensorboard_logging=tensorboard_logging,
            console_logging_level=console_logging_level,
            save_best_model=save_best_model,
            save_model_history=save_model_history,
            metrics_to_track=metrics_to_track,
            early_stopping_metric=early_stopping_metric,
            checkpoint_format=checkpoint_format,
            log_model_summary=log_model_summary,
            tensorboard_dir=tensorboard_dir,
            log_frequency=log_frequency,
            save_checkpoints=save_checkpoints,
            tensorboard=tensorboard,
            stability_metrics=stability_metrics,
            performance_metrics=performance_metrics,
            profiling_enabled=profiling_enabled,
            device=device,
            recommended_gpu_memory=recommended_gpu_memory,
            minimum_system_requirements=minimum_system_requirements,
            optimal_system_requirements=optimal_system_requirements,
            memory_management=memory_management,
            performance_optimization=performance_optimization,
            detected_gpu_memory=detected_gpu_memory,
            detected_system_memory=detected_system_memory,
            system_performance_class=system_performance_class,
            optimization_recommendations=optimization_recommendations,
            model_dir=model_dir,
            log_dir=log_dir,
            config_dir=config_dir,
            data_dir=data_dir,
            checkpoint_dir=checkpoint_dir,
            results_dir=results_dir,
            debug=debug,
            verbose=verbose,
            random_seed=random_seed,
            reproducible=reproducible,
            parallel_processing=parallel_processing,
            max_workers=max_workers,
            export_onnx=export_onnx,
            non_interactive=non_interactive,
            cuda_optimizations=cuda_optimizations,
            onnx_export=onnx_export,
            distributed_training=distributed_training,
            python_executable=python_executable,
            working_directory=working_directory,
            environment_health=environment_health,
            available_presets=available_presets,
            current_preset=current_preset,
            current_override=current_override,
            override_rules=override_rules,
            preset_configs=preset_configs,
            custom_presets_available=custom_presets_available,
            auto_apply=auto_apply,
            validate_compatibility=validate_compatibility,
            system_recommended_preset=system_recommended_preset,
            preset_compatibility=preset_compatibility,
            hpo_enabled=hpo_enabled,
            hpo_strategy=hpo_strategy,
            study_name=study_name,
            direction=direction,
            n_trials=n_trials,
            timeout=timeout,
            sampler=sampler,
            pruner=pruner,
            objective_metric=objective_metric,
            optimization_space=optimization_space,
            hpo_early_stopping=hpo_early_stopping,
            timeout_seconds=timeout_seconds,
            trial_epochs=trial_epochs,
            trial_patience=trial_patience,
            cleanup_trials=cleanup_trials,
            generate_plots=generate_plots,
            search_space=search_space,
            hpo_sampler=hpo_sampler,
            hpo_pruner=hpo_pruner,
            scoring=scoring,
            storage=storage,
            cross_validation=cross_validation,
            metrics=metrics,
            validation_frequency=validation_frequency,
            save_validation_results=save_validation_results,
            detailed_metrics=detailed_metrics,
            robustness_testing=robustness_testing,
            performance_benchmarking=performance_benchmarking,
            confidence_intervals=confidence_intervals,
            experimental_features=experimental_features,
            experimental_settings=experimental_settings,
            description=description,
            version=version,
            config_version=config_version,
            config_type=config_type,
            created=created,
            last_modified=last_modified,
            preset_used=preset_used,
            recommended_hardware=recommended_hardware,
            compatibility=compatibility,
            system_info=system_info,
            validation_info=validation_info,
            config_loaded_at=config_loaded_at,
            config_source=config_source,
            runtime_id=runtime_id,
            process_id=process_id,
            system_analysis_completed=system_analysis_completed,
            system_performance_score=system_performance_score,
            system_class=system_class,
            optimizations_applied=optimizations_applied,
            resource_status=resource_status,
            system_warnings=system_warnings,
            recommendations=recommendations,
            configuration_health=configuration_health,
            **kwargs
        )
        
        # Extract processed data from centralized initialization
        self.config = init_data['config']
        params = init_data['processed_params']
        self.device = init_data['device']
        self.mixed_precision = init_data['mixed_precision']
        self.initialization_timestamp = init_data['initialization_timestamp']
        #self.preset_name = init_data['preset_name']
        # PROPERLY STORE PRESET NAME WITH FALLBACKS
        self.preset_name = init_data.get('preset_name') or preset
        #self.preset_name = init_data['preset_name'] or preset
        
        # If still no preset name, try to extract from config
        if not self.preset_name:
            self.preset_name = (
                self.config.get('metadata', {}).get('preset_used') or
                self.config.get('presets', {}).get('current_preset') or
                self.config.get('runtime', {}).get('applied_preset') or
                "Custom"
            )
        
        # ENSURE PRESET NAME IS STORED IN CONFIG FOR CONSISTENCY
        if self.preset_name and self.preset_name != "Custom":
            if 'metadata' not in self.config:
                self.config['metadata'] = {}
            self.config['metadata']['preset_used'] = self.preset_name
            
            if 'presets' not in self.config:
                self.config['presets'] = {}
            self.config['presets']['current_preset'] = self.preset_name
        
        training_config = init_data['training_config']
        
        # Set instance attributes from processed parameters
        self.input_dim = params['input_dim']
        self.encoding_dim = params['encoding_dim']
        self.hidden_dims = params['hidden_dims']
        self.dropout_rates = params['dropout_rates']
        self.activation = params['activation']
        self.activation_param = params['activation_param']
        self.normalization = params['normalization']
        self.use_batch_norm = params['use_batch_norm']
        self.use_layer_norm = params['use_layer_norm']
        self.bias = params['bias']
        self.weight_init = params['weight_init']
        self.skip_connection = params['skip_connection']
        self.residual_blocks = params['residual_blocks']
        self.use_attention = params['use_attention']
        self.model_type = params['model_type']
        self.min_features = params['min_features']
        
        # Build network architecture
        self._build_network()
        
        # Initialize weights
        self._initialize_weights()
        
        # Move to device
        self.to(self.device)
        
        # Log successful initialization
        if hasattr(globals().get('display_model_initialization_summary'), '__call__'):
            try:
                architecture_info = {
                    'hidden_dims': self.hidden_dims,
                    'encoding_dim': self.encoding_dim,
                    'dropout_rates': self.dropout_rates,
                    'activation': self.activation,
                    'normalization': self.normalization
                }
                
                display_model_initialization_summary(
                    model_instance=self,
                    model_type='SimpleAutoencoder',
                    input_dim=self.input_dim,
                    architecture_info=architecture_info,
                    device=self.device,
                    mixed_precision=self.mixed_precision,
                    preset_name=self.preset_name,
                    training_config=training_config
                )
            except Exception as e:
                logger.debug(f"Could not display initialization summary: {e}")
    
    def _build_network(self) -> None:
        """Build the autoencoder network architecture."""
        try:
            # Encoder layers
            encoder_layers = []
            dims = [self.input_dim] + self.hidden_dims + [self.encoding_dim]
            
            for i in range(len(dims) - 1):
                # Linear layer
                encoder_layers.append(nn.Linear(dims[i], dims[i + 1], bias=self.bias))
                
                # Normalization (except for last layer)
                if i < len(dims) - 2:
                    norm_layer = self._get_normalization_layer(dims[i + 1])
                    if norm_layer is not None:
                        encoder_layers.append(norm_layer)
                    
                    # Activation
                    encoder_layers.append(self._get_activation_function())
                    
                    # Dropout
                    if i < len(self.dropout_rates):
                        dropout_rate = self.dropout_rates[i]
                        if dropout_rate > 0:
                            encoder_layers.append(nn.Dropout(dropout_rate))
            
            self.encoder = nn.Sequential(*encoder_layers)
            
            # Decoder layers (reverse of encoder)
            decoder_layers = []
            dims = [self.encoding_dim] + list(reversed(self.hidden_dims)) + [self.input_dim]
            
            for i in range(len(dims) - 1):
                # Linear layer
                decoder_layers.append(nn.Linear(dims[i], dims[i + 1], bias=self.bias))
                
                # Normalization (except for last layer)
                if i < len(dims) - 2:
                    norm_layer = self._get_normalization_layer(dims[i + 1])
                    if norm_layer is not None:
                        decoder_layers.append(norm_layer)
                    
                    # Activation
                    decoder_layers.append(self._get_activation_function())
                    
                    # Dropout (reversed order)
                    if i < len(self.dropout_rates):
                        dropout_idx = len(self.dropout_rates) - 1 - i
                        dropout_rate = self.dropout_rates[dropout_idx]
                        if dropout_rate > 0:
                            decoder_layers.append(nn.Dropout(dropout_rate))
            
            # Final activation for decoder output
            decoder_layers.append(nn.Sigmoid())
            
            self.decoder = nn.Sequential(*decoder_layers)
            
            logger.debug(f"Built SimpleAutoencoder: {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}")
            
        except Exception as e:
            logger.error(f"Failed to build SimpleAutoencoder network: {e}")
            raise RuntimeError(f"Network construction failed: {e}")
    
    def _get_activation_function(self) -> nn.Module:
        """Get activation function based on configuration."""
        activation_map = {
            'relu': nn.ReLU(inplace=True),
            'leaky_relu': nn.LeakyReLU(negative_slope=self.activation_param, inplace=True),
            'gelu': nn.GELU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid(),
            'swish': nn.SiLU(),
            'elu': nn.ELU(alpha=self.activation_param, inplace=True)
        }
        
        if self.activation not in activation_map:
            logger.warning(f"Unknown activation '{self.activation}', using ReLU")
            return nn.ReLU(inplace=True)
        
        return activation_map[self.activation]
    
    def _get_normalization_layer(self, num_features: int) -> Optional[nn.Module]:
        """Get normalization layer based on configuration."""
        if self.normalization is None or self.normalization == 'none':
            return None
        
        if self.normalization == 'batch' and self.use_batch_norm:
            return nn.BatchNorm1d(num_features)
        elif self.normalization == 'layer' and self.use_layer_norm:
            return nn.LayerNorm(num_features)
        elif self.normalization == 'instance':
            return nn.InstanceNorm1d(num_features)
        else:
            return None
    
    def _initialize_weights(self) -> None:
        """Initialize network weights based on configuration."""
        try:
            init_method = self.weight_init
            
            for module in self.modules():
                if isinstance(module, nn.Linear):
                    if init_method == 'xavier_uniform':
                        nn.init.xavier_uniform_(module.weight)
                    elif init_method == 'xavier_normal':
                        nn.init.xavier_normal_(module.weight)
                    elif init_method == 'kaiming_uniform':
                        nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')
                    elif init_method == 'kaiming_normal':
                        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')
                    elif init_method == 'orthogonal':
                        nn.init.orthogonal_(module.weight)
                    else:
                        logger.warning(f"Unknown weight init '{init_method}', using xavier_uniform")
                        nn.init.xavier_uniform_(module.weight)
                    
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0)
                
                elif isinstance(module, (nn.BatchNorm1d, nn.LayerNorm)):
                    if hasattr(module, 'weight') and module.weight is not None:
                        nn.init.constant_(module.weight, 1)
                    if hasattr(module, 'bias') and module.bias is not None:
                        nn.init.constant_(module.bias, 0)
            
            logger.debug(f"Initialized weights using {init_method}")
            
        except Exception as e:
            logger.error(f"Weight initialization failed: {e}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the autoencoder."""
        # Input validation
        if not isinstance(x, torch.Tensor):
            raise ValueError(f"Expected torch.Tensor input, got {type(x)}")
        
        if x.dim() != 2:
            raise ValueError(f"Expected 2D input (batch_size, features), got {x.dim()}D")
        
        if x.size(-1) != self.input_dim:
            raise ValueError(f"Input size {x.size(-1)} doesn't match expected {self.input_dim}")
        
        try:
            # Mixed precision forward pass
            #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
            device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
            with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
                # Encode
                encoded = self.encoder(x)
                
                # Decode
                decoded = self.decoder(encoded)
                
                return decoded
                
        except Exception as e:
            logger.error(f"Forward pass failed: {e}")
            raise RuntimeError(f"Forward pass error: {e}")
    
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode input data to latent representation."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            return self.encoder(x)
    
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode latent representation to original space."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            return self.decoder(z)
    
    def get_config(self) -> Dict[str, Any]:
        """Get comprehensive model configuration."""
        config = self.config.copy()
        
        # Add runtime information
        config.setdefault('runtime', {}).update({
            'model_initialized_at': self.initialization_timestamp,
            'total_parameters': sum(p.numel() for p in self.parameters()),
            'trainable_parameters': sum(p.numel() for p in self.parameters() if p.requires_grad),
            'device': str(self.device),
            'mixed_precision_active': self.mixed_precision,
            'preset_used': self.preset_name,
            'input_dim': self.input_dim,
            'architecture_summary': f"{self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            'centralized_config_used': True,
            'helper_functions_leveraged': True
        })
        
        return config
    
    def update_config(self, new_config: Dict[str, Any], reinitialize: bool = False) -> None:
        """Update configuration and optionally reinitialize components."""
        old_config = self.config.copy()
        
        try:
            # Update configuration using existing helper
            self.config = deep_update(self.config, new_config)
            
            # Reinitialize components if requested
            if reinitialize:
                logger.debug("Reinitializing model components due to config update")
                
                # Check if architecture changed
                architecture_changed = any(
                    key in new_config.get('model', {}) 
                    for key in ['encoding_dim', 'hidden_dims', 'dropout_rates']
                )
                
                if architecture_changed:
                    logger.warning("Architecture parameters changed - full model rebuild required")
                    raise ValueError("Architecture changes require creating a new model instance")
                
                # Update device configuration if changed
                if 'hardware' in new_config:
                    old_device = self.device
                    device_setting = new_config['hardware'].get('device')
                    if device_setting:
                        if device_setting == 'auto':
                            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                        elif device_setting in ['cpu', 'cuda']:
                            self.device = torch.device(device_setting)
                        
                        if self.device != old_device:
                            self.to(self.device)
                            logger.debug(f"Device changed from {old_device} to {self.device}")
            
            logger.debug("Configuration updated successfully using centralized helper")
            
        except Exception as e:
            # Restore old configuration on error
            self.config = old_config
            logger.error(f"Configuration update failed, restored previous config: {e}")
            raise
    
    def save_model(self, path: str, include_config: bool = True) -> None:
        """Save model state and configuration."""
        save_dict = {
            'model_state_dict': self.state_dict(),
            'model_class': self.__class__.__name__,
            'input_dim': self.input_dim,
            'architecture': {
                'encoding_dim': self.encoding_dim,
                'hidden_dims': self.hidden_dims,
                'dropout_rates': self.dropout_rates
            },
            'centralized_config_used': True
        }
        
        if include_config:
            save_dict['config'] = self.config
        
        torch.save(save_dict, path)
        logger.debug(f"SimpleAutoencoder saved to {path} (centralized config)")
    
    @classmethod
    def load_model(cls, path: str, **kwargs):
        """Load model from saved state."""
        checkpoint = torch.load(path, map_location='cpu')
        
        # Extract architecture parameters
        input_dim = checkpoint['input_dim']
        config = checkpoint.get('config', {})
        
        # Create model instance using centralized config if available
        if checkpoint.get('centralized_config_used', False):
            try:
                model = create_model_instance('SimpleAutoencoder', input_dim, config, **kwargs)
                #model = cls(input_dim=input_dim, config=config, **kwargs)
                #model = create_model_instance(model_type='SimpleAutoencoder', input_dim=input_dim, config=config, **kwargs)
            except:
                # Fallback to direct instantiation
                model = cls(input_dim=input_dim, config=config, **kwargs)
                #model = create_model_instance(model_type='SimpleAutoencoder', input_dim=input_dim, config=config, **kwargs)
        else:
            model = cls(input_dim=input_dim, config=config, **kwargs)
        
        # Load state dict
        model.load_state_dict(checkpoint['model_state_dict'])
        
        logger.debug(f"SimpleAutoencoder loaded from {path}")
        return model
    
    def get_model_summary(self) -> str:
        """Get detailed model summary."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        summary = [
            f"SimpleAutoencoder Summary (Factory Pattern)",
            f"{'-'*40}",
            f"Model Type: {self.model_type}",
            f"Preset: {self.preset_name or 'Custom'}",
            f"Architecture: {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            f"Activation: {self.activation}",
            f"Normalization: {self.normalization}",
            f"Device: {self.device}",
            f"Mixed Precision: {self.mixed_precision}",
            f"Parameters: {total_params:,} total, {trainable_params:,} trainable",
            f"Memory Usage: ~{total_params * 4 / (1024**2):.1f} MB (FP32)",
            f"Factory Pattern: Enabled",
            f"Centralized Config: Enabled",
            f"Helper Functions: Leveraged",
            f"{'-'*40}"
        ]
        
        return '\n'.join(summary)
    
    def __repr__(self) -> str:
        """String representation of the model."""
        return (f"SimpleAutoencoder(input_dim={self.input_dim}, "
                f"encoding_dim={self.encoding_dim}, "
                f"hidden_dims={self.hidden_dims}, "
                f"device={self.device}, "
                f"preset='{self.preset_name or 'Custom'}', "
                f"factory_pattern=True, "
                f"centralized_config=True)")

class EnhancedAutoencoder(nn.Module):
    """
    Streamlined EnhancedAutoencoder using centralized configuration initialization.
    
    This class leverages the _initialize_autoencoder_config() helper function to eliminate
    redundancy and provide consistent parameter processing across all autoencoder variants.
    Enhanced features include attention mechanisms, residual blocks, skip connections, and
    advanced architectures while maintaining full compatibility with all preset parameters.
    """
    
    def __init__(
        self,
        # Core parameter - always required
        input_dim: Optional[int] = None,
        
        # Configuration override - can contain any/all preset parameters
        config: Optional[Dict[str, Any]] = None,
        preset: Optional[str] = None,
        
        # ALL preset parameters accepted for backward compatibility
        # Core Model Architecture Parameters
        encoding_dim: Optional[int] = None,
        hidden_dims: Optional[List[int]] = None,
        dropout_rates: Optional[List[float]] = None,
        activation: Optional[str] = None,
        activation_param: Optional[float] = None,
        normalization: Optional[str] = None,
        use_batch_norm: Optional[bool] = None,
        use_layer_norm: Optional[bool] = None,
        bias: Optional[bool] = None,
        weight_init: Optional[str] = None,
        skip_connection: Optional[bool] = None,
        residual_blocks: Optional[bool] = None,
        use_attention: Optional[bool] = None,
        
        # Model Type and Variants
        model_type: Optional[str] = None,
        model_types: Optional[List[str]] = None,
        available_activations: Optional[List[str]] = None,
        available_normalizations: Optional[List[str]] = None,
        available_initializers: Optional[List[str]] = None,
        legacy_mode: Optional[bool] = None,
        
        # Ensemble Parameters (accepted for compatibility)
        diversity_factor: Optional[float] = None,
        min_features: Optional[int] = None,
        num_models: Optional[int] = None,
        
        # Training Parameters
        batch_size: Optional[int] = None,
        epochs: Optional[int] = None,
        learning_rate: Optional[float] = None,
        patience: Optional[int] = None,
        weight_decay: Optional[float] = None,
        gradient_clip: Optional[float] = None,
        gradient_accumulation_steps: Optional[int] = None,
        mixed_precision: Optional[bool] = None,
        num_workers: Optional[int] = None,
        optimizer: Optional[str] = None,
        scheduler: Optional[str] = None,
        scheduler_params: Optional[Dict[str, Any]] = None,
        early_stopping: Optional[bool] = None,
        validation_split: Optional[float] = None,
        shuffle: Optional[bool] = None,
        pin_memory: Optional[bool] = None,
        persistent_workers: Optional[bool] = None,
        adam_betas: Optional[Tuple[float, float]] = None,
        adam_eps: Optional[float] = None,
        lr_patience: Optional[int] = None,
        lr_factor: Optional[float] = None,
        min_lr: Optional[float] = None,
        
        # Data Parameters
        normal_samples: Optional[int] = None,
        attack_samples: Optional[int] = None,
        features: Optional[int] = None,
        use_real_data: Optional[bool] = None,
        data_normalization: Optional[str] = None,
        anomaly_factor: Optional[float] = None,
        random_state: Optional[int] = None,
        test_split: Optional[float] = None,
        stratified_split: Optional[bool] = None,
        data_path: Optional[str] = None,
        artifacts_path: Optional[str] = None,
        synthetic_generation: Optional[Dict[str, Any]] = None,
        preprocessing: Optional[Dict[str, Any]] = None,
        
        # Security Parameters
        percentile: Optional[float] = None,
        attack_threshold: Optional[float] = None,
        false_negative_cost: Optional[float] = None,
        enable_security_metrics: Optional[bool] = None,
        anomaly_threshold_strategy: Optional[str] = None,
        early_warning_threshold: Optional[float] = None,
        adaptive_threshold: Optional[bool] = None,
        confidence_interval: Optional[float] = None,
        detection_methods: Optional[List[str]] = None,
        alert_levels: Optional[List[str]] = None,
        threshold_validation: Optional[bool] = None,
        robust_detection: Optional[bool] = None,
        false_positive_tolerance: Optional[float] = None,
        performance_optimized_detection: Optional[bool] = None,
        real_time_monitoring: Optional[bool] = None,
        ensemble_voting: Optional[str] = None,
        uncertainty_threshold: Optional[float] = None,
        
        # Monitoring Parameters
        metrics_frequency: Optional[int] = None,
        checkpoint_frequency: Optional[int] = None,
        tensorboard_logging: Optional[bool] = None,
        console_logging_level: Optional[str] = None,
        save_best_model: Optional[bool] = None,
        save_model_history: Optional[bool] = None,
        metrics_to_track: Optional[List[str]] = None,
        early_stopping_metric: Optional[str] = None,
        checkpoint_format: Optional[str] = None,
        log_model_summary: Optional[bool] = None,
        tensorboard_dir: Optional[str] = None,
        log_frequency: Optional[int] = None,
        save_checkpoints: Optional[bool] = None,
        tensorboard: Optional[Dict[str, Any]] = None,
        stability_metrics: Optional[bool] = None,
        performance_metrics: Optional[bool] = None,
        profiling_enabled: Optional[bool] = None,
        
        # Hardware Parameters
        device: Optional[str] = None,
        recommended_gpu_memory: Optional[float] = None,
        minimum_system_requirements: Optional[Dict[str, Any]] = None,
        optimal_system_requirements: Optional[Dict[str, Any]] = None,
        memory_management: Optional[Dict[str, Any]] = None,
        performance_optimization: Optional[Dict[str, Any]] = None,
        detected_gpu_memory: Optional[float] = None,
        detected_system_memory: Optional[float] = None,
        system_performance_class: Optional[str] = None,
        optimization_recommendations: Optional[List[str]] = None,
        
        # System Parameters
        model_dir: Optional[str] = None,
        log_dir: Optional[str] = None,
        config_dir: Optional[str] = None,
        data_dir: Optional[str] = None,
        checkpoint_dir: Optional[str] = None,
        results_dir: Optional[str] = None,
        debug: Optional[bool] = None,
        verbose: Optional[bool] = None,
        random_seed: Optional[int] = None,
        reproducible: Optional[bool] = None,
        parallel_processing: Optional[bool] = None,
        max_workers: Optional[int] = None,
        export_onnx: Optional[bool] = None,
        non_interactive: Optional[bool] = None,
        cuda_optimizations: Optional[bool] = None,
        onnx_export: Optional[Dict[str, Any]] = None,
        distributed_training: Optional[bool] = None,
        python_executable: Optional[str] = None,
        working_directory: Optional[str] = None,
        environment_health: Optional[str] = None,
        
        # Preset Parameters
        available_presets: Optional[List[str]] = None,
        current_preset: Optional[str] = None,
        current_override: Optional[str] = None,
        override_rules: Optional[Dict[str, bool]] = None,
        preset_configs: Optional[Dict[str, str]] = None,
        custom_presets_available: Optional[List[str]] = None,
        auto_apply: Optional[bool] = None,
        validate_compatibility: Optional[bool] = None,
        system_recommended_preset: Optional[str] = None,
        preset_compatibility: Optional[Dict[str, Any]] = None,
        
        # Hyperparameter Optimization Parameters
        hpo_enabled: Optional[bool] = None,
        hpo_strategy: Optional[str] = None,
        study_name: Optional[str] = None,
        direction: Optional[str] = None,
        n_trials: Optional[int] = None,
        timeout: Optional[int] = None,
        sampler: Optional[str] = None,
        pruner: Optional[str] = None,
        objective_metric: Optional[str] = None,
        optimization_space: Optional[Dict[str, Any]] = None,
        hpo_early_stopping: Optional[Dict[str, Any]] = None,
        timeout_seconds: Optional[int] = None,
        trial_epochs: Optional[int] = None,
        trial_patience: Optional[int] = None,
        cleanup_trials: Optional[bool] = None,
        generate_plots: Optional[bool] = None,
        search_space: Optional[Dict[str, Any]] = None,
        hpo_sampler: Optional[Dict[str, Any]] = None,
        hpo_pruner: Optional[Dict[str, Any]] = None,
        scoring: Optional[Dict[str, Any]] = None,
        storage: Optional[Dict[str, Any]] = None,
        
        # Validation Parameters
        cross_validation: Optional[Dict[str, Any]] = None,
        metrics: Optional[List[str]] = None,
        validation_frequency: Optional[int] = None,
        save_validation_results: Optional[bool] = None,
        detailed_metrics: Optional[bool] = None,
        robustness_testing: Optional[bool] = None,
        performance_benchmarking: Optional[bool] = None,
        confidence_intervals: Optional[bool] = None,
        
        # Experimental Parameters
        experimental_features: Optional[Dict[str, bool]] = None,
        experimental_settings: Optional[Dict[str, bool]] = None,
        
        # Metadata Parameters
        description: Optional[str] = None,
        version: Optional[str] = None,
        config_version: Optional[str] = None,
        config_type: Optional[str] = None,
        created: Optional[str] = None,
        last_modified: Optional[str] = None,
        preset_used: Optional[str] = None,
        recommended_hardware: Optional[Dict[str, Any]] = None,
        compatibility: Optional[List[str]] = None,
        system_info: Optional[Dict[str, Any]] = None,
        validation_info: Optional[Dict[str, Any]] = None,
        
        # Runtime Parameters
        config_loaded_at: Optional[str] = None,
        config_source: Optional[str] = None,
        runtime_id: Optional[str] = None,
        process_id: Optional[int] = None,
        system_analysis_completed: Optional[bool] = None,
        system_performance_score: Optional[float] = None,
        system_class: Optional[str] = None,
        optimizations_applied: Optional[Dict[str, bool]] = None,
        resource_status: Optional[Dict[str, bool]] = None,
        system_warnings: Optional[List[str]] = None,
        recommendations: Optional[List[str]] = None,
        configuration_health: Optional[Dict[str, Any]] = None,
        
        **kwargs  # Catch any additional parameters
    ):
        super(EnhancedAutoencoder, self).__init__()
        
        # Use centralized initialization helper
        init_data = _initialize_autoencoder_config(
            model_class_name='EnhancedAutoencoder',
            input_dim=input_dim,
            config=config,
            preset=preset,
            # Pass through all parameters
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            bias=bias,
            weight_init=weight_init,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention,
            model_type=model_type,
            model_types=model_types,
            available_activations=available_activations,
            available_normalizations=available_normalizations,
            available_initializers=available_initializers,
            legacy_mode=legacy_mode,
            diversity_factor=diversity_factor,
            min_features=min_features,
            num_models=num_models,
            batch_size=batch_size,
            epochs=epochs,
            learning_rate=learning_rate,
            patience=patience,
            weight_decay=weight_decay,
            gradient_clip=gradient_clip,
            gradient_accumulation_steps=gradient_accumulation_steps,
            mixed_precision=mixed_precision,
            num_workers=num_workers,
            optimizer=optimizer,
            scheduler=scheduler,
            scheduler_params=scheduler_params,
            early_stopping=early_stopping,
            validation_split=validation_split,
            shuffle=shuffle,
            pin_memory=pin_memory,
            persistent_workers=persistent_workers,
            adam_betas=adam_betas,
            adam_eps=adam_eps,
            lr_patience=lr_patience,
            lr_factor=lr_factor,
            min_lr=min_lr,
            normal_samples=normal_samples,
            attack_samples=attack_samples,
            features=features,
            use_real_data=use_real_data,
            data_normalization=data_normalization,
            anomaly_factor=anomaly_factor,
            random_state=random_state,
            test_split=test_split,
            stratified_split=stratified_split,
            data_path=data_path,
            artifacts_path=artifacts_path,
            synthetic_generation=synthetic_generation,
            preprocessing=preprocessing,
            percentile=percentile,
            attack_threshold=attack_threshold,
            false_negative_cost=false_negative_cost,
            enable_security_metrics=enable_security_metrics,
            anomaly_threshold_strategy=anomaly_threshold_strategy,
            early_warning_threshold=early_warning_threshold,
            adaptive_threshold=adaptive_threshold,
            confidence_interval=confidence_interval,
            detection_methods=detection_methods,
            alert_levels=alert_levels,
            threshold_validation=threshold_validation,
            robust_detection=robust_detection,
            false_positive_tolerance=false_positive_tolerance,
            performance_optimized_detection=performance_optimized_detection,
            real_time_monitoring=real_time_monitoring,
            ensemble_voting=ensemble_voting,
            uncertainty_threshold=uncertainty_threshold,
            metrics_frequency=metrics_frequency,
            checkpoint_frequency=checkpoint_frequency,
            tensorboard_logging=tensorboard_logging,
            console_logging_level=console_logging_level,
            save_best_model=save_best_model,
            save_model_history=save_model_history,
            metrics_to_track=metrics_to_track,
            early_stopping_metric=early_stopping_metric,
            checkpoint_format=checkpoint_format,
            log_model_summary=log_model_summary,
            tensorboard_dir=tensorboard_dir,
            log_frequency=log_frequency,
            save_checkpoints=save_checkpoints,
            tensorboard=tensorboard,
            stability_metrics=stability_metrics,
            performance_metrics=performance_metrics,
            profiling_enabled=profiling_enabled,
            device=device,
            recommended_gpu_memory=recommended_gpu_memory,
            minimum_system_requirements=minimum_system_requirements,
            optimal_system_requirements=optimal_system_requirements,
            memory_management=memory_management,
            performance_optimization=performance_optimization,
            detected_gpu_memory=detected_gpu_memory,
            detected_system_memory=detected_system_memory,
            system_performance_class=system_performance_class,
            optimization_recommendations=optimization_recommendations,
            model_dir=model_dir,
            log_dir=log_dir,
            config_dir=config_dir,
            data_dir=data_dir,
            checkpoint_dir=checkpoint_dir,
            results_dir=results_dir,
            debug=debug,
            verbose=verbose,
            random_seed=random_seed,
            reproducible=reproducible,
            parallel_processing=parallel_processing,
            max_workers=max_workers,
            export_onnx=export_onnx,
            non_interactive=non_interactive,
            cuda_optimizations=cuda_optimizations,
            onnx_export=onnx_export,
            distributed_training=distributed_training,
            python_executable=python_executable,
            working_directory=working_directory,
            environment_health=environment_health,
            available_presets=available_presets,
            current_preset=current_preset,
            current_override=current_override,
            override_rules=override_rules,
            preset_configs=preset_configs,
            custom_presets_available=custom_presets_available,
            auto_apply=auto_apply,
            validate_compatibility=validate_compatibility,
            system_recommended_preset=system_recommended_preset,
            preset_compatibility=preset_compatibility,
            hpo_enabled=hpo_enabled,
            hpo_strategy=hpo_strategy,
            study_name=study_name,
            direction=direction,
            n_trials=n_trials,
            timeout=timeout,
            sampler=sampler,
            pruner=pruner,
            objective_metric=objective_metric,
            optimization_space=optimization_space,
            hpo_early_stopping=hpo_early_stopping,
            timeout_seconds=timeout_seconds,
            trial_epochs=trial_epochs,
            trial_patience=trial_patience,
            cleanup_trials=cleanup_trials,
            generate_plots=generate_plots,
            search_space=search_space,
            hpo_sampler=hpo_sampler,
            hpo_pruner=hpo_pruner,
            scoring=scoring,
            storage=storage,
            cross_validation=cross_validation,
            metrics=metrics,
            validation_frequency=validation_frequency,
            save_validation_results=save_validation_results,
            detailed_metrics=detailed_metrics,
            robustness_testing=robustness_testing,
            performance_benchmarking=performance_benchmarking,
            confidence_intervals=confidence_intervals,
            experimental_features=experimental_features,
            experimental_settings=experimental_settings,
            description=description,
            version=version,
            config_version=config_version,
            config_type=config_type,
            created=created,
            last_modified=last_modified,
            preset_used=preset_used,
            recommended_hardware=recommended_hardware,
            compatibility=compatibility,
            system_info=system_info,
            validation_info=validation_info,
            config_loaded_at=config_loaded_at,
            config_source=config_source,
            runtime_id=runtime_id,
            process_id=process_id,
            system_analysis_completed=system_analysis_completed,
            system_performance_score=system_performance_score,
            system_class=system_class,
            optimizations_applied=optimizations_applied,
            resource_status=resource_status,
            system_warnings=system_warnings,
            recommendations=recommendations,
            configuration_health=configuration_health,
            **kwargs
        )
        
        # Extract processed data from centralized initialization
        self.config = init_data['config']
        params = init_data['processed_params']
        self.device = init_data['device']
        self.mixed_precision = init_data['mixed_precision']
        self.mixed_precision_requested = init_data['mixed_precision_requested']
        self.initialization_timestamp = init_data['initialization_timestamp']
        #self.preset_name = init_data['preset_name']
        # PROPERLY STORE PRESET NAME WITH FALLBACKS
        self.preset_name = init_data.get('preset_name') or preset
        #self.preset_name = init_data['preset_name'] or preset
        
        # If still no preset name, try to extract from config
        if not self.preset_name:
            self.preset_name = (
                self.config.get('metadata', {}).get('preset_used') or
                self.config.get('presets', {}).get('current_preset') or
                self.config.get('runtime', {}).get('applied_preset') or
                "Custom"
            )
        
        # ENSURE PRESET NAME IS STORED IN CONFIG FOR CONSISTENCY
        if self.preset_name and self.preset_name != "Custom":
            if 'metadata' not in self.config:
                self.config['metadata'] = {}
            self.config['metadata']['preset_used'] = self.preset_name
            
            if 'presets' not in self.config:
                self.config['presets'] = {}
            self.config['presets']['current_preset'] = self.preset_name
        
        training_config = init_data['training_config']
        
        # Set instance attributes from processed parameters
        self.input_dim = params['input_dim']
        self.encoding_dim = params['encoding_dim']
        self.hidden_dims = params['hidden_dims']
        self.dropout_rates = params['dropout_rates']
        self.activation = params['activation']
        self.activation_param = params['activation_param']
        self.normalization = params['normalization']
        self.use_batch_norm = params['use_batch_norm']
        self.use_layer_norm = params['use_layer_norm']
        self.bias = params['bias']
        self.weight_init = params['weight_init']
        self.skip_connection = params['skip_connection']
        self.residual_blocks = params['residual_blocks']
        self.use_attention = params['use_attention']
        self.model_type = params['model_type']
        self.min_features = params['min_features']
        self.legacy_mode = params['legacy_mode']
        
        # Build enhanced network architecture
        self._build_enhanced_architecture()
        
        # Initialize weights
        self._initialize_weights()
        
        # Move to device
        self.to(self.device)
        
        # Log successful initialization
        if hasattr(globals().get('display_model_initialization_summary'), '__call__'):
            try:
                architecture_info = {
                    'hidden_dims': self.hidden_dims,
                    'encoding_dim': self.encoding_dim,
                    'dropout_rates': self.dropout_rates,
                    'activation': self.activation,
                    'normalization': self.normalization
                }
                
                enhanced_features = {
                    'use_attention': self.use_attention and hasattr(self, 'attention') and self.attention is not None,
                    'residual_blocks': self.residual_blocks,
                    'skip_connections': self.skip_connection,
                    'legacy_mode': self.legacy_mode
                }
                
                display_model_initialization_summary(
                    model_instance=self,
                    model_type='EnhancedAutoencoder',
                    input_dim=self.input_dim,
                    architecture_info=architecture_info,
                    device=self.device,
                    mixed_precision=self.mixed_precision,
                    preset_name=self.preset_name,
                    training_config=training_config,
                    enhanced_features=enhanced_features
                )
            except Exception as e:
                logger.debug(f"Could not display initialization summary: {e}")
    
    def _build_enhanced_architecture(self) -> None:
        """Build the enhanced autoencoder network architecture."""
        try:
            if self.legacy_mode:
                # Simple architecture for backward compatibility
                self._build_simple_architecture()
                return
            
            # Build advanced encoder
            encoder_layers = []
            dims = [self.input_dim] + self.hidden_dims + [self.encoding_dim]
            
            for i in range(len(dims) - 1):
                # Linear layer
                encoder_layers.append(nn.Linear(dims[i], dims[i + 1], bias=self.bias))
                
                # Normalization (except for last layer)
                if i < len(dims) - 2:
                    norm_layer = self._get_normalization_layer(dims[i + 1])
                    if norm_layer is not None:
                        encoder_layers.append(norm_layer)
                    
                    # Activation
                    encoder_layers.append(self._get_activation_function())
                    
                    # Residual blocks if enabled
                    if self.residual_blocks and dims[i + 1] >= 64:
                        encoder_layers.append(self.ResidualBlock(
                            dims[i + 1],
                            self._get_activation_function(),
                            self._get_normalization_layer(dims[i + 1]),
                            self.dropout_rates[i] * 0.5 if i < len(self.dropout_rates) else 0.1
                        ))
                    
                    # Dropout
                    if i < len(self.dropout_rates):
                        dropout_rate = self.dropout_rates[i]
                        if dropout_rate > 0:
                            encoder_layers.append(nn.Dropout(dropout_rate))
            
            # Final encoding activation
            encoder_layers.append(nn.Tanh())
            
            self.encoder = nn.Sequential(*encoder_layers)
            
            # Attention mechanism if enabled
            if self.use_attention and self.encoding_dim >= 64:
                # Ensure encoding_dim is divisible by number of attention heads
                num_heads = 8 if self.encoding_dim >= 64 else 4
                while self.encoding_dim % num_heads != 0 and num_heads > 1:
                    num_heads -= 1
                
                if num_heads > 0:
                    self.attention = self.MultiHeadAttention(self.encoding_dim, num_heads)
                else:
                    self.attention = None
                    logger.warning(f"Could not create attention with encoding_dim={self.encoding_dim}")
            else:
                self.attention = None
            
            # Build decoder (reverse of encoder)
            decoder_layers = []
            dims = [self.encoding_dim] + list(reversed(self.hidden_dims)) + [self.input_dim]
            
            for i in range(len(dims) - 1):
                # Linear layer
                decoder_layers.append(nn.Linear(dims[i], dims[i + 1], bias=self.bias))
                
                # Normalization (except for last layer)
                if i < len(dims) - 2:
                    norm_layer = self._get_normalization_layer(dims[i + 1])
                    if norm_layer is not None:
                        decoder_layers.append(norm_layer)
                    
                    # Activation
                    decoder_layers.append(self._get_activation_function())
                    
                    # Residual blocks if enabled
                    if self.residual_blocks and dims[i + 1] >= 64:
                        dropout_idx = len(self.dropout_rates) - 1 - i
                        dropout_rate = self.dropout_rates[dropout_idx] if dropout_idx < len(self.dropout_rates) and dropout_idx >= 0 else 0.1
                        decoder_layers.append(self.ResidualBlock(
                            dims[i + 1],
                            self._get_activation_function(),
                            self._get_normalization_layer(dims[i + 1]),
                            dropout_rate * 0.5
                        ))
                    
                    # Dropout (reversed order)
                    if i < len(self.dropout_rates):
                        dropout_idx = len(self.dropout_rates) - 1 - i
                        dropout_rate = self.dropout_rates[dropout_idx] if dropout_idx >= 0 else 0.0
                        if dropout_rate > 0:
                            decoder_layers.append(nn.Dropout(dropout_rate))
            
            # Final decoder activation
            decoder_layers.append(nn.Sigmoid())
            
            self.decoder = nn.Sequential(*decoder_layers)
            
            # Skip connections if enabled
            if self.skip_connection:
                # Create skip connections for compatible dimensions
                self.skip_layers = nn.ModuleDict()
                
                # Input to hidden layers skip connections
                for i, dim in enumerate(self.hidden_dims):
                    if self.input_dim <= dim * 2:  # Only if architecturally reasonable
                        self.skip_layers[f'input_to_hidden_{i}'] = nn.Linear(self.input_dim, dim, bias=False)
                
                # Input to output skip connection
                self.skip_layers['input_to_output'] = nn.Linear(self.input_dim, self.input_dim, bias=False)
            
            logger.debug(f"Built enhanced network: {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}")
            logger.debug(f"Enhanced features: attention={self.attention is not None}, "
                        f"residual_blocks={self.residual_blocks}, skip_connections={len(getattr(self, 'skip_layers', {}))}")
            
        except Exception as e:
            logger.error(f"Failed to build enhanced architecture: {e}")
            logger.warning("Falling back to simple architecture")
            self._build_simple_architecture()
    
    def _build_simple_architecture(self) -> None:
        """Build simple fallback architecture for legacy mode."""
        try:
            # Simple encoder
            self.encoder = nn.Sequential(
                nn.Linear(self.input_dim, self.hidden_dims[0] if self.hidden_dims else 128),
                self._get_activation_function(),
                nn.Dropout(self.dropout_rates[0] if self.dropout_rates else 0.2),
                nn.Linear(self.hidden_dims[0] if self.hidden_dims else 128, self.encoding_dim),
                nn.Tanh()
            )
            
            # Simple decoder
            self.decoder = nn.Sequential(
                nn.Linear(self.encoding_dim, self.hidden_dims[0] if self.hidden_dims else 128),
                self._get_activation_function(),
                nn.Dropout(self.dropout_rates[0] if self.dropout_rates else 0.2),
                nn.Linear(self.hidden_dims[0] if self.hidden_dims else 128, self.input_dim),
                nn.Sigmoid()
            )
            
            # No advanced features in simple mode
            self.attention = None
            self.skip_layers = None
            
            logger.debug("Built simple fallback architecture")
            
        except Exception as e:
            logger.error(f"Failed to build simple architecture: {e}")
            raise RuntimeError(f"Architecture construction completely failed: {e}")
    
    def _get_activation_function(self) -> nn.Module:
        """Get activation function based on configuration."""
        activation_map = {
            'relu': nn.ReLU(inplace=True),
            'leaky_relu': nn.LeakyReLU(negative_slope=self.activation_param, inplace=True),
            'gelu': nn.GELU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid(),
            'swish': nn.SiLU(),  # SiLU is PyTorch's implementation of Swish
            'elu': nn.ELU(alpha=self.activation_param, inplace=True),
            'selu': nn.SELU(inplace=True),
            'prelu': nn.PReLU()
        }
        
        if self.activation not in activation_map:
            logger.warning(f"Unknown activation '{self.activation}', using LeakyReLU")
            return nn.LeakyReLU(negative_slope=0.2, inplace=True)
        
        return activation_map[self.activation]
    
    def _get_normalization_layer(self, num_features: int) -> Optional[nn.Module]:
        """Get normalization layer based on configuration."""
        if self.normalization is None or self.normalization == 'none':
            return None
        
        if self.normalization == 'batch' and self.use_batch_norm:
            return nn.BatchNorm1d(num_features)
        elif self.normalization == 'layer' and self.use_layer_norm:
            return nn.LayerNorm(num_features)
        elif self.normalization == 'instance':
            return nn.InstanceNorm1d(num_features)
        elif self.normalization == 'group':
            # Use 8 groups or num_features, whichever is smaller
            num_groups = min(8, num_features)
            return nn.GroupNorm(num_groups, num_features)
        else:
            return None
    
    class ResidualBlock(nn.Module):
        """Residual block for enhanced architecture."""
        def __init__(self, dim: int, activation: nn.Module, normalization: Optional[nn.Module] = None, dropout: float = 0.0):
            super().__init__()
            self.dim = dim
            self.linear1 = nn.Linear(dim, dim)
            self.linear2 = nn.Linear(dim, dim)
            self.activation = activation
            self.normalization = normalization
            self.dropout = nn.Dropout(dropout) if dropout > 0 else None
            
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            residual = x
            
            out = self.linear1(x)
            if self.normalization is not None:
                out = self.normalization(out)
            out = self.activation(out)
            
            if self.dropout is not None:
                out = self.dropout(out)
            
            out = self.linear2(out)
            if self.normalization is not None and hasattr(self, 'normalization2'):
                out = self.normalization2(out)
            
            # Add residual connection
            out = out + residual
            
            return self.activation(out)
    
    class MultiHeadAttention(nn.Module):
        """Multi-head attention mechanism for enhanced features."""
        def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):
            super().__init__()
            self.dim = dim
            self.num_heads = num_heads
            self.head_dim = dim // num_heads
            
            if self.head_dim * num_heads != dim:
                raise ValueError(f"dim ({dim}) must be divisible by num_heads ({num_heads})")
            
            self.query = nn.Linear(dim, dim)
            self.key = nn.Linear(dim, dim)
            self.value = nn.Linear(dim, dim)
            self.out = nn.Linear(dim, dim)
            self.dropout = nn.Dropout(dropout)
            
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            batch_size = x.size(0)
            
            # Generate queries, keys, and values
            q = self.query(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
            k = self.key(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
            v = self.value(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
            
            # Attention mechanism
            scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
            attn_weights = torch.softmax(scores, dim=-1)
            attn_weights = self.dropout(attn_weights)
            
            # Apply attention
            context = torch.matmul(attn_weights, v)
            context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.dim)
            
            return self.out(context)
    
    def _initialize_weights(self) -> None:
        """Initialize network weights based on configuration."""
        try:
            init_method = self.weight_init
            
            for module in self.modules():
                if isinstance(module, nn.Linear):
                    if init_method == 'xavier_uniform':
                        nn.init.xavier_uniform_(module.weight)
                    elif init_method == 'xavier_normal':
                        nn.init.xavier_normal_(module.weight)
                    elif init_method == 'kaiming_uniform':
                        nn.init.kaiming_uniform_(module.weight, nonlinearity='leaky_relu' if self.activation == 'leaky_relu' else 'relu')
                    elif init_method == 'kaiming_normal':
                        nn.init.kaiming_normal_(module.weight, nonlinearity='leaky_relu' if self.activation == 'leaky_relu' else 'relu')
                    elif init_method == 'orthogonal':
                        nn.init.orthogonal_(module.weight)
                    elif init_method == 'he_uniform':
                        nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='relu')
                    elif init_method == 'he_normal':
                        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
                    else:
                        logger.warning(f"Unknown weight init '{init_method}', using xavier_uniform")
                        nn.init.xavier_uniform_(module.weight)
                    
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0)
                
                elif isinstance(module, (nn.BatchNorm1d, nn.LayerNorm, nn.GroupNorm)):
                    if hasattr(module, 'weight') and module.weight is not None:
                        nn.init.constant_(module.weight, 1)
                    if hasattr(module, 'bias') and module.bias is not None:
                        nn.init.constant_(module.bias, 0)
                
                elif isinstance(module, nn.InstanceNorm1d):
                    if hasattr(module, 'weight') and module.weight is not None:
                        nn.init.constant_(module.weight, 1)
                    if hasattr(module, 'bias') and module.bias is not None:
                        nn.init.constant_(module.bias, 0)
            
            logger.debug(f"Initialized weights using {init_method}")
            
        except Exception as e:
            logger.error(f"Weight initialization failed: {e}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Enhanced forward pass with attention and skip connections."""
        # Input validation
        if not isinstance(x, torch.Tensor):
            raise ValueError(f"Expected torch.Tensor input, got {type(x)}")
        
        if x.dim() != 2:
            raise ValueError(f"Expected 2D input (batch_size, features), got {x.dim()}D")
        
        if x.size(-1) != self.input_dim:
            raise ValueError(f"Input size {x.size(-1)} doesn't match expected {self.input_dim}")
        
        try:
            # Mixed precision forward pass
            #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
            device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
            with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
                # Store input for skip connections
                input_residual = x
                
                # Encode
                encoded = self.encoder(x)
                
                # Apply attention if available
                if hasattr(self, 'attention') and self.attention is not None:
                    # Reshape for attention (add sequence dimension)
                    batch_size = encoded.size(0)
                    encoded_reshaped = encoded.unsqueeze(1)  # [batch_size, 1, encoding_dim]
                    attended = self.attention(encoded_reshaped)
                    encoded = attended.squeeze(1) + encoded  # Residual connection
                
                # Decode
                decoded = self.decoder(encoded)
                
                # Apply skip connections if available
                if hasattr(self, 'skip_layers') and self.skip_layers is not None and not self.legacy_mode:
                    if 'input_to_output' in self.skip_layers:
                        skip_connection = self.skip_layers['input_to_output'](input_residual)
                        decoded = decoded + skip_connection
                
                return decoded
                
        except Exception as e:
            logger.error(f"EnhancedAutoencoder forward pass failed: {e}")
            raise RuntimeError(f"Forward pass error: {e}")
    
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode input data to latent representation with enhanced features."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            encoded = self.encoder(x)
            
            # Apply attention if available
            if hasattr(self, 'attention') and self.attention is not None:
                batch_size = encoded.size(0)
                encoded_reshaped = encoded.unsqueeze(1)
                attended = self.attention(encoded_reshaped)
                encoded = attended.squeeze(1) + encoded  # Residual connection
            
            return encoded
    
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode latent representation to original space."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            return self.decoder(z)
    
    @property
    def original_mixed_precision_setting(self) -> bool:
        """Returns the originally requested mixed precision setting."""
        return getattr(self, 'mixed_precision_requested', True)
    
    def get_config(self) -> Dict[str, Any]:
        """Get comprehensive model configuration."""
        config = self.config.copy()
        
        # Add runtime information
        config.setdefault('runtime', {}).update({
            'model_initialized_at': self.initialization_timestamp,
            'total_parameters': sum(p.numel() for p in self.parameters()),
            'trainable_parameters': sum(p.numel() for p in self.parameters() if p.requires_grad),
            'device': str(self.device),
            'mixed_precision_active': self.mixed_precision,
            'mixed_precision_requested': self.original_mixed_precision_setting,
            'preset_used': self.preset_name,
            'input_dim': self.input_dim,
            'architecture_summary': f"{self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            'centralized_config_used': True,
            'helper_functions_leveraged': True,
            'enhanced_features': {
                'attention': self.use_attention and hasattr(self, 'attention') and self.attention is not None,
                'residual_blocks': self.residual_blocks,
                'skip_connections': self.skip_connection and hasattr(self, 'skip_layers'),
                'legacy_mode': self.legacy_mode
            }
        })
        
        return config
    
    def update_config(self, new_config: Dict[str, Any], reinitialize: bool = False) -> None:
        """Update configuration and optionally reinitialize components."""
        old_config = self.config.copy()
        
        try:
            # Update configuration using existing helper
            self.config = deep_update(self.config, new_config)
            
            # Reinitialize components if requested
            if reinitialize:
                logger.debug("Reinitializing model components due to config update")
                
                # Check if architecture changed
                architecture_changed = any(
                    key in new_config.get('model', {}) 
                    for key in ['encoding_dim', 'hidden_dims', 'dropout_rates', 'use_attention', 'residual_blocks']
                )
                
                if architecture_changed:
                    logger.warning("Architecture parameters changed - full model rebuild required")
                    raise ValueError("Architecture changes require creating a new model instance")
                
                # Update device configuration if changed
                if 'hardware' in new_config:
                    old_device = self.device
                    device_setting = new_config['hardware'].get('device')
                    if device_setting:
                        if device_setting == 'auto':
                            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                        elif device_setting in ['cpu', 'cuda']:
                            self.device = torch.device(device_setting)
                        
                        if self.device != old_device:
                            self.to(self.device)
                            logger.debug(f"Device changed from {old_device} to {self.device}")
            
            logger.debug("Configuration updated successfully using centralized helper")
            
        except Exception as e:
            # Restore old configuration on error
            self.config = old_config
            logger.error(f"Configuration update failed, restored previous config: {e}")
            raise
    
    def save_model(self, path: str, include_config: bool = True) -> None:
        """Save model state and configuration."""
        save_dict = {
            'model_state_dict': self.state_dict(),
            'model_class': self.__class__.__name__,
            'input_dim': self.input_dim,
            'architecture': {
                'encoding_dim': self.encoding_dim,
                'hidden_dims': self.hidden_dims,
                'dropout_rates': self.dropout_rates,
                'use_attention': self.use_attention,
                'residual_blocks': self.residual_blocks,
                'skip_connection': self.skip_connection,
                'legacy_mode': self.legacy_mode
            },
            'centralized_config_used': True
        }
        
        if include_config:
            save_dict['config'] = self.config
        
        torch.save(save_dict, path)
        logger.debug(f"EnhancedAutoencoder saved to {path} (centralized config)")
    
    @classmethod
    def load_model(cls, path: str, **kwargs):
        """Load model from saved state."""
        checkpoint = torch.load(path, map_location='cpu')
        
        # Extract architecture parameters
        input_dim = checkpoint['input_dim']
        config = checkpoint.get('config', {})
        
        # Create model instance using centralized config if available
        if checkpoint.get('centralized_config_used', False):
            try:
                model = create_model_instance('EnhancedAutoencoder', input_dim, config, **kwargs)
                #model = cls(input_dim=input_dim, config=config, **kwargs)
                #model = create_model_instance(model_type='EnhancedAutoencoder', input_dim=input_dim, config=config, **kwargs)
            except:
                # Fallback to direct instantiation
                model = cls(input_dim=input_dim, config=config, **kwargs)
                #model = create_model_instance(model_type='EnhancedAutoencoder', input_dim=input_dim, config=config, **kwargs)
        else:
            model = cls(input_dim=input_dim, config=config, **kwargs)
        
        # Load state dict
        model.load_state_dict(checkpoint['model_state_dict'])
        
        logger.debug(f"EnhancedAutoencoder loaded from {path}")
        return model
    
    def get_model_summary(self) -> str:
        """Get detailed model summary."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        # Enhanced features summary
        enhanced_features = []
        if self.use_attention and hasattr(self, 'attention') and self.attention is not None:
            enhanced_features.append("Multi-Head Attention")
        if self.residual_blocks:
            enhanced_features.append("Residual Blocks")
        if self.skip_connection and hasattr(self, 'skip_layers'):
            enhanced_features.append("Skip Connections")
        if self.normalization:
            enhanced_features.append(f"{self.normalization.title()} Normalization")
        
        summary = [
            f"EnhancedAutoencoder Summary (Factory Pattern)",
            f"{'-'*40}",
            f"Model Type: {self.model_type}",
            f"Preset: {self.preset_name or 'Custom'}",
            f"Architecture: {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            f"Activation: {self.activation}",
            f"Enhanced Features: {', '.join(enhanced_features) if enhanced_features else 'None'}",
            f"Legacy Mode: {self.legacy_mode}",
            f"Device: {self.device}",
            f"Mixed Precision: {self.mixed_precision}",
            f"Parameters: {total_params:,} total, {trainable_params:,} trainable",
            f"Memory Usage: ~{total_params * 4 / (1024**2):.1f} MB (FP32)",
            f"Factory Pattern: Enabled",
            f"Centralized Config: Enabled",
            f"Helper Functions: Leveraged",
            f"{'-'*40}"
        ]
        
        return '\n'.join(summary)
    
    def __repr__(self) -> str:
        """String representation of the model."""
        enhanced_features = []
        if self.use_attention:
            enhanced_features.append("attention")
        if self.residual_blocks:
            enhanced_features.append("residual")
        if self.skip_connection:
            enhanced_features.append("skip")
        
        features_str = f", features=[{','.join(enhanced_features)}]" if enhanced_features else ""
        
        return (f"EnhancedAutoencoder(input_dim={self.input_dim}, "
                f"encoding_dim={self.encoding_dim}, "
                f"hidden_dims={self.hidden_dims}, "
                f"device={self.device}, "
                f"preset='{self.preset_name or 'Custom'}', "
                f"factory_pattern=True{features_str}, "
                f"centralized_config=True)")

class AutoencoderEnsemble(nn.Module):
    """
    Streamlined AutoencoderEnsemble using centralized configuration initialization.
    
    This class leverages the _initialize_autoencoder_config() helper function to eliminate
    redundancy and provide consistent parameter processing across all autoencoder variants.
    The ensemble creates diverse autoencoder models for improved robustness and performance
    while maintaining full compatibility with all preset parameters.
    """
    
    def __init__(
        self,
        # Core parameter - always required
        input_dim: Optional[int] = None,
        
        # Configuration override - can contain any/all preset parameters
        config: Optional[Dict[str, Any]] = None,
        preset: Optional[str] = None,
        
        # ALL preset parameters accepted for backward compatibility
        # Core Model Architecture Parameters
        encoding_dim: Optional[int] = None,
        hidden_dims: Optional[List[int]] = None,
        dropout_rates: Optional[List[float]] = None,
        activation: Optional[str] = None,
        activation_param: Optional[float] = None,
        normalization: Optional[str] = None,
        use_batch_norm: Optional[bool] = None,
        use_layer_norm: Optional[bool] = None,
        bias: Optional[bool] = None,
        weight_init: Optional[str] = None,
        skip_connection: Optional[bool] = None,
        residual_blocks: Optional[bool] = None,
        use_attention: Optional[bool] = None,
        
        # Model Type and Variants
        model_type: Optional[str] = None,
        model_types: Optional[List[str]] = None,
        available_activations: Optional[List[str]] = None,
        available_normalizations: Optional[List[str]] = None,
        available_initializers: Optional[List[str]] = None,
        legacy_mode: Optional[bool] = None,
        
        # Ensemble Parameters
        diversity_factor: Optional[float] = None,
        min_features: Optional[int] = None,
        num_models: Optional[int] = None,
        
        # Training Parameters
        batch_size: Optional[int] = None,
        epochs: Optional[int] = None,
        learning_rate: Optional[float] = None,
        patience: Optional[int] = None,
        weight_decay: Optional[float] = None,
        gradient_clip: Optional[float] = None,
        gradient_accumulation_steps: Optional[int] = None,
        mixed_precision: Optional[bool] = None,
        num_workers: Optional[int] = None,
        optimizer: Optional[str] = None,
        scheduler: Optional[str] = None,
        scheduler_params: Optional[Dict[str, Any]] = None,
        early_stopping: Optional[bool] = None,
        validation_split: Optional[float] = None,
        shuffle: Optional[bool] = None,
        pin_memory: Optional[bool] = None,
        persistent_workers: Optional[bool] = None,
        adam_betas: Optional[Tuple[float, float]] = None,
        adam_eps: Optional[float] = None,
        lr_patience: Optional[int] = None,
        lr_factor: Optional[float] = None,
        min_lr: Optional[float] = None,
        
        # Data Parameters
        normal_samples: Optional[int] = None,
        attack_samples: Optional[int] = None,
        features: Optional[int] = None,
        use_real_data: Optional[bool] = None,
        data_normalization: Optional[str] = None,
        anomaly_factor: Optional[float] = None,
        random_state: Optional[int] = None,
        test_split: Optional[float] = None,
        stratified_split: Optional[bool] = None,
        data_path: Optional[str] = None,
        artifacts_path: Optional[str] = None,
        synthetic_generation: Optional[Dict[str, Any]] = None,
        preprocessing: Optional[Dict[str, Any]] = None,
        
        # Security Parameters
        percentile: Optional[float] = None,
        attack_threshold: Optional[float] = None,
        false_negative_cost: Optional[float] = None,
        enable_security_metrics: Optional[bool] = None,
        anomaly_threshold_strategy: Optional[str] = None,
        early_warning_threshold: Optional[float] = None,
        adaptive_threshold: Optional[bool] = None,
        confidence_interval: Optional[float] = None,
        detection_methods: Optional[List[str]] = None,
        alert_levels: Optional[List[str]] = None,
        threshold_validation: Optional[bool] = None,
        robust_detection: Optional[bool] = None,
        false_positive_tolerance: Optional[float] = None,
        performance_optimized_detection: Optional[bool] = None,
        real_time_monitoring: Optional[bool] = None,
        ensemble_voting: Optional[str] = None,
        uncertainty_threshold: Optional[float] = None,
        
        # Monitoring Parameters
        metrics_frequency: Optional[int] = None,
        checkpoint_frequency: Optional[int] = None,
        tensorboard_logging: Optional[bool] = None,
        console_logging_level: Optional[str] = None,
        save_best_model: Optional[bool] = None,
        save_model_history: Optional[bool] = None,
        metrics_to_track: Optional[List[str]] = None,
        early_stopping_metric: Optional[str] = None,
        checkpoint_format: Optional[str] = None,
        log_model_summary: Optional[bool] = None,
        tensorboard_dir: Optional[str] = None,
        log_frequency: Optional[int] = None,
        save_checkpoints: Optional[bool] = None,
        tensorboard: Optional[Dict[str, Any]] = None,
        stability_metrics: Optional[bool] = None,
        performance_metrics: Optional[bool] = None,
        profiling_enabled: Optional[bool] = None,
        
        # Hardware Parameters
        device: Optional[str] = None,
        recommended_gpu_memory: Optional[float] = None,
        minimum_system_requirements: Optional[Dict[str, Any]] = None,
        optimal_system_requirements: Optional[Dict[str, Any]] = None,
        memory_management: Optional[Dict[str, Any]] = None,
        performance_optimization: Optional[Dict[str, Any]] = None,
        detected_gpu_memory: Optional[float] = None,
        detected_system_memory: Optional[float] = None,
        system_performance_class: Optional[str] = None,
        optimization_recommendations: Optional[List[str]] = None,
        
        # System Parameters
        model_dir: Optional[str] = None,
        log_dir: Optional[str] = None,
        config_dir: Optional[str] = None,
        data_dir: Optional[str] = None,
        checkpoint_dir: Optional[str] = None,
        results_dir: Optional[str] = None,
        debug: Optional[bool] = None,
        verbose: Optional[bool] = None,
        random_seed: Optional[int] = None,
        reproducible: Optional[bool] = None,
        parallel_processing: Optional[bool] = None,
        max_workers: Optional[int] = None,
        export_onnx: Optional[bool] = None,
        non_interactive: Optional[bool] = None,
        cuda_optimizations: Optional[bool] = None,
        onnx_export: Optional[Dict[str, Any]] = None,
        distributed_training: Optional[bool] = None,
        python_executable: Optional[str] = None,
        working_directory: Optional[str] = None,
        environment_health: Optional[str] = None,
        
        # Preset Parameters
        available_presets: Optional[List[str]] = None,
        current_preset: Optional[str] = None,
        current_override: Optional[str] = None,
        override_rules: Optional[Dict[str, bool]] = None,
        preset_configs: Optional[Dict[str, str]] = None,
        custom_presets_available: Optional[List[str]] = None,
        auto_apply: Optional[bool] = None,
        validate_compatibility: Optional[bool] = None,
        system_recommended_preset: Optional[str] = None,
        preset_compatibility: Optional[Dict[str, Any]] = None,
        
        # Hyperparameter Optimization Parameters
        hpo_enabled: Optional[bool] = None,
        hpo_strategy: Optional[str] = None,
        study_name: Optional[str] = None,
        direction: Optional[str] = None,
        n_trials: Optional[int] = None,
        timeout: Optional[int] = None,
        sampler: Optional[str] = None,
        pruner: Optional[str] = None,
        objective_metric: Optional[str] = None,
        optimization_space: Optional[Dict[str, Any]] = None,
        hpo_early_stopping: Optional[Dict[str, Any]] = None,
        timeout_seconds: Optional[int] = None,
        trial_epochs: Optional[int] = None,
        trial_patience: Optional[int] = None,
        cleanup_trials: Optional[bool] = None,
        generate_plots: Optional[bool] = None,
        search_space: Optional[Dict[str, Any]] = None,
        hpo_sampler: Optional[Dict[str, Any]] = None,
        hpo_pruner: Optional[Dict[str, Any]] = None,
        scoring: Optional[Dict[str, Any]] = None,
        storage: Optional[Dict[str, Any]] = None,
        
        # Validation Parameters
        cross_validation: Optional[Dict[str, Any]] = None,
        metrics: Optional[List[str]] = None,
        validation_frequency: Optional[int] = None,
        save_validation_results: Optional[bool] = None,
        detailed_metrics: Optional[bool] = None,
        robustness_testing: Optional[bool] = None,
        performance_benchmarking: Optional[bool] = None,
        confidence_intervals: Optional[bool] = None,
        
        # Experimental Parameters
        experimental_features: Optional[Dict[str, bool]] = None,
        experimental_settings: Optional[Dict[str, bool]] = None,
        
        # Metadata Parameters
        description: Optional[str] = None,
        version: Optional[str] = None,
        config_version: Optional[str] = None,
        config_type: Optional[str] = None,
        created: Optional[str] = None,
        last_modified: Optional[str] = None,
        preset_used: Optional[str] = None,
        recommended_hardware: Optional[Dict[str, Any]] = None,
        compatibility: Optional[List[str]] = None,
        system_info: Optional[Dict[str, Any]] = None,
        validation_info: Optional[Dict[str, Any]] = None,
        
        # Runtime Parameters
        config_loaded_at: Optional[str] = None,
        config_source: Optional[str] = None,
        runtime_id: Optional[str] = None,
        process_id: Optional[int] = None,
        system_analysis_completed: Optional[bool] = None,
        system_performance_score: Optional[float] = None,
        system_class: Optional[str] = None,
        optimizations_applied: Optional[Dict[str, bool]] = None,
        resource_status: Optional[Dict[str, bool]] = None,
        system_warnings: Optional[List[str]] = None,
        recommendations: Optional[List[str]] = None,
        configuration_health: Optional[Dict[str, Any]] = None,
        
        **kwargs  # Catch any additional parameters
    ):
        super(AutoencoderEnsemble, self).__init__()
        
        # Use centralized initialization helper
        init_data = _initialize_autoencoder_config(
            model_class_name='AutoencoderEnsemble',
            input_dim=input_dim,
            config=config,
            preset=preset,
            # Pass through all parameters
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            bias=bias,
            weight_init=weight_init,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention,
            model_type=model_type,
            model_types=model_types,
            available_activations=available_activations,
            available_normalizations=available_normalizations,
            available_initializers=available_initializers,
            legacy_mode=legacy_mode,
            diversity_factor=diversity_factor,
            min_features=min_features,
            num_models=num_models,
            batch_size=batch_size,
            epochs=epochs,
            learning_rate=learning_rate,
            patience=patience,
            weight_decay=weight_decay,
            gradient_clip=gradient_clip,
            gradient_accumulation_steps=gradient_accumulation_steps,
            mixed_precision=mixed_precision,
            num_workers=num_workers,
            optimizer=optimizer,
            scheduler=scheduler,
            scheduler_params=scheduler_params,
            early_stopping=early_stopping,
            validation_split=validation_split,
            shuffle=shuffle,
            pin_memory=pin_memory,
            persistent_workers=persistent_workers,
            adam_betas=adam_betas,
            adam_eps=adam_eps,
            lr_patience=lr_patience,
            lr_factor=lr_factor,
            min_lr=min_lr,
            normal_samples=normal_samples,
            attack_samples=attack_samples,
            features=features,
            use_real_data=use_real_data,
            data_normalization=data_normalization,
            anomaly_factor=anomaly_factor,
            random_state=random_state,
            test_split=test_split,
            stratified_split=stratified_split,
            data_path=data_path,
            artifacts_path=artifacts_path,
            synthetic_generation=synthetic_generation,
            preprocessing=preprocessing,
            percentile=percentile,
            attack_threshold=attack_threshold,
            false_negative_cost=false_negative_cost,
            enable_security_metrics=enable_security_metrics,
            anomaly_threshold_strategy=anomaly_threshold_strategy,
            early_warning_threshold=early_warning_threshold,
            adaptive_threshold=adaptive_threshold,
            confidence_interval=confidence_interval,
            detection_methods=detection_methods,
            alert_levels=alert_levels,
            threshold_validation=threshold_validation,
            robust_detection=robust_detection,
            false_positive_tolerance=false_positive_tolerance,
            performance_optimized_detection=performance_optimized_detection,
            real_time_monitoring=real_time_monitoring,
            ensemble_voting=ensemble_voting,
            uncertainty_threshold=uncertainty_threshold,
            metrics_frequency=metrics_frequency,
            checkpoint_frequency=checkpoint_frequency,
            tensorboard_logging=tensorboard_logging,
            console_logging_level=console_logging_level,
            save_best_model=save_best_model,
            save_model_history=save_model_history,
            metrics_to_track=metrics_to_track,
            early_stopping_metric=early_stopping_metric,
            checkpoint_format=checkpoint_format,
            log_model_summary=log_model_summary,
            tensorboard_dir=tensorboard_dir,
            log_frequency=log_frequency,
            save_checkpoints=save_checkpoints,
            tensorboard=tensorboard,
            stability_metrics=stability_metrics,
            performance_metrics=performance_metrics,
            profiling_enabled=profiling_enabled,
            device=device,
            recommended_gpu_memory=recommended_gpu_memory,
            minimum_system_requirements=minimum_system_requirements,
            optimal_system_requirements=optimal_system_requirements,
            memory_management=memory_management,
            performance_optimization=performance_optimization,
            detected_gpu_memory=detected_gpu_memory,
            detected_system_memory=detected_system_memory,
            system_performance_class=system_performance_class,
            optimization_recommendations=optimization_recommendations,
            model_dir=model_dir,
            log_dir=log_dir,
            config_dir=config_dir,
            data_dir=data_dir,
            checkpoint_dir=checkpoint_dir,
            results_dir=results_dir,
            debug=debug,
            verbose=verbose,
            random_seed=random_seed,
            reproducible=reproducible,
            parallel_processing=parallel_processing,
            max_workers=max_workers,
            export_onnx=export_onnx,
            non_interactive=non_interactive,
            cuda_optimizations=cuda_optimizations,
            onnx_export=onnx_export,
            distributed_training=distributed_training,
            python_executable=python_executable,
            working_directory=working_directory,
            environment_health=environment_health,
            available_presets=available_presets,
            current_preset=current_preset,
            current_override=current_override,
            override_rules=override_rules,
            preset_configs=preset_configs,
            custom_presets_available=custom_presets_available,
            auto_apply=auto_apply,
            validate_compatibility=validate_compatibility,
            system_recommended_preset=system_recommended_preset,
            preset_compatibility=preset_compatibility,
            hpo_enabled=hpo_enabled,
            hpo_strategy=hpo_strategy,
            study_name=study_name,
            direction=direction,
            n_trials=n_trials,
            timeout=timeout,
            sampler=sampler,
            pruner=pruner,
            objective_metric=objective_metric,
            optimization_space=optimization_space,
            hpo_early_stopping=hpo_early_stopping,
            timeout_seconds=timeout_seconds,
            trial_epochs=trial_epochs,
            trial_patience=trial_patience,
            cleanup_trials=cleanup_trials,
            generate_plots=generate_plots,
            search_space=search_space,
            hpo_sampler=hpo_sampler,
            hpo_pruner=hpo_pruner,
            scoring=scoring,
            storage=storage,
            cross_validation=cross_validation,
            metrics=metrics,
            validation_frequency=validation_frequency,
            save_validation_results=save_validation_results,
            detailed_metrics=detailed_metrics,
            robustness_testing=robustness_testing,
            performance_benchmarking=performance_benchmarking,
            confidence_intervals=confidence_intervals,
            experimental_features=experimental_features,
            experimental_settings=experimental_settings,
            description=description,
            version=version,
            config_version=config_version,
            config_type=config_type,
            created=created,
            last_modified=last_modified,
            preset_used=preset_used,
            recommended_hardware=recommended_hardware,
            compatibility=compatibility,
            system_info=system_info,
            validation_info=validation_info,
            config_loaded_at=config_loaded_at,
            config_source=config_source,
            runtime_id=runtime_id,
            process_id=process_id,
            system_analysis_completed=system_analysis_completed,
            system_performance_score=system_performance_score,
            system_class=system_class,
            optimizations_applied=optimizations_applied,
            resource_status=resource_status,
            system_warnings=system_warnings,
            recommendations=recommendations,
            configuration_health=configuration_health,
            **kwargs
        )
        
        # Extract processed data from centralized initialization
        self.config = init_data['config']
        params = init_data['processed_params']
        self.device = init_data['device']
        self.mixed_precision = init_data['mixed_precision']
        self.mixed_precision_requested = init_data['mixed_precision_requested']
        self.initialization_timestamp = init_data['initialization_timestamp']
        #self.preset_name = init_data['preset_name']
        # PROPERLY STORE PRESET NAME WITH FALLBACKS
        self.preset_name = init_data.get('preset_name') or preset
        #self.preset_name = init_data['preset_name'] or preset
        
        # If still no preset name, try to extract from config
        if not self.preset_name:
            self.preset_name = (
                self.config.get('metadata', {}).get('preset_used') or
                self.config.get('presets', {}).get('current_preset') or
                self.config.get('runtime', {}).get('applied_preset') or
                "Custom"
            )
        
        # ENSURE PRESET NAME IS STORED IN CONFIG FOR CONSISTENCY
        if self.preset_name and self.preset_name != "Custom":
            if 'metadata' not in self.config:
                self.config['metadata'] = {}
            self.config['metadata']['preset_used'] = self.preset_name
            
            if 'presets' not in self.config:
                self.config['presets'] = {}
            self.config['presets']['current_preset'] = self.preset_name
        
        training_config = init_data['training_config']
        
        # Set instance attributes from processed parameters
        self.input_dim = params['input_dim']
        self.encoding_dim = params['encoding_dim']
        self.hidden_dims = params['hidden_dims']
        self.dropout_rates = params['dropout_rates']
        self.activation = params['activation']
        self.activation_param = params['activation_param']
        self.normalization = params['normalization']
        self.use_batch_norm = params['use_batch_norm']
        self.use_layer_norm = params['use_layer_norm']
        self.bias = params['bias']
        self.weight_init = params['weight_init']
        self.skip_connection = params['skip_connection']
        self.residual_blocks = params['residual_blocks']
        self.use_attention = params['use_attention']
        self.model_type = params['model_type']
        self.min_features = params['min_features']
        self.legacy_mode = params['legacy_mode']
        self.num_models = params['num_models']
        self.diversity_factor = params['diversity_factor']
        
        # Build ensemble of diverse autoencoder models
        self._build_ensemble()
        
        # Setup training components if specified
        self._setup_training_components()
        
        # Setup monitoring and logging
        self._setup_monitoring()
        
        # Move to device
        self.to(self.device)
        
        # Log successful initialization
        if hasattr(globals().get('display_model_initialization_summary'), '__call__'):
            try:
                architecture_info = {
                    'hidden_dims': self.hidden_dims,
                    'encoding_dim': self.encoding_dim,
                    'dropout_rates': self.dropout_rates,
                    'activation': self.activation,
                    'normalization': self.normalization
                }
                
                ensemble_info = {
                    'num_models': self.num_models,
                    'model_types': [type(model).__name__ for model in self.models] if hasattr(self, 'models') else [],
                    'diversity_factor': self.diversity_factor,
                    'actual_models': len(self.models) if hasattr(self, 'models') else 0,
                    'use_attention': self.use_attention,
                    'residual_blocks': self.residual_blocks,
                    'skip_connections': self.skip_connection,
                    'legacy_mode': self.legacy_mode
                }
                
                display_model_initialization_summary(
                    model_instance=self,
                    model_type='AutoencoderEnsemble',
                    input_dim=self.input_dim,
                    architecture_info=architecture_info,
                    device=self.device,
                    mixed_precision=self.mixed_precision,
                    preset_name=self.preset_name,
                    training_config=training_config,
                    ensemble_info=ensemble_info
                )
            except Exception as e:
                logger.debug(f"Could not display initialization summary: {e}")
    
    def _build_ensemble(self) -> None:
        """Build the ensemble of diverse autoencoder models using centralized factory pattern."""
        try:
            self.models = nn.ModuleList()
            successful_models = 0
            
            # Prepare base configuration for ensemble members
            base_config = self.config.copy()
            
            for i in range(self.num_models):
                try:
                    # Calculate diversity parameters
                    diversity_offset = (i - self.num_models // 2) * self.diversity_factor
                    encoding_dim_variant = max(8, int(self.encoding_dim * (1 + diversity_offset * 0.5)))
                    
                    # Create diverse hidden layer configurations
                    hidden_diversity = 1 + diversity_offset * 0.3
                    hidden_dims_variant = [max(16, int(dim * hidden_diversity)) for dim in self.hidden_dims]
                    
                    # Adjust dropout rates with diversity
                    dropout_diversity = max(0.05, min(0.4, diversity_offset * 0.1))
                    dropout_rates_variant = [max(0.05, min(0.5, rate + dropout_diversity)) for rate in self.dropout_rates]
                    
                    # Different architectures for diversity
                    if i % 3 == 0:
                        # Enhanced architecture
                        model_type_variant = 'EnhancedAutoencoder'
                        #model_type_variant = EnhancedAutoencoder
                        model_config = base_config.copy()
                    elif i % 3 == 1:
                        # Enhanced with different features
                        model_type_variant = 'EnhancedAutoencoder'
                        #model_type_variant = EnhancedAutoencoder
                        model_config = base_config.copy()
                        model_config.setdefault('model', {}).update({
                            'use_attention': not self.legacy_mode and (i % 2 == 0),
                            'residual_blocks': not self.legacy_mode and (i % 2 == 1),
                            'skip_connection': i % 2 == 0,
                            'normalization': 'batch' if i % 2 == 0 else 'layer'
                        })
                    else:
                        # Simple architecture for diversity
                        model_type_variant = 'SimpleAutoencoder'
                        #model_type_variant = SimpleAutoencoder
                        model_config = base_config.copy()
                        model_config.setdefault('model', {}).update({
                            'use_attention': False,
                            'residual_blocks': False
                        })
                    
                    # Update model configuration with variants
                    model_config['model'].update({
                        'encoding_dim': encoding_dim_variant,
                        'hidden_dims': hidden_dims_variant,
                        'dropout_rates': dropout_rates_variant,
                        'activation_param': max(0.1, self.activation_param + diversity_offset * 0.1),
                        'model_type': model_type_variant
                    })
                    
                    # Create ensemble member using factory pattern
                    try:
                        model = create_model_instance(
                            model_type=model_type_variant,
                            input_dim=self.input_dim,
                            config=model_config
                        )
                        # model = model_type_variant(
                        #     input_dim=self.input_dim,
                        #     config=model_config
                        # )
                    except Exception as factory_error:
                        # Fallback to direct instantiation if factory fails
                        logger.debug(f"Factory creation failed for model {i}, using direct instantiation: {factory_error}")
                        if model_type_variant == 'EnhancedAutoencoder':
                            model = EnhancedAutoencoder(input_dim=self.input_dim, config=model_config)
                        else:
                            model = SimpleAutoencoder(input_dim=self.input_dim, config=model_config)
                    
                    # Move to device
                    model.to(self.device)
                    
                    self.models.append(model)
                    successful_models += 1
                    
                    logger.debug(f"Ensemble model {i} ({model_type_variant}) created successfully: "
                                f"encoding_dim={encoding_dim_variant}, hidden_dims={hidden_dims_variant}")
                    
                except Exception as e:
                    logger.error(f"Failed to create ensemble model {i}: {e}")
                    
                    # Try to create a simple fallback model
                    try:
                        fallback_config = {
                            'model': {
                                'encoding_dim': max(4, self.encoding_dim // 2),
                                'hidden_dims': [max(32, self.hidden_dims[0] // 2)],
                                'dropout_rates': [0.2],
                                'model_type': 'SimpleAutoencoder',
                                'use_attention': False,
                                'residual_blocks': False,
                                'skip_connection': False,
                                'legacy_mode': True
                            },
                            'training': {'mixed_precision': self.mixed_precision},
                            'hardware': {'device': str(self.device)},
                            'system': self.config.get('system', {})
                        }
                        
                        fallback_model = SimpleAutoencoder(
                            input_dim=self.input_dim,
                            config=fallback_config
                        )
                        fallback_model.to(self.device)
                        
                        self.models.append(fallback_model)
                        successful_models += 1
                        logger.warning(f"Created fallback SimpleAutoencoder for ensemble position {i}")
                        
                    except Exception as fallback_error:
                        logger.error(f"Failed to create fallback model for position {i}: {fallback_error}")
            
            # Validate that at least one model was created successfully
            if successful_models == 0:
                raise RuntimeError("Failed to create any ensemble models")
            
            if successful_models < self.num_models:
                logger.warning(f"Only {successful_models} out of {self.num_models} ensemble models created successfully")
                self.num_models = successful_models  # Update to reflect actual count
            
        except Exception as e:
            logger.error(f"Failed to build ensemble: {e}")
            raise RuntimeError(f"Ensemble construction failed: {e}")
    
    def _setup_training_components(self) -> None:
        """Setup training components for ensemble members."""
        training_config = self.config.get('training', {})
        
        if not training_config or not hasattr(self, 'models'):
            return
        
        # Setup ensemble-level training components
        all_parameters = []
        for model in self.models:
            all_parameters.extend(list(model.parameters()))
        
        if not all_parameters:
            logger.warning("No parameters found in ensemble models")
            return
        
        # Setup optimizer for entire ensemble
        optimizer_type = training_config.get('optimizer', 'AdamW').lower()
        learning_rate = training_config.get('learning_rate', 0.001)
        weight_decay = training_config.get('weight_decay', 1e-4)
        
        try:
            if optimizer_type == 'adam':
                adam_betas = training_config.get('adam_betas', (0.9, 0.999))
                adam_eps = training_config.get('adam_eps', 1e-8)
                self.optimizer = torch.optim.Adam(
                    all_parameters,
                    lr=learning_rate,
                    betas=adam_betas,
                    eps=adam_eps,
                    weight_decay=weight_decay
                )
            elif optimizer_type == 'adamw':
                adam_betas = training_config.get('adam_betas', (0.9, 0.999))
                adam_eps = training_config.get('adam_eps', 1e-8)
                self.optimizer = torch.optim.AdamW(
                    all_parameters,
                    lr=learning_rate,
                    betas=adam_betas,
                    eps=adam_eps,
                    weight_decay=weight_decay
                )
            elif optimizer_type == 'sgd':
                self.optimizer = torch.optim.SGD(
                    all_parameters,
                    lr=learning_rate,
                    momentum=0.9,
                    weight_decay=weight_decay
                )
            else:
                logger.warning(f"Unknown optimizer '{optimizer_type}', using AdamW")
                self.optimizer = torch.optim.AdamW(all_parameters, lr=learning_rate, weight_decay=weight_decay)
        except Exception as e:
            logger.error(f"Failed to setup ensemble optimizer: {e}")
            self.optimizer = None
        
        # Setup scheduler
        scheduler_type = training_config.get('scheduler')
        scheduler_params = training_config.get('scheduler_params', {})
        
        if scheduler_type and hasattr(self, 'optimizer') and self.optimizer:
            try:
                if scheduler_type == 'ReduceLROnPlateau':
                    self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                        self.optimizer,
                        mode=scheduler_params.get('mode', 'min'),
                        factor=scheduler_params.get('factor', 0.5),
                        patience=scheduler_params.get('patience', 10),
                        min_lr=scheduler_params.get('min_lr', 1e-6)
                    )
                elif scheduler_type == 'StepLR':
                    self.scheduler = torch.optim.lr_scheduler.StepLR(
                        self.optimizer,
                        step_size=scheduler_params.get('step_size', 30),
                        gamma=scheduler_params.get('gamma', 0.1)
                    )
                else:
                    logger.warning(f"Scheduler '{scheduler_type}' not implemented for ensemble")
                    self.scheduler = None
            except Exception as e:
                logger.error(f"Failed to setup ensemble scheduler: {e}")
                self.scheduler = None
        else:
            self.scheduler = None
        
        # Setup loss function
        self.loss_fn = nn.MSELoss()
        
        # Setup scaler for mixed precision
        if self.mixed_precision:
            #self.scaler = torch.cuda.amp.GradScaler()
            #self.scaler = torch.amp.GradScaler()
            device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
            self.scaler = torch.amp.GradScaler(device_type=device_type)
        
        logger.debug(f"Setup ensemble training components: {optimizer_type} optimizer, {scheduler_type} scheduler")
    
    def _setup_monitoring(self) -> None:
        """Setup monitoring and logging based on configuration."""
        monitoring_config = self.config.get('monitoring', {})
        
        if not monitoring_config:
            return
        
        # Setup metrics tracking
        self.metrics_to_track = monitoring_config.get('metrics_to_track', ['loss', 'reconstruction_error', 'ensemble_variance'])
        self.metrics_frequency = monitoring_config.get('metrics_frequency', 10)
        self.save_best_model = monitoring_config.get('save_best_model', True)
        
        # Training history for ensemble
        self.training_history = {metric: [] for metric in self.metrics_to_track}
        self.best_loss = float('inf')
        self.patience_counter = 0
        
        # Checkpoint settings
        self.checkpoint_frequency = monitoring_config.get('checkpoint_frequency', 10)
        self.save_checkpoints = monitoring_config.get('save_checkpoints', True)
        
        logger.debug(f"Setup ensemble monitoring: tracking {len(self.metrics_to_track)} metrics")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Enhanced forward pass with mixed precision support and robust error handling."""
        # Input validation
        if not isinstance(x, torch.Tensor):
            raise ValueError(f"Expected torch.Tensor input, got {type(x)}")
        
        if x.dim() != 2:
            raise ValueError(f"Expected 2D input tensor (batch_size, input_dim), got shape {x.shape}")
        
        if x.size(-1) != self.input_dim:
            raise ValueError(f"Input feature dimension {x.size(-1)} doesn't match expected {self.input_dim}")
        
        try:
            #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
            device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
            with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
                reconstructions = []
                successful_outputs = 0
                
                for i, model in enumerate(self.models):
                    try:
                        output = model(x)
                        reconstructions.append(output)
                        successful_outputs += 1
                    except Exception as e:
                        logger.warning(f"Ensemble model {i} failed during forward pass: {e}")
                        continue
                
                if successful_outputs == 0:
                    raise RuntimeError("All ensemble models failed during forward pass")
                
                if successful_outputs < len(self.models):
                    logger.debug(f"Only {successful_outputs} out of {len(self.models)} ensemble models "
                                f"produced outputs")
                
                # Average the successful reconstructions
                ensemble_output = torch.stack(reconstructions).mean(dim=0)
                return ensemble_output
                
        except Exception as e:
            logger.error(f"AutoencoderEnsemble forward pass failed: {e}")
            raise RuntimeError(f"Ensemble forward pass error: {e}")
    
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode input data using ensemble average of latent representations."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            encodings = []
            for model in self.models:
                try:
                    encoding = model.encode(x)
                    encodings.append(encoding)
                except Exception as e:
                    logger.warning(f"Model encoding failed: {e}")
                    continue
            
            if not encodings:
                raise RuntimeError("All ensemble models failed during encoding")
            
            return torch.stack(encodings).mean(dim=0)
    
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode latent representation using ensemble average."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            decodings = []
            for model in self.models:
                try:
                    decoding = model.decode(z)
                    decodings.append(decoding)
                except Exception as e:
                    logger.warning(f"Model decoding failed: {e}")
                    continue
            
            if not decodings:
                raise RuntimeError("All ensemble models failed during decoding")
            
            return torch.stack(decodings).mean(dim=0)
    
    @property
    def original_mixed_precision_setting(self) -> bool:
        """Returns the originally requested mixed precision setting."""
        return getattr(self, 'mixed_precision_requested', True)
    
    def get_config(self) -> Dict[str, Any]:
        """Get comprehensive ensemble configuration."""
        config = self.config.copy()
        
        # Get individual model configs
        model_configs = []
        for i, model in enumerate(self.models):
            try:
                model_configs.append(model.get_config())
            except Exception as e:
                logger.warning(f"Failed to get config for ensemble model {i}: {e}")
                model_configs.append({"error": str(e), "model_index": i})
        
        # Add runtime information
        config.setdefault('runtime', {}).update({
            'model_initialized_at': self.initialization_timestamp,
            'total_parameters': sum(p.numel() for p in self.parameters()),
            'trainable_parameters': sum(p.numel() for p in self.parameters() if p.requires_grad),
            'device': str(self.device),
            'mixed_precision_active': self.mixed_precision,
            'mixed_precision_requested': self.original_mixed_precision_setting,
            'preset_used': self.preset_name,
            'input_dim': self.input_dim,
            'ensemble_size': self.num_models,
            'actual_models': len(self.models),
            'diversity_factor': self.diversity_factor,
            'model_types': [type(model).__name__ for model in self.models],
            'architecture_summary': f"Ensemble({self.num_models}): {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            'centralized_config_used': True,
            'helper_functions_leveraged': True,
            'ensemble_configs': model_configs
        })
        
        return config
    
    def update_config(self, new_config: Dict[str, Any], reinitialize: bool = False) -> None:
        """Update configuration and optionally reinitialize components."""
        old_config = self.config.copy()
        
        try:
            # Update configuration using existing helper
            self.config = deep_update(self.config, new_config)
            
            # Reinitialize components if requested
            if reinitialize:
                logger.info("Reinitializing ensemble components due to config update")
                
                # Check if ensemble architecture changed
                ensemble_changed = any(
                    key in new_config.get('model', {}) 
                    for key in ['num_models', 'diversity_factor', 'encoding_dim', 'hidden_dims']
                )
                
                if ensemble_changed:
                    logger.warning("Ensemble architecture parameters changed - full rebuild required")
                    raise ValueError("Ensemble architecture changes require creating a new instance")
                
                # Update training components
                if 'training' in new_config:
                    self._setup_training_components()
                
                # Update device configuration
                if 'hardware' in new_config:
                    old_device = self.device
                    device_setting = new_config['hardware'].get('device')
                    if device_setting:
                        if device_setting == 'auto':
                            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                        elif device_setting in ['cpu', 'cuda']:
                            self.device = torch.device(device_setting)
                        
                        if self.device != old_device:
                            # Move all models to new device
                            for model in self.models:
                                model.to(self.device)
                            logger.info(f"Moved ensemble from {old_device} to {self.device}")
                
                # Update individual model configs
                for model in self.models:
                    try:
                        model.update_config(new_config, reinitialize=False)
                    except Exception as e:
                        logger.warning(f"Failed to update model config: {e}")
            
            logger.debug("Configuration updated successfully using centralized helper")
            
        except Exception as e:
            # Restore old configuration on error
            self.config = old_config
            logger.error(f"Configuration update failed, restored previous config: {e}")
            raise
    
    def save_model(self, path: str, include_config: bool = True) -> None:
        """Save ensemble state and configuration."""
        save_dict = {
            'model_state_dict': self.state_dict(),
            'model_class': self.__class__.__name__,
            'input_dim': self.input_dim,
            'ensemble_info': {
                'num_models': self.num_models,
                'diversity_factor': self.diversity_factor,
                'encoding_dim': self.encoding_dim,
                'hidden_dims': self.hidden_dims,
                'dropout_rates': self.dropout_rates,
                'model_types': [type(model).__name__ for model in self.models],
                'use_attention': self.use_attention,
                'residual_blocks': self.residual_blocks,
                'skip_connection': self.skip_connection,
                'legacy_mode': self.legacy_mode
            },
            'centralized_config_used': True
        }
        
        if include_config:
            save_dict['config'] = self.config
        
        if hasattr(self, 'optimizer') and self.optimizer:
            save_dict['optimizer_state_dict'] = self.optimizer.state_dict()
        
        if hasattr(self, 'scheduler') and self.scheduler is not None:
            save_dict['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(save_dict, path)
        logger.debug(f"AutoencoderEnsemble saved to {path} (centralized config)")
    
    @classmethod
    def load_model(cls, path: str, **kwargs):
        """Load ensemble from saved state."""
        checkpoint = torch.load(path, map_location='cpu')
        
        # Extract parameters
        input_dim = checkpoint['input_dim']
        config = checkpoint.get('config', {})
        ensemble_info = checkpoint.get('ensemble_info', {})
        
        # Merge ensemble info into config
        if ensemble_info:
            config.setdefault('model', {}).update(ensemble_info)
        
        # Create ensemble instance using centralized config if available
        if checkpoint.get('centralized_config_used', False):
            try:
                ensemble = create_model_instance('AutoencoderEnsemble', input_dim, config, **kwargs)
                #ensemble = cls(input_dim=input_dim, config=config, **kwargs)
                #ensemble = create_model_instance(model_type='AutoencoderEnsemble', input_dim=input_dim, config=config, **kwargs)
            except:
                # Fallback to direct instantiation
                ensemble = cls(input_dim=input_dim, config=config, **kwargs)
                #ensemble = create_model_instance(model_type='AutoencoderEnsemble', input_dim=input_dim, config=config, **kwargs)
        else:
            ensemble = cls(input_dim=input_dim, config=config, **kwargs)
        
        # Load state dict
        ensemble.load_state_dict(checkpoint['model_state_dict'])
        
        # Load optimizer and scheduler if available
        if hasattr(ensemble, 'optimizer') and 'optimizer_state_dict' in checkpoint:
            ensemble.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if hasattr(ensemble, 'scheduler') and ensemble.scheduler is not None and 'scheduler_state_dict' in checkpoint:
            ensemble.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        logger.debug(f"AutoencoderEnsemble loaded from {path}")
        return ensemble
    
    def get_model_summary(self) -> str:
        """Get detailed ensemble summary."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        # Model type distribution
        model_types = [type(model).__name__ for model in self.models]
        type_counts = {}
        for model_type in model_types:
            type_counts[model_type] = type_counts.get(model_type, 0) + 1
        
        # Enhanced features summary
        enhanced_features = []
        if self.use_attention:
            enhanced_features.append("Multi-Head Attention")
        if self.residual_blocks:
            enhanced_features.append("Residual Blocks")
        if self.skip_connection:
            enhanced_features.append("Skip Connections")
        if self.normalization:
            enhanced_features.append(f"{self.normalization.title()} Normalization")
        
        summary = [
            f"AutoencoderEnsemble Summary (Factory Pattern)",
            f"{'-'*40}",
            f"Ensemble Size: {self.num_models} models",
            f"Model Distribution: {dict(type_counts)}",
            f"Preset: {self.preset_name or 'Custom'}",
            f"Base Architecture: {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            f"Diversity Factor: {self.diversity_factor}",
            f"Base Activation: {self.activation}",
            f"Enhanced Features: {', '.join(enhanced_features) if enhanced_features else 'None'}",
            f"Legacy Mode: {self.legacy_mode}",
            f"Device: {self.device}",
            f"Mixed Precision: {self.mixed_precision}",
            f"Parameters: {total_params:,} total, {trainable_params:,} trainable",
            f"Memory Usage: ~{total_params * 4 / (1024**2):.1f} MB (FP32)",
            f"Factory Pattern: Enabled",
            f"Centralized Config: Enabled",
            f"Helper Functions: Leveraged",
            f"{'-'*40}"
        ]
        
        return '\n'.join(summary)
    
    def __repr__(self) -> str:
        """String representation of the ensemble."""
        model_types = [type(model).__name__ for model in self.models]
        type_counts = {}
        for model_type in model_types:
            type_counts[model_type] = type_counts.get(model_type, 0) + 1
        
        enhanced_features = []
        if self.use_attention:
            enhanced_features.append("attention")
        if self.residual_blocks:
            enhanced_features.append("residual")
        if self.skip_connection:
            enhanced_features.append("skip")
        
        features_str = f", features=[{','.join(enhanced_features)}]" if enhanced_features else ""
        
        return (f"AutoencoderEnsemble(input_dim={self.input_dim}, "
                f"num_models={self.num_models}, "
                f"encoding_dim={self.encoding_dim}, "
                f"diversity_factor={self.diversity_factor}, "
                f"model_types={dict(type_counts)}, "
                f"device={self.device}, "
                f"preset='{self.preset_name or 'Custom'}', "
                f"centralized_config=True, "
                f"factory_pattern=True{features_str})")

def load_autoencoder_model(
    model_path: Path,
    input_dim: Optional[int] = None,
    encoding_dim: Optional[int] = None,
    config: Optional[Dict] = None,
    model_type: Optional[str] = None,
    **kwargs
) -> Union[SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble]:
    """
    Load autoencoder with automatic architecture detection and comprehensive config handling.
    
    This function supports loading all three autoencoder types (SimpleAutoencoder,
    EnhancedAutoencoder, AutoencoderEnsemble) with full configuration compatibility.
    
    Args:
        model_path: Path to the saved model file
        input_dim: Expected input dimension (optional, will be inferred if None)
        encoding_dim: Default encoding dimension if not found in saved model
        config: Optional configuration dictionary for model parameters
        model_type: Force specific model type ('SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble')
        **kwargs: Additional parameters passed to model constructors
        
    Returns:
        Loaded model instance with proper configuration
        
    Raises:
        FileNotFoundError: If model file doesn't exist
        RuntimeError: If model loading fails
        ValueError: If architecture parameters are invalid
    """
    from datetime import datetime
    
    # Validate input path
    if not isinstance(model_path, Path):
        model_path = Path(model_path)
    
    if not model_path.exists():
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    logger.info(f"Loading autoencoder model from: {model_path}")
    
    try:
        # Load checkpoint with error handling
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
        except Exception as e:
            logger.error(f"Failed to load checkpoint from {model_path}: {e}")
            raise RuntimeError(f"Failed to load model file: {str(e)}")
        
        # Handle different checkpoint formats
        if isinstance(checkpoint, dict):
            state_dict = checkpoint.get('model_state_dict', checkpoint)
            saved_config = checkpoint.get('config', {})
            saved_architecture = checkpoint.get('architecture', {})
            saved_ensemble_info = checkpoint.get('ensemble_info', {})
            saved_model_class = checkpoint.get('model_class', None)
            saved_input_dim = checkpoint.get('input_dim', None)
        else:
            # Legacy format - assume the checkpoint is the state dict
            state_dict = checkpoint
            saved_config = {}
            saved_architecture = {}
            saved_ensemble_info = {}
            saved_model_class = None
            saved_input_dim = None
        
        # Merge configurations with precedence: kwargs > config > saved_config > defaults
        final_config = {}
        
        # Start with default configuration
        try:
            default_config = get_current_config() if 'get_current_config' in globals() else {}
            final_config.update(default_config)
        except Exception as e:
            logger.warning(f"Could not load default config: {e}")
        
        # Apply saved configuration
        if saved_config:
            final_config.update(saved_config)
            logger.debug("Applied saved model configuration")
        
        # Apply provided configuration
        if config:
            final_config.update(config)
            logger.debug("Applied provided configuration")
        
        # Apply kwargs
        if kwargs:
            # Organize kwargs into appropriate sections
            for key, value in kwargs.items():
                # Try to place in appropriate config section
                if key in ['input_dim', 'encoding_dim', 'hidden_dims', 'dropout_rates', 
                          'activation', 'normalization', 'num_models', 'diversity_factor']:
                    final_config.setdefault('model', {})[key] = value
                elif key in ['batch_size', 'learning_rate', 'epochs', 'optimizer']:
                    final_config.setdefault('training', {})[key] = value
                elif key in ['device', 'mixed_precision']:
                    final_config.setdefault('hardware', {})[key] = value
                else:
                    # Put in model section by default
                    final_config.setdefault('model', {})[key] = value
        
        # Determine input dimension with multiple fallback methods
        final_input_dim = input_dim
        
        if final_input_dim is None:
            # Try to get from saved model
            final_input_dim = saved_input_dim
        
        if final_input_dim is None:
            # Try to get from config
            final_input_dim = final_config.get('data', {}).get('features') or \
                             final_config.get('model', {}).get('input_dim')
        
        if final_input_dim is None:
            # Try to infer from state dict
            try:
                # Look for encoder input layer in various possible locations
                encoder_patterns = [
                    'encoder.0.weight',           # SimpleAutoencoder
                    'encoder.net.0.weight',       # Alternative format
                    'models.0.encoder.0.weight',  # AutoencoderEnsemble
                ]
                
                for pattern in encoder_patterns:
                    if pattern in state_dict:
                        final_input_dim = state_dict[pattern].shape[1]
                        logger.debug(f"Inferred input_dim={final_input_dim} from {pattern}")
                        break
                
                # If not found in patterns, search dynamically
                if final_input_dim is None:
                    for key, tensor in state_dict.items():
                        if 'encoder' in key and 'weight' in key and len(tensor.shape) == 2:
                            final_input_dim = tensor.shape[1]
                            logger.debug(f"Inferred input_dim={final_input_dim} from {key}")
                            break
                            
            except Exception as e:
                logger.warning(f"Failed to infer input_dim from state_dict: {e}")
        
        if final_input_dim is None:
            raise ValueError("Could not determine input_dim. Please provide it explicitly.")
        
        # Determine model type with multiple detection methods
        detected_model_type = model_type or saved_model_class
        
        if detected_model_type is None:
            # Detect from state dict structure
            if any(k.startswith('models.') for k in state_dict.keys()):
                detected_model_type = 'AutoencoderEnsemble'
                logger.debug("Detected AutoencoderEnsemble from state dict keys")
            elif any(k.startswith('attention.') for k in state_dict.keys()) or \
                 any('ResidualBlock' in str(type(v)) for v in state_dict.values() if hasattr(v, 'dtype')):
                detected_model_type = 'EnhancedAutoencoder'
                logger.debug("Detected EnhancedAutoencoder from state dict structure")
            else:
                # Check complexity - simple models have fewer layers
                encoder_layers = [k for k in state_dict.keys() if 'encoder' in k and 'weight' in k]
                # Simple encoder typically has 2 linear layers
                if len(encoder_layers) <= 4:
                    detected_model_type = 'SimpleAutoencoder'
                    logger.debug("Detected SimpleAutoencoder from layer count")
                else:
                    detected_model_type = 'EnhancedAutoencoder'
                    logger.debug("Detected EnhancedAutoencoder from layer complexity")
        
        logger.info(f"Loading {detected_model_type} with input_dim={final_input_dim}")
        
        # Extract architecture parameters with fallbacks
        model_config = final_config.setdefault('model', {})
        
        # Set input_dim in config
        model_config['input_dim'] = final_input_dim
        final_config.setdefault('data', {})['features'] = final_input_dim
        
        # Handle encoding_dim
        final_encoding_dim = encoding_dim
        if final_encoding_dim is None:
            final_encoding_dim = saved_architecture.get('encoding_dim') or \
                                saved_ensemble_info.get('encoding_dim') or \
                                model_config.get('encoding_dim')
        
        if final_encoding_dim is None:
            # Try to infer from state dict
            try:
                # Look for encoder output or decoder input
                # SimpleAutoencoder final layer
                if 'encoder.2.weight' in state_dict:
                    final_encoding_dim = state_dict['encoder.2.weight'].shape[0]
                # Decoder input
                elif 'decoder.0.weight' in state_dict:
                    final_encoding_dim = state_dict['decoder.0.weight'].shape[1]
                else:
                    # Search dynamically
                    for key, tensor in state_dict.items():
                        if 'decoder' in key and 'weight' in key and len(tensor.shape) == 2:
                            final_encoding_dim = tensor.shape[1]
                            break
            except Exception as e:
                logger.warning(f"Failed to infer encoding_dim: {e}")
        
        if final_encoding_dim is None:
            # Use defaults based on model type
            if detected_model_type == 'SimpleAutoencoder':
                final_encoding_dim = max(4, final_input_dim // 2)
            elif detected_model_type == 'EnhancedAutoencoder':
                final_encoding_dim = max(8, final_input_dim // 3)
            else:  # AutoencoderEnsemble
                final_encoding_dim = max(12, final_input_dim // 4)
        
        model_config['encoding_dim'] = final_encoding_dim
        
        # Extract other architecture parameters
        if saved_architecture:
            for key, value in saved_architecture.items():
                if key not in model_config:
                    model_config[key] = value
        
        if saved_ensemble_info:
            for key, value in saved_ensemble_info.items():
                if key not in model_config:
                    model_config[key] = value
        
        # Set default architecture parameters if not present
        architecture_defaults = {
            'SimpleAutoencoder': {
                'hidden_dims': [max(32, final_input_dim // 2)],
                'dropout_rates': [0.2],
                'activation': 'leaky_relu',
                'activation_param': 0.2,
                'normalization': None,
                'use_batch_norm': False,
                'use_layer_norm': False,
                'skip_connection': False,
                'residual_blocks': False,
                'use_attention': False,
                'model_type': 'SimpleAutoencoder'
            },
            'EnhancedAutoencoder': {
                'hidden_dims': [256, 128, 64],
                'dropout_rates': [0.2, 0.15, 0.1],
                'activation': 'leaky_relu',
                'activation_param': 0.2,
                'normalization': 'batch',
                'use_batch_norm': True,
                'use_layer_norm': False,
                'skip_connection': True,
                'residual_blocks': True,
                'use_attention': True,
                'model_type': 'EnhancedAutoencoder',
                'legacy_mode': False
            },
            'AutoencoderEnsemble': {
                'hidden_dims': [192, 96, 48],
                'dropout_rates': [0.25, 0.2, 0.15],
                'activation': 'leaky_relu',
                'activation_param': 0.2,
                'normalization': 'batch',
                'use_batch_norm': True,
                'use_layer_norm': False,
                'skip_connection': True,
                'residual_blocks': True,
                'use_attention': True,
                'num_models': 3,
                'diversity_factor': 0.3,
                'model_type': 'AutoencoderEnsemble'
            }
        }
        
        defaults = architecture_defaults.get(detected_model_type, architecture_defaults['EnhancedAutoencoder'])
        for key, default_value in defaults.items():
            if key not in model_config:
                model_config[key] = default_value
        
        # Add runtime metadata
        final_config.setdefault('runtime', {}).update({
            'loaded_at': datetime.now().isoformat(),
            'loaded_from': str(model_path),
            'detected_model_type': detected_model_type,
            'inferred_input_dim': final_input_dim,
            'inferred_encoding_dim': final_encoding_dim
        })
        
        # Create model instance based on detected type
        logger.info(f"Creating {detected_model_type} instance")
        
        try:
            if detected_model_type == 'SimpleAutoencoder':
                model = SimpleAutoencoder(
                    input_dim=final_input_dim,
                    config=final_config
                )
            elif detected_model_type == 'EnhancedAutoencoder':
                model = EnhancedAutoencoder(
                    input_dim=final_input_dim,
                    config=final_config
                )
            elif detected_model_type == 'AutoencoderEnsemble':
                model = AutoencoderEnsemble(
                    input_dim=final_input_dim,
                    config=final_config
                )
            else:
                raise ValueError(f"Unknown model type: {detected_model_type}")
                
        except Exception as e:
            logger.error(f"Failed to create {detected_model_type} instance: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            # Try fallback to SimpleAutoencoder
            logger.warning("Attempting fallback to SimpleAutoencoder")
            try:
                fallback_config = final_config.copy()
                fallback_config['model'].update({
                    'model_type': 'SimpleAutoencoder',
                    'hidden_dims': [max(32, final_input_dim // 2)],
                    'dropout_rates': [0.2],
                    'use_attention': False,
                    'residual_blocks': False,
                    'skip_connection': False,
                    'normalization': None
                })
                
                model = SimpleAutoencoder(
                    input_dim=final_input_dim,
                    config=fallback_config
                )
                logger.info("Successfully created fallback SimpleAutoencoder")
                
            except Exception as fallback_error:
                raise RuntimeError(f"Failed to create model instance and fallback failed: {fallback_error}")
        
        # Load state dict with comprehensive error handling
        logger.info("Loading model state dict")
        try:
            # Get current model state dict for comparison
            model_state_dict = model.state_dict()
            
            # Filter state dict to only include compatible keys
            compatible_state_dict = {}
            incompatible_keys = []
            missing_keys = []
            
            for key, tensor in state_dict.items():
                if key in model_state_dict:
                    if tensor.shape == model_state_dict[key].shape:
                        compatible_state_dict[key] = tensor
                    else:
                        incompatible_keys.append(f"{key}: saved={tensor.shape}, model={model_state_dict[key].shape}")
                        logger.warning(f"Shape mismatch for {key}: saved={tensor.shape}, model={model_state_dict[key].shape}")
            
            # Check for missing keys
            for key in model_state_dict:
                if key not in state_dict:
                    missing_keys.append(key)
            
            # Load compatible state dict
            if compatible_state_dict:
                model.load_state_dict(compatible_state_dict, strict=False)
                loaded_keys = len(compatible_state_dict)
                total_keys = len(model_state_dict)
                
                logger.info(f"Loaded {loaded_keys}/{total_keys} parameters successfully")
                
                if incompatible_keys:
                    logger.warning(f"Skipped {len(incompatible_keys)} incompatible keys")
                    if len(incompatible_keys) <= 5:
                        for key_info in incompatible_keys:
                            logger.warning(f"  Incompatible: {key_info}")
                    else:
                        logger.warning(f"  First 5 incompatible keys:")
                        for key_info in incompatible_keys[:5]:
                            logger.warning(f"    {key_info}")
                
                if missing_keys:
                    logger.info(f"Initialized {len(missing_keys)} missing keys with random values")
                    if len(missing_keys) <= 10:
                        logger.debug(f"Missing keys: {missing_keys}")
            else:
                logger.warning("No compatible parameters found - model will use random initialization")
        
        except Exception as e:
            logger.error(f"Failed to load state dict: {e}")
            logger.warning("Model created with random initialization")
        
        # Load optimizer and scheduler states if present and model has them
        if hasattr(model, 'optimizer') and 'optimizer_state_dict' in checkpoint:
            try:
                model.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                logger.debug("Loaded optimizer state")
            except Exception as e:
                logger.warning(f"Failed to load optimizer state: {e}")
        
        if hasattr(model, 'scheduler') and model.scheduler is not None and 'scheduler_state_dict' in checkpoint:
            try:
                model.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                logger.debug("Loaded scheduler state")
            except Exception as e:
                logger.warning(f"Failed to load scheduler state: {e}")
        
        # Set model to evaluation mode
        model.eval()
        
        # Log successful loading summary
        logger.info(f"Successfully loaded {type(model).__name__}")
        logger.info(f"Model summary: input_dim={final_input_dim}, encoding_dim={final_encoding_dim}")
        logger.info(f"Device: {model.device}, Mixed precision: {model.mixed_precision}")
        
        if hasattr(model, 'num_models'):
            logger.info(f"Ensemble size: {model.num_models} models")
        
        return model
        
    except FileNotFoundError:
        raise
    except ValueError:
        raise
    except Exception as e:
        error_msg = f"Failed to load autoencoder model from {model_path}: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        raise RuntimeError(error_msg)

def save_autoencoder_model(
    model: Union[SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble],
    model_path: Path,
    include_config: bool = True,
    include_training_state: bool = True
) -> None:
    """
    Save autoencoder model with comprehensive metadata and configuration.
    
    Args:
        model: Model instance to save
        model_path: Path where to save the model
        include_config: Whether to include full configuration
        include_training_state: Whether to include optimizer and scheduler states
    """
    try:
        model_path = Path(model_path)
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Use the model's built-in save method
        model.save_model(
            str(model_path),
            include_config=include_config
        )
        
        logger.info(f"Model saved successfully to {model_path}")
        
    except Exception as e:
        logger.error(f"Failed to save model to {model_path}: {e}")
        raise RuntimeError(f"Model saving failed: {str(e)}")

# Helper function for backward compatibility
def get_autocast_context(device, mixed_precision=True, enabled=True):
    """
    Get appropriate autocast context for different PyTorch versions and devices.
    
    Args:
        device: torch.device object
        mixed_precision: bool, whether mixed precision is enabled
        enabled: bool, whether autocast should be enabled
    
    Returns:
        Context manager for autocast or nullcontext
    """
    if not mixed_precision or not enabled:
        return nullcontext()
    
    if device.type == 'cuda':
        try:
            # Try new API first (PyTorch 1.10+)
            return torch.cuda.amp.autocast(enabled=True)
        except TypeError:
            try:
                # Fallback for older PyTorch versions
                return torch.cuda.amp.autocast()
            except Exception:
                return nullcontext()
    else:
        try:
            # Try CPU autocast (PyTorch 1.10+)
            return torch.cpu.amp.autocast(enabled=True)
        except (TypeError, AttributeError):
            # CPU autocast not available or different API
            return nullcontext()

def create_autoencoder_from_config(
    config: Dict[str, Any],
    model_type: Optional[str] = None
) -> Union[SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble]:
    """
    Create autoencoder model from configuration dictionary.
    
    Args:
        config: Configuration dictionary
        model_type: Force specific model type
        
    Returns:
        Created model instance
    """
    model_config = config.get('model', {})
    data_config = config.get('data', {})
    
    # Determine input dimension
    input_dim = model_config.get('input_dim') or data_config.get('features', 20)
    
    # Determine model type
    if model_type is None:
        model_type = model_config.get('model_type', 'EnhancedAutoencoder')
    
    try:
        if model_type == 'SimpleAutoencoder':
            return SimpleAutoencoder(input_dim=input_dim, config=config)
        elif model_type == 'EnhancedAutoencoder':
            return EnhancedAutoencoder(input_dim=input_dim, config=config)
        elif model_type == 'AutoencoderEnsemble':
            return AutoencoderEnsemble(input_dim=input_dim, config=config)
        else:
            raise ValueError(f"Unknown model type: {model_type}")
            
    except Exception as e:
        logger.error(f"Failed to create {model_type}: {e}")
        raise RuntimeError(f"Model creation failed: {str(e)}")

def load_and_validate_data(
    # Core Data Parameters
    data_path: Optional[Union[str, Path]] = None,
    artifacts_path: Optional[Union[str, Path]] = None,
    config: Optional[Dict[str, Any]] = None,
    
    # Data Loading Parameters
    use_real_data: Optional[bool] = None,
    data_format: Optional[str] = None,
    encoding: Optional[str] = None,
    delimiter: Optional[str] = None,
    header: Optional[Union[int, str]] = None,
    index_col: Optional[Union[int, str]] = None,
    skiprows: Optional[Union[int, List[int]]] = None,
    nrows: Optional[int] = None,
    chunk_size: Optional[int] = None,
    low_memory: Optional[bool] = None,
    memory_map: Optional[bool] = None,
    
    # Data Validation Parameters
    min_samples: Optional[int] = None,
    max_samples: Optional[int] = None,
    min_features: Optional[int] = None,
    max_features: Optional[int] = None,
    required_columns: Optional[List[str]] = None,
    label_column: Optional[str] = None,
    feature_columns: Optional[List[str]] = None,
    exclude_columns: Optional[List[str]] = None,
    
    # Data Processing Parameters
    normalization: Optional[str] = None,
    scaling_method: Optional[str] = None,
    handle_missing: Optional[str] = None,
    missing_value_strategy: Optional[str] = None,
    outlier_detection: Optional[bool] = None,
    outlier_method: Optional[str] = None,
    outlier_threshold: Optional[float] = None,
    
    # Data Splitting Parameters
    train_split: Optional[float] = None,
    validation_split: Optional[float] = None,
    test_split: Optional[float] = None,
    stratified_split: Optional[bool] = None,
    shuffle: Optional[bool] = None,
    random_state: Optional[int] = None,
    
    # Class Balance Parameters
    balance_classes: Optional[bool] = None,
    balance_method: Optional[str] = None,
    min_class_samples: Optional[int] = None,
    max_class_ratio: Optional[float] = None,
    
    # Feature Engineering Parameters
    feature_selection: Optional[bool] = None,
    feature_selection_method: Optional[str] = None,
    n_features_select: Optional[int] = None,
    correlation_threshold: Optional[float] = None,
    variance_threshold: Optional[float] = None,
    
    # Synthetic Data Parameters
    synthetic_normal_samples: Optional[int] = None,
    synthetic_attack_samples: Optional[int] = None,
    synthetic_generation_method: Optional[str] = None,
    synthetic_noise_level: Optional[float] = None,
    synthetic_seed: Optional[int] = None,
    
    # Performance Parameters
    parallel_loading: Optional[bool] = None,
    n_jobs: Optional[int] = None,
    batch_processing: Optional[bool] = None,
    memory_efficient: Optional[bool] = None,
    cache_data: Optional[bool] = None,
    
    # Security Parameters
    validate_data_integrity: Optional[bool] = None,
    check_data_corruption: Optional[bool] = None,
    anomaly_detection_threshold: Optional[float] = None,
    security_validation: Optional[bool] = None,
    
    # Monitoring Parameters
    verbose: Optional[bool] = None,
    log_level: Optional[str] = None,
    progress_bar: Optional[bool] = None,
    silent: Optional[bool] = None,
    save_statistics: Optional[bool] = None,
    statistics_path: Optional[Union[str, Path]] = None,
    
    # Compatibility Parameters
    legacy_format: Optional[bool] = None,
    backward_compatibility: Optional[bool] = None,
    version_check: Optional[bool] = None,
    
    # Advanced Parameters
    data_quality_checks: Optional[bool] = None,
    statistical_validation: Optional[bool] = None,
    distribution_checks: Optional[bool] = None,
    cross_validation_ready: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_features: Optional[bool] = None,
    advanced_preprocessing: Optional[bool] = None,
    
    **kwargs
) -> Dict[str, Union[np.ndarray, Dict[str, Any]]]:

    # Start timing
    start_time = datetime.now()
    
    # Initialize configuration with comprehensive defaults
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply all parameters to configuration
    final_config = {}
    
    # Merge with existing config
    final_config.update(config)
    
    # Apply individual parameters with intelligent organization
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {'config', 'kwargs', 'start_time', 'datetime', 'traceback', 'hashlib', 'train_test_split', 'StratifiedShuffleSplit',
                       'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'QuantileTransformer','SelectKBest', 'f_classif', 'VarianceThreshold',
                       'SimpleImputer', 'KNNImputer', 'IsolationForest', 'stats', 'joblib', 'os', 'pd', 'np'}
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Organize parameters into logical sections
    param_sections = {
        'data_loading': [
            'use_real_data', 'data_format', 'encoding', 'delimiter', 'header', 'index_col', 'skiprows', 'nrows', 'chunk_size', 'low_memory', 'memory_map'
        ],
        'data_validation': [
            'min_samples', 'max_samples', 'min_features', 'max_features', 'required_columns', 'label_column', 'feature_columns', 'exclude_columns'
        ],
        'data_processing': [
            'normalization', 'scaling_method', 'handle_missing', 'missing_value_strategy', 'outlier_detection', 'outlier_method', 'outlier_threshold'
        ],
        'data_splitting': [
            'train_split', 'validation_split', 'test_split', 'stratified_split', 'shuffle', 'random_state'
        ],
        'class_balance': [
            'balance_classes', 'balance_method', 'min_class_samples', 'max_class_ratio'
        ],
        'feature_engineering': [
            'feature_selection', 'feature_selection_method', 'n_features_select', 'correlation_threshold', 'variance_threshold'
        ],
        'synthetic_data': [
            'synthetic_normal_samples', 'synthetic_attack_samples', 'synthetic_generation_method', 'synthetic_noise_level', 'synthetic_seed'
        ],
        'performance': [
            'parallel_loading', 'n_jobs', 'batch_processing', 'memory_efficient', 'cache_data'
        ],
        'security': [
            'validate_data_integrity', 'check_data_corruption', 'anomaly_detection_threshold', 'security_validation'
        ],
        'monitoring': [
            'verbose', 'log_level', 'progress_bar', 'silent', 'save_statistics', 'statistics_path'
        ],
        'compatibility': [
            'legacy_format', 'backward_compatibility', 'version_check'
        ],
        'advanced': [
            'data_quality_checks', 'statistical_validation', 'distribution_checks', 'cross_validation_ready', 'experimental_features', 'advanced_preprocessing'
        ]
    }
    
    # Apply parameters to appropriate sections
    for section, param_list in param_sections.items():
        section_config = final_config.setdefault(section, {})
        for param in param_list:
            if param in cleaned_params:
                section_config[param] = cleaned_params[param]
    
    # Set up comprehensive defaults
    data_loading_config = final_config.setdefault('data_loading', {})
    data_validation_config = final_config.setdefault('data_validation', {})
    data_processing_config = final_config.setdefault('data_processing', {})
    data_splitting_config = final_config.setdefault('data_splitting', {})
    class_balance_config = final_config.setdefault('class_balance', {})
    feature_engineering_config = final_config.setdefault('feature_engineering', {})
    synthetic_data_config = final_config.setdefault('synthetic_data', {})
    performance_config = final_config.setdefault('performance', {})
    security_config = final_config.setdefault('security', {})
    monitoring_config = final_config.setdefault('monitoring', {})
    compatibility_config = final_config.setdefault('compatibility', {})
    advanced_config = final_config.setdefault('advanced', {})
    
    # Handle silent mode - override verbose and progress_bar if silent is True
    silent_mode = monitoring_config.get('silent', False)
    if silent_mode:
        # Force silent mode behavior
        monitoring_config['verbose'] = False
        monitoring_config['progress_bar'] = False
    
    # Set intelligent defaults
    use_real_data = data_loading_config.setdefault('use_real_data', True)
    data_format = data_loading_config.setdefault('data_format', 'csv')
    encoding = data_loading_config.setdefault('encoding', 'utf-8')
    delimiter = data_loading_config.setdefault('delimiter', ',')
    header = data_loading_config.setdefault('header', 0)
    low_memory = data_loading_config.setdefault('low_memory', True)
    
    # Validation parameters
    min_features = data_validation_config.setdefault('min_features', MIN_FEATURES)
    min_samples = data_validation_config.setdefault('min_samples', 100)
    label_column = data_validation_config.setdefault('label_column', 'Label')
    required_columns = data_validation_config.setdefault('required_columns', [label_column])
    
    # Processing parameters
    normalization = data_processing_config.setdefault('normalization', 'standard')
    handle_missing = data_processing_config.setdefault('handle_missing', 'drop')
    outlier_detection = data_processing_config.setdefault('outlier_detection', False)
    outlier_method = data_processing_config.setdefault('outlier_method', 'iqr')
    outlier_threshold = data_processing_config.setdefault('outlier_threshold', 1.5)
    
    # Splitting parameters
    validation_split = data_splitting_config.setdefault('validation_split', 0.2)
    test_split = data_splitting_config.setdefault('test_split', 0.2)
    stratified_split = data_splitting_config.setdefault('stratified_split', True)
    shuffle_data = data_splitting_config.setdefault('shuffle', True)
    random_state = data_splitting_config.setdefault('random_state', 42)
    
    # Performance parameters
    parallel_loading = performance_config.setdefault('parallel_loading', False)
    n_jobs = performance_config.setdefault('n_jobs', -1)
    memory_efficient = performance_config.setdefault('memory_efficient', True)
    cache_data = performance_config.setdefault('cache_data', False)
    
    # Security parameters
    validate_data_integrity = security_config.setdefault('validate_data_integrity', True)
    check_data_corruption = security_config.setdefault('check_data_corruption', False)
    
    # Monitoring parameters - respect silent mode
    verbose = monitoring_config.setdefault('verbose', False)
    progress_bar = monitoring_config.setdefault('progress_bar', not silent_mode)
    save_statistics = monitoring_config.setdefault('save_statistics', True)
    
    # Advanced parameters
    data_quality_checks = advanced_config.setdefault('data_quality_checks', True)
    statistical_validation = advanced_config.setdefault('statistical_validation', False)
    
    # Set up logging level based on silent mode
    if not silent_mode and verbose:
        original_level = logger.level
        logger.setLevel(logging.INFO)
    
    if not silent_mode:
        logger.info("Starting comprehensive data loading and validation")
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'current_substage': None,
        'rows_processed': 0,
        'features_processed': 0,
        'data_quality_score': 0.0,
        'validation_passed': 0,
        'validation_failed': 0
    }
    
    # Initialize comprehensive statistics
    loading_stats = {
        'start_time': start_time.isoformat(),
        'data_path': None,
        'artifacts_path': None,
        'use_real_data': use_real_data,
        'config_applied': final_config,
        'stages_completed': [],
        'errors_encountered': [],
        'warnings_encountered': [],
        'performance_metrics': {}
    }
    
    try:
        # Calculate total stages for progress tracking
        total_stages = 8  # Configuration, Paths, Loading, Validation, Processing, Splitting, Statistics, Finalization
        
        # Use progress bar only if not in silent mode and progress_bar is True
        if not silent_mode and progress_bar:
            bar_context = alive_bar(total_stages, title='Data Loading & Validation\t', unit='stages')
        else:
            # Create a dummy context manager that does nothing
            class DummyBar:
                def __enter__(self):
                    return self
                def __exit__(self, *args):
                    pass
                def __call__(self):
                    pass
                def __setattr__(self, name, value):
                    pass
            bar_context = DummyBar()
        
        with bar_context as main_bar:
            
            # STAGE 1: Configuration and Setup
            progress_data['current_stage'] = "Configuration Setup"
            if not silent_mode:
                main_bar.text = "Setting up configuration and parameters..."
            
            # Determine paths with multiple fallback strategies
            if data_path is None:
                # Try to get from config
                data_path = final_config.get('data', {}).get('data_path') or final_config.get('system', {}).get('data_dir')
                
                if data_path is None:
                    # Use default path
                    data_path = DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"
                else:
                    data_path = Path(data_path) / "preprocessed_dataset.csv"
            else:
                data_path = Path(data_path)
            
            if artifacts_path is None:
                # Try to get from config or derive from data path
                artifacts_path = final_config.get('data', {}).get('artifacts_path')
                if artifacts_path is None:
                    artifacts_path = data_path.parent / "preprocessing_artifacts.pkl"
                else:
                    artifacts_path = Path(artifacts_path)
            else:
                artifacts_path = Path(artifacts_path)
            
            loading_stats['data_path'] = str(data_path)
            loading_stats['artifacts_path'] = str(artifacts_path)
            
            if not silent_mode:
                logger.info(f"Data path: {data_path}")
                logger.info(f"Artifacts path: {artifacts_path}")
            
            if not silent_mode:
                main_bar.text = "Configuration complete"
            loading_stats['stages_completed'].append('configuration')
            if not silent_mode:
                main_bar()
            
            # STAGE 2: Path Validation and File Discovery
            progress_data['current_stage'] = "Path Validation"
            if not silent_mode:
                main_bar.text = "Validating paths and discovering files..."
            
            # Validate file existence with helpful error messages
            if use_real_data:
                if not data_path.exists():
                    # Try alternative locations
                    alternative_paths = [
                        data_path.parent / "dataset.csv",
                        data_path.parent / "data.csv",
                        Path.cwd() / "data" / data_path.name,
                        Path.cwd() / data_path.name
                    ]
                    
                    found_alternative = None
                    for alt_path in alternative_paths:
                        if alt_path.exists():
                            found_alternative = alt_path
                            break
                    
                    if found_alternative:
                        if not silent_mode:
                            logger.warning(f"Data file not found at {data_path}, using {found_alternative}")
                        data_path = found_alternative
                        loading_stats['data_path'] = str(data_path)
                        loading_stats['warnings_encountered'].append(f"Used alternative path: {found_alternative}")
                    else:
                        error_msg = f"Data file not found: {data_path}\nTried alternatives: {alternative_paths}"
                        loading_stats['errors_encountered'].append(error_msg)
                        raise FileNotFoundError(error_msg)
                
                if not artifacts_path.exists() and normalization != 'none':
                    if not silent_mode:
                        logger.warning(f"Artifacts file not found: {artifacts_path}")
                        logger.info("Will attempt to load data without preprocessing artifacts")
                    loading_stats['warnings_encountered'].append("Artifacts file not found")
            
            if not silent_mode:
                main_bar.text = "Path validation complete"
            loading_stats['stages_completed'].append('path_validation')
            if not silent_mode:
                main_bar()
            
            # STAGE 3: Data Loading
            progress_data['current_stage'] = "Data Loading"
            if not silent_mode:
                main_bar.text = "Loading data from source..."
            
            # Load data based on format and parameters
            if not silent_mode:
                logger.info(f"Loading data in {data_format} format")
            
            if use_real_data:
                try:
                    # Load data based on format
                    if data_format.lower() == 'csv':
                        load_params = {
                            'encoding': encoding,
                            'sep': delimiter,
                            'header': header,
                            'low_memory': low_memory
                        }
                        
                        if index_col is not None:
                            load_params['index_col'] = index_col
                        if skiprows is not None:
                            load_params['skiprows'] = skiprows
                        if nrows is not None:
                            load_params['nrows'] = nrows
                        
                        if chunk_size is not None:
                            # Handle large files with chunking
                            if not silent_mode:
                                logger.info(f"Loading data in chunks of size {chunk_size}")
                            chunk_iter = pd.read_csv(data_path, chunksize=chunk_size, **load_params)
                            df_chunks = []
                            
                            # Show chunk loading progress only if not in silent mode and progress_bar is True
                            if not silent_mode and progress_bar:
                                total_chunks = (os.path.getsize(data_path) // (chunk_size * 1000)) + 1
                                with alive_bar(total_chunks, title='Loading Chunks\t\t', unit='chunks') as chunk_bar:
                                    for i, chunk in enumerate(chunk_iter):
                                        df_chunks.append(chunk)
                                        chunk_bar.text = f"Chunk {i+1}: {len(chunk)} rows"
                                        chunk_bar()
                                        if len(df_chunks) * chunk_size >= (max_samples or float('inf')):
                                            break
                            else:
                                # Silent mode or no progress bar - just load chunks
                                for i, chunk in enumerate(chunk_iter):
                                    df_chunks.append(chunk)
                                    if len(df_chunks) * chunk_size >= (max_samples or float('inf')):
                                        break
                            
                            df = pd.concat(df_chunks, ignore_index=True)
                        else:
                            df = pd.read_csv(data_path, **load_params)
                            
                    elif data_format.lower() == 'parquet':
                        df = pd.read_parquet(data_path)
                    elif data_format.lower() == 'json':
                        df = pd.read_json(data_path, encoding=encoding)
                    elif data_format.lower() == 'pickle':
                        df = pd.read_pickle(data_path)
                    else:
                        raise ValueError(f"Unsupported data format: {data_format}")
                    
                    loading_stats['rows_loaded'] = len(df)
                    loading_stats['columns_loaded'] = len(df.columns)
                    progress_data['rows_processed'] = len(df)
                    progress_data['features_processed'] = len(df.columns)
                    
                    if not silent_mode:
                        logger.info(f"Loaded {len(df)} rows and {len(df.columns)} columns")
                    
                except Exception as e:
                    if not silent_mode:
                        logger.error(f"Failed to load data from {data_path}: {e}")
                    loading_stats['errors_encountered'].append(f"Data loading failed: {str(e)}")
                    
                    if not use_real_data:
                        if not silent_mode:
                            logger.info("Falling back to synthetic data generation")
                    else:
                        raise RuntimeError(f"Data loading failed: {str(e)}")
            
            # Generate synthetic data if needed
            if not use_real_data or (use_real_data and 'df' not in locals()):
                progress_data['current_substage'] = "Synthetic Data Generation"
                if not silent_mode:
                    main_bar.text = "Generating synthetic data..."
                
                if not silent_mode:
                    logger.info("Generating synthetic data")
                
                normal_samples = synthetic_data_config.get('synthetic_normal_samples', 1000)
                attack_samples = synthetic_data_config.get('synthetic_attack_samples', 200)
                n_features = min_features
                generation_method = synthetic_data_config.get('synthetic_generation_method', 'gaussian')
                noise_level = synthetic_data_config.get('synthetic_noise_level', 0.1)
                synthetic_seed = synthetic_data_config.get('synthetic_seed', random_state)
                
                np.random.seed(synthetic_seed)
                
                if generation_method == 'gaussian':
                    # Generate normal data
                    X_normal = np.random.normal(0, 1, (normal_samples, n_features))
                    
                    # Generate attack data with different distribution
                    X_attack = np.random.normal(2, 1.5, (attack_samples, n_features))
                    X_attack += np.random.normal(0, noise_level, X_attack.shape)
                    
                elif generation_method == 'mixed':
                    # More complex synthetic data
                    X_normal = np.random.multivariate_normal(
                        np.zeros(n_features), 
                        np.eye(n_features), 
                        normal_samples
                    )
                    
                    # Create correlated attack features
                    attack_mean = np.random.uniform(-2, 2, n_features)
                    attack_cov = np.eye(n_features) * np.random.uniform(0.5, 2, n_features)
                    X_attack = np.random.multivariate_normal(attack_mean, attack_cov, attack_samples)
                    
                else:
                    raise ValueError(f"Unknown synthetic generation method: {generation_method}")
                
                # Combine data
                X = np.vstack([X_normal, X_attack])
                y = np.hstack([np.zeros(normal_samples), np.ones(attack_samples)])
                
                # Create DataFrame
                feature_names = [f'feature_{i}' for i in range(n_features)]
                df = pd.DataFrame(X, columns=feature_names)
                df[label_column] = y
                
                loading_stats.update({
                    'synthetic_data_generated': True,
                    'normal_samples': normal_samples,
                    'attack_samples': attack_samples,
                    'generation_method': generation_method,
                    'noise_level': noise_level
                })
                
                progress_data['rows_processed'] = len(df)
                progress_data['features_processed'] = len(df.columns)
                
                if not silent_mode:
                    logger.info(f"Generated synthetic data: {normal_samples} normal, {attack_samples} attack samples")
            
            if not silent_mode:
                main_bar.text = "Data loading complete"
            loading_stats['stages_completed'].append('data_loading')
            if not silent_mode:
                main_bar()
            
            # STAGE 4: Data Validation
            progress_data['current_stage'] = "Data Validation"
            if not silent_mode:
                main_bar.text = "Validating data structure and quality..."
            
            if not silent_mode:
                logger.info("Performing comprehensive data validation")
            
            if df is None or df.empty:
                error_msg = "No data loaded or generated"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            # Check required columns
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                error_msg = f"Missing required columns: {missing_columns}"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            # Validate label column
            if label_column not in df.columns:
                error_msg = f"Label column '{label_column}' not found in dataset"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            # Check data types and handle mixed types
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            if label_column in numeric_columns:
                numeric_columns.remove(label_column)
            
            if len(numeric_columns) < min_features:
                if not silent_mode:
                    logger.warning(f"Only {len(numeric_columns)} numeric features found, minimum is {min_features}")
                loading_stats['warnings_encountered'].append(f"Low numeric features: {len(numeric_columns)}")
                
                # Try to convert non-numeric columns
                conversion_attempts = 0
                for col in df.columns:
                    if col != label_column and col not in numeric_columns:
                        try:
                            df[col] = pd.to_numeric(df[col], errors='coerce')
                            if not df[col].isna().all():
                                numeric_columns.append(col)
                                conversion_attempts += 1
                        except Exception:
                            continue
                
                if conversion_attempts > 0 and not silent_mode:
                    logger.info(f"Converted {conversion_attempts} columns to numeric")
            
            # Update feature columns
            if feature_columns is None:
                feature_columns = [col for col in numeric_columns if col != label_column]
            
            # Exclude specified columns
            if exclude_columns:
                feature_columns = [col for col in feature_columns if col not in exclude_columns]
            
            # Validate final feature set
            if len(feature_columns) < min_features:
                error_msg = f"Too few valid features ({len(feature_columns)}), need at least {min_features}"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            if max_features and len(feature_columns) > max_features:
                if not silent_mode:
                    logger.info(f"Reducing features from {len(feature_columns)} to {max_features}")
                feature_columns = feature_columns[:max_features]
            
            loading_stats['feature_columns'] = feature_columns
            loading_stats['n_features'] = len(feature_columns)
            progress_data['features_processed'] = len(feature_columns)
            
            # Extract features and labels
            X = df[feature_columns].values.astype(np.float32)
            y = df[label_column].values
            
            progress_data['validation_passed'] += 1
            if not silent_mode:
                main_bar.text = "Data validation complete"
            loading_stats['stages_completed'].append('data_validation')
            if not silent_mode:
                main_bar()
            
            # STAGE 5: Data Processing
            progress_data['current_stage'] = "Data Processing"
            if not silent_mode:
                main_bar.text = "Processing data (cleaning, scaling, etc.)..."
            
            # Validate data quality
            if data_quality_checks:
                progress_data['current_substage'] = "Quality Checks"
                if not silent_mode:
                    main_bar.text = "Performing data quality checks..."
                
                if not silent_mode:
                    logger.info("Performing comprehensive data quality checks")
                
                # Check for infinite values
                inf_mask = np.isinf(X)
                if inf_mask.any():
                    if not silent_mode:
                        logger.warning(f"Found {inf_mask.sum()} infinite values, replacing with NaN")
                    X[inf_mask] = np.nan
                    loading_stats['warnings_encountered'].append(f"Fixed {inf_mask.sum()} infinite values")
                
                # Check for excessive missing values
                missing_per_feature = np.isnan(X).sum(axis=0) / len(X)
                problematic_features = np.where(missing_per_feature > 0.5)[0]
                if len(problematic_features) > 0:
                    warning_msg = f"Features with >50% missing values: {problematic_features}"
                    if not silent_mode:
                        logger.warning(warning_msg)
                    loading_stats['warnings_encountered'].append(warning_msg)
                
                # Check for zero variance features
                if feature_engineering_config.get('variance_threshold', 0) > 0:
                    variances = np.var(X, axis=0)
                    zero_var_features = np.where(variances < feature_engineering_config['variance_threshold'])[0]
                    if len(zero_var_features) > 0:
                        warning_msg = f"Low variance features detected: {zero_var_features}"
                        if not silent_mode:
                            logger.warning(warning_msg)
                        loading_stats['warnings_encountered'].append(warning_msg)
            
            # Handle missing values
            if handle_missing and np.isnan(X).any():
                progress_data['current_substage'] = "Missing Value Handling"
                if not silent_mode:
                    main_bar.text = "Handling missing values..."
                
                if not silent_mode:
                    logger.info(f"Handling missing values using strategy: {handle_missing}")
                
                if handle_missing == 'drop':
                    # Drop rows with any missing values
                    valid_mask = ~np.isnan(X).any(axis=1)
                    rows_dropped = (~valid_mask).sum()
                    X = X[valid_mask]
                    y = y[valid_mask]
                    if not silent_mode:
                        logger.info(f"Dropped {rows_dropped} rows with missing values")
                    loading_stats['rows_dropped_missing'] = rows_dropped
                    
                elif handle_missing == 'fill':
                    strategy = data_processing_config.get('missing_value_strategy', 'mean')
                    if strategy in ['mean', 'median', 'most_frequent']:
                        imputer = SimpleImputer(strategy=strategy)
                    elif strategy == 'knn':
                        imputer = KNNImputer(n_neighbors=5)
                    else:
                        imputer = SimpleImputer(strategy='mean')
                    
                    X = imputer.fit_transform(X)
                    if not silent_mode:
                        logger.info(f"Filled missing values using {strategy} strategy")
                    loading_stats['missing_values_filled'] = True
                    loading_stats['imputation_strategy'] = strategy
            
            # Validate sample sizes
            if len(X) < min_samples:
                error_msg = f"Too few samples ({len(X)}), need at least {min_samples}"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            if max_samples and len(X) > max_samples:
                if not silent_mode:
                    logger.info(f"Limiting dataset to {max_samples} samples")
                indices = np.random.choice(len(X), max_samples, replace=False)
                X = X[indices]
                y = y[indices]
                loading_stats['samples_limited'] = max_samples
            
            # Validate label distribution
            unique_labels = np.unique(y)
            label_counts = {label: np.sum(y == label) for label in unique_labels}
            
            if not silent_mode:
                logger.info(f"Label distribution: {label_counts}")
            loading_stats['label_distribution'] = label_counts
            
            # Check for class balance issues
            if class_balance_config.get('min_class_samples', 10):
                min_class_size = min(label_counts.values())
                if min_class_size < class_balance_config['min_class_samples']:
                    warning_msg = f"Smallest class has only {min_class_size} samples"
                    if not silent_mode:
                        logger.warning(warning_msg)
                    loading_stats['warnings_encountered'].append(warning_msg)
            
            if class_balance_config.get('max_class_ratio', 10):
                max_class_size = max(label_counts.values())
                min_class_size = min(label_counts.values())
                class_ratio = max_class_size / min_class_size if min_class_size > 0 else float('inf')
                if class_ratio > class_balance_config['max_class_ratio']:
                    warning_msg = f"Class imbalance ratio: {class_ratio:.2f}"
                    if not silent_mode:
                        logger.warning(warning_msg)
                    loading_stats['warnings_encountered'].append(warning_msg)
            
            # Outlier detection and handling
            if outlier_detection:
                progress_data['current_substage'] = "Outlier Detection"
                if not silent_mode:
                    main_bar.text = "Detecting and handling outliers..."
                
                if not silent_mode:
                    logger.info(f"Detecting outliers using {outlier_method} method")
                
                if outlier_method == 'iqr':
                    Q1 = np.percentile(X, 25, axis=0)
                    Q3 = np.percentile(X, 75, axis=0)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - outlier_threshold * IQR
                    upper_bound = Q3 + outlier_threshold * IQR
                    
                    outlier_mask = ((X < lower_bound) | (X > upper_bound)).any(axis=1)
                    
                elif outlier_method == 'zscore':
                    z_scores = np.abs(stats.zscore(X, axis=0, nan_policy='omit'))
                    outlier_mask = (z_scores > outlier_threshold).any(axis=1)
                    
                elif outlier_method == 'isolation':
                    iso_forest = IsolationForest(contamination=0.1, random_state=random_state)
                    outlier_labels = iso_forest.fit_predict(X)
                    outlier_mask = outlier_labels == -1
                    
                else:
                    if not silent_mode:
                        logger.warning(f"Unknown outlier method: {outlier_method}")
                    outlier_mask = np.zeros(len(X), dtype=bool)
                    loading_stats['warnings_encountered'].append(f"Unknown outlier method: {outlier_method}")
                
                n_outliers = outlier_mask.sum()
                if n_outliers > 0:
                    if not silent_mode:
                        logger.info(f"Detected {n_outliers} outliers ({n_outliers/len(X)*100:.1f}%)")
                    # Remove outliers
                    X = X[~outlier_mask]
                    y = y[~outlier_mask]
                    if not silent_mode:
                        logger.info(f"Removed outliers, dataset size: {len(X)}")
                    loading_stats['outliers_removed'] = n_outliers
                    loading_stats['outlier_percentage'] = n_outliers/len(X)*100
            
            # Load preprocessing artifacts if available
            scaler = None
            feature_selector = None
            artifacts = {}
            
            if artifacts_path.exists():
                try:
                    progress_data['current_substage'] = "Loading Artifacts"
                    if not silent_mode:
                        main_bar.text = "Loading preprocessing artifacts..."
                    
                    if not silent_mode:
                        logger.info("Loading preprocessing artifacts")
                    artifacts = joblib.load(artifacts_path)
                    
                    # Validate artifacts
                    artifacts_features = artifacts.get("feature_names", [])
                    if artifacts_features and set(artifacts_features) != set(feature_columns):
                        warning_msg = "Feature names in artifacts don't match current features"
                        if not silent_mode:
                            logger.warning(warning_msg)
                            logger.warning(f"Artifacts features: {len(artifacts_features)}")
                            logger.warning(f"Current features: {len(feature_columns)}")
                        loading_stats['warnings_encountered'].append(warning_msg)
                    
                    scaler = artifacts.get("scaler")
                    feature_selector = artifacts.get("feature_selector")
                    
                    loading_stats['artifacts_loaded'] = True
                    loading_stats['scaler_type'] = type(scaler).__name__ if scaler else None
                    
                except Exception as e:
                    warning_msg = f"Failed to load artifacts: {e}"
                    if not silent_mode:
                        logger.warning(warning_msg)
                    loading_stats['warnings_encountered'].append(warning_msg)
                    artifacts = {}
            
            # Apply or create scaling
            if normalization and normalization != 'none':
                progress_data['current_substage'] = "Feature Scaling"
                if not silent_mode:
                    main_bar.text = "Applying feature scaling..."
                
                if scaler is not None:
                    if not silent_mode:
                        logger.info(f"Applying existing {type(scaler).__name__} scaler")
                    try:
                        X_scaled = scaler.transform(X)
                        loading_stats['scaling_applied'] = 'existing'
                    except Exception as e:
                        warning_msg = f"Failed to apply existing scaler: {e}"
                        if not silent_mode:
                            logger.warning(warning_msg)
                        loading_stats['warnings_encountered'].append(warning_msg)
                        scaler = None
                
                if scaler is None:
                    if not silent_mode:
                        logger.info(f"Creating new {normalization} scaler")
                    
                    if normalization == 'standard':
                        scaler = StandardScaler()
                    elif normalization == 'minmax':
                        scaler = MinMaxScaler()
                    elif normalization == 'robust':
                        scaler = RobustScaler()
                    elif normalization == 'quantile':
                        scaler = QuantileTransformer()
                    else:
                        if not silent_mode:
                            logger.warning(f"Unknown normalization method: {normalization}, using standard")
                        scaler = StandardScaler()
                    
                    X_scaled = scaler.fit_transform(X)
                    loading_stats['scaling_applied'] = 'new'
                
                X = X_scaled.astype(np.float32)
            else:
                loading_stats['scaling_applied'] = 'none'
            
            # Feature selection
            if feature_engineering_config.get('feature_selection', False):
                progress_data['current_substage'] = "Feature Selection"
                if not silent_mode:
                    main_bar.text = "Performing feature selection..."
                
                n_features_select = feature_engineering_config.get('n_features_select', min(50, len(feature_columns)))
                selection_method = feature_engineering_config.get('feature_selection_method', 'k_best')
                
                if not silent_mode:
                    logger.info(f"Performing feature selection: {selection_method}")
                
                if selection_method == 'k_best':
                    if feature_selector is None:
                        feature_selector = SelectKBest(f_classif, k=n_features_select)
                        X_selected = feature_selector.fit_transform(X, y)
                        loading_stats['feature_selection_applied'] = 'new'
                    else:
                        X_selected = feature_selector.transform(X)
                        loading_stats['feature_selection_applied'] = 'existing'
                    
                    selected_features = feature_selector.get_support()
                    selected_feature_names = [feature_columns[i] for i in range(len(feature_columns)) if selected_features[i]]
                    
                    X = X_selected
                    feature_columns = selected_feature_names
                    
                    if not silent_mode:
                        logger.info(f"Selected {len(feature_columns)} features from {len(selected_features)}")
                    loading_stats['features_selected'] = len(feature_columns)
            
            progress_data['validation_passed'] += 1
            if not silent_mode:
                main_bar.text = "Data processing complete"
            loading_stats['stages_completed'].append('data_processing')
            if not silent_mode:
                main_bar()
            
            # STAGE 6: Data Splitting
            progress_data['current_stage'] = "Data Splitting"
            if not silent_mode:
                main_bar.text = "Splitting data into train/validation/test sets..."
            
            # Split data into normal and attack samples
            normal_mask = (y == 0)
            attack_mask = (y == 1) if len(unique_labels) == 2 else (~normal_mask)
            
            X_normal = X[normal_mask]
            X_attack = X[attack_mask]
            
            if not silent_mode:
                logger.info(f"Data split: {len(X_normal)} normal, {len(X_attack)} attack samples")
            
            # Perform train/validation/test splits
            if len(X_normal) == 0:
                error_msg = "No normal samples found in dataset"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            # Split normal data for training and validation
            if validation_split > 0:
                X_train_normal, X_val_normal = train_test_split(
                    X_normal,
                    test_size=validation_split,
                    random_state=random_state,
                    shuffle=shuffle_data
                )
            else:
                X_train_normal = X_normal
                X_val_normal = np.array([]).reshape(0, X_normal.shape[1])
            
            # Handle test data
            if len(X_attack) > 0:
                if test_split > 0 and test_split < 1.0:
                    test_size = int(len(X_attack) * test_split)
                    X_test = X_attack[:test_size] if test_size > 0 else X_attack
                else:
                    X_test = X_attack
            else:
                # If no attack data, use a portion of normal data for testing
                if test_split > 0:
                    test_size = int(len(X_train_normal) * test_split)
                    if test_size > 0:
                        X_test = X_train_normal[:test_size]
                        X_train_normal = X_train_normal[test_size:]
                    else:
                        X_test = np.array([]).reshape(0, X_train_normal.shape[1])
                else:
                    X_test = np.array([]).reshape(0, X_train_normal.shape[1])
            
            loading_stats.update({
                'train_samples': len(X_train_normal),
                'val_samples': len(X_val_normal),
                'test_samples': len(X_test),
                'final_feature_count': len(feature_columns)
            })
            
            progress_data['validation_passed'] += 1
            if not silent_mode:
                main_bar.text = "Data splitting complete"
            loading_stats['stages_completed'].append('data_splitting')
            if not silent_mode:
                main_bar()
            
            # STAGE 7: Statistical Validation and Quality Assessment
            progress_data['current_stage'] = "Quality Assessment"
            if not silent_mode:
                main_bar.text = "Performing final quality assessment..."
            
            # Statistical validation
            if statistical_validation:
                if not silent_mode:
                    logger.info("Performing statistical validation")
                
                # Check data distributions
                if advanced_config.get('distribution_checks', False):
                    # Kolmogorov-Smirnov test for normality
                    normality_results = []
                    for i, feature_name in enumerate(feature_columns):
                        # Ensure we don't exceed dimensions
                        if i < X_train_normal.shape[1]:
                            feature_data = X_train_normal[:, i]
                            ks_stat, p_value = stats.kstest(feature_data, 'norm')
                            normality_results.append({
                                'feature': feature_name,
                                'ks_statistic': ks_stat,
                                'p_value': p_value,
                                'normal': p_value >= 0.05
                            })
                            if p_value < 0.05 and not silent_mode:
                                logger.debug(f"Feature {feature_name} may not be normally distributed (p={p_value:.4f})")
                    
                    loading_stats['normality_tests'] = normality_results
            
            # Calculate data quality score
            quality_metrics = []
            
            # Sample size adequacy
            sample_score = min(1.0, len(X) / max(min_samples, 1000))
            quality_metrics.append(('sample_size', sample_score))
            
            # Feature adequacy
            feature_score = min(1.0, len(feature_columns) / max(min_features, 10))
            quality_metrics.append(('feature_count', feature_score))
            
            # Class balance (if applicable)
            if len(label_counts) > 1:
                min_class = min(label_counts.values())
                max_class = max(label_counts.values())
                balance_score = min_class / max_class if max_class > 0 else 1.0
                quality_metrics.append(('class_balance', balance_score))
            else:
                quality_metrics.append(('class_balance', 1.0))
            
            # Missing data handling
            missing_score = 1.0 if not np.isnan(X).any() else 0.8
            quality_metrics.append(('missing_data', missing_score))
            
            # Overall quality score (weighted average)
            weights = [0.3, 0.3, 0.2, 0.2]  # Adjust weights as needed
            quality_score = sum(score * weight for (_, score), weight in zip(quality_metrics, weights))
            
            progress_data['data_quality_score'] = quality_score
            loading_stats['data_quality_score'] = quality_score
            loading_stats['quality_metrics'] = dict(quality_metrics)
            
            if not silent_mode:
                main_bar.text = "Quality assessment complete"
            loading_stats['stages_completed'].append('quality_assessment')
            if not silent_mode:
                main_bar()
            
            # STAGE 8: Finalization and Statistics
            progress_data['current_stage'] = "Finalization"
            if not silent_mode:
                main_bar.text = "Finalizing data preparation..."
            
            # Prepare final data dictionary
            data_dict = {
                "X_train": X_train_normal.astype(np.float32),
                "X_val": X_val_normal.astype(np.float32),
                "X_test": X_test.astype(np.float32),
                "feature_names": feature_columns,
                "metadata": {
                    # Basic statistics
                    "total_samples": len(X),
                    "total_normal": len(X_normal),
                    "total_attack": len(X_attack),
                    "n_features": len(feature_columns),
                    "train_samples": len(X_train_normal),
                    "val_samples": len(X_val_normal),
                    "test_samples": len(X_test),
                    
                    # Configuration used
                    "validation_split": validation_split,
                    "test_split": test_split,
                    "stratified_split": stratified_split,
                    "random_state": random_state,
                    "normalization": normalization,
                    
                    # Processing applied
                    "scaler_applied": scaler is not None,
                    "scaler_type": type(scaler).__name__ if scaler else None,
                    "feature_selection_applied": feature_selector is not None,
                    "outliers_removed": outlier_detection,
                    "missing_values_handled": handle_missing if np.isnan(X).any() else 'none',
                    
                    # Data quality metrics
                    "class_balance_ratio": max(label_counts.values()) / min(label_counts.values()) if min(label_counts.values()) > 0 else float('inf'),
                    "data_quality_score": quality_score,
                    
                    # Loading statistics
                    "loading_time_seconds": (datetime.now() - start_time).total_seconds(),
                    "data_source": "real" if use_real_data else "synthetic",
                    "config_applied": final_config,
                    
                    # Artifacts information
                    "artifacts_available": scaler is not None or feature_selector is not None,
                    "preprocessing_pipeline": {
                        "scaler": type(scaler).__name__ if scaler else None,
                        "feature_selector": type(feature_selector).__name__ if feature_selector else None,
                        "steps_applied": loading_stats.get('steps_applied', [])
                    }
                },
                
                # Include preprocessing components for future use
                "scaler": scaler,
                "feature_selector": feature_selector,
                "label_encoder": artifacts.get("label_encoder"),
                
                # Raw data for advanced use cases
                "raw_data": {
                    "X_full": X,
                    "y_full": y,
                    "original_features": df.columns.tolist() if 'df' in locals() else feature_columns
                } if advanced_config.get('include_raw_data', False) else None
            }
            
            # Validate final data shapes and consistency
            if not silent_mode:
                logger.info("Performing final data validation")
            
            # Shape consistency checks
            expected_features = len(feature_columns)
            for key in ["X_train", "X_val", "X_test"]:
                # Only check non-empty arrays
                if data_dict[key].size > 0:
                    if data_dict[key].shape[1] != expected_features:
                        error_msg = f"{key} has {data_dict[key].shape[1]} features, expected {expected_features}"
                        loading_stats['errors_encountered'].append(error_msg)
                        raise ValueError(error_msg)
            
            # Data type consistency
            for key in ["X_train", "X_val", "X_test"]:
                if data_dict[key].dtype != np.float32:
                    if not silent_mode:
                        logger.warning(f"{key} has dtype {data_dict[key].dtype}, converting to float32")
                    data_dict[key] = data_dict[key].astype(np.float32)
            
            # Save statistics if requested
            if save_statistics:
                stats_path = monitoring_config.get('statistics_path', data_path.parent / "data_loading_statistics.json")
                try:
                    with open(stats_path, 'w') as f:
                        # Make loading_stats JSON serializable
                        serializable_stats = {}
                        for key, value in loading_stats.items():
                            if isinstance(value, (str, int, float, bool, list, dict)):
                                serializable_stats[key] = value
                            else:
                                serializable_stats[key] = str(value)
                        
                        json.dump(serializable_stats, f, indent=2)
                    if not silent_mode:
                        logger.info(f"Saved loading statistics to {stats_path}")
                except Exception as e:
                    if not silent_mode:
                        logger.warning(f"Failed to save statistics: {e}")
            
            if not silent_mode:
                main_bar.text = "Finalization complete"
            loading_stats['stages_completed'].append('finalization')
            if not silent_mode:
                main_bar()
        
        # Log summary only if not in silent mode
        total_time = (datetime.now() - start_time).total_seconds()
        loading_stats['total_processing_time'] = total_time
        loading_stats['completion_status'] = 'success'
        
        if not silent_mode:
            logger.info("-" * 40)
            logger.info("DATA LOADING SUMMARY")
            logger.info("-" * 40)
            logger.info(f"Data source: {'Real data' if use_real_data else 'Synthetic data'}")
            logger.info(f"Total samples: {len(X):,} ({len(X_normal):,} normal, {len(X_attack):,} attack)")
            logger.info(f"Features: {len(feature_columns)} (after processing)")
            logger.info(f"Train/Val/Test split: {len(X_train_normal)}/{len(X_val_normal)}/{len(X_test)}")
            logger.info(f"Normalization: {normalization} ({'applied' if scaler else 'none'})")
            logger.info(f"Feature selection: {'applied' if feature_selector else 'none'}")
            logger.info(f"Data quality score: {quality_score:.3f}")
            logger.info(f"Processing time: {total_time:.2f} seconds")
            logger.info(f"Stages completed: {len(loading_stats['stages_completed'])}/{total_stages}")
            logger.info(f"Warnings encountered: {len(loading_stats['warnings_encountered'])}")
            logger.info("-" * 40)
        
        # Restore original logging level if it was changed
        if not silent_mode and verbose and 'original_level' in locals():
            logger.setLevel(original_level)
        
        return data_dict
        
    except Exception as e:
        # Update loading stats with error information
        loading_stats['completion_status'] = 'failed'
        loading_stats['error_message'] = str(e)
        loading_stats['error_traceback'] = traceback.format_exc()
        
        # Restore original logging level on error
        if not silent_mode and verbose and 'original_level' in locals():
            logger.setLevel(original_level)
        
        error_msg = f"Data loading and validation failed: {str(e)}"
        if not silent_mode:
            logger.error(error_msg)
            logger.error(f"Full traceback: {traceback.format_exc()}")
            
            # Provide helpful error context
            logger.error(f"Error occurred while processing: {data_path}")
            logger.error(f"Configuration used: {final_config}")
            logger.error(f"Stages completed: {loading_stats['stages_completed']}")
        
        raise RuntimeError(error_msg)

def validate_data_integrity(
    data_dict: Dict[str, Union[np.ndarray, Dict[str, Any]]],
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Perform comprehensive data integrity validation.
    
    Args:
        data_dict: Data dictionary from load_and_validate_data
        config: Configuration dictionary
        
    Returns:
        Dictionary with validation results and recommendations
    """
    validation_results = {
        'passed': True,
        'warnings': [],
        'errors': [],
        'recommendations': [],
        'quality_score': 1.0,
        'detailed_checks': {}
    }
    
    try:
        # Extract data components
        X_train = data_dict.get('X_train', np.array([]))
        X_val = data_dict.get('X_val', np.array([]))
        X_test = data_dict.get('X_test', np.array([]))
        metadata = data_dict.get('metadata', {})
        
        # Check data presence
        if X_train.size == 0:
            validation_results['errors'].append("No training data available")
            validation_results['passed'] = False
        
        # Check data shapes consistency
        if X_train.size > 0 and X_val.size > 0:
            if X_train.shape[1] != X_val.shape[1]:
                validation_results['errors'].append(f"Feature dimension mismatch: train={X_train.shape[1]}, val={X_val.shape[1]}")
                validation_results['passed'] = False
        
        # Check for data quality issues
        for name, data in [('train', X_train), ('val', X_val), ('test', X_test)]:
            if data.size > 0:
                # Check for NaN values
                nan_count = np.isnan(data).sum()
                if nan_count > 0:
                    validation_results['warnings'].append(f"{name} data contains {nan_count} NaN values")
                
                # Check for infinite values
                inf_count = np.isinf(data).sum()
                if inf_count > 0:
                    validation_results['warnings'].append(f"{name} data contains {inf_count} infinite values")
                
                # Check data ranges
                if np.any(data > 1e6) or np.any(data < -1e6):
                    validation_results['warnings'].append(f"{name} data contains extreme values")
        
        # Calculate quality score
        n_warnings = len(validation_results['warnings'])
        n_errors = len(validation_results['errors'])
        validation_results['quality_score'] = max(0.0, 1.0 - (n_warnings * 0.1 + n_errors * 0.5))
        
        return validation_results
        
    except Exception as e:
        validation_results['errors'].append(f"Validation process failed: {str(e)}")
        validation_results['passed'] = False
        validation_results['quality_score'] = 0.0
        return validation_results

def create_data_pipeline(
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Create a comprehensive data processing pipeline based on configuration.
    
    Args:
        config: Complete configuration dictionary
        
    Returns:
        Data pipeline configuration
    """
    pipeline_config = {
        'steps': [],
        'parameters': {},
        'validation_rules': {},
        'monitoring': {}
    }
    
    # Extract relevant configuration sections
    data_config = config.get('data_loading', {})
    processing_config = config.get('data_processing', {})
    validation_config = config.get('data_validation', {})
    
    # Build pipeline steps
    pipeline_steps = []
    
    # Step 1: Data Loading
    pipeline_steps.append({
        'name': 'data_loading',
        'function': 'load_raw_data',
        'parameters': data_config
    })
    
    # Step 2: Data Validation
    pipeline_steps.append({
        'name': 'validation',
        'function': 'validate_data_structure',
        'parameters': validation_config
    })
    
    # Step 3: Data Processing
    if processing_config.get('normalization', 'none') != 'none':
        pipeline_steps.append({
            'name': 'normalization',
            'function': 'apply_normalization',
            'parameters': {'method': processing_config['normalization']}
        })
    
    # Step 4: Feature Engineering
    if config.get('feature_engineering', {}).get('feature_selection', False):
        pipeline_steps.append({
            'name': 'feature_selection',
            'function': 'select_features',
            'parameters': config['feature_engineering']
        })
    
    # Step 5: Data Splitting
    pipeline_steps.append({
        'name': 'data_splitting',
        'function': 'split_data',
        'parameters': config.get('data_splitting', {})
    })
    
    pipeline_config['steps'] = pipeline_steps
    pipeline_config['parameters'] = config
    
    return pipeline_config

def generate_synthetic_data(
    # Core Data Generation Parameters
    normal_samples: Optional[int] = None,
    attack_samples: Optional[int] = None,
    features: Optional[int] = None,
    anomaly_factor: Optional[float] = None,
    random_state: Optional[int] = None,
    
    # Data Structure Parameters
    label_column: Optional[str] = None,
    feature_names: Optional[List[str]] = None,
    feature_prefix: Optional[str] = None,
    output_format: Optional[str] = None,
    data_type: Optional[str] = None,
    
    # Generation Method Parameters
    generation_method: Optional[str] = None,
    generation_strategy: Optional[str] = None,
    distribution_type: Optional[str] = None,
    cluster_centers: Optional[int] = None,
    cluster_variance: Optional[float] = None,
    cluster_separation: Optional[float] = None,
    
    # Normal Data Parameters
    normal_mean: Optional[float] = None,
    normal_std: Optional[float] = None,
    normal_distribution: Optional[str] = None,
    normal_clusters: Optional[int] = None,
    normal_cluster_variance: Optional[float] = None,
    normal_bounds: Optional[Tuple[float, float]] = None,
    normal_correlation: Optional[float] = None,
    
    # Attack Data Parameters
    attack_mean: Optional[float] = None,
    attack_std: Optional[float] = None,
    attack_distribution: Optional[str] = None,
    attack_clusters: Optional[int] = None,
    attack_cluster_variance: Optional[float] = None,
    attack_bounds: Optional[Tuple[float, float]] = None,
    attack_correlation: Optional[float] = None,
    attack_types: Optional[List[str]] = None,
    attack_proportions: Optional[List[float]] = None,
    
    # Anomaly Configuration Parameters
    anomaly_sparsity: Optional[float] = None,
    anomaly_intensity: Optional[float] = None,
    anomaly_patterns: Optional[List[str]] = None,
    anomaly_correlation: Optional[float] = None,
    seasonal_anomalies: Optional[bool] = None,
    contextual_anomalies: Optional[bool] = None,
    point_anomalies: Optional[bool] = None,
    collective_anomalies: Optional[bool] = None,
    
    # Data Splitting Parameters
    validation_split: Optional[float] = None,
    test_split: Optional[float] = None,
    stratified_split: Optional[bool] = None,
    shuffle: Optional[bool] = None,
    
    # Noise Parameters
    noise_level: Optional[float] = None,
    noise_type: Optional[str] = None,
    gaussian_noise_std: Optional[float] = None,
    uniform_noise_range: Optional[float] = None,
    salt_pepper_ratio: Optional[float] = None,
    
    # Feature Engineering Parameters
    correlated_features: Optional[bool] = None,
    correlation_matrix: Optional[np.ndarray] = None,
    feature_dependencies: Optional[Dict[str, List[str]]] = None,
    redundant_features: Optional[int] = None,
    informative_features: Optional[int] = None,
    feature_scaling: Optional[str] = None,
    
    # Temporal Parameters
    temporal_data: Optional[bool] = None,
    time_steps: Optional[int] = None,
    temporal_pattern: Optional[str] = None,
    trend_component: Optional[bool] = None,
    seasonal_component: Optional[bool] = None,
    temporal_noise: Optional[float] = None,
    
    # Quality Control Parameters
    outlier_contamination: Optional[float] = None,
    class_balance_ratio: Optional[float] = None,
    minimum_separation: Optional[float] = None,
    overlap_factor: Optional[float] = None,
    separability_score: Optional[float] = None,
    
    # Advanced Generation Parameters
    multimodal_data: Optional[bool] = None,
    mixture_components: Optional[int] = None,
    mixture_weights: Optional[List[float]] = None,
    non_linear_relationships: Optional[bool] = None,
    polynomial_degree: Optional[int] = None,
    interaction_features: Optional[bool] = None,
    
    # Statistical Properties Parameters
    skewness: Optional[float] = None,
    kurtosis: Optional[float] = None,
    heavy_tails: Optional[bool] = None,
    distribution_parameters: Optional[Dict[str, Any]] = None,
    moment_constraints: Optional[Dict[str, float]] = None,
    
    # Scalability Parameters
    batch_generation: Optional[bool] = None,
    batch_size: Optional[int] = None,
    memory_efficient: Optional[bool] = None,
    parallel_generation: Optional[bool] = None,
    n_jobs: Optional[int] = None,
    
    # Reproducibility Parameters
    deterministic: Optional[bool] = None,
    seed_sequence: Optional[List[int]] = None,
    generator_state: Optional[Dict[str, Any]] = None,
    
    # Validation Parameters
    validate_generation: Optional[bool] = None,
    statistical_tests: Optional[bool] = None,
    quality_metrics: Optional[List[str]] = None,
    distribution_tests: Optional[bool] = None,
    
    # Directory Parameters
    data_dir: Optional[Union[str, Path]] = None,
    dataset_dir: Optional[Union[str, Path]] = None,
    results_dir: Optional[Union[str, Path]] = None,
    reports_dir: Optional[Union[str, Path]] = None,

    # Export Parameters
    save_data: Optional[bool] = None,
    output_path: Optional[Union[str, Path]] = None,
    file_format: Optional[str] = None,
    compression: Optional[str] = None,
    metadata_file: Optional[bool] = None,
    
    # Monitoring Parameters
    silent: Optional[bool] = None,
    verbose: Optional[bool] = None,
    progress_bar: Optional[bool] = None,
    log_generation_stats: Optional[bool] = None,
    generation_report: Optional[bool] = None,
    
    # Compatibility Parameters
    sklearn_format: Optional[bool] = None,
    pandas_output: Optional[bool] = None,
    torch_tensors: Optional[bool] = None,
    numpy_arrays: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_methods: Optional[bool] = None,
    gan_based_generation: Optional[bool] = None,
    autoencoder_generation: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    synthetic_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Dict[str, Union[np.ndarray, pd.DataFrame, Dict[str, Any]]]:

    # Start timing
    start_time = datetime.now()
    
    # Initialize configuration
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply synthetic configuration
    if synthetic_config:
        config.setdefault('synthetic_data', {}).update(synthetic_config)
    
    # Apply all parameters to configuration
    final_config = {}
    
    # Merge with existing config
    final_config.update(config)
    
    # Apply individual parameters with intelligent organization
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'synthetic_config', 'kwargs', 'start_time', 'datetime', 'traceback', 'stats', 'make_classification', 'make_blobs',
        'StandardScaler', 'MinMaxScaler', 'train_test_split'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Organize parameters into logical sections
    param_sections = {
        'core_generation': [
            'normal_samples', 'attack_samples', 'features', 'anomaly_factor', 'random_state'
        ],
        'data_structure': [
            'label_column', 'feature_names', 'feature_prefix', 'output_format', 'data_type'
        ],
        'generation_methods': [
            'generation_method', 'generation_strategy', 'distribution_type', 'cluster_centers',
            'cluster_variance', 'cluster_separation'
        ],
        'normal_data': [
            'normal_mean', 'normal_std', 'normal_distribution', 'normal_clusters',
            'normal_cluster_variance', 'normal_bounds', 'normal_correlation'
        ],
        'attack_data': [
            'attack_mean', 'attack_std', 'attack_distribution', 'attack_clusters',
            'attack_cluster_variance', 'attack_bounds', 'attack_correlation',
            'attack_types', 'attack_proportions'
        ],
        'anomaly_config': [
            'anomaly_sparsity', 'anomaly_intensity', 'anomaly_patterns', 'anomaly_correlation',
            'seasonal_anomalies', 'contextual_anomalies', 'point_anomalies', 'collective_anomalies'
        ],
        'data_splitting': [
            'validation_split', 'test_split', 'stratified_split', 'shuffle'
        ],
        'noise_parameters': [
            'noise_level', 'noise_type', 'gaussian_noise_std', 'uniform_noise_range',
            'salt_pepper_ratio'
        ],
        'feature_engineering': [
            'correlated_features', 'correlation_matrix', 'feature_dependencies',
            'redundant_features', 'informative_features', 'feature_scaling'
        ],
        'temporal_parameters': [
            'temporal_data', 'time_steps', 'temporal_pattern', 'trend_component',
            'seasonal_component', 'temporal_noise'
        ],
        'quality_control': [
            'outlier_contamination', 'class_balance_ratio', 'minimum_separation',
            'overlap_factor', 'separability_score'
        ],
        'advanced_generation': [
            'multimodal_data', 'mixture_components', 'mixture_weights',
            'non_linear_relationships', 'polynomial_degree', 'interaction_features'
        ],
        'statistical_properties': [
            'skewness', 'kurtosis', 'heavy_tails', 'distribution_parameters',
            'moment_constraints'
        ],
        'scalability': [
            'batch_generation', 'batch_size', 'memory_efficient', 'parallel_generation', 'n_jobs'
        ],
        'reproducibility': [
            'deterministic', 'seed_sequence', 'generator_state'
        ],
        'validation': [
            'validate_generation', 'statistical_tests', 'quality_metrics', 'distribution_tests'
        ],
        'export': [
            'save_data', 'output_path', 'file_format', 'compression', 'metadata_file'
        ],
        'monitoring': [
            'verbose', 'progress_bar', 'log_generation_stats', 'generation_report', 'silent'
        ],
        'compatibility': [
            'sklearn_format', 'pandas_output', 'torch_tensors', 'numpy_arrays'
        ],
        'experimental': [
            'experimental_methods', 'gan_based_generation', 'autoencoder_generation'
        ]
    }
    
    # Apply parameters to appropriate sections
    for section, param_list in param_sections.items():
        section_config = final_config.setdefault(section, {})
        for param in param_list:
            if param in cleaned_params:
                section_config[param] = cleaned_params[param]
    
    # Set up comprehensive defaults
    core_config = final_config.setdefault('core_generation', {})
    data_structure_config = final_config.setdefault('data_structure', {})
    generation_config = final_config.setdefault('generation_methods', {})
    normal_config = final_config.setdefault('normal_data', {})
    attack_config = final_config.setdefault('attack_data', {})
    anomaly_config = final_config.setdefault('anomaly_config', {})
    splitting_config = final_config.setdefault('data_splitting', {})
    noise_config = final_config.setdefault('noise_parameters', {})
    feature_config = final_config.setdefault('feature_engineering', {})
    quality_config = final_config.setdefault('quality_control', {})
    advanced_config = final_config.setdefault('advanced_generation', {})
    monitoring_config = final_config.setdefault('monitoring', {})
    export_config = final_config.setdefault('export', {})
    
    # Handle silent mode - override verbose and progress_bar if silent is True
    silent_mode = monitoring_config.get('silent', False)
    if silent_mode:
        # Force silent mode behavior
        monitoring_config['verbose'] = False
        monitoring_config['progress_bar'] = False
    
    # Set intelligent defaults
    normal_samples = core_config.setdefault('normal_samples', NORMAL_SAMPLES)
    attack_samples = core_config.setdefault('attack_samples', ATTACK_SAMPLES)
    features = core_config.setdefault('features', FEATURES)
    anomaly_factor = core_config.setdefault('anomaly_factor', ANOMALY_FACTOR)
    random_state = core_config.setdefault('random_state', RANDOM_STATE)
    
    # Data structure defaults
    label_column = data_structure_config.setdefault('label_column', 'Label')
    feature_prefix = data_structure_config.setdefault('feature_prefix', 'feature')
    output_format = data_structure_config.setdefault('output_format', 'dict')
    data_type = data_structure_config.setdefault('data_type', 'float32')
    
    # Generation method defaults
    generation_method = generation_config.setdefault('generation_method', 'mixed')
    distribution_type = generation_config.setdefault('distribution_type', 'gaussian')
    cluster_centers = generation_config.setdefault('cluster_centers', 3)
    cluster_variance = generation_config.setdefault('cluster_variance', 0.1)
    
    # Normal data defaults
    normal_mean = normal_config.setdefault('normal_mean', 0.5)
    normal_std = normal_config.setdefault('normal_std', 0.1)
    normal_distribution = normal_config.setdefault('normal_distribution', 'gaussian')
    normal_clusters = normal_config.setdefault('normal_clusters', 3)
    
    # Attack data defaults
    attack_mean = attack_config.setdefault('attack_mean', 0.7)
    attack_std = attack_config.setdefault('attack_std', 0.2)
    attack_distribution = attack_config.setdefault('attack_distribution', 'mixed')
    attack_types = attack_config.setdefault('attack_types', ['high_variance', 'shifted_mean', 'sparse_extreme', 'clustered_outliers'])
    
    # Anomaly configuration defaults
    anomaly_sparsity = anomaly_config.setdefault('anomaly_sparsity', 0.3)
    anomaly_intensity = anomaly_config.setdefault('anomaly_intensity', 2.0)
    point_anomalies = anomaly_config.setdefault('point_anomalies', True)
    collective_anomalies = anomaly_config.setdefault('collective_anomalies', False)
    
    # Splitting defaults
    validation_split = splitting_config.setdefault('validation_split', 0.2)
    # Use all attack data for test
    test_split = splitting_config.setdefault('test_split', 1.0)
    shuffle = splitting_config.setdefault('shuffle', True)
    
    # Noise defaults
    noise_level = noise_config.setdefault('noise_level', 0.01)
    noise_type = noise_config.setdefault('noise_type', 'gaussian')
    
    # Feature engineering defaults
    correlated_features = feature_config.setdefault('correlated_features', False)
    informative_features = feature_config.setdefault('informative_features', min(features, max(5, features // 2)))
    
    # Quality control defaults
    class_balance_ratio = quality_config.setdefault('class_balance_ratio', attack_samples / normal_samples)
    minimum_separation = quality_config.setdefault('minimum_separation', 0.1)
    
    # Monitoring defaults - respect silent mode
    verbose = monitoring_config.setdefault('verbose', False)
    progress_bar = monitoring_config.setdefault('progress_bar', not silent_mode)
    log_generation_stats = monitoring_config.setdefault('log_generation_stats', True)
    generation_report = monitoring_config.setdefault('generation_report', True)
    
    # Set up logging level
    if verbose:
        original_level = logger.level
        logger.setLevel(logging.INFO)
    
    if verbose:
        logger.info("Starting comprehensive synthetic data generation")
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'current_substage': None,
        'samples_generated': 0,
        'features_created': 0,
        'attack_types_processed': 0,
        'data_quality_score': 0.0,
        'generation_quality': 0.0
    }
    
    # Initialize generation statistics
    generation_stats = {
        'start_time': start_time.isoformat(),
        'normal_samples': normal_samples,
        'attack_samples': attack_samples,
        'features': features,
        'anomaly_factor': anomaly_factor,
        'random_state': random_state,
        'generation_method': generation_method,
        'config_applied': final_config,
        'stages_completed': [],
        'warnings_encountered': [],
        'performance_metrics': {},
        'log_generation_stats': {}
    }
    
    try:
        # Validate parameters
        if normal_samples <= 0 or attack_samples <= 0 or features <= 0:
            raise ValueError("Sample counts and features must be positive")
        
        if not 0 < validation_split < 1:
            raise ValueError("Validation split must be between 0 and 1")
        
        if anomaly_factor <= 0:
            raise ValueError("Anomaly factor must be positive")
        
        # Initialize random number generator
        np.random.seed(random_state)
        
        # Calculate total stages for progress tracking
        total_stages = 10  # Configuration, Setup, Normal Data, Attack Data, Correlation, Noise, Scaling, Splitting, Validation, Finalization
        
        # Use progress bar only if not in silent mode and progress_bar is True
        if not silent_mode and progress_bar:
            bar_context = alive_bar(total_stages, title='Synthetic Data Generation\t', unit='stages')
        else:
            # Create a dummy context manager that does nothing
            class DummyBar:
                def __enter__(self):
                    return self
                def __exit__(self, *args):
                    pass
                def __call__(self):
                    pass
                def __setattr__(self, name, value):
                    pass
            bar_context = DummyBar()
        
        with bar_context as main_bar:
            
            # STAGE 1: Configuration and Validation
            progress_data['current_stage'] = "Configuration"
            if not silent_mode:
                main_bar.text = "Validating parameters and setting up configuration..."
            
            if verbose:
                logger.info(f"Generating synthetic data: {normal_samples} normal, {attack_samples} attack samples")
                logger.info(f"Features: {features}, method: {generation_method}, anomaly_factor: {anomaly_factor}")
            
            # Generate feature names
            if feature_names is None:
                feature_names = [f"{feature_prefix}_{i}" for i in range(features)]
            elif len(feature_names) != features:
                if verbose:
                    logger.warning(f"Feature names length ({len(feature_names)}) doesn't match features ({features})")
                feature_names = [f"{feature_prefix}_{i}" for i in range(features)]
            
            progress_data['features_created'] = len(feature_names)
            if not silent_mode:
                main_bar.text = "Configuration complete"
            generation_stats['stages_completed'].append('configuration')
            if not silent_mode:
                main_bar()
            
            # STAGE 2: Data Generation Setup
            progress_data['current_stage'] = "Initialization"
            if not silent_mode:
                main_bar.text = "Initializing data generation framework..."
            
            # Initialize data containers
            X_normal = None
            X_attack_parts = []
            attack_type_counts = {}
            
            if not silent_mode:
                main_bar.text = "Initialization complete"
            generation_stats['stages_completed'].append('initialization')
            if not silent_mode:
                main_bar()
            
            # STAGE 3: Normal Data Generation
            progress_data['current_stage'] = "Normal Data Generation"
            if not silent_mode:
                main_bar.text = f"Generating {normal_samples} normal samples..."
            
            # Generate normal data based on method
            if generation_method == 'sklearn':
                if not silent_mode:
                    main_bar.text = f"Using sklearn: {normal_samples} samples, {features} features..."
                
                if verbose:
                    logger.info(f"Using sklearn: {normal_samples} samples, {features} features, {informative_features} informative features, {normal_clusters} clusters")
                
                X_normal, _ = make_classification(
                    n_samples=normal_samples,
                    n_features=features,
                    n_informative=informative_features,
                    n_redundant=max(0, features - informative_features),
                    n_clusters_per_class=normal_clusters,
                    class_sep=minimum_separation,
                    random_state=random_state
                )
                # Convert to probability-like values
                if not silent_mode:
                    main_bar.text = "Scaling sklearn data to [0,1] range..."
                scaler = MinMaxScaler()
                X_normal = scaler.fit_transform(X_normal)
                
            elif generation_method == 'clustering':
                if not silent_mode:
                    main_bar.text = f"Using clustering: {normal_samples} samples, {normal_clusters} clusters..."
                
                if verbose:
                    logger.info(f"Using clustering: {normal_samples} samples, {features} features, {normal_clusters} clusters")

                X_normal, _ = make_blobs(
                    n_samples=normal_samples,
                    centers=normal_clusters,
                    n_features=features,
                    cluster_std=cluster_variance,
                    random_state=random_state
                )
                # Normalize to [0, 1] range
                if not silent_mode:
                    main_bar.text = "Normalizing cluster data to [0,1] range..."
                X_normal = (X_normal - X_normal.min()) / (X_normal.max() - X_normal.min())
                
            elif generation_method == 'mixed':
                if not silent_mode:
                    main_bar.text = f"Using {normal_distribution} distribution for normal data..."
                
                if verbose:
                    logger.info(f"Using mixed distribution generation: {normal_distribution} for normal data, {normal_samples} samples, {features} features")
                
                if normal_distribution == 'gaussian':
                    X_normal = np.random.normal(normal_mean, normal_std, (normal_samples, features))
                elif normal_distribution == 'uniform':
                    bounds = normal_config.get('normal_bounds', (0.0, 1.0))
                    X_normal = np.random.uniform(bounds[0], bounds[1], (normal_samples, features))
                elif normal_distribution == 'beta':
                    alpha, beta = normal_config.get('distribution_parameters', {}).get('beta_params', (2, 2))
                    X_normal = np.random.beta(alpha, beta, (normal_samples, features))
                elif normal_distribution == 'gamma':
                    shape, scale = normal_config.get('distribution_parameters', {}).get('gamma_params', (2, 0.1))
                    X_normal = np.random.gamma(shape, scale, (normal_samples, features))
                else:
                    if verbose:
                        logger.warning(f"Unknown normal distribution '{normal_distribution}', using gaussian")
                    X_normal = np.random.normal(normal_mean, normal_std, (normal_samples, features))
                
                # Ensure values are in reasonable range
                if not silent_mode:
                    main_bar.text = "Clipping normal data to [0,1] range..."
                X_normal = np.clip(X_normal, 0.0, 1.0)
                
            else:
                raise ValueError(f"Unknown generation method: {generation_method}")
            
            progress_data['samples_generated'] += normal_samples
            if not silent_mode:
                main_bar.text = f"Generated {normal_samples} normal samples"
            generation_stats['stages_completed'].append('normal_data_generation')
            if not silent_mode:
                main_bar()
            
            # STAGE 4: Attack Data Generation
            progress_data['current_stage'] = "Attack Data Generation"
            if not silent_mode:
                main_bar.text = f"Generating {attack_samples} attack samples across {len(attack_types)} types..."
            
            if verbose:
                logger.info(f"Generating attack data with types: {attack_types}")
            
            # Calculate samples per attack type
            samples_per_type = attack_samples // len(attack_types)
            remainder = attack_samples % len(attack_types)
            
            # Generate each attack type with progress tracking using the main bar
            for i, attack_type in enumerate(attack_types):
                progress_data['current_substage'] = f"Attack Type: {attack_type}"
                if not silent_mode:
                    main_bar.text = f"Generating {attack_type}: {i+1}/{len(attack_types)} attack types..."
                
                n_samples = samples_per_type + (1 if i < remainder else 0)
                attack_type_counts[attack_type] = n_samples
                
                if attack_type == 'high_variance':
                    # High variance anomalies
                    X_attack_type = np.random.normal(
                        normal_mean, 
                        normal_std * anomaly_factor * 2, 
                        (n_samples, features)
                    )
                    
                elif attack_type == 'shifted_mean':
                    # Shifted mean anomalies
                    shift = anomaly_factor * 0.3
                    X_attack_type = np.random.normal(
                        normal_mean + shift, 
                        normal_std, 
                        (n_samples, features)
                    )
                    
                elif attack_type == 'sparse_extreme':
                    # Sparse extreme anomalies
                    X_attack_type = np.random.normal(normal_mean, normal_std, (n_samples, features))
                    # Make some features extreme
                    extreme_mask = np.random.random((n_samples, features)) < anomaly_sparsity
                    extreme_values = np.random.choice([0.05, 0.95], size=np.sum(extreme_mask))
                    X_attack_type[extreme_mask] = extreme_values * anomaly_intensity
                    
                elif attack_type == 'clustered_outliers':
                    # Clustered outliers
                    outlier_center = normal_mean + (0.4 * anomaly_factor)
                    X_attack_type = np.random.normal(
                        outlier_center, 
                        cluster_variance, 
                        (n_samples, features)
                    )
                    
                elif attack_type == 'mixed_distribution':
                    # Mixed distribution anomalies
                    n_mixed = n_samples // 2
                    X_part1 = np.random.uniform(0.8, 1.0, (n_mixed, features)) * anomaly_factor
                    X_part2 = np.random.exponential(0.1 * anomaly_factor, (n_samples - n_mixed, features))
                    X_attack_type = np.vstack([X_part1, X_part2])
                    
                elif attack_type == 'temporal_anomalies':
                    # Temporal pattern anomalies
                    X_attack_type = np.random.normal(normal_mean, normal_std, (n_samples, features))
                    # Add temporal patterns
                    for j in range(features):
                        temporal_pattern = np.sin(np.linspace(0, 2*np.pi, n_samples)) * anomaly_factor * 0.2
                        X_attack_type[:, j] += temporal_pattern
                        
                elif attack_type == 'contextual_anomalies':
                    # Context-dependent anomalies
                    X_attack_type = np.random.normal(normal_mean, normal_std, (n_samples, features))
                    # Make anomalous in specific feature combinations
                    context_features = np.random.choice(features, size=min(3, features), replace=False)
                    for cf in context_features:
                        mask = X_attack_type[:, cf] > normal_mean
                        X_attack_type[mask, cf] *= (1 + anomaly_factor)
                        
                else:
                    if verbose:
                        logger.warning(f"Unknown attack type '{attack_type}', using high_variance")
                    X_attack_type = np.random.normal(
                        normal_mean, 
                        normal_std * anomaly_factor * 2, 
                        (n_samples, features)
                    )
                
                # Clip to valid range
                X_attack_type = np.clip(X_attack_type, 0.0, 1.0)
                X_attack_parts.append(X_attack_type)
                
                progress_data['samples_generated'] += n_samples
                progress_data['attack_types_processed'] += 1
                if not silent_mode:
                    main_bar.text = f"Generated {n_samples} {attack_type} samples ({i+1}/{len(attack_types)})"
            
            # Combine all attack data
            if not silent_mode:
                main_bar.text = "Combining all attack data..."
            X_attack = np.vstack(X_attack_parts)
            generation_stats['attack_type_counts'] = attack_type_counts
            
            if not silent_mode:
                main_bar.text = f"Generated {attack_samples} attack samples across {len(attack_types)} types"
            generation_stats['stages_completed'].append('attack_data_generation')
            if not silent_mode:
                main_bar()
            
            # STAGE 5: Feature Correlation
            progress_data['current_stage'] = "Feature Engineering"
            if not silent_mode:
                main_bar.text = "Applying feature correlation structure..."
            
            # Add correlation structure if requested
            if correlated_features and features > 1:
                if verbose:
                    logger.info("Adding correlation structure to normal data")
                
                correlation_strength = normal_config.get('normal_correlation', 0.3)
                
                if correlation_matrix is not None and correlation_matrix.shape == (features, features):
                    # Use provided correlation matrix
                    if not silent_mode:
                        main_bar.text = "Applying provided correlation matrix..."
                    L = np.linalg.cholesky(correlation_matrix)
                    X_normal = X_normal @ L.T
                else:
                    # Create simple correlation structure
                    if not silent_mode:
                        main_bar.text = "Creating correlation structure..."
                    correlation_indices = np.random.choice(features, size=min(features//2, 5), replace=False)
                    for i in range(len(correlation_indices)-1):
                        idx1, idx2 = correlation_indices[i], correlation_indices[i+1]
                        X_normal[:, idx2] = (X_normal[:, idx2] * (1 - correlation_strength) + X_normal[:, idx1] * correlation_strength)
                
                # Renormalize
                if not silent_mode:
                    main_bar.text = "Renormalizing correlated data..."
                X_normal = np.clip(X_normal, 0.0, 1.0)
                generation_stats['correlation_applied'] = True
            
            if not silent_mode:
                main_bar.text = "Feature engineering complete"
            generation_stats['stages_completed'].append('feature_engineering')
            if not silent_mode:
                main_bar()
            
            # STAGE 6: Noise Addition
            progress_data['current_stage'] = "Noise Addition"
            if not silent_mode:
                main_bar.text = f"Adding {noise_type} noise..."
            
            # Add noise if requested
            if noise_level > 0:
                if verbose:
                    logger.info(f"Adding {noise_type} noise with level {noise_level}")
                
                if noise_type == 'gaussian':
                    noise_std = noise_config.get('gaussian_noise_std', noise_level)
                    if not silent_mode:
                        main_bar.text = f"Adding Gaussian noise (std={noise_std:.3f})..."
                    normal_noise = np.random.normal(0, noise_std, X_normal.shape)
                    attack_noise = np.random.normal(0, noise_std, X_attack.shape)
                    
                    X_normal = X_normal + normal_noise
                    X_attack = X_attack + attack_noise
                    
                elif noise_type == 'uniform':
                    noise_range = noise_config.get('uniform_noise_range', noise_level)
                    if not silent_mode:
                        main_bar.text = f"Adding uniform noise (range=±{noise_range:.3f})..."
                    normal_noise = np.random.uniform(-noise_range, noise_range, X_normal.shape)
                    attack_noise = np.random.uniform(-noise_range, noise_range, X_attack.shape)
                    
                    X_normal = X_normal + normal_noise
                    X_attack = X_attack + attack_noise
                    
                elif noise_type == 'salt_pepper':
                    sp_ratio = noise_config.get('salt_pepper_ratio', 0.5)
                    if not silent_mode:
                        main_bar.text = f"Adding salt & pepper noise (ratio={sp_ratio:.2f})..."
                    
                    # Apply salt and pepper noise
                    for X_data in [X_normal, X_attack]:
                        noise_mask = np.random.random(X_data.shape) < noise_level
                        salt_mask = noise_mask & (np.random.random(X_data.shape) < sp_ratio)
                        pepper_mask = noise_mask & ~salt_mask
                        
                        X_data[salt_mask] = 1.0
                        X_data[pepper_mask] = 0.0
                
                # Ensure values remain in valid range
                if not silent_mode:
                    main_bar.text = "Clipping noisy data to [0,1] range..."
                X_normal = np.clip(X_normal, 0.0, 1.0)
                X_attack = np.clip(X_attack, 0.0, 1.0)
                generation_stats['noise_applied'] = True
                generation_stats['noise_type'] = noise_type
                generation_stats['noise_level'] = noise_level
            
            if not silent_mode:
                main_bar.text = "Noise addition complete"
            generation_stats['stages_completed'].append('noise_addition')
            if not silent_mode:
                main_bar()
            
            # STAGE 7: Feature Scaling
            progress_data['current_stage'] = "Feature Scaling"
            if not silent_mode:
                main_bar.text = "Applying feature scaling..."
            
            # Apply feature scaling if requested
            scaler = None
            if feature_scaling:
                if not silent_mode:
                    main_bar.text = f"Applying {feature_scaling} scaling..."
                
                if verbose:
                    logger.info(f"Applying {feature_scaling} scaling to data")
                
                if feature_scaling == 'standard':
                    scaler = StandardScaler()
                elif feature_scaling == 'minmax':
                    scaler = MinMaxScaler()
                else:
                    if verbose:
                        logger.warning(f"Unknown scaling method '{feature_scaling}'")
                    scaler = MinMaxScaler()
                
                # Fit on normal data and transform both
                X_normal = scaler.fit_transform(X_normal)
                X_attack = scaler.transform(X_attack)
                generation_stats['scaling_applied'] = True
                generation_stats['scaling_method'] = feature_scaling
            
            if not silent_mode:
                main_bar.text = "Feature scaling complete"
            generation_stats['stages_completed'].append('feature_scaling')
            if not silent_mode:
                main_bar()
            
            # STAGE 8: Data Splitting
            progress_data['current_stage'] = "Data Splitting"
            if not silent_mode:
                main_bar.text = "Splitting data into train/validation/test sets..."
            
            # Create labels
            y_normal = np.zeros(normal_samples, dtype=np.int32)
            y_attack = np.ones(attack_samples, dtype=np.int32)
            
            # Perform data splitting
            if verbose:
                logger.info(f"Splitting data: validation={validation_split}, test={test_split}")
            
            # Split normal data for training and validation
            if validation_split > 0:
                if not silent_mode:
                    main_bar.text = f"Splitting normal data (validation={validation_split:.1%})..."
                X_train_normal, X_val_normal, y_train_normal, y_val_normal = train_test_split(
                    X_normal, y_normal,
                    test_size=validation_split,
                    random_state=random_state,
                    shuffle=shuffle
                )
            else:
                X_train_normal = X_normal
                X_val_normal = np.array([]).reshape(0, features)
                y_train_normal = y_normal
                y_val_normal = np.array([])
            
            # Handle test data
            if test_split > 0 and test_split <= 1.0:
                if test_split < 1.0:
                    if not silent_mode:
                        main_bar.text = f"Splitting attack data (test={test_split:.1%})..."
                    test_size = int(len(X_attack) * test_split)
                    indices = np.random.choice(len(X_attack), size=test_size, replace=False)
                    X_test = X_attack[indices]
                    y_test = y_attack[indices]
                else:
                    if not silent_mode:
                        main_bar.text = "Using all attack data for testing..."
                    X_test = X_attack
                    y_test = y_attack
            else:
                X_test = np.array([]).reshape(0, features)
                y_test = np.array([])
            
            generation_stats.update({
                'train_samples': len(X_train_normal),
                'val_samples': len(X_val_normal),
                'test_samples': len(X_test)
            })
            
            if not silent_mode:
                main_bar.text = "Data splitting complete"
            generation_stats['stages_completed'].append('data_splitting')
            if not silent_mode:
                main_bar()
            
            # STAGE 9: Data Validation
            progress_data['current_stage'] = "Data Validation"
            if not silent_mode:
                main_bar.text = "Validating generated data quality..."
            
            # Data quality validation if requested
            validation_results = {}
            quality_score = 0.5  # Default quality score
            
            if final_config.get('validation', {}).get('validate_generation', True):
                if verbose:
                    logger.info("Performing data quality validation")
                
                # Basic statistics
                validation_results['normal_data_stats'] = {
                    'mean': np.mean(X_train_normal, axis=0).tolist(),
                    'std': np.std(X_train_normal, axis=0).tolist(),
                    'min': np.min(X_train_normal, axis=0).tolist(),
                    'max': np.max(X_train_normal, axis=0).tolist()
                }
                
                if len(X_test) > 0:
                    validation_results['attack_data_stats'] = {
                        'mean': np.mean(X_test, axis=0).tolist(),
                        'std': np.std(X_test, axis=0).tolist(),
                        'min': np.min(X_test, axis=0).tolist(),
                        'max': np.max(X_test, axis=0).tolist()
                    }
                
                # Statistical tests if requested
                if final_config.get('validation', {}).get('statistical_tests', False):
                    if not silent_mode:
                        main_bar.text = "Running statistical tests..."
                    
                    if verbose:
                        logger.info(f"Performing statistical tests on generated data")
                    
                    validation_results['statistical_tests'] = {}
                    
                    # Normality tests on normal data
                    # Test first 5 features
                    for i in range(min(5, features)):
                        feature_data = X_train_normal[:, i]
                        if len(feature_data) > 3:
                            try:
                                stat, p_value = stats.normaltest(feature_data)
                                validation_results['statistical_tests'][f'normality_feature_{i}'] = {
                                    'statistic': float(stat),
                                    'p_value': float(p_value),
                                    'is_normal': p_value > 0.05
                                }
                            except Exception as e:
                                if verbose:
                                    logger.warning(f"Normality test failed for feature {i}: {e}")
                    
                    # Separation test
                    if len(X_test) > 0:
                        try:
                            # Simple separation measure
                            normal_centroid = np.mean(X_train_normal, axis=0)
                            attack_centroid = np.mean(X_test, axis=0)
                            separation = np.linalg.norm(normal_centroid - attack_centroid)
                            
                            validation_results['statistical_tests']['class_separation'] = {
                                'euclidean_distance': float(separation),
                                'normalized_distance': float(separation / np.sqrt(features))
                            }
                        except Exception as e:
                            if verbose:
                                logger.warning(f"Separation test failed: {e}")
                
                # Calculate data quality score
                if not silent_mode:
                    main_bar.text = "Calculating data quality metrics..."
                quality_metrics = []
                
                # Sample adequacy
                sample_score = min(1.0, (len(X_train_normal) + len(X_test)) / max(1000, normal_samples + attack_samples))
                quality_metrics.append(('sample_adequacy', sample_score))
                
                # Feature quality
                feature_score = min(1.0, features / max(10, features))
                quality_metrics.append(('feature_quality', feature_score))
                
                # Class separation (if test data exists)
                if len(X_test) > 0:
                    try:
                        separation = np.linalg.norm(np.mean(X_train_normal, axis=0) - np.mean(X_test, axis=0))
                        separation_score = min(1.0, separation / np.sqrt(features))
                        quality_metrics.append(('class_separation', separation_score))
                    except:
                        quality_metrics.append(('class_separation', 0.5))
                else:
                    quality_metrics.append(('class_separation', 0.5))
                
                # Overall quality score
                quality_score = sum(score for _, score in quality_metrics) / len(quality_metrics)
                progress_data['data_quality_score'] = quality_score
                progress_data['generation_quality'] = quality_score
                validation_results['quality_score'] = quality_score
                validation_results['quality_metrics'] = dict(quality_metrics)
            
            if not silent_mode:
                main_bar.text = f"Data validation complete (Quality: {quality_score:.3f})"
            generation_stats['stages_completed'].append('data_validation')
            if not silent_mode:
                main_bar()
            
            # STAGE 10: Finalization
            progress_data['current_stage'] = "Finalization"
            if not silent_mode:
                main_bar.text = "Finalizing data preparation and metadata..."
            
            # Convert to specified data type
            if data_type == 'float32':
                if not silent_mode:
                    main_bar.text = "Converting to float32..."
                X_train_normal = X_train_normal.astype(np.float32)
                X_val_normal = X_val_normal.astype(np.float32)
                X_test = X_test.astype(np.float32)
            elif data_type == 'float64':
                if not silent_mode:
                    main_bar.text = "Converting to float64..."
                X_train_normal = X_train_normal.astype(np.float64)
                X_val_normal = X_val_normal.astype(np.float64)
                X_test = X_test.astype(np.float64)
            
            # Collect log generation statistics
            if log_generation_stats:
                # Calculate statistics
                total_samples = normal_samples + attack_samples
                current_time = datetime.now()
                generation_duration = (current_time - start_time).total_seconds()
                
                # Calculate data quality metrics
                normal_data_quality = 0.0
                attack_data_quality = 0.0
                separation_quality = 0.0
                effective_noise_level = 0.0
                
                try:
                    # Normal data quality (based on consistency)
                    if len(X_train_normal) > 0:
                        normal_variance = np.mean(np.var(X_train_normal, axis=0))
                        normal_data_quality = max(0.0, 1.0 - normal_variance * 2)  # Lower variance = higher quality
                    
                    # Attack data separation quality
                    if len(X_test) > 0 and len(X_train_normal) > 0:
                        normal_mean = np.mean(X_train_normal, axis=0)
                        attack_mean = np.mean(X_test, axis=0)
                        separation_distance = np.linalg.norm(normal_mean - attack_mean)
                        separation_quality = min(1.0, separation_distance / np.sqrt(features))
                        attack_data_quality = separation_quality
                    
                    # Estimate effective noise impact
                    if noise_level > 0:
                        effective_noise_level = min(1.0, noise_level * 5)  # Simplified estimation
                except Exception as e:
                    if verbose:
                        logger.warning(f"Could not calculate some quality metrics: {e}")
                
                # Feature statistics
                informative_count = feature_config.get('informative_features', min(features, max(5, features // 2)))
                redundant_count = feature_config.get('redundant_features', 0)
                
                # Compile log statistics
                stats = {
                    # Core sample statistics
                    'total_samples': total_samples,
                    'normal_samples': normal_samples,
                    'attack_samples': attack_samples,
                    'class_imbalance_ratio': attack_samples / normal_samples if normal_samples > 0 else float('inf'),
                    'samples_per_second': total_samples / generation_duration if generation_duration > 0 else 0,
                    
                    # Feature configuration
                    'features': features,
                    'informative_features': informative_count,
                    'redundant_features': redundant_count,
                    'feature_names_count': len(feature_names),
                    'feature_prefix': feature_prefix,
                    
                    # Generation methodology
                    'generation_method': generation_method,
                    'generation_strategy': generation_config.get('generation_strategy', 'default'),
                    'distribution_type': distribution_type,
                    'cluster_centers': cluster_centers,
                    'cluster_variance': cluster_variance,
                    
                    # Normal data characteristics
                    'normal_distribution': normal_distribution,
                    'normal_mean': normal_mean,
                    'normal_std': normal_std,
                    'normal_clusters': normal_clusters,
                    'normal_data_quality': normal_data_quality,
                    
                    # Attack data characteristics
                    'attack_distribution': attack_distribution,
                    'attack_mean': attack_mean,
                    'attack_std': attack_std,
                    'attack_types': attack_types,
                    'attack_type_counts': attack_type_counts,
                    'attack_data_quality': attack_data_quality,
                    
                    # Anomaly configuration
                    'anomaly_factor': anomaly_factor,
                    'anomaly_sparsity': anomaly_sparsity,
                    'anomaly_intensity': anomaly_intensity,
                    'anomaly_patterns': anomaly_config.get('anomaly_patterns', []),
                    'point_anomalies': point_anomalies,
                    'collective_anomalies': collective_anomalies,
                    'contextual_anomalies': contextual_anomalies,
                    'seasonal_anomalies': seasonal_anomalies,
                    
                    # Data splits
                    'train_samples': len(X_train_normal),
                    'validation_samples': len(X_val_normal),
                    'test_samples': len(X_test),
                    'validation_split': validation_split,
                    'test_split': test_split,
                    'shuffle_enabled': shuffle,
                    'stratified_splitting': splitting_config.get('stratified_split', False),
                    
                    # Noise and preprocessing
                    'noise_level': noise_level,
                    'effective_noise_level': effective_noise_level,
                    'noise_type': noise_type,
                    'gaussian_noise_std': noise_config.get('gaussian_noise_std', noise_level),
                    'salt_pepper_ratio': noise_config.get('salt_pepper_ratio', 0.5),
                    'feature_scaling': feature_scaling,
                    'scaling_method': generation_stats.get('scaling_method', 'none'),
                    
                    # Feature engineering
                    'correlated_features': correlated_features,
                    'correlation_strength': normal_config.get('normal_correlation', 0.3),
                    'correlation_applied': generation_stats.get('correlation_applied', False),
                    
                    # Data quality metrics
                    'overall_quality_score': quality_score,
                    'separation_quality': separation_quality,
                    'class_balance_ratio': class_balance_ratio,
                    'minimum_separation': minimum_separation,
                    'overlap_factor': quality_config.get('overlap_factor', 0.0),
                    'separability_score': quality_config.get('separability_score', 0.0),
                    
                    # Performance metrics
                    'generation_duration_seconds': generation_duration,
                    'stages_completed': len(generation_stats['stages_completed']),
                    'total_stages': total_stages,
                    'completion_percentage': (len(generation_stats['stages_completed']) / total_stages) * 100,
                    'memory_estimate_mb': (total_samples * features * 4) / (1024 * 1024),  # float32 estimate
                    
                    # Configuration and reproducibility
                    'random_state': random_state,
                    'deterministic': final_config.get('reproducibility', {}).get('deterministic', True),
                    'config_sections_used': list(final_config.keys()),
                    
                    # Advanced features usage
                    'multimodal_data': advanced_config.get('multimodal_data', False),
                    'mixture_components': advanced_config.get('mixture_components', 1),
                    'non_linear_relationships': advanced_config.get('non_linear_relationships', False),
                    'interaction_features': advanced_config.get('interaction_features', False),
                    'polynomial_degree': advanced_config.get('polynomial_degree', 1),
                    
                    # Temporal features
                    'temporal_data': final_config.get('temporal_parameters', {}).get('temporal_data', False),
                    'time_steps': final_config.get('temporal_parameters', {}).get('time_steps', 1),
                    'temporal_pattern': final_config.get('temporal_parameters', {}).get('temporal_pattern', 'none'),
                    
                    # Statistical properties
                    'skewness_constraint': final_config.get('statistical_properties', {}).get('skewness'),
                    'kurtosis_constraint': final_config.get('statistical_properties', {}).get('kurtosis'),
                    'heavy_tails': final_config.get('statistical_properties', {}).get('heavy_tails', False),
                    
                    # Scalability configuration
                    'batch_generation': final_config.get('scalability', {}).get('batch_generation', False),
                    'batch_size': final_config.get('scalability', {}).get('batch_size', total_samples),
                    'memory_efficient': final_config.get('scalability', {}).get('memory_efficient', False),
                    'parallel_generation': final_config.get('scalability', {}).get('parallel_generation', False),
                    'n_jobs': final_config.get('scalability', {}).get('n_jobs', 1),
                    
                    # Validation and monitoring
                    'validation_performed': final_config.get('validation', {}).get('validate_generation', True),
                    'statistical_tests_performed': final_config.get('validation', {}).get('statistical_tests', False),
                    'quality_metrics_tracked': final_config.get('validation', {}).get('quality_metrics', []),
                    'verbose_logging': verbose,
                    'progress_bar_enabled': progress_bar and not silent_mode,
                    'silent_mode': silent_mode,
                    
                    # Output configuration
                    'output_format': output_format,
                    'data_type': data_type,
                    'label_column': label_column,
                    'save_to_disk': export_config.get('save_data', False),
                    'file_format': export_config.get('file_format', 'none'),
                    'metadata_saved': export_config.get('metadata_file', False),
                    
                    # Progress tracking summary
                    'progress_data': {
                        'current_stage': progress_data['current_stage'],
                        'samples_generated': progress_data['samples_generated'],
                        'features_created': progress_data['features_created'],
                        'attack_types_processed': progress_data['attack_types_processed'],
                        'data_quality_score': progress_data['data_quality_score'],
                        'generation_quality': progress_data['generation_quality']
                    },
                    
                    # Timestamps and timing
                    'generation_start_time': start_time.isoformat(),
                    'generation_end_time': current_time.isoformat(),
                    'generation_date': current_time.strftime('%Y-%m-%d'),
                    'generation_time': current_time.strftime('%H:%M:%S'),
                    
                    # Success and error tracking
                    'completion_status': 'success',
                    'warnings_encountered': len(generation_stats.get('warnings_encountered', [])),
                    'warning_messages': generation_stats.get('warnings_encountered', []),
                    'error_occurred': False,
                    
                    # Resource usage summary
                    'estimated_memory_footprint': {
                        'normal_data_mb': (normal_samples * features * 4) / (1024 * 1024),
                        'attack_data_mb': (attack_samples * features * 4) / (1024 * 1024),
                        'total_mb': (total_samples * features * 4) / (1024 * 1024)
                    },
                    
                    # Data characteristics summary
                    'data_characteristics': {
                        'data_range': (0.0, 1.0),
                        'has_labels': True,
                        'label_types': ['normal', 'attack'],
                        'feature_data_type': data_type,
                        'is_synthetic': True,
                        'generation_version': '1.0'  # Could be made dynamic
                    }
                }
                
                # Add validation results summary if available
                if 'validation_results' in locals() and validation_results:
                    stats['validation_summary'] = {
                        'quality_score': validation_results.get('quality_score', 0.0),
                        'quality_metrics': validation_results.get('quality_metrics', {}),
                        'statistical_tests_count': len(validation_results.get('statistical_tests', {})),
                        'has_normal_stats': 'normal_data_stats' in validation_results,
                        'has_attack_stats': 'attack_data_stats' in validation_results
                    }
                
                # Add configuration fingerprint
                stats['configuration_fingerprint'] = {
                    'core_parameters_hash': hash(frozenset({
                        'normal_samples': normal_samples,
                        'attack_samples': attack_samples,
                        'features': features,
                        'random_state': random_state
                    }.items())),
                    'method_signature': f"{generation_method}_{distribution_type}",
                    'anomaly_config_hash': hash(frozenset({
                        'factor': anomaly_factor,
                        'sparsity': anomaly_sparsity,
                        'intensity': anomaly_intensity
                    }.items()))
                }
                
                # Store in generation stats
                generation_stats['log_generation_stats'] = stats
            
            # Prepare metadata
            total_time = (datetime.now() - start_time).total_seconds()
            
            metadata = {
                # Basic generation info
                'generation_time_seconds': total_time,
                'generation_method': generation_method,
                'distribution_type': distribution_type,
                'random_state': random_state,
                
                # Sample statistics
                'normal_samples': normal_samples,
                'attack_samples': attack_samples,
                'total_samples': normal_samples + attack_samples,
                'features': features,
                'feature_names': feature_names,
                
                # Data splits
                'train_samples': len(X_train_normal),
                'val_samples': len(X_val_normal),
                'test_samples': len(X_test),
                'validation_split': validation_split,
                'test_split': test_split,
                
                # Generation parameters
                'anomaly_factor': anomaly_factor,
                'noise_level': noise_level,
                'noise_type': noise_type,
                'attack_types': attack_types,
                'attack_type_counts': generation_stats.get('attack_type_counts', {}),
                
                # Quality metrics
                'class_balance_ratio': class_balance_ratio,
                'correlated_features': correlated_features,
                'feature_scaling': feature_scaling,
                'data_quality_score': quality_score,
                
                # Configuration applied
                'config_applied': final_config,
                'generation_stats': generation_stats,
                
                # Validation results
                'validation_results': validation_results,
                
                # Data characteristics
                'data_bounds': (0.0, 1.0),
                'label_column': label_column,
                'data_type': data_type,
                'output_format': output_format,
                
                # Progress tracking summary
                'progress_summary': {
                    'stages_completed': len(generation_stats['stages_completed']),
                    'total_samples_generated': progress_data['samples_generated'],
                    'features_created': progress_data['features_created'],
                    'attack_types_processed': progress_data['attack_types_processed'],
                    'final_quality_score': progress_data['data_quality_score']
                }
            }
            
            if not silent_mode:
                main_bar.text = "Finalization complete"
            generation_stats['stages_completed'].append('finalization')
            if not silent_mode:
                main_bar()
        
        # Prepare output data structure
        if output_format == 'dict':
            result = {
                "X_train": X_train_normal,
                "X_val": X_val_normal,
                "X_test": X_test,
                "y_train": y_train_normal,
                "y_val": y_val_normal,
                "y_test": y_test,
                "feature_names": feature_names,
                "metadata": metadata
            }
            
        elif output_format == 'dataframe':
            # Create DataFrames
            if not silent_mode and 'main_bar' in locals():
                main_bar.text = "Creating DataFrames..."
            train_df = pd.DataFrame(X_train_normal, columns=feature_names)
            train_df[label_column] = y_train_normal
            
            val_df = pd.DataFrame(X_val_normal, columns=feature_names)
            val_df[label_column] = y_val_normal
            
            test_df = pd.DataFrame(X_test, columns=feature_names)
            test_df[label_column] = y_test
            
            result = {
                "train_df": train_df,
                "val_df": val_df,
                "test_df": test_df,
                "metadata": metadata
            }
            
        elif output_format == 'sklearn':
            # sklearn-compatible format
            if not silent_mode and 'main_bar' in locals():
                main_bar.text = "Creating sklearn-compatible format..."
            X_combined = np.vstack([X_train_normal, X_val_normal, X_test]) if len(X_val_normal) > 0 and len(X_test) > 0 else X_train_normal
            y_combined = np.hstack([y_train_normal, y_val_normal, y_test]) if len(y_val_normal) > 0 and len(y_test) > 0 else y_train_normal
            
            result = {
                "data": X_combined,
                "target": y_combined,
                "feature_names": feature_names,
                "target_names": ['normal', 'attack'],
                "DESCR": f"Synthetic dataset with {normal_samples} normal and {attack_samples} attack samples",
                "splits": {
                    "X_train": X_train_normal,
                    "X_val": X_val_normal,
                    "X_test": X_test,
                    "y_train": y_train_normal,
                    "y_val": y_val_normal,
                    "y_test": y_test
                },
                "metadata": metadata
            }
            
        else:
            raise ValueError(f"Unknown output format: {output_format}")
        
        # Save data if requested
        export_config = final_config.get('export', {})
        if export_config.get('save_data', False):
            output_path = export_config.get('output_path', 'synthetic_data.csv')
            file_format = export_config.get('file_format', 'csv')
            
            if verbose:
                logger.info(f"Saving synthetic data to {output_path}")
            
            try:
                output_path = Path(output_path)
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                if file_format == 'csv':
                    # Save as CSV
                    if not silent_mode and 'main_bar' in locals():
                        main_bar.text = "Saving as CSV..."
                    combined_X = np.vstack([X_train_normal, X_val_normal, X_test])
                    combined_y = np.hstack([y_train_normal, y_val_normal, y_test])
                    
                    df_combined = pd.DataFrame(combined_X, columns=feature_names)
                    df_combined[label_column] = combined_y
                    df_combined.to_csv(output_path, index=False)
                    
                elif file_format == 'pickle':
                    # Save as pickle
                    if not silent_mode and 'main_bar' in locals():
                        main_bar.text = "Saving as pickle..."
                    with open(output_path, 'wb') as f:
                        pickle.dump(result, f)
                        
                elif file_format == 'numpy':
                    # Save as numpy arrays
                    if not silent_mode and 'main_bar' in locals():
                        main_bar.text = "Saving as numpy arrays..."
                    np.savez(
                        output_path,
                        X_train=X_train_normal,
                        X_val=X_val_normal,
                        X_test=X_test,
                        y_train=y_train_normal,
                        y_val=y_val_normal,
                        y_test=y_test,
                        feature_names=feature_names,
                        metadata=metadata
                    )
                
                # Save metadata if requested
                if export_config.get('metadata_file', False):
                    metadata_path = output_path.with_suffix('.json')
                    if not silent_mode and 'main_bar' in locals():
                        main_bar.text = "Saving metadata..."
                    with open(metadata_path, 'w') as f:
                        # Make metadata JSON serializable
                        serializable_metadata = {}
                        for key, value in metadata.items():
                            try:
                                # Test if serializable
                                json.dumps(value)
                                serializable_metadata[key] = value
                            except TypeError:
                                serializable_metadata[key] = str(value)
                        
                        json.dump(serializable_metadata, f, indent=2)
                    
                    if verbose:
                        logger.info(f"Saved metadata to {metadata_path}")
                
            except Exception as e:
                if verbose:
                    logger.error(f"Failed to save data: {e}")
        
        # Log summary
        if verbose:
            generation_stats['completion_status'] = 'success'
            generation_stats['total_processing_time'] = total_time
            
            logger.info("-" * 40)
            logger.info("SYNTHETIC DATA GENERATION SUMMARY")
            logger.info("-" * 40)
            logger.info(f"Generation method: {generation_method}")
            logger.info(f"Total samples: {normal_samples + attack_samples:,} ({normal_samples:,} normal, {attack_samples:,} attack)")
            logger.info(f"Features: {features} ({data_type})")
            logger.info(f"Attack types: {len(attack_types)} types")
            logger.info(f"Train/Val/Test split: {len(X_train_normal)}/{len(X_val_normal)}/{len(X_test)}")
            logger.info(f"Anomaly factor: {anomaly_factor}")
            logger.info(f"Noise level: {noise_level} ({noise_type})")
            logger.info(f"Feature scaling: {feature_scaling or 'none'}")
            logger.info(f"Correlated features: {correlated_features}")
            logger.info(f"Data quality score: {quality_score:.3f}")
            logger.info(f"Output format: {output_format}")
            logger.info(f"Generation time: {total_time:.2f} seconds")
            logger.info(f"Stages completed: {len(generation_stats['stages_completed'])}/{total_stages}")
            
            # If log generation stats were collected
            if log_generation_stats and 'log_generation_stats' in generation_stats:
                stats = generation_stats['log_generation_stats']
                logger.info("-" * 40)
                logger.info("LOG GENERATION STATISTICS")
                logger.info("-" * 40)
                logger.info(f"Samples: {stats['total_samples']:,} total ({stats['normal_samples']:,} normal, {stats['attack_samples']:,} attack)")
                logger.info(f"Features: {stats['features']} ({stats['informative_features']} informative, {stats['redundant_features']} redundant)")
                logger.info(f"Quality: {stats['overall_quality_score']:.3f} overall, {stats['separation_quality']:.3f} separation")
                logger.info(f"Performance: {stats['generation_duration_seconds']:.2f}s, {stats['samples_per_second']:.1f} samples/sec")
                logger.info(f"Methods: {stats['generation_method']}, {len(stats['attack_types'])} attack types")
                logger.info(f"Completion: {stats['stages_completed']}/{stats['total_stages']} stages ({stats['completion_percentage']:.1f}%)")
                logger.info(f"Memory estimate: {stats['memory_estimate_mb']:.2f} MB")
                logger.info(f"Class imbalance: {stats['class_imbalance_ratio']:.3f}")
                logger.info(f"Normal data quality: {stats['normal_data_quality']:.3f}")
                logger.info(f"Attack data quality: {stats['attack_data_quality']:.3f}")
                logger.info(f"Noise impact: {stats['effective_noise_level']:.3f}")
                
                # Configuration details
                logger.info(f"Distribution: {stats['distribution_type']}")
                logger.info(f"Clusters: {stats['cluster_centers']} centers, {stats['cluster_variance']:.3f} variance")
                logger.info(f"Anomaly config: factor={stats['anomaly_factor']}, sparsity={stats['anomaly_sparsity']:.3f}, intensity={stats['anomaly_intensity']:.2f}")
                logger.info(f"Validation performed: {stats['validation_performed']}")
                logger.info(f"Statistical tests: {stats['statistical_tests_performed']}")
                
                # Advanced features summary
                advanced_features = []
                if stats['multimodal_data']:
                    advanced_features.append(f"multimodal({stats['mixture_components']} components)")
                if stats['non_linear_relationships']:
                    advanced_features.append(f"non-linear(degree {stats['polynomial_degree']})")
                if stats['interaction_features']:
                    advanced_features.append("interactions")
                if stats['temporal_data']:
                    advanced_features.append(f"temporal({stats['time_steps']} steps)")
                
                if advanced_features:
                    logger.info(f"Advanced features: {', '.join(advanced_features)}")
                
                # Resource usage
                memory_footprint = stats['estimated_memory_footprint']
                logger.info(f"Memory footprint: {memory_footprint['total_mb']:.2f} MB total ({memory_footprint['normal_data_mb']:.2f} MB normal, {memory_footprint['attack_data_mb']:.2f} MB attack)")
                
                # Configuration sections used
                config_sections = len(stats['config_sections_used'])
                logger.info(f"Configuration sections: {config_sections} sections used")
                
                # Warnings and errors
                if stats['warnings_encountered'] > 0:
                    logger.info(f"Warnings encountered: {stats['warnings_encountered']}")
            
            logger.info("-" * 40)

        # Restore original logging level if it was changed
        if verbose and 'original_level' in locals():
            logger.setLevel(original_level)
        
        return result
        
    except Exception as e:
        # Update generation stats with error information
        generation_stats['completion_status'] = 'failed'
        generation_stats['error_message'] = str(e)
        generation_stats['error_traceback'] = traceback.format_exc()
        
        # Restore original logging level on error
        if verbose and 'original_level' in locals():
            logger.setLevel(original_level)
        
        error_msg = f"Synthetic data generation failed: {str(e)}"
        if verbose:
            logger.error(error_msg)
            logger.error(f"Full traceback: {traceback.format_exc()}")
            
            # Provide helpful error context
            logger.error(f"Generation parameters: normal={normal_samples}, attack={attack_samples}, features={features}")
            logger.error(f"Configuration used: {final_config}")
            logger.error(f"Stages completed: {generation_stats['stages_completed']}")
        
        raise RuntimeError(error_msg)

def validate_synthetic_data(
    data_dict: Dict[str, Union[np.ndarray, Dict[str, Any]]],
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Perform comprehensive validation of synthetic data quality and characteristics.
    
    Args:
        data_dict: Data dictionary from generate_synthetic_data
        config: Configuration dictionary
        
    Returns:
        Dictionary with validation results and quality metrics
    """
    validation_results = {
        'passed': True,
        'warnings': [],
        'errors': [],
        'quality_metrics': {},
        'statistical_tests': {},
        'recommendations': []
    }
    
    try:
        # Extract data components
        X_train = data_dict.get('X_train', np.array([]))
        X_val = data_dict.get('X_val', np.array([]))
        X_test = data_dict.get('X_test', np.array([]))
        metadata = data_dict.get('metadata', {})
        
        # Basic data presence checks
        if X_train.size == 0:
            validation_results['errors'].append("No training data found")
            validation_results['passed'] = False
            return validation_results
        
        # Shape consistency checks
        expected_features = X_train.shape[1]
        for name, data in [('validation', X_val), ('test', X_test)]:
            if data.size > 0 and data.shape[1] != expected_features:
                validation_results['errors'].append(
                    f"{name} data has {data.shape[1]} features, expected {expected_features}"
                )
                validation_results['passed'] = False
        
        # Data quality checks
        for name, data in [('train', X_train), ('val', X_val), ('test', X_test)]:
            if data.size > 0:
                # Check for invalid values
                if np.any(np.isnan(data)):
                    validation_results['warnings'].append(f"{name} data contains NaN values")
                
                if np.any(np.isinf(data)):
                    validation_results['warnings'].append(f"{name} data contains infinite values")
                
                # Check data ranges
                data_min, data_max = np.min(data), np.max(data)
                validation_results['quality_metrics'][f'{name}_range'] = (float(data_min), float(data_max))
                
                if data_min < -10 or data_max > 10:
                    validation_results['warnings'].append(f"{name} data has extreme values")
        
        # Statistical validation
        if len(X_train) > 10:
            # Distribution tests
            # Test first 3 features
            for i in range(min(3, X_train.shape[1])):
                feature_data = X_train[:, i]
                
                # Normality test
                try:
                    stat, p_value = stats.normaltest(feature_data)
                    validation_results['statistical_tests'][f'normality_feature_{i}'] = {
                        'statistic': float(stat),
                        'p_value': float(p_value)
                    }
                except Exception as e:
                    validation_results['warnings'].append(f"Statistical test failed for feature {i}: {e}")
        
        # Class separation analysis
        if X_test.size > 0:
            try:
                train_centroid = np.mean(X_train, axis=0)
                test_centroid = np.mean(X_test, axis=0)
                separation = np.linalg.norm(train_centroid - test_centroid)
                
                validation_results['quality_metrics']['class_separation'] = float(separation)
                validation_results['quality_metrics']['normalized_separation'] = float(separation / np.sqrt(expected_features))
                
                if separation < 0.1:
                    validation_results['warnings'].append("Low class separation detected")
                    validation_results['recommendations'].append("Consider increasing anomaly_factor")
                
            except Exception as e:
                validation_results['warnings'].append(f"Class separation analysis failed: {e}")
        
        # Metadata validation
        expected_samples = metadata.get('normal_samples', 0) + metadata.get('attack_samples', 0)
        actual_samples = len(X_train) + len(X_val) + len(X_test)
        
        if abs(expected_samples - actual_samples) > 1:
            validation_results['warnings'].append(
                f"Sample count mismatch: expected {expected_samples}, got {actual_samples}"
            )
        
        # Calculate overall quality score
        n_warnings = len(validation_results['warnings'])
        n_errors = len(validation_results['errors'])
        quality_score = max(0.0, 1.0 - (n_warnings * 0.1 + n_errors * 0.5))
        validation_results['quality_metrics']['overall_quality_score'] = quality_score
        
        if quality_score < 0.7:
            validation_results['recommendations'].append("Consider regenerating data with different parameters")
        
        return validation_results
        
    except Exception as e:
        validation_results['errors'].append(f"Validation process failed: {str(e)}")
        validation_results['passed'] = False
        validation_results['quality_metrics']['overall_quality_score'] = 0.0
        return validation_results

def create_synthetic_data_pipeline(
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Create a comprehensive synthetic data generation pipeline based on configuration.
    
    Args:
        config: Complete configuration dictionary
        
    Returns:
        Synthetic data pipeline configuration
    """
    pipeline_config = {
        'steps': [],
        'parameters': {},
        'validation_rules': {},
        'quality_checks': {}
    }
    
    # Extract relevant configuration sections
    core_config = config.get('core_generation', {})
    generation_config = config.get('generation_methods', {})
    quality_config = config.get('quality_control', {})
    
    # Build pipeline steps
    pipeline_steps = []
    
    # Step 1: Parameter validation
    pipeline_steps.append({
        'name': 'parameter_validation',
        'function': 'validate_generation_parameters',
        'parameters': core_config
    })
    
    # Step 2: Data generation
    pipeline_steps.append({
        'name': 'data_generation',
        'function': 'generate_base_data',
        'parameters': generation_config
    })
    
    # Step 3: Anomaly injection
    pipeline_steps.append({
        'name': 'anomaly_injection',
        'function': 'inject_anomalies',
        'parameters': config.get('anomaly_config', {})
    })
    
    # Step 4: Quality control
    if quality_config:
        pipeline_steps.append({
            'name': 'quality_control',
            'function': 'apply_quality_control',
            'parameters': quality_config
        })
    
    # Step 5: Data splitting
    pipeline_steps.append({
        'name': 'data_splitting',
        'function': 'split_synthetic_data',
        'parameters': config.get('data_splitting', {})
    })
    
    # Step 6: Validation
    pipeline_steps.append({
        'name': 'validation',
        'function': 'validate_synthetic_data',
        'parameters': config.get('validation', {})
    })
    
    pipeline_config['steps'] = pipeline_steps
    pipeline_config['parameters'] = config
    
    return pipeline_config

class EnhancedCollateFn:
    """
    Picklable collate function for DataLoader multiprocessing support.
    
    This class replaces the nested function approach to ensure compatibility
    with multiprocessing workers that require picklable objects.
    """
    
    def __init__(self, config=None, dtype=torch.float32, error_handling='graceful'):
        """
        Initialize the enhanced collate function with configuration.
        
        Args:
            config: Configuration dictionary containing processing parameters
            dtype: Target PyTorch data type for tensors
            error_handling: Error handling strategy ('strict' or 'graceful')
        """
        self.config = config or {}
        self.dtype = dtype
        self.error_handling = error_handling
        
        # Extract advanced features configuration
        advanced_config = self.config.get('advanced_features', {})
        self.variable_length_sequences = advanced_config.get('variable_length_sequences', False)
        self.max_sequence_length = advanced_config.get('max_sequence_length', 512)
        self.custom_collate = advanced_config.get('custom_collate')
        
        # Extract data format configuration
        data_format_config = self.config.get('data_format', {})
        self.squeeze_dims = data_format_config.get('squeeze_dims', False)
        self.unsqueeze_dims = data_format_config.get('unsqueeze_dims')
        
        # Store base collate function reference
        if self.custom_collate and callable(self.custom_collate):
            self.base_collate = self.custom_collate
        else:
            self.base_collate = default_collate
    
    def __call__(self, batch):
        """
        Process a batch of data with enhanced collation.
        
        Args:
            batch: List of samples to collate
            
        Returns:
            Collated batch tensor or tuple of tensors
        """
        try:
            # Apply base collation
            if self.base_collate == default_collate:
                collated = default_collate(batch)
            else:
                try:
                    collated = self.base_collate(batch)
                except Exception as e:
                    logger.warning(f"Custom collate function failed: {e}, using default")
                    collated = default_collate(batch)
            
            # Apply variable length sequence handling
            if self.variable_length_sequences:
                collated = self._handle_variable_sequences(collated)
            
            # Apply data type conversion and dimension adjustments
            collated = self._process_tensor_format(collated)
            
            return collated
            
        except Exception as e:
            if self.error_handling == 'strict':
                raise RuntimeError(f"Enhanced collate function failed: {e}")
            else:
                logger.warning(f"Collate function error, using default: {e}")
                return default_collate(batch)
    
    def _handle_variable_sequences(self, collated):
        """Handle variable length sequences by truncating to max length."""
        if isinstance(collated, torch.Tensor) and collated.dim() > 1:
            if collated.size(1) > self.max_sequence_length:
                collated = collated[:, :self.max_sequence_length]
        elif isinstance(collated, (tuple, list)) and len(collated) >= 2:
            processed = []
            for item in collated:
                if isinstance(item, torch.Tensor) and item.dim() > 1:
                    if item.size(1) > self.max_sequence_length:
                        item = item[:, :self.max_sequence_length]
                processed.append(item)
            collated = tuple(processed) if isinstance(collated, tuple) else processed
        
        return collated
    
    def _process_tensor_format(self, collated):
        """Apply dtype conversion and dimension adjustments."""
        if isinstance(collated, torch.Tensor):
            # Apply dtype conversion
            collated = collated.to(dtype=self.dtype)
            
            # Apply dimension adjustments
            if self.squeeze_dims:
                collated = collated.squeeze()
            
            if self.unsqueeze_dims:
                for dim in self.unsqueeze_dims:
                    collated = collated.unsqueeze(dim)
                    
        elif isinstance(collated, (tuple, list)) and len(collated) >= 2:
            # Handle tuple of tensors (input, target)
            processed = []
            for i, item in enumerate(collated):
                if isinstance(item, torch.Tensor):
                    # Apply dtype conversion
                    item = item.to(dtype=self.dtype)
                    
                    # Apply dimension adjustments
                    if self.squeeze_dims:
                        item = item.squeeze()
                    
                    if self.unsqueeze_dims:
                        for dim in self.unsqueeze_dims:
                            item = item.unsqueeze(dim)
                
                processed.append(item)
            
            collated = tuple(processed) if isinstance(collated, tuple) else processed
        
        return collated
    
    def __getstate__(self):
        """Custom pickling to handle non-serializable objects."""
        state = self.__dict__.copy()
        
        # Handle custom_collate function that might not be picklable
        if 'base_collate' in state and state['base_collate'] != default_collate:
            # Store a reference to the custom function name if possible
            if hasattr(state['base_collate'], '__name__'):
                state['custom_collate_name'] = state['base_collate'].__name__
            else:
                state['custom_collate_name'] = None
            # Remove the actual function reference
            state['base_collate'] = default_collate
        
        return state
    
    def __setstate__(self, state):
        """Custom unpickling to restore the object state."""
        self.__dict__.update(state)
        
        # Restore custom collate function if it was stored
        if hasattr(self, 'custom_collate_name') and self.custom_collate_name:
            # Try to restore the custom function (this is a best-effort approach)
            # In practice, you might need to register custom functions in a global registry
            if self.custom_collate and callable(self.custom_collate):
                self.base_collate = self.custom_collate
            else:
                self.base_collate = default_collate

class WorkerInitializer:
    """
    Picklable worker initialization class for DataLoader multiprocessing support.
    """
    
    def __init__(self, config=None):
        """
        Initialize the worker initializer with configuration.
        
        Args:
            config: Configuration dictionary containing worker parameters
        """
        self.config = config or {}
        
        # Extract configuration parameters
        self.shuffle_seed = self.config.get('sampling', {}).get('shuffle_seed')
        self.cpu_affinity = self.config.get('performance', {}).get('cpu_affinity')
        self.worker_memory_limit = self.config.get('memory_management', {}).get('worker_memory_limit')
        self.custom_worker_init_fn = self.config.get('core', {}).get('worker_init_fn')
        self.verbose = self.config.get('monitoring', {}).get('verbose', False)
    
    def __call__(self, worker_id):
        """
        Initialize a worker process.
        
        Args:
            worker_id: ID of the worker process
        """
        try:
            # Set worker-specific random seeds for reproducibility
            if self.shuffle_seed is not None:
                np.random.seed(self.shuffle_seed + worker_id)
                torch.manual_seed(self.shuffle_seed + worker_id)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(self.shuffle_seed + worker_id)
            
            # Apply CPU affinity if specified
            if self.cpu_affinity:
                try:
                    if isinstance(self.cpu_affinity, list) and self.cpu_affinity:
                        cpu_id = self.cpu_affinity[worker_id % len(self.cpu_affinity)]
                        psutil.Process().cpu_affinity([cpu_id])
                        if self.verbose:
                            print(f"Worker {worker_id} bound to CPU {cpu_id}")
                except Exception as e:
                    if self.verbose:
                        print(f"Failed to set CPU affinity for worker {worker_id}: {e}")
            
            # Memory management for workers
            if self.worker_memory_limit:
                try:
                    # Convert MB to bytes
                    memory_limit = self.worker_memory_limit * 1024 * 1024
                    import resource
                    resource.setrlimit(resource.RLIMIT_AS, (memory_limit, memory_limit))
                    if self.verbose:
                        print(f"Worker {worker_id} memory limit set to {self.worker_memory_limit}MB")
                except Exception as e:
                    if self.verbose:
                        print(f"Failed to set memory limit for worker {worker_id}: {e}")
            
            # Apply any custom worker initialization
            if self.custom_worker_init_fn and callable(self.custom_worker_init_fn):
                try:
                    self.custom_worker_init_fn(worker_id)
                except Exception as e:
                    if self.verbose:
                        print(f"Custom worker init failed for worker {worker_id}: {e}")
            
            if self.verbose:
                print(f"Initialized worker {worker_id}")
                
        except Exception as e:
            # Don't let worker initialization failures crash the process
            print(f"Worker {worker_id} initialization failed: {e}")

def create_enhanced_collate_fn(config=None, dtype=torch.float32, error_handling='graceful'):
    """
    Create a picklable enhanced collate function.
    
    This function creates an instance of the EnhancedCollateFn class that can be
    properly pickled for multiprocessing DataLoader workers.
    
    Args:
        config: Configuration dictionary containing processing parameters
        dtype: Target PyTorch data type for tensors
        error_handling: Error handling strategy ('strict' or 'graceful')
        
    Returns:
        EnhancedCollateFn instance that can be used as a collate function
    """
    # Extract configuration from current context if available
    final_config = config or {}
    
    # Try to get configuration from local context (if called from within create_dataloaders)
    import inspect
    frame = inspect.currentframe()
    try:
        # Look for configuration variables in the calling frame
        caller_locals = frame.f_back.f_locals if frame.f_back else {}
        
        # Extract relevant config variables from the calling context
        if 'advanced_config' in caller_locals:
            final_config.setdefault('advanced_features', {}).update(caller_locals['advanced_config'])
        
        if 'data_format_config' in caller_locals:
            final_config.setdefault('data_format', {}).update(caller_locals['data_format_config'])
        
        if 'error_handling' in caller_locals:
            error_handling = caller_locals['error_handling']
            
        if 'dtype' in caller_locals:
            dtype = caller_locals['dtype']
            
    except Exception as e:
        logger.debug(f"Could not extract context from calling frame: {e}")
    finally:
        del frame
    
    # Create and return the picklable collate function
    return EnhancedCollateFn(
        config=final_config,
        dtype=dtype,
        error_handling=error_handling
    )

def create_dataloaders(
    # Core Data Parameters
    data: Optional[Dict[str, np.ndarray]] = None,
    X_train: Optional[np.ndarray] = None,
    X_val: Optional[np.ndarray] = None,
    X_test: Optional[np.ndarray] = None,
    y_train: Optional[np.ndarray] = None,
    y_val: Optional[np.ndarray] = None,
    y_test: Optional[np.ndarray] = None,
    
    # Core DataLoader Parameters
    batch_size: Optional[int] = None,
    shuffle: Optional[bool] = None,
    num_workers: Optional[int] = None,
    pin_memory: Optional[bool] = None,
    drop_last: Optional[bool] = None,
    timeout: Optional[float] = None,
    worker_init_fn: Optional[Callable] = None,
    multiprocessing_context: Optional[str] = None,
    generator: Optional[torch.Generator] = None,
    prefetch_factor: Optional[int] = None,
    persistent_workers: Optional[bool] = None,
    
    # Batch Size Configuration
    train_batch_size: Optional[int] = None,
    val_batch_size: Optional[int] = None,
    test_batch_size: Optional[int] = None,
    eval_batch_size: Optional[int] = None,
    dynamic_batch_sizing: Optional[bool] = None,
    adaptive_batch_size: Optional[bool] = None,
    max_batch_size: Optional[int] = None,
    min_batch_size: Optional[int] = None,
    
    # Shuffling and Sampling
    train_shuffle: Optional[bool] = None,
    val_shuffle: Optional[bool] = None,
    test_shuffle: Optional[bool] = None,
    sampler: Optional[torch.utils.data.Sampler] = None,
    batch_sampler: Optional[torch.utils.data.BatchSampler] = None,
    shuffle_seed: Optional[int] = None,
    stratified_sampling: Optional[bool] = None,
    weighted_sampling: Optional[bool] = None,
    sample_weights: Optional[np.ndarray] = None,
    
    # Performance Optimization
    optimization_level: Optional[str] = None,
    memory_efficient: Optional[bool] = None,
    cpu_count: Optional[int] = None,
    max_workers: Optional[int] = None,
    worker_memory_limit: Optional[int] = None,
    dataloader_optimization: Optional[bool] = None,
    fast_dev_run: Optional[bool] = None,
    benchmark_mode: Optional[bool] = None,
    
    # Data Processing and Augmentation
    data_transforms: Optional[List[Callable]] = None,
    train_transforms: Optional[List[Callable]] = None,
    val_transforms: Optional[List[Callable]] = None,
    test_transforms: Optional[List[Callable]] = None,
    augmentation_pipeline: Optional[List[Callable]] = None,
    normalize_data: Optional[bool] = None,
    standardize_data: Optional[bool] = None,
    
    # Data Type and Format Parameters
    dtype: Optional[torch.dtype] = None,
    device: Optional[str] = None,
    tensor_format: Optional[str] = None,
    data_format: Optional[str] = None,
    squeeze_dims: Optional[bool] = None,
    unsqueeze_dims: Optional[List[int]] = None,
    transpose_dims: Optional[List[int]] = None,
    
    # Memory Management
    memory_management: Optional[str] = None,
    shared_memory: Optional[bool] = None,
    mmap_mode: Optional[str] = None,
    cache_datasets: Optional[bool] = None,
    preload_data: Optional[bool] = None,
    lazy_loading: Optional[bool] = None,
    memory_mapping: Optional[bool] = None,
    gc_collection: Optional[bool] = None,
    
    # Validation and Quality Control
    validate_data: Optional[bool] = None,
    check_data_integrity: Optional[bool] = None,
    handle_nan_values: Optional[str] = None,
    handle_inf_values: Optional[str] = None,
    data_consistency_checks: Optional[bool] = None,
    shape_validation: Optional[bool] = None,
    dtype_validation: Optional[bool] = None,
    
    # Distributed and Parallel Processing
    distributed: Optional[bool] = None,
    world_size: Optional[int] = None,
    rank: Optional[int] = None,
    local_rank: Optional[int] = None,
    distributed_sampler: Optional[bool] = None,
    ddp_backend: Optional[str] = None,
    sync_batchnorm: Optional[bool] = None,
    
    # Advanced Features
    collate_fn: Optional[Callable] = None,
    custom_collate: Optional[Callable] = None,
    variable_length_sequences: Optional[bool] = None,
    padding_strategy: Optional[str] = None,
    sequence_length: Optional[int] = None,
    max_sequence_length: Optional[int] = None,
    
    # Monitoring and Debugging
    profile_dataloaders: Optional[bool] = None,
    benchmark_dataloaders: Optional[bool] = None,
    dataloader_stats: Optional[bool] = None,
    timing_analysis: Optional[bool] = None,
    memory_profiling: Optional[bool] = None,
    bottleneck_detection: Optional[bool] = None,
    verbose: Optional[bool] = None,
    progress_bar: Optional[bool] = None,
    silent: Optional[bool] = None,
    debug_mode: Optional[bool] = None,
    
    # Error Handling and Resilience
    error_handling: Optional[str] = None,
    retry_failed_batches: Optional[bool] = None,
    max_retries: Optional[int] = None,
    fallback_batch_size: Optional[int] = None,
    graceful_degradation: Optional[bool] = None,
    fault_tolerance: Optional[bool] = None,
    
    # Cross-validation Support
    cross_validation: Optional[bool] = None,
    cv_folds: Optional[int] = None,
    cv_strategy: Optional[str] = None,
    fold_dataloaders: Optional[bool] = None,
    stratified_cv: Optional[bool] = None,
    
    # Experimental and Advanced Options
    experimental_features: Optional[bool] = None,
    mixed_precision_loading: Optional[bool] = None,
    gradient_checkpointing: Optional[bool] = None,
    dataloader_sharding: Optional[bool] = None,
    pipeline_parallelism: Optional[bool] = None,
    
    # System Integration
    system_optimization: Optional[bool] = None,
    numa_awareness: Optional[bool] = None,
    cpu_affinity: Optional[List[int]] = None,
    gpu_affinity: Optional[List[int]] = None,
    io_optimization: Optional[bool] = None,
    
    # Compatibility and Legacy Support
    pytorch_version_check: Optional[bool] = None,
    legacy_compatibility: Optional[bool] = None,
    backwards_compatibility: Optional[bool] = None,
    version_specific_optimizations: Optional[bool] = None,
    
    # Configuration and Metadata
    config: Optional[Dict[str, Any]] = None,
    dataloader_config: Optional[Dict[str, Any]] = None,
    preset: Optional[str] = None,
    
    **kwargs
) -> Union[Tuple[DataLoader, DataLoader, DataLoader], Dict[str, DataLoader], DataLoader]:
    # Start timing
    start_time = datetime.now()
    
    # Initialize configuration with defaults
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply dataloader-specific configuration
    if dataloader_config:
        config.setdefault('dataloader', {}).update(dataloader_config)
    
    # Load preset configuration if specified
    if preset and preset in globals().get('PRESET_CONFIGS', {}):
        try:
            preset_config = globals()['PRESET_CONFIGS'][preset].copy()
            # Merge preset config, giving precedence to existing config
            for key, value in preset_config.items():
                if key not in config:
                    config[key] = value
            if not silent_mode:
                logger.info(f"Applied preset configuration: {preset}")
        except Exception as e:
            if not silent_mode:
                logger.warning(f"Failed to apply preset '{preset}': {e}")
    
    # Apply all parameters to configuration
    final_config = {}
    
    # Merge with existing config
    final_config.update(config)
    
    # Apply individual parameters with intelligent organization
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'dataloader_config', 'preset', 'kwargs', 'start_time', 'datetime',
        'traceback', 'psutil', 'gc', 'partial', 'WeightedRandomSampler', 'DistributedSampler',
        'default_collate', 'TensorDataset', 'DataLoader', 'torch'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Organize parameters into logical sections
    param_sections = {
        'core': [
            'batch_size', 'shuffle', 'num_workers', 'pin_memory', 'drop_last', 'timeout', 'worker_init_fn', 'multiprocessing_context', 'generator',
            'prefetch_factor', 'persistent_workers'
        ],
        'batch_configuration': [
            'train_batch_size', 'val_batch_size', 'test_batch_size', 'eval_batch_size', 'dynamic_batch_sizing', 'adaptive_batch_size', 'max_batch_size', 'min_batch_size'
        ],
        'sampling': [
            'train_shuffle', 'val_shuffle', 'test_shuffle', 'sampler', 'batch_sampler', 'shuffle_seed', 'stratified_sampling', 'weighted_sampling', 'sample_weights'
        ],
        'performance': [
            'optimization_level', 'memory_efficient', 'cpu_count', 'max_workers', 'worker_memory_limit', 'dataloader_optimization', 'fast_dev_run', 'benchmark_mode'
        ],
        'data_processing': [
            'data_transforms', 'train_transforms', 'val_transforms', 'test_transforms', 'augmentation_pipeline', 'normalize_data', 'standardize_data'
        ],
        'data_format': [
            'dtype', 'device', 'tensor_format', 'data_format', 'squeeze_dims', 'unsqueeze_dims', 'transpose_dims'
        ],
        'memory_management': [
            'memory_management', 'shared_memory', 'mmap_mode', 'cache_datasets', 'preload_data', 'lazy_loading', 'memory_mapping', 'gc_collection'
        ],
        'validation': [
            'validate_data', 'check_data_integrity', 'handle_nan_values', 'handle_inf_values', 'data_consistency_checks', 'shape_validation', 'dtype_validation'
        ],
        'distributed': [
            'distributed', 'world_size', 'rank', 'local_rank', 'distributed_sampler', 'ddp_backend', 'sync_batchnorm'
        ],
        'advanced_features': [
            'collate_fn', 'custom_collate', 'variable_length_sequences', 'padding_strategy', 'sequence_length', 'max_sequence_length'
        ],
        'monitoring': [
            'profile_dataloaders', 'benchmark_dataloaders', 'dataloader_stats', 'timing_analysis', 'memory_profiling', 'bottleneck_detection', 'verbose',
            'progress_bar', 'silent', 'debug_mode'
        ],
        'error_handling': [
            'error_handling', 'retry_failed_batches', 'max_retries', 'fallback_batch_size', 'graceful_degradation', 'fault_tolerance'
        ],
        'cross_validation': [
            'cross_validation', 'cv_folds', 'cv_strategy', 'fold_dataloaders', 'stratified_cv'
        ],
        'experimental': [
            'experimental_features', 'mixed_precision_loading', 'gradient_checkpointing', 'dataloader_sharding', 'pipeline_parallelism'
        ],
        'system_integration': [
            'system_optimization', 'numa_awareness', 'cpu_affinity', 'gpu_affinity', 'io_optimization'
        ],
        'compatibility': [
            'pytorch_version_check', 'legacy_compatibility', 'backwards_compatibility', 'version_specific_optimizations'
        ]
    }
    
    # Apply parameters to appropriate sections
    for section, param_list in param_sections.items():
        section_config = final_config.setdefault(section, {})
        for param in param_list:
            if param in cleaned_params:
                section_config[param] = cleaned_params[param]
    
    # Set up defaults
    core_config = final_config.setdefault('core', {})
    batch_config = final_config.setdefault('batch_configuration', {})
    sampling_config = final_config.setdefault('sampling', {})
    performance_config = final_config.setdefault('performance', {})
    data_processing_config = final_config.setdefault('data_processing', {})
    data_format_config = final_config.setdefault('data_format', {})
    memory_config = final_config.setdefault('memory_management', {})
    validation_config = final_config.setdefault('validation', {})
    distributed_config = final_config.setdefault('distributed', {})
    advanced_config = final_config.setdefault('advanced_features', {})
    monitoring_config = final_config.setdefault('monitoring', {})
    error_config = final_config.setdefault('error_handling', {})
    cv_config = final_config.setdefault('cross_validation', {})
    experimental_config = final_config.setdefault('experimental', {})
    
    # Handle silent mode - override verbose and progress_bar if silent is True
    silent_mode = monitoring_config.get('silent', False)
    if silent_mode:
        # Force silent mode behavior
        monitoring_config['verbose'] = False
        monitoring_config['progress_bar'] = False
    
    # Apply intelligent defaults with system awareness
    batch_size = core_config.setdefault('batch_size', DEFAULT_BATCH_SIZE)
    shuffle = core_config.setdefault('shuffle', True)
    num_workers = core_config.setdefault('num_workers', NUM_WORKERS)
    pin_memory = core_config.setdefault('pin_memory', torch.cuda.is_available())
    drop_last = core_config.setdefault('drop_last', False)
    timeout = core_config.setdefault('timeout', 0)
    
    if num_workers > 0:
        prefetch_factor = core_config.setdefault('prefetch_factor', 2)
        persistent_workers = core_config.setdefault('persistent_workers', True)
    else:
        prefetch_factor = None
        persistent_workers = False
    
    # Performance defaults
    optimization_level = performance_config.setdefault('optimization_level', 'standard')
    memory_efficient = performance_config.setdefault('memory_efficient', True)
    dataloader_optimization = performance_config.setdefault('dataloader_optimization', True)
    fast_dev_run = performance_config.setdefault('fast_dev_run', False)
    benchmark_mode = performance_config.setdefault('benchmark_mode', False)
    
    # System optimization defaults
    system_cpu_count = os.cpu_count() or 1
    max_workers = performance_config.setdefault('max_workers', system_cpu_count)
    cpu_count = performance_config.setdefault('cpu_count', system_cpu_count)
    
    # Data format defaults
    dtype = data_format_config.setdefault('dtype', torch.float32)
    device = data_format_config.setdefault('device', 'auto')
    
    # Validation defaults
    validate_data = validation_config.setdefault('validate_data', True)
    check_data_integrity = validation_config.setdefault('check_data_integrity', True)
    handle_nan_values = validation_config.setdefault('handle_nan_values', 'error')
    handle_inf_values = validation_config.setdefault('handle_inf_values', 'error')
    
    # Monitoring defaults - respect silent mode
    verbose = monitoring_config.setdefault('verbose', False)
    progress_bar = monitoring_config.setdefault('progress_bar', not silent_mode)
    debug_mode = monitoring_config.setdefault('debug_mode', False)
    profile_dataloaders = monitoring_config.setdefault('profile_dataloaders', True)
    dataloader_stats = monitoring_config.setdefault('dataloader_stats', True)
    
    # Error handling defaults
    error_handling = error_config.setdefault('error_handling', 'strict')
    graceful_degradation = error_config.setdefault('graceful_degradation', True)
    max_retries = error_config.setdefault('max_retries', 3)
    
    # Set up logging level
    if verbose:
        original_level = logger.level
        logger.setLevel(logging.INFO)
    
    if verbose:
        logger.info("Starting comprehensive DataLoader creation")
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'current_substage': None,
        'dataloaders_created': 0,
        'datasets_processed': 0,
        'transforms_applied': 0,
        'samplers_configured': 0,
        'performance_score': 0.0,
        'fallback_attempts': 0
    }
    
    # Initialize creation statistics
    creation_stats = {
        'start_time': start_time.isoformat(),
        'config_applied': final_config,
        'system_info': {
            'cpu_count': system_cpu_count,
            'available_memory_gb': psutil.virtual_memory().available / (1024**3),
            'cuda_available': torch.cuda.is_available(),
            'pytorch_version': torch.__version__
        },
        'stages_completed': [],
        'warnings_encountered': [],
        'performance_metrics': {}
    }
    
    try:
        # Calculate total stages for progress tracking
        total_stages = 12  # Configuration, Data Validation, System Optimization, Transforms, Datasets, Samplers, Collate, Training DL, Validation DL, Test DL, CV DL, Finalization
        
        # Use progress bar only if not in silent mode and progress_bar is True
        if not silent_mode and progress_bar:
            bar_context = alive_bar(total_stages, title='DataLoader Creation\t\t', unit='stages')
        else:
            # Create a dummy context manager that does nothing
            class DummyBar:
                def __enter__(self):
                    return self
                def __exit__(self, *args):
                    pass
                def __call__(self):
                    pass
                def __setattr__(self, name, value):
                    pass
            bar_context = DummyBar()
        
        with bar_context as main_bar:
            
            # STAGE 1: Configuration and Setup
            progress_data['current_stage'] = "Configuration"
            if not silent_mode:
                main_bar.text = "Setting up configuration and parameters..."
            
            # Determine device configuration
            if device == 'auto':
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            if not silent_mode:
                main_bar.text = "Configuration complete"
            creation_stats['stages_completed'].append('configuration')
            if not silent_mode:
                main_bar()
            
            # STAGE 2: Data Validation
            progress_data['current_stage'] = "Data Validation"
            if not silent_mode:
                main_bar.text = "Validating and preparing input data..."
            
            # Extract data from various input formats
            datasets = {}
            
            if data is not None:
                # Primary method: data dictionary
                X_train = data.get('X_train', X_train)
                X_val = data.get('X_val', X_val)
                X_test = data.get('X_test', X_test)
                y_train = data.get('y_train', y_train)
                y_val = data.get('y_val', y_val)
                y_test = data.get('y_test', y_test)
            
            # Validate required data
            if X_train is None:
                raise ValueError("Training data (X_train) is required")
            
            # Data validation if requested
            if validate_data:
                if verbose:
                    logger.info("Performing comprehensive data validation")
                
                def validate_array(arr, name):
                    if arr is None:
                        return None
                    
                    if not isinstance(arr, np.ndarray):
                        try:
                            arr = np.array(arr)
                        except Exception as e:
                            raise TypeError(f"{name} could not be converted to numpy array: {e}")
                    
                    if arr.size == 0:
                        if verbose:
                            logger.warning(f"{name} is empty")
                        return arr
                    
                    # Check for invalid values
                    if handle_nan_values != 'ignore':
                        nan_count = np.isnan(arr).sum()
                        if nan_count > 0:
                            if handle_nan_values == 'error':
                                raise ValueError(f"{name} contains {nan_count} NaN values")
                            elif handle_nan_values == 'remove':
                                valid_mask = ~np.isnan(arr).any(axis=1)
                                arr = arr[valid_mask]
                                if verbose:
                                    logger.warning(f"Removed {(~valid_mask).sum()} samples with NaN values from {name}")
                            elif handle_nan_values == 'replace':
                                arr = np.nan_to_num(arr, nan=0.0)
                                if verbose:
                                    logger.warning(f"Replaced {nan_count} NaN values with 0.0 in {name}")
                    
                    if handle_inf_values != 'ignore':
                        inf_count = np.isinf(arr).sum()
                        if inf_count > 0:
                            if handle_inf_values == 'error':
                                raise ValueError(f"{name} contains {inf_count} infinite values")
                            elif handle_inf_values == 'replace':
                                arr = np.nan_to_num(arr, posinf=1e6, neginf=-1e6)
                                if verbose:
                                    logger.warning(f"Replaced {inf_count} infinite values in {name}")
                    
                    return arr
                
                # Validate all arrays
                X_train = validate_array(X_train, 'X_train')
                X_val = validate_array(X_val, 'X_val') if X_val is not None else None
                X_test = validate_array(X_test, 'X_test') if X_test is not None else None
                y_train = validate_array(y_train, 'y_train') if y_train is not None else None
                y_val = validate_array(y_val, 'y_val') if y_val is not None else None
                y_test = validate_array(y_test, 'y_test') if y_test is not None else None
                
                # Shape consistency validation
                if validation_config.get('shape_validation', True):
                    n_features = X_train.shape[1] if len(X_train.shape) > 1 else 1
                    
                    for name, arr in [('X_val', X_val), ('X_test', X_test)]:
                        if arr is not None and len(arr.shape) > 1:
                            if arr.shape[1] != n_features:
                                raise ValueError(f"Feature dimension mismatch: X_train has {n_features} features, {name} has {arr.shape[1]}")
                    
                    # Label consistency
                    if y_train is not None and len(y_train) != len(X_train):
                        raise ValueError(f"Training data size mismatch: X_train has {len(X_train)} samples, y_train has {len(y_train)}")
            
            creation_stats['data_validation_passed'] = True
            creation_stats['data_shapes'] = {
                'X_train': X_train.shape,
                'X_val': X_val.shape if X_val is not None else None,
                'X_test': X_test.shape if X_test is not None else None
            }
            
            progress_data['datasets_processed'] = 1 + (1 if X_val is not None else 0) + (1 if X_test is not None else 0)
            if not silent_mode:
                main_bar.text = f"Data validation complete ({progress_data['datasets_processed']} datasets)"
            creation_stats['stages_completed'].append('data_validation')
            if not silent_mode:
                main_bar()
            
            # STAGE 3: System Optimization
            progress_data['current_stage'] = "System Optimization"
            if not silent_mode:
                main_bar.text = "Optimizing system parameters..."
            
            # Optimize system parameters based on available resources
            if system_optimization:
                if verbose:
                    logger.info("Applying system optimizations")
                
                # Memory-based optimization
                available_memory_gb = psutil.virtual_memory().available / (1024**3)
                # Less than 4GB
                if available_memory_gb < 4:
                    num_workers = min(num_workers, 2)
                    batch_size = min(batch_size, 32)
                    if num_workers > 0:
                        prefetch_factor = 1
                    else:
                        prefetch_factor = None
                    if verbose:
                        logger.info("Applied memory-constrained optimizations")
                # More than 16GB
                elif available_memory_gb > 16:
                    num_workers = min(num_workers, max_workers)
                    prefetch_factor = min(prefetch_factor, 4)
                    if verbose:
                        logger.info("Applied high-memory optimizations")
                
                # CPU-based optimization
                if system_cpu_count <= 2:
                    num_workers = min(num_workers, 1)
                    persistent_workers = False
                elif system_cpu_count >= 8:
                    num_workers = min(num_workers, system_cpu_count - 1)
                    persistent_workers = True
            
            # Apply batch size configurations
            train_batch_size = batch_config.get('train_batch_size', batch_size)
            val_batch_size = batch_config.get('val_batch_size', batch_config.get('eval_batch_size', min(batch_size * 2, 1024)))
            test_batch_size = batch_config.get('test_batch_size', batch_config.get('eval_batch_size', min(batch_size * 2, 1024)))
            
            # Adaptive batch sizing based on data size and memory
            if batch_config.get('adaptive_batch_size', False):
                if verbose:
                    logger.info("Applying adaptive batch sizing")
                
                def calculate_optimal_batch_size(data_size, base_batch_size, memory_factor=1.0):
                    # Calculate based on data size and available memory
                    if data_size < 1000:
                        return min(base_batch_size, data_size // 4) if data_size > 4 else 1
                    elif data_size > 100000:
                        return min(base_batch_size * 2, batch_config.get('max_batch_size', 2048))
                    else:
                        return int(base_batch_size * memory_factor)
                
                memory_factor = min(2.0, available_memory_gb / 8.0) if 'available_memory_gb' in locals() else 1.0
                
                train_batch_size = calculate_optimal_batch_size(len(X_train), train_batch_size, memory_factor)
                if X_val is not None:
                    val_batch_size = calculate_optimal_batch_size(len(X_val), val_batch_size, memory_factor)
                if X_test is not None:
                    test_batch_size = calculate_optimal_batch_size(len(X_test), test_batch_size, memory_factor)
            
            # Apply batch size limits
            min_batch_size = batch_config.get('min_batch_size', 1)
            max_batch_size = batch_config.get('max_batch_size', 4096)
            
            train_batch_size = max(min_batch_size, min(train_batch_size, max_batch_size))
            val_batch_size = max(min_batch_size, min(val_batch_size, max_batch_size))
            test_batch_size = max(min_batch_size, min(test_batch_size, max_batch_size))
            
            # Shuffle configuration
            train_shuffle = sampling_config.get('train_shuffle', shuffle)
            val_shuffle = sampling_config.get('val_shuffle', False)
            test_shuffle = sampling_config.get('test_shuffle', False)
            
            if not silent_mode:
                main_bar.text = "System optimization complete"
            creation_stats['stages_completed'].append('system_optimization')
            if not silent_mode:
                main_bar()
            
            # STAGE 4: Transform Pipeline Setup
            progress_data['current_stage'] = "Transform Setup"
            if not silent_mode:
                main_bar.text = "Setting up data transforms..."
            
            # Setup multiprocessing context
            mp_context = core_config.get('multiprocessing_context')
            if mp_context and num_workers > 0:
                try:
                    import multiprocessing as mp
                    if mp_context in ['spawn', 'fork', 'forkserver']:
                        mp_ctx = mp.get_context(mp_context)
                        if verbose:
                            logger.info(f"Using multiprocessing context: {mp_context}")
                    else:
                        mp_ctx = None
                        if verbose:
                            logger.warning(f"Invalid multiprocessing context: {mp_context}")
                except Exception as e:
                    if verbose:
                        logger.warning(f"Failed to set multiprocessing context: {e}")
                    mp_ctx = None
            else:
                mp_ctx = None
            
            # Create data transforms pipeline
            def create_transform_pipeline(transforms_list):
                if not transforms_list:
                    return None
                
                def apply_transforms(tensor):
                    for transform in transforms_list:
                        tensor = transform(tensor)
                    return tensor
                return apply_transforms
            
            # Data processing transforms
            train_transform = create_transform_pipeline(data_processing_config.get('train_transforms'))
            val_transform = create_transform_pipeline(data_processing_config.get('val_transforms'))
            test_transform = create_transform_pipeline(data_processing_config.get('test_transforms'))
            
            # Generic transforms applied to all data
            if data_processing_config.get('data_transforms'):
                generic_transform = create_transform_pipeline(data_processing_config['data_transforms'])
            elif data_processing_config.get('normalize_data', False):
                # Simple normalization transform
                def normalize_transform(tensor):
                    return (tensor - tensor.mean()) / (tensor.std() + 1e-8)
                generic_transform = normalize_transform
            elif data_processing_config.get('standardize_data', False):
                # Standardization transform
                def standardize_transform(tensor):
                    return (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-8)
                generic_transform = standardize_transform
            else:
                generic_transform = None
            
            progress_data['transforms_applied'] = sum(1 for t in [train_transform, val_transform, test_transform, generic_transform] if t is not None)
            if not silent_mode:
                main_bar.text = f"Transform setup complete ({progress_data['transforms_applied']} transforms)"
            creation_stats['stages_completed'].append('transform_setup')
            if not silent_mode:
                main_bar()
            
            # STAGE 5: Dataset Creation
            progress_data['current_stage'] = "Dataset Creation"
            if not silent_mode:
                main_bar.text = "Creating tensor datasets..."
            
            # Create tensor datasets
            def create_enhanced_dataset(X, y=None, transform=None):
                # Convert to tensors
                X_tensor = torch.tensor(X, dtype=dtype)
                
                if y is not None:
                    y_tensor = torch.tensor(y, dtype=torch.long if y.dtype in [np.int32, np.int64] else dtype)
                    dataset = TensorDataset(X_tensor, y_tensor)
                else:
                    dataset = TensorDataset(X_tensor)
                
                # Apply transforms if specified
                if transform or generic_transform:
                    original_dataset = dataset
                    
                    def transformed_getitem(idx):
                        item = original_dataset[idx]
                        if generic_transform:
                            if isinstance(item, tuple):
                                item = (generic_transform(item[0]), *item[1:])
                            else:
                                item = generic_transform(item)
                        if transform:
                            if isinstance(item, tuple):
                                item = (transform(item[0]), *item[1:])
                            else:
                                item = transform(item)
                        return item
                    
                    # Create a custom dataset class that applies transforms
                    class TransformedDataset(torch.utils.data.Dataset):
                        def __init__(self, base_dataset, transform_fn):
                            self.base_dataset = base_dataset
                            self.transform_fn = transform_fn
                        
                        def __len__(self):
                            return len(self.base_dataset)
                        
                        def __getitem__(self, idx):
                            return self.transform_fn(idx)
                    
                    dataset = TransformedDataset(original_dataset, transformed_getitem)
                
                return dataset
            
            # Create datasets
            if verbose:
                logger.info("Creating tensor datasets")
            
            train_dataset = create_enhanced_dataset(X_train, y_train, train_transform)
            val_dataset = create_enhanced_dataset(X_val, y_val, val_transform) if X_val is not None else None
            test_dataset = create_enhanced_dataset(X_test, y_test, test_transform) if X_test is not None else None
            
            if not silent_mode:
                main_bar.text = "Dataset creation complete"
            creation_stats['stages_completed'].append('dataset_creation')
            if not silent_mode:
                main_bar()
            
            # STAGE 6: Sampler Configuration
            progress_data['current_stage'] = "Sampler Configuration"
            if not silent_mode:
                main_bar.text = "Configuring data samplers..."
            
            # Setup samplers
            train_sampler = None
            val_sampler = None
            test_sampler = None
            
            # Distributed sampling
            if distributed_config.get('distributed', False) and distributed_config.get('distributed_sampler', True):
                world_size = distributed_config.get('world_size', 1)
                rank = distributed_config.get('rank', 0)
                
                if verbose:
                    logger.info(f"Setting up distributed samplers: world_size={world_size}, rank={rank}")
                
                train_sampler = DistributedSampler(
                    train_dataset,
                    num_replicas=world_size,
                    rank=rank,
                    shuffle=train_shuffle,
                    seed=sampling_config.get('shuffle_seed', 0)
                )
                
                if val_dataset:
                    val_sampler = DistributedSampler(
                        val_dataset,
                        num_replicas=world_size,
                        rank=rank,
                        shuffle=val_shuffle,
                        seed=sampling_config.get('shuffle_seed', 0)
                    )
                
                if test_dataset:
                    test_sampler = DistributedSampler(
                        test_dataset,
                        num_replicas=world_size,
                        rank=rank,
                        shuffle=test_shuffle,
                        seed=sampling_config.get('shuffle_seed', 0)
                    )
                
                # Override shuffle when using distributed sampler
                train_shuffle = False
                val_shuffle = False
                test_shuffle = False
            
            # Weighted sampling for training data
            elif sampling_config.get('weighted_sampling', False) and y_train is not None:
                if verbose:
                    logger.info("Setting up weighted sampling")
                
                if sampling_config.get('sample_weights') is not None:
                    sample_weights = sampling_config['sample_weights']
                else:
                    # Calculate class weights
                    class_counts = np.bincount(y_train)
                    class_weights = 1.0 / class_counts
                    sample_weights = class_weights[y_train]
                
                train_sampler = WeightedRandomSampler(
                    weights=sample_weights,
                    num_samples=len(sample_weights),
                    replacement=True
                )
                train_shuffle = False  # Don't shuffle when using custom sampler
            
            # Stratified sampling
            elif sampling_config.get('stratified_sampling', False) and y_train is not None:
                if verbose:
                    logger.info("Setting up stratified sampling")
                
                # Create stratified sampler (simplified implementation)
                class_indices = defaultdict(list)
                for idx, label in enumerate(y_train):
                    class_indices[label].append(idx)
                
                # Balance classes by sampling
                min_class_size = min(len(indices) for indices in class_indices.values())
                balanced_indices = []
                
                for indices in class_indices.values():
                    sampled_indices = np.random.choice(indices, size=min_class_size, replace=False)
                    balanced_indices.extend(sampled_indices)
                
                # Custom sampler that uses balanced indices
                class StratifiedSampler(torch.utils.data.Sampler):
                    def __init__(self, indices):
                        self.indices = indices
                    
                    def __iter__(self):
                        return iter(np.random.permutation(self.indices))
                    
                    def __len__(self):
                        return len(self.indices)
                
                train_sampler = StratifiedSampler(balanced_indices)
                train_shuffle = False
            
            # Custom samplers from configuration
            if sampling_config.get('sampler') is not None:
                train_sampler = sampling_config['sampler']
                train_shuffle = False
            
            if sampling_config.get('batch_sampler') is not None:
                batch_sampler = sampling_config['batch_sampler']
                train_sampler = None
                train_shuffle = False
            else:
                batch_sampler = None
            
            # Setup generator for reproducibility
            generator = core_config.get('generator')
            if generator is None and sampling_config.get('shuffle_seed') is not None:
                generator = torch.Generator()
                generator.manual_seed(sampling_config['shuffle_seed'])
            
            progress_data['samplers_configured'] = sum(1 for s in [train_sampler, val_sampler, test_sampler] if s is not None)
            if not silent_mode:
                main_bar.text = f"Sampler configuration complete ({progress_data['samplers_configured']} samplers)"
            creation_stats['stages_completed'].append('sampler_configuration')
            if not silent_mode:
                main_bar()
            
            # STAGE 7: Collate Function Setup
            progress_data['current_stage'] = "Collate Function"
            if not silent_mode:
                main_bar.text = "Setting up collate functions..."
            
            # Custom collate function
            enhanced_collate_fn = None
            
            # Only create enhanced collate if we have specific requirements or configurations
            needs_enhanced_collate = (
                advanced_config.get('variable_length_sequences', False) or 
                advanced_config.get('custom_collate') or
                data_format_config.get('squeeze_dims', False) or
                data_format_config.get('unsqueeze_dims') or
                dtype != torch.float32 or
                advanced_config.get('collate_fn') != default_collate
            )
            
            if needs_enhanced_collate:
                if verbose:
                    logger.debug("Creating enhanced collate function with custom processing")
                
                # Create the enhanced collate function with current configuration
                enhanced_collate_fn = EnhancedCollateFn(
                    config=final_config,
                    dtype=dtype,
                    error_handling=error_handling
                )
                
                # Test if the collate function is picklable when using multiprocessing
                if num_workers > 0:
                    try:
                        pickle.dumps(enhanced_collate_fn)
                        if verbose:
                            logger.debug("Enhanced collate function is picklable and ready for multiprocessing")
                    except Exception as pickle_error:
                        if verbose:
                            logger.warning(f"Enhanced collate function pickling test failed: {pickle_error}")
                            logger.info("Falling back to default collate function to avoid multiprocessing issues")
                        enhanced_collate_fn = None
                
            else:
                if verbose:
                    logger.debug("Using default PyTorch collate function (no custom processing needed)")
                # Use default collate
                enhanced_collate_fn = None
            
            # Additional fallback for problematic scenarios
            if enhanced_collate_fn is None and needs_enhanced_collate:
                if verbose:
                    logger.warning("Enhanced collate features requested but not available - some functionality may be limited")
            
            # Worker initialization function
            worker_init_fn = None
            
            if num_workers > 0:
                # Check if we need custom worker initialization
                needs_custom_worker_init = (
                    sampling_config.get('shuffle_seed') is not None or
                    performance_config.get('cpu_affinity') or
                    memory_config.get('worker_memory_limit') or
                    core_config.get('worker_init_fn')
                )
                
                if needs_custom_worker_init:
                    # Create picklable worker initializer
                    worker_init_fn = WorkerInitializer(config=final_config)
                    
                    # Test if the worker initializer is picklable
                    try:
                        pickle.dumps(worker_init_fn)
                        if verbose:
                            logger.debug("Worker initializer is picklable and ready for multiprocessing")
                    except Exception as pickle_error:
                        if verbose:
                            logger.warning(f"Worker initializer pickling test failed: {pickle_error}")
                            logger.info("Disabling custom worker initialization to avoid multiprocessing issues")
                        worker_init_fn = None
                else:
                    if verbose:
                        logger.debug("No custom worker initialization needed")
                    worker_init_fn = None
            else:
                worker_init_fn = None
            
            if not silent_mode:
                main_bar.text = "Collate function setup complete"
            creation_stats['stages_completed'].append('collate_setup')
            if not silent_mode:
                main_bar()
            
            # STAGE 8: Training DataLoader Creation
            progress_data['current_stage'] = "Training DataLoader"
            if not silent_mode:
                main_bar.text = "Creating training DataLoader..."
            
            # Common DataLoader parameters
            common_params = {
                'pin_memory': pin_memory,
                'timeout': timeout,
                'generator': generator
            }
            
            # Only add prefetch_factor if num_workers > 0
            if num_workers > 0:
                common_params['prefetch_factor'] = prefetch_factor
            
            # Add multiprocessing context if available
            if mp_ctx and num_workers > 0:
                common_params['multiprocessing_context'] = mp_ctx
            
            # Add collate function and worker configuration
            if num_workers > 0:
                common_params.update({
                    'num_workers': num_workers,
                    'worker_init_fn': worker_init_fn,  # This is now picklable
                    'persistent_workers': persistent_workers,
                    'collate_fn': enhanced_collate_fn
                })
            else:
                common_params.update({
                    'num_workers': 0,
                    'worker_init_fn': None,
                    'persistent_workers': False,
                    'collate_fn': enhanced_collate_fn
                })
            
            # Create training DataLoader with error handling
            if verbose:
                logger.info(f"Creating training DataLoader: batch_size={train_batch_size}, num_workers={num_workers}")
            
            train_loader_params = common_params.copy()
            train_loader_params.update({
                'batch_size': train_batch_size,
                'shuffle': train_shuffle,
                'drop_last': drop_last,
                'sampler': train_sampler,
                'batch_sampler': batch_sampler
            })
            
            # Multiple fallback attempts for training DataLoader
            train_loader = None
            fallback_attempts = 0
            
            for attempt in range(max_retries):
                try:
                    train_loader = DataLoader(train_dataset, **train_loader_params)
                    if verbose:
                        logger.debug(f"Training DataLoader created successfully on attempt {attempt + 1}")
                    break
                    
                except Exception as e:
                    fallback_attempts += 1
                    error_str = str(e).lower()
                    
                    # Handle prefetch_factor error specifically
                    if "prefetch_factor" in error_str and train_loader_params.get('num_workers', 0) == 0:
                        if verbose:
                            logger.warning(f"Attempt {attempt + 1}: Removing prefetch_factor for single-threaded mode")
                        if 'prefetch_factor' in train_loader_params:
                            del train_loader_params['prefetch_factor']
                    
                    elif "pickle" in error_str or "worker_init" in error_str:
                        if verbose:
                            logger.warning(f"Attempt {attempt + 1}: Worker initialization pickling error detected: {e}")
                        # Progressive fallback strategy
                        if attempt == 0:
                            # First attempt: Remove custom worker_init_fn and reduce workers
                            if verbose:
                                logger.info("Fallback 1: Removing custom worker initialization and reducing workers")
                            train_loader_params['worker_init_fn'] = None
                            train_loader_params['num_workers'] = min(2, num_workers)
                            train_loader_params['persistent_workers'] = False
                            # Remove prefetch_factor if switching to single-threaded
                            if train_loader_params['num_workers'] == 0 and 'prefetch_factor' in train_loader_params:
                                del train_loader_params['prefetch_factor']
                        elif attempt == 1:
                            # Second attempt: Single-threaded
                            if verbose:
                                logger.info("Fallback 2: Switching to single-threaded DataLoader")
                            train_loader_params.update({
                                'num_workers': 0,
                                'worker_init_fn': None,
                                'persistent_workers': False,
                                'pin_memory': False,
                                'collate_fn': None
                            })
                            # Remove prefetch_factor for single-threaded
                            if 'prefetch_factor' in train_loader_params:
                                del train_loader_params['prefetch_factor']
                            if 'multiprocessing_context' in train_loader_params:
                                del train_loader_params['multiprocessing_context']
                        else:
                            # Final attempt: Minimal configuration
                            if verbose:
                                logger.info("Fallback 3: Using minimal DataLoader configuration")
                            train_loader_params = {
                                'batch_size': error_config.get('fallback_batch_size', 32),
                                'shuffle': train_shuffle,
                                'num_workers': 0,
                                'pin_memory': False,
                                'drop_last': False
                            }
                    
                    elif "memory" in error_str or "out of memory" in error_str:
                        if verbose:
                            logger.warning(f"Memory error detected: {e}")
                        # Reduce batch size and workers
                        current_batch_size = train_loader_params.get('batch_size', batch_size)
                        new_batch_size = max(1, current_batch_size // 2)
                        train_loader_params['batch_size'] = new_batch_size
                        train_loader_params['num_workers'] = max(0, train_loader_params.get('num_workers', 0) - 1)
                        # Remove prefetch_factor if switching to single-threaded
                        if train_loader_params['num_workers'] == 0 and 'prefetch_factor' in train_loader_params:
                            del train_loader_params['prefetch_factor']
                        if verbose:
                            logger.info(f"Reduced batch size to {new_batch_size} and workers to {train_loader_params['num_workers']}")
                    
                    else:
                        if verbose:
                            logger.warning(f"Attempt {attempt + 1}: Unexpected error: {e}")
                        if attempt == max_retries - 1:
                            if graceful_degradation:
                                if verbose:
                                    logger.warning("Using absolute minimal DataLoader configuration")
                                train_loader_params = {
                                    'batch_size': 1,
                                    'shuffle': False,
                                    'num_workers': 0
                                }
                            else:
                                raise RuntimeError(f"Failed to create training DataLoader after {max_retries} attempts: {e}")
            
            if train_loader is None:
                raise RuntimeError("Failed to create training DataLoader with all fallback attempts")
            
            progress_data['dataloaders_created'] += 1
            progress_data['fallback_attempts'] = fallback_attempts
            if not silent_mode:
                main_bar.text = f"Training DataLoader created (attempts: {fallback_attempts + 1})"
            creation_stats['stages_completed'].append('training_dataloader')
            if not silent_mode:
                main_bar()
            
            # STAGE 9: Validation DataLoader Creation
            progress_data['current_stage'] = "Validation DataLoader"
            if not silent_mode:
                main_bar.text = "Creating validation DataLoader..."
            
            # Create validation DataLoader
            val_loader = None
            if val_dataset is not None:
                if verbose:
                    logger.info(f"Creating validation DataLoader: batch_size={val_batch_size}")
                val_loader_params = common_params.copy()
                val_loader_params.update({
                    'batch_size': val_batch_size,
                    'shuffle': val_shuffle,
                    'drop_last': False,
                    'sampler': val_sampler
                })
                
                for attempt in range(max_retries):
                    try:
                        val_loader = DataLoader(val_dataset, **val_loader_params)
                        break
                    except Exception as e:
                        error_str = str(e).lower()
                        
                        if "prefetch_factor" in error_str and val_loader_params.get('num_workers', 0) == 0:
                            if 'prefetch_factor' in val_loader_params:
                                del val_loader_params['prefetch_factor']
                        elif attempt == max_retries - 1:
                            if graceful_degradation:
                                val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)
                            else:
                                raise RuntimeError(f"Failed to create validation DataLoader: {e}")
                
                progress_data['dataloaders_created'] += 1
            
            if not silent_mode:
                main_bar.text = f"Validation DataLoader {'created' if val_loader else 'skipped'}"
            creation_stats['stages_completed'].append('validation_dataloader')
            if not silent_mode:
                main_bar()
            
            # STAGE 10: Test DataLoader Creation
            progress_data['current_stage'] = "Test DataLoader"
            if not silent_mode:
                main_bar.text = "Creating test DataLoader..."
            
            # Create test DataLoader
            test_loader = None
            if test_dataset is not None:
                if verbose:
                    logger.info(f"Creating test DataLoader: batch_size={test_batch_size}")
                test_loader_params = common_params.copy()
                test_loader_params.update({
                    'batch_size': test_batch_size,
                    'shuffle': test_shuffle,
                    'drop_last': False,
                    'sampler': test_sampler
                })
                
                for attempt in range(max_retries):
                    try:
                        test_loader = DataLoader(test_dataset, **test_loader_params)
                        break
                    except Exception as e:
                        error_str = str(e).lower()
                        
                        if "prefetch_factor" in error_str and test_loader_params.get('num_workers', 0) == 0:
                            if 'prefetch_factor' in test_loader_params:
                                del test_loader_params['prefetch_factor']
                        elif attempt == max_retries - 1:
                            if graceful_degradation:
                                test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)
                            else:
                                raise RuntimeError(f"Failed to create test DataLoader: {e}")
                
                progress_data['dataloaders_created'] += 1
            
            if not silent_mode:
                main_bar.text = f"Test DataLoader {'created' if test_loader else 'skipped'}"
            creation_stats['stages_completed'].append('test_dataloader')
            if not silent_mode:
                main_bar()
            
            # STAGE 11: Cross-validation DataLoaders
            progress_data['current_stage'] = "Cross-validation"
            if not silent_mode:
                main_bar.text = "Creating cross-validation DataLoaders..."
            
            # Cross-validation DataLoaders
            cv_loaders = None
            if cv_config.get('cross_validation', False):
                if verbose:
                    logger.info("Creating cross-validation DataLoaders")
                
                cv_folds = cv_config.get('cv_folds', 5)
                cv_strategy = cv_config.get('cv_strategy', 'kfold')
                stratified_cv = cv_config.get('stratified_cv', False)
                
                try:
                    if stratified_cv and y_train is not None:
                        cv_splitter = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=sampling_config.get('shuffle_seed', 42))
                        splits = list(cv_splitter.split(X_train, y_train))
                    else:
                        cv_splitter = KFold(n_splits=cv_folds, shuffle=True, random_state=sampling_config.get('shuffle_seed', 42))
                        splits = list(cv_splitter.split(X_train))
                    
                    cv_loaders = []
                    
                    # Create progress bar for CV folds only if not in silent mode and progress_bar is True
                    if not silent_mode and progress_bar:
                        cv_bar_context = alive_bar(cv_folds, title='CV Folds\t\t\t', unit='folds')
                    else:
                        cv_bar_context = DummyBar()
                    
                    with cv_bar_context as cv_bar:
                        for fold_idx, (train_idx, val_idx) in enumerate(splits):
                            if not silent_mode:
                                cv_bar.text = f"Creating fold {fold_idx + 1}/{cv_folds}"
                            
                            fold_train_X = X_train[train_idx]
                            fold_val_X = X_train[val_idx]
                            fold_train_y = y_train[train_idx] if y_train is not None else None
                            fold_val_y = y_train[val_idx] if y_train is not None else None
                            
                            fold_train_dataset = create_enhanced_dataset(fold_train_X, fold_train_y, train_transform)
                            fold_val_dataset = create_enhanced_dataset(fold_val_X, fold_val_y, val_transform)
                            
                            fold_train_loader = DataLoader(
                                fold_train_dataset,
                                batch_size=train_batch_size,
                                shuffle=train_shuffle,
                                # Reduce workers for CV
                                num_workers=max(1, num_workers // 2),
                                pin_memory=pin_memory,
                                collate_fn=enhanced_collate_fn
                            )
                            
                            fold_val_loader = DataLoader(
                                fold_val_dataset,
                                batch_size=val_batch_size,
                                shuffle=False,
                                num_workers=max(1, num_workers // 2),
                                pin_memory=pin_memory,
                                collate_fn=enhanced_collate_fn
                            )
                            
                            cv_loaders.append({
                                'fold': fold_idx,
                                'train': fold_train_loader,
                                'val': fold_val_loader
                            })
                            
                            if not silent_mode:
                                cv_bar()
                    
                    if verbose:
                        logger.info(f"Created {cv_folds} cross-validation fold DataLoaders")
                    
                except Exception as e:
                    if verbose:
                        logger.error(f"Failed to create cross-validation DataLoaders: {e}")
                    cv_loaders = None
            
            if not silent_mode:
                main_bar.text = f"Cross-validation {'completed' if cv_loaders else 'skipped'}"
            creation_stats['stages_completed'].append('cross_validation')
            if not silent_mode:
                main_bar()
            
            # STAGE 12: Finalization and Performance Analysis
            progress_data['current_stage'] = "Finalization"
            if not silent_mode:
                main_bar.text = "Finalizing DataLoader creation..."
            
            # Performance profiling and benchmarking
            if profile_dataloaders or benchmark_dataloaders:
                if verbose:
                    logger.info("Performing DataLoader performance analysis")
                
                def benchmark_dataloader(dataloader, name, num_batches=10):
                    if dataloader is None:
                        return None
                    times = []
                    
                    dataloader_iter = iter(dataloader)
                    for i in range(min(num_batches, len(dataloader))):
                        start_time = time.time()
                        try:
                            batch = next(dataloader_iter)
                            end_time = time.time()
                            times.append(end_time - start_time)
                        except StopIteration:
                            break
                        except Exception as e:
                            if verbose:
                                logger.warning(f"Benchmark error for {name} batch {i}: {e}")
                            continue
                    
                    if times:
                        return {
                            'name': name,
                            'avg_batch_time': np.mean(times),
                            'std_batch_time': np.std(times),
                            'min_batch_time': np.min(times),
                            'max_batch_time': np.max(times),
                            'total_time': np.sum(times),
                            'batches_tested': len(times)
                        }
                    return None
                
                benchmark_results = {}
                if benchmark_dataloaders:
                    benchmark_results['train'] = benchmark_dataloader(train_loader, 'train', 5)
                    benchmark_results['val'] = benchmark_dataloader(val_loader, 'val', 3)
                    benchmark_results['test'] = benchmark_dataloader(test_loader, 'test', 3)
                    
                    creation_stats['benchmark_results'] = benchmark_results
                    
                    for name, result in benchmark_results.items():
                        if result:
                            if verbose:
                                logger.info(f"{name.title()} DataLoader: {result['avg_batch_time']:.4f}s avg batch time")
            
            # Collect DataLoader statistics
            if dataloader_stats:
                stats = {
                    'train': {
                        'dataset_size': len(train_dataset),
                        'batch_size': train_batch_size,
                        'num_batches': len(train_loader),
                        'shuffle': train_shuffle,
                        'sampler': type(train_sampler).__name__ if train_sampler else None,
                        'drop_last': drop_last
                    },
                    'val': {
                        'dataset_size': len(val_dataset) if val_dataset else 0,
                        'batch_size': val_batch_size,
                        'num_batches': len(val_loader) if val_loader else 0,
                        'shuffle': val_shuffle,
                        'sampler': type(val_sampler).__name__ if val_sampler else None
                    } if val_dataset else None,
                    'test': {
                        'dataset_size': len(test_dataset) if test_dataset else 0,
                        'batch_size': test_batch_size,
                        'num_batches': len(test_loader) if test_loader else 0,
                        'shuffle': test_shuffle,
                        'sampler': type(test_sampler).__name__ if test_sampler else None
                    } if test_dataset else None,
                    'common': {
                        'num_workers': num_workers,
                        'pin_memory': pin_memory,
                        'persistent_workers': persistent_workers,
                        'prefetch_factor': prefetch_factor,
                        'timeout': timeout,
                        'dtype': str(dtype),
                        'device': device
                    }
                }
                
                creation_stats['dataloader_stats'] = stats
            
            # Garbage collection if requested
            if memory_config.get('gc_collection', False):
                gc.collect()
                if verbose:
                    logger.debug("Performed garbage collection")
            
            # Calculate performance score
            performance_metrics = []
            
            # Worker efficiency
            worker_score = min(1.0, num_workers / max(1, system_cpu_count))
            performance_metrics.append(('worker_efficiency', worker_score))
            
            # Batch size adequacy
            batch_score = min(1.0, train_batch_size / max(32, train_batch_size))
            performance_metrics.append(('batch_adequacy', batch_score))
            
            # Memory optimization
            memory_score = 0.8 if pin_memory else 0.5
            performance_metrics.append(('memory_optimization', memory_score))
            
            # Overall performance score
            performance_score = sum(score for _, score in performance_metrics) / len(performance_metrics)
            progress_data['performance_score'] = performance_score
            creation_stats['performance_score'] = performance_score
            
            if not silent_mode:
                main_bar.text = f"Finalization complete (Performance: {performance_score:.3f})"
            creation_stats['stages_completed'].append('finalization')
            if not silent_mode:
                main_bar()
        
        # Prepare return value based on configuration
        total_time = (datetime.now() - start_time).total_seconds()
        creation_stats['total_processing_time'] = total_time
        creation_stats['completion_status'] = 'success'
        
        # Create metadata
        metadata = {
            'creation_time_seconds': total_time,
            'dataloaders_created': {
                'train': True,
                'val': val_loader is not None,
                'test': test_loader is not None,
                'cv_folds': len(cv_loaders) if cv_loaders else 0
            },
            'configuration_applied': final_config,
            'creation_stats': creation_stats,
            'system_info': creation_stats['system_info'],
            'optimization_level': optimization_level,
            'performance_optimizations_applied': dataloader_optimization,
            'error_handling_mode': error_handling,
            'progress_summary': {
                'stages_completed': len(creation_stats['stages_completed']),
                'dataloaders_created': progress_data['dataloaders_created'],
                'datasets_processed': progress_data['datasets_processed'],
                'transforms_applied': progress_data['transforms_applied'],
                'samplers_configured': progress_data['samplers_configured'],
                'fallback_attempts': progress_data['fallback_attempts'],
                'final_performance_score': progress_data['performance_score']
            }
        }
        
        # Determine return format
        return_format = final_config.get('output', {}).get('format', 'tuple')
        
        if return_format == 'dict':
            result = {
                'train': train_loader,
                'val': val_loader,
                'test': test_loader,
                'metadata': metadata
            }
            
            if cv_loaders:
                result['cv_folds'] = cv_loaders
                
            return result
        
        elif return_format == 'single':
            # Return only training loader for specific use cases
            return train_loader
        
        else:
            # Default tuple format
            result = (train_loader, val_loader, test_loader)
            
            # Add metadata as attribute to train_loader for access
            train_loader.dataloader_metadata = metadata
            
            if cv_loaders:
                train_loader.cv_folds = cv_loaders
            
            return result
        
    except Exception as e:
        # Update creation stats with error information
        creation_stats['completion_status'] = 'failed'
        creation_stats['error_message'] = str(e)
        creation_stats['error_traceback'] = traceback.format_exc()
        
        # Restore original logging level on error
        if verbose and 'original_level' in locals():
            logger.setLevel(original_level)
        
        error_msg = f"DataLoader creation failed: {str(e)}"
        if verbose:
            logger.error(error_msg)
            logger.error(f"Full traceback: {traceback.format_exc()}")
            
            # Provide helpful error context
            logger.error(f"Configuration used: {final_config}")
            logger.error(f"Stages completed: {creation_stats['stages_completed']}")
        
        # Attempt graceful fallback if enabled
        if graceful_degradation and data is not None and X_train is not None:
            if verbose:
                logger.warning("Attempting graceful fallback to basic DataLoaders")
            
            try:
                # Create minimal DataLoaders
                train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32))
                val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32)) if X_val is not None else None
                test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32)) if X_test is not None else None
                
                fallback_batch_size = error_config.get('fallback_batch_size', 32)
                
                train_loader = DataLoader(train_data, batch_size=fallback_batch_size, shuffle=True, num_workers=0)
                val_loader = DataLoader(val_data, batch_size=fallback_batch_size, shuffle=False, num_workers=0) if val_data else None
                test_loader = DataLoader(test_data, batch_size=fallback_batch_size, shuffle=False, num_workers=0) if test_data else None
                
                if verbose:
                    logger.warning("Created fallback DataLoaders with minimal configuration")
                
                # Restore original logging level
                if verbose and 'original_level' in locals():
                    logger.setLevel(original_level)
                
                return (train_loader, val_loader, test_loader)
                
            except Exception as fallback_error:
                if verbose:
                    logger.error(f"Fallback DataLoader creation also failed: {fallback_error}")
                raise RuntimeError(f"Both primary and fallback DataLoader creation failed: {error_msg}")
        
        raise RuntimeError(error_msg)
    
    finally:
        # Restore original logging level
        if verbose and 'original_level' in locals():
            logger.setLevel(original_level)
        
        # Final cleanup and summary logging
        if final_config.get('monitoring', {}).get('log_creation_summary', True):
            total_time = (datetime.now() - start_time).total_seconds()
            logger.info("-" * 40)
            logger.info("DATALOADER CREATION SUMMARY")
            logger.info("-" * 40)
            logger.info(f"Creation time: {total_time:.2f} seconds")
            logger.info(f"Training DataLoader: batch_size={train_batch_size}, num_batches={len(train_loader) if 'train_loader' in locals() else 'N/A'}")
            logger.info(f"Validation DataLoader: {'Created' if val_loader else 'Not created'}")
            logger.info(f"Test DataLoader: {'Created' if test_loader else 'Not created'}")
            logger.info(f"Workers: {num_workers}, Pin memory: {pin_memory}")
            logger.info(f"Optimization level: {optimization_level}")
            logger.info(f"Performance score: {progress_data.get('performance_score', 0):.3f}")
            logger.info(f"Fallback attempts: {progress_data.get('fallback_attempts', 0)}")
            if 'cv_loaders' in locals() and cv_loaders:
                logger.info(f"Cross-validation folds: {len(cv_loaders)}")
            logger.info("-" * 40)

def train_epoch(
    # Core Training Parameters
    model: Optional[nn.Module] = None,
    loader: Optional[DataLoader] = None,
    criterion: Optional[nn.Module] = None,
    optimizer: Optional[optim.Optimizer] = None,
    device: Optional[torch.device] = None,
    epoch: Optional[int] = None,
    
    # Training Configuration Parameters
    learning_rate: Optional[float] = None,
    batch_size: Optional[int] = None,
    gradient_clip: Optional[float] = None,
    gradient_accumulation_steps: Optional[int] = None,
    max_grad_norm: Optional[float] = None,
    gradient_clipping_mode: Optional[str] = None,
    gradient_scaling: Optional[bool] = None,
    
    # Mixed Precision Parameters
    mixed_precision: Optional[bool] = None,
    amp_enabled: Optional[bool] = None,
    scaler: Optional[GradScaler] = None,
    loss_scaling: Optional[str] = None,
    dynamic_loss_scaling: Optional[bool] = None,
    init_scale: Optional[float] = None,
    growth_factor: Optional[float] = None,
    backoff_factor: Optional[float] = None,
    growth_interval: Optional[int] = None,
    
    # Scheduler Parameters
    scheduler: Optional[Any] = None,
    scheduler_step_on_epoch: Optional[bool] = None,
    scheduler_step_on_batch: Optional[bool] = None,
    scheduler_step_after_epoch: Optional[bool] = None,
    scheduler_metric: Optional[str] = None,
    warmup_steps: Optional[int] = None,
    warmup_scheduler: Optional[Any] = None,
    
    # Loss Function Parameters
    loss_function: Optional[str] = None,
    loss_weights: Optional[Dict[str, float]] = None,
    multi_task_learning: Optional[bool] = None,
    auxiliary_loss_weight: Optional[float] = None,
    regularization_loss_weight: Optional[float] = None,
    reconstruction_loss_weight: Optional[float] = None,
    
    # Monitoring and Metrics Parameters
    track_metrics: Optional[bool] = None,
    metrics_to_track: Optional[List[str]] = None,
    calculate_detailed_metrics: Optional[bool] = None,
    metrics_frequency: Optional[int] = None,
    log_frequency: Optional[int] = None,
    progress_bar: Optional[bool] = None,
    progress_bar_desc: Optional[str] = None,
    
    # Performance and Optimization Parameters
    performance_mode: Optional[str] = None,
    benchmark_mode: Optional[bool] = None,
    cudnn_benchmark: Optional[bool] = None,
    cudnn_deterministic: Optional[bool] = None,
    channels_last: Optional[bool] = None,
    compile_model: Optional[bool] = None,
    torch_compile_mode: Optional[str] = None,
    
    # Memory Management Parameters
    memory_efficient: Optional[bool] = None,
    memory_optimization: Optional[str] = None,
    gradient_checkpointing: Optional[bool] = None,
    empty_cache_frequency: Optional[int] = None,
    gc_collection_frequency: Optional[int] = None,
    pin_memory: Optional[bool] = None,
    non_blocking_transfer: Optional[bool] = None,
    
    # Data Processing Parameters
    data_preprocessing: Optional[bool] = None,
    input_transforms: Optional[List[Callable]] = None,
    target_transforms: Optional[List[Callable]] = None,
    augmentation_during_training: Optional[bool] = None,
    mixup_alpha: Optional[float] = None,
    cutmix_alpha: Optional[float] = None,
    
    # Validation and Quality Control Parameters
    validate_inputs: Optional[bool] = None,
    check_finite: Optional[bool] = None,
    detect_anomaly: Optional[bool] = None,
    handle_nan_loss: Optional[str] = None,
    loss_spike_detection: Optional[bool] = None,
    loss_spike_threshold: Optional[float] = None,
    gradient_explosion_detection: Optional[bool] = None,
    
    # Distributed Training Parameters
    distributed: Optional[bool] = None,
    world_size: Optional[int] = None,
    rank: Optional[int] = None,
    local_rank: Optional[int] = None,
    ddp_backend: Optional[str] = None,
    find_unused_parameters: Optional[bool] = None,
    broadcast_buffers: Optional[bool] = None,
    gradient_as_bucket_view: Optional[bool] = None,
    
    # Checkpointing and Saving Parameters
    save_checkpoint: Optional[bool] = None,
    checkpoint_frequency: Optional[int] = None,
    checkpoint_path: Optional[str] = None,
    save_best_model: Optional[bool] = None,
    best_metric: Optional[str] = None,
    model_state_dict: Optional[bool] = None,
    save_optimizer_state: Optional[bool] = None,
    save_scheduler_state: Optional[bool] = None,
    
    # Early Stopping Parameters
    early_stopping: Optional[bool] = None,
    early_stopping_patience: Optional[int] = None,
    early_stopping_delta: Optional[float] = None,
    early_stopping_metric: Optional[str] = None,
    early_stopping_mode: Optional[str] = None,
    restore_best_weights: Optional[bool] = None,
    
    # Debugging and Profiling Parameters
    debug_mode: Optional[bool] = None,
    verbose: Optional[bool] = None,
    log_level: Optional[str] = None,
    profile_training: Optional[bool] = None,
    profile_memory: Optional[bool] = None,
    profile_compute: Optional[bool] = None,
    timing_analysis: Optional[bool] = None,
    bottleneck_detection: Optional[bool] = None,
    
    # Advanced Training Techniques Parameters
    teacher_forcing_ratio: Optional[float] = None,
    curriculum_learning: Optional[bool] = None,
    curriculum_schedule: Optional[str] = None,
    progressive_training: Optional[bool] = None,
    adaptive_training: Optional[bool] = None,
    self_supervised_pretext: Optional[bool] = None,
    
    # Regularization Parameters
    dropout_rate: Optional[float] = None,
    weight_decay: Optional[float] = None,
    l1_regularization: Optional[float] = None,
    l2_regularization: Optional[float] = None,
    spectral_normalization: Optional[bool] = None,
    batch_norm_momentum: Optional[float] = None,
    layer_norm_eps: Optional[float] = None,
    
    # Loss Smoothing and Stability Parameters
    label_smoothing: Optional[float] = None,
    focal_loss_alpha: Optional[float] = None,
    focal_loss_gamma: Optional[float] = None,
    loss_smoothing: Optional[str] = None,
    stable_loss_computation: Optional[bool] = None,
    
    # Batch Processing Parameters
    batch_processing_mode: Optional[str] = None,
    variable_batch_size: Optional[bool] = None,
    dynamic_batching: Optional[bool] = None,
    batch_size_adaptation: Optional[str] = None,
    max_tokens_per_batch: Optional[int] = None,
    
    # Hardware Optimization Parameters
    use_gpu: Optional[bool] = None,
    multi_gpu: Optional[bool] = None,
    gpu_ids: Optional[List[int]] = None,
    device_placement: Optional[str] = None,
    tensor_parallel: Optional[bool] = None,
    data_parallel: Optional[bool] = None,
    pipeline_parallel: Optional[bool] = None,
    
    # Random State and Reproducibility Parameters
    random_seed: Optional[int] = None,
    deterministic: Optional[bool] = None,
    reproducible: Optional[bool] = None,
    seed_workers: Optional[bool] = None,
    
    # Custom Callback Parameters
    callbacks: Optional[List[Callable]] = None,
    custom_training_step: Optional[Callable] = None,
    custom_loss_computation: Optional[Callable] = None,
    custom_metric_computation: Optional[Callable] = None,
    pre_batch_callback: Optional[Callable] = None,
    post_batch_callback: Optional[Callable] = None,
    
    # Export and Logging Parameters
    export_metrics: Optional[bool] = None,
    export_path: Optional[str] = None,
    tensorboard_logging: Optional[bool] = None,
    wandb_logging: Optional[bool] = None,
    mlflow_logging: Optional[bool] = None,
    log_images: Optional[bool] = None,
    log_gradients: Optional[bool] = None,
    log_weights: Optional[bool] = None,
    
    # Compatibility Parameters
    pytorch_version_check: Optional[bool] = None,
    legacy_mode: Optional[bool] = None,
    backward_compatibility: Optional[bool] = None,
    version_specific_optimizations: Optional[bool] = None,
    
    # Error Handling Parameters
    error_handling: Optional[str] = None,
    continue_on_error: Optional[bool] = None,
    max_retries: Optional[int] = None,
    fallback_mode: Optional[bool] = None,
    graceful_degradation: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_features: Optional[bool] = None,
    experimental_optimizations: Optional[bool] = None,
    beta_features: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    training_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Tuple[float, Dict[str, Any]]:
    
    # Initialize default return values FIRST
    default_loss = float('inf')
    default_metrics = {
        'loss': default_loss,
        'epoch': epoch if epoch is not None else 0,
        'num_batches': 0,
        'num_samples': 0,
        'training_completed': False,
        'error': None
    }
    
    # Start timing
    start_time = datetime.now()
    epoch_start_time = time.time()
    
    # Initialize variables that will be used in finally block
    num_batches = 0
    num_samples = 0
    total_loss = 0.0
    batch_idx = -1
    pbar = None
    profiler = None
    original_level = None
    
    try:
        # Initialize configuration
        if config is None:
            try:
                config = get_current_config() if 'get_current_config' in globals() else {}
            except Exception:
                config = {}
        
        # Apply training-specific configuration
        if training_config:
            config.setdefault('training', {}).update(training_config)
        
        # Apply all parameters to configuration
        final_config = {}
        
        # Merge with existing config
        final_config.update(config)
        
        # Apply individual parameters with intelligent organization
        params = locals().copy()
        params.update(kwargs)
        
        # Remove non-parameter items
        params_to_remove = {
            'config', 'training_config', 'kwargs', 'start_time', 'epoch_start_time', 'datetime', 'traceback', 'time', 'gc', 'warnings', 'defaultdict', 'deque',
            'nullcontext', 'nn', 'optim', 'DataLoader', 'GradScaler', 'autocast'
        }
        
        cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
        
        # Organize parameters into logical sections
        param_sections = {
            'core_training': [
                'learning_rate', 'batch_size', 'gradient_clip', 'gradient_accumulation_steps', 'max_grad_norm', 'gradient_clipping_mode', 'gradient_scaling'
            ],
            'mixed_precision': [
                'mixed_precision', 'amp_enabled', 'scaler', 'loss_scaling', 'dynamic_loss_scaling', 'init_scale', 'growth_factor', 'backoff_factor', 'growth_interval'
            ],
            'scheduler': [
                'scheduler_step_on_epoch', 'scheduler_step_on_batch', 'scheduler_step_after_epoch', 'scheduler_metric', 'warmup_steps', 'warmup_scheduler'
            ],
            'loss_function': [
                'loss_function', 'loss_weights', 'multi_task_learning', 'auxiliary_loss_weight', 'regularization_loss_weight', 'reconstruction_loss_weight'
            ],
            'monitoring': [
                'track_metrics', 'metrics_to_track', 'calculate_detailed_metrics', 'metrics_frequency', 'log_frequency', 'progress_bar', 'progress_bar_desc'
            ],
            'performance': [
                'performance_mode', 'benchmark_mode', 'cudnn_benchmark', 'cudnn_deterministic', 'channels_last', 'compile_model', 'torch_compile_mode'
            ],
            'memory_management': [
                'memory_efficient', 'memory_optimization', 'gradient_checkpointing', 'empty_cache_frequency', 'gc_collection_frequency', 'pin_memory', 'non_blocking_transfer'
            ],
            'data_processing': [
                'data_preprocessing', 'input_transforms', 'target_transforms', 'augmentation_during_training', 'mixup_alpha', 'cutmix_alpha'
            ],
            'validation': [
                'validate_inputs', 'check_finite', 'detect_anomaly', 'handle_nan_loss', 'loss_spike_detection', 'loss_spike_threshold', 'gradient_explosion_detection'
            ],
            'distributed': [
                'distributed', 'world_size', 'rank', 'local_rank', 'ddp_backend', 'find_unused_parameters', 'broadcast_buffers', 'gradient_as_bucket_view'
            ],
            'checkpointing': [
                'save_checkpoint', 'checkpoint_frequency', 'checkpoint_path', 'save_best_model', 'best_metric', 'model_state_dict', 'save_optimizer_state', 'save_scheduler_state'
            ],
            'early_stopping': [
                'early_stopping', 'early_stopping_patience', 'early_stopping_delta', 'early_stopping_metric', 'early_stopping_mode', 'restore_best_weights'
            ],
            'debugging': [
                'debug_mode', 'verbose', 'log_level', 'profile_training', 'profile_memory', 'profile_compute', 'timing_analysis', 'bottleneck_detection'
            ],
            'advanced_training': [
                'teacher_forcing_ratio', 'curriculum_learning', 'curriculum_schedule', 'progressive_training', 'adaptive_training', 'self_supervised_pretext'
            ],
            'regularization': [
                'dropout_rate', 'weight_decay', 'l1_regularization', 'l2_regularization', 'spectral_normalization', 'batch_norm_momentum', 'layer_norm_eps'
            ],
            'loss_stability': [
                'label_smoothing', 'focal_loss_alpha', 'focal_loss_gamma', 'loss_smoothing', 'stable_loss_computation'
            ],
            'batch_processing': [
                'batch_processing_mode', 'variable_batch_size', 'dynamic_batching', 'batch_size_adaptation', 'max_tokens_per_batch'
            ],
            'hardware_optimization': [
                'use_gpu', 'multi_gpu', 'gpu_ids', 'device_placement', 'tensor_parallel', 'data_parallel', 'pipeline_parallel'
            ],
            'reproducibility': [
                'random_seed', 'deterministic', 'reproducible', 'seed_workers'
            ],
            'callbacks': [
                'callbacks', 'custom_training_step', 'custom_loss_computation', 'custom_metric_computation', 'pre_batch_callback', 'post_batch_callback'
            ],
            'export_logging': [
                'export_metrics', 'export_path', 'tensorboard_logging', 'wandb_logging', 'mlflow_logging', 'log_images', 'log_gradients', 'log_weights'
            ],
            'compatibility': [
                'pytorch_version_check', 'legacy_mode', 'backward_compatibility', 'version_specific_optimizations'
            ],
            'error_handling': [
                'error_handling', 'continue_on_error', 'max_retries', 'fallback_mode', 'graceful_degradation'
            ],
            'experimental': [
                'experimental_features', 'experimental_optimizations', 'beta_features'
            ]
        }
        
        # Apply parameters to appropriate sections
        for section, param_list in param_sections.items():
            section_config = final_config.setdefault(section, {})
            for param in param_list:
                if param in cleaned_params:
                    section_config[param] = cleaned_params[param]
        
        # Set up defaults
        core_config = final_config.setdefault('core_training', {})
        mixed_precision_config = final_config.setdefault('mixed_precision', {})
        scheduler_config = final_config.setdefault('scheduler', {})
        loss_config = final_config.setdefault('loss_function', {})
        monitoring_config = final_config.setdefault('monitoring', {})
        performance_config = final_config.setdefault('performance', {})
        memory_config = final_config.setdefault('memory_management', {})
        data_processing_config = final_config.setdefault('data_processing', {})
        validation_config = final_config.setdefault('validation', {})
        distributed_config = final_config.setdefault('distributed', {})
        checkpoint_config = final_config.setdefault('checkpointing', {})
        early_stopping_config = final_config.setdefault('early_stopping', {})
        debug_config = final_config.setdefault('debugging', {})
        advanced_config = final_config.setdefault('advanced_training', {})
        regularization_config = final_config.setdefault('regularization', {})
        loss_stability_config = final_config.setdefault('loss_stability', {})
        batch_config = final_config.setdefault('batch_processing', {})
        hardware_config = final_config.setdefault('hardware_optimization', {})
        reproducibility_config = final_config.setdefault('reproducibility', {})
        callback_config = final_config.setdefault('callbacks', {})
        export_config = final_config.setdefault('export_logging', {})
        compatibility_config = final_config.setdefault('compatibility', {})
        error_config = final_config.setdefault('error_handling', {})
        experimental_config = final_config.setdefault('experimental', {})
        
        # Apply intelligent defaults with system awareness
        learning_rate = core_config.setdefault('learning_rate', LEARNING_RATE)
        batch_size = core_config.setdefault('batch_size', DEFAULT_BATCH_SIZE)
        gradient_clip = core_config.setdefault('gradient_clip', GRADIENT_CLIP)
        gradient_accumulation_steps = core_config.setdefault('gradient_accumulation_steps', GRADIENT_ACCUMULATION_STEPS)
        max_grad_norm = core_config.setdefault('max_grad_norm', gradient_clip)
        gradient_clipping_mode = core_config.setdefault('gradient_clipping_mode', 'norm')
        gradient_scaling = core_config.setdefault('gradient_scaling', True)
        
        # Mixed precision defaults
        mixed_precision = mixed_precision_config.setdefault('mixed_precision', MIXED_PRECISION and torch.cuda.is_available())
        amp_enabled = mixed_precision_config.setdefault('amp_enabled', mixed_precision)
        dynamic_loss_scaling = mixed_precision_config.setdefault('dynamic_loss_scaling', True)
        init_scale = mixed_precision_config.setdefault('init_scale', 65536.0)
        growth_factor = mixed_precision_config.setdefault('growth_factor', 2.0)
        backoff_factor = mixed_precision_config.setdefault('backoff_factor', 0.5)
        growth_interval = mixed_precision_config.setdefault('growth_interval', 2000)
        
        # Scheduler defaults
        scheduler_step_on_epoch = scheduler_config.setdefault('scheduler_step_on_epoch', True)
        scheduler_step_on_batch = scheduler_config.setdefault('scheduler_step_on_batch', False)
        scheduler_step_after_epoch = scheduler_config.setdefault('scheduler_step_after_epoch', False)
        scheduler_metric = scheduler_config.setdefault('scheduler_metric', 'loss')
        
        # Monitoring defaults
        track_metrics = monitoring_config.setdefault('track_metrics', True)
        metrics_to_track = monitoring_config.setdefault('metrics_to_track', ['loss', 'learning_rate', 'gradient_norm', 'batch_time'])
        calculate_detailed_metrics = monitoring_config.setdefault('calculate_detailed_metrics', False)
        metrics_frequency = monitoring_config.setdefault('metrics_frequency', 10)
        log_frequency = monitoring_config.setdefault('log_frequency', 10)
        progress_bar = monitoring_config.setdefault('progress_bar', True)
        progress_bar_desc = monitoring_config.setdefault('progress_bar_desc', f"Epoch {epoch if epoch is not None else 'N/A'}")
        
        # Performance defaults
        performance_mode = performance_config.setdefault('performance_mode', 'standard')
        benchmark_mode = performance_config.setdefault('benchmark_mode', False)
        cudnn_benchmark = performance_config.setdefault('cudnn_benchmark', torch.cuda.is_available())
        cudnn_deterministic = performance_config.setdefault('cudnn_deterministic', False)
        compile_model = performance_config.setdefault('compile_model', False)
        torch_compile_mode = performance_config.setdefault('torch_compile_mode', 'default')
        
        # Memory management defaults
        memory_efficient = memory_config.setdefault('memory_efficient', True)
        memory_optimization = memory_config.setdefault('memory_optimization', 'balanced')
        gradient_checkpointing = memory_config.setdefault('gradient_checkpointing', False)
        empty_cache_frequency = memory_config.setdefault('empty_cache_frequency', 10)
        gc_collection_frequency = memory_config.setdefault('gc_collection_frequency', 100)
        non_blocking_transfer = memory_config.setdefault('non_blocking_transfer', True)
        
        # Data processing defaults
        data_preprocessing = data_processing_config.setdefault('data_preprocessing', False)
        augmentation_during_training = data_processing_config.setdefault('augmentation_during_training', False)
        
        # Validation defaults
        validate_inputs = validation_config.setdefault('validate_inputs', True)
        check_finite = validation_config.setdefault('check_finite', True)
        detect_anomaly = validation_config.setdefault('detect_anomaly', debug_config.get('debug_mode', False))
        handle_nan_loss = validation_config.setdefault('handle_nan_loss', 'error')
        loss_spike_detection = validation_config.setdefault('loss_spike_detection', True)
        loss_spike_threshold = validation_config.setdefault('loss_spike_threshold', 10.0)
        gradient_explosion_detection = validation_config.setdefault('gradient_explosion_detection', True)
        
        # Distributed defaults
        distributed = distributed_config.setdefault('distributed', False)
        find_unused_parameters = distributed_config.setdefault('find_unused_parameters', False)
        broadcast_buffers = distributed_config.setdefault('broadcast_buffers', True)
        gradient_as_bucket_view = distributed_config.setdefault('gradient_as_bucket_view', False)
        
        # Debug defaults
        debug_mode = debug_config.setdefault('debug_mode', False)
        verbose = debug_config.setdefault('verbose', False)
        log_level = debug_config.setdefault('log_level', 'INFO' if verbose else 'WARNING')
        profile_training = debug_config.setdefault('profile_training', False)
        timing_analysis = debug_config.setdefault('timing_analysis', False)
        
        # Error handling defaults
        error_handling = error_config.setdefault('error_handling', 'strict')
        continue_on_error = error_config.setdefault('continue_on_error', False)
        max_retries = error_config.setdefault('max_retries', 3)
        graceful_degradation = error_config.setdefault('graceful_degradation', True)
        
        # Set up logging level
        # if verbose:
        #     original_level = logger.level
        #     logger.setLevel(getattr(logging, log_level.upper()))
        
        logger.info(f"Starting training epoch {epoch if epoch is not None else 'N/A'}")
        
        # Parameter validation
        if model is None:
            raise ValueError("Model is required for training")
        if loader is None:
            raise ValueError("DataLoader is required for training")
        if criterion is None:
            raise ValueError("Loss criterion is required for training")
        if optimizer is None:
            raise ValueError("Optimizer is required for training")
        
        # Device configuration
        if device is None:
            device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')
        
        # Set up reproducibility
        if reproducibility_config.get('deterministic', False):
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            if reproducibility_config.get('random_seed'):
                torch.manual_seed(reproducibility_config['random_seed'])
                if torch.cuda.is_available():
                    torch.cuda.manual_seed_all(reproducibility_config['random_seed'])
        elif benchmark_mode or cudnn_benchmark:
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        
        # Initialize training statistics
        training_stats = {
            'start_time': start_time.isoformat(),
            'epoch': epoch,
            'config_applied': final_config,
            'device': str(device),
            'mixed_precision_enabled': mixed_precision,
            'gradient_accumulation_steps': gradient_accumulation_steps,
            'total_batches': len(loader),
            'batch_size': batch_size
        }
        
        # Set up mixed precision scaler
        if mixed_precision and scaler is None:
            scaler = GradScaler(
                enabled=amp_enabled,
                init_scale=init_scale,
                growth_factor=growth_factor,
                backoff_factor=backoff_factor,
                growth_interval=growth_interval
            )
        
        # Model compilation if requested
        if compile_model and hasattr(torch, 'compile'):
            try:
                logger.info(f"Compiling model with mode: {torch_compile_mode}")
                model = torch.compile(model, mode=torch_compile_mode)
                training_stats['model_compiled'] = True
            except Exception as e:
                logger.warning(f"Model compilation failed: {e}")
                training_stats['model_compiled'] = False
        
        # Set model to training mode
        model.train()
        
        # Memory optimization setup
        if memory_efficient:
            if gradient_checkpointing and hasattr(model, 'gradient_checkpointing_enable'):
                try:
                    model.gradient_checkpointing_enable()
                    logger.debug("Enabled gradient checkpointing")
                except Exception as e:
                    logger.warning(f"Failed to enable gradient checkpointing: {e}")
        
        # Initialize metrics tracking
        metrics_tracker = {
            'losses': [],
            'batch_times': [],
            'learning_rates': [],
            'gradient_norms': [],
            'memory_usage': [],
            'detailed_metrics': defaultdict(list) if calculate_detailed_metrics else None
        }
        
        # Initialize loss tracking for spike detection
        if loss_spike_detection:
            recent_losses = deque(maxlen=10)
        
        # Set up alive-progress bar
        if progress_bar:
            try:
                pbar = alive_bar(
                    total=len(loader),
                    title=f'Training {progress_bar_desc}\t',
                    unit='batches',
                    bar='smooth',
                    spinner='dots',
                    stats=False,
                    monitor=True,
                    elapsed=True,
                    stats_end=False
                )
                pbar.__enter__()  # Manually enter the context since we're not using 'with' statement
            except ImportError:
                logger.warning("alive-progress not available, progress bar disabled")
                pbar = None
            except Exception as e:
                logger.warning(f"Failed to initialize alive-progress bar: {e}")
                pbar = None
        else:
            pbar = None
        
        # Initialize profiling if requested
        if profile_training:
            try:
                profiler = torch.profiler.profile(
                    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
                    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),
                    on_trace_ready=torch.profiler.tensorboard_trace_handler(
                        export_config.get('export_path', './profiler_logs')
                    ),
                    record_shapes=True,
                    profile_memory=memory_config.get('profile_memory', True),
                    with_stack=True
                )
                profiler.start()
                training_stats['profiling_enabled'] = True
            except Exception as e:
                logger.warning(f"Failed to initialize profiler: {e}")
                profiler = None
                training_stats['profiling_enabled'] = False
        else:
            profiler = None
        
        # Anomaly detection setup
        anomaly_context = torch.autograd.detect_anomaly() if detect_anomaly else nullcontext()
        
        # Initialize training variables
        total_loss = 0.0
        num_batches = 0
        num_samples = 0
        accumulated_steps = 0
        best_loss = float('inf')
        
        # Initialize batch processing variables
        batch_start_time = None
        data_loading_time = 0
        forward_time = 0
        backward_time = 0
        optimizer_time = 0
        
        # Custom callbacks setup
        callbacks = callback_config.get('callbacks', [])
        custom_training_step = callback_config.get('custom_training_step')
        pre_batch_callback = callback_config.get('pre_batch_callback')
        post_batch_callback = callback_config.get('post_batch_callback')
        
        logger.info(f"Starting epoch with {len(loader)} batches")
        
        # Main training loop
        with anomaly_context:
            # Zero gradients before starting
            optimizer.zero_grad()
            
            for batch_idx, batch in enumerate(loader):
                batch_start_time = time.time()
                
                try:
                    # Pre-batch callback
                    if pre_batch_callback:
                        pre_batch_callback(batch_idx, batch, model, optimizer)
                    
                    # Data loading timing
                    data_load_end_time = time.time()
                    if batch_idx > 0:  # Skip first batch for timing accuracy
                        data_loading_time += (data_load_end_time - batch_start_time)
                    
                    # Move data to device with timing
                    if isinstance(batch, (list, tuple)):
                        inputs = batch[0].to(device, non_blocking=non_blocking_transfer)
                        targets = batch[1].to(device, non_blocking=non_blocking_transfer) if len(batch) > 1 else inputs
                    else:
                        inputs = batch.to(device, non_blocking=non_blocking_transfer)
                        targets = inputs
                    
                    # Input validation
                    if validate_inputs:
                        if check_finite:
                            if not torch.isfinite(inputs).all():
                                if handle_nan_loss == 'error':
                                    raise ValueError(f"Non-finite input values detected in batch {batch_idx}")
                                elif handle_nan_loss == 'skip':
                                    logger.warning(f"Skipping batch {batch_idx} due to non-finite inputs")
                                    continue
                                elif handle_nan_loss == 'clip':
                                    inputs = torch.nan_to_num(inputs, nan=0.0, posinf=1e6, neginf=-1e6)
                    
                    # Apply input transforms if specified
                    if data_processing_config.get('input_transforms'):
                        for transform in data_processing_config['input_transforms']:
                            inputs = transform(inputs)
                    
                    # Apply data augmentation during training
                    if augmentation_during_training:
                        # Mixup augmentation
                        if data_processing_config.get('mixup_alpha', 0) > 0:
                            mixup_alpha = data_processing_config['mixup_alpha']
                            lam = np.random.beta(mixup_alpha, mixup_alpha)
                            batch_size_current = inputs.size(0)
                            index = torch.randperm(batch_size_current).to(device)
                            inputs = lam * inputs + (1 - lam) * inputs[index]
                            if isinstance(targets, torch.Tensor) and targets.numel() > 0:
                                targets = lam * targets + (1 - lam) * targets[index]
                    
                    current_batch_size = inputs.size(0)
                    num_samples += current_batch_size
                    
                    # Forward pass with mixed precision and timing
                    forward_start_time = time.time()
                    
                    if custom_training_step:
                        # Custom training step
                        loss = custom_training_step(model, inputs, targets, criterion, optimizer, scaler, batch_idx)
                    else:
                        autocast_context = get_autocast_context(device, mixed_precision, True)
                        
                        with autocast_context:
                            outputs = model(inputs)
                            
                            # CRITICAL: Ensure targets match outputs for autoencoder
                            if targets.shape != outputs.shape:
                                if hasattr(model, '__class__') and 'autoencoder' in model.__class__.__name__.lower():
                                    targets = inputs  # For autoencoder, target should be input
                                    logger.debug("Set targets = inputs for autoencoder training")
                                else:
                                    # Try to reshape
                                    if outputs.numel() == targets.numel():
                                        targets = targets.view_as(outputs)
                                        logger.debug(f"Reshaped targets to match outputs: {outputs.shape}")
                                    else:
                                        # Try to match dimensions intelligently
                                        batch_size = outputs.size(0)
                                        if len(outputs.shape) == 2:  # [batch_size, features]
                                            output_features = outputs.size(1)
                                            if targets.size(1) != output_features:
                                                if targets.size(1) > output_features:
                                                    # Truncate targets
                                                    targets = targets[:, :output_features]
                                                else:
                                                    # Pad targets
                                                    padding_size = output_features - targets.size(1)
                                                    padding = torch.zeros(batch_size, padding_size, device=device, dtype=targets.dtype)
                                                    targets = torch.cat([targets, padding], dim=1)
                                                logger.debug(f"Adjusted targets shape to {targets.shape}")
                                        else:
                                            # For complex shapes, use inputs as targets
                                            targets = inputs
                                            logger.warning(f"Using inputs as targets for complex shape mismatch")
                            
                            # Custom loss computation with error handling
                            if callback_config.get('custom_loss_computation'):
                                try:
                                    loss = callback_config['custom_loss_computation'](outputs, targets, criterion)
                                except Exception as loss_e:
                                    logger.error(f"Custom loss computation failed: {loss_e}")
                                    if handle_nan_loss == 'skip':
                                        logger.warning(f"Skipping batch {batch_idx} due to custom loss error")
                                        continue
                                    else:
                                        raise
                            else:
                                try:
                                    loss = criterion(outputs, targets)
                                except Exception as loss_e:
                                    logger.error(f"Standard loss computation failed: {loss_e}")
                                    logger.error(f"Shapes - outputs: {outputs.shape}, targets: {targets.shape}")
                                    
                                    # Emergency shape fix
                                    try:
                                        if 'autoencoder' in str(type(model)).lower():
                                            targets = inputs
                                            logger.info("Emergency fix: Set targets = inputs for autoencoder")
                                        else:
                                            # Force reshape
                                            if outputs.dim() == 2 and targets.dim() == 2:
                                                min_features = min(outputs.size(1), targets.size(1))
                                                outputs_fixed = outputs[:, :min_features]
                                                targets_fixed = targets[:, :min_features]
                                                loss = criterion(outputs_fixed, targets_fixed)
                                                logger.info(f"Emergency fix: Used {min_features} features for loss")
                                            else:
                                                targets = targets.view_as(outputs)
                                                loss = criterion(outputs, targets)
                                                logger.info("Emergency fix: Forced target reshape")
                                        
                                        if 'targets_fixed' not in locals():
                                            loss = criterion(outputs, targets)
                                        
                                    except Exception as final_e:
                                        logger.error(f"Final loss computation attempt failed: {final_e}")
                                        if handle_nan_loss == 'skip':
                                            logger.warning(f"Skipping batch {batch_idx} due to unresolvable shape mismatch")
                                            continue
                                        elif handle_nan_loss == 'error':
                                            raise RuntimeError(f"Unresolvable shape mismatch in batch {batch_idx}") from loss_e
                                        else:
                                            # Create a dummy loss to continue training
                                            loss = torch.tensor(0.0, device=device, requires_grad=True)
                                            logger.warning(f"Using dummy loss for batch {batch_idx}")
                            
                            # Scale loss for gradient accumulation
                            loss = loss / gradient_accumulation_steps
                    
                    forward_end_time = time.time()
                    forward_time += (forward_end_time - forward_start_time)
                    
                    # Loss validation
                    if check_finite:
                        if not torch.isfinite(loss):
                            if handle_nan_loss == 'error':
                                raise ValueError(f"Non-finite loss detected in batch {batch_idx}: {loss.item()}")
                            elif handle_nan_loss == 'skip':
                                logger.warning(f"Skipping batch {batch_idx} due to non-finite loss: {loss.item()}")
                                continue
                            elif handle_nan_loss == 'clip':
                                loss = torch.nan_to_num(loss, nan=0.0, posinf=1e6, neginf=-1e6)
                    
                    # Loss spike detection
                    if loss_spike_detection and len(recent_losses) > 0:
                        current_loss = loss.item() * gradient_accumulation_steps
                        avg_recent_loss = sum(recent_losses) / len(recent_losses)
                        if current_loss > avg_recent_loss * loss_spike_threshold:
                            logger.warning(f"Loss spike detected: current={current_loss:.4f}, avg_recent={avg_recent_loss:.4f}")
                            if handle_nan_loss == 'skip':
                                continue
                        recent_losses.append(current_loss)
                    elif loss_spike_detection:
                        recent_losses.append(loss.item() * gradient_accumulation_steps)
                    
                    # Backward pass with timing
                    backward_start_time = time.time()
                    
                    if mixed_precision and scaler is not None:
                        scaler.scale(loss).backward()
                    else:
                        loss.backward()
                    
                    backward_end_time = time.time()
                    backward_time += (backward_end_time - backward_start_time)
                    
                    accumulated_steps += 1
                    
                    # Optimizer step with gradient accumulation
                    if accumulated_steps % gradient_accumulation_steps == 0:
                        optimizer_start_time = time.time()
                        
                        # Gradient clipping
                        if gradient_clip > 0 or max_grad_norm > 0:
                            if mixed_precision and scaler is not None:
                                scaler.unscale_(optimizer)
                            
                            if gradient_clipping_mode == 'norm':
                                grad_norm = torch.nn.utils.clip_grad_norm_(
                                    model.parameters(), 
                                    max_grad_norm or gradient_clip
                                )
                            elif gradient_clipping_mode == 'value':
                                grad_norm = torch.nn.utils.clip_grad_value_(
                                    model.parameters(), 
                                    gradient_clip
                                )
                                # Calculate norm for tracking
                                grad_norm = torch.sqrt(sum(param.grad.data.norm()**2 for param in model.parameters() if param.grad is not None))
                            else:
                                grad_norm = torch.sqrt(sum(param.grad.data.norm()**2 for param in model.parameters() if param.grad is not None))
                        else:
                            grad_norm = torch.sqrt(sum(param.grad.data.norm()**2 for param in model.parameters() if param.grad is not None))
                        
                        # Gradient explosion detection
                        if gradient_explosion_detection and grad_norm > 100.0:
                            logger.warning(f"Large gradient norm detected: {grad_norm:.4f}")
                        
                        # Optimizer step
                        if mixed_precision and scaler is not None:
                            scaler.step(optimizer)
                            scaler.update()
                        else:
                            optimizer.step()
                        
                        # Scheduler step on batch if configured
                        if scheduler and scheduler_step_on_batch:
                            if hasattr(scheduler, 'step'):
                                try:
                                    if 'ReduceLROnPlateau' in str(type(scheduler)):
                                        # Don't step ReduceLROnPlateau on batch
                                        pass
                                    else:
                                        scheduler.step()
                                except Exception as e:
                                    logger.warning(f"Scheduler step failed: {e}")
                        
                        optimizer.zero_grad()
                        
                        optimizer_end_time = time.time()
                        optimizer_time += (optimizer_end_time - optimizer_start_time)
                        
                        # Store gradient norm for tracking
                        if track_metrics and 'gradient_norm' in metrics_to_track:
                            metrics_tracker['gradient_norms'].append(grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm)
                    
                    # Update running loss
                    total_loss += loss.item() * gradient_accumulation_steps
                    num_batches += 1
                    
                    # Store metrics
                    if track_metrics:
                        if 'loss' in metrics_to_track:
                            metrics_tracker['losses'].append(loss.item() * gradient_accumulation_steps)
                        
                        if 'learning_rate' in metrics_to_track:
                            current_lr = optimizer.param_groups[0]['lr']
                            metrics_tracker['learning_rates'].append(current_lr)
                        
                        if 'batch_time' in metrics_to_track and batch_start_time:
                            batch_time = time.time() - batch_start_time
                            metrics_tracker['batch_times'].append(batch_time)
                        
                        # Memory usage tracking
                        if 'memory_usage' in metrics_to_track and torch.cuda.is_available():
                            memory_allocated = torch.cuda.memory_allocated(device) / 1024**2  # MB
                            metrics_tracker['memory_usage'].append(memory_allocated)
                    
                    # Memory management
                    if memory_efficient:
                        if batch_idx % empty_cache_frequency == 0 and torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        
                        if batch_idx % gc_collection_frequency == 0:
                            gc.collect()
                    
                    # Logging
                    if batch_idx % log_frequency == 0 and batch_idx > 0:
                        avg_loss = total_loss / num_batches
                        current_lr = optimizer.param_groups[0]['lr']
                        
                        if timing_analysis:
                            logger.info(
                                f"Batch {batch_idx}/{len(loader)} | "
                                f"Loss: {avg_loss:.6f} | "
                                f"LR: {current_lr:.2e} | "
                                f"Forward: {forward_time/batch_idx:.3f}s | "
                                f"Backward: {backward_time/batch_idx:.3f}s | "
                                f"Optimizer: {optimizer_time/(accumulated_steps//gradient_accumulation_steps):.3f}s"
                            )
                        else:
                            logger.info(
                                f"Batch {batch_idx}/{len(loader)} | "
                                f"Loss: {avg_loss:.6f} | "
                                f"LR: {current_lr:.2e}"
                            )
                    
                    # Update alive-progress bar
                    if pbar:
                        current_loss_display = loss.item() * gradient_accumulation_steps
                        avg_loss_display = total_loss / num_batches
                        current_lr_display = optimizer.param_groups[0]['lr']
                        
                        # Format the text for the progress bar
                        progress_text = (f"Loss: {current_loss_display:.4f}, Avg: {avg_loss_display:.4f}, LR: {current_lr_display:.2e}, Batch: {batch_idx+1}/{len(loader)}")
                        #progress_text = f"Loss: {current_loss_display:.4f}, Avg: {avg_loss_display:.4f}, LR: {current_lr_display:.2e}, Batch: {batch_idx+1}/{len(loader)}"
                        
                        # Update the progress bar text
                        pbar.text(progress_text)
                        
                        # Update the progress
                        pbar()
                    
                    # Post-batch callback
                    if post_batch_callback:
                        post_batch_callback(batch_idx, batch, model, optimizer, loss)
                    
                    # Profiler step
                    if profiler:
                        profiler.step()
                    
                    # Execute callbacks
                    for callback in callbacks:
                        try:
                            callback(
                                batch_idx=batch_idx,
                                loss=loss.item() * gradient_accumulation_steps,
                                model=model,
                                optimizer=optimizer,
                                metrics=metrics_tracker
                            )
                        except Exception as e:
                            logger.warning(f"Callback execution failed: {e}")
                    
                except Exception as batch_error:
                    logger.error(f"Error in batch {batch_idx}: {batch_error}")
                    if continue_on_error:
                        if error_handling == 'skip':
                            continue
                        elif error_handling == 'retry':
                            retry_count = 0
                            while retry_count < max_retries:
                                try:
                                    logger.info(f"Retrying batch {batch_idx}, attempt {retry_count + 1}")
                                    optimizer.zero_grad()
                                    break
                                except Exception as retry_e:
                                    retry_count += 1
                                    logger.warning(f"Retry {retry_count} failed: {retry_e}")
                            
                            if retry_count >= max_retries:
                                logger.error(f"Max retries exceeded for batch {batch_idx}")
                                if graceful_degradation:
                                    continue
                                else:
                                    raise
                        else:
                            # Add dummy values to continue
                            total_loss += 1.0
                            num_batches += 1
                            continue
                    else:
                        # Return partial results
                        avg_loss = total_loss / max(num_batches, 1)
                        partial_metrics = default_metrics.copy()
                        partial_metrics.update({
                            'loss': avg_loss,
                            'num_batches': num_batches,
                            'num_samples': num_samples,
                            'error': f"Batch {batch_idx} failed: {str(batch_error)}",
                            'partial_completion': True
                        })
                        return avg_loss, partial_metrics
        
        # Handle remaining gradients after main loop
        if accumulated_steps % gradient_accumulation_steps != 0:
            if mixed_precision and scaler is not None:
                if gradient_clip > 0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                scaler.step(optimizer)
                scaler.update()
            else:
                if gradient_clip > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                optimizer.step()
        
        # Scheduler step after epoch
        if scheduler and (scheduler_step_on_epoch or scheduler_step_after_epoch):
            try:
                if 'ReduceLROnPlateau' in str(type(scheduler)):
                    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
                    scheduler.step(avg_loss)
                else:
                    scheduler.step()
                logger.debug("Scheduler step completed")
            except Exception as e:
                logger.warning(f"Scheduler step failed: {e}")
        
        # Calculate final metrics
        epoch_end_time = time.time()
        total_epoch_time = epoch_end_time - epoch_start_time
        avg_loss = total_loss / num_batches if num_batches > 0 else default_loss
        
        # Metrics dictionary
        metrics = {
            'loss': avg_loss,
            'epoch': epoch if epoch is not None else 0,
            'num_batches': num_batches,
            'num_samples': num_samples,
            'total_epoch_time': total_epoch_time,
            'avg_batch_time': total_epoch_time / num_batches if num_batches > 0 else 0,
            'learning_rate': optimizer.param_groups[0]['lr'],
            'final_learning_rate': optimizer.param_groups[0]['lr'],
            'gradient_accumulation_steps': gradient_accumulation_steps,
            'effective_batch_size': batch_size * gradient_accumulation_steps,
            'mixed_precision_enabled': mixed_precision,
            'scaler_scale': scaler.get_scale() if scaler else None,
            'memory_allocated_mb': torch.cuda.memory_allocated(device) / 1024**2 if torch.cuda.is_available() else 0,
            'memory_reserved_mb': torch.cuda.memory_reserved(device) / 1024**2 if torch.cuda.is_available() else 0,
            'samples_per_second': num_samples / total_epoch_time if total_epoch_time > 0 else 0,
            'batches_per_second': num_batches / total_epoch_time if total_epoch_time > 0 else 0,
            'timing': {
                'forward_time': forward_time,
                'backward_time': backward_time,
                'optimizer_time': optimizer_time,
                'data_loading_time': data_loading_time,
                'avg_forward_time': forward_time / num_batches if num_batches > 0 else 0,
                'avg_backward_time': backward_time / num_batches if num_batches > 0 else 0,
                'avg_optimizer_time': optimizer_time / max(accumulated_steps // gradient_accumulation_steps, 1) if accumulated_steps > 0 else 0,
                'avg_data_loading_time': data_loading_time / num_batches if num_batches > 0 else 0
            },
            'config_applied': final_config,
            'device': str(device),
            'distributed_training': distributed,
            'training_stable': not any(loss > 1000 for loss in metrics_tracker['losses'][-10:]) if metrics_tracker['losses'] else True,
            'loss_spikes_detected': len([l for l in metrics_tracker['losses'] if l > avg_loss * loss_spike_threshold]) if loss_spike_detection else 0,
            'training_completed': True
        }
        
        # Add tracked metrics
        if track_metrics:
            if metrics_tracker['losses']:
                metrics['loss_std'] = float(np.std(metrics_tracker['losses']))
                metrics['min_loss'] = float(np.min(metrics_tracker['losses']))
                metrics['max_loss'] = float(np.max(metrics_tracker['losses']))
            
            if metrics_tracker['gradient_norms']:
                metrics['avg_gradient_norm'] = float(np.mean(metrics_tracker['gradient_norms']))
                metrics['max_gradient_norm'] = float(np.max(metrics_tracker['gradient_norms']))
                metrics['gradient_norm_std'] = float(np.std(metrics_tracker['gradient_norms']))
            
            if metrics_tracker['batch_times']:
                metrics['avg_batch_time'] = float(np.mean(metrics_tracker['batch_times']))
                metrics['batch_time_std'] = float(np.std(metrics_tracker['batch_times']))
            
            if metrics_tracker['memory_usage']:
                metrics['avg_memory_usage_mb'] = float(np.mean(metrics_tracker['memory_usage']))
                metrics['max_memory_usage_mb'] = float(np.max(metrics_tracker['memory_usage']))
        
        logger.info(f"Training epoch completed: loss={avg_loss:.6f}, batches={num_batches}")
        return avg_loss, metrics
        
    except Exception as e:
        logger.error(f"Training epoch failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Calculate what we can from available data
        try:
            final_loss = total_loss / max(num_batches, 1) if num_batches > 0 else default_loss
        except:
            final_loss = default_loss
        
        error_metrics = default_metrics.copy()
        error_metrics.update({
            'loss': final_loss,
            'num_batches': num_batches,
            'num_samples': num_samples,
            'error': str(e),
            'error_type': type(e).__name__,
            'training_failed': True
        })
        
        if graceful_degradation:
            logger.warning(f"Returning error results: {error_metrics}")
            return final_loss, error_metrics
        else:
            # Still return something to avoid unpacking errors
            return default_loss, error_metrics
    
    finally:
        # Restore original logging level
        # if verbose and original_level is not None:
        #     logger.setLevel(original_level)
        
        # Cleanup
        try:
            if pbar:
                # Close progress bars
                if hasattr(pbar, '__exit__'):
                    pbar.__exit__(None, None, None)
                else:
                    # Fallback for different alive_bar versions
                    pbar.close()
            
            if profiler:
                profiler.stop()
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        except:
            pass

def validate(
    # Core Validation Parameters
    model: Optional[nn.Module] = None,
    loader: Optional[DataLoader] = None,
    criterion: Optional[nn.Module] = None,
    device: Optional[torch.device] = None,
    epoch: Optional[int] = None,
    
    # Validation Configuration Parameters
    validation_type: Optional[str] = None,
    batch_size: Optional[int] = None,
    eval_mode: Optional[bool] = None,
    no_grad: Optional[bool] = None,
    inference_mode: Optional[bool] = None,
    
    # Mixed Precision Parameters
    mixed_precision: Optional[bool] = None,
    amp_enabled: Optional[bool] = None,
    scaler: Optional[GradScaler] = None,
    autocast_enabled: Optional[bool] = None,
    
    # Metrics and Evaluation Parameters
    calculate_metrics: Optional[bool] = None,
    metrics_to_calculate: Optional[List[str]] = None,
    detailed_metrics: Optional[bool] = None,
    per_sample_metrics: Optional[bool] = None,
    aggregate_metrics: Optional[bool] = None,
    statistical_metrics: Optional[bool] = None,
    distribution_analysis: Optional[bool] = None,
    anomaly_detection: Optional[bool] = None,
    threshold_analysis: Optional[bool] = None,
    
    # Performance and Optimization Parameters
    performance_mode: Optional[str] = None,
    benchmark_mode: Optional[bool] = None,
    memory_efficient: Optional[bool] = None,
    batch_processing: Optional[bool] = None,
    parallel_validation: Optional[bool] = None,
    
    # Data Processing Parameters
    data_preprocessing: Optional[bool] = None,
    input_transforms: Optional[List[Callable]] = None,
    target_transforms: Optional[List[Callable]] = None,
    normalize_outputs: Optional[bool] = None,
    denormalize_outputs: Optional[bool] = None,
    
    # Validation Quality Control Parameters
    validate_inputs: Optional[bool] = None,
    check_finite: Optional[bool] = None,
    handle_nan_outputs: Optional[str] = None,
    handle_inf_outputs: Optional[str] = None,
    output_validation: Optional[bool] = None,
    consistency_checks: Optional[bool] = None,
    
    # Reconstruction and Anomaly Parameters
    reconstruction_threshold: Optional[float] = None,
    anomaly_threshold: Optional[float] = None,
    percentile_threshold: Optional[float] = None,
    adaptive_threshold: Optional[bool] = None,
    threshold_method: Optional[str] = None,
    contamination_rate: Optional[float] = None,
    
    # Statistical Analysis Parameters
    confidence_interval: Optional[float] = None,
    significance_level: Optional[float] = None,
    statistical_tests: Optional[bool] = None,
    distribution_tests: Optional[bool] = None,
    normality_tests: Optional[bool] = None,
    outlier_analysis: Optional[bool] = None,
    
    # Monitoring and Logging Parameters
    progress_bar: Optional[bool] = None,
    progress_bar_desc: Optional[str] = None,
    log_frequency: Optional[int] = None,
    verbose: Optional[bool] = None,
    debug_mode: Optional[bool] = None,
    timing_analysis: Optional[bool] = None,
    
    # Memory Management Parameters
    empty_cache_frequency: Optional[int] = None,
    gc_collection_frequency: Optional[int] = None,
    memory_monitoring: Optional[bool] = None,
    
    # Distributed Validation Parameters
    distributed: Optional[bool] = None,
    world_size: Optional[int] = None,
    rank: Optional[int] = None,
    reduce_metrics: Optional[bool] = None,
    
    # Export and Visualization Parameters
    save_results: Optional[bool] = None,
    results_path: Optional[str] = None,
    save_predictions: Optional[bool] = None,
    save_reconstructions: Optional[bool] = None,
    visualization: Optional[bool] = None,
    plot_distributions: Optional[bool] = None,
    plot_reconstructions: Optional[bool] = None,
    
    # Cross-validation Parameters
    cross_validation: Optional[bool] = None,
    cv_folds: Optional[int] = None,
    stratified_cv: Optional[bool] = None,
    cv_metrics: Optional[bool] = None,
    
    # Advanced Analysis Parameters
    feature_importance: Optional[bool] = None,
    attention_analysis: Optional[bool] = None,
    gradient_analysis: Optional[bool] = None,
    activation_analysis: Optional[bool] = None,
    layer_analysis: Optional[bool] = None,
    
    # Error Handling Parameters
    error_handling: Optional[str] = None,
    continue_on_error: Optional[bool] = None,
    max_retries: Optional[int] = None,
    graceful_degradation: Optional[bool] = None,
    fallback_mode: Optional[bool] = None,
    
    # Compatibility Parameters
    legacy_mode: Optional[bool] = None,
    backward_compatibility: Optional[bool] = None,
    version_check: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_features: Optional[bool] = None,
    experimental_metrics: Optional[bool] = None,
    beta_features: Optional[bool] = None,
    
    # Custom Functions Parameters
    custom_metric_fn: Optional[Callable] = None,
    custom_threshold_fn: Optional[Callable] = None,
    custom_analysis_fn: Optional[Callable] = None,
    validation_callbacks: Optional[List[Callable]] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    validation_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Tuple[float, np.ndarray, Dict[str, Any]]:
    
    # Start timing
    start_time = datetime.now()
    validation_start_time = time.time()
    
    # Initialize configuration with comprehensive defaults
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply validation-specific configuration
    if validation_config:
        config.setdefault('validation', {}).update(validation_config)
    
    # Apply all parameters to configuration
    final_config = {}
    
    # Merge with existing config
    final_config.update(config)
    
    # Apply individual parameters with intelligent organization
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'validation_config', 'kwargs', 'start_time', 'validation_start_time',
        'datetime', 'traceback', 'time', 'gc', 'warnings', 'defaultdict', 'deque',
        'nullcontext', 'nn', 'optim', 'DataLoader', 'GradScaler', 'autocast', 'stats'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Organize parameters into logical sections
    param_sections = {
        'core_validation': [
            'validation_type', 'batch_size', 'eval_mode', 'no_grad', 'inference_mode'
        ],
        'mixed_precision': [
            'mixed_precision', 'amp_enabled', 'scaler', 'autocast_enabled'
        ],
        'metrics_evaluation': [
            'calculate_metrics', 'metrics_to_calculate', 'detailed_metrics',
            'per_sample_metrics', 'aggregate_metrics', 'statistical_metrics',
            'distribution_analysis', 'anomaly_detection', 'threshold_analysis'
        ],
        'performance': [
            'performance_mode', 'benchmark_mode', 'memory_efficient',
            'batch_processing', 'parallel_validation'
        ],
        'data_processing': [
            'data_preprocessing', 'input_transforms', 'target_transforms',
            'normalize_outputs', 'denormalize_outputs'
        ],
        'validation_quality': [
            'validate_inputs', 'check_finite', 'handle_nan_outputs',
            'handle_inf_outputs', 'output_validation', 'consistency_checks'
        ],
        'reconstruction_anomaly': [
            'reconstruction_threshold', 'anomaly_threshold', 'percentile_threshold',
            'adaptive_threshold', 'threshold_method', 'contamination_rate'
        ],
        'statistical_analysis': [
            'confidence_interval', 'significance_level', 'statistical_tests',
            'distribution_tests', 'normality_tests', 'outlier_analysis'
        ],
        'monitoring_logging': [
            'progress_bar', 'progress_bar_desc', 'log_frequency',
            'verbose', 'debug_mode', 'timing_analysis'
        ],
        'memory_management': [
            'empty_cache_frequency', 'gc_collection_frequency', 'memory_monitoring'
        ],
        'distributed': [
            'distributed', 'world_size', 'rank', 'reduce_metrics'
        ],
        'export_visualization': [
            'save_results', 'results_path', 'save_predictions', 'save_reconstructions',
            'visualization', 'plot_distributions', 'plot_reconstructions'
        ],
        'cross_validation': [
            'cross_validation', 'cv_folds', 'stratified_cv', 'cv_metrics'
        ],
        'advanced_analysis': [
            'feature_importance', 'attention_analysis', 'gradient_analysis',
            'activation_analysis', 'layer_analysis'
        ],
        'error_handling': [
            'error_handling', 'continue_on_error', 'max_retries',
            'graceful_degradation', 'fallback_mode'
        ],
        'compatibility': [
            'legacy_mode', 'backward_compatibility', 'version_check'
        ],
        'experimental': [
            'experimental_features', 'experimental_metrics', 'beta_features'
        ],
        'custom_functions': [
            'custom_metric_fn', 'custom_threshold_fn', 'custom_analysis_fn',
            'validation_callbacks'
        ]
    }
    
    # Apply parameters to appropriate sections
    for section, param_list in param_sections.items():
        section_config = final_config.setdefault(section, {})
        for param in param_list:
            if param in cleaned_params:
                section_config[param] = cleaned_params[param]
    
    # Set up comprehensive defaults
    core_config = final_config.setdefault('core_validation', {})
    mixed_precision_config = final_config.setdefault('mixed_precision', {})
    metrics_config = final_config.setdefault('metrics_evaluation', {})
    performance_config = final_config.setdefault('performance', {})
    data_processing_config = final_config.setdefault('data_processing', {})
    validation_quality_config = final_config.setdefault('validation_quality', {})
    reconstruction_config = final_config.setdefault('reconstruction_anomaly', {})
    statistical_config = final_config.setdefault('statistical_analysis', {})
    monitoring_config = final_config.setdefault('monitoring_logging', {})
    memory_config = final_config.setdefault('memory_management', {})
    distributed_config = final_config.setdefault('distributed', {})
    export_config = final_config.setdefault('export_visualization', {})
    cv_config = final_config.setdefault('cross_validation', {})
    advanced_config = final_config.setdefault('advanced_analysis', {})
    error_config = final_config.setdefault('error_handling', {})
    experimental_config = final_config.setdefault('experimental', {})
    custom_config = final_config.setdefault('custom_functions', {})
    
    # Apply intelligent defaults with system awareness
    validation_type = core_config.setdefault('validation_type', 'standard')
    eval_mode = core_config.setdefault('eval_mode', True)
    no_grad = core_config.setdefault('no_grad', True)
    inference_mode = core_config.setdefault('inference_mode', False)
    
    # Mixed precision defaults
    mixed_precision = mixed_precision_config.setdefault('mixed_precision', MIXED_PRECISION and torch.cuda.is_available())
    amp_enabled = mixed_precision_config.setdefault('amp_enabled', mixed_precision)
    autocast_enabled = mixed_precision_config.setdefault('autocast_enabled', mixed_precision)
    
    # Metrics defaults
    calculate_metrics = metrics_config.setdefault('calculate_metrics', True)
    metrics_to_calculate = metrics_config.setdefault('metrics_to_calculate', ['loss', 'mse', 'mae', 'reconstruction_error'])
    detailed_metrics = metrics_config.setdefault('detailed_metrics', True)
    per_sample_metrics = metrics_config.setdefault('per_sample_metrics', True)
    aggregate_metrics = metrics_config.setdefault('aggregate_metrics', True)
    statistical_metrics = metrics_config.setdefault('statistical_metrics', True)
    distribution_analysis = metrics_config.setdefault('distribution_analysis', False)
    anomaly_detection = metrics_config.setdefault('anomaly_detection', validation_type == 'anomaly_detection')
    threshold_analysis = metrics_config.setdefault('threshold_analysis', anomaly_detection)
    
    # Performance defaults
    performance_mode = performance_config.setdefault('performance_mode', 'standard')
    benchmark_mode = performance_config.setdefault('benchmark_mode', False)
    memory_efficient = performance_config.setdefault('memory_efficient', True)
    batch_processing = performance_config.setdefault('batch_processing', True)
    
    # Validation quality defaults
    validate_inputs = validation_quality_config.setdefault('validate_inputs', True)
    check_finite = validation_quality_config.setdefault('check_finite', True)
    handle_nan_outputs = validation_quality_config.setdefault('handle_nan_outputs', 'error')
    handle_inf_outputs = validation_quality_config.setdefault('handle_inf_outputs', 'error')
    output_validation = validation_quality_config.setdefault('output_validation', True)
    consistency_checks = validation_quality_config.setdefault('consistency_checks', True)
    
    # Reconstruction and anomaly defaults
    percentile_threshold = reconstruction_config.setdefault('percentile_threshold', 95.0)
    adaptive_threshold = reconstruction_config.setdefault('adaptive_threshold', True)
    threshold_method = reconstruction_config.setdefault('threshold_method', 'percentile')
    contamination_rate = reconstruction_config.setdefault('contamination_rate', 0.1)
    
    # Statistical analysis defaults
    confidence_interval = statistical_config.setdefault('confidence_interval', 0.95)
    significance_level = statistical_config.setdefault('significance_level', 0.05)
    statistical_tests = statistical_config.setdefault('statistical_tests', False)
    distribution_tests = statistical_config.setdefault('distribution_tests', False)
    outlier_analysis = statistical_config.setdefault('outlier_analysis', False)
    
    # Monitoring defaults
    progress_bar = monitoring_config.setdefault('progress_bar', True)
    progress_bar_desc = monitoring_config.setdefault('progress_bar_desc', f"{epoch if epoch is not None else ''}")
    log_frequency = monitoring_config.setdefault('log_frequency', 100)
    verbose = monitoring_config.setdefault('verbose', False)
    debug_mode = monitoring_config.setdefault('debug_mode', False)
    timing_analysis = monitoring_config.setdefault('timing_analysis', False)
    
    # Memory management defaults
    empty_cache_frequency = memory_config.setdefault('empty_cache_frequency', 50)
    gc_collection_frequency = memory_config.setdefault('gc_collection_frequency', 200)
    memory_monitoring = memory_config.setdefault('memory_monitoring', torch.cuda.is_available())
    
    # Distributed defaults
    distributed = distributed_config.setdefault('distributed', False)
    reduce_metrics = distributed_config.setdefault('reduce_metrics', distributed)
    
    # Error handling defaults
    error_handling = error_config.setdefault('error_handling', 'strict')
    continue_on_error = error_config.setdefault('continue_on_error', False)
    max_retries = error_config.setdefault('max_retries', 3)
    graceful_degradation = error_config.setdefault('graceful_degradation', True)
    
    # Set up logging level
    if verbose:
        original_level = logger.level
        logger.setLevel(logging.INFO)
    
    logger.info(f"Starting comprehensive validation for epoch {epoch if epoch is not None else 'N/A'}")
    
    # Initialize variables for cleanup
    pbar = None
    
    try:
        # Parameter validation
        if model is None:
            raise ValueError("Model is required for validation")
        if loader is None:
            raise ValueError("DataLoader is required for validation")
        if criterion is None:
            raise ValueError("Loss criterion is required for validation")
        
        # Device configuration
        if device is None:
            device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')
        
        # Initialize validation statistics
        validation_stats = {
            'start_time': start_time.isoformat(),
            'epoch': epoch,
            'validation_type': validation_type,
            'config_applied': final_config,
            'device': str(device),
            'mixed_precision_enabled': mixed_precision,
            'total_batches': len(loader),
            'batch_size': getattr(loader, 'batch_size', 'unknown')
        }
        
        # Set model to appropriate mode
        if eval_mode:
            model.eval()
            logger.debug("Set model to eval mode")
        
        # Initialize metrics tracking
        metrics_tracker = {
            'losses': [],
            'batch_times': [],
            'memory_usage': [],
            'reconstruction_errors': [],
            'per_sample_mse': [],
            'per_sample_mae': [],
            'detailed_metrics': defaultdict(list) if detailed_metrics else None
        }
        
        # Initialize validation variables
        total_loss = 0.0
        num_batches = 0
        num_samples = 0
        all_reconstruction_errors = []
        all_predictions = []
        all_targets = []
        
        # Initialize batch processing variables
        validation_time = 0
        forward_time = 0
        metrics_time = 0
        
        # Set up alive-progress bar
        if progress_bar:
            try:
                pbar_context = alive_bar(
                    total=len(loader), 
                    title=f'Validation {progress_bar_desc}\t',
                    unit='batches',
                    bar='smooth',
                    spinner='dots',
                    stats=False,  # We'll handle stats manually for better control
                    monitor=True,
                    elapsed=True,
                    stats_end=False
                )
                #pbar.__enter__()  # Manually enter the context since we're not using 'with' statement
                pbar = pbar_context.__enter__()  # Get the actual progress bar iterator
            except ImportError:
                logger.warning("alive-progress not available, progress bar disabled")
                pbar = None
                pbar_context = None
            except Exception as e:
                logger.warning(f"Failed to initialize alive-progress bar: {e}")
                pbar = None
                pbar_context = None
        else:
            pbar = None
            pbar_context = None
        
        # Validation callbacks setup
        validation_callbacks = custom_config.get('validation_callbacks', [])
        
        logger.info(f"Starting validation with {len(loader)} batches")
        
        # Determine validation context
        if inference_mode and hasattr(torch, 'inference_mode'):
            validation_context = torch.inference_mode()
        elif no_grad:
            validation_context = torch.no_grad()
        else:
            validation_context = nullcontext()
        
        # Main validation loop
        with validation_context:
            for batch_idx, batch in enumerate(loader):
                batch_start_time = time.time()
                
                try:
                    # Update progress bar with current batch processing status
                    if pbar:
                        pbar.text = f"Processing batch {batch_idx+1}/{len(loader)}"
                    
                    # Move data to device
                    if isinstance(batch, (list, tuple)):
                        inputs = batch[0].to(device, non_blocking=True)
                        targets = batch[1].to(device, non_blocking=True) if len(batch) > 1 else inputs
                    else:
                        inputs = batch.to(device, non_blocking=True)
                        targets = inputs
                    
                    # Input validation
                    if validate_inputs:
                        if check_finite:
                            if not torch.isfinite(inputs).all():
                                if error_handling == 'strict':
                                    raise ValueError(f"Non-finite input values detected in batch {batch_idx}")
                                else:
                                    logger.warning(f"Skipping batch {batch_idx} due to non-finite inputs")
                                    continue
                    
                    # Apply input transforms if specified
                    if data_processing_config.get('input_transforms'):
                        for transform in data_processing_config['input_transforms']:
                            inputs = transform(inputs)
                    
                    current_batch_size = inputs.size(0)
                    num_samples += current_batch_size
                    
                    # Update progress bar with data loading status
                    if pbar:
                        pbar.text = f"Batch {batch_idx+1}/{len(loader)} | Data loaded | Processing..."
                    
                    # Forward pass with mixed precision and timing
                    forward_start_time = time.time()
                    
                    autocast_context = get_autocast_context(device, mixed_precision, autocast_enabled)
                    with autocast_context:
                        outputs = model(inputs)
                        
                        # CRITICAL: Ensure targets match outputs for loss calculation
                        if targets.shape != outputs.shape:
                            if hasattr(model, '__class__') and 'autoencoder' in model.__class__.__name__.lower():
                                # For autoencoder, target should be input
                                targets = inputs
                                logger.debug("Set targets = inputs for autoencoder validation")
                            else:
                                # Try to reshape targets to match outputs
                                if outputs.numel() == targets.numel():
                                    targets = targets.view_as(outputs)
                                    logger.debug(f"Reshaped targets from {targets.shape} to {outputs.shape}")
                                else:
                                    # Try to match batch size and feature dimensions
                                    batch_size = outputs.size(0)
                                    if len(outputs.shape) == 2:  # [batch_size, features]
                                        if targets.size(0) == batch_size:
                                            # Try to extract the correct number of features
                                            target_features = outputs.size(1)
                                            if targets.size(1) >= target_features:
                                                targets = targets[:, :target_features]
                                            else:
                                                # Pad or repeat features if needed
                                                targets = torch.cat([targets, targets[:, :target_features - targets.size(1)]], dim=1)
                                        else:
                                            # As last resort, use inputs as targets
                                            targets = inputs
                                            logger.warning(f"Using inputs as targets due to irreconcilable shape mismatch")
                                    else:
                                        # For other shapes, try to use inputs
                                        targets = inputs
                                        logger.warning(f"Complex shape mismatch, using inputs as targets")
                            
                            logger.debug(f"Fixed shape mismatch: outputs {outputs.shape}, targets {targets.shape}")
                        
                        # Calculate loss with proper error handling
                        try:
                            loss = criterion(outputs, targets)
                        except RuntimeError as loss_error:
                            logger.error(f"Loss calculation failed even after shape fixing: {loss_error}")
                            logger.error(f"Final shapes - outputs: {outputs.shape}, targets: {targets.shape}")
                            
                            # Final attempt: force targets to match outputs exactly
                            if 'autoencoder' in str(type(model)).lower():
                                targets = inputs
                            else:
                                targets = outputs.detach().clone()  # Use model output as target (will give 0 loss)
                            
                            try:
                                loss = criterion(outputs, targets)
                                logger.warning("Used fallback target matching for loss calculation")
                            except Exception as final_error:
                                logger.error(f"Final loss calculation attempt failed: {final_error}")
                                if handle_nan_outputs == 'skip':
                                    logger.warning(f"Skipping batch {batch_idx} due to unresolvable loss calculation")
                                    continue
                                elif graceful_degradation:
                                    # Create a dummy loss to continue
                                    loss = torch.tensor(0.0, device=device, requires_grad=True)
                                    logger.warning(f"Using dummy loss for batch {batch_idx}")
                                else:
                                    raise
                    
                    forward_end_time = time.time()
                    forward_time += (forward_end_time - forward_start_time)
                    
                    # Output validation
                    if output_validation:
                        if check_finite:
                            if not torch.isfinite(outputs).all():
                                if handle_nan_outputs == 'error':
                                    raise ValueError(f"Non-finite output values detected in batch {batch_idx}")
                                elif handle_nan_outputs == 'skip':
                                    logger.warning(f"Skipping batch {batch_idx} due to non-finite outputs")
                                    continue
                                elif handle_nan_outputs == 'replace':
                                    outputs = torch.nan_to_num(outputs, nan=0.0, posinf=1.0, neginf=0.0)
                    
                    # Update progress bar with forward pass completion
                    if pbar:
                        pbar.text = f"Batch {batch_idx+1}/{len(loader)} | Forward pass completed | Calculating metrics..."
                    
                    # Calculate metrics with timing
                    metrics_start_time = time.time()
                    
                    # Basic metrics
                    total_loss += loss.item()
                    num_batches += 1
                    
                    if per_sample_metrics:
                        # Calculate per-sample reconstruction error (MSE)
                        per_sample_mse = torch.mean((inputs - outputs)**2, dim=tuple(range(1, inputs.dim()))).cpu().numpy()
                        metrics_tracker['per_sample_mse'].extend(per_sample_mse)
                        all_reconstruction_errors.extend(per_sample_mse)
                        
                        # Calculate per-sample MAE
                        if 'mae' in metrics_to_calculate:
                            per_sample_mae = torch.mean(torch.abs(inputs - outputs), dim=tuple(range(1, inputs.dim()))).cpu().numpy()
                            metrics_tracker['per_sample_mae'].extend(per_sample_mae)
                    
                    # Store predictions and targets for advanced analysis
                    if advanced_config.get('feature_importance', False) or export_config.get('save_predictions', False):
                        all_predictions.append(outputs.cpu().numpy())
                        all_targets.append(targets.cpu().numpy())
                    
                    # Custom metric calculation
                    if custom_config.get('custom_metric_fn'):
                        try:
                            custom_metrics = custom_config['custom_metric_fn'](inputs, outputs, targets, loss)
                            if detailed_metrics and metrics_tracker['detailed_metrics'] is not None:
                                for metric_name, metric_value in custom_metrics.items():
                                    metrics_tracker['detailed_metrics'][metric_name].append(metric_value)
                        except Exception as e:
                            logger.warning(f"Custom metric calculation failed: {e}")
                    
                    # Detailed metrics calculation
                    if detailed_metrics and batch_idx % 10 == 0:
                        # Statistical metrics
                        if statistical_metrics:
                            reconstruction_error = torch.mean((inputs - outputs)**2)
                            reconstruction_std = torch.std((inputs - outputs)**2)
                            
                            if metrics_tracker['detailed_metrics'] is not None:
                                metrics_tracker['detailed_metrics']['reconstruction_mean'].append(reconstruction_error.item())
                                metrics_tracker['detailed_metrics']['reconstruction_std'].append(reconstruction_std.item())
                        
                        # Distribution analysis
                        if distribution_analysis:
                            input_mean = torch.mean(inputs).item()
                            output_mean = torch.mean(outputs).item()
                            input_std = torch.std(inputs).item()
                            output_std = torch.std(outputs).item()
                            
                            if metrics_tracker['detailed_metrics'] is not None:
                                metrics_tracker['detailed_metrics']['input_mean'].append(input_mean)
                                metrics_tracker['detailed_metrics']['output_mean'].append(output_mean)
                                metrics_tracker['detailed_metrics']['input_std'].append(input_std)
                                metrics_tracker['detailed_metrics']['output_std'].append(output_std)
                    
                    metrics_end_time = time.time()
                    metrics_time += (metrics_end_time - metrics_start_time)
                    
                    # Store batch metrics
                    if calculate_metrics:
                        batch_time = time.time() - batch_start_time
                        metrics_tracker['losses'].append(loss.item())
                        metrics_tracker['batch_times'].append(batch_time)
                        
                        # Memory usage tracking
                        if memory_monitoring and torch.cuda.is_available():
                            memory_allocated = torch.cuda.memory_allocated(device) / 1024**2  # MB
                            metrics_tracker['memory_usage'].append(memory_allocated)
                    
                    # Memory management
                    if memory_efficient:
                        if batch_idx % empty_cache_frequency == 0 and torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        
                        if batch_idx % gc_collection_frequency == 0:
                            gc.collect()
                    
                    # Update progress bar with comprehensive metrics
                    if pbar:
                        current_loss_display = loss.item()
                        avg_loss_display = total_loss / num_batches
                        
                        # Format the text for the progress bar with detailed status
                        progress_text = (
                            f"Batch {batch_idx+1}/{len(loader)} | "
                            f"Loss: {current_loss_display:.4f} | "
                            f"Avg: {avg_loss_display:.4f} | "
                            f"Samples: {num_samples:,} | "
                            f"Memory: {torch.cuda.memory_allocated(device)/1024**2:.0f}MB"
                        )
                        
                        # Update the progress bar text
                        pbar.text = progress_text
                        
                        # Update the progress
                        pbar()
                    
                    # Logging
                    if batch_idx % log_frequency == 0 and batch_idx > 0:
                        avg_loss = total_loss / num_batches
                        
                        if timing_analysis:
                            logger.info(
                                f"Validation batch {batch_idx}/{len(loader)} | "
                                f"Loss: {avg_loss:.6f} | "
                                f"Forward: {forward_time/batch_idx:.3f}s | "
                                f"Metrics: {metrics_time/batch_idx:.3f}s"
                            )
                        else:
                            logger.info(
                                f"Validation batch {batch_idx}/{len(loader)} | "
                                f"Loss: {avg_loss:.6f}"
                            )
                    
                    # Execute validation callbacks
                    for callback in validation_callbacks:
                        try:
                            callback(
                                batch_idx=batch_idx,
                                inputs=inputs,
                                outputs=outputs,
                                loss=loss.item(),
                                model=model,
                                metrics=metrics_tracker
                            )
                        except Exception as e:
                            logger.warning(f"Validation callback execution failed: {e}")
                    
                except Exception as e:
                    if continue_on_error:
                        logger.error(f"Error in validation batch {batch_idx}: {e}")
                        if error_handling == 'skip':
                            continue
                        elif error_handling == 'retry':
                            retry_count = 0
                            while retry_count < max_retries:
                                try:
                                    logger.info(f"Retrying validation batch {batch_idx}, attempt {retry_count + 1}")
                                    # Re-process the batch (simplified retry logic)
                                    break
                                except Exception as retry_e:
                                    retry_count += 1
                                    logger.warning(f"Retry {retry_count} failed: {retry_e}")
                            
                            if retry_count >= max_retries:
                                logger.error(f"Max retries exceeded for batch {batch_idx}")
                                if graceful_degradation:
                                    continue
                                else:
                                    raise
                    else:
                        raise RuntimeError(f"Validation failed on batch {batch_idx}: {e}") from e
        
        # Update progress bar for post-processing phase
        if pbar:
            pbar.text = "Validation completed | Calculating final metrics..."
        
        # Calculate final metrics
        validation_end_time = time.time()
        total_validation_time = validation_end_time - validation_start_time
        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
        
        # Convert reconstruction errors to numpy array
        reconstruction_errors_array = np.array(all_reconstruction_errors)
        
        # Update progress bar for anomaly detection
        if pbar and (anomaly_detection or threshold_analysis):
            pbar.text = "Validation completed | Performing anomaly detection..."
        
        # Anomaly detection and threshold analysis
        anomaly_results = {}
        if anomaly_detection or threshold_analysis:
            logger.info("Performing anomaly detection and threshold analysis")
            
            if len(reconstruction_errors_array) > 0:
                # Calculate threshold based on method
                if threshold_method == 'percentile':
                    threshold = np.percentile(reconstruction_errors_array, percentile_threshold)
                elif threshold_method == 'mean_std':
                    mean_error = np.mean(reconstruction_errors_array)
                    std_error = np.std(reconstruction_errors_array)
                    threshold = mean_error + 2 * std_error
                elif threshold_method == 'iqr':
                    Q1 = np.percentile(reconstruction_errors_array, 25)
                    Q3 = np.percentile(reconstruction_errors_array, 75)
                    IQR = Q3 - Q1
                    threshold = Q3 + 1.5 * IQR
                elif custom_config.get('custom_threshold_fn'):
                    threshold = custom_config['custom_threshold_fn'](reconstruction_errors_array)
                else:
                    threshold = np.percentile(reconstruction_errors_array, percentile_threshold)
                
                # Anomaly detection results
                anomalies = reconstruction_errors_array > threshold
                n_anomalies = np.sum(anomalies)
                anomaly_rate = n_anomalies / len(reconstruction_errors_array) if len(reconstruction_errors_array) > 0 else 0
                
                anomaly_results = {
                    'threshold': float(threshold),
                    'threshold_method': threshold_method,
                    'n_anomalies': int(n_anomalies),
                    'anomaly_rate': float(anomaly_rate),
                    'anomaly_indices': np.where(anomalies)[0].tolist() if export_config.get('save_predictions', False) else []
                }
            else:
                logger.warning("No reconstruction errors available for anomaly detection")
        
        # Update progress bar for statistical analysis
        if pbar and statistical_metrics:
            pbar.text = "Validation completed | Performing statistical analysis..."
        
        # Statistical analysis
        statistical_results = {}
        if statistical_metrics and len(reconstruction_errors_array) > 0:
            logger.info("Performing statistical analysis")
            
            # Basic statistics
            statistical_results.update({
                'mean_reconstruction_error': float(np.mean(reconstruction_errors_array)),
                'std_reconstruction_error': float(np.std(reconstruction_errors_array)),
                'min_reconstruction_error': float(np.min(reconstruction_errors_array)),
                'max_reconstruction_error': float(np.max(reconstruction_errors_array)),
                'median_reconstruction_error': float(np.median(reconstruction_errors_array)),
                'q25_reconstruction_error': float(np.percentile(reconstruction_errors_array, 25)),
                'q75_reconstruction_error': float(np.percentile(reconstruction_errors_array, 75))
            })
            
            # Advanced statistical analysis
            if statistical_tests and len(reconstruction_errors_array) > 10:
                try:
                    # Normality test
                    normality_stat, normality_p = stats.normaltest(reconstruction_errors_array)
                    statistical_results.update({
                        'normality_test_statistic': float(normality_stat),
                        'normality_test_pvalue': float(normality_p),
                        'is_normal': normality_p > significance_level
                    })
                    
                    # Skewness and kurtosis
                    statistical_results.update({
                        'skewness': float(stats.skew(reconstruction_errors_array)),
                        'kurtosis': float(stats.kurtosis(reconstruction_errors_array))
                    })
                    
                except Exception as e:
                    logger.warning(f"Statistical tests failed: {e}")
            
            # Confidence interval
            if confidence_interval < 1.0:
                alpha = 1 - confidence_interval
                ci_lower = np.percentile(reconstruction_errors_array, 100 * alpha / 2)
                ci_upper = np.percentile(reconstruction_errors_array, 100 * (1 - alpha / 2))
                statistical_results.update({
                    f'ci_{confidence_interval}_lower': float(ci_lower),
                    f'ci_{confidence_interval}_upper': float(ci_upper)
                })
        
        # Outlier analysis
        outlier_results = {}
        if outlier_analysis and len(reconstruction_errors_array) > 0:
            logger.info("Performing outlier analysis")
            
            # IQR-based outliers
            Q1 = np.percentile(reconstruction_errors_array, 25)
            Q3 = np.percentile(reconstruction_errors_array, 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            iqr_outliers = (reconstruction_errors_array < lower_bound) | (reconstruction_errors_array > upper_bound)
            n_iqr_outliers = np.sum(iqr_outliers)
            
            outlier_results.update({
                'iqr_outliers': int(n_iqr_outliers),
                'iqr_outlier_rate': float(n_iqr_outliers / len(reconstruction_errors_array)),
                'iqr_lower_bound': float(lower_bound),
                'iqr_upper_bound': float(upper_bound)
            })
        
        # Distributed metrics reduction
        if distributed and reduce_metrics:
            try:
                # Reduce loss across processes
                loss_tensor = torch.tensor(avg_loss, device=device)
                dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
                avg_loss = (loss_tensor / distributed_config.get('world_size', 1)).item()
                
                # Reduce sample count
                samples_tensor = torch.tensor(num_samples, device=device)
                dist.all_reduce(samples_tensor, op=dist.ReduceOp.SUM)
                num_samples = samples_tensor.item()
                
                logger.info("Reduced metrics across distributed processes")
                
            except Exception as e:
                logger.warning(f"Failed to reduce distributed metrics: {e}")
        
        # Update progress bar for custom analysis
        if pbar and custom_config.get('custom_analysis_fn'):
            pbar.text("Validation completed | Performing custom analysis...")
            pbar.text = "Validation completed | Performing custom analysis..."
        
        # Custom analysis
        custom_analysis_results = {}
        if custom_config.get('custom_analysis_fn'):
            try:
                custom_analysis_results = custom_config['custom_analysis_fn'](
                    model=model,
                    predictions=np.concatenate(all_predictions) if all_predictions else None,
                    targets=np.concatenate(all_targets) if all_targets else None,
                    reconstruction_errors=reconstruction_errors_array,
                    metrics_tracker=metrics_tracker
                )
                logger.info("Completed custom analysis")
            except Exception as e:
                logger.warning(f"Custom analysis failed: {e}")
        
        # Update progress bar for finalization
        if pbar:
            pbar.text = "Validation completed | Finalizing results..."
        
        # Comprehensive metrics dictionary
        comprehensive_metrics = {
            # Core metrics
            'loss': avg_loss,
            'epoch': epoch,
            'validation_type': validation_type,
            'num_batches': num_batches,
            'num_samples': num_samples,
            'total_validation_time': total_validation_time,
            'avg_batch_time': total_validation_time / num_batches if num_batches > 0 else 0,
            
            # Basic reconstruction metrics
            'mean_mse': float(np.mean(reconstruction_errors_array)) if len(reconstruction_errors_array) > 0 else 0.0,
            'std_mse': float(np.std(reconstruction_errors_array)) if len(reconstruction_errors_array) > 0 else 0.0,
            'min_mse': float(np.min(reconstruction_errors_array)) if len(reconstruction_errors_array) > 0 else 0.0,
            'max_mse': float(np.max(reconstruction_errors_array)) if len(reconstruction_errors_array) > 0 else 0.0,
            'samples_validated': len(reconstruction_errors_array),
            
            # Mixed precision information
            'mixed_precision_enabled': mixed_precision,
            'autocast_enabled': autocast_enabled,
            
            # Memory information
            'memory_allocated_mb': torch.cuda.memory_allocated(device) / 1024**2 if torch.cuda.is_available() else 0,
            'memory_reserved_mb': torch.cuda.memory_reserved(device) / 1024**2 if torch.cuda.is_available() else 0,
            
            # Performance metrics
            'samples_per_second': num_samples / total_validation_time if total_validation_time > 0 else 0,
            'batches_per_second': num_batches / total_validation_time if total_validation_time > 0 else 0,
            
            # Timing breakdown
            'timing': {
                'forward_time': forward_time,
                'metrics_time': metrics_time,
                'total_time': total_validation_time,
                'avg_forward_time': forward_time / num_batches if num_batches > 0 else 0,
                'avg_metrics_time': metrics_time / num_batches if num_batches > 0 else 0
            },
            
            # Configuration info
            'config_applied': final_config,
            'device': str(device),
            'distributed_validation': distributed,
            
            # Quality metrics
            'validation_stable': not any(loss > 1000 for loss in metrics_tracker['losses'][-10:]) if metrics_tracker['losses'] else True
        }
        
        # Add tracked metrics
        if calculate_metrics:
            if metrics_tracker['losses']:
                comprehensive_metrics.update({
                    'loss_std': float(np.std(metrics_tracker['losses'])),
                    'min_batch_loss': float(np.min(metrics_tracker['losses'])),
                    'max_batch_loss': float(np.max(metrics_tracker['losses']))
                })
            
            if metrics_tracker['batch_times']:
                comprehensive_metrics.update({
                    'avg_batch_time': float(np.mean(metrics_tracker['batch_times'])),
                    'batch_time_std': float(np.std(metrics_tracker['batch_times']))
                })
            
            if metrics_tracker['memory_usage']:
                comprehensive_metrics.update({
                    'avg_memory_usage_mb': float(np.mean(metrics_tracker['memory_usage'])),
                    'max_memory_usage_mb': float(np.max(metrics_tracker['memory_usage']))
                })
        
        # Add detailed metrics
        if detailed_metrics and metrics_tracker['detailed_metrics']:
            comprehensive_metrics['detailed_metrics'] = {}
            for metric_name, values in metrics_tracker['detailed_metrics'].items():
                if values:
                    comprehensive_metrics['detailed_metrics'][metric_name] = {
                        'mean': float(np.mean(values)),
                        'std': float(np.std(values)),
                        'min': float(np.min(values)),
                        'max': float(np.max(values))
                    }
        
        # Add anomaly detection results
        if anomaly_results:
            comprehensive_metrics['anomaly_detection'] = anomaly_results
        
        # Add statistical results
        if statistical_results:
            comprehensive_metrics['statistics'] = statistical_results
        
        # Add outlier results
        if outlier_results:
            comprehensive_metrics['outliers'] = outlier_results
        
        # Add custom analysis results
        if custom_analysis_results:
            comprehensive_metrics['custom_analysis'] = custom_analysis_results
        
        # Save results if requested
        if export_config.get('save_results', False):
            results_path = export_config.get('results_path', f'./validation_results_epoch_{epoch}.json')
            try:
                with open(results_path, 'w') as f:
                    # Make metrics JSON serializable
                    serializable_metrics = {}
                    for key, value in comprehensive_metrics.items():
                        try:
                            json.dumps(value)
                            serializable_metrics[key] = value
                        except TypeError:
                            serializable_metrics[key] = str(value)
                    
                    json.dump(serializable_metrics, f, indent=2)
                logger.info(f"Saved validation results to {results_path}")
            except Exception as e:
                logger.warning(f"Failed to save validation results: {e}")
        
        # Save predictions if requested
        if export_config.get('save_predictions', False) and all_predictions:
            predictions_path = export_config.get('results_path', './').replace('.json', '_predictions.npy')
            try:
                np.save(predictions_path, np.concatenate(all_predictions))
                logger.info(f"Saved predictions to {predictions_path}")
            except Exception as e:
                logger.warning(f"Failed to save predictions: {e}")
        
        # Log comprehensive summary
        logger.info("=" * 80)
        logger.info(f"VALIDATION EPOCH {epoch if epoch is not None else 'N/A'} SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Validation Type: {validation_type}")
        logger.info(f"Average Loss: {avg_loss:.6f}")
        logger.info(f"Batches Processed: {num_batches:,}")
        logger.info(f"Samples Processed: {num_samples:,}")
        logger.info(f"Total Time: {total_validation_time:.2f}s")
        logger.info(f"Throughput: {comprehensive_metrics['samples_per_second']:.1f} samples/sec")
        
        if len(reconstruction_errors_array) > 0:
            logger.info(f"Mean Reconstruction Error: {comprehensive_metrics['mean_mse']:.6f}")
            logger.info(f"Std Reconstruction Error: {comprehensive_metrics['std_mse']:.6f}")
        
        if mixed_precision:
            logger.info(f"Mixed Precision: Enabled")
        
        if torch.cuda.is_available():
            logger.info(f"Memory: {comprehensive_metrics['memory_allocated_mb']:.1f}MB allocated")
        
        if anomaly_results:
            logger.info(f"Anomalies Detected: {anomaly_results['n_anomalies']} ({anomaly_results['anomaly_rate']*100:.1f}%)")
            logger.info(f"Anomaly Threshold: {anomaly_results['threshold']:.6f}")
        
        if timing_analysis:
            timing = comprehensive_metrics['timing']
            logger.info(f"Timing - Forward: {timing['forward_time']:.2f}s")
            logger.info(f"Timing - Metrics: {timing['metrics_time']:.2f}s")
        
        logger.info("=" * 80)
        
        # Final progress bar update
        if pbar:
            pbar.text = f"Validation completed | Loss: {avg_loss:.4f} | Time: {total_validation_time:.1f}s"
        
        # Restore original logging level
        if verbose and 'original_level' in locals():
            logger.setLevel(original_level)
        
        return avg_loss, reconstruction_errors_array, comprehensive_metrics
        
    except Exception as e:
        # Restore original logging level on error
        if verbose and 'original_level' in locals():
            logger.setLevel(original_level)
        
        error_msg = f"Validation failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Provide helpful error context
        logger.error(f"Error occurred at epoch {epoch}, batch {batch_idx if 'batch_idx' in locals() else 'N/A'}")
        logger.error(f"Configuration used: {final_config}")
        
        # Attempt graceful recovery if enabled
        if graceful_degradation and 'num_batches' in locals() and num_batches > 0:
            logger.warning("Attempting graceful recovery with partial results")
            try:
                # Return partial results
                avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
                partial_errors = np.array(all_reconstruction_errors)
                partial_metrics = {
                    'loss': avg_loss,
                    'epoch': epoch,
                    'num_batches': num_batches,
                    'num_samples': num_samples,
                    'error': str(e),
                    'partial_results': True
                }
                
                logger.warning(f"Returning partial results from {num_batches} processed batches")
                return avg_loss, partial_errors, partial_metrics
                
            except Exception as recovery_e:
                logger.error(f"Graceful recovery also failed: {recovery_e}")
        
        raise RuntimeError(error_msg) from e
    
    finally:
        # Final cleanup
        try:
            if pbar_context:
                pbar_context.__exit__(None, None, None)
            
            if memory_efficient:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
            
            # Restore model to original training state if it was changed
            if eval_mode and hasattr(model, 'training') and not model.training:
                # Don't automatically restore to training mode - let caller decide
                pass
        except:
            pass

def calculate_threshold(
    # Core Threshold Calculation Parameters
    model: Optional[nn.Module] = None,
    loader: Optional[DataLoader] = None,
    data: Optional[Union[np.ndarray, torch.Tensor]] = None,
    reconstruction_errors: Optional[np.ndarray] = None,
    percentile: Optional[float] = None,
    device: Optional[torch.device] = None,
    
    # Threshold Method Parameters
    threshold_method: Optional[str] = None,
    threshold_strategy: Optional[str] = None,
    threshold_type: Optional[str] = None,
    adaptive_threshold: Optional[bool] = None,
    dynamic_threshold: Optional[bool] = None,
    multi_threshold: Optional[bool] = None,
    hierarchical_threshold: Optional[bool] = None,
    
    # Statistical Threshold Parameters
    statistical_method: Optional[str] = None,
    confidence_level: Optional[float] = None,
    significance_level: Optional[float] = None,
    z_score_threshold: Optional[float] = None,
    iqr_multiplier: Optional[float] = None,
    mad_multiplier: Optional[float] = None,
    std_multiplier: Optional[float] = None,
    contamination_rate: Optional[float] = None,
    
    # Percentile-based Parameters
    percentile_method: Optional[str] = None,
    percentile_range: Optional[Tuple[float, float]] = None,
    multi_percentile: Optional[List[float]] = None,
    percentile_interpolation: Optional[str] = None,
    robust_percentile: Optional[bool] = None,
    
    # Distribution-based Parameters
    distribution_type: Optional[str] = None,
    distribution_params: Optional[Dict[str, float]] = None,
    distribution_fit_method: Optional[str] = None,
    use_empirical_distribution: Optional[bool] = None,
    kernel_density_estimation: Optional[bool] = None,
    kde_bandwidth: Optional[Union[str, float]] = None,
    
    # Machine Learning Threshold Parameters
    ml_threshold_method: Optional[str] = None,
    isolation_forest_contamination: Optional[float] = None,
    local_outlier_factor_neighbors: Optional[int] = None,
    one_class_svm_nu: Optional[float] = None,
    one_class_svm_kernel: Optional[str] = None,
    elliptic_envelope_contamination: Optional[float] = None,
    
    # Cross-validation Parameters
    cross_validation_threshold: Optional[bool] = None,
    cv_folds: Optional[int] = None,
    cv_strategy: Optional[str] = None,
    threshold_stability_analysis: Optional[bool] = None,
    bootstrap_threshold: Optional[bool] = None,
    bootstrap_samples: Optional[int] = None,
    
    # Time Series Threshold Parameters
    temporal_threshold: Optional[bool] = None,
    window_size: Optional[int] = None,
    sliding_window: Optional[bool] = None,
    seasonal_adjustment: Optional[bool] = None,
    trend_adjustment: Optional[bool] = None,
    change_point_detection: Optional[bool] = None,
    
    # Multi-modal Threshold Parameters
    mixture_model_threshold: Optional[bool] = None,
    n_components: Optional[int] = None,
    mixture_type: Optional[str] = None,
    component_weights: Optional[List[float]] = None,
    
    # Validation and Quality Parameters
    validate_threshold: Optional[bool] = None,
    threshold_validation_data: Optional[np.ndarray] = None,
    quality_metrics: Optional[List[str]] = None,
    stability_check: Optional[bool] = None,
    sensitivity_analysis: Optional[bool] = None,
    robustness_test: Optional[bool] = None,
    
    # Performance Parameters
    performance_mode: Optional[str] = None,
    batch_processing: Optional[bool] = None,
    parallel_processing: Optional[bool] = None,
    n_jobs: Optional[int] = None,
    memory_efficient: Optional[bool] = None,
    cache_results: Optional[bool] = None,
    
    # Data Processing Parameters
    data_preprocessing: Optional[bool] = None,
    outlier_removal: Optional[bool] = None,
    outlier_method: Optional[str] = None,
    data_transformation: Optional[str] = None,
    normalization: Optional[str] = None,
    scaling_method: Optional[str] = None,
    
    # Model Evaluation Parameters
    eval_mode: Optional[bool] = None,
    no_grad: Optional[bool] = None,
    mixed_precision: Optional[bool] = None,
    batch_size: Optional[int] = None,
    model_ensemble: Optional[bool] = None,
    ensemble_method: Optional[str] = None,
    
    # Advanced Analysis Parameters
    feature_importance: Optional[bool] = None,
    feature_weights: Optional[np.ndarray] = None,
    weighted_threshold: Optional[bool] = None,
    dimension_reduction: Optional[bool] = None,
    dimension_reduction_method: Optional[str] = None,
    n_components_pca: Optional[int] = None,
    
    # Uncertainty Quantification Parameters
    uncertainty_estimation: Optional[bool] = None,
    confidence_intervals: Optional[bool] = None,
    prediction_intervals: Optional[bool] = None,
    bayesian_threshold: Optional[bool] = None,
    mcmc_samples: Optional[int] = None,
    
    # Multi-class Threshold Parameters
    multi_class_threshold: Optional[bool] = None,
    class_specific_thresholds: Optional[bool] = None,
    class_weights: Optional[Dict[str, float]] = None,
    threshold_per_class: Optional[Dict[str, float]] = None,
    
    # Cost-sensitive Parameters
    cost_sensitive_threshold: Optional[bool] = None,
    false_positive_cost: Optional[float] = None,
    false_negative_cost: Optional[float] = None,
    cost_matrix: Optional[np.ndarray] = None,
    business_objective: Optional[str] = None,
    
    # Monitoring and Logging Parameters
    verbose: Optional[bool] = None,
    debug_mode: Optional[bool] = None,
    log_level: Optional[str] = None,
    progress_bar: Optional[bool] = None,
    timing_analysis: Optional[bool] = None,
    memory_profiling: Optional[bool] = None,
    
    # Export and Visualization Parameters
    save_results: Optional[bool] = None,
    results_path: Optional[str] = None,
    save_threshold_analysis: Optional[bool] = None,
    visualization: Optional[bool] = None,
    plot_distribution: Optional[bool] = None,
    plot_threshold: Optional[bool] = None,
    plot_roc_curve: Optional[bool] = None,
    
    # Error Handling Parameters
    error_handling: Optional[str] = None,
    handle_edge_cases: Optional[bool] = None,
    min_samples_required: Optional[int] = None,
    fallback_threshold: Optional[float] = None,
    graceful_degradation: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_methods: Optional[bool] = None,
    deep_learning_threshold: Optional[bool] = None,
    autoencoder_threshold: Optional[bool] = None,
    gan_threshold: Optional[bool] = None,
    
    # System Parameters
    random_state: Optional[int] = None,
    reproducible: Optional[bool] = None,
    deterministic: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    threshold_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Tuple[Union[float, Dict[str, float]], Dict[str, Any]]:
    # Start timing
    start_time = datetime.now()
    
    # Initialize configuration with comprehensive defaults
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply threshold-specific configuration
    if threshold_config:
        config.setdefault('threshold', {}).update(threshold_config)
    
    # Apply all parameters to configuration
    final_config = {}
    
    # Merge with existing config
    final_config.update(config)
    
    # Apply individual parameters with intelligent organization
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'threshold_config', 'kwargs', 'start_time', 'datetime',
        'traceback', 'time', 'gc', 'warnings', 'defaultdict', 'deque',
        'nullcontext', 'nn', 'optim', 'DataLoader', 'stats', 'IsolationForest',
        'LocalOutlierFactor', 'OneClassSVM', 'EllipticEnvelope', 'GaussianMixture'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Organize parameters into logical sections
    param_sections = {
        'core_threshold': [
            'percentile', 'threshold_method', 'threshold_strategy', 'threshold_type',
            'adaptive_threshold', 'dynamic_threshold', 'multi_threshold', 'hierarchical_threshold'
        ],
        'statistical_methods': [
            'statistical_method', 'confidence_level', 'significance_level', 'z_score_threshold',
            'iqr_multiplier', 'mad_multiplier', 'std_multiplier', 'contamination_rate'
        ],
        'percentile_methods': [
            'percentile_method', 'percentile_range', 'multi_percentile',
            'percentile_interpolation', 'robust_percentile'
        ],
        'distribution_methods': [
            'distribution_type', 'distribution_params', 'distribution_fit_method',
            'use_empirical_distribution', 'kernel_density_estimation', 'kde_bandwidth'
        ],
        'ml_methods': [
            'ml_threshold_method', 'isolation_forest_contamination', 'local_outlier_factor_neighbors',
            'one_class_svm_nu', 'one_class_svm_kernel', 'elliptic_envelope_contamination'
        ],
        'cross_validation': [
            'cross_validation_threshold', 'cv_folds', 'cv_strategy',
            'threshold_stability_analysis', 'bootstrap_threshold', 'bootstrap_samples'
        ],
        'temporal': [
            'temporal_threshold', 'window_size', 'sliding_window',
            'seasonal_adjustment', 'trend_adjustment', 'change_point_detection'
        ],
        'multi_modal': [
            'mixture_model_threshold', 'n_components', 'mixture_type', 'component_weights'
        ],
        'validation': [
            'validate_threshold', 'threshold_validation_data', 'quality_metrics',
            'stability_check', 'sensitivity_analysis', 'robustness_test'
        ],
        'performance': [
            'performance_mode', 'batch_processing', 'parallel_processing',
            'n_jobs', 'memory_efficient', 'cache_results'
        ],
        'data_processing': [
            'data_preprocessing', 'outlier_removal', 'outlier_method',
            'data_transformation', 'normalization', 'scaling_method'
        ],
        'model_evaluation': [
            'eval_mode', 'no_grad', 'mixed_precision', 'batch_size',
            'model_ensemble', 'ensemble_method'
        ],
        'advanced_analysis': [
            'feature_importance', 'feature_weights', 'weighted_threshold',
            'dimension_reduction', 'dimension_reduction_method', 'n_components_pca'
        ],
        'uncertainty': [
            'uncertainty_estimation', 'confidence_intervals', 'prediction_intervals',
            'bayesian_threshold', 'mcmc_samples'
        ],
        'multi_class': [
            'multi_class_threshold', 'class_specific_thresholds', 'class_weights',
            'threshold_per_class'
        ],
        'cost_sensitive': [
            'cost_sensitive_threshold', 'false_positive_cost', 'false_negative_cost',
            'cost_matrix', 'business_objective'
        ],
        'monitoring': [
            'verbose', 'debug_mode', 'log_level', 'progress_bar',
            'timing_analysis', 'memory_profiling'
        ],
        'export': [
            'save_results', 'results_path', 'save_threshold_analysis',
            'visualization', 'plot_distribution', 'plot_threshold', 'plot_roc_curve'
        ],
        'error_handling': [
            'error_handling', 'handle_edge_cases', 'min_samples_required',
            'fallback_threshold', 'graceful_degradation'
        ],
        'experimental': [
            'experimental_methods', 'deep_learning_threshold', 'autoencoder_threshold',
            'gan_threshold'
        ],
        'system': [
            'random_state', 'reproducible', 'deterministic'
        ]
    }
    
    # Apply parameters to appropriate sections
    for section, param_list in param_sections.items():
        section_config = final_config.setdefault(section, {})
        for param in param_list:
            if param in cleaned_params:
                section_config[param] = cleaned_params[param]
    
    # Set up comprehensive defaults
    core_config = final_config.setdefault('core_threshold', {})
    statistical_config = final_config.setdefault('statistical_methods', {})
    percentile_config = final_config.setdefault('percentile_methods', {})
    distribution_config = final_config.setdefault('distribution_methods', {})
    ml_config = final_config.setdefault('ml_methods', {})
    validation_config = final_config.setdefault('validation', {})
    performance_config = final_config.setdefault('performance', {})
    data_processing_config = final_config.setdefault('data_processing', {})
    model_config = final_config.setdefault('model_evaluation', {})
    monitoring_config = final_config.setdefault('monitoring', {})
    error_config = final_config.setdefault('error_handling', {})
    system_config = final_config.setdefault('system', {})
    
    # Apply intelligent defaults
    percentile = core_config.setdefault('percentile', DEFAULT_PERCENTILE)
    threshold_method = core_config.setdefault('threshold_method', 'percentile')
    adaptive_threshold = core_config.setdefault('adaptive_threshold', True)
    contamination_rate = statistical_config.setdefault('contamination_rate', 0.1)
    
    # Statistical defaults
    confidence_level = statistical_config.setdefault('confidence_level', 0.95)
    z_score_threshold = statistical_config.setdefault('z_score_threshold', 2.0)
    iqr_multiplier = statistical_config.setdefault('iqr_multiplier', 1.5)
    std_multiplier = statistical_config.setdefault('std_multiplier', 2.0)
    
    # Performance defaults
    performance_mode = performance_config.setdefault('performance_mode', 'standard')
    batch_processing = performance_config.setdefault('batch_processing', True)
    memory_efficient = performance_config.setdefault('memory_efficient', True)
    n_jobs = performance_config.setdefault('n_jobs', -1)
    
    # Model evaluation defaults
    eval_mode = model_config.setdefault('eval_mode', True)
    no_grad = model_config.setdefault('no_grad', True)
    mixed_precision = model_config.setdefault('mixed_precision', torch.cuda.is_available())
    
    # Error handling defaults
    error_handling = error_config.setdefault('error_handling', 'strict')
    min_samples_required = error_config.setdefault('min_samples_required', 10)
    fallback_threshold = error_config.setdefault('fallback_threshold', 0.1)
    graceful_degradation = error_config.setdefault('graceful_degradation', True)
    
    # System defaults
    random_state = system_config.setdefault('random_state', 42)
    reproducible = system_config.setdefault('reproducible', True)
    
    # Monitoring defaults
    verbose = monitoring_config.setdefault('verbose', False)
    debug_mode = monitoring_config.setdefault('debug_mode', False)
    progress_bar = monitoring_config.setdefault('progress_bar', True)
    timing_analysis = monitoring_config.setdefault('timing_analysis', False)
    
    # Set up logging level
    # if verbose:
    #     original_level = logger.level
    #     logger.setLevel(logging.INFO)
    
    logger.info(f"Starting comprehensive threshold calculation using method: {threshold_method}")
    
    # Initialize variables for cleanup
    pbar = None
    
    try:
        # Initialize calculation statistics
        calculation_stats = {
            'start_time': start_time.isoformat(),
            'threshold_method': threshold_method,
            'config_applied': final_config
        }
        
        # Set random seed for reproducibility
        if reproducible:
            np.random.seed(random_state)
            if torch.cuda.is_available():
                torch.manual_seed(random_state)
        
        # Device configuration
        if device is None:
            if model is not None:
                device = next(model.parameters()).device
            else:
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Set up main progress bar for the entire threshold calculation process
        if progress_bar:
            try:
                pbar_context = alive_bar(
                    title='Threshold Calculation\t',
                    bar='smooth',
                    spinner='dots',
                    stats=False,
                    monitor=True,
                    elapsed=True,
                    stats_end=False
                )
                pbar = pbar_context.__enter__()  # Get the actual progress bar iterator
            except ImportError:
                logger.warning("alive-progress not available, progress bar disabled")
                pbar = None
                pbar_context = None
            except Exception as e:
                logger.warning(f"Failed to initialize alive-progress bar: {e}")
                pbar = None
                pbar_context = None
        
        # Data preparation and validation
        if pbar:
            pbar.text = "Preparing and validating input data..."
        logger.info("Preparing and validating input data")
        
        mse_values = None
        data_source = None
        
        # Extract reconstruction errors from various sources
        if reconstruction_errors is not None:
            mse_values = np.array(reconstruction_errors)
            data_source = "provided_reconstruction_errors"
            logger.debug(f"Using provided reconstruction errors: {len(mse_values)} samples")
            
        elif data is not None and model is not None:
            # Calculate reconstruction errors from data and model
            if pbar:
                pbar.text = "Calculating reconstruction errors from model and data..."
            logger.info("Calculating reconstruction errors from model and data")
            data_source = "calculated_from_model"
            
            if model_config.get('eval_mode', True):
                model.eval()
            
            mse_list = []
            
            # Convert data to tensor if needed
            if isinstance(data, np.ndarray):
                data_tensor = torch.tensor(data, dtype=torch.float32)
            else:
                data_tensor = data
            
            # Batch processing for memory efficiency
            if batch_processing:
                batch_size = model_config.get('batch_size', 32)
                n_samples = len(data_tensor)
                total_batches = n_samples // batch_size + 1
                
                context = torch.no_grad() if no_grad else nullcontext()
                
                with context:
                    for i in range(0, n_samples, batch_size):
                        current_batch = i // batch_size + 1
                        
                        # Update progress bar with batch processing status
                        if pbar:
                            progress_text = (
                                f"Processing batch {current_batch}/{total_batches} | "
                                f"Samples: {len(mse_list):,} | "
                                f"Avg MSE: {np.mean(mse_list) if mse_list else 0:.4f}"
                            )
                            pbar.text = progress_text
                        
                        batch_data = data_tensor[i:i+batch_size].to(device)
                        
                        # Mixed precision inference
                        autocast_context = get_autocast_context(device, mixed_precision, True)
                        with autocast_context:
                            outputs = model(batch_data)
                        
                        # Calculate MSE per sample with proper shape handling
                        if outputs.shape != batch_data.shape:
                            if outputs.numel() == batch_data.numel():
                                outputs = outputs.view_as(batch_data)
                            else:
                                # For autoencoders, ensure output matches input
                                min_features = min(outputs.size(-1), batch_data.size(-1))
                                batch_data_adjusted = batch_data[..., :min_features]
                                outputs_adjusted = outputs[..., :min_features]
                                outputs = outputs_adjusted
                                batch_data = batch_data_adjusted
                                batch_mse = torch.mean((batch_data - outputs)**2, dim=tuple(range(1, batch_data.dim()))).cpu().numpy()
                            if 'batch_mse' not in locals():
                                batch_mse = torch.mean((batch_data - outputs)**2, dim=tuple(range(1, batch_data.dim()))).cpu().numpy()
                        else:
                            batch_mse = torch.mean((batch_data - outputs)**2, dim=tuple(range(1, batch_data.dim()))).cpu().numpy()
                        mse_list.extend(batch_mse)
                        
                        # Memory management
                        if memory_efficient and torch.cuda.is_available():
                            torch.cuda.empty_cache()
                
                mse_values = np.array(mse_list)
                
            else:
                # Process all data at once
                data_tensor = data_tensor.to(device)
                
                context = torch.no_grad() if no_grad else nullcontext()
                
                with context:
                    autocast_context = get_autocast_context(device, mixed_precision, True)
                    with autocast_context:
                        outputs = model(data_tensor)
                    
                    # Calculate MSE per sample with proper shape handling
                    if data_tensor.shape != outputs.shape:
                        if outputs.numel() == data_tensor.numel():
                            outputs = outputs.view_as(data_tensor)
                        else:
                            min_features = min(outputs.size(-1), data_tensor.size(-1))
                            outputs_adjusted = outputs[..., :min_features]
                            outputs = outputs_adjusted
                            data_tensor_adjusted = data_tensor[..., :min_features]
                            data_tensor = data_tensor_adjusted
                            batch_mse = torch.mean((data_tensor - outputs)**2, dim=tuple(range(1, data_tensor.dim()))).cpu().numpy()
                        if 'batch_mse' not in locals():
                            batch_mse = torch.mean((data_tensor - outputs)**2, dim=tuple(range(1, data_tensor.dim()))).cpu().numpy()
                    else:
                        batch_mse = torch.mean((data_tensor - outputs)**2, dim=tuple(range(1, data_tensor.dim()))).cpu().numpy()
                    mse_list.extend(batch_mse)
                    
                    if memory_efficient and torch.cuda.is_available():
                        torch.cuda.empty_cache
                    
                    if 'mse_list' in locals():
                        mse_values = np.array(mse_list)
                    else:
                        mse_values = batch_mse
            
        elif loader is not None and model is not None:
            # Calculate reconstruction errors from DataLoader
            if pbar:
                pbar.text = "Calculating reconstruction errors from DataLoader..."
            logger.info("Calculating reconstruction errors from DataLoader")
            data_source = "calculated_from_dataloader"
            
            if model_config.get('eval_mode', True):
                model.eval()
            
            mse_list = []
            context = torch.no_grad() if no_grad else nullcontext()
            
            with context:
                for batch_idx, batch in enumerate(loader):
                    # Update progress bar with DataLoader processing status
                    if pbar:
                        progress_text = (
                            f"Processing batch {batch_idx+1}/{len(loader)} | "
                            f"Samples: {len(mse_list):,} | "
                            f"Avg MSE: {np.mean(mse_list) if mse_list else 0:.4f}"
                        )
                        pbar.text = progress_text
                    
                    if isinstance(batch, (list, tuple)):
                        inputs = batch[0].to(device)
                    else:
                        inputs = batch.to(device)
                    
                    autocast_context = get_autocast_context(device, mixed_precision, True)
                    with autocast_context:
                        outputs = model(inputs)
                    
                    batch_mse = torch.mean((inputs - outputs)**2, dim=tuple(range(1, inputs.dim()))).cpu().numpy()
                    mse_list.extend(batch_mse)
                    
                    # Memory management
                    if memory_efficient and torch.cuda.is_available():
                        torch.cuda.empty_cache()
            
            mse_values = np.array(mse_list)
            
        else:
            raise ValueError("Must provide either reconstruction_errors, or (model and data), or (model and loader)")
        
        # Validate data
        if mse_values is None or len(mse_values) == 0:
            raise ValueError("No reconstruction errors available for threshold calculation")
        
        if len(mse_values) < min_samples_required:
            if graceful_degradation:
                logger.warning(f"Only {len(mse_values)} samples available, less than minimum {min_samples_required}")
                logger.warning(f"Using fallback threshold: {fallback_threshold}")
                return fallback_threshold, {'method': 'fallback', 'samples': len(mse_values)}
            else:
                raise ValueError(f"Insufficient samples: {len(mse_values)}, minimum required: {min_samples_required}")
        
        calculation_stats['n_samples'] = len(mse_values)
        calculation_stats['data_source'] = data_source
        
        # Data preprocessing if requested
        if data_processing_config.get('data_preprocessing', False):
            if pbar:
                pbar.text = "Applying data preprocessing..."
            logger.info("Applying data preprocessing")
            
            # Remove outliers if requested
            if data_processing_config.get('outlier_removal', False):
                outlier_method = data_processing_config.get('outlier_method', 'iqr')
                original_size = len(mse_values)
                
                if outlier_method == 'iqr':
                    Q1 = np.percentile(mse_values, 25)
                    Q3 = np.percentile(mse_values, 75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    mse_values = mse_values[(mse_values >= lower_bound) & (mse_values <= upper_bound)]
                elif outlier_method == 'zscore':
                    z_scores = np.abs(stats.zscore(mse_values))
                    mse_values = mse_values[z_scores < 3]
                
                logger.info(f"Outlier removal: {original_size} -> {len(mse_values)} samples")
            
            # Apply data transformation
            transformation = data_processing_config.get('data_transformation')
            if transformation == 'log':
                mse_values = np.log(mse_values + 1e-8)
            elif transformation == 'sqrt':
                mse_values = np.sqrt(mse_values)
            elif transformation == 'box_cox':
                try:
                    mse_values, lambda_param = stats.boxcox(mse_values + 1e-8)
                    calculation_stats['box_cox_lambda'] = lambda_param
                except Exception as e:
                    logger.warning(f"Box-Cox transformation failed: {e}")
        
        # Calculate basic statistics
        if pbar:
            pbar.text = "Calculating basic statistics..."
        
        basic_stats = {
            'mean': float(np.mean(mse_values)),
            'std': float(np.std(mse_values)),
            'min': float(np.min(mse_values)),
            'max': float(np.max(mse_values)),
            'median': float(np.median(mse_values)),
            'q25': float(np.percentile(mse_values, 25)),
            'q75': float(np.percentile(mse_values, 75)),
            'skewness': float(stats.skew(mse_values)),
            'kurtosis': float(stats.kurtosis(mse_values))
        }
        
        calculation_stats['basic_statistics'] = basic_stats
        
        # Main threshold calculation based on method
        threshold_results = {}
        
        if threshold_method == 'percentile':
            if pbar:
                pbar.text = f"Calculating percentile-based threshold: P{percentile}..."
            logger.info(f"Calculating percentile-based threshold: P{percentile}")
            
            interpolation = percentile_config.get('percentile_interpolation', 'linear')
            threshold = np.percentile(mse_values, percentile, interpolation=interpolation)
            
            # Multi-percentile analysis if requested
            if percentile_config.get('multi_percentile'):
                multi_perc = percentile_config['multi_percentile']
                multi_thresholds = {}
                for p in multi_perc:
                    multi_thresholds[f'P{p}'] = float(np.percentile(mse_values, p, interpolation=interpolation))
                threshold_results['multi_percentile_thresholds'] = multi_thresholds
            
            threshold_results.update({
                'threshold': float(threshold),
                'method': 'percentile',
                'percentile_used': percentile,
                'interpolation': interpolation
            })
            
        elif threshold_method == 'statistical':
            statistical_method = statistical_config.get('statistical_method', 'mean_std')
            if pbar:
                pbar.text = f"Calculating statistical threshold using: {statistical_method}..."
            logger.info(f"Calculating statistical threshold using: {statistical_method}")
            
            if statistical_method == 'mean_std':
                threshold = basic_stats['mean'] + std_multiplier * basic_stats['std']
            elif statistical_method == 'median_mad':
                mad = np.median(np.abs(mse_values - basic_stats['median']))
                threshold = basic_stats['median'] + statistical_config.get('mad_multiplier', 2.0) * mad
                threshold_results['mad'] = float(mad)
            elif statistical_method == 'iqr':
                IQR = basic_stats['q75'] - basic_stats['q25']
                threshold = basic_stats['q75'] + iqr_multiplier * IQR
                threshold_results['iqr'] = float(IQR)
            elif statistical_method == 'z_score':
                threshold = basic_stats['mean'] + z_score_threshold * basic_stats['std']
            elif statistical_method == 'modified_z_score':
                mad = np.median(np.abs(mse_values - basic_stats['median']))
                modified_z_scores = 0.6745 * (mse_values - basic_stats['median']) / mad
                threshold = basic_stats['median'] + z_score_threshold * mad / 0.6745
                threshold_results['modified_z_mad'] = float(mad)
            else:
                threshold = basic_stats['mean'] + 2 * basic_stats['std']
            
            threshold_results.update({
                'threshold': float(threshold),
                'method': 'statistical',
                'statistical_method': statistical_method,
                'multiplier_used': std_multiplier if statistical_method == 'mean_std' else iqr_multiplier
            })
            
        elif threshold_method == 'distribution':
            distribution_type = distribution_config.get('distribution_type', 'normal')
            if pbar:
                pbar.text = f"Calculating distribution-based threshold using: {distribution_type}..."
            logger.info(f"Calculating distribution-based threshold using: {distribution_type}")
            
            if distribution_type == 'normal':
                # Fit normal distribution
                mu, sigma = stats.norm.fit(mse_values)
                threshold = stats.norm.ppf(confidence_level, mu, sigma)
                threshold_results.update({
                    'mu': float(mu),
                    'sigma': float(sigma),
                    'distribution_params': {'mu': mu, 'sigma': sigma}
                })
                
            elif distribution_type == 'gamma':
                # Fit gamma distribution
                shape, loc, scale = stats.gamma.fit(mse_values)
                threshold = stats.gamma.ppf(confidence_level, shape, loc, scale)
                threshold_results.update({
                    'shape': float(shape),
                    'loc': float(loc),
                    'scale': float(scale),
                    'distribution_params': {'shape': shape, 'loc': loc, 'scale': scale}
                })
                
            elif distribution_type == 'lognorm':
                # Fit lognormal distribution
                shape, loc, scale = stats.lognorm.fit(mse_values)
                threshold = stats.lognorm.ppf(confidence_level, shape, loc, scale)
                threshold_results.update({
                    'shape': float(shape),
                    'loc': float(loc),
                    'scale': float(scale),
                    'distribution_params': {'shape': shape, 'loc': loc, 'scale': scale}
                })
                
            elif distribution_type == 'exponential':
                # Fit exponential distribution
                loc, scale = stats.expon.fit(mse_values)
                threshold = stats.expon.ppf(confidence_level, loc, scale)
                threshold_results.update({
                    'loc': float(loc),
                    'scale': float(scale),
                    'distribution_params': {'loc': loc, 'scale': scale}
                })
                
            else:
                # Default to normal distribution
                mu, sigma = stats.norm.fit(mse_values)
                threshold = stats.norm.ppf(confidence_level, mu, sigma)
                threshold_results['distribution_params'] = {'mu': mu, 'sigma': sigma}
            
            threshold_results.update({
                'threshold': float(threshold),
                'method': 'distribution',
                'distribution_type': distribution_type,
                'confidence_level': confidence_level
            })
            
        elif threshold_method == 'ml':
            ml_method = ml_config.get('ml_threshold_method', 'isolation_forest')
            if pbar:
                pbar.text = f"Calculating ML-based threshold using: {ml_method}..."
            logger.info(f"Calculating ML-based threshold using: {ml_method}")
            
            # Reshape data for sklearn
            X = mse_values.reshape(-1, 1)
            
            if ml_method == 'isolation_forest':
                contamination = ml_config.get('isolation_forest_contamination', contamination_rate)
                clf = IsolationForest(contamination=contamination, random_state=random_state)
                clf.fit(X)
                
                # Get decision scores
                scores = clf.decision_function(X)
                threshold = np.percentile(scores, (1 - contamination) * 100)
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'ml',
                    'ml_method': ml_method,
                    'contamination': contamination,
                    'decision_scores_range': [float(np.min(scores)), float(np.max(scores))]
                })
                
            elif ml_method == 'local_outlier_factor':
                n_neighbors = ml_config.get('local_outlier_factor_neighbors', 20)
                contamination = ml_config.get('contamination_rate', contamination_rate)
                
                clf = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)
                outlier_labels = clf.fit_predict(X)
                
                # Get LOF scores
                lof_scores = -clf.negative_outlier_factor_
                threshold = np.percentile(lof_scores, (1 - contamination) * 100)
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'ml',
                    'ml_method': ml_method,
                    'n_neighbors': n_neighbors,
                    'contamination': contamination,
                    'lof_scores_range': [float(np.min(lof_scores)), float(np.max(lof_scores))]
                })
                
            elif ml_method == 'one_class_svm':
                nu = ml_config.get('one_class_svm_nu', contamination_rate)
                kernel = ml_config.get('one_class_svm_kernel', 'rbf')
                
                clf = OneClassSVM(nu=nu, kernel=kernel)
                clf.fit(X)
                
                # Get decision scores
                scores = clf.decision_function(X)
                threshold = np.percentile(scores, nu * 100)
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'ml',
                    'ml_method': ml_method,
                    'nu': nu,
                    'kernel': kernel,
                    'decision_scores_range': [float(np.min(scores)), float(np.max(scores))]
                })
                
            elif ml_method == 'elliptic_envelope':
                contamination = ml_config.get('elliptic_envelope_contamination', contamination_rate)
                
                clf = EllipticEnvelope(contamination=contamination, random_state=random_state)
                clf.fit(X)
                
                # Get Mahalanobis distances
                scores = clf.decision_function(X)
                threshold = np.percentile(scores, (1 - contamination) * 100)
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'ml',
                    'ml_method': ml_method,
                    'contamination': contamination,
                    'mahalanobis_distances_range': [float(np.min(scores)), float(np.max(scores))]
                })
                
            else:
                # Fallback to isolation forest
                clf = IsolationForest(contamination=contamination_rate, random_state=random_state)
                clf.fit(X)
                scores = clf.decision_function(X)
                threshold = np.percentile(scores, (1 - contamination_rate) * 100)
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'ml',
                    'ml_method': 'isolation_forest_fallback',
                    'contamination': contamination_rate
                })
                
        elif threshold_method == 'mixture':
            if pbar:
                pbar.text = "Calculating mixture model threshold..."
            logger.info("Calculating mixture model threshold")
            
            n_components = final_config.get('multi_modal', {}).get('n_components', 2)
            mixture_type = final_config.get('multi_modal', {}).get('mixture_type', 'gaussian')
            
            if mixture_type == 'gaussian':
                # Fit Gaussian Mixture Model
                gmm = GaussianMixture(n_components=n_components, random_state=random_state)
                gmm.fit(mse_values.reshape(-1, 1))
                
                # Get component with highest mean (anomaly component)
                component_means = gmm.means_.flatten()
                anomaly_component = np.argmax(component_means)
                
                # Calculate threshold based on anomaly component
                anomaly_mean = component_means[anomaly_component]
                anomaly_std = np.sqrt(gmm.covariances_[anomaly_component].flatten()[0])
                
                threshold = anomaly_mean - 2 * anomaly_std  # Conservative threshold
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'mixture',
                    'mixture_type': mixture_type,
                    'n_components': n_components,
                    'component_means': component_means.tolist(),
                    'component_weights': gmm.weights_.tolist(),
                    'anomaly_component': int(anomaly_component)
                })
            else:
                # Fallback to percentile method
                threshold = np.percentile(mse_values, percentile)
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'mixture_fallback_percentile',
                    'percentile_used': percentile
                })
                
        elif threshold_method == 'adaptive':
            if pbar:
                pbar.text = "Calculating adaptive threshold..."
            logger.info("Calculating adaptive threshold")
            
            # Calculate multiple thresholds and select based on data characteristics
            percentile_thresh = np.percentile(mse_values, percentile)
            statistical_thresh = basic_stats['mean'] + 2 * basic_stats['std']
            iqr_thresh = basic_stats['q75'] + 1.5 * (basic_stats['q75'] - basic_stats['q25'])
            
            # Selection based on distribution characteristics
            if basic_stats['skewness'] > 1:  # Right-skewed
                threshold = percentile_thresh
                method_used = 'percentile_skewed'
            elif basic_stats['kurtosis'] > 3:  # Heavy-tailed
                threshold = iqr_thresh
                method_used = 'iqr_heavy_tailed'
            else:  # Approximately normal
                threshold = statistical_thresh
                method_used = 'statistical_normal'
            
            threshold_results.update({
                'threshold': float(threshold),
                'method': 'adaptive',
                'adaptive_method_used': method_used,
                'candidate_thresholds': {
                    'percentile': float(percentile_thresh),
                    'statistical': float(statistical_thresh),
                    'iqr': float(iqr_thresh)
                },
                'selection_criteria': {
                    'skewness': basic_stats['skewness'],
                    'kurtosis': basic_stats['kurtosis']
                }
            })
            
        else:
            # Default to percentile method
            logger.warning(f"Unknown threshold method '{threshold_method}', using percentile")
            threshold = np.percentile(mse_values, percentile)
            threshold_results.update({
                'threshold': float(threshold),
                'method': 'percentile_default',
                'percentile_used': percentile
            })
        
        # Cross-validation if requested
        cv_results = {}
        if final_config.get('cross_validation', {}).get('cross_validation_threshold', False):
            if pbar:
                pbar.text = "Performing cross-validation threshold analysis..."
            logger.info("Performing cross-validation threshold analysis")
            
            cv_folds = final_config.get('cross_validation', {}).get('cv_folds', 5)
            fold_thresholds = []
            
            # Simple k-fold validation
            fold_size = len(mse_values) // cv_folds
            for fold in range(cv_folds):
                start_idx = fold * fold_size
                end_idx = start_idx + fold_size if fold < cv_folds - 1 else len(mse_values)
                fold_data = mse_values[start_idx:end_idx]
                
                # Calculate threshold for this fold
                fold_threshold = np.percentile(fold_data, percentile)
                fold_thresholds.append(fold_threshold)
                
                # Update progress bar with CV status
                if pbar:
                    progress_text = (
                        f"Cross-validation: Fold {fold+1}/{cv_folds} | "
                        f"Threshold: {fold_threshold:.4f} | "
                        f"Samples: {len(fold_data):,}"
                    )
                    pbar.text = progress_text
            
            cv_results = {
                'cv_folds': cv_folds,
                'fold_thresholds': [float(t) for t in fold_thresholds],
                'cv_mean_threshold': float(np.mean(fold_thresholds)),
                'cv_std_threshold': float(np.std(fold_thresholds)),
                'cv_min_threshold': float(np.min(fold_thresholds)),
                'cv_max_threshold': float(np.max(fold_thresholds))
            }
        
        # Bootstrap confidence intervals if requested
        bootstrap_results = {}
        if final_config.get('cross_validation', {}).get('bootstrap_threshold', False):
            if pbar:
                pbar.text = "Performing bootstrap confidence interval analysis..."
            logger.info("Performing bootstrap confidence interval analysis")
            
            bootstrap_samples = final_config.get('cross_validation', {}).get('bootstrap_samples', 1000)
            bootstrap_thresholds = []
            
            for i in range(bootstrap_samples):
                # Bootstrap sample
                bootstrap_data = np.random.choice(mse_values, size=len(mse_values), replace=True)
                bootstrap_threshold = np.percentile(bootstrap_data, percentile)
                bootstrap_thresholds.append(bootstrap_threshold)
                
                # Update progress bar every 100 samples to avoid performance hit
                if pbar and i % 100 == 0:
                    progress_text = (
                        f"Bootstrap: {i+1}/{bootstrap_samples} | "
                        f"Current CI: [{np.percentile(bootstrap_thresholds, 2.5):.4f}, "
                        f"{np.percentile(bootstrap_thresholds, 97.5):.4f}]"
                    )
                    pbar.text = progress_text
            
            bootstrap_results = {
                'bootstrap_samples': bootstrap_samples,
                'bootstrap_mean': float(np.mean(bootstrap_thresholds)),
                'bootstrap_std': float(np.std(bootstrap_thresholds)),
                'bootstrap_ci_lower': float(np.percentile(bootstrap_thresholds, 2.5)),
                'bootstrap_ci_upper': float(np.percentile(bootstrap_thresholds, 97.5))
            }
        
        # Validation if requested
        validation_results = {}
        if validation_config.get('validate_threshold', False):
            if pbar:
                pbar.text = "Performing threshold validation..."
            logger.info("Performing threshold validation")
            
            # Calculate some validation metrics
            threshold_value = threshold_results['threshold']
            anomalies = mse_values > threshold_value
            
            validation_results = {
                'anomaly_rate': float(np.mean(anomalies)),
                'n_anomalies': int(np.sum(anomalies)),
                'n_normal': int(np.sum(~anomalies)),
                'threshold_percentile_actual': float(stats.percentileofscore(mse_values, threshold_value)),
                'validation_passed': True
            }
            
            # Additional validation checks
            quality_metrics = validation_config.get('quality_metrics', [])
            for metric in quality_metrics:
                if metric == 'stability':
                    # Simple stability check
                    validation_results['stability_score'] = 1.0 - (np.std(mse_values) / np.mean(mse_values))
                elif metric == 'sensitivity':
                    # Sensitivity to threshold changes
                    thresh_variations = [threshold_value * 0.9, threshold_value * 1.1]
                    anomaly_rates = []
                    for tv in thresh_variations:
                        anomaly_rates.append(np.mean(mse_values > tv))
                    validation_results['sensitivity_score'] = float(np.std(anomaly_rates))
        
        # Calculate final timing
        calculation_time = (datetime.now() - start_time).total_seconds()
        
        # Prepare comprehensive results
        final_threshold = threshold_results.get('threshold', fallback_threshold)
        
        comprehensive_results = {
            # Core results
            'threshold': final_threshold,
            'method': threshold_results.get('method', threshold_method),
            'calculation_time_seconds': calculation_time,
            
            # Data information
            'n_samples': len(mse_values),
            'data_source': data_source,
            'basic_statistics': basic_stats,
            
            # Method-specific results
            'method_details': threshold_results,
            
            # Configuration applied
            'config_applied': final_config,
            
            # Additional analyses
            'cross_validation': cv_results if cv_results else None,
            'bootstrap_analysis': bootstrap_results if bootstrap_results else None,
            'validation_results': validation_results if validation_results else None,
            
            # System information
            'device': str(device),
            'random_state': random_state,
            'reproducible': reproducible
        }
        
        # Add calculation statistics
        comprehensive_results.update(calculation_stats)
        
        # Save results if requested
        export_config = final_config.get('export', {})
        if export_config.get('save_results', False):
            if pbar:
                pbar.text = "Saving results..."
            results_path = export_config.get('results_path', f'./threshold_results.json')
            try:
                with open(results_path, 'w') as f:
                    # Make results JSON serializable
                    serializable_results = {}
                    for key, value in comprehensive_results.items():
                        try:
                            json.dumps(value)
                            serializable_results[key] = value
                        except TypeError:
                            serializable_results[key] = str(value)
                    
                    json.dump(serializable_results, f, indent=2)
                logger.info(f"Saved threshold results to {results_path}")
            except Exception as e:
                logger.warning(f"Failed to save results: {e}")
        
        # Visualization if requested
        if export_config.get('visualization', False):
            try:
                if export_config.get('plot_distribution', False):
                    if pbar:
                        pbar.text("Generating visualization...")
                    plt.figure(figsize=(10, 6))
                    plt.hist(mse_values, bins=50, alpha=0.7, density=True, label='Reconstruction Errors')
                    plt.axvline(final_threshold, color='red', linestyle='--', 
                              label=f'Threshold ({threshold_method}): {final_threshold:.4f}')
                    plt.xlabel('Reconstruction Error (MSE)')
                    plt.ylabel('Density')
                    plt.title('Reconstruction Error Distribution and Threshold')
                    plt.legend()
                    plt.grid(True, alpha=0.3)
                    
                    plot_path = export_config.get('results_path', './').replace('.json', '_distribution.png')
                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                    plt.close()
                    logger.info(f"Saved distribution plot to {plot_path}")
                    
            except ImportError:
                logger.warning("Matplotlib not available, skipping visualization")
            except Exception as e:
                logger.warning(f"Visualization failed: {e}")
        
        # Final progress bar update
        if pbar:
            pbar.text = f"Threshold calculation completed: {final_threshold:.4f}"
        
        # Log comprehensive summary
        logger.info("=" * 80)
        logger.info("THRESHOLD CALCULATION SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Method: {threshold_results.get('method', threshold_method)}")
        logger.info(f"Threshold: {final_threshold:.6f}")
        logger.info(f"Samples Used: {len(mse_values):,}")
        logger.info(f"Data Source: {data_source}")
        logger.info(f"Calculation Time: {calculation_time:.3f} seconds")
        
        if 'anomaly_rate' in validation_results:
            logger.info(f"Expected Anomaly Rate: {validation_results['anomaly_rate']*100:.2f}%")
        
        logger.info(f"Data Statistics:")
        logger.info(f"  Mean: {basic_stats['mean']:.6f}")
        logger.info(f"  Std: {basic_stats['std']:.6f}")
        logger.info(f"  Min: {basic_stats['min']:.6f}")
        logger.info(f"  Max: {basic_stats['max']:.6f}")
        logger.info(f"  Skewness: {basic_stats['skewness']:.3f}")
        
        if cv_results:
            logger.info(f"Cross-validation: {cv_results['cv_mean_threshold']:.6f} ± {cv_results['cv_std_threshold']:.6f}")
        
        if bootstrap_results:
            logger.info(f"Bootstrap CI: [{bootstrap_results['bootstrap_ci_lower']:.6f}, {bootstrap_results['bootstrap_ci_upper']:.6f}]")
        
        logger.info("=" * 80)
        
        # Restore original logging level
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        # Return threshold and comprehensive results
        return final_threshold, comprehensive_results
    
    except (EOFError, KeyboardInterrupt):
        logger.warning("Threshold calculation interrupted by user")

    except Exception as e:
        # Restore original logging level on error
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        error_msg = f"Threshold calculation failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Provide helpful error context
        logger.error(f"Method used: {threshold_method}")
        logger.error(f"Configuration: {final_config}")
        
        # Attempt graceful recovery if enabled
        if graceful_degradation:
            logger.warning(f"Using fallback threshold: {fallback_threshold}")
            
            fallback_results = {
                'threshold': fallback_threshold,
                'method': 'fallback',
                'error': str(e),
                'graceful_degradation': True,
                'config_applied': final_config
            }
            
            return fallback_threshold, fallback_results
        
        raise RuntimeError(error_msg) from e
    
    finally:
        # Final cleanup
        try:
            if pbar_context:
                # Properly exit the context manager
                pbar_context.__exit__(None, None, None)
            
            if final_config.get('performance', {}).get('memory_efficient', True):
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
        except:
            pass

def train_model(
    # Core Training Parameters
    model_type: Optional[str] = None,
    input_dim: Optional[int] = None,
    
    # Model Architecture Parameters
    encoding_dim: Optional[int] = None,
    hidden_dims: Optional[List[int]] = None,
    dropout_rates: Optional[List[float]] = None,
    activation: Optional[str] = None,
    activation_param: Optional[float] = None,
    normalization: Optional[str] = None,
    use_batch_norm: Optional[bool] = None,
    use_layer_norm: Optional[bool] = None,
    bias: Optional[bool] = None,
    weight_init: Optional[str] = None,
    skip_connection: Optional[bool] = None,
    residual_blocks: Optional[bool] = None,
    use_attention: Optional[bool] = None,
    
    # Model Type and Variants
    model_types: Optional[List[str]] = None,
    available_activations: Optional[List[str]] = None,
    available_normalizations: Optional[List[str]] = None,
    available_initializers: Optional[List[str]] = None,
    legacy_mode: Optional[bool] = None,
    
    # Ensemble Parameters
    diversity_factor: Optional[float] = None,
    min_features: Optional[int] = None,
    num_models: Optional[int] = None,
    
    # Training Configuration Parameters
    batch_size: Optional[int] = None,
    epochs: Optional[int] = None,
    learning_rate: Optional[float] = None,
    patience: Optional[int] = None,
    weight_decay: Optional[float] = None,
    gradient_clip: Optional[float] = None,
    gradient_accumulation_steps: Optional[int] = None,
    mixed_precision: Optional[bool] = None,
    num_workers: Optional[int] = None,
    optimizer: Optional[str] = None,
    optimizer_type: Optional[str] = None,
    scheduler: Optional[str] = None,
    scheduler_type: Optional[str] = None,
    scheduler_params: Optional[Dict[str, Any]] = None,
    early_stopping: Optional[bool] = None,
    validation_split: Optional[float] = None,
    shuffle: Optional[bool] = None,
    pin_memory: Optional[bool] = None,
    persistent_workers: Optional[bool] = None,
    adam_betas: Optional[Tuple[float, float]] = None,
    adam_eps: Optional[float] = None,
    lr_patience: Optional[int] = None,
    lr_factor: Optional[float] = None,
    min_lr: Optional[float] = None,
    
    # Data Parameters
    normal_samples: Optional[int] = None,
    attack_samples: Optional[int] = None,
    features: Optional[int] = None,
    use_real_data: Optional[bool] = None,
    data_path: Optional[Union[str, Path]] = None,
    artifacts_path: Optional[Union[str, Path]] = None,
    data_normalization: Optional[str] = None,
    anomaly_factor: Optional[float] = None,
    random_state: Optional[int] = None,
    test_split: Optional[float] = None,
    stratified_split: Optional[bool] = None,
    synthetic_generation: Optional[Dict[str, Any]] = None,
    preprocessing: Optional[Dict[str, Any]] = None,
    synthetic_config: Optional[Dict[str, Any]] = None,
    data_preprocessing: Optional[bool] = None,
    normalization_method: Optional[str] = None,
    
    # Security Parameters
    percentile: Optional[float] = None,
    attack_threshold: Optional[float] = None,
    false_negative_cost: Optional[float] = None,
    enable_security_metrics: Optional[bool] = None,
    anomaly_threshold_strategy: Optional[str] = None,
    early_warning_threshold: Optional[float] = None,
    adaptive_threshold: Optional[bool] = None,
    confidence_interval: Optional[float] = None,
    detection_methods: Optional[List[str]] = None,
    alert_levels: Optional[List[str]] = None,
    threshold_validation: Optional[bool] = None,
    robust_detection: Optional[bool] = None,
    false_positive_tolerance: Optional[float] = None,
    performance_optimized_detection: Optional[bool] = None,
    real_time_monitoring: Optional[bool] = None,
    ensemble_voting: Optional[str] = None,
    uncertainty_threshold: Optional[float] = None,
    threshold_method: Optional[str] = None,
    
    # Monitoring Parameters
    metrics_frequency: Optional[int] = None,
    checkpoint_frequency: Optional[int] = None,
    tensorboard_logging: Optional[bool] = None,
    console_logging_level: Optional[str] = None,
    save_best_model: Optional[bool] = None,
    save_model_history: Optional[bool] = None,
    metrics_to_track: Optional[List[str]] = None,
    early_stopping_metric: Optional[str] = None,
    checkpoint_format: Optional[str] = None,
    log_model_summary: Optional[bool] = None,
    tensorboard_dir: Optional[Union[str, Path]] = None,
    tb_dir: Optional[Union[str, Path]] = None,
    log_frequency: Optional[int] = None,
    save_checkpoints: Optional[bool] = None,
    tensorboard: Optional[Dict[str, Any]] = None,
    stability_metrics: Optional[bool] = None,
    performance_metrics: Optional[bool] = None,
    profiling_enabled: Optional[bool] = None,
    
    # Hardware Parameters
    device: Optional[str] = None,
    recommended_gpu_memory: Optional[float] = None,
    minimum_system_requirements: Optional[Dict[str, Any]] = None,
    optimal_system_requirements: Optional[Dict[str, Any]] = None,
    memory_management: Optional[Dict[str, Any]] = None,
    performance_optimization: Optional[Dict[str, Any]] = None,
    detected_gpu_memory: Optional[float] = None,
    detected_system_memory: Optional[float] = None,
    system_performance_class: Optional[str] = None,
    optimization_recommendations: Optional[List[str]] = None,
    
    # System Parameters
    model_dir: Optional[Union[str, Path]] = None,
    log_dir: Optional[Union[str, Path]] = None,
    config_dir: Optional[Union[str, Path]] = None,
    data_dir: Optional[Union[str, Path]] = None,
    checkpoint_dir: Optional[Union[str, Path]] = None,
    results_dir: Optional[str] = None,
    random_seed: Optional[int] = None,
    reproducible: Optional[bool] = None,
    parallel_processing: Optional[bool] = None,
    max_workers: Optional[int] = None,
    export_onnx: Optional[bool] = None,
    non_interactive: Optional[bool] = None,
    cuda_optimizations: Optional[bool] = None,
    onnx_export: Optional[Dict[str, Any]] = None,
    distributed_training: Optional[bool] = None,
    python_executable: Optional[str] = None,
    working_directory: Optional[str] = None,
    environment_health: Optional[str] = None,
    
    # Preset Parameters
    available_presets: Optional[List[str]] = None,
    current_preset: Optional[str] = None,
    current_override: Optional[str] = None,
    override_rules: Optional[Dict[str, bool]] = None,
    preset_configs: Optional[Dict[str, str]] = None,
    custom_presets_available: Optional[List[str]] = None,
    auto_apply: Optional[bool] = None,
    validate_compatibility: Optional[bool] = None,
    system_recommended_preset: Optional[str] = None,
    preset_compatibility: Optional[Dict[str, Any]] = None,
    
    # Hyperparameter Optimization Parameters
    hpo_enabled: Optional[bool] = None,
    hpo_strategy: Optional[str] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    n_trials: Optional[int] = None,
    timeout: Optional[int] = None,
    sampler: Optional[str] = None,
    pruner: Optional[str] = None,
    objective_metric: Optional[str] = None,
    optimization_space: Optional[Dict[str, Any]] = None,
    hpo_early_stopping: Optional[Dict[str, Any]] = None,
    timeout_seconds: Optional[int] = None,
    trial_epochs: Optional[int] = None,
    trial_patience: Optional[int] = None,
    cleanup_trials: Optional[bool] = None,
    generate_plots: Optional[bool] = None,
    search_space: Optional[Dict[str, Any]] = None,
    hpo_sampler: Optional[Dict[str, Any]] = None,
    hpo_pruner: Optional[Dict[str, Any]] = None,
    scoring: Optional[Dict[str, Any]] = None,
    storage: Optional[Dict[str, Any]] = None,
    
    # Validation Parameters
    cross_validation: Optional[Dict[str, Any]] = None,
    cv_folds: Optional[int] = None,
    metrics: Optional[List[str]] = None,
    validation_frequency: Optional[int] = None,
    save_validation_results: Optional[bool] = None,
    detailed_metrics: Optional[bool] = None,
    robustness_testing: Optional[bool] = None,
    performance_benchmarking: Optional[bool] = None,
    confidence_intervals: Optional[bool] = None,
    calculate_detailed_metrics: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_features: Optional[Dict[str, bool]] = None,
    experimental_settings: Optional[Dict[str, bool]] = None,
    auto_optimize: Optional[bool] = None,
    
    # Metadata Parameters
    description: Optional[str] = None,
    version: Optional[str] = None,
    config_version: Optional[str] = None,
    config_type: Optional[str] = None,
    created: Optional[str] = None,
    last_modified: Optional[str] = None,
    preset_used: Optional[str] = None,
    recommended_hardware: Optional[Dict[str, Any]] = None,
    compatibility: Optional[List[str]] = None,
    system_info: Optional[Dict[str, Any]] = None,
    validation_info: Optional[Dict[str, Any]] = None,
    
    # Runtime Parameters
    config_loaded_at: Optional[str] = None,
    config_source: Optional[str] = None,
    runtime_id: Optional[str] = None,
    process_id: Optional[int] = None,
    system_analysis_completed: Optional[bool] = None,
    system_performance_score: Optional[float] = None,
    system_class: Optional[str] = None,
    optimizations_applied: Optional[Dict[str, bool]] = None,
    resource_status: Optional[Dict[str, bool]] = None,
    system_warnings: Optional[List[str]] = None,
    recommendations: Optional[List[str]] = None,
    configuration_health: Optional[Dict[str, Any]] = None,
    
    # Training System Parameters
    silent: Optional[bool] = None,
    verbose: Optional[bool] = None,
    debug: Optional[bool] = None,
    debug_mode: Optional[bool] = None,
    progress_bar: Optional[bool] = None,
    
    # Export Parameters
    save_model: Optional[bool] = None,
    save_metadata: Optional[bool] = None,
    save_training_history: Optional[bool] = None,
    
    # Advanced Training Parameters
    compile_model: Optional[bool] = None,
    benchmark_mode: Optional[bool] = None,
    memory_efficient: Optional[bool] = None,
    gradient_checkpointing: Optional[bool] = None,
    
    # Error Handling Parameters
    error_handling: Optional[str] = None,
    continue_on_error: Optional[bool] = None,
    graceful_degradation: Optional[bool] = None,
    fallback_mode: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    training_config: Optional[Dict[str, Any]] = None,
    preset: Optional[str] = None,
    args: Optional[argparse.Namespace] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """
    Model training function with full parameter compatibility.
    
    This function now accepts ALL parameters supported by the autoencoder classes
    (SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble) and uses the
    same centralized configuration and parameter processing infrastructure.
    
    Args:
        All parameters supported by the autoencoder model classes and their
        helper functions (_initialize_autoencoder_config, _structure_kwargs_into_config_sections)
        
    Returns:
        Dictionary containing training results and metadata
    """
    # Start timing
    start_time = datetime.now()
    training_start_time = time.time()
    
    # Initialize configuration
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply training-specific configuration
    if training_config:
        config.setdefault('training', {}).update(training_config)
    
    # Apply preset configuration if specified
    if preset and preset in globals().get('PRESET_CONFIGS', {}):
        try:
            preset_config = globals()['PRESET_CONFIGS'][preset].copy()
            # Merge preset config
            config = deep_update(preset_config, config)
            logger.info(f"Applied preset configuration: {preset}")
        except Exception as e:
            logger.warning(f"Failed to apply preset '{preset}': {e}")
    
    # Handle legacy args parameter for backward compatibility
    if args is not None:
        # Extract parameters from args object
        legacy_params = {}
        for attr_name in dir(args):
            if not attr_name.startswith('_'):
                value = getattr(args, attr_name)
                if value is not None:
                    legacy_params[attr_name] = value
        
        # Apply legacy parameters
        kwargs.update(legacy_params)
        logger.debug("Applied legacy args parameters")
    
    # Collect all non-None parameters for processing
    local_params = locals().copy()
    params_to_remove = {
        'config', 'training_config', 'preset', 'args', 'kwargs', 'start_time', 'training_start_time', 'datetime', 'traceback', 'time', 'gc', 'warnings',
        'defaultdict', 'deque', 'nullcontext', 'nn', 'optim', 'DataLoader', 'GradScaler', 'autocast', 'SummaryWriter', 'Path', 'np', 'torch'
    }
    
    individual_params = {k: v for k, v in local_params.items() if k not in params_to_remove and v is not None}
    
    # Add kwargs
    if kwargs:
        individual_params.update(kwargs)
    
    # Parameter structuring
    if individual_params:
        structured_params = _structure_kwargs_into_config_sections(individual_params)
        config = deep_update(config, structured_params)
        logger.debug(f"Processed {len(individual_params)} individual parameters using centralized helper")
    
    # Extract final configuration values
    model_config = config.get('model', {})
    training_config = config.get('training', {})
    data_config = config.get('data', {})
    security_config = config.get('security', {})
    system_config = config.get('system', {})
    monitoring_config = config.get('monitoring', {})
    export_config = config.get('export', {}) if 'export' in config else {}
    advanced_config = config.get('advanced_training', {}) if 'advanced_training' in config else {}
    validation_config = config.get('validation', {})
    error_config = config.get('error_handling', {}) if 'error_handling' in config else {}
    hardware_config = config.get('hardware', {})
    
    # Parameter extraction
    model_type = _extract_and_validate_config_param(
        model_config, 'model_type', 'EnhancedAutoencoder', 'MODEL_TYPE',
        lambda x: isinstance(x, str) and x in ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble'],
        "model type"
    )
    
    # Handle backward compatibility for optimizer/scheduler naming
    optimizer_type = training_config.get('optimizer_type') or training_config.get('optimizer', 'AdamW')
    scheduler_type = training_config.get('scheduler_type') or training_config.get('scheduler', 'ReduceLROnPlateau')
    training_config['optimizer'] = optimizer_type
    training_config['scheduler'] = scheduler_type
    
    # Handle backward compatibility for tensorboard directory naming
    if 'tb_dir' in system_config and 'tensorboard_dir' not in system_config:
        system_config['tensorboard_dir'] = system_config['tb_dir']
    
    # Set default values using existing constants and patterns
    input_dim = model_config.get('input_dim')
    encoding_dim = model_config.setdefault('encoding_dim', DEFAULT_ENCODING_DIM)
    hidden_dims = model_config.setdefault('hidden_dims', HIDDEN_LAYER_SIZES)
    dropout_rates = model_config.setdefault('dropout_rates', DROPOUT_RATES)
    activation = model_config.setdefault('activation', ACTIVATION)
    normalization = model_config.setdefault('normalization', NORMALIZATION)
    
    # Training defaults
    batch_size = training_config.setdefault('batch_size', DEFAULT_BATCH_SIZE)
    epochs = training_config.setdefault('epochs', DEFAULT_EPOCHS)
    learning_rate = training_config.setdefault('learning_rate', LEARNING_RATE)
    patience = training_config.setdefault('patience', EARLY_STOPPING_PATIENCE)
    weight_decay = training_config.setdefault('weight_decay', WEIGHT_DECAY)
    gradient_clip = training_config.setdefault('gradient_clip', GRADIENT_CLIP)
    gradient_accumulation_steps = training_config.setdefault('gradient_accumulation_steps', GRADIENT_ACCUMULATION_STEPS)
    mixed_precision = training_config.setdefault('mixed_precision', MIXED_PRECISION)
    early_stopping = training_config.setdefault('early_stopping', True)
    validation_split = training_config.setdefault('validation_split', 0.2)
    
    # Data defaults
    normal_samples = data_config.setdefault('normal_samples', NORMAL_SAMPLES)
    attack_samples = data_config.setdefault('attack_samples', ATTACK_SAMPLES)
    features = data_config.setdefault('features', FEATURES)
    use_real_data = data_config.setdefault('use_real_data', False)
    data_preprocessing = data_config.setdefault('preprocessing', {}).get('enabled', True)
    
    # Security defaults
    percentile = security_config.setdefault('percentile', DEFAULT_PERCENTILE)
    threshold_method = security_config.setdefault('threshold_method', 'percentile')
    enable_security_metrics = security_config.setdefault('enable_security_metrics', SECURITY_METRICS)
    
    # System defaults
    config_dir = Path(system_config.setdefault('config_dir', CONFIG_DIR))
    if config_dir is None:
        config_dir = Path(__file__).resolve().parent / "config"
    
    model_dir = Path(system_config.setdefault('model_dir', DEFAULT_MODEL_DIR))
    log_dir = Path(system_config.setdefault('log_dir', LOG_DIR))
    tensorboard_dir = Path(monitoring_config.setdefault('tensorboard_dir', TB_DIR)) or Path(system_config.setdefault('tensorboard_dir', TB_DIR))
    checkpoint_dir = Path(system_config.setdefault('checkpoint_dir', CHECKPOINTS_DIR))
    data_dir = Path(system_config.setdefault('data_dir', DATA_DIR))
    results_dir = Path(system_config.setdefault('results_dir', RESULTS_DIR))
    device = system_config.setdefault('device', 'auto')
    random_seed = system_config.setdefault('random_seed', RANDOM_STATE)
    reproducible = system_config.setdefault('reproducible', True)

    # Handle silent mode - override progress_bar if silent is True
    silent_mode = monitoring_config.get('silent', False)
    # if silent_mode:
    #     # Force silent mode behavior
    #     monitoring_config['progress_bar'] = False
    
    # Monitoring defaults
    verbose = monitoring_config.setdefault('verbose', True)
    debug_mode = monitoring_config.setdefault('debug_mode', False)
    tensorboard_logging = monitoring_config.setdefault('tensorboard_logging', True)
    save_checkpoints = monitoring_config.setdefault('save_checkpoints', True)
    checkpoint_frequency = monitoring_config.setdefault('checkpoint_frequency', 10)
    log_frequency = monitoring_config.setdefault('log_frequency', 1)
    progress_bar = monitoring_config.setdefault('progress_bar', True)
    #progress_bar = monitoring_config.setdefault('progress_bar', not silent_mode)
    
    # Export defaults
    export_onnx = export_config.setdefault('export_onnx', False)
    save_model = export_config.setdefault('save_model', True)
    save_metadata = export_config.setdefault('save_metadata', True)
    save_training_history = export_config.setdefault('save_training_history', True)
    
    # Advanced training defaults
    num_workers = advanced_config.setdefault('num_workers', NUM_WORKERS)
    pin_memory = advanced_config.setdefault('pin_memory', torch.cuda.is_available())
    persistent_workers = advanced_config.setdefault('persistent_workers', num_workers > 0)
    compile_model = advanced_config.setdefault('compile_model', False)
    benchmark_mode = advanced_config.setdefault('benchmark_mode', False)
    memory_efficient = advanced_config.setdefault('memory_efficient', True)
    
    # Error handling defaults
    error_handling = error_config.setdefault('error_handling', 'strict')
    graceful_degradation = error_config.setdefault('graceful_degradation', True)
    
    logger.info("Starting model training pipeline...")
    
    # Initialize alive_bar context managers
    main_pbar = None
    main_pbar_context = None
    epoch_pbar = None
    epoch_pbar_context = None
    
    try:
        # Initialize training statistics
        training_stats = {
            'start_time': start_time.isoformat(),
            'config_applied': config,
            'model_type': model_type,
            'training_mode': 'comprehensive',
            'parameter_compatibility': 'full_autoencoder_class_compatibility'
        }
        
        # Set up reproducibility
        if reproducible:
            torch.manual_seed(random_seed)
            np.random.seed(random_seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(random_seed)
                torch.backends.cudnn.deterministic = True
                torch.backends.cudnn.benchmark = False
            logger.info(f"Set random seed to {random_seed} for reproducibility")
        elif benchmark_mode:
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            logger.info("Enabled benchmark mode for performance")
        
        # Device configuration
        if device == 'auto':
            if torch.cuda.is_available():
                device = torch.device('cuda')
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = torch.device('mps')
            else:
                device = torch.device('cpu')
        else:
            device = torch.device(device)
        
        training_stats['device'] = str(device)
        training_stats['device_type'] = device.type
        
        # Hardware optimization
        if device.type == 'cuda':
            training_stats['cuda_version'] = torch.version.cuda
            training_stats['gpu_count'] = torch.cuda.device_count()
            training_stats['gpu_name'] = torch.cuda.get_device_name() if torch.cuda.is_available() else None
        
        # Create directories
        model_dir.mkdir(parents=True, exist_ok=True)
        log_dir.mkdir(parents=True, exist_ok=True)
        tensorboard_dir.mkdir(parents=True, exist_ok=True)
        config_dir.mkdir(parents=True, exist_ok=True)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup experiment tracking
        timestamp = start_time.strftime("%Y%m%d_%H%M%S")
        run_id = f"train_{model_type}_{timestamp}"
        experiment_dir = tensorboard_dir / run_id
        experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize TensorBoard writer
        writer = None
        if tensorboard_logging:
            try:
                writer = SummaryWriter(log_dir=experiment_dir)
                logger.info(f"TensorBoard logging enabled: {experiment_dir}")
            except ImportError:
                logger.warning("TensorBoard not available, logging disabled")
                tensorboard_logging = False
        
        training_stats['run_id'] = run_id
        training_stats['experiment_dir'] = str(experiment_dir)
        
        logger.info("-"*40)
        logger.info("TRAINING CONFIGURATION")
        logger.info("-"*40)
        logger.info(f"Model Configuration:")
        logger.info(f"  Type: {model_type}")
        logger.info(f"  Encoding Dimension: {encoding_dim}")
        logger.info(f"  Hidden Dimensions: {hidden_dims}")
        logger.info(f"  Dropout Rates: {dropout_rates}")
        logger.info(f"  Activation: {activation}")
        logger.info(f"  Normalization: {normalization}")
        
        logger.info(f"Training Configuration:")
        logger.info(f"  Batch Size: {batch_size}")
        logger.info(f"  Epochs: {epochs}")
        logger.info(f"  Learning Rate: {learning_rate:.2e}")
        logger.info(f"  Weight Decay: {weight_decay:.2e}")
        logger.info(f"  Patience: {patience}")
        logger.info(f"  Mixed Precision: {mixed_precision}")
        logger.info(f"  Optimizer: {optimizer_type}")
        logger.info(f"  Scheduler: {scheduler_type}")
        
        logger.info(f"Data Configuration:")
        logger.info(f"  Use Real Data: {use_real_data}")
        logger.info(f"  Normal Samples: {normal_samples}")
        logger.info(f"  Attack Samples: {attack_samples}")
        logger.info(f"  Features: {features}")
        logger.info(f"  Validation Split: {validation_split}")
        
        logger.info(f"System Configuration:")
        logger.info(f"  Device: {device}")
        logger.info(f"  Model Directory: {model_dir}")
        logger.info(f"  Workers: {num_workers}")
        logger.info(f"  Pin Memory: {pin_memory}")
        logger.info(f"  Parameter Compatibility: Full Autoencoder Class Support")
        logger.info("-"*40)
        
        # Data preparation
        logger.info("PREPARING DATA")
        logger.info("-" * 40)
        
        data = None
        data_metadata = {}
        
        if use_real_data:
            try:
                logger.info("Loading real data...")
                data = load_and_validate_data(
                    data_path=data_config.get('data_path'),
                    artifacts_path=data_config.get('artifacts_path'),
                    silent=silent_mode,
                    config=config,
                    **{k: v for k, v in data_config.items() if k not in ['data_path', 'artifacts_path']}
                )
                
                # Update input dimension from real data
                actual_features = len(data["feature_names"])
                if input_dim is None:
                    input_dim = actual_features
                    model_config['input_dim'] = input_dim
                elif actual_features != input_dim:
                    logger.warning(f"Specified input_dim ({input_dim}) differs from data features ({actual_features})")
                    input_dim = actual_features
                    model_config['input_dim'] = input_dim
                
                data_metadata = data.get("metadata", {})
                logger.info(f"Loaded real data: {len(data['X_train'])} train, {len(data['X_val'])} val, {len(data['X_test'])} test samples")
                
            except Exception as e:
                logger.error(f"Failed to load real data: {e}")
                if graceful_degradation:
                    logger.warning("Falling back to synthetic data generation")
                    use_real_data = False
                else:
                    raise RuntimeError(f"Real data loading failed: {e}") from e
        
        if not use_real_data:
            try:
                logger.info("Generating synthetic data...")
                
                # Set input dimension if not specified
                if input_dim is None:
                    input_dim = features
                    model_config['input_dim'] = input_dim
                
                synthetic_params = {
                    'normal_samples': normal_samples,
                    'attack_samples': attack_samples,
                    'features': input_dim,
                    'validation_split': validation_split,
                    'random_state': random_seed,
                    'config': config
                }
                
                # Add synthetic-specific config
                if data_config.get('synthetic_generation'):
                    synthetic_params.update(data_config['synthetic_generation'])
                
                data = generate_synthetic_data(silent=silent_mode, **synthetic_params)
                data_metadata = data.get("metadata", {})
                logger.info(f"Generated synthetic data: {len(data['X_train'])} train, {len(data['X_val'])} val, {len(data['X_test'])} test samples")
                
            except Exception as e:
                logger.error(f"Synthetic data generation failed: {e}")
                raise RuntimeError(f"Data preparation failed: {e}") from e
        
        # Validate that we have data
        if data is None or not data.get('X_train', np.array([])).size:
            raise RuntimeError("No valid data available for training")
        
        # Update training statistics with data information
        training_stats.update({
            'data_source': 'real' if use_real_data else 'synthetic',
            'train_samples': len(data['X_train']),
            'val_samples': len(data['X_val']),
            'test_samples': len(data['X_test']),
            'features': len(data['feature_names']),
            'input_dim': input_dim,
            'data_metadata': data_metadata
        })
        
        logger.info(f"Data prepared successfully: {input_dim} features, {len(data['X_train'])} training samples")
        
        # Create dataloaders
        logger.info("CREATING DATA LOADERS")
        logger.info("-" * 40)
        
        try:
            dataloader_config = {
                'batch_size': batch_size,
                'num_workers': num_workers,
                'pin_memory': pin_memory,
                'persistent_workers': persistent_workers,
                'shuffle': True,
                'drop_last': False,
                'config': config
            }
            
            train_loader, val_loader, test_loader = create_dataloaders(
                data=data,
                silent=silent_mode,
                **dataloader_config
            )
            
            logger.info(f"Created dataloaders: train={len(train_loader)} batches, val={len(val_loader)} batches, test={len(test_loader)} batches")
            
        except Exception as e:
            logger.error(f"DataLoader creation failed: {e}")
            if graceful_degradation:
                # Create basic dataloaders as fallback
                train_dataset = TensorDataset(
                    torch.tensor(data['X_train'], dtype=torch.float32)
                )
                val_dataset = TensorDataset(
                    torch.tensor(data['X_val'], dtype=torch.float32)
                )
                test_dataset = TensorDataset(
                    torch.tensor(data['X_test'], dtype=torch.float32)
                )
                
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
                
                logger.warning("Using basic fallback dataloaders")
            else:
                raise RuntimeError(f"DataLoader creation failed: {e}") from e
        
        # Model initialization
        logger.info("INITIALIZING MODEL")
        logger.info("-" * 40)
        
        # Initialize model variants if needed
        if not globals().get('MODEL_VARIANTS'):
            initialize_model_variants(silent=False)
        
        try:
            # Model instantiation
            model = create_model_instance(
                model_type=model_type,
                input_dim=input_dim,
                config=config,
                preset=preset
            )
            
            # Move to device
            model = model.to(device)
            
            logger.info(f"Created {model_type} successfully using factory pattern")
            
        except Exception as e:
            logger.error(f"Model creation failed: {e}")
            
            if graceful_degradation:
                logger.warning("Attempting to create fallback SimpleAutoencoder")
                try:
                    fallback_model = SimpleAutoencoder(
                        input_dim=input_dim,
                        encoding_dim=min(encoding_dim, input_dim // 2),
                        config={'model': {'model_type': 'SimpleAutoencoder'}}
                    )
                    model = fallback_model.to(device)
                    model_type = 'SimpleAutoencoder'
                    logger.warning("Using fallback SimpleAutoencoder")
                except Exception as fallback_error:
                    raise RuntimeError(f"Model creation failed and fallback failed: {fallback_error}") from e
            else:
                raise RuntimeError(f"Model creation failed: {e}") from e
        
        # Model compilation if requested
        if compile_model and hasattr(torch, 'compile'):
            try:
                logger.info("Compiling model with torch.compile")
                model = torch.compile(model, mode='default')
                training_stats['model_compiled'] = True
            except Exception as e:
                logger.warning(f"Model compilation failed: {e}")
                training_stats['model_compiled'] = False
        
        # Log model information
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        model_size_mb = total_params * 4 / 1024 / 1024
        
        training_stats.update({
            'model_class': type(model).__name__,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': model_size_mb,
            'factory_pattern_used': True,
            'centralized_config_used': True
        })
        
        logger.info(f"Model: {type(model).__name__}")
        logger.info(f"Total parameters: {total_params:,}")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        logger.info(f"Model size: {model_size_mb:.2f} MB")
        logger.info(f"Factory Pattern: Enabled")
        logger.info(f"Centralized Config: Enabled")
        
        # Training components setup
        logger.info("SETTING UP TRAINING COMPONENTS")
        logger.info("-" * 40)
        
        # Optimizer setup
        optimizer_params = {
            'lr': learning_rate,
            'weight_decay': weight_decay
        }
        
        if optimizer_type.lower() == 'adam':
            if 'adam_betas' in training_config:
                optimizer_params['betas'] = training_config['adam_betas']
            if 'adam_eps' in training_config:
                optimizer_params['eps'] = training_config['adam_eps']
            optimizer = optim.Adam(model.parameters(), **optimizer_params)
        elif optimizer_type.lower() == 'adamw':
            if 'adam_betas' in training_config:
                optimizer_params['betas'] = training_config['adam_betas']
            if 'adam_eps' in training_config:
                optimizer_params['eps'] = training_config['adam_eps']
            optimizer = optim.AdamW(model.parameters(), **optimizer_params)
        elif optimizer_type.lower() == 'sgd':
            optimizer_params['momentum'] = training_config.get('momentum', 0.9)
            optimizer = optim.SGD(model.parameters(), **optimizer_params)
        else:
            logger.warning(f"Unknown optimizer '{optimizer_type}', using AdamW")
            optimizer = optim.AdamW(model.parameters(), **optimizer_params)
        
        # Scheduler setup
        scheduler = None
        if scheduler_type:
            scheduler_params_config = training_config.get('scheduler_params', {})
            
            if scheduler_type == 'ReduceLROnPlateau':
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer,
                    mode='min',
                    patience=scheduler_params_config.get('patience', training_config.get('lr_patience', 5)),
                    factor=scheduler_params_config.get('factor', training_config.get('lr_factor', 0.5)),
                    min_lr=scheduler_params_config.get('min_lr', training_config.get('min_lr', 1e-7))
                )
            elif scheduler_type == 'StepLR':
                scheduler = optim.lr_scheduler.StepLR(
                    optimizer,
                    step_size=scheduler_params_config.get('step_size', 30),
                    gamma=scheduler_params_config.get('gamma', 0.1)
                )
            elif scheduler_type == 'CosineAnnealingLR':
                scheduler = optim.lr_scheduler.CosineAnnealingLR(
                    optimizer,
                    T_max=scheduler_params_config.get('T_max', epochs),
                    eta_min=scheduler_params_config.get('eta_min', 1e-7)
                )
        
        # Loss function
        criterion = nn.MSELoss()
        
        # Mixed precision scaler
        scaler = None
        if mixed_precision and device.type == 'cuda':
            try:
                scaler = GradScaler()
                logger.info("Mixed precision training enabled")
            except Exception as e:
                logger.warning(f"Failed to initialize mixed precision: {e}")
                mixed_precision = False
        
        logger.info(f"Optimizer: {optimizer_type}")
        logger.info(f"Scheduler: {scheduler_type or 'None'}")
        logger.info(f"Loss function: MSE")
        logger.info(f"Mixed precision: {'Enabled' if mixed_precision else 'Disabled'}")
        
        # Training loop
        logger.info("-"*40)
        logger.info("STARTING TRAINING")
        logger.info("-"*40)
        
        # Training state
        best_val_loss = float('inf')
        patience_counter = 0
        training_history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': [],
            'epoch_times': [],
            'memory_usage': [],
            'detailed_metrics': {}
        }
        
        # Set up epoch progress bar with alive_bar
        if progress_bar:
            try:
                epoch_pbar_context = alive_bar(
                    total=epochs,
                    title='Training Epochs\t',
                    bar='smooth',
                    spinner='dots_waves2',
                    stats=True,
                    monitor=True,
                    elapsed=True,
                    stats_end=True
                )
                epoch_pbar = epoch_pbar_context.__enter__()
            except ImportError:
                logger.warning("alive-progress not available, epoch progress bar disabled")
                epoch_pbar = None
                epoch_pbar_context = None
            except Exception as e:
                logger.warning(f"Failed to initialize alive-progress epoch bar: {e}")
                epoch_pbar = None
                epoch_pbar_context = None
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            
            try:
                # Update epoch progress bar
                if epoch_pbar:
                    train_loss_history = f"{training_history['train_loss'][-1]:.6f}" if training_history['train_loss'] else "Initializing..."
                    epoch_pbar.text = (f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss_history}")
                
                # Training phase
                train_loss, train_metrics = train_epoch(
                    model=model,
                    loader=train_loader,
                    criterion=criterion,
                    optimizer=optimizer,
                    device=device,
                    epoch=epoch,
                    learning_rate=learning_rate,
                    gradient_clip=gradient_clip,
                    gradient_accumulation_steps=gradient_accumulation_steps,
                    mixed_precision=mixed_precision,
                    scaler=scaler,
                    scheduler=scheduler,
                    progress_bar=progress_bar and not epoch_pbar,
                    verbose=debug_mode,
                    config=config
                )
                
                # Validation phase
                val_loss, val_reconstruction_errors, val_metrics = validate(
                    model=model,
                    loader=val_loader,
                    criterion=criterion,
                    device=device,
                    epoch=epoch,
                    mixed_precision=mixed_precision,
                    calculate_metrics=True,
                    detailed_metrics=validation_config.get('detailed_metrics', False),
                    progress_bar=progress_bar and not epoch_pbar,
                    verbose=debug_mode,
                    config=config
                )
                
                # Update training history
                epoch_time = time.time() - epoch_start_time
                current_lr = optimizer.param_groups[0]['lr']
                
                training_history['train_loss'].append(train_loss)
                training_history['val_loss'].append(val_loss)
                training_history['learning_rate'].append(current_lr)
                training_history['epoch_times'].append(epoch_time)
                
                # Memory tracking
                if device.type == 'cuda':
                    memory_usage = torch.cuda.memory_allocated(device) / 1024**3  # GB
                    training_history['memory_usage'].append(memory_usage)
                
                if device.type == 'cpu':
                    process = psutil.Process()
                    memory_usage = process.memory_info().rss / 1024**3  #GB
                    training_history['memory_usage'].append(memory_usage)
                
                # Store detailed metrics
                if train_metrics:
                    for key, value in train_metrics.items():
                        if isinstance(value, (int, float)):
                            training_history['detailed_metrics'].setdefault(f'train_{key}', []).append(value)
                
                if val_metrics:
                    for key, value in val_metrics.items():
                        if isinstance(value, (int, float)):
                            training_history['detailed_metrics'].setdefault(f'val_{key}', []).append(value)
                
                # Learning rate scheduling
                if scheduler:
                    if scheduler_type == 'ReduceLROnPlateau':
                        scheduler.step(val_loss)
                    else:
                        scheduler.step()
                
                # TensorBoard logging
                if tensorboard_logging and writer:
                    writer.add_scalar("Loss/Train", train_loss, epoch)
                    writer.add_scalar("Loss/Validation", val_loss, epoch)
                    writer.add_scalar("Learning_Rate", current_lr, epoch)
                    writer.add_scalar("Epoch_Time", epoch_time, epoch)
                    
                    # Additional metrics
                    for key, value in train_metrics.items():
                        if isinstance(value, (int, float)):
                            writer.add_scalar(f"Train/{key}", value, epoch)
                    
                    for key, value in val_metrics.items():
                        if isinstance(value, (int, float)):
                            writer.add_scalar(f"Validation/{key}", value, epoch)
                    
                    if device.type == 'cuda':
                        writer.add_scalar("System/GPU_Memory_GB", memory_usage, epoch)
                
                # Console logging
                if epoch % log_frequency == 0 or epoch == epochs - 1:
                    memory_info = f"GPU: {memory_usage:.3f}GB" if device.type == 'cuda' and 'memory_usage' in locals() else f"RAM: {memory_usage:.3f}GB" if device.type == 'cpu' and 'memory_usage' in locals() else "Mem: N/A"
                    logger.info(f"Epoch {epoch+1:3d}/{epochs}, Train: {train_loss:.6f}, Val: {val_loss:.6f}, LR: {current_lr:.2e}, Time: {epoch_time:.1f}s, {memory_info}")
                
                # Model checkpointing and early stopping
                is_best = val_loss < best_val_loss
                if is_best:
                    best_val_loss = val_loss
                    patience_counter = 0
                    
                    # Save best model
                    if save_model:
                        best_model_path = model_dir / "best_model.pth"
                        torch.save({
                            'epoch': epoch,
                            'model_state_dict': model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                            'best_val_loss': best_val_loss,
                            'config': config,
                            'model_type': model_type,
                            'training_stats': training_stats
                        }, best_model_path)
                        
                        if epoch % (checkpoint_frequency * 5) == 0:
                            logger.info(f"New best model saved (epoch {epoch+1}, loss: {best_val_loss:.6f})")
                else:
                    patience_counter += 1
                
                # Early stopping check
                if early_stopping and patience_counter >= patience:
                    logger.info(f"Early stopping triggered at epoch {epoch+1} (patience: {patience})")
                    break
                
                # Periodic checkpointing
                if save_checkpoints and epoch % checkpoint_frequency == 0 and epoch > 0:
                    checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch+1}.pth"
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                        'val_loss': val_loss,
                        'training_history': training_history,
                        'config': config
                    }, checkpoint_path)
                
                # Update epoch progress bar
                if epoch_pbar:
                    epoch_pbar()
                    # Update the text with current metrics
                    epoch_pbar.text = (f"Epoch {epoch+1}/{epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | Best: {best_val_loss:.4f} | LR: {current_lr:.2e}")
                
            except Exception as e:
                logger.error(f"Training error at epoch {epoch+1}: {e}")
                if error_config.get('continue_on_error', False):
                    logger.warning(f"Continuing training after error: {e}")
                    continue
                else:
                    raise RuntimeError(f"Training failed at epoch {epoch+1}: {e}") from e
        
        # Close epoch progress bar
        if epoch_pbar_context:
            epoch_pbar_context.__exit__(None, None, None)
        
        total_training_time = time.time() - training_start_time
        final_epoch = epoch + 1
        
        training_stats.update({
            'total_training_time': total_training_time,
            'final_epoch': final_epoch,
            'best_val_loss': best_val_loss,
            'early_stopped': patience_counter >= patience,
            'training_history': training_history
        })
        
        logger.info(f"Training completed in {total_training_time/60:.1f} minutes ({final_epoch} epochs)")
        
        # Load best model for evaluation
        if save_model and (model_dir / "best_model.pth").exists():
            logger.info("Loading best model for final evaluation...")
            try:
                checkpoint = torch.load(model_dir / "best_model.pth", map_location=device, weights_only=False)
                model.load_state_dict(checkpoint['model_state_dict'])
                logger.info(f"Loaded best model from epoch {checkpoint['epoch']+1}")
            except Exception as e:
                logger.warning(f"Failed to load best model: {e}, using final model")
        
        # Calculate anomaly threshold
        logger.info("CALCULATING ANOMALY THRESHOLD")
        logger.info("-" * 40)
        
        try:
            threshold, threshold_metadata = calculate_threshold(
                model=model,
                loader=val_loader,
                percentile=percentile,
                device=device,
                threshold_method=threshold_method,
                adaptive_threshold=security_config.get('adaptive_threshold', True),
                config=config
            )
            
            logger.info(f"Anomaly threshold calculated: {threshold:.6f} (method: {threshold_method})")
            training_stats['threshold_data'] = {
                'threshold': threshold,
                'method': threshold_method,
                'metadata': threshold_metadata
            }
            
        except Exception as e:
            logger.error(f"Threshold calculation failed: {e}")
            if graceful_degradation:
                # Use fallback threshold
                threshold = np.percentile(val_reconstruction_errors, percentile) if 'val_reconstruction_errors' in locals() else 0.1
                threshold_metadata = {'method': 'fallback', 'error': str(e)}
                logger.warning(f"Using fallback threshold: {threshold:.6f}")
                training_stats['threshold_data'] = {
                    'threshold': threshold,
                    'method': 'fallback',
                    'metadata': threshold_metadata
                }
            else:
                raise RuntimeError(f"Threshold calculation failed: {e}") from e
        
        # Final evaluation on test set
        logger.info("FINAL EVALUATION")
        logger.info("-" * 40)
        
        try:
            test_loss, test_reconstruction_errors, test_metrics = validate(
                model=model,
                loader=test_loader,
                criterion=criterion,
                device=device,
                mixed_precision=mixed_precision,
                calculate_metrics=True,
                detailed_metrics=True,
                statistical_metrics=True,
                anomaly_detection=True,
                anomaly_threshold=threshold,
                progress_bar=progress_bar,
                config=config
            )
            
            # Calculate detection metrics
            if len(test_reconstruction_errors) > 0:
                anomaly_predictions = test_reconstruction_errors > threshold
                anomaly_rate = anomaly_predictions.mean()
                
                # Additional evaluation metrics
                mse_stats = {
                    'mean': float(np.mean(test_reconstruction_errors)),
                    'std': float(np.std(test_reconstruction_errors)),
                    'min': float(np.min(test_reconstruction_errors)),
                    'max': float(np.max(test_reconstruction_errors)),
                    'median': float(np.median(test_reconstruction_errors)),
                    'p95': float(np.percentile(test_reconstruction_errors, 95)),
                    'p99': float(np.percentile(test_reconstruction_errors, 99))
                }
            else:
                anomaly_rate = 0.0
                mse_stats = {}
            
            training_stats['final_evaluation'] = {
                'test_loss': test_loss,
                'test_metrics': test_metrics,
                'anomaly_rate': anomaly_rate,
                'mse_statistics': mse_stats,
                'reconstruction_errors_count': len(test_reconstruction_errors)
            }
            
            logger.info(f"Test Loss: {test_loss:.6f}")
            logger.info(f"Anomaly Detection Rate: {anomaly_rate:.2%}")
            logger.info(f"MSE Statistics: mean={mse_stats.get('mean', 0):.6f}, std={mse_stats.get('std', 0):.6f}")
            
        except Exception as e:
            logger.error(f"Final evaluation failed: {e}")
            training_stats['final_evaluation'] = {'error': str(e)}
            if not graceful_degradation:
                raise RuntimeError(f"Final evaluation failed: {e}") from e
        
        # Save model and artifacts
        logger.info("SAVING ARTIFACTS")
        logger.info("-" * 40)
        
        saved_artifacts = {}
        
        # Save final model using the model's own save method if available
        if save_model:
            try:
                final_model_path = model_dir / "autoencoder_model.pth"
                
                # Use model's save method if it has one
                if hasattr(model, 'save_model'):
                    model.save_model(str(final_model_path), include_config=True)
                    logger.info(f"Final model saved using model.save_model(): {final_model_path}")
                else:
                    torch.save(model.state_dict(), final_model_path)
                    logger.info(f"Final model saved: {final_model_path}")
                
                saved_artifacts['model_path'] = str(final_model_path)
                
            except Exception as e:
                logger.error(f"Failed to save model: {e}")
        
        # Save threshold data
        try:
            threshold_path = model_dir / "anomaly_threshold.pkl"
            threshold_data = {
                'threshold': threshold,
                'metadata': threshold_metadata,
                'mse_statistics': mse_stats if 'mse_stats' in locals() else {},
                'percentile': percentile,
                'method': threshold_method
            }
            joblib.dump(threshold_data, threshold_path)
            saved_artifacts['threshold_path'] = str(threshold_path)
            logger.info(f"Threshold data saved: {threshold_path}")
        except Exception as e:
            logger.error(f"Failed to save threshold data: {e}")
        
        # Export to ONNX if requested
        if export_onnx:
            try:
                # Get export configuration from config if available
                export_config_section = config.get('export', {}).get('onnx_export', {})
                
                # Call export_to_onnx function
                onnx_path = export_to_onnx(
                    model=model,
                    input_dim=input_dim,
                    device=device,
                    model_dir=model_dir,
                    opset_version=export_config_section.get('opset_version', 14),
                    config={
                        'system': {
                            'onnx_export': {
                                'max_ram_gb': export_config_section.get('max_ram_gb', 8),
                                'max_vram_gb': export_config_section.get('max_vram_gb', 2),
                                'chunk_size': export_config_section.get('chunk_size', min(128, input_dim)),
                                'runtime_validation': export_config_section.get('runtime_validation', True),
                                'validation_tolerance': export_config_section.get('validation_tolerance', 1e-5),
                                'strict_validation': export_config_section.get('strict_validation', False),
                                'dynamic_axes': export_config_section.get('dynamic_axes', {'input': {0: 'batch_size'}}),
                                'constant_folding': export_config_section.get('constant_folding', True),
                                'verbose': export_config_section.get('verbose', False),
                                'fail_silently': export_config_section.get('fail_silently', False)
                            }
                        }
                    }
                )

                if onnx_path:
                    saved_artifacts['onnx_path'] = str(onnx_path)
                    logger.info(f"ONNX model exported successfully: {onnx_path}")
                    
                    # Optionally validate the exported model
                    if export_config_section.get('validate_export', True):
                        try:
                            validation_result = validate_onnx_model(
                                model=model,
                                onnx_path=onnx_path,
                                device=device,
                                tolerance=export_config_section.get('validation_tolerance', 1e-5),
                                strict=export_config_section.get('strict_validation', False)
                            )
                            
                            if validation_result['status'] == 'passed':
                                logger.info("ONNX model validation passed")
                                saved_artifacts['onnx_validation'] = validation_result
                            else:
                                logger.warning(f"ONNX model validation {validation_result['status']}: {validation_result.get('error', 'unknown error')}")
                        except Exception as val_error:
                            logger.warning(f"ONNX model validation failed: {val_error}")
                else:
                    logger.warning("ONNX export completed but no model path returned")
                    
            except Exception as e:
                error_msg = f"ONNX export failed: {str(e)}"
                logger.warning(error_msg)
                saved_artifacts['onnx_export_error'] = error_msg
                
                if export_config_section.get('fail_silently', False):
                    logger.info("Continuing training process due to fail_silently=True")
                else:
                    raise RuntimeError(error_msg) from e
        
        # Save training metadata
        if save_training_history:
            try:
                history_path = model_dir / "training_history.pkl"
                joblib.dump(training_history, history_path)
                saved_artifacts['history_path'] = str(history_path)
                logger.info(f"Training history saved: {history_path}")
            except Exception as e:
                logger.error(f"Failed to save training history: {e}")
        
        # Save configuration used
        try:
            save_dir = config_dir if config_dir is not None else model_dir
            if isinstance(save_dir, str):
                save_dir = Path(save_dir)
            
            config_path = save_dir / "training_config.json"
            
            # Create a serializable copy of config to avoid circular references
            serializable_config = {}
            for key, value in config.items():
                if isinstance(value, (str, int, float, bool, type(None))):
                    serializable_config[key] = value
                elif isinstance(value, (list, tuple)):
                    serializable_config[key] = [
                        item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                        for item in value
                    ]
                elif isinstance(value, dict):
                    serializable_config[key] = {
                        str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                        for k, v in value.items()
                    }
                elif isinstance(value, (Path,)):
                    serializable_config[key] = str(value)
                else:
                    serializable_config[key] = str(value)
            
            with open(config_path, "w") as f:
                json.dump(serializable_config, f, indent=2)
            saved_artifacts['config_path'] = str(config_path)
            logger.info(f"Training configuration saved: {config_path}")
        except Exception as e:
            logger.error(f"Failed to save config: {e}")
        
        # Prepare final results with model class information
        final_results = {
            "success": True,
            "run_id": run_id,
            "timestamp": timestamp,
            "model_type": model_type,
            "training_time_minutes": total_training_time / 60,
            "final_metrics": {
                "best_validation_loss": best_val_loss,
                "test_loss": training_stats.get('final_evaluation', {}).get('test_loss', float('inf')),
                "anomaly_detection_rate": training_stats.get('final_evaluation', {}).get('anomaly_rate', 0.0),
                "threshold": threshold,
                "final_epoch": final_epoch
            },
            "model_info": {
                "type": model_type,
                "class_name": type(model).__name__,
                "parameters": total_params,
                "trainable_parameters": trainable_params,
                "size_mb": model_size_mb,
                "input_dim": input_dim,
                "encoding_dim": encoding_dim,
                "factory_pattern_used": True,
                "centralized_config_used": True,
                "parameter_compatibility": "full_autoencoder_class_support"
            },
            "data_info": {
                "source": 'real' if use_real_data else 'synthetic',
                "train_samples": len(data["X_train"]),
                "val_samples": len(data["X_val"]),
                "test_samples": len(data["X_test"]),
                "features": len(data["feature_names"])
            },
            "system_info": {
                "device": str(device),
                "device_type": device.type,
                "mixed_precision": mixed_precision,
                "pytorch_version": torch.__version__
            },
            "artifacts": saved_artifacts,
            "configuration": config,
            "training_stats": training_stats
        }
        
        # Save final results summary
        try:
            results_path = results_dir / "training_results.json"
            
            # Create a serializable copy of final_results to avoid circular references
            serializable_results = {}
            for key, value in final_results.items():
                if key == 'configuration':
                    # Handle configuration separately with the same serialization logic
                    serializable_config = {}
                    for config_key, config_value in value.items():
                        if isinstance(config_value, (str, int, float, bool, type(None))):
                            serializable_config[config_key] = config_value
                        elif isinstance(config_value, (list, tuple)):
                            serializable_config[config_key] = [
                                item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                                for item in config_value
                            ]
                        elif isinstance(config_value, dict):
                            serializable_config[config_key] = {
                                str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                                for k, v in config_value.items()
                            }
                        elif isinstance(config_value, (Path,)):
                            serializable_config[config_key] = str(config_value)
                        else:
                            serializable_config[config_key] = str(config_value)
                    serializable_results[key] = serializable_config
                elif key == 'training_stats':
                    # Handle training_stats with special care for complex objects
                    serializable_stats = {}
                    for stats_key, stats_value in value.items():
                        if stats_key == 'config_applied':
                            # Skip the full config in training_stats to avoid duplication
                            serializable_stats[stats_key] = 'config_saved_separately'
                        elif isinstance(stats_value, (str, int, float, bool, type(None))):
                            serializable_stats[stats_key] = stats_value
                        elif isinstance(stats_value, (list, tuple)):
                            serializable_stats[stats_key] = [
                                item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                                for item in stats_value
                            ]
                        elif isinstance(stats_value, dict):
                            serializable_stats[stats_key] = {
                                str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                                for k, v in stats_value.items()
                            }
                        elif isinstance(stats_value, (Path,)):
                            serializable_stats[stats_key] = str(stats_value)
                        else:
                            serializable_stats[stats_key] = str(stats_value)
                    serializable_results[key] = serializable_stats
                elif isinstance(value, (str, int, float, bool, type(None))):
                    serializable_results[key] = value
                elif isinstance(value, (list, tuple)):
                    serializable_results[key] = [
                        item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                        for item in value
                    ]
                elif isinstance(value, dict):
                    serializable_results[key] = {
                        str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                        for k, v in value.items()
                    }
                elif isinstance(value, (Path,)):
                    serializable_results[key] = str(value)
                else:
                    serializable_results[key] = str(value)
            
            with open(results_path, "w") as f:
                json.dump(serializable_results, f, indent=2)
            saved_artifacts['results_path'] = str(results_path)
            logger.info(f"Final results summary saved: {results_path}")
        except Exception as e:
            logger.error(f"Failed to save results summary: {e}")
        
        # Close TensorBoard writer
        if writer:
            try:
                writer.close()
                logger.debug("TensorBoard writer closed successfully")
            except Exception as e:
                logger.warning(f"Failed to close TensorBoard writer: {e}")
        
        # Log final summary
        logger.info("-"*40)
        logger.info("TRAINING COMPLETED SUCCESSFULLY")
        logger.info("-"*40)
        logger.info(f"Model Type: {model_type}")
        logger.info(f"Training Time: {total_training_time/60:.1f} minutes ({final_epoch} epochs)")
        logger.info(f"Best Validation Loss: {best_val_loss:.6f}")
        logger.info(f"Test Loss: {training_stats.get('final_evaluation', {}).get('test_loss', 'N/A')}")
        logger.info(f"Anomaly Threshold: {threshold:.6f}")
        logger.info(f"Anomaly Detection Rate: {training_stats.get('final_evaluation', {}).get('anomaly_rate', 0)*100:.1f}%")
        logger.info(f"Model Parameters: {total_params:,}")
        logger.info(f"Model Size: {model_size_mb:.3f} MB")
        logger.info(f"Device: {device}")
        logger.info(f"Mixed Precision: {mixed_precision}")
        logger.info(f"Factory Pattern: Enabled")
        logger.info(f"Centralized Config: Enabled")
        logger.info(f"Parameter Compatibility: Full Autoencoder Class Support")
        logger.info("Saved Artifacts:")
        for artifact_type, artifact_path in saved_artifacts.items():
            logger.info(f"  {artifact_type}: {artifact_path}")
        logger.info("-"*40)
        
        return final_results
    
    except (EOFError, KeyboardInterrupt):
        logger.warning("Training interrupted by user")
        
    except Exception as e:
        error_msg = f"Training pipeline failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Save error information
        error_info = {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "timestamp": start_time.isoformat() if 'start_time' in locals() else datetime.now().isoformat(),
            "training_interrupted": True,
            "run_id": locals().get('run_id', f"error_{datetime.now().strftime('%Y%m%d_%H%M%S')}"),
            "model_type": locals().get('model_type', 'unknown'),
            "configuration": locals().get('config', config or {}),
            "training_stats": locals().get('training_stats', {}),
            "system_info": {
                "device": str(locals().get('device', 'unknown')),
                "pytorch_version": torch.__version__,
                "cuda_available": torch.cuda.is_available(),
                "parameter_compatibility": "full_autoencoder_class_support",
                "factory_pattern_used": True,
                "centralized_config_used": True
            },
            "traceback": traceback.format_exc(),
            "partial_results": {}
        }
        
        # Add partial training results if available
        if 'training_history' in locals() and training_history.get('train_loss'):
            error_info["partial_results"] = {
                "epochs_completed": len(training_history['train_loss']),
                "best_val_loss": locals().get('best_val_loss', float('inf')),
                "training_history": training_history,
                "last_train_loss": training_history['train_loss'][-1] if training_history['train_loss'] else None,
                "last_val_loss": training_history['val_loss'][-1] if training_history['val_loss'] else None
            }
            logger.info(f"Partial training completed: {len(training_history['train_loss'])} epochs")
        
        # Add model information if available
        if 'model' in locals() and hasattr(locals()['model'], 'parameters'):
            try:
                total_params = sum(p.numel() for p in locals()['model'].parameters())
                error_info["model_info"] = {
                    "class_name": type(locals()['model']).__name__,
                    "total_parameters": total_params,
                    "model_size_mb": total_params * 4 / 1024 / 1024,
                    "factory_pattern_used": True,
                    "centralized_config_used": True
                }
            except Exception:
                error_info["model_info"] = {"error": "Could not extract model info"}
        
        # Add data information if available
        if 'data' in locals() and data:
            try:
                error_info["data_info"] = {
                    "train_samples": len(data.get('X_train', [])),
                    "val_samples": len(data.get('X_val', [])),
                    "test_samples": len(data.get('X_test', [])),
                    "features": len(data.get('feature_names', [])),
                    "data_source": 'real' if locals().get('use_real_data', False) else 'synthetic'
                }
            except Exception:
                error_info["data_info"] = {"error": "Could not extract data info"}
        
        # Save error information to file
        try:
            error_dir = Path(locals().get('results_dir', RESULTS_DIR))
            error_dir.mkdir(parents=True, exist_ok=True)
            error_path = error_dir / f"training_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            # Create serializable error info
            serializable_error_info = {}
            for key, value in error_info.items():
                if key == 'configuration':
                    # Skip complex configuration in error info to avoid circular references
                    serializable_error_info[key] = 'configuration_available_in_separate_file'
                elif key == 'training_stats':
                    # Handle training_stats with special care
                    serializable_stats = {}
                    for stats_key, stats_value in value.items():
                        if stats_key == 'config_applied':
                            serializable_stats[stats_key] = 'config_not_included_in_error_file'
                        elif isinstance(stats_value, (str, int, float, bool, type(None))):
                            serializable_stats[stats_key] = stats_value
                        elif isinstance(stats_value, (list, tuple)):
                            serializable_stats[stats_key] = [
                                item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                                for item in stats_value
                            ]
                        elif isinstance(stats_value, dict):
                            serializable_stats[stats_key] = {
                                str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                                for k, v in stats_value.items()
                            }
                        else:
                            serializable_stats[stats_key] = str(stats_value)
                    serializable_error_info[key] = serializable_stats
                elif isinstance(value, (str, int, float, bool, type(None))):
                    serializable_error_info[key] = value
                elif isinstance(value, (list, tuple)):
                    serializable_error_info[key] = [
                        item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                        for item in value
                    ]
                elif isinstance(value, dict):
                    serializable_error_info[key] = {
                        str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                        for k, v in value.items()
                    }
                else:
                    serializable_error_info[key] = str(value)
            
            with open(error_path, "w") as f:
                json.dump(serializable_error_info, f, indent=2)
            
            logger.error(f"Error information saved to: {error_path}")
            error_info["error_log_path"] = str(error_path)
            
        except Exception as save_error:
            logger.error(f"Failed to save error information: {save_error}")
            error_info["save_error"] = str(save_error)
        
        # Close progress bars if they exist
        try:
            if 'epoch_pbar_context' in locals() and locals()['epoch_pbar_context']:
                locals()['epoch_pbar_context'].__exit__(None, None, None)
        except Exception:
            pass
        
        # Close TensorBoard writer if it exists
        if 'writer' in locals() and locals()['writer']:
            try:
                locals()['writer'].close()
                logger.debug("Closed TensorBoard writer")
            except Exception as writer_error:
                logger.warning(f"Failed to close TensorBoard writer: {writer_error}")
        
        # Attempt graceful recovery if enabled
        if locals().get('graceful_degradation', True):
            logger.warning("Attempting graceful recovery and partial results return")
            
            try:
                # Prepare partial results if any training was completed
                partial_results = {
                    "success": False,
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "graceful_recovery": True,
                    "timestamp": error_info["timestamp"],
                    "run_id": error_info["run_id"],
                    "model_type": error_info["model_type"],
                    "configuration": error_info["configuration"],
                    "system_info": error_info["system_info"],
                    "error_log_path": error_info.get("error_log_path"),
                    "parameter_compatibility": "full_autoencoder_class_support"
                }
                
                # Add partial training results if available
                if error_info["partial_results"]:
                    partial_results.update({
                        "partial_training_completed": True,
                        "epochs_completed": error_info["partial_results"]["epochs_completed"],
                        "training_metrics": {
                            "best_val_loss": error_info["partial_results"]["best_val_loss"],
                            "last_train_loss": error_info["partial_results"]["last_train_loss"],
                            "last_val_loss": error_info["partial_results"]["last_val_loss"]
                        },
                        "training_history": error_info["partial_results"]["training_history"]
                    })
                    
                    # Save partial model if it exists
                    if 'model' in locals() and locals().get('model_dir'):
                        try:
                            partial_model_path = Path(locals()['model_dir']) / "partial_model_error_recovery.pth"
                            
                            # Use model's save method if available (from centralized config classes)
                            if hasattr(locals()['model'], 'save_model'):
                                locals()['model'].save_model(str(partial_model_path), include_config=True)
                                logger.info(f"Partial model saved using model.save_model(): {partial_model_path}")
                            else:
                                torch.save(locals()['model'].state_dict(), partial_model_path)
                                logger.info(f"Partial model saved: {partial_model_path}")
                            
                            partial_results["partial_model_path"] = str(partial_model_path)
                        except Exception as model_save_error:
                            logger.warning(f"Failed to save partial model: {model_save_error}")
                
                # Add model and data info if available
                if error_info.get("model_info"):
                    partial_results["model_info"] = error_info["model_info"]
                
                if error_info.get("data_info"):
                    partial_results["data_info"] = error_info["data_info"]
                
                # Save partial results
                try:
                    if locals().get('results_dir'):
                        partial_results_path = Path(locals()['results_dir']) / "partial_training_results.json"
                        
                        # Create serializable partial results
                        serializable_partial = {}
                        for key, value in partial_results.items():
                            if key == 'configuration':
                                # Skip complex configuration
                                serializable_partial[key] = 'configuration_not_included_in_partial_results'
                            elif key == 'training_history':
                                # Handle training_history carefully
                                serializable_history = {}
                                for history_key, history_value in value.items():
                                    if isinstance(history_value, (str, int, float, bool, type(None))):
                                        serializable_history[history_key] = history_value
                                    elif isinstance(history_value, (list, tuple)):
                                        serializable_history[history_key] = [
                                            item if isinstance(item, (str, int, float, bool, type(None))) else float(item) if isinstance(item, (np.number,)) else str(item)
                                            for item in history_value
                                        ]
                                    elif isinstance(history_value, dict):
                                        serializable_history[history_key] = {
                                            str(k): v if isinstance(v, (str, int, float, bool, type(None))) else float(v) if isinstance(v, (np.number,)) else str(v)
                                            for k, v in history_value.items()
                                        }
                                    else:
                                        serializable_history[history_key] = str(history_value)
                                serializable_partial[key] = serializable_history
                            elif isinstance(value, (str, int, float, bool, type(None))):
                                serializable_partial[key] = value
                            elif isinstance(value, (list, tuple)):
                                serializable_partial[key] = [
                                    item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                                    for item in value
                                ]
                            elif isinstance(value, dict):
                                serializable_partial[key] = {
                                    str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                                    for k, v in value.items()
                                }
                            else:
                                serializable_partial[key] = str(value)
                        
                        with open(partial_results_path, "w") as f:
                            json.dump(serializable_partial, f, indent=2)
                        partial_results["partial_results_path"] = str(partial_results_path)
                        logger.info(f"Partial results saved: {partial_results_path}")
                except Exception as partial_save_error:
                    logger.warning(f"Failed to save partial results: {partial_save_error}")
                
                logger.warning("Graceful recovery completed - returning partial results")
                return partial_results
                
            except Exception as recovery_error:
                logger.error(f"Graceful recovery failed: {recovery_error}")
                error_info["recovery_error"] = str(recovery_error)
        
        # If graceful recovery is disabled or failed, prepare error response
        error_response = {
            "success": False,
            "error": error_msg,
            "error_type": type(e).__name__,
            "timestamp": error_info["timestamp"],
            "run_id": error_info["run_id"],
            "model_type": error_info["model_type"],
            "configuration": error_info["configuration"],
            "system_info": error_info["system_info"],
            "error_details": error_info,
            "graceful_recovery_attempted": locals().get('graceful_degradation', True),
            "graceful_recovery_succeeded": False,
            "parameter_compatibility": "full_autoencoder_class_support"
        }
        
        # Add debugging information for development
        if locals().get('debug_mode', False):
            error_response["debug_info"] = {
                "local_variables": {k: str(v) for k, v in locals().items() if not k.startswith('_') and not callable(v)},
                "exception_context": {
                    "filename": traceback.extract_tb(e.__traceback__)[-1].filename,
                    "line_number": traceback.extract_tb(e.__traceback__)[-1].lineno,
                    "function_name": traceback.extract_tb(e.__traceback__)[-1].name
                }
            }
        
        logger.error("Training failed - returning error response")
        
        # Determine whether to raise exception or return error response
        error_handling_mode = locals().get('error_handling', 'strict')
        
        if error_handling_mode == 'return_error':
            return error_response
        elif error_handling_mode == 'continue':
            logger.warning("Error handling set to 'continue' - returning error response instead of raising")
            return error_response
        else:  # strict mode (default)
            raise RuntimeError(error_msg) from e
    
    finally:
        # Final cleanup operations
        try:
            # Close progress bars in finally block to ensure cleanup
            if 'epoch_pbar_context' in locals() and locals()['epoch_pbar_context']:
                locals()['epoch_pbar_context'].__exit__(None, None, None)
            
            # Memory cleanup
            if 'model' in locals():
                del locals()['model']
            
            # Clear CUDA cache if available
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.debug("Cleared CUDA cache")
            
            # Clear MPS cache if available
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                try:
                    torch.mps.empty_cache()
                    logger.debug("Cleared MPS cache")
                except AttributeError:
                    pass  # MPS cache clearing not available in all versions
            
            # Garbage collection
            gc.collect()
            logger.debug("Performed garbage collection")
            
        except Exception as cleanup_error:
            logger.warning(f"Cleanup operations failed: {cleanup_error}")
        
        # Log final cleanup message
        try:
            logger.debug("Training function cleanup completed")
        except Exception:
            # Silent fail for final logging attempt
            pass

def train_model_interactive(
    use_real_data: Optional[bool] = None,
    use_current_config: bool = False,
    preset: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None,
    non_interactive: bool = False,
    **kwargs
) -> Optional[Dict[str, Any]]:
    """Interactive model training setup with context display and error handling."""
    try:
        # Clear screen and show banner with config
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Extract configuration context with error handling
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu header with context
        #print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "INTERACTIVE AUTOENCODER TRAINING SETUP")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Active Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        
        if config:
            base_config = config.copy()
            print(Fore.GREEN + Style.BRIGHT + "\nUsing provided configuration as base:")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
        elif use_current_config:
            try:
                base_config = get_current_config() if 'get_current_config' in globals() else {}
                print(Fore.GREEN + Style.BRIGHT + "\nUsing current system configuration:")
                print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            except Exception as e:
                logger.warning(f"Failed to load current config: {e}")
                console.print(
                    Panel.fit(
                        #f"[bold yellow]Warning: Failed to load current config, using defaults: {str(e)}[/bold yellow]",
                        f"Failed to load current config, using defaults: {str(e)}",
                        title="WARNING",
                        style="bold red",
                        border_style="red",
                        padding=(1, 1),
                        box=box.ROUNDED
                    )
                )
                base_config = {}
        else:
            base_config = {}
            print(Fore.YELLOW + Style.BRIGHT + "\nUsing default configuration:")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
        
        if preset:
            try:
                if preset in globals().get('PRESET_CONFIGS', {}):
                    preset_config = globals()['PRESET_CONFIGS'][preset].copy()
                    for section, values in preset_config.items():
                        if section not in base_config:
                            base_config[section] = values
                        elif isinstance(values, dict):
                            base_config[section].update(values)
                    print(Fore.GREEN + Style.BRIGHT + f"\nApplied preset configuration: {preset}")
                    print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
                else:
                    print(Fore.RED + Style.BRIGHT + f"\nPreset '{preset}' not found, using base configuration:")
                    print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            except Exception as e:
                logger.warning(f"Failed to apply preset '{preset}': {e}")
                console.print(
                    Panel.fit(
                        #f"[bold yellow]Warning: Failed to apply preset '{preset}': {str(e)}[/bold yellow]",
                        f"Failed to apply preset '{preset}': {str(e)}",
                        title="WARNING",
                        style="bold red",
                        border_style="red",
                        padding=(1, 1),
                        box=box.ROUNDED
                    )
                )
        
        if non_interactive or use_current_config:
            
            # Display configuration summary with context
            model_config = base_config.get('model', {})
            training_config = base_config.get('training', {})
            data_config = base_config.get('data', {})
            hardware_config = base_config.get('hardware', {})
            system_config = base_config.get('system', {})
            
            # Apply defaults if not set
            model_type = model_config.get('model_type', 'EnhancedAutoencoder')
            epochs = training_config.get('epochs', DEFAULT_EPOCHS)
            batch_size = training_config.get('batch_size', DEFAULT_BATCH_SIZE)
            learning_rate = training_config.get('learning_rate', LEARNING_RATE)
            use_real_data_final = data_config.get('use_real_data', use_real_data if use_real_data is not None else False)
            mixed_precision = training_config.get('mixed_precision', torch.cuda.is_available())
            device_config = hardware_config.get('device', system_config.get('device', 'auto'))
            encoding_dim = model_config.get('encoding_dim', DEFAULT_ENCODING_DIM)
            hidden_dims = model_config.get('hidden_dims', HIDDEN_LAYER_SIZES)
            
            # Configuration display
            # print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
            # print(Fore.GREEN + Style.BRIGHT + "CONFIGURATION SUMMARY")
            # print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            
            # Core configuration
            print(Fore.YELLOW + Style.BRIGHT + "Core Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data Source: " + Fore.GREEN + f"{'Real Data' if use_real_data_final else 'Synthetic Data'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + f"{model_type}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training: " + Fore.GREEN + f"{epochs} epochs (~{_estimate_training_time(epochs, model_type)} min)")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + f"{batch_size}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + f"{learning_rate}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Mixed Precision: " + Fore.GREEN + f"{'Enabled' if mixed_precision else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Device: " + Fore.GREEN + f"{'GPU' if torch.cuda.is_available() else 'CPU'} ({device_config})")
            
            # Architecture details
            print(Fore.YELLOW + Style.BRIGHT + "\nArchitecture Details:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Encoding Dim: " + Fore.GREEN + f"{encoding_dim}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Hidden Dims: " + Fore.GREEN + f"{hidden_dims}")
            if model_type == 'AutoencoderEnsemble':
                num_models = model_config.get('num_models', 3)
                diversity_factor = model_config.get('diversity_factor', 0.3)
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Ensemble Size: " + Fore.GREEN + f"{num_models}")
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Diversity Factor: " + Fore.GREEN + f"{diversity_factor}")
            elif model_type == 'EnhancedAutoencoder':
                features = []
                if model_config.get('use_attention', True):
                    features.append("Attention")
                if model_config.get('residual_blocks', True):
                    features.append("Residual Blocks")
                if model_config.get('skip_connection', True):
                    features.append("Skip Connections")
                if features:
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Enhanced Features: " + Fore.GREEN + f"{', '.join(features)}")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Legacy Mode: " + Fore.GREEN + f"{model_config.get('legacy_mode', False)}")
            
            # Data configuration
            if use_real_data_final:
                data_path = data_config.get('data_path', 'Default')
                artifacts_path = data_config.get('artifacts_path', 'Default')
                print(Fore.YELLOW + Style.BRIGHT + "\nReal Data Configuration:")
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data Path: " + Fore.GREEN + f"{data_path}")
                if artifacts_path != 'Default':
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Artifacts Path: " + Fore.GREEN + f"{artifacts_path}")
            else:
                normal_samples = data_config.get('normal_samples', NORMAL_SAMPLES)
                attack_samples = data_config.get('attack_samples', ATTACK_SAMPLES)
                features = data_config.get('features', FEATURES)
                print(Fore.YELLOW + Style.BRIGHT + "\nSynthetic Data Configuration:")
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.GREEN + f"{normal_samples:,}")
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.GREEN + f"{attack_samples:,}")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Features: " + Fore.GREEN + f"{features}")
            
            # System configuration
            model_dir = system_config.get('model_dir', DEFAULT_MODEL_DIR)
            reproducible = system_config.get('reproducible', True)
            random_seed = system_config.get('random_seed', RANDOM_STATE)
            print(Fore.YELLOW + Style.BRIGHT + "\nSystem Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Directory: " + Fore.GREEN + f"{model_dir}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Reproducible: " + Fore.GREEN + f"{reproducible}")
            if reproducible:
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Random Seed: " + Fore.GREEN + f"{random_seed}")
            
            # Monitoring configuration
            monitoring_config = base_config.get('monitoring', {})
            tensorboard_logging = monitoring_config.get('tensorboard_logging', True)
            save_checkpoints = monitoring_config.get('save_checkpoints', True)
            verbose_mode = monitoring_config.get('verbose', True)
            print(Fore.YELLOW + Style.BRIGHT + "\nMonitoring Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ TensorBoard Logging: " + Fore.GREEN + f"{'Enabled' if tensorboard_logging else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Save Checkpoints: " + Fore.GREEN + f"{'Enabled' if save_checkpoints else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Verbose Output: " + Fore.GREEN + f"{'Enabled' if verbose_mode else 'Disabled'}")
            
            # Export configuration
            export_config = base_config.get('export', {})
            save_model = export_config.get('save_model', True)
            save_metadata = export_config.get('save_metadata', True)
            save_training_history = export_config.get('save_training_history', True)
            export_onnx = export_config.get('export_onnx', False)
            print(Fore.YELLOW + Style.BRIGHT + "\nExport Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Save Model: " + Fore.GREEN + f"{'Enabled' if save_model else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Save Metadata: " + Fore.GREEN + f"{'Enabled' if save_metadata else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Save Training History: " + Fore.GREEN + f"{'Enabled' if save_training_history else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Export ONNX: " + Fore.GREEN + f"{'Enabled' if export_onnx else 'Disabled'}")
            
            # Security configuration
            security_config = base_config.get('security', {})
            percentile = security_config.get('percentile', DEFAULT_PERCENTILE)
            threshold_method = security_config.get('anomaly_threshold_strategy', 'percentile')
            adaptive_threshold = security_config.get('adaptive_threshold', True)
            print(Fore.YELLOW + Style.BRIGHT + "\nSecurity Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Anomaly Threshold: " + Fore.GREEN + f"{percentile}th percentile")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Threshold Method: " + Fore.GREEN + f"{threshold_method}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Adaptive Threshold: " + Fore.GREEN + f"{'Enabled' if adaptive_threshold else 'Disabled'}")
            
            # Confirmation prompt
            print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart training with these settings? (Y/n/c to cancel): " + Style.RESET_ALL).strip().lower()
            
            if confirm in ('', 'y', 'yes'):
                print(Fore.GREEN + Style.BRIGHT + "\nLaunching training with configured defaults...")
                return _launch_training_with_config(config=base_config, **kwargs)
            elif confirm in ('c', 'cancel'):
                print(Fore.RED + Style.BRIGHT + "\nTraining cancelled by user")
                return None
            else:
                # Fallback options
                console.print(
                    Panel.fit(
                        "Would you like to switch setup mode?",
                        style="bold yellow",
                        border_style="yellow",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                print(Fore.WHITE + Style.BRIGHT + "\n1. Switch to interactive setup")
                print(Fore.WHITE + Style.BRIGHT + "2. Try again with current settings")
                print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
                
                while True:
                    try:
                        choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-2): " + Style.RESET_ALL).strip()
                        if choice in ['1', '2', '0']:
                            break
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                        return None
                
                if choice == '1':
                    print(Fore.GREEN + Style.BRIGHT + "\nSwitching to interactive setup...")
                    # Fall through to interactive mode
                    pass
                elif choice == '2':
                    print(Fore.GREEN + Style.BRIGHT + "\nRetrying with current settings...")
                    return train_model_interactive(
                        use_real_data=use_real_data,
                        use_current_config=use_current_config,
                        preset=preset,
                        config=config,
                        non_interactive=non_interactive,
                        **kwargs
                    )
                else:
                    print(Fore.RED + Style.BRIGHT + "\nCancelled by user")
                    return None
        
        # Interactive mode continues
        print(Fore.YELLOW + Style.BRIGHT + "\nSetup includes:")
        print(Fore.GREEN + Style.BRIGHT + "  ├─ Data source selection (real/synthetic)")
        print(Fore.GREEN + Style.BRIGHT + "  ├─ Model architecture configuration") 
        print(Fore.GREEN + Style.BRIGHT + "  ├─ Training parameters setup")
        print(Fore.GREEN + Style.BRIGHT + "  ├─ System and monitoring options")
        print(Fore.GREEN + Style.BRIGHT + "  └─ Export and saving preferences")
        
        print(Fore.YELLOW + Style.BRIGHT + "\nQuick Start Options:")
        print(Fore.WHITE + Style.BRIGHT + "1. Express Setup " + Fore.GREEN + Style.BRIGHT + "(recommended defaults)")
        print(Fore.WHITE + Style.BRIGHT + "2. Custom Configuration " + Fore.GREEN + Style.BRIGHT + "(full control)")
        print(Fore.WHITE + Style.BRIGHT + "3. Use Preset Configuration " + Fore.GREEN + Style.BRIGHT + f"(available: {len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'})")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        while True:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-3): " + Style.RESET_ALL).strip()
                if choice in ['1', '2', '3', '0']:
                    break
                print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                return None
        
        if choice == '1':
            print(Fore.GREEN + Style.BRIGHT + "\nEXPRESS SETUP")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            return _interactive_express_setup(base_config, use_real_data, **kwargs)
        elif choice == '2':
            print(Fore.GREEN + Style.BRIGHT + "\nCUSTOM CONFIGURATION")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            return _interactive_custom_setup(base_config, use_real_data, **kwargs)
        elif choice == '3':
            print(Fore.GREEN + Style.BRIGHT + "\nPRESET SELECTION")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            return _interactive_preset_setup(base_config, use_real_data, **kwargs)
        elif choice == '0':
            print(Fore.RED + Style.BRIGHT + "Training cancelled by user")
            return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\n\nTraining setup interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"Interactive training setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during interactive training setup: {str(e)}\n"
            f"Context:\n"
            f"- Use Real Data: {use_real_data}\n"
            f"- Use Current Config: {use_current_config}\n"
            f"- Preset: {preset}\n"
            f"- Non-interactive: {non_interactive}\n\n"
            f"This could be due to:\n"
            f"- Configuration file corruption\n"
            f"- Missing dependencies\n"
            f"- System resource issues\n"
            f"- Invalid parameter combinations"
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="TRAINING SETUP ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        return None

def _interactive_express_setup(
    base_config: Dict[str, Any],
    use_real_data: Optional[bool],
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Interactive express setup for quick model training configuration.
    
    Provides a streamlined setup process with smart defaults while maintaining
    full compatibility with the centralized configuration system.
    """
    try:
        # Clear screen and show banner
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if base_config is None and banner_config is not None:
            base_config = banner_config
        else:
            base_config = base_config or {}
        
        # Extract context for display
        preset_name = "Custom/Default"
        model_type = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = base_config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = base_config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
            current_model_type = model_type
        
        # Clear screen and show context
        print(Fore.MAGENTA + Style.BRIGHT + "EXPRESS SETUP - QUICK CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        print(Fore.YELLOW + Style.BRIGHT + f"Base Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Model: " + Fore.CYAN + Style.BRIGHT + f"{current_model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Mode: " + Fore.CYAN + Style.BRIGHT + f"Express Setup")
        
        # Data Source Selection
        if use_real_data is None:
            print(Fore.MAGENTA + Style.BRIGHT + "\nDATA SOURCE SELECTION")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            print(Fore.WHITE + Style.BRIGHT + "1. Real network data " + Fore.GREEN + Style.BRIGHT + "(recommended for production)")
            print(Fore.WHITE + Style.BRIGHT + "2. Synthetic data " + Fore.GREEN + Style.BRIGHT + "(good for testing and development)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            while True:
                try:
                    data_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect data source (0-2): " + Style.RESET_ALL).strip()
                    if data_choice in ['1', '2', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                    return None
            
            if data_choice == '0':
                print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                return None
                
            use_real_data = data_choice == '1'
            print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {'Real Data' if use_real_data else 'Synthetic Data'}")
        
        # Model Architecture Selection
        print(Fore.MAGENTA + Style.BRIGHT + "\nMODEL ARCHITECTURE SELECTION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + "1. EnhancedAutoencoder " + Fore.GREEN + Style.BRIGHT + "(recommended - advanced features, good balance)")
        print(Fore.WHITE + Style.BRIGHT + "2. SimpleAutoencoder " + Fore.GREEN + Style.BRIGHT + "(fast and lightweight, minimal resources)")  
        print(Fore.WHITE + Style.BRIGHT + "3. AutoencoderEnsemble " + Fore.GREEN + Style.BRIGHT + "(best accuracy, slower, more resources)")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        while True:
            try:
                model_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect model type (0-3): " + Style.RESET_ALL).strip()
                if model_choice in ['1', '2', '3', '0']:
                    break
                print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nModel selection cancelled")
                return None
        
        if model_choice == '0':
            print(Fore.RED + Style.BRIGHT + "\nModel selection cancelled")
            return None
            
        model_types = ['EnhancedAutoencoder', 'SimpleAutoencoder', 'AutoencoderEnsemble']
        model_type = model_types[int(model_choice)-1]
        print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {model_type}")
        
        # Training Duration Selection
        print(Fore.MAGENTA + Style.BRIGHT + "\nTRAINING DURATION SELECTION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + "1. Quick Test " + Fore.GREEN + Style.BRIGHT + "(10 epochs - 1-3 minutes)")
        print(Fore.WHITE + Style.BRIGHT + "2. Standard " + Fore.GREEN + Style.BRIGHT + "(50 epochs - 5-15 minutes)")
        print(Fore.WHITE + Style.BRIGHT + "3. Thorough " + Fore.GREEN + Style.BRIGHT + "(100 epochs - 15-30 minutes)")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        while True:
            try:
                duration_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect training duration (0-3): " + Style.RESET_ALL).strip()
                if duration_choice in ['1', '2', '3', '0']:
                    break
                print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nDuration selection cancelled")
                return None
        
        if duration_choice == '0':
            print(Fore.RED + Style.BRIGHT + "\nDuration selection cancelled")
            return None
            
        epochs_map = {'1': 10, '2': 50, '3': 100}
        epochs = epochs_map.get(duration_choice, 50)
        print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {epochs} epochs")
        
        # Performance Configuration
        print(Fore.MAGENTA + Style.BRIGHT + "\nPERFORMANCE CONFIGURATION")
        (Fore.CYAN + Style.BRIGHT + "-" * 40)
        mixed_precision = torch.cuda.is_available()
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name() if torch.cuda.is_available() else "Unknown"
            print(Fore.GREEN + Style.BRIGHT + f"GPU detected: {gpu_name}")
            print(Fore.GREEN + Style.BRIGHT + "  Using mixed precision for faster training")
        else:
            print(Fore.MAGENTA + Style.BRIGHT + "CPU mode: Standard precision")
            print(Fore.MAGENTA + Style.BRIGHT + "  └─ Consider using GPU for better performance")
        
        # Build final configuration with smart defaults
        final_config = base_config.copy()
        
        # Model Configuration - using same structure as centralized config
        model_config = final_config.setdefault('model', {})
        model_config.update({
            'model_type': model_type,
            'activation': 'leaky_relu',
            'activation_param': 0.2,
            'normalization': 'batch',
            'use_batch_norm': True,
            'use_layer_norm': False,
            'bias': True,
            'weight_init': 'xavier_uniform',
            'skip_connection': True,
            # Enhanced features for better performance
            'use_attention': model_type != 'SimpleAutoencoder',
            'residual_blocks': model_type != 'SimpleAutoencoder',
            'legacy_mode': False
        })
        
        # Model-specific optimizations
        if model_type == 'SimpleAutoencoder':
            model_config.update({
                'encoding_dim': 16,
                'hidden_dims': [128, 64],
                'dropout_rates': [0.2, 0.15],
            })
        elif model_type == 'EnhancedAutoencoder':
            model_config.update({
                'encoding_dim': 32,
                'hidden_dims': [256, 128, 64],
                'dropout_rates': [0.2, 0.15, 0.1],
            })
        elif model_type == 'AutoencoderEnsemble':
            model_config.update({
                'encoding_dim': 24,
                'hidden_dims': [192, 96, 48],
                'dropout_rates': [0.25, 0.2, 0.15],
                'num_models': 3,
                'diversity_factor': 0.3,
            })
        
        # Training Configuration - aligned with train_model parameters
        training_config = final_config.setdefault('training', {})
        training_config.update({
            'epochs': epochs,
            'batch_size': 64,
            'learning_rate': 0.001,
            'patience': min(15, max(5, epochs // 3)),  # Adaptive patience
            'mixed_precision': mixed_precision,
            'optimizer': 'AdamW',
            'scheduler': 'ReduceLROnPlateau',
            'early_stopping': True,
            'validation_split': 0.2,
            'weight_decay': 1e-4,
            'adam_betas': (0.9, 0.999),
            'adam_eps': 1e-8,
            'gradient_clip': 1.0,
            'gradient_accumulation_steps': 1,
            'num_workers': 4 if torch.cuda.is_available() else 2,
            'shuffle': True,
            'pin_memory': torch.cuda.is_available(),
            'persistent_workers': True,
            'lr_patience': 5,
            'lr_factor': 0.5,
            'min_lr': 1e-6
        })
        
        # Data Configuration - consistent with interactive_main context
        data_config = final_config.setdefault('data', {})
        data_config.update({
            'use_real_data': use_real_data,
            'features': 20 if not use_real_data else None,
            'normal_samples': 8000 if not use_real_data else None,
            'attack_samples': 2000 if not use_real_data else None,
            'test_split': 0.2,
            'random_state': 42,
            'stratified_split': True,
            'data_normalization': 'standard',
            'anomaly_factor': 0.1,
            'data_preprocessing': True,
            'synthetic_generation': {
                'complexity': 'medium',
                'noise_level': 0.05,
                'correlation_strength': 0.3
            } if not use_real_data else {},
            'preprocessing': {
                'enabled': True,
                'feature_scaling': True,
                'outlier_handling': 'clip'
            }
        })
        
        # System Configuration - using same defaults as train_model_interactive
        system_config = final_config.setdefault('system', {})
        system_config.update({
            'reproducible': True,
            'random_seed': 42,
            'non_interactive': False,
            'model_dir': DEFAULT_MODEL_DIR,
            'log_dir': LOG_DIR,
            'config_dir': CONFIG_DIR,
            'data_dir': DATA_DIR,
            'checkpoint_dir': CHECKPOINTS_DIR,
            'results_dir': RESULTS_DIR,
            'debug': False,
            'parallel_processing': False,
            'max_workers': 4,
            'export_onnx': False,
            'cuda_optimizations': torch.cuda.is_available(),
            'onnx_export': {},
            'distributed_training': False,
            'python_executable': sys.executable,
            'working_directory': os.getcwd(),
            'environment_health': 'auto'
        })
        
        # Hardware Configuration - enhanced detection
        hardware_config = final_config.setdefault('hardware', {})
        hardware_config.update({
            'device': 'auto',
            'cuda_optimizations': torch.cuda.is_available(),
            'memory_management': {'enable_memory_efficient': True},
            'recommended_gpu_memory': 4.0,
            'minimum_system_requirements': {},
            'optimal_system_requirements': {},
            'performance_optimization': {},
            'detected_gpu_memory': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else None,
            'detected_system_memory': psutil.virtual_memory().total / 1e9 if 'psutil' in sys.modules else None,
            'system_performance_class': 'auto',
            'optimization_recommendations': []
        })
        
        # Monitoring Configuration - comprehensive logging
        monitoring_config = final_config.setdefault('monitoring', {})
        monitoring_config.update({
            'verbose': True,
            'debug_mode': False,
            'tensorboard_logging': True,
            'save_checkpoints': True,
            'save_best_model': True,
            'metrics_to_track': ['loss', 'reconstruction_error', 'learning_rate'],
            'checkpoint_frequency': max(10, epochs // 5),
            'log_frequency': 1,
            'metrics_frequency': 1,
            'console_logging_level': 'INFO',
            'save_model_history': True,
            'early_stopping_metric': 'val_loss',
            'checkpoint_format': 'pth',
            'log_model_summary': True,
            'tensorboard_dir': TB_DIR,
            'tensorboard': {},
            'stability_metrics': True,
            'performance_metrics': True,
            'profiling_enabled': False,
            'progress_bar': True
        })
        
        # Export Configuration
        export_config = final_config.setdefault('export', {})
        export_config.update({
            'save_model': True,
            'save_metadata': True,
            'save_training_history': True,
            'export_onnx': False
        })
        
        # Security Configuration - using same structure as main functions
        security_config = final_config.setdefault('security', {})
        security_config.update({
            'percentile': 95.0,
            'enable_security_metrics': True,
            'anomaly_threshold_strategy': 'percentile',
            'adaptive_threshold': True,
            'attack_threshold': None,
            'false_negative_cost': None,
            'early_warning_threshold': None,
            'confidence_interval': None,
            'detection_methods': ['reconstruction_error'],
            'alert_levels': ['low', 'medium', 'high'],
            'threshold_validation': True,
            'robust_detection': True,
            'false_positive_tolerance': None,
            'performance_optimized_detection': True,
            'real_time_monitoring': False,
            'ensemble_voting': 'average' if model_type == 'AutoencoderEnsemble' else 'single',
            'uncertainty_threshold': None
        })
        
        # Advanced Training Configuration
        advanced_config = final_config.setdefault('advanced_training', {})
        advanced_config.update({
            'memory_efficient': True,
            'compile_model': False,
            'benchmark_mode': False,
            'gradient_checkpointing': False
        })
        
        # Presets Configuration - maintain context
        presets_config = final_config.setdefault('presets', {})
        presets_config.update({
            'available_presets': list(globals().get('PRESET_CONFIGS', {}).keys()),
            'current_preset': 'express_setup',
            'current_override': None,
            'override_rules': {},
            'preset_configs': {},
            'custom_presets_available': [],
            'auto_apply': False,
            'validate_compatibility': True,
            'system_recommended_preset': None,
            'preset_compatibility': {}
        })
        
        # Runtime Configuration - track express setup
        runtime_config = final_config.setdefault('runtime', {})
        runtime_config.update({
            'config_loaded_at': datetime.now().isoformat(),
            'config_source': 'interactive_express_setup',
            'runtime_id': f"express_{int(time.time())}",
            'process_id': os.getpid(),
            'system_analysis_completed': True,
            'system_performance_score': None,
            'system_class': 'express',
            'optimizations_applied': {
                'mixed_precision': mixed_precision,
                'memory_efficient': True,
                'adaptive_patience': True
            },
            'resource_status': {
                'gpu_available': torch.cuda.is_available(),
                'mixed_precision_enabled': mixed_precision
            },
            'system_warnings': [],
            'recommendations': [
                "Express setup optimized for quick results",
                f"Using {model_type} with {epochs} epochs",
                "Monitor training progress in TensorBoard"
            ],
            'configuration_health': {
                'status': 'healthy',
                'checks_passed': True,
                'express_optimized': True
            }
        })
        
        # Display comprehensive configuration summary
        print(Fore.MAGENTA + Style.BRIGHT + "EXPRESS SETUP - CONFIGURATION SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Core configuration display
        print(Fore.YELLOW + Style.BRIGHT + "Core Configuration:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Data Source: " + Fore.GREEN + f"{'Real Data' if use_real_data else 'Synthetic Data'}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Training Duration: " + Fore.GREEN + f"{epochs} epochs (~{_estimate_training_time(epochs, model_type)} min)")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + f"64")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + f"0.001")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Mixed Precision: " + Fore.GREEN + f"{'Enabled' if mixed_precision else 'Disabled'}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Device: " + Fore.GREEN + f"{'GPU' if torch.cuda.is_available() else 'CPU'}")
        
        # Architecture details
        print(Fore.YELLOW + Style.BRIGHT + "\nArchitecture Details:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Encoding Dimension: " + Fore.GREEN + f"{model_config['encoding_dim']}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Hidden Layers: " + Fore.GREEN + f"{model_config['hidden_dims']}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Dropout Rates: " + Fore.GREEN + f"{model_config['dropout_rates']}")
        if model_type == 'AutoencoderEnsemble':
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Ensemble Size: " + Fore.GREEN + f"{model_config['num_models']}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Diversity Factor: " + Fore.GREEN + f"{model_config['diversity_factor']}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Attention Mechanism: " + Fore.GREEN + f"{'Enabled' if model_config.get('use_attention', False) else 'Disabled'}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Residual Blocks: " + Fore.GREEN + f"{'Enabled' if model_config.get('residual_blocks', False) else 'Disabled'}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Skip Connections: " + Fore.GREEN + f"{'Enabled' if model_config.get('skip_connection', False) else 'Disabled'}")
        
        # Data configuration
        if use_real_data:
            data_path = data_config.get('data_path', 'Default')
            print(Fore.YELLOW + Style.BRIGHT + "\nReal Data Configuration:")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Data Path: " + Fore.GREEN + f"{data_path}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preprocessing: " + Fore.GREEN + f"Enabled")
            print(Fore.WHITE + Style.BRIGHT + f"  └─ Normalization: " + Fore.GREEN + f"Standard")
        else:
            print(Fore.YELLOW + Style.BRIGHT + "\nSynthetic Data Configuration:")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.GREEN + f"{data_config.get('normal_samples', 0):,}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.GREEN + f"{data_config.get('attack_samples', 0):,}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Features: " + Fore.GREEN + f"{data_config.get('features', 0)}")
            print(Fore.WHITE + Style.BRIGHT + f"  └─ Anomaly Factor: " + Fore.GREEN + f"{data_config.get('anomaly_factor', 0)}")
        
        # System configuration
        print(Fore.YELLOW + Style.BRIGHT + "\nSystem Configuration:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Reproducible: " + Fore.GREEN + f"{system_config.get('reproducible', True)}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Workers: " + Fore.GREEN + f"{training_config.get('num_workers', 0)}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ TensorBoard: " + Fore.GREEN + f"{'Enabled' if monitoring_config.get('tensorboard_logging', False) else 'Disabled'}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Checkpoints: " + Fore.GREEN + f"{'Enabled' if monitoring_config.get('save_checkpoints', False) else 'Disabled'}")
        
        # Confirmation with enhanced styling
        try:
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart training with these express settings? (Y/n/c to cancel): " + Style.RESET_ALL).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print(Fore.RED + Style.BRIGHT + "\nConfiguration cancelled by user")
            return None
        
        if confirm in ('', 'y', 'yes'):
            print(Fore.GREEN + Style.BRIGHT + "\nLaunching training with express configuration...")
            return _launch_training_with_config(final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print(Fore.RED + Style.BRIGHT + "\nTraining cancelled")
            return None
        else:
            # Enhanced fallback options with styling
            print(Fore.YELLOW + Style.BRIGHT + "\nWould you like to?")
            print(Fore.WHITE + Style.BRIGHT + "1. Try express setup again with different settings")
            print(Fore.WHITE + Style.BRIGHT + "2. Switch to custom configuration for full control")
            print(Fore.RED + Style.BRIGHT + "0. Return to previous menu")
            
            while True:
                try:
                    retry_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-2): " + Style.RESET_ALL).strip()
                    if retry_choice in ['1', '2', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                    return None
            
            if retry_choice == '1':
                print(Fore.GREEN + Style.BRIGHT + "\nRestarting express setup...")
                return _interactive_express_setup(base_config, use_real_data, **kwargs)
            elif retry_choice == '2':
                print(Fore.GREEN + Style.BRIGHT + "\nSwitching to custom configuration...")
                return _interactive_custom_setup(base_config, use_real_data, **kwargs)
            else:
                print(Fore.RED + Style.BRIGHT + "\nReturning to previous menu")
                return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\nExpress setup interrupted by user")
        return None
    except Exception as e:
        logger.error(f"Express setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during express setup: {str(e)}\n"
            f"Context:\n"
            f"- Use Real Data: {use_real_data}\n"
            f"- Base Config: {bool(base_config)}\n\n"
            f"This could be due to:\n"
            f"- Configuration structure issues\n"
            f"- Missing required parameters\n"
            f"- System resource constraints\n"
            f"- Invalid user input handling"
        )
        print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
        print(Fore.RED + Style.BRIGHT + "EXPRESS SETUP ERROR")
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + message)
        print(Fore.RED + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        return None

def _interactive_preset_setup(
    base_config: Dict[str, Any],
    use_real_data: Optional[bool],
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Interactive preset configuration setup with comprehensive context integration.
    
    Provides a streamlined preset selection experience while maintaining full
    compatibility with the centralized configuration system.
    """
    try:
        # Clear screen and show banner for consistency
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if base_config is None and banner_config is not None:
            base_config = banner_config
        else:
            base_config = base_config or {}
        
        # Extract context for display
        preset_name = "Custom/Default"
        model_type = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = base_config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
            current_preset = preset_name
        
        # Extract model type
        model_section = base_config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
            current_model_type = model_type
        
        # Clear screen and show context
        print(Fore.MAGENTA + Style.BRIGHT + "PRESET CONFIGURATION SETUP")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        print(Fore.YELLOW + Style.BRIGHT + f"Base Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.YELLOW + Style.BRIGHT + f"{current_preset}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Model: " + Fore.YELLOW + Style.BRIGHT + f"{current_model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Mode: " + Fore.YELLOW + Style.BRIGHT + f"Preset Selection")
        
        print(Fore.YELLOW + Style.BRIGHT + "\nAvailable Preset Configurations:\n")
        
        available_presets = list(globals().get('PRESET_CONFIGS', {}).keys())
        
        if not available_presets:
            print(Fore.RED + Style.BRIGHT + "\nNo presets available! Using express setup instead.")
            return _interactive_express_setup(base_config, use_real_data, **kwargs)
        
        # Display available presets with enhanced formatting
        for i, preset_name in enumerate(available_presets, 1):
            preset_config = globals()['PRESET_CONFIGS'][preset_name]
            metadata = preset_config.get('metadata', {})
            description = metadata.get('description', 'No description available')
            recommended_use = metadata.get('recommended_use', 'General purpose')
            model_type = preset_config.get('model', {}).get('model_type', 'Unknown')
            
            print(Fore.WHITE + Style.BRIGHT + f"{i}. {preset_name}\n")
            print(Fore.CYAN + Style.BRIGHT + f"   Model: " + Fore.GREEN + Style.BRIGHT + f"{model_type}")
            print(Fore.CYAN + Style.BRIGHT + f"   Description: " + Fore.MAGENTA + Style.BRIGHT + f"{description}")
            print(Fore.CYAN + Style.BRIGHT + f"   Best for: " + Fore.YELLOW + Style.BRIGHT + f"{recommended_use}")
            
            # Show key configuration highlights
            training_config = preset_config.get('training', {})
            epochs = training_config.get('epochs', 'Default')
            batch_size = training_config.get('batch_size', 'Default')
            learning_rate = training_config.get('learning_rate', 'Default')
            
            print(Fore.CYAN + Style.BRIGHT + f"   Configuration: " + Fore.GREEN + Style.BRIGHT + f"{epochs} epochs, " + Fore.GREEN + Style.BRIGHT + f"batch {batch_size}, " + Fore.GREEN + Style.BRIGHT + f"LR {learning_rate}")
            print()  # Empty line for spacing
        
        # Enhanced navigation options
        print(Fore.YELLOW + Style.BRIGHT + "\nNavigation Options:")
        print(Fore.WHITE + Style.BRIGHT + f"{len(available_presets)+1}. " + Fore.GREEN + Style.BRIGHT + "Switch to Express Setup")
        print(Fore.WHITE + Style.BRIGHT + f"{len(available_presets)+2}. " + Fore.GREEN + Style.BRIGHT + "Switch to Custom Configuration")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        # Preset selection with robust error handling
        while True:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect preset (1-{len(available_presets)}) or navigation option: " + Style.RESET_ALL).strip()
                
                if not choice:
                    continue
                    
                choice_num = int(choice)
                
                if 1 <= choice_num <= len(available_presets):
                    selected_preset = available_presets[choice_num-1]
                    break
                elif choice_num == 0:
                    print(Fore.RED + Style.BRIGHT + "\nPreset selection cancelled")
                    return None
                elif choice_num == len(available_presets) + 1:
                    print(Fore.GREEN + Style.BRIGHT + "\nSwitching to express setup...")
                    return _interactive_express_setup(base_config, use_real_data, **kwargs)
                elif choice_num == len(available_presets) + 2:
                    print(Fore.GREEN + Style.BRIGHT + "\nSwitching to custom configuration...")
                    return _interactive_custom_setup(base_config, use_real_data, **kwargs)
                else:
                    print(Fore.RED + Style.BRIGHT + f"\nInvalid choice. Please select 1-{len(available_presets)+2} or 0 to cancel.")
                    
            except ValueError:
                print(Fore.RED + Style.BRIGHT + f"\nInvalid input. Please enter a number 1-{len(available_presets)+2} or 0 to cancel.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nPreset selection cancelled")
                return None
        
        # Load and display selected preset details
        preset_config = globals()['PRESET_CONFIGS'][selected_preset].copy()
        
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + f"SELECTED PRESET: {selected_preset}")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Extract preset details for display
        model_config = preset_config.get('model', {})
        training_config = preset_config.get('training', {})
        data_config = preset_config.get('data', {})
        metadata = preset_config.get('metadata', {})
        
        model_type = model_config.get('model_type', 'Unknown')
        epochs = training_config.get('epochs', 'Default')
        batch_size = training_config.get('batch_size', 'Default')
        learning_rate = training_config.get('learning_rate', 'Default')
        description = metadata.get('description', 'No description available')
        recommended_use = metadata.get('recommended_use', 'General purpose')
        
        # Display preset overview
        print(Fore.YELLOW + Style.BRIGHT + "Preset Overview:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Description: " + Fore.GREEN + Style.BRIGHT + f"{description}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Recommended Use: " + Fore.GREEN + Style.BRIGHT + f"{recommended_use}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + Style.BRIGHT + f"{model_type}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training: " + Fore.GREEN + Style.BRIGHT + f"{epochs} epochs (~{_estimate_training_time(epochs, model_type)} min)")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + Style.BRIGHT + f"{batch_size}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Learning Rate: " + Fore.GREEN + Style.BRIGHT + f"{learning_rate}")
        
        # Architecture details
        if 'encoding_dim' in model_config:
            print(Fore.YELLOW + Style.BRIGHT + "\nArchitecture Details:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Encoding Dimension: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('encoding_dim', 'N/A')}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Hidden Layers: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('hidden_dims', 'N/A')}")
            
            if model_type == 'AutoencoderEnsemble':
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Ensemble Size: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('num_models', 'N/A')}")
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Diversity Factor: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('diversity_factor', 'N/A')}")
            
            # Enhanced features
            enhanced_features = []
            if model_config.get('use_attention', False):
                enhanced_features.append("Attention")
            if model_config.get('residual_blocks', False):
                enhanced_features.append("Residual Blocks")
            if model_config.get('skip_connection', False):
                enhanced_features.append("Skip Connections")
            
            if enhanced_features:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Enhanced Features: " + Fore.GREEN + Style.BRIGHT + f"{', '.join(enhanced_features)}")
            
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Normalization: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('normalization', 'Default')}")
        
        # Training configuration details
        print(Fore.YELLOW + Style.BRIGHT + "\nTraining Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Optimizer: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('optimizer', 'Default')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Scheduler: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('scheduler', 'Default')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Early Stopping: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if training_config.get('early_stopping', True) else 'Disabled'}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Mixed Precision: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if training_config.get('mixed_precision', False) else 'Disabled'}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Validation Split: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('validation_split', 'Default')}")
        
        # Customization options with enhanced descriptions
        print(Fore.YELLOW + Style.BRIGHT + "\nCustomization Options:")
        print(Fore.WHITE + Style.BRIGHT + "1. Use preset as-is " + Fore.GREEN + Style.BRIGHT + "(recommended for first use)")
        print(Fore.WHITE + Style.BRIGHT + "2. Customize data source only " + Fore.GREEN + Style.BRIGHT + "(keep preset, change data)")
        print(Fore.WHITE + Style.BRIGHT + "3. Customize training duration " + Fore.GREEN + Style.BRIGHT + "(adjust epochs and patience)")
        print(Fore.WHITE + Style.BRIGHT + "4. Customize model architecture " + Fore.GREEN + Style.BRIGHT + "(change model type or layers)")
        print(Fore.WHITE + Style.BRIGHT + "5. Full customization " + Fore.GREEN + Style.BRIGHT + "(complete control)")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        while True:
            try:
                custom_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect customization option (0-5): " + Style.RESET_ALL).strip()
                if custom_choice in ['1', '2', '3', '4', '5', '0']:
                    break
                print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-5.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nCustomization selection cancelled")
                return None
        
        if custom_choice == '0':
            print(Fore.RED + Style.BRIGHT + "\nPreset selection cancelled")
            return None
        
        # Merge configurations using the same pattern as train_model_interactive
        final_config = _merge_configs(base_config, preset_config)
        
        # Data source customization
        if custom_choice in ['2', '3', '4', '5'] or use_real_data is None:
            if use_real_data is None:
                print(Fore.MAGENTA + Style.BRIGHT + "\nDATA SOURCE SELECTION")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                print(Fore.WHITE + Style.BRIGHT + "1. Real network data " + Fore.GREEN + Style.BRIGHT + "(production use)")
                print(Fore.WHITE + Style.BRIGHT + "2. Synthetic data " + Fore.GREEN + Style.BRIGHT + "(testing/development)")
                print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
                
                while True:
                    try:
                        data_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect data source (0-2): " + Style.RESET_ALL).strip()
                        if data_choice in ['1', '2', '0']:
                            break
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                        return None
                
                if data_choice == '0':
                    print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                    return None
                    
                use_real_data = data_choice == '1'
                print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {'Real Data' if use_real_data else 'Synthetic Data'}")
            
            final_config.setdefault('data', {})['use_real_data'] = use_real_data
        
        # Training duration customization
        if custom_choice in ['3', '4', '5']:
            print(Fore.MAGENTA + Style.BRIGHT + "\nTRAINING DURATION CUSTOMIZATION")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            current_epochs = training_config.get('epochs', 50)
            estimated_time = _estimate_training_time(current_epochs, model_type)
            
            print(Fore.CYAN + Style.BRIGHT + f"Current: " + Fore.GREEN + Style.BRIGHT + f"{current_epochs} epochs (~{estimated_time} min)")
            print(Fore.CYAN + Style.BRIGHT + "Quick: 10 epochs | Standard: 50 epochs | Thorough: 100+ epochs")
            
            try:
                new_epochs = input(Fore.YELLOW + Style.BRIGHT + f"\nNew epochs (Enter for {current_epochs}, 'c' to cancel): " + Style.RESET_ALL).strip()
                
                if new_epochs.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nDuration customization cancelled")
                    return None
                elif new_epochs:
                    new_epochs_int = int(new_epochs)
                    final_config.setdefault('training', {})['epochs'] = new_epochs_int
                    # Adaptive patience based on epochs
                    final_config['training']['patience'] = min(20, max(5, new_epochs_int // 3))
                    
                    # Store the user-configured epochs to preserve it
                    final_config['training']['user_configured_epochs'] = new_epochs_int
                    
                    print(Fore.GREEN + Style.BRIGHT + f"\nUpdated: {new_epochs_int} epochs (~{_estimate_training_time(new_epochs_int, model_type)} min)")
                else:
                    print(Fore.GREEN + Style.BRIGHT + f"\nKeeping: {current_epochs} epochs")
                    
            except ValueError:
                print(Fore.RED + Style.BRIGHT + f"\nInvalid input, keeping {current_epochs} epochs")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nDuration customization cancelled")
                return None
        
        # Model architecture customization
        if custom_choice in ['4', '5']:
            print(Fore.MAGENTA + Style.BRIGHT + "\nMODEL ARCHITECTURE CUSTOMIZATION")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            current_model_type = model_config.get('model_type', 'EnhancedAutoencoder')
            
            print(Fore.YELLOW + Style.BRIGHT + f"Current model: " + Fore.CYAN + Style.BRIGHT + f"{current_model_type}")
            print(Fore.WHITE + Style.BRIGHT + "1. Keep current model type")
            print(Fore.WHITE + Style.BRIGHT + "2. SimpleAutoencoder " + Fore.GREEN + Style.BRIGHT + "(fast, lightweight)")
            print(Fore.WHITE + Style.BRIGHT + "3. EnhancedAutoencoder " + Fore.GREEN + Style.BRIGHT + "(balanced, recommended)")
            print(Fore.WHITE + Style.BRIGHT + "4. AutoencoderEnsemble " + Fore.GREEN + Style.BRIGHT + "(accurate, slower)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            try:
                model_change = input(Fore.YELLOW + Style.BRIGHT + "\nSelect model type (0-4, default=1): " + Style.RESET_ALL).strip()
                
                if model_change == '0':
                    print(Fore.RED + Style.BRIGHT + "\nModel customization cancelled")
                    return None
                elif model_change in ['2', '3', '4']:
                    new_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                    new_model_type = new_types[int(model_change)-2]
                    
                    # Store user-configured epochs BEFORE applying model defaults
                    user_epochs = final_config.get('training', {}).get('user_configured_epochs')
                    user_patience = final_config.get('training', {}).get('patience')
                    
                    final_config.setdefault('model', {})['model_type'] = new_model_type
                    
                    print(Fore.GREEN + Style.BRIGHT + f"\nChanged model type to: {new_model_type}")
                    
                    # Apply model-specific defaults
                    _apply_model_type_defaults(final_config, new_model_type)
                    
                    # RESTORE user-configured epochs if they were set
                    if user_epochs is not None:
                        final_config['training']['epochs'] = user_epochs
                        if user_patience is not None:
                            final_config['training']['patience'] = user_patience
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Preserved your configured epochs: {user_epochs}")
                else:
                    print(Fore.GREEN + Style.BRIGHT + f"\nKeeping: {current_model_type}")
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nModel customization cancelled")
                return None
        
        # Full customization fallback
        if custom_choice == '5':
            print(Fore.GREEN + Style.BRIGHT + "\nSwitching to full customization mode...")
            return _interactive_custom_setup(final_config, use_real_data, **kwargs)
        
        # Final confirmation with comprehensive summary
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "FINAL CONFIGURATION SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        final_model_type = final_config.get('model', {}).get('model_type', model_type)
        final_epochs = final_config.get('training', {}).get('epochs', epochs)
        final_data_source = final_config.get('data', {}).get('use_real_data', use_real_data)
        
        print(Fore.YELLOW + Style.BRIGHT + "Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Preset: " + Fore.GREEN + Style.BRIGHT + f"{selected_preset}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model: " + Fore.GREEN + Style.BRIGHT + f"{final_model_type}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + Style.BRIGHT + f"{'Real' if final_data_source else 'Synthetic'}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.GREEN + Style.BRIGHT + f"{final_epochs}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + Style.BRIGHT + f"~{_estimate_training_time(final_epochs, final_model_type)} min")
        
        if custom_choice != '1':
            print(Fore.YELLOW + Style.BRIGHT + f"\nCustomizations Applied:")
            if custom_choice in ['2', '3', '4', '5']:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data source customized")
            if custom_choice in ['3', '4', '5']:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training duration adjusted")
            if custom_choice in ['4', '5']:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model architecture modified")
        
        # Enhanced confirmation with fallback options
        try:
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart training with this configuration? (Y/n/c to cancel): " + Style.RESET_ALL).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print(Fore.RED + Style.BRIGHT + "\nTraining confirmation cancelled")
            return None
        
        if confirm in ('', 'y', 'yes'):
            print(Fore.GREEN + Style.BRIGHT + "\nLaunching training with preset configuration...")
            return _launch_training_with_config(final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print(Fore.RED + Style.BRIGHT + "\nTraining cancelled")
            return None
        else:
            # Enhanced fallback options
            print(Fore.YELLOW + Style.BRIGHT + "\nWould you like to?")
            print(Fore.WHITE + Style.BRIGHT + "1. Try preset selection again")
            print(Fore.WHITE + Style.BRIGHT + "2. Switch to express setup")
            print(Fore.WHITE + Style.BRIGHT + "3. Switch to custom configuration")
            print(Fore.RED + Style.BRIGHT + "0. Return to previous menu")
            
            while True:
                try:
                    retry_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-3): " + Style.RESET_ALL).strip()
                    if retry_choice in ['1', '2', '3', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                    return None
            
            if retry_choice == '1':
                print(Fore.GREEN + Style.BRIGHT + "\nRestarting preset selection...")
                return _interactive_preset_setup(base_config, use_real_data, **kwargs)
            elif retry_choice == '2':
                print(Fore.GREEN + Style.BRIGHT + "\nSwitching to express setup...")
                return _interactive_express_setup(base_config, use_real_data, **kwargs)
            elif retry_choice == '3':
                print(Fore.GREEN + Style.BRIGHT + "\nSwitching to custom configuration...")
                return _interactive_custom_setup(base_config, use_real_data, **kwargs)
            else:
                print(Fore.RED + Style.BRIGHT + "\nReturning to previous menu")
                return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\nPreset setup interrupted by user")
        return None
    except Exception as e:
        logger.error(f"Preset setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during preset setup: {str(e)}\n"
            f"Context:\n"
            f"- Base Config: {bool(base_config)}\n"
            f"- Use Real Data: {use_real_data}\n"
            f"- Available Presets: {len(available_presets) if 'available_presets' in locals() else 0}\n\n"
            f"This could be due to:\n"
            f"- Preset configuration corruption\n"
            f"- Invalid preset structure\n"
            f"- Configuration merging issues\n"
            f"- User input handling problems"
        )
        print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
        print(Fore.RED + Style.BRIGHT + "PRESET SETUP ERROR")
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + message)
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        return None

def _interactive_custom_setup(
    base_config: Dict[str, Any],
    use_real_data: Optional[bool],
    **kwargs
) -> Optional[Dict[str, Any]]:
    """Full interactive configuration with all options matching class parameters."""
    
    try:
        # Clear screen and show banner for consistency
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if base_config is None and banner_config is not None:
            base_config = banner_config
        else:
            base_config = base_config or {}
        
        # Extract context for display
        preset_name = "Custom/Default"
        model_type = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = base_config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = base_config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Menu header with context matching other functions
        print(Fore.MAGENTA + Style.BRIGHT + "CUSTOM CONFIGURATION SETUP")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Active Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Mode: " + Fore.YELLOW + Style.BRIGHT + "Full Custom Control")
        
        print("\nFull interactive configuration with complete control over all parameters.\n")
        print("Press Enter to accept defaults shown in parentheses.\n")
        print("Enter 'c' at any prompt to cancel and return to previous menu")
        
        final_config = base_config.copy()
        
        # DATA CONFIGURATION SECTION
        print(Fore.MAGENTA + Style.BRIGHT + "DATA CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        if use_real_data is None:
            print(Fore.YELLOW + Style.BRIGHT + "Data Source Selection:")
            print(Fore.WHITE + Style.BRIGHT + "1. Real network data " + Fore.YELLOW + Style.BRIGHT + "(requires data files)")
            print(Fore.WHITE + Style.BRIGHT + "2. Synthetic data " + Fore.YELLOW + Style.BRIGHT + "(generated automatically)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            while True:
                try:
                    data_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect data source (0-2): " + Style.RESET_ALL).strip()
                    if data_choice in ['1', '2', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nPlease select 1, 2, or 0")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                    return None
            
            if data_choice == '0':
                print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                return None
                
            use_real_data = data_choice == '1'
        
        data_config = final_config.setdefault('data', {})
        data_config['use_real_data'] = use_real_data
        
        if not use_real_data:
            print(Fore.GREEN + Style.BRIGHT + "\nSynthetic Data Parameters:")
            cancel_msg = Fore.RED + Style.BRIGHT + "\nSynthetic data configuration cancelled"
            
            normal_samples = input(Fore.YELLOW + Style.BRIGHT + "Normal samples " + Fore.WHITE + Style.BRIGHT + "(8000): " + Style.RESET_ALL).strip()
            if normal_samples.lower() == 'c':
                print(cancel_msg)
                return None
            data_config['normal_samples'] = int(normal_samples) if normal_samples else 8000
            
            attack_samples = input(Fore.YELLOW + Style.BRIGHT + "Attack samples " + Fore.WHITE + Style.BRIGHT + "(2000): " + Style.RESET_ALL).strip()
            if attack_samples.lower() == 'c':
                print(cancel_msg)
                return None
            data_config['attack_samples'] = int(attack_samples) if attack_samples else 2000
            
            features = input(Fore.YELLOW + Style.BRIGHT + "Number of features " + Fore.WHITE + Style.BRIGHT + "(20): " + Style.RESET_ALL).strip()
            if features.lower() == 'c':
                print(cancel_msg)
                return None
            data_config['features'] = int(features) if features else 20
            
            anomaly_factor = input(Fore.YELLOW + Style.BRIGHT + "Anomaly factor " + Fore.WHITE + Style.BRIGHT + "(0.1): " + Style.RESET_ALL).strip()
            if anomaly_factor.lower() == 'c':
                print(cancel_msg)
                return None
            data_config['anomaly_factor'] = float(anomaly_factor) if anomaly_factor else 0.1
        else:
            print(Fore.GREEN + Style.BRIGHT + "\nReal Data Configuration:")
            cancel_msg = Fore.RED + Style.BRIGHT + "\nReal data configuration cancelled"
            
            data_path = input(Fore.YELLOW + Style.BRIGHT + "Data file path " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
            if data_path.lower() == 'c':
                print(cancel_msg)
                return None
            if data_path:
                data_config['data_path'] = data_path
            
            artifacts_path = input(Fore.YELLOW + Style.BRIGHT + "Artifacts path " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
            if artifacts_path.lower() == 'c':
                print(cancel_msg)
                return None
            if artifacts_path:
                data_config['artifacts_path'] = artifacts_path
        
        # Data processing parameters
        random_state = input(Fore.YELLOW + Style.BRIGHT + "Random state " + Fore.WHITE + Style.BRIGHT + "(42): " + Style.RESET_ALL).strip()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nData configuration cancelled"
        
        if random_state.lower() == 'c':
            print(cancel_msg)
            return None
        data_config['random_state'] = int(random_state) if random_state else 42
        
        test_split = input(Fore.YELLOW + Style.BRIGHT + "Test split ratio " + Fore.WHITE + Style.BRIGHT + "(0.2): " + Style.RESET_ALL).strip()
        if test_split.lower() == 'c':
            print(cancel_msg)
            return None
        data_config['test_split'] = float(test_split) if test_split else 0.2
        
        stratified = input(Fore.YELLOW + Style.BRIGHT + "Use stratified split? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if stratified.lower() == 'c':
            print(cancel_msg)
            return None
        data_config['stratified_split'] = stratified in ('', 'y', 'yes')
        
        normalization = input(Fore.YELLOW + Style.BRIGHT + "Data normalization " + Fore.WHITE + Style.BRIGHT + "(standard/minmax/none): " + Style.RESET_ALL).strip()
        if normalization.lower() == 'c':
            print(cancel_msg)
            return None
        data_config['data_normalization'] = normalization if normalization else 'standard'
        
        preprocessing = input(Fore.YELLOW + Style.BRIGHT + "Enable data preprocessing? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if preprocessing.lower() == 'c':
            print(cancel_msg)
            return None
        data_config['data_preprocessing'] = preprocessing in ('', 'y', 'yes')

        # MODEL ARCHITECTURE SECTION
        print(Fore.MAGENTA + Style.BRIGHT + "MODEL ARCHITECTURE")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        print(Fore.GREEN + Style.BRIGHT + "Model Type Selection:")
        model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
        for i, mtype in enumerate(model_types, 1):
            print(Fore.WHITE + Style.BRIGHT + f"{i}. {mtype}")
        print(Fore.RED + Style.BRIGHT + f"{len(model_types)+1}. Cancel and return to previous menu")
        
        while True:
            try:
                model_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect model (1-{len(model_types)+1}): " + Style.RESET_ALL).strip()
                if model_choice in [str(i) for i in range(1, len(model_types)+2)]:
                    break
                print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(model_types)+1}")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nModel selection cancelled")
                return None
        
        if model_choice == str(len(model_types)+1):
            print(Fore.RED + Style.BRIGHT + "\nModel selection cancelled")
            return None
        
        model_type = model_types[int(model_choice)-1]
        
        model_config = final_config.setdefault('model', {})
        model_config['model_type'] = model_type
        
        print(Fore.GREEN + Style.BRIGHT + f"\nArchitecture Configuration for {model_type}:")
        cancel_msg = Fore.RED + Style.BRIGHT + "\nArchitecture configuration cancelled"
        
        # Encoding dimension with model-specific defaults
        default_encoding = {
            'SimpleAutoencoder': 16,
            'EnhancedAutoencoder': 32, 
            'AutoencoderEnsemble': 24
        }
        encoding_dim = input(Fore.YELLOW + Style.BRIGHT + f"Encoding dimension " + Fore.WHITE + Style.BRIGHT + f"({default_encoding[model_type]}): " + Style.RESET_ALL).strip()
        if encoding_dim.lower() == 'c':
            print(cancel_msg)
            return None
        model_config['encoding_dim'] = int(encoding_dim) if encoding_dim else default_encoding[model_type]
        
        # Hidden dimensions with model-specific defaults
        default_hidden_dims = {
            'SimpleAutoencoder': [128, 64],
            'EnhancedAutoencoder': [256, 128, 64],
            'AutoencoderEnsemble': [192, 96, 48]
        }
        hidden_dims_input = input(Fore.YELLOW + Style.BRIGHT + "Hidden layers " + Fore.WHITE + Style.BRIGHT + f"(comma-separated, default: {','.join(map(str, default_hidden_dims[model_type]))}): " + Style.RESET_ALL).strip()
        if hidden_dims_input.lower() == 'c':
            print(cancel_msg)
            return None
        if hidden_dims_input:
            model_config['hidden_dims'] = [int(x.strip()) for x in hidden_dims_input.split(',')]
        else:
            model_config['hidden_dims'] = default_hidden_dims[model_type]
        
        # Dropout rates with intelligent defaults
        dropout_input = input(Fore.YELLOW + Style.BRIGHT + "Dropout rates " + Fore.WHITE + Style.BRIGHT + "(comma-separated, e.g., 0.2,0.15,0.1): " + Style.RESET_ALL).strip()
        if dropout_input.lower() == 'c':
            print(cancel_msg)
            return None
        if dropout_input:
            model_config['dropout_rates'] = [float(x.strip()) for x in dropout_input.split(',')]
        else:
            hidden_len = len(model_config['hidden_dims'])
            # Create decreasing dropout rates
            model_config['dropout_rates'] = [0.2 - i*0.05 for i in range(hidden_len)]
        
        print(Fore.GREEN + Style.BRIGHT + "\nNetwork Configuration:")
        cancel_msg = Fore.RED + Style.BRIGHT + "\nNetwork configuration cancelled"
        
        activation = input(Fore.YELLOW + Style.BRIGHT + "Activation function " + Fore.WHITE + Style.BRIGHT + "(leaky_relu/relu/gelu/tanh): " + Style.RESET_ALL).strip()
        if activation.lower() == 'c':
            print(cancel_msg)
            return None
        model_config['activation'] = activation if activation else 'leaky_relu'
        
        if model_config['activation'] == 'leaky_relu':
            activation_param = input(Fore.YELLOW + Style.BRIGHT + "Negative slope for LeakyReLU " + Fore.WHITE + Style.BRIGHT + "(0.2): " + Style.RESET_ALL).strip()
            if activation_param.lower() == 'c':
                print(cancel_msg)
                return None
            model_config['activation_param'] = float(activation_param) if activation_param else 0.2
        
        normalization = input(Fore.YELLOW + Style.BRIGHT + "Normalization " + Fore.WHITE + Style.BRIGHT + "(batch/layer/none): " + Style.RESET_ALL).strip()
        if normalization.lower() == 'c':
            print(cancel_msg)
            return None
        model_config['normalization'] = normalization if normalization else 'batch'
        
        if model_config['normalization'] == 'batch':
            model_config['use_batch_norm'] = True
            model_config['use_layer_norm'] = False
        elif model_config['normalization'] == 'layer':
            model_config['use_batch_norm'] = False
            model_config['use_layer_norm'] = True
        else:
            model_config['use_batch_norm'] = False
            model_config['use_layer_norm'] = False
        
        bias = input(Fore.YELLOW + Style.BRIGHT + "Use bias in layers? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if bias.lower() == 'c':
            print(cancel_msg)
            return None
        model_config['bias'] = bias in ('', 'y', 'yes')
        
        weight_init = input(Fore.YELLOW + Style.BRIGHT + "Weight initialization " + Fore.WHITE + Style.BRIGHT + "(xavier_uniform/xavier_normal/kaiming_uniform): " + Style.RESET_ALL).strip()
        if weight_init.lower() == 'c':
            print(cancel_msg)
            return None
        model_config['weight_init'] = weight_init if weight_init else 'xavier_uniform'
        
        # Enhanced features for advanced models
        if model_type in ['EnhancedAutoencoder', 'AutoencoderEnsemble']:
            print(Fore.GREEN + Style.BRIGHT + "\nEnhanced Features Configuration:")
            cancel_msg = Fore.RED + Style.BRIGHT + "\nEnhanced features configuration cancelled"
            
            attention = input(Fore.YELLOW + Style.BRIGHT + "Use attention mechanism? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if attention == 'c':
                print(cancel_msg)
                return None
            model_config['use_attention'] = attention in ('', 'y', 'yes')
            
            residual = input(Fore.YELLOW + Style.BRIGHT + "Use residual blocks? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if residual == 'c':
                print(cancel_msg)
                return None
            model_config['residual_blocks'] = residual in ('', 'y', 'yes')
            
            skip_conn = input(Fore.YELLOW + Style.BRIGHT + "Use skip connections? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if skip_conn == 'c':
                print(cancel_msg)
                return None
            model_config['skip_connection'] = skip_conn in ('', 'y', 'yes')
            
            if model_type == 'EnhancedAutoencoder':
                legacy = input(Fore.YELLOW + Style.BRIGHT + "Use legacy mode? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
                if legacy == 'c':
                    print(cancel_msg)
                    return None
                model_config['legacy_mode'] = legacy in ('y', 'yes')
        
        # Ensemble-specific configuration
        if model_type == 'AutoencoderEnsemble':
            print(Fore.GREEN + Style.BRIGHT + "\nEnsemble Configuration:")
            cancel_msg = Fore.RED + Style.BRIGHT + "\nEnsemble configuration cancelled"
            
            num_models = input(Fore.YELLOW + Style.BRIGHT + "Number of ensemble models " + Fore.WHITE + Style.BRIGHT + "(3): " + Style.RESET_ALL).strip()
            if num_models.lower() == 'c':
                print(cancel_msg)
                return None
            model_config['num_models'] = int(num_models) if num_models else 3
            
            diversity = input(Fore.YELLOW + Style.BRIGHT + "Diversity factor " + Fore.WHITE + Style.BRIGHT + "(0.3): " + Style.RESET_ALL).strip()
            if diversity.lower() == 'c':
                print(cancel_msg)
                return None
            model_config['diversity_factor'] = float(diversity) if diversity else 0.3
        
        # Validation configuration
        min_features = input(Fore.YELLOW + Style.BRIGHT + "Minimum features validation " + Fore.WHITE + Style.BRIGHT + "(5): " + Style.RESET_ALL).strip()
        if min_features.lower() == 'c':
            print(Fore.RED + Style.BRIGHT + "\nValidation configuration cancelled")
            return None
        model_config['min_features'] = int(min_features) if min_features else 5

        # TRAINING CONFIGURATION SECTION
        print(Fore.MAGENTA + Style.BRIGHT + "TRAINING CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        training_config = final_config.setdefault('training', {})
        
        print(Fore.YELLOW + Style.BRIGHT + "Basic Training Parameters:")
        cancel_msg = Fore.RED + Style.BRIGHT + "\nTraining configuration cancelled"
        
        epochs = input(Fore.YELLOW + Style.BRIGHT + "Number of epochs " + Fore.WHITE + Style.BRIGHT + "(50): " + Style.RESET_ALL).strip()
        if epochs.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['epochs'] = int(epochs) if epochs else 50
        
        batch_size = input(Fore.YELLOW + Style.BRIGHT + "Batch size " + Fore.WHITE + Style.BRIGHT + "(64): " + Style.RESET_ALL).strip()
        if batch_size.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['batch_size'] = int(batch_size) if batch_size else 64
        
        lr = input(Fore.YELLOW + Style.BRIGHT + "Learning rate " + Fore.WHITE + Style.BRIGHT + "(0.001): " + Style.RESET_ALL).strip()
        if lr.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['learning_rate'] = float(lr) if lr else 0.001
        
        patience = input(Fore.YELLOW + Style.BRIGHT + "Early stopping patience " + Fore.WHITE + Style.BRIGHT + "(15): " + Style.RESET_ALL).strip()
        if patience.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['patience'] = int(patience) if patience else 15
        
        validation_split = input(Fore.YELLOW + Style.BRIGHT + "Validation split " + Fore.WHITE + Style.BRIGHT + "(0.2): " + Style.RESET_ALL).strip()
        if validation_split.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['validation_split'] = float(validation_split) if validation_split else 0.2
        
        print(Fore.GREEN + Style.BRIGHT + "\nAdvanced Training Configuration:")
        weight_decay = input(Fore.YELLOW + Style.BRIGHT + "Weight decay " + Fore.WHITE + Style.BRIGHT + "(1e-4): " + Style.RESET_ALL).strip()
        if weight_decay.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['weight_decay'] = float(weight_decay) if weight_decay else 1e-4
        
        gradient_clip = input(Fore.YELLOW + Style.BRIGHT + "Gradient clipping threshold " + Fore.WHITE + Style.BRIGHT + "(1.0): " + Style.RESET_ALL).strip()
        if gradient_clip.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['gradient_clip'] = float(gradient_clip) if gradient_clip else 1.0
        
        grad_accum = input(Fore.YELLOW + Style.BRIGHT + "Gradient accumulation steps " + Fore.WHITE + Style.BRIGHT + "(1): " + Style.RESET_ALL).strip()
        if grad_accum.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['gradient_accumulation_steps'] = int(grad_accum) if grad_accum else 1
        
        # Optimizer selection with enhanced UI
        print(Fore.GREEN + Style.BRIGHT + "\nOptimizer Selection:")
        optimizers = ['AdamW', 'Adam', 'SGD', 'RMSprop']
        for i, opt in enumerate(optimizers, 1):
            print(Fore.WHITE + Style.BRIGHT + f"{i}. {opt}")
        print(Fore.RED + Style.BRIGHT + f"{len(optimizers)+1}. Cancel and return to previous menu")
        
        while True:
            try:
                opt_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect optimizer (1-{len(optimizers)+1}): " + Style.RESET_ALL).strip()
                if opt_choice in [str(i) for i in range(1, len(optimizers)+2)]:
                    break
                print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(optimizers)+1}")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nOptimizer selection cancelled")
                return None
        
        if opt_choice == str(len(optimizers)+1):
            print(Fore.RED + Style.BRIGHT + "\nOptimizer selection cancelled")
            return None
        
        optimizer = optimizers[int(opt_choice)-1]
        training_config['optimizer'] = optimizer
        
        # Optimizer-specific parameters
        if optimizer in ['AdamW', 'Adam']:
            adam_betas_input = input(Fore.YELLOW + Style.BRIGHT + "Adam betas " + Fore.WHITE + Style.BRIGHT + "(0.9,0.999): " + Style.RESET_ALL).strip()
            if adam_betas_input.lower() == 'c':
                print(Fore.RED + Style.BRIGHT + "\nOptimizer configuration cancelled")
                return None
            if adam_betas_input:
                betas = [float(x.strip()) for x in adam_betas_input.split(',')]
                training_config['adam_betas'] = tuple(betas)
            else:
                training_config['adam_betas'] = (0.9, 0.999)
            
            adam_eps = input(Fore.YELLOW + Style.BRIGHT + "Adam epsilon " + Fore.WHITE + Style.BRIGHT + "(1e-8): " + Style.RESET_ALL).strip()
            if adam_eps.lower() == 'c':
                print(Fore.RED + Style.BRIGHT + "\nOptimizer configuration cancelled")
                return None
            training_config['adam_eps'] = float(adam_eps) if adam_eps else 1e-8
        
        # Learning rate scheduler
        scheduler = input(Fore.YELLOW + Style.BRIGHT + "Use learning rate scheduler? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nScheduler configuration cancelled"
        
        if scheduler == 'c':
            print(cancel_msg)
            return None
        if scheduler in ('', 'y', 'yes'):
            print(Fore.GREEN + Style.BRIGHT + "\nScheduler Options:")
            scheduler_types = ['ReduceLROnPlateau', 'StepLR', 'CosineAnnealingLR', 'ExponentialLR']
            for i, sched in enumerate(scheduler_types, 1):
                print(Fore.WHITE + Style.BRIGHT + f"{i}. {sched}")
            print(Fore.RED + Style.BRIGHT + f"{len(scheduler_types)+1}. Cancel and return to previous menu")
            
            while True:
                try:
                    sched_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect scheduler (1-{len(scheduler_types)+1}): " + Style.RESET_ALL).strip()
                    if sched_choice in [str(i) for i in range(1, len(scheduler_types)+2)]:
                        break
                    print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(scheduler_types)+1}")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nScheduler selection cancelled")
                    return None
            
            if sched_choice == str(len(scheduler_types)+1):
                print(Fore.RED + Style.BRIGHT + "\nScheduler selection cancelled")
                return None
            
            training_config['scheduler'] = scheduler_types[int(sched_choice)-1]
            
            # Scheduler-specific parameters
            if training_config['scheduler'] == 'ReduceLROnPlateau':
                lr_patience = input(Fore.YELLOW + Style.BRIGHT + "LR scheduler patience " + Fore.WHITE + Style.BRIGHT + "(5): " + Style.RESET_ALL).strip()
                if lr_patience == 'c':
                    print(cancel_msg)
                    return None
                lr_factor = input(Fore.YELLOW + Style.BRIGHT + "LR reduction factor " + Fore.WHITE + Style.BRIGHT + "(0.5): " + Style.RESET_ALL).strip()
                if lr_factor == 'c':
                    print(cancel_msg)
                    return None
                min_lr = input(Fore.YELLOW + Style.BRIGHT + "Minimum learning rate " + Fore.WHITE + Style.BRIGHT + "(1e-6): " + Style.RESET_ALL).strip()
                if min_lr == 'c':
                    print(cancel_msg)
                    return None
                
                scheduler_params = {
                    'patience': int(lr_patience) if lr_patience else 5,
                    'factor': float(lr_factor) if lr_factor else 0.5,
                    'min_lr': float(min_lr) if min_lr else 1e-6
                }
                training_config['scheduler_params'] = scheduler_params
            elif training_config['scheduler'] == 'StepLR':
                step_size = input(Fore.YELLOW + Style.BRIGHT + "Step size " + Fore.WHITE + Style.BRIGHT + "(30): " + Style.RESET_ALL).strip()
                if step_size == 'c':
                    print(cancel_msg)
                    return None
                gamma = input(Fore.YELLOW + Style.BRIGHT + "Gamma " + Fore.WHITE + Style.BRIGHT + "(0.1): " + Style.RESET_ALL).strip()
                if gamma == 'c':
                    print(cancel_msg)
                    return None
                
                scheduler_params = {
                    'step_size': int(step_size) if step_size else 30,
                    'gamma': float(gamma) if gamma else 0.1
                }
                training_config['scheduler_params'] = scheduler_params
            elif training_config['scheduler'] == 'CosineAnnealingLR':
                t_max = input(Fore.YELLOW + Style.BRIGHT + f"T_max " + Fore.WHITE + Style.BRIGHT + f"({training_config['epochs']}): " + Style.RESET_ALL).strip()
                if t_max == 'c':
                    print(cancel_msg)
                    return None
                eta_min = input(Fore.YELLOW + Style.BRIGHT + "Eta min " + Fore.WHITE + Style.BRIGHT + "(1e-7): " + Style.RESET_ALL).strip()
                if eta_min == 'c':
                    print(cancel_msg)
                    return None
                
                scheduler_params = {
                    'T_max': int(t_max) if t_max else training_config['epochs'],
                    'eta_min': float(eta_min) if eta_min else 1e-7
                }
                training_config['scheduler_params'] = scheduler_params
            elif training_config['scheduler'] == 'ExponentialLR':
                gamma = input(Fore.YELLOW + Style.BRIGHT + "Gamma " + Fore.WHITE + Style.BRIGHT + "(0.95): " + Style.RESET_ALL).strip()
                if gamma == 'c':
                    print(cancel_msg)
                    return None
                
                scheduler_params = {
                    'gamma': float(gamma) if gamma else 0.95
                }
                training_config['scheduler_params'] = scheduler_params
        else:
            training_config['scheduler'] = None
        
        # Additional training options
        early_stopping = input(Fore.YELLOW + Style.BRIGHT + "Enable early stopping? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nTraining configuration cancelled"
        
        if early_stopping == 'c':
            print(cancel_msg)
            return None
        training_config['early_stopping'] = early_stopping in ('', 'y', 'yes')
        
        shuffle = input(Fore.YELLOW + Style.BRIGHT + "Shuffle training data? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if shuffle == 'c':
            print(cancel_msg)
            return None
        training_config['shuffle'] = shuffle in ('', 'y', 'yes')
        
        # Mixed precision training
        if torch.cuda.is_available():
            mixed_prec = input(Fore.YELLOW + Style.BRIGHT + "Use mixed precision training? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if mixed_prec == 'c':
                print(cancel_msg)
                return None
            training_config['mixed_precision'] = mixed_prec in ('', 'y', 'yes')
        else:
            training_config['mixed_precision'] = False

        # SYSTEM & HARDWARE CONFIGURATION
        print(Fore.MAGENTA + Style.BRIGHT + "SYSTEM & HARDWARE CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        hardware_config = final_config.setdefault('hardware', {})
        system_config = final_config.setdefault('system', {})
        advanced_config = final_config.setdefault('advanced_training', {})
        
        device_choice = input(Fore.YELLOW + Style.BRIGHT + "Device " + Fore.WHITE + Style.BRIGHT + "(auto/cpu/cuda): " + Style.RESET_ALL).strip()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nHardware configuration cancelled"
        
        if device_choice.lower() == 'c':
            print(cancel_msg)
            return None
        hardware_config['device'] = device_choice if device_choice else 'auto'
        
        cuda_opt = input(Fore.YELLOW + Style.BRIGHT + "Enable CUDA optimizations? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if cuda_opt == 'c':
            print(cancel_msg)
            return None
        hardware_config['cuda_optimizations'] = cuda_opt in ('', 'y', 'yes') and torch.cuda.is_available()
        
        memory_mgmt = input(Fore.YELLOW + Style.BRIGHT + "Enable memory management? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if memory_mgmt == 'c':
            print(cancel_msg)
            return None
        hardware_config['memory_management'] = {'enable_memory_efficient': memory_mgmt in ('', 'y', 'yes')}
        
        gpu_memory = input(Fore.YELLOW + Style.BRIGHT + "Recommended GPU memory GB " + Fore.WHITE + Style.BRIGHT + "(4.0): " + Style.RESET_ALL).strip()
        if gpu_memory.lower() == 'c':
            print(cancel_msg)
            return None
        hardware_config['recommended_gpu_memory'] = float(gpu_memory) if gpu_memory else 4.0
        
        perf_class = input(Fore.YELLOW + Style.BRIGHT + "System performance class " + Fore.WHITE + Style.BRIGHT + "(auto/low/medium/high): " + Style.RESET_ALL).strip()
        if perf_class.lower() == 'c':
            print(cancel_msg)
            return None
        hardware_config['system_performance_class'] = perf_class if perf_class else 'auto'
        
        # Advanced training configuration
        num_workers = input(Fore.YELLOW + Style.BRIGHT + "Data loader workers " + Fore.WHITE + Style.BRIGHT + "(4): " + Style.RESET_ALL).strip()
        if num_workers.lower() == 'c':
            print(cancel_msg)
            return None
        advanced_config['num_workers'] = int(num_workers) if num_workers else 4
        
        pin_memory = input(Fore.YELLOW + Style.BRIGHT + "Pin memory for GPU? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if pin_memory == 'c':
            print(cancel_msg)
            return None
        advanced_config['pin_memory'] = pin_memory in ('', 'y', 'yes') and torch.cuda.is_available()
        
        persistent_workers = input(Fore.YELLOW + Style.BRIGHT + "Use persistent workers? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if persistent_workers == 'c':
            print(cancel_msg)
            return None
        advanced_config['persistent_workers'] = persistent_workers in ('', 'y', 'yes')
        
        memory_efficient = input(Fore.YELLOW + Style.BRIGHT + "Enable memory efficient training? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if memory_efficient == 'c':
            print(cancel_msg)
            return None
        advanced_config['memory_efficient'] = memory_efficient in ('', 'y', 'yes')
        
        compile_model = input(Fore.YELLOW + Style.BRIGHT + "Compile model with torch.compile? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if compile_model == 'c':
            print(cancel_msg)
            return None
        advanced_config['compile_model'] = compile_model in ('y', 'yes')
        
        benchmark_mode = input(Fore.YELLOW + Style.BRIGHT + "Enable benchmark mode? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if benchmark_mode == 'c':
            print(cancel_msg)
            return None
        advanced_config['benchmark_mode'] = benchmark_mode in ('y', 'yes')
        
        gradient_checkpointing = input(Fore.YELLOW + Style.BRIGHT + "Use gradient checkpointing? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if gradient_checkpointing == 'c':
            print(cancel_msg)
            return None
        advanced_config['gradient_checkpointing'] = gradient_checkpointing in ('y', 'yes')
        
        # System configuration
        parallel_processing = input(Fore.YELLOW + Style.BRIGHT + "Enable parallel processing? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nSystem configuration cancelled"
        
        if parallel_processing == 'c':
            print(cancel_msg)
            return None
        system_config['parallel_processing'] = parallel_processing in ('y', 'yes')
        
        max_workers = input(Fore.YELLOW + Style.BRIGHT + "Maximum workers " + Fore.WHITE + Style.BRIGHT + "(4): " + Style.RESET_ALL).strip()
        if max_workers.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['max_workers'] = int(max_workers) if max_workers else 4
        
        distributed_training = input(Fore.YELLOW + Style.BRIGHT + "Enable distributed training? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if distributed_training == 'c':
            print(cancel_msg)
            return None
        system_config['distributed_training'] = distributed_training in ('y', 'yes')

        # MONITORING & LOGGING CONFIGURATION
        print(Fore.MAGENTA + Style.BRIGHT + "MONITORING & LOGGING")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        monitoring_config = final_config.setdefault('monitoring', {})
        
        tensorboard = input(Fore.YELLOW + Style.BRIGHT + "Enable TensorBoard logging? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nMonitoring configuration cancelled"
        
        if tensorboard == 'c':
            print(cancel_msg)
            return None
        monitoring_config['tensorboard_logging'] = tensorboard in ('', 'y', 'yes')
        
        verbose = input(Fore.YELLOW + Style.BRIGHT + "Verbose output? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if verbose == 'c':
            print(cancel_msg)
            return None
        monitoring_config['verbose'] = verbose in ('', 'y', 'yes')
        
        debug = input(Fore.YELLOW + Style.BRIGHT + "Enable debug mode? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if debug == 'c':
            print(cancel_msg)
            return None
        system_config['debug'] = debug in ('y', 'yes')
        
        checkpoints = input(Fore.YELLOW + Style.BRIGHT + "Save training checkpoints? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if checkpoints == 'c':
            print(cancel_msg)
            return None
        monitoring_config['save_checkpoints'] = checkpoints in ('', 'y', 'yes')
        
        if monitoring_config['save_checkpoints']:
            checkpoint_freq = input(Fore.YELLOW + Style.BRIGHT + "Checkpoint frequency in epochs " + Fore.WHITE + Style.BRIGHT + "(10): " + Style.RESET_ALL).strip()
            if checkpoint_freq.lower() == 'c':
                print(cancel_msg)
                return None
            monitoring_config['checkpoint_frequency'] = int(checkpoint_freq) if checkpoint_freq else 10
        
        save_best = input(Fore.YELLOW + Style.BRIGHT + "Save best model? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if save_best == 'c':
            print(cancel_msg)
            return None
        monitoring_config['save_best_model'] = save_best in ('', 'y', 'yes')
        
        save_history = input(Fore.YELLOW + Style.BRIGHT + "Save model history? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if save_history == 'c':
            print(cancel_msg)
            return None
        monitoring_config['save_model_history'] = save_history in ('', 'y', 'yes')
        
        log_freq = input(Fore.YELLOW + Style.BRIGHT + "Log frequency " + Fore.WHITE + Style.BRIGHT + "(1): " + Style.RESET_ALL).strip()
        if log_freq.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['log_frequency'] = int(log_freq) if log_freq else 1
        
        metrics_freq = input(Fore.YELLOW + Style.BRIGHT + "Metrics frequency " + Fore.WHITE + Style.BRIGHT + "(1): " + Style.RESET_ALL).strip()
        if metrics_freq.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['metrics_frequency'] = int(metrics_freq) if metrics_freq else 1
        
        print(Fore.YELLOW + Style.BRIGHT + "\nMetrics to track (space-separated):")
        print(Fore.WHITE + Style.BRIGHT + "Available: loss, reconstruction_error, anomaly_detection_rate, mse_statistics")
        metrics_input = input(Fore.YELLOW + Style.BRIGHT + "Metrics " + Fore.WHITE + Style.BRIGHT + "(default: loss reconstruction_error): " + Style.RESET_ALL).strip()
        if metrics_input.lower() == 'c':
            print(cancel_msg)
            return None
        if metrics_input:
            monitoring_config['metrics_to_track'] = metrics_input.split()
        else:
            monitoring_config['metrics_to_track'] = ['loss', 'reconstruction_error']
        
        console_level = input(Fore.YELLOW + Style.BRIGHT + "Console logging level " + Fore.WHITE + Style.BRIGHT + "(INFO/DEBUG/WARNING): " + Style.RESET_ALL).strip()
        if console_level.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['console_logging_level'] = console_level if console_level else 'INFO'
        
        early_metric = input(Fore.YELLOW + Style.BRIGHT + "Early stopping metric " + Fore.WHITE + Style.BRIGHT + "(val_loss): " + Style.RESET_ALL).strip()
        if early_metric.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['early_stopping_metric'] = early_metric if early_metric else 'val_loss'
        
        checkpoint_format = input(Fore.YELLOW + Style.BRIGHT + "Checkpoint format " + Fore.WHITE + Style.BRIGHT + "(pth/pt): " + Style.RESET_ALL).strip()
        if checkpoint_format.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['checkpoint_format'] = checkpoint_format if checkpoint_format else 'pth'
        
        log_summary = input(Fore.YELLOW + Style.BRIGHT + "Log model summary? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if log_summary == 'c':
            print(cancel_msg)
            return None
        monitoring_config['log_model_summary'] = log_summary in ('', 'y', 'yes')
        
        tensorboard_dir = input(Fore.YELLOW + Style.BRIGHT + f"TensorBoard directory " + Fore.WHITE + Style.BRIGHT + f"({TB_DIR}): " + Style.RESET_ALL).strip()
        if tensorboard_dir.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['tensorboard_dir'] = tensorboard_dir if tensorboard_dir else TB_DIR
        
        stability_metrics = input(Fore.YELLOW + Style.BRIGHT + "Track stability metrics? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if stability_metrics == 'c':
            print(cancel_msg)
            return None
        monitoring_config['stability_metrics'] = stability_metrics in ('', 'y', 'yes')
        
        performance_metrics = input(Fore.YELLOW + Style.BRIGHT + "Track performance metrics? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if performance_metrics == 'c':
            print(cancel_msg)
            return None
        monitoring_config['performance_metrics'] = performance_metrics in ('', 'y', 'yes')
        
        profiling = input(Fore.YELLOW + Style.BRIGHT + "Enable profiling? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if profiling == 'c':
            print(cancel_msg)
            return None
        monitoring_config['profiling_enabled'] = profiling in ('y', 'yes')

        # SECURITY & ANOMALY DETECTION CONFIGURATION
        print(Fore.MAGENTA + Style.BRIGHT + "SECURITY & ANOMALY DETECTION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        security_config = final_config.setdefault('security', {})
        
        percentile = input(Fore.YELLOW + Style.BRIGHT + "Anomaly threshold percentile " + Fore.WHITE + Style.BRIGHT + "(95.0): " + Style.RESET_ALL).strip()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nSecurity configuration cancelled"
        
        if percentile.lower() == 'c':
            print(cancel_msg)
            return None
        security_config['percentile'] = float(percentile) if percentile else 95.0
        
        threshold_method = input(Fore.YELLOW + Style.BRIGHT + "Threshold calculation method " + Fore.WHITE + Style.BRIGHT + "(percentile/adaptive): " + Style.RESET_ALL).strip()
        if threshold_method.lower() == 'c':
            print(cancel_msg)
            return None
        security_config['anomaly_threshold_strategy'] = threshold_method if threshold_method else 'percentile'
        
        enable_security = input(Fore.YELLOW + Style.BRIGHT + "Enable security metrics? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if enable_security == 'c':
            print(cancel_msg)
            return None
        security_config['enable_security_metrics'] = enable_security in ('', 'y', 'yes')
        
        adaptive_thresh = input(Fore.YELLOW + Style.BRIGHT + "Use adaptive thresholding? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if adaptive_thresh == 'c':
            print(cancel_msg)
            return None
        security_config['adaptive_threshold'] = adaptive_thresh in ('', 'y', 'yes')
        
        attack_thresh = input(Fore.YELLOW + Style.BRIGHT + "Attack threshold " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        if attack_thresh.lower() == 'c':
            print(cancel_msg)
            return None
        if attack_thresh:
            security_config['attack_threshold'] = float(attack_thresh)
        
        false_neg_cost = input(Fore.YELLOW + Style.BRIGHT + "False negative cost " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        if false_neg_cost.lower() == 'c':
            print(cancel_msg)
            return None
        if false_neg_cost:
            security_config['false_negative_cost'] = float(false_neg_cost)
        
        early_warning = input(Fore.YELLOW + Style.BRIGHT + "Early warning threshold " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        if early_warning.lower() == 'c':
            print(cancel_msg)
            return None
        if early_warning:
            security_config['early_warning_threshold'] = float(early_warning)
        
        confidence_interval = input(Fore.YELLOW + Style.BRIGHT + "Confidence interval " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        if confidence_interval.lower() == 'c':
            print(cancel_msg)
            return None
        if confidence_interval:
            security_config['confidence_interval'] = float(confidence_interval)
        
        detection_methods = input(Fore.YELLOW + Style.BRIGHT + "Detection methods " + Fore.WHITE + Style.BRIGHT + "(space-separated, optional): " + Style.RESET_ALL).strip()
        if detection_methods.lower() == 'c':
            print(cancel_msg)
            return None
        if detection_methods:
            security_config['detection_methods'] = detection_methods.split()
        
        alert_levels = input(Fore.YELLOW + Style.BRIGHT + "Alert levels " + Fore.WHITE + Style.BRIGHT + "(space-separated, optional): " + Style.RESET_ALL).strip()
        if alert_levels.lower() == 'c':
            print(cancel_msg)
            return None
        if alert_levels:
            security_config['alert_levels'] = alert_levels.split()
        
        threshold_validation = input(Fore.YELLOW + Style.BRIGHT + "Enable threshold validation? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if threshold_validation == 'c':
            print(cancel_msg)
            return None
        security_config['threshold_validation'] = threshold_validation in ('', 'y', 'yes')
        
        robust_detection = input(Fore.YELLOW + Style.BRIGHT + "Enable robust detection? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if robust_detection == 'c':
            print(cancel_msg)
            return None
        security_config['robust_detection'] = robust_detection in ('', 'y', 'yes')
        
        fp_tolerance = input(Fore.YELLOW + Style.BRIGHT + "False positive tolerance " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        if fp_tolerance.lower() == 'c':
            print(cancel_msg)
            return None
        if fp_tolerance:
            security_config['false_positive_tolerance'] = float(fp_tolerance)
        
        perf_optimized = input(Fore.YELLOW + Style.BRIGHT + "Performance optimized detection? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if perf_optimized == 'c':
            print(cancel_msg)
            return None
        security_config['performance_optimized_detection'] = perf_optimized in ('', 'y', 'yes')
        
        real_time = input(Fore.YELLOW + Style.BRIGHT + "Real-time monitoring? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if real_time == 'c':
            print(cancel_msg)
            return None
        security_config['real_time_monitoring'] = real_time in ('y', 'yes')
        
        if model_type == 'AutoencoderEnsemble':
            voting = input(Fore.YELLOW + Style.BRIGHT + "Ensemble voting method " + Fore.WHITE + Style.BRIGHT + "(average/majority/weighted): " + Style.RESET_ALL).strip()
            if voting.lower() == 'c':
                print(cancel_msg)
                return None
            security_config['ensemble_voting'] = voting if voting else 'average'
            
            uncertainty = input(Fore.YELLOW + Style.BRIGHT + "Uncertainty threshold " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
            if uncertainty.lower() == 'c':
                print(cancel_msg)
                return None
            if uncertainty:
                security_config['uncertainty_threshold'] = float(uncertainty)

        # VALIDATION & TESTING CONFIGURATION
        print(Fore.MAGENTA + Style.BRIGHT + "VALIDATION & TESTING")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        validation_config = final_config.setdefault('validation', {})
        
        detailed_metrics = input(Fore.YELLOW + Style.BRIGHT + "Calculate detailed metrics? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nValidation configuration cancelled"
        
        if detailed_metrics == 'c':
            print(cancel_msg)
            return None
        validation_config['detailed_metrics'] = detailed_metrics in ('', 'y', 'yes')
        
        cross_validation = input(Fore.YELLOW + Style.BRIGHT + "Enable cross-validation? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if cross_validation == 'c':
            print(cancel_msg)
            return None
        if cross_validation in ('y', 'yes'):
            cv_folds = input(Fore.YELLOW + Style.BRIGHT + "Number of CV folds " + Fore.WHITE + Style.BRIGHT + "(5): " + Style.RESET_ALL).strip()
            if cv_folds.lower() == 'c':
                print(cancel_msg)
                return None
            validation_config['cross_validation'] = {
                'enabled': True,
                'folds': int(cv_folds) if cv_folds else 5
            }
        else:
            validation_config['cross_validation'] = {'enabled': False}
        
        val_freq = input(Fore.YELLOW + Style.BRIGHT + "Validation frequency " + Fore.WHITE + Style.BRIGHT + "(1): " + Style.RESET_ALL).strip()
        if val_freq.lower() == 'c':
            print(cancel_msg)
            return None
        validation_config['validation_frequency'] = int(val_freq) if val_freq else 1
        
        save_val_results = input(Fore.YELLOW + Style.BRIGHT + "Save validation results? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if save_val_results == 'c':
            print(cancel_msg)
            return None
        validation_config['save_validation_results'] = save_val_results in ('', 'y', 'yes')
        
        robustness = input(Fore.YELLOW + Style.BRIGHT + "Enable robustness testing? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if robustness == 'c':
            print(cancel_msg)
            return None
        validation_config['robustness_testing'] = robustness in ('y', 'yes')
        
        benchmarking = input(Fore.YELLOW + Style.BRIGHT + "Enable performance benchmarking? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if benchmarking == 'c':
            print(cancel_msg)
            return None
        validation_config['performance_benchmarking'] = benchmarking in ('y', 'yes')
        
        confidence_intervals = input(Fore.YELLOW + Style.BRIGHT + "Calculate confidence intervals? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if confidence_intervals == 'c':
            print(cancel_msg)
            return None
        validation_config['confidence_intervals'] = confidence_intervals in ('y', 'yes')
        
        validation_metrics = input(Fore.YELLOW + Style.BRIGHT + "Validation metrics " + Fore.WHITE + Style.BRIGHT + "(space-separated, default: loss reconstruction_error): " + Style.RESET_ALL).strip()
        if validation_metrics.lower() == 'c':
            print(cancel_msg)
            return None
        if validation_metrics:
            validation_config['metrics'] = validation_metrics.split()
        else:
            validation_config['metrics'] = ['loss', 'reconstruction_error']

        # EXPORT & SAVING CONFIGURATION
        print(Fore.MAGENTA + Style.BRIGHT + "EXPORT & SAVING")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        export_config = final_config.setdefault('export', {})
        
        save_model = input(Fore.YELLOW + Style.BRIGHT + "Save trained model? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nExport configuration cancelled"
        
        if save_model == 'c':
            print(cancel_msg)
            return None
        export_config['save_model'] = save_model in ('', 'y', 'yes')
        
        save_metadata = input(Fore.YELLOW + Style.BRIGHT + "Save training metadata? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if save_metadata == 'c':
            print(cancel_msg)
            return None
        export_config['save_metadata'] = save_metadata in ('', 'y', 'yes')
        
        save_history = input(Fore.YELLOW + Style.BRIGHT + "Save training history? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if save_history == 'c':
            print(cancel_msg)
            return None
        export_config['save_training_history'] = save_history in ('', 'y', 'yes')
        
        export_onnx = input(Fore.YELLOW + Style.BRIGHT + "Export to ONNX format? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if export_onnx == 'c':
            print(cancel_msg)
            return None
        export_config['export_onnx'] = export_onnx in ('y', 'yes')
        
        model_dir = input(Fore.YELLOW + Style.BRIGHT + f"Model directory " + Fore.WHITE + f"({DEFAULT_MODEL_DIR}): " + Style.RESET_ALL).strip()
        if model_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['model_dir'] = model_dir if model_dir else DEFAULT_MODEL_DIR
        
        log_dir = input(Fore.YELLOW + Style.BRIGHT + f"Log directory " + Fore.WHITE + f"({LOG_DIR}): " + Style.RESET_ALL).strip()
        if log_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['log_dir'] = log_dir if log_dir else LOG_DIR
        
        config_dir = input(Fore.YELLOW + Style.BRIGHT + f"Config directory " + Fore.WHITE + f"({DEFAULT_MODEL_DIR}): " + Style.RESET_ALL).strip()
        if config_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['config_dir'] = config_dir if config_dir else DEFAULT_MODEL_DIR
        
        data_dir = input(Fore.YELLOW + Style.BRIGHT + f"Data directory " + Fore.WHITE + f"({DEFAULT_MODEL_DIR}): " + Style.RESET_ALL).strip()
        if data_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['data_dir'] = data_dir if data_dir else DEFAULT_MODEL_DIR
        
        checkpoint_dir = input(Fore.YELLOW + Style.BRIGHT + f"Checkpoint directory " + Fore.WHITE + f"({DEFAULT_MODEL_DIR}): " + Style.RESET_ALL).strip()
        if checkpoint_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['checkpoint_dir'] = checkpoint_dir if checkpoint_dir else DEFAULT_MODEL_DIR

        # REPRODUCIBILITY & SYSTEM CONFIGURATION
        print(Fore.MAGENTA + Style.BRIGHT + "REPRODUCIBILITY & SYSTEM")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        reproducible = input(Fore.YELLOW + Style.BRIGHT + "Enable reproducible training? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nSystem configuration cancelled"
        
        if reproducible == 'c':
            print(cancel_msg)
            return None
        system_config['reproducible'] = reproducible in ('', 'y', 'yes')
        
        if system_config['reproducible']:
            random_seed = input(Fore.YELLOW + Style.BRIGHT + "Random seed " + Fore.WHITE + Style.BRIGHT + "(42): " + Style.RESET_ALL).strip()
            if random_seed.lower() == 'c':
                print(cancel_msg)
                return None
            system_config['random_seed'] = int(random_seed) if random_seed else 42
        
        non_interactive = input(Fore.YELLOW + Style.BRIGHT + "Set non-interactive mode? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if non_interactive == 'c':
            print(cancel_msg)
            return None
        system_config['non_interactive'] = non_interactive in ('y', 'yes')
        
        python_exec = input(Fore.YELLOW + Style.BRIGHT + f"Python executable " + Fore.WHITE + f"({sys.executable}): " + Style.RESET_ALL).strip()
        if python_exec.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['python_executable'] = python_exec if python_exec else sys.executable
        
        work_dir = input(Fore.YELLOW + Style.BRIGHT + f"Working directory " + Fore.WHITE + f"({os.getcwd()}): " + Style.RESET_ALL).strip()
        if work_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['working_directory'] = work_dir if work_dir else os.getcwd()
        
        env_health = input(Fore.YELLOW + Style.BRIGHT + "Environment health check " + Fore.WHITE + Style.BRIGHT + "(auto/skip): " + Style.RESET_ALL).strip()
        if env_health.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['environment_health'] = env_health if env_health else 'auto'

        # HYPERPARAMETER OPTIMIZATION CONFIGURATION
        print(Fore.MAGENTA + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        
        hpo_enabled = input(Fore.YELLOW + Style.BRIGHT + "Enable hyperparameter optimization? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nHPO configuration cancelled"
        
        if hpo_enabled == 'c':
            print(cancel_msg)
            return None
        hpo_config['enabled'] = hpo_enabled in ('y', 'yes')
        
        if hpo_config['enabled']:
            hpo_strategy = input(Fore.YELLOW + Style.BRIGHT + "HPO strategy " + Fore.WHITE + Style.BRIGHT + "(optuna/hyperopt/random): " + Style.RESET_ALL).strip()
            if hpo_strategy.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['strategy'] = hpo_strategy if hpo_strategy else 'optuna'
            
            study_name = input(Fore.YELLOW + Style.BRIGHT + "Study name " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
            if study_name.lower() == 'c':
                print(cancel_msg)
                return None
            if study_name:
                hpo_config['study_name'] = study_name
            
            direction = input(Fore.YELLOW + Style.BRIGHT + "Optimization direction " + Fore.WHITE + Style.BRIGHT + "(minimize/maximize): " + Style.RESET_ALL).strip()
            if direction.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['direction'] = direction if direction else 'minimize'
            
            n_trials = input(Fore.YELLOW + Style.BRIGHT + "Number of trials " + Fore.WHITE + Style.BRIGHT + "(100): " + Style.RESET_ALL).strip()
            if n_trials.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['n_trials'] = int(n_trials) if n_trials else 100
            
            timeout_input = input(Fore.YELLOW + Style.BRIGHT + "Timeout in seconds " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
            if timeout_input.lower() == 'c':
                print(cancel_msg)
                return None
            if timeout_input:
                hpo_config['timeout'] = int(timeout_input)
            
            sampler = input(Fore.YELLOW + Style.BRIGHT + "Sampler " + Fore.WHITE + Style.BRIGHT + "(TPE/Random/Grid): " + Style.RESET_ALL).strip()
            if sampler.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['sampler'] = sampler if sampler else 'TPE'
            
            pruner = input(Fore.YELLOW + Style.BRIGHT + "Pruner " + Fore.WHITE + Style.BRIGHT + "(MedianPruner/HyperbandPruner/None): " + Style.RESET_ALL).strip()
            if pruner.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['pruner'] = pruner if pruner else 'MedianPruner'
            
            objective_metric = input(Fore.YELLOW + Style.BRIGHT + "Objective metric " + Fore.WHITE + Style.BRIGHT + "(val_loss): " + Style.RESET_ALL).strip()
            if objective_metric.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['objective_metric'] = objective_metric if objective_metric else 'val_loss'
            
            trial_epochs = input(Fore.YELLOW + Style.BRIGHT + "Trial epochs " + Fore.WHITE + Style.BRIGHT + "(10): " + Style.RESET_ALL).strip()
            if trial_epochs.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['trial_epochs'] = int(trial_epochs) if trial_epochs else 10
            
            trial_patience = input(Fore.YELLOW + Style.BRIGHT + "Trial patience " + Fore.WHITE + Style.BRIGHT + "(5): " + Style.RESET_ALL).strip()
            if trial_patience.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['trial_patience'] = int(trial_patience) if trial_patience else 5
            
            cleanup_trials = input(Fore.YELLOW + Style.BRIGHT + "Cleanup trials after completion? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if cleanup_trials == 'c':
                print(cancel_msg)
                return None
            hpo_config['cleanup_trials'] = cleanup_trials in ('', 'y', 'yes')
            
            generate_plots = input(Fore.YELLOW + Style.BRIGHT + "Generate optimization plots? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if generate_plots == 'c':
                print(cancel_msg)
                return None
            hpo_config['generate_plots'] = generate_plots in ('', 'y', 'yes')

        # PRESETS & ADVANCED CONFIGURATION
        print(Fore.MAGENTA + Style.BRIGHT + "PRESETS & ADVANCED")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        presets_config = final_config.setdefault('presets', {})
        
        current_preset = input(Fore.YELLOW + Style.BRIGHT + "Current preset name " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nPresets configuration cancelled"
        
        if current_preset.lower() == 'c':
            print(cancel_msg)
            return None
        if current_preset:
            presets_config['current_preset'] = current_preset
        
        auto_apply = input(Fore.YELLOW + Style.BRIGHT + "Auto-apply compatible presets? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if auto_apply == 'c':
            print(cancel_msg)
            return None
        presets_config['auto_apply'] = auto_apply in ('y', 'yes')
        
        validate_compatibility = input(Fore.YELLOW + Style.BRIGHT + "Validate preset compatibility? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if validate_compatibility == 'c':
            print(cancel_msg)
            return None
        presets_config['validate_compatibility'] = validate_compatibility in ('', 'y', 'yes')
        
        experimental_config = final_config.setdefault('experimental', {})
        
        experimental_features = input(Fore.YELLOW + Style.BRIGHT + "\nEnable experimental features? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nExperimental configuration cancelled"
        
        if experimental_features == 'c':
            print(cancel_msg)
            return None
        if experimental_features in ('y', 'yes'):
            print(Fore.YELLOW + Style.BRIGHT + "\nAvailable experimental features:")
            print(Fore.WHITE + Style.BRIGHT + "1. Advanced attention mechanisms")
            print(Fore.WHITE + Style.BRIGHT + "2. Dynamic architecture adjustment")
            print(Fore.WHITE + Style.BRIGHT + "3. Adaptive learning rates")
            print(Fore.WHITE + Style.BRIGHT + "4. Novel regularization techniques")
            
            exp_features_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect features " + Fore.WHITE + Style.BRIGHT + "(space-separated numbers, optional): " + Style.RESET_ALL).strip()
            if exp_features_input.lower() == 'c':
                print(cancel_msg)
                return None
            if exp_features_input:
                feature_map = {
                    '1': 'advanced_attention',
                    '2': 'dynamic_architecture',
                    '3': 'adaptive_lr',
                    '4': 'novel_regularization'
                }
                selected_features = {}
                for num in exp_features_input.split():
                    if num in feature_map:
                        selected_features[feature_map[num]] = True
                experimental_config['experimental_features'] = selected_features
        
        auto_optimize = input(Fore.YELLOW + Style.BRIGHT + "Enable automatic optimization? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if auto_optimize == 'c':
            print(cancel_msg)
            return None
        experimental_config['auto_optimize'] = auto_optimize in ('y', 'yes')

        # METADATA & DOCUMENTATION CONFIGURATION
        print(Fore.MAGENTA + Style.BRIGHT + "METADATA & DOCUMENTATION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        metadata_config = final_config.setdefault('metadata', {})
        
        description = input(Fore.YELLOW + Style.BRIGHT + "Experiment description " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nMetadata configuration cancelled"
        
        if description.lower() == 'c':
            print(cancel_msg)
            return None
        if description:
            metadata_config['description'] = description
        
        version = input(Fore.YELLOW + Style.BRIGHT + "Configuration version " + Fore.WHITE + Style.BRIGHT + "(1.0): " + Style.RESET_ALL).strip()
        if version.lower() == 'c':
            print(cancel_msg)
            return None
        metadata_config['version'] = version if version else '1.0'
        
        config_version = input(Fore.YELLOW + Style.BRIGHT + "Config format version " + Fore.WHITE + Style.BRIGHT + "(1.0): " + Style.RESET_ALL).strip()
        if config_version.lower() == 'c':
            print(cancel_msg)
            return None
        metadata_config['config_version'] = config_version if config_version else '1.0'
        
        config_type = input(Fore.YELLOW + Style.BRIGHT + "Configuration type " + Fore.WHITE + Style.BRIGHT + "(custom): " + Style.RESET_ALL).strip()
        if config_type.lower() == 'c':
            print(cancel_msg)
            return None
        metadata_config['config_type'] = config_type if config_type else 'custom'
        
        metadata_config['created'] = datetime.now().isoformat()
        metadata_config['last_modified'] = datetime.now().isoformat()
        
        if final_config.get('presets', {}).get('current_preset'):
            metadata_config['preset_used'] = final_config['presets']['current_preset']
        
        compatibility = input(Fore.YELLOW + Style.BRIGHT + "Compatibility tags " + Fore.WHITE + Style.BRIGHT + "(space-separated, optional): " + Style.RESET_ALL).strip()
        if compatibility.lower() == 'c':
            print(cancel_msg)
            return None
        if compatibility:
            metadata_config['compatibility'] = compatibility.split()
        else:
            metadata_config['compatibility'] = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
        
        runtime_config = final_config.setdefault('runtime', {})
        runtime_config.update({
            'config_loaded_at': datetime.now().isoformat(),
            'config_source': 'interactive_custom',
            'runtime_id': f"custom_{int(time.time())}",
            'process_id': os.getpid(),
            'system_analysis_completed': False,
            'system_performance_score': None,
            'system_class': hardware_config.get('system_performance_class', 'auto'),
            'optimizations_applied': {},
            'resource_status': {},
            'system_warnings': [],
            'recommendations': [],
            'configuration_health': {}
        })

        # FINAL CONFIGURATION REVIEW
        print(Fore.MAGENTA + Style.BRIGHT + "CONFIGURATION REVIEW")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Comprehensive configuration summary
        estimated_time = _estimate_training_time(
            training_config.get('epochs', 50), 
            model_type,
            training_config.get('batch_size', 64)
        )
        
        print(Fore.YELLOW + Style.BRIGHT + f"Configuration Summary:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + Style.BRIGHT + f"{'Real Data' if use_real_data else 'Synthetic Data'}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model: " + Fore.GREEN + Style.BRIGHT + f"{model_type}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('epochs', 50)} epochs")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('batch_size', 64)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('learning_rate', 0.001)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Optimizer: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('optimizer', 'AdamW')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Scheduler: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('scheduler', 'None')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Mixed Precision: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('mixed_precision', False)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Device: " + Fore.GREEN + Style.BRIGHT + f"{hardware_config.get('device', 'auto')}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + Style.BRIGHT + f"~{estimated_time} minutes")
        
        print(Fore.YELLOW + Style.BRIGHT + f"\nArchitecture Details:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Encoding Dim: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('encoding_dim')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Hidden Dims: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('hidden_dims')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Dropout Rates: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('dropout_rates')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Activation: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('activation')}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Normalization: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('normalization')}")
        
        # Enhanced features display
        if model_type in ['EnhancedAutoencoder', 'AutoencoderEnsemble']:
            enhanced_features = []
            if model_config.get('use_attention'):
                enhanced_features.append("Attention")
            if model_config.get('residual_blocks'):
                enhanced_features.append("Residual Blocks")
            if model_config.get('skip_connection'):
                enhanced_features.append("Skip Connections")
            
            if enhanced_features:
                print(Fore.MAGENTA + Style.BRIGHT + f"\n   Enhanced Features:")
                print(Fore.GREEN + Style.BRIGHT + f"     └─ Enhanced Features: " + Fore.CYAN + Style.BRIGHT + f"{', '.join(enhanced_features)}")
        
        if model_type == 'AutoencoderEnsemble':
            print(Fore.MAGENTA + Style.BRIGHT + f"\n   Ensemble Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"     ├─ Ensemble Size: " + Fore.CYAN + Style.BRIGHT + f"{model_config.get('num_models', 3)} models")
            print(Fore.GREEN + Style.BRIGHT + f"     └─ Diversity Factor: " + Fore.CYAN + Style.BRIGHT + f"{model_config.get('diversity_factor', 0.3)}")
        
        # Security configuration display
        print(Fore.YELLOW + Style.BRIGHT + f"\nSecurity Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Anomaly Threshold: " + Fore.GREEN + Style.BRIGHT + f"{security_config.get('percentile', 95.0)}th percentile")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Threshold Method: " + Fore.GREEN + Style.BRIGHT + f"{security_config.get('anomaly_threshold_strategy', 'percentile')}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Adaptive Threshold: " + Fore.GREEN + Style.BRIGHT + f"{security_config.get('adaptive_threshold', True)}")
        
        # System configuration display
        print(Fore.YELLOW + Style.BRIGHT + f"\nSystem Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Reproducible: " + Fore.GREEN + Style.BRIGHT + f"{system_config.get('reproducible', True)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Workers: " + Fore.GREEN + Style.BRIGHT + f"{advanced_config.get('num_workers', 4)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Directory: " + Fore.GREEN + Style.BRIGHT + f"{system_config.get('model_dir', DEFAULT_MODEL_DIR)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Config Directory: " + Fore.GREEN + Style.BRIGHT + f"{system_config.get('config_dir', CONFIG_DIR)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Log Directory: " + Fore.GREEN + Style.BRIGHT + f"{system_config.get('log_dir', LOG_DIR)}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Tensorboard Directory: " + Fore.GREEN + Style.BRIGHT + f"{system_config.get('tb_dir', TB_DIR)}")
        
        # Monitoring configuration display
        print(Fore.YELLOW + Style.BRIGHT + f"\nMonitoring Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ TensorBoard: " + Fore.GREEN + Style.BRIGHT + f"{monitoring_config.get('tensorboard_logging', True)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Checkpoints: " + Fore.GREEN + Style.BRIGHT + f"{monitoring_config.get('save_checkpoints', True)}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Metrics: " + Fore.GREEN + Style.BRIGHT + f"{monitoring_config.get('metrics_to_track', ['loss', 'reconstruction_error'])}")
        
        # HPO configuration display
        if hpo_config.get('enabled'):
            print(Fore.YELLOW + Style.BRIGHT + f"\nHPO Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Strategy: " + Fore.GREEN + Style.BRIGHT + f"{hpo_config.get('strategy', 'optuna')}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Trials: " + Fore.GREEN + Style.BRIGHT + f"{hpo_config.get('n_trials', 100)}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Objective: " + Fore.GREEN + Style.BRIGHT + f"{hpo_config.get('objective_metric', 'val_loss')}")
        
        # Final confirmation with enhanced options
        #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*60)
        confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart training with this configuration? " + Fore.WHITE + Style.BRIGHT + "(Y/n/c to cancel): " + Style.RESET_ALL).strip().lower()
        
        if confirm in ('', 'y', 'yes'):
            console.clear()
            
            launch_panel = Panel.fit(
                "Launching training with custom configuration...",
                style="bold green",
                border_style="green",
                padding=(1, 2),
                box=box.ROUNDED
            )
            # Small delay to ensure panel is rendered before proceeding
            time.sleep(3)
            console.clear()
            
            return _launch_training_with_config(final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print(Fore.RED + Style.BRIGHT + "\nTraining cancelled by user")
            return None
        else:
            # Enhanced fallback options matching other functions
            print(Fore.YELLOW + Style.BRIGHT + "\nWould you like to:")
            print(Fore.WHITE + Style.BRIGHT + "1. Try again with different settings")
            print(Fore.WHITE + Style.BRIGHT + "2. Return to training menu")
            print(Fore.RED + Style.BRIGHT + "0. Cancel completely")
            
            while True:
                try:
                    retry_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-2): " + Style.RESET_ALL).strip()
                    if retry_choice in ['0', '1', '2']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                    return None
            
            if retry_choice == '1':
                print(Fore.CYAN + Style.BRIGHT + "\nRestarting custom configuration...")
                return _interactive_custom_setup(base_config, use_real_data, **kwargs)
            elif retry_choice == '2':
                print(Fore.YELLOW + Style.BRIGHT + "\nReturning to training menu...")
                return None
            else:
                print(Fore.RED + Style.BRIGHT + "\nConfiguration cancelled")
                return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\n\nCustom setup interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"Custom setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during custom configuration setup: {str(e)}\n"
            f"Context:\n"
            f"- Base Config: {bool(base_config)}\n"
            f"- Use Real Data: {use_real_data}\n\n"
            f"This could be due to:\n"
            f"- Invalid input values\n"
            f"- Configuration validation issues\n"
            f"- System resource constraints\n"
            f"- Interactive input handling problems"
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="CUSTOM SETUP ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        return None

def _launch_training_with_config(config: Dict[str, Any], **kwargs) -> Optional[Dict[str, Any]]:
    try:
        print("\nLAUNCHING TRAINING")
        print("-"*40)
        
        training_params = {}
        
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        security_config = config.get('security', {})
        system_config = config.get('system', {})
        monitoring_config = config.get('monitoring', {})
        export_config = config.get('export', {})
        hardware_config = config.get('hardware', {})
        advanced_config = config.get('advanced_training', {})
        validation_config = config.get('validation', {})
        hpo_config = config.get('hyperparameter_optimization', {})
        presets_config = config.get('presets', {})
        experimental_config = config.get('experimental', {})
        metadata_config = config.get('metadata', {})
        runtime_config = config.get('runtime', {})
        
        for key, value in model_config.items():
            if key in ['model_type', 'encoding_dim', 'hidden_dims', 'dropout_rates',
                      'activation', 'normalization', 'skip_connection', 'residual_blocks',
                      'use_attention', 'legacy_mode', 'num_models', 'diversity_factor',
                      'activation_param', 'use_batch_norm', 'use_layer_norm', 'bias',
                      'weight_init', 'available_activations', 'available_normalizations',
                      'available_initializers', 'model_types', 'min_features']:
                training_params[key] = value
        
        for key, value in training_config.items():
            if key in ['batch_size', 'epochs', 'learning_rate', 'patience', 'weight_decay',
                      'gradient_clip', 'gradient_accumulation_steps', 'mixed_precision',
                      'optimizer', 'scheduler', 'scheduler_params', 'early_stopping',
                      'validation_split', 'shuffle', 'pin_memory', 'persistent_workers',
                      'adam_betas', 'adam_eps', 'lr_patience', 'lr_factor', 'min_lr']:
                if key == 'optimizer':
                    training_params['optimizer_type'] = value
                elif key == 'scheduler':
                    training_params['scheduler_type'] = value
                else:
                    training_params[key] = value
        
        for key, value in data_config.items():
            if key in ['normal_samples', 'attack_samples', 'features', 'use_real_data',
                      'data_path', 'artifacts_path', 'synthetic_generation', 'preprocessing',
                      'anomaly_factor', 'random_state', 'test_split', 'stratified_split',
                      'data_preprocessing']:
                training_params[key] = value
            elif key == 'normalization':
                training_params['normalization_method'] = value
        
        for key, value in security_config.items():
            if key in ['percentile', 'attack_threshold', 'false_negative_cost',
                      'enable_security_metrics', 'adaptive_threshold', 'confidence_interval',
                      'detection_methods', 'alert_levels', 'threshold_validation',
                      'robust_detection', 'false_positive_tolerance',
                      'performance_optimized_detection', 'real_time_monitoring',
                      'ensemble_voting', 'uncertainty_threshold',
                      'early_warning_threshold']:
                training_params[key] = value
            elif key == 'anomaly_threshold_strategy':
                training_params['threshold_method'] = value
        
        for key, value in system_config.items():
            if key in ['model_dir', 'log_dir', 'config_dir', 'random_seed',
                      'reproducible', 'data_dir', 'checkpoint_dir', 'debug',
                      'verbose', 'parallel_processing', 'max_workers', 'export_onnx',
                      'non_interactive', 'cuda_optimizations', 'onnx_export',
                      'distributed_training', 'python_executable', 'working_directory',
                      'environment_health']:
                training_params[key] = value
        
        for key, value in monitoring_config.items():
            if key in ['verbose', 'tensorboard_logging', 'save_checkpoints',
                      'checkpoint_frequency', 'log_frequency', 'metrics_frequency',
                      'console_logging_level', 'save_best_model', 'save_model_history',
                      'metrics_to_track', 'early_stopping_metric', 'checkpoint_format',
                      'log_model_summary', 'tensorboard_dir', 'tensorboard',
                      'stability_metrics', 'performance_metrics', 'profiling_enabled']:
                training_params[key] = value
            elif key == 'verbose':
                training_params['debug_mode'] = value
        
        for key, value in export_config.items():
            if key in ['export_onnx', 'save_model', 'save_metadata',
                      'save_training_history']:
                training_params[key] = value
        
        for key, value in hardware_config.items():
            if key in ['device', 'recommended_gpu_memory', 'minimum_system_requirements',
                      'optimal_system_requirements', 'memory_management',
                      'performance_optimization', 'detected_gpu_memory',
                      'detected_system_memory', 'system_performance_class',
                      'optimization_recommendations', 'cuda_optimizations']:
                training_params[key] = value
        
        for key, value in advanced_config.items():
            if key in ['num_workers', 'pin_memory', 'persistent_workers',
                      'memory_efficient', 'compile_model', 'benchmark_mode',
                      'gradient_checkpointing']:
                training_params[key] = value
        
        for key, value in validation_config.items():
            if key in ['cross_validation', 'metrics', 'validation_frequency',
                      'save_validation_results', 'detailed_metrics', 'robustness_testing',
                      'performance_benchmarking', 'confidence_intervals']:
                if key == 'detailed_metrics':
                    training_params['calculate_detailed_metrics'] = value
                elif key == 'cross_validation' and isinstance(value, dict):
                    if value.get('enabled', False):
                        training_params['cross_validation'] = True
                        training_params['cv_folds'] = value.get('folds', 5)
                    else:
                        training_params['cross_validation'] = False
                else:
                    training_params[key] = value
        
        for key, value in hpo_config.items():
            if key in ['enabled', 'strategy', 'study_name', 'direction', 'n_trials',
                      'timeout', 'sampler', 'pruner', 'objective_metric',
                      'trial_epochs', 'trial_patience', 'cleanup_trials',
                      'generate_plots']:
                hpo_key = f'hpo_{key}' if key != 'enabled' else 'hpo_enabled'
                training_params[hpo_key] = value
        
        for key, value in experimental_config.items():
            if key in ['experimental_features', 'auto_optimize']:
                training_params[key] = value
        
        training_params.update(kwargs)
        training_params['config'] = config
        
        training_params.setdefault('model_dir', DEFAULT_MODEL_DIR)
        training_params.setdefault('log_dir', LOG_DIR)
        training_params.setdefault('tensorboard_dir', TB_DIR)
        
        training_params.setdefault('checkpoint_dir', CHECKPOINTS_DIR)
        training_params.setdefault('data_dir', DATA_DIR)
        training_params.setdefault('config_dir', CONFIG_DIR)
        training_params.setdefault('results_dir', RESULTS_DIR)
        
        if monitoring_config.get('tensorboard_dir'):
            training_params['tensorboard_dir'] = monitoring_config['tensorboard_dir']
        elif system_config.get('log_dir'):
            training_params['tensorboard_dir'] = system_config['log_dir']
        
        if 'tb_dir' in system_config:
            training_params['tb_dir'] = system_config['tb_dir']
        
        results = train_model(**training_params)
        
        if results and results.get('success', False):
            print("\n" + "="*40)
            print("TRAINING COMPLETED SUCCESSFULLY!")
            print("="*40)
            
            _display_training_results(results)
            
        else:
            print("\nTRAINING FAILED")
            if results:
                error_msg = results.get('error', 'Unknown error')
                print(f"Error: {error_msg}")
                
                if results.get('partial_training_completed', False):
                    print(f"\nPartial training completed:")
                    print(f"   Epochs: {results.get('epochs_completed', 0)}")
                    partial_metrics = results.get('training_metrics', {})
                    if partial_metrics:
                        print(f"   Best Loss: {partial_metrics.get('best_val_loss', 'N/A')}")
        
        return results
        
    except Exception as e:
        logger.error(f"Training launch failed: {e}")
        print(f"\nFailed to launch training: {str(e)}")
        return None

def _merge_configs(base_config: Dict[str, Any], preset_config: Dict[str, Any]) -> Dict[str, Any]:
    merged = {}
    
    all_keys = set(base_config.keys()) | set(preset_config.keys())
    
    for key in all_keys:
        base_value = base_config.get(key)
        preset_value = preset_config.get(key)
        
        if base_value is None and preset_value is None:
            continue
        elif base_value is None:
            merged[key] = preset_value.copy() if isinstance(preset_value, dict) else preset_value
        elif preset_value is None:
            merged[key] = base_value.copy() if isinstance(base_value, dict) else base_value
        elif isinstance(base_value, dict) and isinstance(preset_value, dict):
            merged[key] = _merge_configs(base_value, preset_value)
        else:
            merged[key] = base_value
    
    return merged

def _apply_model_type_defaults(config: Dict[str, Any], model_type: str) -> None:
    model_config = config.setdefault('model', {})
    training_config = config.setdefault('training', {})
    hardware_config = config.setdefault('hardware', {})
    monitoring_config = config.setdefault('monitoring', {})
    security_config = config.setdefault('security', {})
    
    if model_type == 'SimpleAutoencoder':
        model_config.update({
            'model_type': 'SimpleAutoencoder',
            'encoding_dim': 16,
            'hidden_dims': [128, 64],
            'dropout_rates': [0.2, 0.15],
            'activation': 'leaky_relu',
            'activation_param': 0.2,
            'normalization': 'batch',
            'use_batch_norm': True,
            'use_layer_norm': False,
            'bias': True,
            'weight_init': 'xavier_uniform',
            'skip_connection': False,
            'residual_blocks': False,
            'use_attention': False,
            'legacy_mode': False,
            'diversity_factor': None,
            'min_features': 5,
            'num_models': None
        })
        
        training_config.update({
            'batch_size': 64,
            'epochs': 50,
            'learning_rate': 0.001,
            'patience': 15,
            'weight_decay': 1e-4,
            'gradient_clip': 1.0,
            'gradient_accumulation_steps': 1,
            'mixed_precision': torch.cuda.is_available(),
            'optimizer': 'Adam',
            'scheduler': 'ReduceLROnPlateau',
            'scheduler_params': {
                'patience': 5,
                'factor': 0.5,
                'min_lr': 1e-6
            },
            'early_stopping': True,
            'validation_split': 0.2,
            'shuffle': True,
            'pin_memory': torch.cuda.is_available(),
            'persistent_workers': True,
            'adam_betas': (0.9, 0.999),
            'adam_eps': 1e-8,
            'lr_patience': 5,
            'lr_factor': 0.5,
            'min_lr': 1e-6
        })
        
    elif model_type == 'EnhancedAutoencoder':
        model_config.update({
            'model_type': 'EnhancedAutoencoder',
            'encoding_dim': 32,
            'hidden_dims': [256, 128, 64],
            'dropout_rates': [0.2, 0.15, 0.1],
            'activation': 'leaky_relu',
            'activation_param': 0.2,
            'normalization': 'batch',
            'use_batch_norm': True,
            'use_layer_norm': False,
            'bias': True,
            'weight_init': 'xavier_uniform',
            'skip_connection': True,
            'residual_blocks': True,
            'use_attention': True,
            'legacy_mode': False,
            'diversity_factor': None,
            'min_features': 5,
            'num_models': None
        })
        
        training_config.update({
            'batch_size': 64,
            'epochs': 100,
            'learning_rate': 0.001,
            'patience': 20,
            'weight_decay': 1e-4,
            'gradient_clip': 1.0,
            'gradient_accumulation_steps': 1,
            'mixed_precision': torch.cuda.is_available(),
            'optimizer': 'AdamW',
            'scheduler': 'ReduceLROnPlateau',
            'scheduler_params': {
                'patience': 7,
                'factor': 0.5,
                'min_lr': 1e-6
            },
            'early_stopping': True,
            'validation_split': 0.2,
            'shuffle': True,
            'pin_memory': torch.cuda.is_available(),
            'persistent_workers': True,
            'adam_betas': (0.9, 0.999),
            'adam_eps': 1e-8,
            'lr_patience': 7,
            'lr_factor': 0.5,
            'min_lr': 1e-6
        })
        
    elif model_type == 'AutoencoderEnsemble':
        model_config.update({
            'model_type': 'AutoencoderEnsemble',
            'encoding_dim': 24,
            'hidden_dims': [192, 96, 48],
            'dropout_rates': [0.25, 0.2, 0.15],
            'activation': 'leaky_relu',
            'activation_param': 0.2,
            'normalization': 'batch',
            'use_batch_norm': True,
            'use_layer_norm': False,
            'bias': True,
            'weight_init': 'xavier_uniform',
            'skip_connection': True,
            'residual_blocks': True,
            'use_attention': True,
            'legacy_mode': False,
            'diversity_factor': 0.3,
            'min_features': 5,
            'num_models': 3
        })
        
        training_config.update({
            'batch_size': 64,
            'epochs': 150,
            'learning_rate': 0.001,
            'patience': 25,
            'weight_decay': 1e-4,
            'gradient_clip': 1.0,
            'gradient_accumulation_steps': 1,
            'mixed_precision': torch.cuda.is_available(),
            'optimizer': 'AdamW',
            'scheduler': 'ReduceLROnPlateau',
            'scheduler_params': {
                'patience': 10,
                'factor': 0.5,
                'min_lr': 1e-6
            },
            'early_stopping': True,
            'validation_split': 0.2,
            'shuffle': True,
            'pin_memory': torch.cuda.is_available(),
            'persistent_workers': True,
            'adam_betas': (0.9, 0.999),
            'adam_eps': 1e-8,
            'lr_patience': 10,
            'lr_factor': 0.5,
            'min_lr': 1e-6
        })
    
    hardware_config.setdefault('device', 'auto')
    hardware_config.setdefault('cuda_optimizations', torch.cuda.is_available())
    hardware_config.setdefault('memory_management', {'enable_memory_efficient': True})
    hardware_config.setdefault('recommended_gpu_memory', 4.0 if model_type == 'SimpleAutoencoder' else 8.0)
    hardware_config.setdefault('minimum_system_requirements', {})
    hardware_config.setdefault('optimal_system_requirements', {})
    hardware_config.setdefault('performance_optimization', {})
    hardware_config.setdefault('detected_gpu_memory', None)
    hardware_config.setdefault('detected_system_memory', None)
    hardware_config.setdefault('system_performance_class', 'auto')
    hardware_config.setdefault('optimization_recommendations', [])
    
    monitoring_config.setdefault('verbose', True)
    monitoring_config.setdefault('tensorboard_logging', True)
    monitoring_config.setdefault('save_checkpoints', True)
    monitoring_config.setdefault('save_best_model', True)
    monitoring_config.setdefault('metrics_to_track', ['loss', 'reconstruction_error'])
    monitoring_config.setdefault('checkpoint_frequency', 10)
    monitoring_config.setdefault('log_frequency', 1)
    monitoring_config.setdefault('metrics_frequency', 1)
    monitoring_config.setdefault('console_logging_level', 'INFO')
    monitoring_config.setdefault('save_model_history', True)
    monitoring_config.setdefault('early_stopping_metric', 'val_loss')
    monitoring_config.setdefault('checkpoint_format', 'pth')
    monitoring_config.setdefault('log_model_summary', True)
    monitoring_config.setdefault('tensorboard_dir', TB_DIR)
    monitoring_config.setdefault('tensorboard', {})
    monitoring_config.setdefault('stability_metrics', True)
    monitoring_config.setdefault('performance_metrics', True)
    monitoring_config.setdefault('profiling_enabled', False)
    
    security_config.setdefault('percentile', 95.0)
    security_config.setdefault('enable_security_metrics', True)
    security_config.setdefault('anomaly_threshold_strategy', 'percentile')
    security_config.setdefault('adaptive_threshold', True)
    security_config.setdefault('attack_threshold', None)
    security_config.setdefault('false_negative_cost', None)
    security_config.setdefault('early_warning_threshold', None)
    security_config.setdefault('confidence_interval', None)
    security_config.setdefault('detection_methods', [])
    security_config.setdefault('alert_levels', [])
    security_config.setdefault('threshold_validation', True)
    security_config.setdefault('robust_detection', True)
    security_config.setdefault('false_positive_tolerance', None)
    security_config.setdefault('performance_optimized_detection', True)
    security_config.setdefault('real_time_monitoring', False)
    security_config.setdefault('ensemble_voting', 'average' if model_type == 'AutoencoderEnsemble' else None)
    security_config.setdefault('uncertainty_threshold', None)

def _estimate_training_time(epochs: int, model_type: str, batch_size: int = 64) -> str:
    base_time_per_epoch = {
        'SimpleAutoencoder': 3.0,
        'EnhancedAutoencoder': 8.0,
        'AutoencoderEnsemble': 20.0
    }
    
    base_time = base_time_per_epoch.get(model_type, 8.0)
    
    batch_factor = (64.0 / batch_size) ** 0.7
    
    device_factor = 1.0
    if torch.cuda.is_available():
        device_factor = 0.25
        try:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            if gpu_memory >= 8:
                device_factor = 0.2
            elif gpu_memory >= 16:
                device_factor = 0.15
        except Exception:
            pass
    
    complexity_factor = 1.0
    if model_type == 'SimpleAutoencoder':
        complexity_factor = 0.6
    elif model_type == 'EnhancedAutoencoder':
        complexity_factor = 1.0
    elif model_type == 'AutoencoderEnsemble':
        complexity_factor = 1.8
    
    total_seconds = epochs * base_time * batch_factor * device_factor * complexity_factor
    total_minutes = total_seconds / 60.0
    
    if total_minutes < 0.5:
        return "< 1"
    elif total_minutes < 1:
        return "1"
    elif total_minutes < 5:
        return f"{total_minutes:.1f}"
    elif total_minutes < 60:
        return f"{int(total_minutes)}"
    else:
        hours = int(total_minutes // 60)
        remaining_minutes = int(total_minutes % 60)
        if hours < 24:
            return f"{hours}h {remaining_minutes}m"
        else:
            days = hours // 24
            remaining_hours = hours % 24
            return f"{days}d {remaining_hours}h"

def _display_training_results(results: Dict[str, Any]) -> None:
    try:
        print("\n" + "="*40)
        print("TRAINING RESULTS SUMMARY")
        print("="*40)
        
        success = results.get('success', False)
        timestamp = results.get('timestamp', 'Unknown')
        run_id = results.get('run_id', 'Unknown')
        
        print(f"Status: {'SUCCESS' if success else 'FAILED'}")
        print(f"Completed: {timestamp}")
        print(f"Run ID: {run_id}")
        
        if not success:
            error_msg = results.get('error', 'Unknown error')
            error_type = results.get('error_type', 'Unknown')
            print(f"Error Type: {error_type}")
            print(f"Error Message: {error_msg}")
            
            partial_training = results.get('partial_training_completed', False)
            if partial_training:
                epochs_completed = results.get('epochs_completed', 0)
                print(f"Partial Training: {epochs_completed} epochs completed")
                
                training_metrics = results.get('training_metrics', {})
                if training_metrics:
                    best_val_loss = training_metrics.get('best_val_loss', 'N/A')
                    last_train_loss = training_metrics.get('last_train_loss', 'N/A')
                    last_val_loss = training_metrics.get('last_val_loss', 'N/A')
                    print(f"Best Validation Loss: {best_val_loss}")
                    print(f"Last Training Loss: {last_train_loss}")
                    print(f"Last Validation Loss: {last_val_loss}")
            
            error_log_path = results.get('error_log_path')
            if error_log_path:
                print(f"Error Log: {error_log_path}")
            
            partial_model_path = results.get('partial_model_path')
            if partial_model_path:
                print(f"Partial Model: {partial_model_path}")
            
            graceful_recovery = results.get('graceful_recovery', False)
            if graceful_recovery:
                print("Graceful recovery was attempted")
            
            return
        
        training_time = results.get('training_time_minutes', 0)
        model_type = results.get('model_type', 'Unknown')
        
        print(f"Model Type: {model_type}")
        print(f"Training Duration: {training_time:.2f} minutes")
        
        final_metrics = results.get('final_metrics', {})
        if final_metrics:
            print(f"\nFINAL METRICS")
            print(f"-" * 40)
            
            best_val_loss = final_metrics.get('best_validation_loss', float('inf'))
            test_loss = final_metrics.get('test_loss', float('inf'))
            final_epoch = final_metrics.get('final_epoch', 'N/A')
            threshold = final_metrics.get('threshold', 'N/A')
            detection_rate = final_metrics.get('anomaly_detection_rate', 0)
            
            print(f"Best Validation Loss: {best_val_loss:.6f}")
            print(f"Test Loss: {test_loss:.6f}")
            print(f"Final Epoch: {final_epoch}")
            print(f"Anomaly Threshold: {threshold:.6f}" if isinstance(threshold, (int, float)) else f"Anomaly Threshold: {threshold}")
            print(f"Detection Rate: {detection_rate*100:.2f}%")
        
        model_info = results.get('model_info', {})
        if model_info:
            print(f"\nMODEL INFORMATION")
            print(f"-" * 40)
            
            model_class = model_info.get('class_name', 'N/A')
            total_params = model_info.get('parameters', 0)
            trainable_params = model_info.get('trainable_parameters', 0)
            model_size = model_info.get('size_mb', 0)
            input_dim = model_info.get('input_dim', 'N/A')
            encoding_dim = model_info.get('encoding_dim', 'N/A')
            
            print(f"Class: {model_class}")
            print(f"Parameters: {total_params:,} total, {trainable_params:,} trainable")
            print(f"Model Size: {model_size:.2f} MB")
            print(f"Input Dimension: {input_dim}")
            print(f"Encoding Dimension: {encoding_dim}")
            
            enhanced_features = model_info.get('enhanced_features', {})
            if enhanced_features:
                features = []
                if enhanced_features.get('attention', False):
                    features.append("Attention")
                if enhanced_features.get('residual_blocks', False):
                    features.append("Residual Blocks")
                if enhanced_features.get('skip_connections', False):
                    features.append("Skip Connections")
                if features:
                    print(f"Enhanced Features: {', '.join(features)}")
            
            ensemble_info = results.get('ensemble_info', {})
            if ensemble_info:
                num_models = ensemble_info.get('num_models', 'N/A')
                diversity_factor = ensemble_info.get('diversity_factor', 'N/A')
                actual_models = ensemble_info.get('actual_models', 'N/A')
                model_types = ensemble_info.get('model_types', {})
                
                print(f"Ensemble Size: {num_models}")
                print(f"Actual Models: {actual_models}")
                print(f"Diversity Factor: {diversity_factor}")
                if model_types:
                    type_distribution = ", ".join([f"{k}: {v}" for k, v in model_types.items()])
                    print(f"Model Distribution: {type_distribution}")
        
        data_info = results.get('data_info', {})
        if data_info:
            print(f"\nDATA INFORMATION")
            print(f"-" * 40)
            
            source = data_info.get('source', 'N/A').title()
            train_samples = data_info.get('train_samples', 0)
            val_samples = data_info.get('val_samples', 0)
            test_samples = data_info.get('test_samples', 0)
            features = data_info.get('features', 0)
            
            print(f"Source: {source}")
            print(f"Training Samples: {train_samples:,}")
            print(f"Validation Samples: {val_samples:,}")
            print(f"Test Samples: {test_samples:,}")
            print(f"Features: {features}")
        
        system_info = results.get('system_info', {})
        if system_info:
            print(f"\nSYSTEM INFORMATION")
            print(f"-" * 40)
            
            device = system_info.get('device', 'N/A')
            device_type = system_info.get('device_type', 'N/A')
            mixed_precision = system_info.get('mixed_precision', False)
            pytorch_version = system_info.get('pytorch_version', 'N/A')
            cuda_available = system_info.get('cuda_available', False)
            cuda_version = system_info.get('cuda_version', 'N/A')
            gpu_count = system_info.get('gpu_count', 0)
            gpu_name = system_info.get('gpu_name', 'N/A')
            
            print(f"Device: {device}")
            print(f"Device Type: {device_type}")
            print(f"Mixed Precision: {'Enabled' if mixed_precision else 'Disabled'}")
            print(f"PyTorch Version: {pytorch_version}")
            
            if cuda_available:
                print(f"CUDA Available: Yes")
                print(f"CUDA Version: {cuda_version}")
                print(f"GPU Count: {gpu_count}")
                if gpu_name != 'N/A':
                    print(f"GPU Name: {gpu_name}")
            else:
                print(f"CUDA Available: No")
        
        training_stats = results.get('training_stats', {})
        if training_stats:
            total_training_time = training_stats.get('total_training_time', 0)
            final_epoch = training_stats.get('final_epoch', 0)
            early_stopped = training_stats.get('early_stopped', False)
            model_compiled = training_stats.get('model_compiled', False)
            
            if total_training_time > 0 or final_epoch > 0:
                print(f"\nTRAINING STATISTICS")
                print(f"-" * 40)
                
                if total_training_time > 0:
                    print(f"Total Training Time: {total_training_time/60:.2f} minutes")
                    if final_epoch > 0:
                        print(f"Time per Epoch: {total_training_time/final_epoch:.2f} seconds")
                
                if final_epoch > 0:
                    print(f"Epochs Completed: {final_epoch}")
                
                print(f"Early Stopping: {'Yes' if early_stopped else 'No'}")
                print(f"Model Compiled: {'Yes' if model_compiled else 'No'}")
            
            training_history = training_stats.get('training_history', {})
            if training_history:
                train_losses = training_history.get('train_loss', [])
                val_losses = training_history.get('val_loss', [])
                learning_rates = training_history.get('learning_rate', [])
                epoch_times = training_history.get('epoch_times', [])
                
                if train_losses and val_losses:
                    print(f"\nTRAINING PROGRESSION")
                    print(f"-" * 40)
                    print(f"Initial Train Loss: {train_losses[0]:.6f}")
                    print(f"Final Train Loss: {train_losses[-1]:.6f}")
                    print(f"Initial Val Loss: {val_losses[0]:.6f}")
                    print(f"Final Val Loss: {val_losses[-1]:.6f}")
                    
                    best_val_idx = val_losses.index(min(val_losses))
                    print(f"Best Val Loss: {val_losses[best_val_idx]:.6f} (epoch {best_val_idx + 1})")
                
                if learning_rates:
                    print(f"Initial Learning Rate: {learning_rates[0]:.2e}")
                    print(f"Final Learning Rate: {learning_rates[-1]:.2e}")
                
                if epoch_times:
                    avg_epoch_time = sum(epoch_times) / len(epoch_times)
                    print(f"Average Epoch Time: {avg_epoch_time:.2f} seconds")
        
        config = results.get('configuration', {})
        if config:
            model_config = config.get('model_architecture', config.get('model', {}))
            training_config = config.get('training_config', config.get('training', {}))
            
            if model_config or training_config:
                print(f"\nCONFIGURATION SUMMARY")
                print(f"-" * 40)
                
                if model_config:
                    activation = model_config.get('activation', 'N/A')
                    normalization = model_config.get('normalization', 'N/A')
                    hidden_dims = model_config.get('hidden_dims', [])
                    dropout_rates = model_config.get('dropout_rates', [])
                    
                    print(f"Activation: {activation}")
                    print(f"Normalization: {normalization}")
                    if hidden_dims:
                        print(f"Hidden Dimensions: {hidden_dims}")
                    if dropout_rates:
                        print(f"Dropout Rates: {dropout_rates}")
                
                if training_config:
                    batch_size = training_config.get('batch_size', 'N/A')
                    learning_rate = training_config.get('learning_rate', 'N/A')
                    optimizer = training_config.get('optimizer_type', training_config.get('optimizer', 'N/A'))
                    scheduler = training_config.get('scheduler_type', training_config.get('scheduler', 'N/A'))
                    
                    print(f"Batch Size: {batch_size}")
                    print(f"Learning Rate: {learning_rate}")
                    print(f"Optimizer: {optimizer}")
                    print(f"Scheduler: {scheduler}")
        
        artifacts = results.get('artifacts', {})
        if artifacts:
            print(f"\nSAVED ARTIFACTS")
            print(f"-" * 40)
            
            for artifact_type, path in artifacts.items():
                if path:
                    artifact_name = artifact_type.replace('_', ' ').title()
                    print(f"{artifact_name}: {path}")
        
        threshold_data = results.get('training_stats', {}).get('threshold_data', {})
        if threshold_data:
            threshold = threshold_data.get('threshold', 'N/A')
            method = threshold_data.get('method', 'N/A')
            metadata = threshold_data.get('metadata', {})
            
            print(f"\nTHRESHOLD INFORMATION")
            print(f"-" * 40)
            print(f"Threshold Value: {threshold:.6f}" if isinstance(threshold, (int, float)) else f"Threshold Value: {threshold}")
            print(f"Calculation Method: {method}")
            
            if isinstance(metadata, dict) and metadata:
                percentile = metadata.get('percentile', None)
                adaptive = metadata.get('adaptive', None)
                
                if percentile is not None:
                    print(f"Percentile Used: {percentile}")
                if adaptive is not None:
                    print(f"Adaptive: {'Yes' if adaptive else 'No'}")
        
        final_evaluation = results.get('training_stats', {}).get('final_evaluation', {})
        if final_evaluation:
            test_metrics = final_evaluation.get('test_metrics', {})
            mse_stats = final_evaluation.get('mse_statistics', {})
            
            if test_metrics or mse_stats:
                print(f"\nDETAILED EVALUATION")
                print(f"-" * 40)
                
                if test_metrics:
                    for metric_name, metric_value in test_metrics.items():
                        # Skip displaying the full config_applied dictionary
                        if metric_name.lower() == 'config_applied':
                            # Display a concise summary instead of the full config
                            if isinstance(metric_value, dict):
                                # Generate inline config summary
                                summary_parts = []
                                
                                try:
                                    # Extract key information from config
                                    if 'metadata' in metric_value:
                                        metadata = metric_value['metadata']
                                        if 'version' in metadata:
                                            summary_parts.append(f"v{metadata['version']}")
                                        if 'last_preset_change' in metadata:
                                            summary_parts.append(f"preset changed {metadata['last_preset_change']}")
                                    
                                    if 'model' in metric_value:
                                        model_config = metric_value['model']
                                        if 'model_type' in model_config:
                                            summary_parts.append(f"model={model_config['model_type']}")
                                        if 'encoding_dim' in model_config:
                                            summary_parts.append(f"encoding={model_config['encoding_dim']}")
                                    
                                    if 'training' in metric_value:
                                        training_config = metric_value['training']
                                        if 'epochs' in training_config:
                                            summary_parts.append(f"epochs={training_config['epochs']}")
                                        if 'batch_size' in training_config:
                                            summary_parts.append(f"batch={training_config['batch_size']}")
                                    
                                    # Count total sections
                                    section_count = len([k for k in metric_value.keys() if isinstance(metric_value.get(k), dict)])
                                    summary_parts.append(f"{section_count} sections")
                                    
                                    if summary_parts:
                                        config_summary = " | ".join(summary_parts)
                                    else:
                                        config_summary = f"<{len(metric_value)} configuration items>"
                                        
                                except Exception as e:
                                    config_summary = f"<configuration summary unavailable: {str(e)}>"
                                
                                print(f"Config Applied: {config_summary}")
                            else:
                                print(f"Config Applied: <configuration data available>")
                            continue
                        
                        if isinstance(metric_value, (int, float)):
                            if metric_name.lower().endswith('loss') or metric_name.lower().endswith('error'):
                                print(f"{metric_name.replace('_', ' ').title()}: {metric_value:.6f}")
                            else:
                                print(f"{metric_name.replace('_', ' ').title()}: {metric_value:.4f}")
                        else:
                            print(f"{metric_name.replace('_', ' ').title()}: {metric_value}")
                
                if mse_stats:
                    print(f"\nMSE Statistics:")
                    for stat_name, stat_value in mse_stats.items():
                        if isinstance(stat_value, (int, float)):
                            print(f"  {stat_name.upper()}: {stat_value:.6f}")
        
        error_info = results.get('error_details', {})
        if error_info:
            partial_results = error_info.get('partial_results', {})
            if partial_results:
                print(f"\nPARTIAL RESULTS")
                print(f"-" * 40)
                
                epochs_completed = partial_results.get('epochs_completed', 0)
                best_val_loss = partial_results.get('best_val_loss', 'N/A')
                
                print(f"Epochs Completed: {epochs_completed}")
                print(f"Best Validation Loss: {best_val_loss}")
        
        performance_metrics = results.get('training_stats', {}).get('training_history', {}).get('detailed_metrics', {})
        if performance_metrics:
            print(f"\nPERFORMANCE METRICS")
            print(f"-" * 40)
            
            for metric_name, metric_values in performance_metrics.items():
                if isinstance(metric_values, list) and metric_values:
                    final_value = metric_values[-1]
                    if isinstance(final_value, (int, float)):
                        metric_display = metric_name.replace('_', ' ').title()
                        if 'loss' in metric_name.lower() or 'error' in metric_name.lower():
                            print(f"{metric_display}: {final_value:.6f}")
                        else:
                            print(f"{metric_display}: {final_value:.4f}")
        
        memory_usage = results.get('training_stats', {}).get('training_history', {}).get('memory_usage', [])
        if memory_usage:
            print(f"\nRESOURCE USAGE")
            print(f"-" * 40)
            
            avg_memory = sum(memory_usage) / len(memory_usage)
            max_memory = max(memory_usage)
            print(f"Average GPU Memory: {avg_memory:.2f} GB")
            print(f"Peak GPU Memory: {max_memory:.2f} GB")
        
        experiment_dir = results.get('training_stats', {}).get('experiment_dir')
        if experiment_dir:
            print(f"\nEXPERIMENT TRACKING")
            print(f"-" * 40)
            print(f"TensorBoard Logs: {experiment_dir}")
        
        print("-"*40)
        
        additional_info = []
        
        if success:
            additional_info.append("Training completed successfully!")
            
            if training_time > 0:
                if training_time < 1:
                    additional_info.append("Very fast training (< 1 minute)")
                elif training_time < 5:
                    additional_info.append("Quick training (< 5 minutes)")
                elif training_time < 30:
                    additional_info.append("Normal training duration")
                else:
                    additional_info.append("Extended training duration")
            
            if final_metrics:
                test_loss = final_metrics.get('test_loss', float('inf'))
                val_loss = final_metrics.get('best_validation_loss', float('inf'))
                
                if test_loss != float('inf') and val_loss != float('inf'):
                    if abs(test_loss - val_loss) / val_loss < 0.1:
                        additional_info.append("Good generalization (test/val loss similar)")
                    elif test_loss > val_loss * 1.5:
                        additional_info.append("Possible overfitting detected")
                
                detection_rate = final_metrics.get('anomaly_detection_rate', 0)
                if detection_rate > 0:
                    if detection_rate < 0.05:
                        additional_info.append("Low anomaly detection rate")
                    elif detection_rate > 0.3:
                        additional_info.append("High anomaly detection rate")
                    else:
                        additional_info.append("Normal anomaly detection rate")
            
            if model_info:
                total_params = model_info.get('parameters', 0)
                if total_params > 0:
                    if total_params < 10000:
                        additional_info.append("Lightweight model")
                    elif total_params > 1000000:
                        additional_info.append("Large model")
                
                model_size = model_info.get('size_mb', 0)
                if model_size > 0:
                    if model_size < 1:
                        additional_info.append("Very compact model")
                    elif model_size > 100:
                        additional_info.append("Large model file")
        
        if additional_info:
            print("SUMMARY")
            print("-" * 40)
            for info in additional_info:
                print(f"- {info}")
            print("-"*40)
        
    except Exception as e:
        print(f"\nError displaying training results: {str(e)}")
        print("Raw results structure:")
        try:
            print(f"Success: {results.get('success', 'Unknown')}")
            print(f"Error: {results.get('error', 'None')}")
            print(f"Available keys: {list(results.keys())}")
        except Exception:
            print("Failed to extract basic result information")
        print("-"*40)

def train_model_quick(
    # Quick Test Parameters
    quick_epochs: Optional[int] = None,
    quick_batch_size: Optional[int] = None,
    quick_learning_rate: Optional[float] = None,
    quick_model_type: Optional[str] = None,
    quick_encoding_dim: Optional[int] = None,
    quick_normal_samples: Optional[int] = None,
    quick_attack_samples: Optional[int] = None,
    quick_features: Optional[int] = None,
    
    # Override Parameters
    use_real_data: Optional[bool] = None,
    device: Optional[str] = None,
    verbose: Optional[bool] = None,
    save_results: Optional[bool] = None,
    export_onnx: Optional[bool] = None,
    tensorboard_logging: Optional[bool] = None,
    
    # System Parameters
    model_dir: Optional[Union[str, Path]] = None,
    random_seed: Optional[int] = None,
    non_interactive: Optional[bool] = None,
    
    # Advanced Quick Options
    minimal_logging: Optional[bool] = None,
    skip_validation: Optional[bool] = None,
    fast_mode: Optional[bool] = None,
    benchmark_mode: Optional[bool] = None,
    
    # Interactive Parameters (new)
    interactive: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    quick_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """Quick model training with enhanced context display and error handling."""
    try:
        # Clear screen and show banner with config
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None and banner_config is not None:
            config = banner_config
        else:
            config = get_current_config()
        
        # Extract configuration context with error handling
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu header with context
        print(Fore.MAGENTA + Style.BRIGHT + "QUICK MODEL TRAINING - FAST TEST MODE")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Active Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        
        # Start timing
        start_time = datetime.now()
        quick_start_time = time.time()
        
        # Initialize configuration with quick defaults
        if config is None:
            try:
                config = get_current_config() if 'get_current_config' in globals() else {}
            except Exception as e:
                logger.warning(f"Failed to load current config: {e}")
                config = {}
        
        # Apply quick-specific configuration
        if quick_config:
            config.setdefault('quick_training', {}).update(quick_config)
        
        # Apply all parameters to configuration
        final_config = config.copy()
        
        # Apply individual parameters
        params = locals().copy()
        params.update(kwargs)
        
        # Remove non-parameter items
        params_to_remove = {
            'config', 'quick_config', 'kwargs', 'start_time', 'quick_start_time', 'datetime', 'traceback', 'time', 'gc', 'warnings', 'Path'
        }
        
        cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
        
        # Set up quick training defaults (optimized for speed)
        quick_training_config = final_config.setdefault('quick_training', {})
        
        # Core quick parameters with speed-optimized defaults
        quick_epochs = quick_training_config.setdefault('quick_epochs', cleaned_params.get('quick_epochs', 10))
        quick_batch_size = quick_training_config.setdefault('quick_batch_size', cleaned_params.get('quick_batch_size', 128))
        quick_learning_rate = quick_training_config.setdefault('quick_learning_rate', cleaned_params.get('quick_learning_rate', 0.01))
        quick_model_type = quick_training_config.setdefault('quick_model_type', cleaned_params.get('quick_model_type', 'SimpleAutoencoder'))
        quick_encoding_dim = quick_training_config.setdefault('quick_encoding_dim', cleaned_params.get('quick_encoding_dim', 8))
        quick_normal_samples = quick_training_config.setdefault('quick_normal_samples', cleaned_params.get('quick_normal_samples', 1000))
        quick_attack_samples = quick_training_config.setdefault('quick_attack_samples', cleaned_params.get('quick_attack_samples', 200))
        quick_features = quick_training_config.setdefault('quick_features', cleaned_params.get('quick_features', 20))
        
        # System parameters
        use_real_data = quick_training_config.setdefault('use_real_data', cleaned_params.get('use_real_data', False))
        device = quick_training_config.setdefault('device', cleaned_params.get('device', 'auto'))
        verbose = quick_training_config.setdefault('verbose', cleaned_params.get('verbose', True))
        save_results = quick_training_config.setdefault('save_results', cleaned_params.get('save_results', True))
        export_onnx = quick_training_config.setdefault('export_onnx', cleaned_params.get('export_onnx', False))
        tensorboard_logging = quick_training_config.setdefault('tensorboard_logging', cleaned_params.get('tensorboard_logging', False))
        model_dir = quick_training_config.setdefault('model_dir', cleaned_params.get('model_dir', DEFAULT_MODEL_DIR / "quick_test"))
        results_dir = quick_training_config.setdefault('results_dir', cleaned_params.get('results_dir', RESULTS_DIR / "quick_test"))
        random_seed = quick_training_config.setdefault('random_seed', cleaned_params.get('random_seed', 42))
        non_interactive = quick_training_config.setdefault('non_interactive', cleaned_params.get('non_interactive', False))
        interactive = quick_training_config.setdefault('interactive', cleaned_params.get('interactive', not cleaned_params.get('non_interactive', False)))
        
        # Speed optimization parameters
        minimal_logging = quick_training_config.setdefault('minimal_logging', cleaned_params.get('minimal_logging', True))
        skip_validation = quick_training_config.setdefault('skip_validation', cleaned_params.get('skip_validation', False))
        fast_mode = quick_training_config.setdefault('fast_mode', cleaned_params.get('fast_mode', True))
        benchmark_mode = quick_training_config.setdefault('benchmark_mode', cleaned_params.get('benchmark_mode', False))
        
        # Interactive prompt if enabled
        if interactive:
            print(Fore.YELLOW + Style.BRIGHT + "\nQuick Training Features:")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Optimized parameters for fast training")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Lightweight model for rapid validation")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Performance metrics and system validation")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Basic functionality and compatibility testing")
            print(Fore.GREEN + Style.BRIGHT + "  └─ Immediate feedback with training results")
            
            # Display current configuration with enhanced styling
            print(Fore.YELLOW + Style.BRIGHT + "\nQuick Training Configurations:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + Style.BRIGHT + f"{quick_model_type}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.GREEN + Style.BRIGHT + f"{quick_epochs}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + Style.BRIGHT + f"{quick_batch_size}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + Style.BRIGHT + f"{quick_learning_rate}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + Style.BRIGHT + f"{'Real Data' if use_real_data else f'{quick_normal_samples + quick_attack_samples} synthetic samples'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Features: " + Fore.GREEN + Style.BRIGHT + f"{quick_features}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Encoding Dimension: " + Fore.GREEN + Style.BRIGHT + f"{quick_encoding_dim}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Device: " + Fore.GREEN + Style.BRIGHT + f"{device}")
            
            estimated_time = (quick_epochs * 0.5) if torch.cuda.is_available() else (quick_epochs * 2.0)
            print(Fore.YELLOW + Style.BRIGHT + "\nPerformance Settings:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Fast Mode: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if fast_mode else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Save Results: " + Fore.GREEN + Style.BRIGHT + f"{'Yes' if save_results else 'No'}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + Style.BRIGHT + f"~{estimated_time:.1f} minutes")
            
            print(Fore.YELLOW + Style.BRIGHT + "\nOptimizations:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Minimal Logging: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if minimal_logging else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Skip Validation: " + Fore.GREEN + Style.BRIGHT + f"{'Yes' if skip_validation else 'No'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ TensorBoard Logging: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if tensorboard_logging else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Benchmark Mode: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if benchmark_mode else 'Disabled'}")
            
            # Configuration options
            print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration Options:")
            print(Fore.WHITE + Style.BRIGHT + "1. Run with current settings")
            print(Fore.WHITE + Style.BRIGHT + "2. Modify training parameters")
            print(Fore.WHITE + Style.BRIGHT + "3. Change model type")
            print(Fore.WHITE + Style.BRIGHT + "4. Adjust data settings")
            print(Fore.WHITE + Style.BRIGHT + "5. Toggle optimizations")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to main menu")
            
            while True:
                try:
                    choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-5): " + Style.RESET_ALL).strip()
                    if choice in ['0', '1', '2', '3', '4', '5']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-5.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                    return {'success': False, 'cancelled': True, 'message': 'Quick training cancelled by user'}
            
            if choice == '0':
                print(Fore.RED + Style.BRIGHT + "Quick training cancelled by user")
                return {'success': False, 'cancelled': True, 'message': 'Quick training cancelled by user'}
            
            elif choice == '2':
                print(Fore.MAGENTA + Style.BRIGHT + "\nTRAINING PARAMETERS")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                # Epochs
                new_epochs = input(Fore.YELLOW + Style.BRIGHT + f"Epochs ({quick_epochs}): " + Style.RESET_ALL).strip()
                if new_epochs:
                    try:
                        quick_epochs = int(new_epochs)
                        quick_training_config['quick_epochs'] = quick_epochs
                        print(Fore.GREEN + Style.BRIGHT + f"\nUpdated epochs to {quick_epochs}")
                    except ValueError:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid input, keeping current value")
                
                # Batch size
                new_batch_size = input(Fore.YELLOW + Style.BRIGHT + f"Batch size ({quick_batch_size}): " + Style.RESET_ALL).strip()
                if new_batch_size:
                    try:
                        quick_batch_size = int(new_batch_size)
                        quick_training_config['quick_batch_size'] = quick_batch_size
                        print(Fore.GREEN + Style.BRIGHT + f"\nUpdated batch size to {quick_batch_size}")
                    except ValueError:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid input, keeping current value")
                
                # Learning rate
                new_lr = input(Fore.YELLOW + Style.BRIGHT + f"Learning rate ({quick_learning_rate}): " + Style.RESET_ALL).strip()
                if new_lr:
                    try:
                        quick_learning_rate = float(new_lr)
                        quick_training_config['quick_learning_rate'] = quick_learning_rate
                        print(Fore.GREEN + Style.BRIGHT + f"\nUpdated learning rate to {quick_learning_rate}")
                    except ValueError:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid input, keeping current value")
                
                # Encoding dimension
                new_encoding = input(Fore.YELLOW + Style.BRIGHT + f"Encoding dimension ({quick_encoding_dim}): " + Style.RESET_ALL).strip()
                if new_encoding:
                    try:
                        quick_encoding_dim = int(new_encoding)
                        quick_training_config['quick_encoding_dim'] = quick_encoding_dim
                        print(Fore.GREEN + Style.BRIGHT + f"\nUpdated encoding dimension to {quick_encoding_dim}")
                    except ValueError:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid input, keeping current value")
            
            elif choice == '3':
                print(Fore.MAGENTA + Style.BRIGHT + "\nMODEL TYPE SELECTION")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                for i, mt in enumerate(model_types, 1):
                    current = Fore.GREEN + " (current)" if mt == quick_model_type else ""
                    if mt == 'SimpleAutoencoder':
                        desc = Fore.CYAN + " - Fastest, lightweight"
                    elif mt == 'EnhancedAutoencoder':
                        desc = Fore.CYAN + " - Balanced performance"
                    else:
                        desc = Fore.CYAN + " - Best accuracy, slower"
                    print(Fore.WHITE + Style.BRIGHT + f"{i}. {mt}{current}{desc}")
                
                model_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select model type (1-{len(model_types)}, Enter to keep current): " + Style.RESET_ALL).strip()
                if model_choice and model_choice.isdigit():
                    idx = int(model_choice) - 1
                    if 0 <= idx < len(model_types):
                        quick_model_type = model_types[idx]
                        quick_training_config['quick_model_type'] = quick_model_type
                        
                        # Adjust defaults for model type
                        if quick_model_type == 'SimpleAutoencoder':
                            quick_training_config['quick_encoding_dim'] = 8
                            quick_encoding_dim = 8
                        elif quick_model_type == 'EnhancedAutoencoder':
                            quick_training_config['quick_encoding_dim'] = 16
                            quick_encoding_dim = 16
                        else:  # AutoencoderEnsemble
                            quick_training_config['quick_encoding_dim'] = 12
                            quick_encoding_dim = 12
                        
                        print(Fore.GREEN + Style.BRIGHT + f"\nUpdated model type to {quick_model_type}")
                        print(Fore.GREEN + Style.BRIGHT + f"\nAdjusted encoding dimension to {quick_encoding_dim}")
            
            elif choice == '4':
                print(Fore.MAGENTA + Style.BRIGHT + "\nDATA SETTINGS")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                # Real vs synthetic data
                use_real_choice = input(Fore.YELLOW + Style.BRIGHT + f"Use real data? (y/N, current: {'Yes' if use_real_data else 'No'}): " + Style.RESET_ALL).strip().lower()
                if use_real_choice in ['y', 'yes']:
                    use_real_data = True
                    quick_training_config['use_real_data'] = True
                    print(Fore.GREEN + Style.BRIGHT + "\nSwitched to real data mode")
                elif use_real_choice in ['n', 'no']:
                    use_real_data = False
                    quick_training_config['use_real_data'] = False
                    print(Fore.GREEN + Style.BRIGHT + "\nUsing synthetic data")
                
                if not use_real_data:
                    # Synthetic data parameters
                    new_normal = input(Fore.YELLOW + Style.BRIGHT + f"Normal samples ({quick_normal_samples}): " + Style.RESET_ALL).strip()
                    if new_normal:
                        try:
                            quick_normal_samples = int(new_normal)
                            quick_training_config['quick_normal_samples'] = quick_normal_samples
                            print(Fore.GREEN + Style.BRIGHT + f"\nUpdated normal samples to {quick_normal_samples}")
                        except ValueError:
                            print(Fore.RED + Style.BRIGHT + "\nInvalid input, keeping current value")
                    
                    new_attack = input(Fore.YELLOW + Style.BRIGHT + f"Attack samples ({quick_attack_samples}): " + Style.RESET_ALL).strip()
                    if new_attack:
                        try:
                            quick_attack_samples = int(new_attack)
                            quick_training_config['quick_attack_samples'] = quick_attack_samples
                            print(Fore.GREEN + Style.BRIGHT + f"\nUpdated attack samples to {quick_attack_samples}")
                        except ValueError:
                            print(Fore.RED + Style.BRIGHT + "\nInvalid input, keeping current value")
                    
                    new_features = input(Fore.YELLOW + Style.BRIGHT + f"Features ({quick_features}): " + Style.RESET_ALL).strip()
                    if new_features:
                        try:
                            quick_features = int(new_features)
                            quick_training_config['quick_features'] = quick_features
                            print(Fore.GREEN + Style.BRIGHT + f"\nUpdated features to {quick_features}")
                        except ValueError:
                            print(Fore.RED + Style.BRIGHT + "\nInvalid input, keeping current value")
            
            elif choice == '5':
                print(Fore.MAGENTA + Style.BRIGHT + "\nOPTIMIZATION SETTINGS")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                # Fast mode
                fast_choice = input(Fore.YELLOW + Style.BRIGHT + f"Fast mode? (Y/n, current: {'Yes' if fast_mode else 'No'}): " + Style.RESET_ALL).strip().lower()
                if fast_choice in ['', 'y', 'yes']:
                    fast_mode = True
                    quick_training_config['fast_mode'] = True
                elif fast_choice in ['n', 'no']:
                    fast_mode = False
                    quick_training_config['fast_mode'] = False
                print(Fore.GREEN + Style.BRIGHT + f"\nFast mode: {'Enabled' if fast_mode else 'Disabled'}")
                
                # Minimal logging
                log_choice = input(Fore.YELLOW + Style.BRIGHT + f"Minimal logging? (Y/n, current: {'Yes' if minimal_logging else 'No'}): " + Style.RESET_ALL).strip().lower()
                if log_choice in ['', 'y', 'yes']:
                    minimal_logging = True
                    quick_training_config['minimal_logging'] = True
                elif log_choice in ['n', 'no']:
                    minimal_logging = False
                    quick_training_config['minimal_logging'] = False
                print(Fore.GREEN + Style.BRIGHT + f"\nMinimal logging: {'Enabled' if minimal_logging else 'Disabled'}")
                
                # TensorBoard logging
                tb_choice = input(Fore.YELLOW + Style.BRIGHT + f"TensorBoard logging? (y/N, current: {'Yes' if tensorboard_logging else 'No'}): " + Style.RESET_ALL).strip().lower()
                if tb_choice in ['y', 'yes']:
                    tensorboard_logging = True
                    quick_training_config['tensorboard_logging'] = True
                elif tb_choice in ['', 'n', 'no']:
                    tensorboard_logging = False
                    quick_training_config['tensorboard_logging'] = False
                print(Fore.GREEN + Style.BRIGHT + f"\nTensorBoard logging: {'Enabled' if tensorboard_logging else 'Disabled'}")
                
                # Save results
                save_choice = input(Fore.YELLOW + Style.BRIGHT + f"Save results? (Y/n, current: {'Yes' if save_results else 'No'}): " + Style.RESET_ALL).strip().lower()
                if save_choice in ['', 'y', 'yes']:
                    save_results = True
                    quick_training_config['save_results'] = True
                elif save_choice in ['n', 'no']:
                    save_results = False
                    quick_training_config['save_results'] = False
                print(Fore.GREEN + Style.BRIGHT + f"\nSave results: {'Enabled' if save_results else 'Disabled'}")
                
                # Skip validation
                skip_choice = input(Fore.YELLOW + Style.BRIGHT + f"Skip validation? (y/N, current: {'Yes' if skip_validation else 'No'}): " + Style.RESET_ALL).strip().lower()
                if skip_choice in ['y', 'yes']:
                    skip_validation = True
                    quick_training_config['skip_validation'] = True
                elif skip_choice in ['', 'n', 'no']:
                    skip_validation = False
                    quick_training_config['skip_validation'] = False
                print(Fore.GREEN + Style.BRIGHT + f"\nSkip validation: {'Enabled' if skip_validation else 'Disabled'}")
            
            # Final confirmation
            print(Fore.MAGENTA + Style.BRIGHT + f"\nUpdated Quick Training Configuration:")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            print(Fore.YELLOW + Style.BRIGHT + "Core Settings:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + Style.BRIGHT + f"{quick_model_type}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.GREEN + Style.BRIGHT + f"{quick_epochs}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + Style.BRIGHT + f"{quick_batch_size}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + Style.BRIGHT + f"{quick_learning_rate}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + Style.BRIGHT + f"{'Real Data' if use_real_data else f'{quick_normal_samples + quick_attack_samples} synthetic samples'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Features: " + Fore.GREEN + Style.BRIGHT + f"{quick_features}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Encoding Dimension: " + Fore.GREEN + Style.BRIGHT + f"{quick_encoding_dim}")
            
            estimated_time = (quick_epochs * 0.5) if torch.cuda.is_available() else (quick_epochs * 2.0)
            print(Fore.YELLOW + Style.BRIGHT + "\nPerformance:")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + Style.BRIGHT + f"~{estimated_time:.1f} minutes")
            
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nProceed with quick training? (Y/n): " + Style.RESET_ALL).lower().strip()
            if confirm not in ('', 'y', 'yes'):
                print(Fore.RED + Style.BRIGHT + "\nQuick training cancelled by user")
                return {'success': False, 'cancelled': True, 'message': 'Quick training cancelled by user'}
        
        # Set up logging level
        # if verbose and not minimal_logging:
        #     original_level = logger.level
        #     logger.setLevel(logging.INFO)
        # elif minimal_logging:
        #     original_level = logger.level
        #     logger.setLevel(logging.WARNING)
        
        # Set up logging
        if verbose:
            handlers_to_suppress = []
            for handler in logger.handlers:
                if isinstance(handler, logging.StreamHandler) and handler.stream.name in ['<stdout>', '<stderr>']:
                    handlers_to_suppress.append(handler)
                    handler.setLevel(logging.CRITICAL)  # Temporarily suppress console output
        
        try:
            # Display quick training header with enhanced styling
            if verbose:
                
                print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration:")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{quick_model_type}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{quick_epochs}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data: " + Fore.YELLOW + Style.BRIGHT + f"{quick_normal_samples + quick_attack_samples} samples, {quick_features} features")
                print(Fore.GREEN + Style.BRIGHT + f"  └─ Mode: " + Fore.YELLOW + Style.BRIGHT + f"{'Real data' if use_real_data else 'Synthetic data'}")
                print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            # Prepare comprehensive training configuration optimized for speed
            comprehensive_config = {
                # Model architecture - simplified for speed
                #'model_architecture': {
                'model': {
                    'model_type': quick_model_type,
                    'input_dim': quick_features,
                    'encoding_dim': quick_encoding_dim,
                    # Simplified architecture for quick training
                    'hidden_dims': [max(16, quick_features // 2)] if quick_model_type == 'SimpleAutoencoder' else [64, 32],
                    'dropout_rates': [0.1] if quick_model_type == 'SimpleAutoencoder' else [0.1, 0.1],
                    'activation': 'leaky_relu',
                    'normalization': None if quick_model_type == 'SimpleAutoencoder' else 'batch',
                    'skip_connection': False if quick_model_type == 'SimpleAutoencoder' else True,
                    'residual_blocks': False if quick_model_type == 'SimpleAutoencoder' else False,  # Disabled for speed
                    'use_attention': False,  # Disabled for speed in quick mode
                    'legacy_mode': False,
                    # Ensemble parameters (minimal for speed)
                    'num_models': 2 if quick_model_type == 'AutoencoderEnsemble' else None,
                    'diversity_factor': 0.1 if quick_model_type == 'AutoencoderEnsemble' else None
                },
                
                # Training configuration - optimized for speed
                #'training_config': {
                'training': {
                    'batch_size': quick_batch_size,
                    'epochs': quick_epochs,
                    'learning_rate': quick_learning_rate,
                    'patience': max(3, quick_epochs // 3),  # Early stopping for speed
                    'weight_decay': 1e-4,
                    'gradient_clip': 1.0,
                    'gradient_accumulation_steps': 1,  # No accumulation for speed
                    'mixed_precision': torch.cuda.is_available() and not benchmark_mode,  # Enable if CUDA available
                    'optimizer_type': 'AdamW',
                    'scheduler_type': 'ReduceLROnPlateau' if quick_epochs > 5 else None,  # Skip scheduler for very quick training
                    'scheduler_params': {
                        'patience': 2,
                        'factor': 0.5,
                        'min_lr': 1e-6
                    } if quick_epochs > 5 else {},
                    'early_stopping': True,
                    'validation_split': 0.2
                },
                
                # Data configuration - synthetic for speed
                #'data_config': {
                'data': {
                    'normal_samples': quick_normal_samples,
                    'attack_samples': quick_attack_samples,
                    'features': quick_features,
                    'use_real_data': use_real_data,
                    'data_preprocessing': not fast_mode,  # Skip preprocessing in fast mode
                    'normalization_method': 'standard' if not fast_mode else 'none',
                    'synthetic_config': {
                        'generation_method': 'mixed',
                        'noise_level': 0.01,
                        'anomaly_factor': 1.5,  # Moderate anomaly factor for quick convergence
                        'validation_split': 0.2,
                        'test_split': 1.0,  # Use all attack data for testing
                        'shuffle': True
                    }
                },
                
                # Security configuration - simplified for speed
                #'security_config': {
                'security': {
                    'percentile': 95.0,  # Standard percentile
                    'attack_threshold': None,  # Will be calculated
                    'false_negative_cost': 1.0,
                    'enable_security_metrics': True,
                    'threshold_method': 'percentile',  # Fast threshold method
                    'adaptive_threshold': False  # Disabled for speed
                },
                
                # System configuration - optimized for speed
                #'system_config': {
                'system': {
                    'model_dir': Path(model_dir),
                    'log_dir': Path(model_dir) / "logs",
                    'tensorboard_dir': Path(model_dir) / "tensorboard",
                    'tb_dir': Path(model_dir) / "tensorboard",
                    'config_dir': Path(model_dir) / "config",
                    'checkpoint_dir': Path(model_dir) / "checkpoints",
                    'device': device,
                    'random_seed': random_seed,
                    'reproducible': not benchmark_mode  # Disable for benchmark mode
                },
                
                # Monitoring configuration - minimal for speed
                #'monitoring_config': {
                'monitoring': {
                    'verbose': verbose and not minimal_logging,
                    'debug_mode': False,
                    'tensorboard_dir': Path(model_dir) / "tensorboard",
                    'tb_dir': Path(model_dir) / "tensorboard",
                    'tensorboard_logging': tensorboard_logging,
                    'save_checkpoints': False if fast_mode else True,
                    'checkpoint_frequency': max(5, quick_epochs // 2),
                    'log_frequency': 1 if quick_epochs <= 10 else 2,
                    'metrics_frequency': 5,
                    'progress_bar': verbose and not minimal_logging
                },
                
                # Export configuration - minimal for speed
                #'export_config': {
                'export': {
                    'export_onnx': export_onnx,
                    'save_model': save_results,
                    'save_metadata': save_results,
                    'save_training_history': save_results and not fast_mode
                },
                
                # Advanced training - speed optimized
                'advanced_training': {
                    'num_workers': 0 if fast_mode else min(2, NUM_WORKERS),  # Minimal workers for speed
                    'pin_memory': False if fast_mode else torch.cuda.is_available(),
                    'persistent_workers': False,  # Disabled for speed
                    'compile_model': False,  # Disabled for compatibility and speed
                    'benchmark_mode': benchmark_mode,
                    'memory_efficient': True,
                    'gradient_checkpointing': False  # Disabled for speed
                },
                
                # Validation configuration - minimal for speed
                #'validation_config': {
                'validation': {
                    'calculate_detailed_metrics': not fast_mode,
                    'cross_validation': False,  # Disabled for speed
                    'cv_folds': 3
                },
                
                # Error handling - permissive for quick testing
                'error_handling': {
                    'error_handling': 'continue',  # Continue on errors for quick testing
                    'continue_on_error': True,
                    'graceful_degradation': True,
                    'fallback_mode': True
                },
                
                # Experimental features - disabled for stability and speed
                'experimental': {
                    'experimental_features': False,
                    'auto_optimize': False
                }
            }
            
            # Apply any additional configuration from parameters
            for section_name, section_config in final_config.items():
                if section_name in comprehensive_config:
                    if isinstance(section_config, dict) and isinstance(comprehensive_config[section_name], dict):
                        comprehensive_config[section_name].update(section_config)
                    else:
                        comprehensive_config[section_name] = section_config
                else:
                    comprehensive_config[section_name] = section_config
            
            # Initialize quick training statistics
            quick_stats = {
                'start_time': start_time.isoformat(),
                'mode': 'quick_training',
                'configuration': comprehensive_config,
                'quick_parameters': {
                    'epochs': quick_epochs,
                    'batch_size': quick_batch_size,
                    'learning_rate': quick_learning_rate,
                    'model_type': quick_model_type,
                    'encoding_dim': quick_encoding_dim,
                    'samples': quick_normal_samples + quick_attack_samples,
                    'features': quick_features
                },
                'optimizations': {
                    'fast_mode': fast_mode,
                    'minimal_logging': minimal_logging,
                    'skip_validation': skip_validation,
                    'benchmark_mode': benchmark_mode
                }
            }
            
            if verbose:
                logger.info("Starting quick training with optimized configuration")
                logger.info(f"Target: {quick_epochs} epochs, {quick_batch_size} batch size, {quick_learning_rate} learning rate")
            
            # Execute the comprehensive training pipeline
            training_results = train_model(config=comprehensive_config)
            
            # Calculate quick training duration
            quick_duration = time.time() - quick_start_time
            
            # Process and enhance results for quick training context
            if training_results and training_results.get('success', False):
                # Extract key metrics
                final_metrics = training_results.get('final_metrics', {})
                model_info = training_results.get('model_info', {})
                training_time = training_results.get('training_time_minutes', 0)
                
                # Prepare quick training results
                quick_results = {
                    'success': True,
                    'mode': 'quick_training',
                    'quick_training_time_minutes': quick_duration / 60,
                    'quick_stats': quick_stats,
                    
                    # Core results from comprehensive training
                    'run_id': training_results.get('run_id'),
                    'timestamp': training_results.get('timestamp'),
                    'model_type': quick_model_type,
                    'training_time_minutes': training_time,
                    
                    # Key metrics
                    'final_metrics': {
                        'best_validation_loss': final_metrics.get('best_validation_loss', float('inf')),
                        'test_loss': final_metrics.get('test_loss', float('inf')),
                        'anomaly_detection_rate': final_metrics.get('anomaly_detection_rate', 0.0),
                        'threshold': final_metrics.get('threshold', 0.0),
                        'final_epoch': final_metrics.get('final_epoch', 0),
                        'epochs_target': quick_epochs,
                        'training_completed': final_metrics.get('final_epoch', 0) > 0
                    },
                    
                    # Model information
                    'model_info': {
                        'type': quick_model_type,
                        'class_name': model_info.get('class_name', 'Unknown'),
                        'parameters': model_info.get('parameters', 0),
                        'trainable_parameters': model_info.get('trainable_parameters', 0),
                        'size_mb': model_info.get('size_mb', 0),
                        'input_dim': quick_features,
                        'encoding_dim': quick_encoding_dim
                    },
                    
                    # Data information
                    'data_info': training_results.get('data_info', {}),
                    
                    # System information
                    'system_info': training_results.get('system_info', {}),
                    
                    # Artifacts and paths
                    'artifacts': training_results.get('artifacts', {}),
                    
                    # Quick training specific metrics
                    'quick_metrics': {
                        'epochs_per_minute': final_metrics.get('final_epoch', 0) / (training_time + 0.001),
                        'samples_per_second': (quick_normal_samples + quick_attack_samples) * final_metrics.get('final_epoch', 0) / (quick_duration + 0.001),
                        'convergence_rate': 'fast' if final_metrics.get('best_validation_loss', 1.0) < 0.1 else 'moderate' if final_metrics.get('best_validation_loss', 1.0) < 0.5 else 'slow',
                        'efficiency_score': min(1.0, (quick_epochs - final_metrics.get('final_epoch', quick_epochs)) / quick_epochs + (0.5 - min(0.5, final_metrics.get('best_validation_loss', 1.0))))
                    },
                    
                    # Performance analysis
                    'performance_analysis': {
                        'training_speed': 'excellent' if training_time < 2 else 'good' if training_time < 5 else 'acceptable' if training_time < 10 else 'slow',
                        'convergence_quality': 'excellent' if final_metrics.get('best_validation_loss', 1.0) < 0.05 else 'good' if final_metrics.get('best_validation_loss', 1.0) < 0.1 else 'acceptable',
                        'resource_efficiency': 'optimal' if quick_duration < 300 else 'good' if quick_duration < 600 else 'acceptable',
                        'overall_rating': 'excellent'  # Will be calculated below
                    },
                    
                    # Recommendations for production use
                    'recommendations': [],
                    
                    # Original comprehensive results for reference
                    'comprehensive_results': training_results
                }
                
                # Calculate overall rating
                speed_score = 1.0 if training_time < 2 else 0.8 if training_time < 5 else 0.6 if training_time < 10 else 0.4
                quality_score = 1.0 if final_metrics.get('best_validation_loss', 1.0) < 0.05 else 0.8 if final_metrics.get('best_validation_loss', 1.0) < 0.1 else 0.6
                efficiency_score = quick_results['quick_metrics']['efficiency_score']
                
                overall_score = (speed_score + quality_score + efficiency_score) / 3
                
                if overall_score >= 0.9:
                    quick_results['performance_analysis']['overall_rating'] = 'excellent'
                elif overall_score >= 0.7:
                    quick_results['performance_analysis']['overall_rating'] = 'good'
                elif overall_score >= 0.5:
                    quick_results['performance_analysis']['overall_rating'] = 'acceptable'
                else:
                    quick_results['performance_analysis']['overall_rating'] = 'needs_improvement'
                
                # Generate recommendations
                recommendations = []
                
                if final_metrics.get('best_validation_loss', 1.0) > 0.1:
                    recommendations.append("Consider increasing epochs or adjusting learning rate for better convergence")
                
                if training_time > 10:
                    recommendations.append("Training time is high - consider using GPU acceleration or reducing model complexity")
                
                if final_metrics.get('final_epoch', 0) < quick_epochs * 0.5:
                    recommendations.append("Training converged early - model may be too simple or learning rate too high")
                
                if final_metrics.get('anomaly_detection_rate', 0) < 0.05:
                    recommendations.append("Low anomaly detection rate - consider adjusting threshold or model parameters")
                elif final_metrics.get('anomaly_detection_rate', 0) > 0.3:
                    recommendations.append("High anomaly detection rate - threshold may be too sensitive")
                
                if model_info.get('parameters', 0) > 100000:
                    recommendations.append("Large model detected - consider simplifying architecture for production deployment")
                
                if not torch.cuda.is_available():
                    recommendations.append("GPU acceleration not available - consider enabling CUDA for better performance")
                
                quick_results['recommendations'] = recommendations
                
                # Display quick training results with enhanced styling
                if verbose:
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nTraining Summary:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{quick_model_type}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('final_epoch', 0)}/{quick_epochs}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Duration: " + Fore.YELLOW + Style.BRIGHT + f"{training_time:.1f} minutes")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Status: " + Fore.GREEN + Style.BRIGHT + f"COMPLETED")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nKey Metrics:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Best Validation Loss: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('best_validation_loss', 0):.6f}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Test Loss: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('test_loss', 0):.6f}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Anomaly Threshold: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('threshold', 0):.6f}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Detection Rate: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('anomaly_detection_rate', 0)*100:.1f}%")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nPerformance Analysis:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Training Speed: " + Fore.YELLOW + Style.BRIGHT + f"{quick_results['performance_analysis']['training_speed'].title()}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Convergence Quality: " + Fore.YELLOW + Style.BRIGHT + f"{quick_results['performance_analysis']['convergence_quality'].title()}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Overall Rating: " + Fore.YELLOW + Style.BRIGHT + f"{quick_results['performance_analysis']['overall_rating'].title()}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nModel Information:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Parameters: " + Fore.YELLOW + Style.BRIGHT + f"{model_info.get('parameters', 0):,}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Size: " + Fore.YELLOW + Style.BRIGHT + f"{model_info.get('size_mb', 0):.1f} MB")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Architecture: " + Fore.YELLOW + Style.BRIGHT + f"{quick_features} → {quick_encoding_dim}")
                    
                    # Artifacts
                    artifacts = training_results.get('artifacts', {})
                    if artifacts:
                        print(Fore.YELLOW + Style.BRIGHT + "\nSaved Artifacts:")
                        for artifact_type, path in artifacts.items():
                            if artifact_type in ['model_path', 'threshold_path', 'metadata_path']:
                                print(Fore.GREEN + Style.BRIGHT + f"  ├─ {artifact_type.replace('_', ' ').title()}: " + Fore.YELLOW + Style.BRIGHT + f"{path}")
                    
                    # Recommendations
                    if recommendations:
                        print(Fore.YELLOW + Style.BRIGHT + "\nRecommendations for Improvement:")
                        for i, rec in enumerate(recommendations, 1):
                            print(Fore.CYAN + Style.BRIGHT + f"{i}. {rec}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.GREEN + Style.BRIGHT + "Quick training completed!")
                    print(Fore.GREEN + Style.BRIGHT + "Ready for production or further testing.")
                    print(Fore.YELLOW + Style.BRIGHT + "-"*40)
            
            else:
                # Training failed or returned error
                error_info = training_results.get('error', 'Unknown training error') if training_results else 'No training results returned'
                
                quick_results = {
                    'success': False,
                    'mode': 'quick_training',
                    'error': error_info,
                    'error_type': training_results.get('error_type', 'Unknown') if training_results else 'Unknown',
                    'quick_training_time_minutes': quick_duration / 60,
                    'quick_stats': quick_stats,
                    'timestamp': start_time.isoformat(),
                    'model_type': quick_model_type,
                    'configuration': comprehensive_config,
                    'partial_results': training_results.get('partial_results', {}) if training_results else {},
                    'recommendations': [
                        "Check system compatibility and dependencies",
                        "Try running stability test first: run_stability_test()",
                        "Consider using different model parameters",
                        "Verify data generation is working correctly"
                    ]
                }
                
                if verbose:
                    
                    print(Fore.RED + Style.BRIGHT + "\nError Details:")
                    print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Error: " + Fore.RED + Style.BRIGHT + f"{error_info}")
                    print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Duration: " + Fore.RED + Style.BRIGHT + f"{quick_duration/60:.1f} minutes")
                    print(Fore.YELLOW + Style.BRIGHT + f"  └─ Model Type: " + Fore.RED + Style.BRIGHT + f"{quick_model_type}")
                    
                    if training_results and training_results.get('partial_results'):
                        partial = training_results['partial_results']
                        print(Fore.RED + Style.BRIGHT + "\nPartial Results:")
                        if partial.get('epochs_completed', 0) > 0:
                            print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Epochs Completed: " + Fore.RED + Style.BRIGHT + f"{partial.get('epochs_completed', 0)}")
                            print(Fore.YELLOW + Style.BRIGHT + f"  └─ Last Training Loss: " + Fore.RED + Style.BRIGHT + f"{partial.get('last_train_loss', 'N/A')}")
                    
                    for i, rec in enumerate(quick_results['recommendations'], 1):
                        print(Fore.RED + Style.BRIGHT + f"{i}. {rec}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "-"*40)
            
            # Save quick training results if requested
            if save_results:
                try:
                    results_dir = Path(model_dir)
                    results_dir.mkdir(parents=True, exist_ok=True)
                    
                    quick_results_path = results_dir / f"quick_training_results_{start_time.strftime('%Y%m%d_%H%M%S')}.json"
                    with open(quick_results_path, 'w') as f:
                        json.dump(quick_results, f, indent=2, default=str)
                    
                    quick_results['quick_results_path'] = str(quick_results_path)
                    
                    if verbose:
                        print(Fore.GREEN + Style.BRIGHT + f"\nQuick training results saved to: {quick_results_path}")
                
                except Exception as e:
                    logger.warning(f"Failed to save quick training results: {e}")
                    quick_results.setdefault('warnings', []).append(f"Failed to save results: {e}")
            
            return quick_results
            
        except Exception as e:
            # Handle unexpected errors in quick training
            error_msg = f"Quick training failed: {str(e)}"
            logger.error(error_msg)
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            quick_duration = time.time() - quick_start_time
            
            error_results = {
                'success': False,
                'mode': 'quick_training',
                'error': error_msg,
                'error_type': type(e).__name__,
                'traceback': traceback.format_exc(),
                'quick_training_time_minutes': quick_duration / 60,
                'timestamp': start_time.isoformat(),
                'model_type': quick_model_type,
                'configuration': locals().get('comprehensive_config', {}),
                'quick_parameters': {
                    'epochs': quick_epochs,
                    'batch_size': quick_batch_size,
                    'learning_rate': quick_learning_rate,
                    'model_type': quick_model_type,
                    'encoding_dim': quick_encoding_dim,
                    'samples': quick_normal_samples + quick_attack_samples,
                    'features': quick_features
                },
                'system_info': {
                    'platform': sys.platform,
                    'python_version': sys.version.split()[0],
                    'pytorch_version': torch.__version__,
                    'cuda_available': torch.cuda.is_available()
                },
                'recommendations': [
                    "Run stability test to check system compatibility: run_stability_test()",
                    "Try with simpler parameters: fewer epochs, smaller model",
                    "Check error details in traceback for specific issues",
                    "Verify all dependencies are properly installed",
                    "Try running train_model() with explicit configuration"
                ]
            }
            
            if verbose:
                
                print(Fore.RED + Style.BRIGHT + "Error Details:")
                print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Error: " + Fore.RED + Style.BRIGHT + f"{str(e)}")
                print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Duration: " + Fore.RED + Style.BRIGHT + f"{quick_duration/60:.1f} minutes")
                print(Fore.YELLOW + Style.BRIGHT + f"  └─ Error Type: " + Fore.RED + Style.BRIGHT + f"{type(e).__name__}")
                
                for i, rec in enumerate(error_results['recommendations'], 1):
                    print(Fore.RED + Style.BRIGHT + f"{i}. {rec}")
                
                print(Fore.YELLOW + Style.BRIGHT + "\nFor detailed error information,")
                print(Fore.YELLOW + Style.BRIGHT + "check the returned error_results.")
                print(Fore.RED + Style.BRIGHT + "-"*40)
            
            # Save error results if possible
            if save_results:
                try:
                    results_dir = Path(model_dir)
                    results_dir.mkdir(parents=True, exist_ok=True)
                    
                    error_path = results_dir / f"quick_training_error_{start_time.strftime('%Y%m%d_%H%M%S')}.json"
                    with open(error_path, 'w') as f:
                        json.dump(error_results, f, indent=2, default=str)
                    
                    error_results['error_log_path'] = str(error_path)
                
                except Exception as save_error:
                    logger.warning(f"Failed to save error results: {save_error}")
            
            return error_results
        
        finally:
            # Restore logging level
            # if (verbose or minimal_logging) and 'original_level' in locals():
            #     try:
            #         logger.setLevel(original_level)
            #     except Exception:
            #         pass
            
            # Restore logging level
            if (verbose or minimal_logging) and 'original_level' in locals():
                try:
                    for handler in handlers_to_suppress:
                        handler.setLevel(logging.ERROR)
                except Exception:
                    pass
            
            # Final cleanup
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
            except Exception:
                pass
    
    except (EOFError, KeyboardInterrupt):
        print(Fore.RED + Style.BRIGHT + "\n\nQuick training interrupted by user!")
        return {'success': False, 'cancelled': True, 'message': 'Quick training interrupted by user'}
    except Exception as e:
        logger.error(f"Quick training setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during quick training setup: {str(e)}\n"
            f"Context:\n"
            f"- Quick Epochs: {quick_epochs}\n"
            f"- Model Type: {quick_model_type}\n"
            f"- Interactive Mode: {interactive}\n\n"
            f"This could be due to:\n"
            f"- Configuration file corruption\n"
            f"- Missing dependencies\n"
            f"- System resource issues\n"
            f"- Invalid parameter combinations"
        )

        print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
        print(Fore.RED + Style.BRIGHT + "QUICK TRAINING SETUP ERROR")
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + message)
        print(Fore.RED + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        
        return {'success': False, 'error': str(e), 'message': 'Quick training setup failed'}

def train_model_custom(config: Optional[Dict[str, Any]] = None):
    """Custom model training configuration with context display and error handling."""
    try:
        # Clear screen and show banner with config
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        # if banner_config is not None:
        #     config = banner_config
        # elif config is None:
        #     config = get_current_config()
        
        if config is None and banner_config is not None:
            config = banner_config
        else:
            config = get_current_config()
        
        # Extract configuration context with error handling
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu header with context
        #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "CUSTOM TRAINING CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + f"Active Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        
        base_config = {}
        try:
            base_config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception as e:
            logger.warning(f"Failed to load current config: {e}")
            console.print(
                Panel.fit(
                    #f"Warning: Failed to load current config, using defaults: {str(e)}",
                    f"Failed to load current config, using defaults: {str(e)}",
                    title="WARNING",
                    style="bold yellow",
                    border_style="yellow",
                    padding=(1, 1),
                    box=box.ROUNDED
                )
            )
            base_config = {}
        
        print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration includes:")
        print(Fore.GREEN + Style.BRIGHT + "  ├─ Model architecture parameters")
        print(Fore.GREEN + Style.BRIGHT + "  ├─ Training hyperparameters")
        print(Fore.GREEN + Style.BRIGHT + "  ├─ Data configuration")
        print(Fore.GREEN + Style.BRIGHT + "  ├─ System and hardware settings")
        print(Fore.GREEN + Style.BRIGHT + "  └─ Security and monitoring options")
        
        print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration Sections:")
        print(Fore.WHITE + Style.BRIGHT + "1. Model Architecture " + Fore.GREEN + Style.BRIGHT + "(Layers, Activation, Normalization)")
        print(Fore.WHITE + Style.BRIGHT + "2. Training Parameters " + Fore.GREEN + Style.BRIGHT + "(Optimizer, Scheduler, Regularization)")
        print(Fore.WHITE + Style.BRIGHT + "3. Data Configuration " + Fore.GREEN + Style.BRIGHT + "(Source, Preprocessing, Splits)")
        print(Fore.WHITE + Style.BRIGHT + "4. System & Hardware " + Fore.GREEN + Style.BRIGHT + "(Device, Memory, Performance)")
        print(Fore.WHITE + Style.BRIGHT + "5. Security & Anomaly Detection " + Fore.GREEN + Style.BRIGHT + "(Thresholds, Metrics)")
        print(Fore.WHITE + Style.BRIGHT + "6. Monitoring & Logging " + Fore.GREEN + Style.BRIGHT + "(TensorBoard, Checkpoints)")
        print(Fore.WHITE + Style.BRIGHT + "7. Advanced Options " + Fore.GREEN + Style.BRIGHT + "(Experimental Features)")
        print(Fore.WHITE + Style.BRIGHT + "8. Complete Configuration Review " + Fore.GREEN + Style.BRIGHT + "(Summary & Validation)")
        print(Fore.WHITE + Style.BRIGHT + "9. Start Training " + Fore.GREEN + Style.BRIGHT + "(Launch with Current Settings)")
        print(Fore.RED + Style.BRIGHT + "0. Exit " + Fore.GREEN + Style.BRIGHT + "(Return to Previous Menu)")
        
        final_config = base_config.copy()
        
        while True:
            try:
                section_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect section to configure (0-9): " + Style.RESET_ALL).strip()
                
                if section_choice == '0':
                    print(Fore.RED + Style.BRIGHT + "\nCustom training cancelled")
                    return None
                
                elif section_choice == '1':
                    print(Fore.MAGENTA + Style.BRIGHT + "\nMODEL ARCHITECTURE CONFIGURATION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    model_config = final_config.setdefault('model', {})
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Model Type:")
                    model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                    for i, mtype in enumerate(model_types, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if mtype == model_config.get('model_type', 'EnhancedAutoencoder') else ""
                        if mtype == 'SimpleAutoencoder':
                            desc = Fore.BLUE + Style.BRIGHT + " - Fastest, lightweight"
                        elif mtype == 'EnhancedAutoencoder':
                            desc = Fore.CYAN + Style.BRIGHT + " - Balanced performance"
                        else:
                            desc = Fore.MAGENTA + Style.BRIGHT + " - Best accuracy, slower"
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {mtype}{current}{desc}")
                    
                    while True:
                        try:
                            model_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect model type (1-{len(model_types)}, default=2): " + Style.RESET_ALL).strip()
                            if model_choice in ['', '1', '2', '3']:
                                break
                            print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(model_types)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    model_type = model_types[int(model_choice)-1] if model_choice else 'EnhancedAutoencoder'
                    model_config['model_type'] = model_type
                    
                    print(Fore.YELLOW + Style.BRIGHT + f"\nArchitecture for {model_type}:")
                    
                    encoding_dim = input(Fore.YELLOW + Style.BRIGHT + "Encoding dimension (16/32/24 for Simple/Enhanced/Ensemble): " + Style.RESET_ALL).strip()
                    if encoding_dim:
                        model_config['encoding_dim'] = int(encoding_dim)
                    elif model_type == 'SimpleAutoencoder':
                        model_config['encoding_dim'] = 16
                    elif model_type == 'EnhancedAutoencoder':
                        model_config['encoding_dim'] = 32
                    elif model_type == 'AutoencoderEnsemble':
                        model_config['encoding_dim'] = 24
                    
                    hidden_dims_input = input(Fore.YELLOW + Style.BRIGHT + "Hidden layers (comma-separated, e.g., 256,128,64): " + Style.RESET_ALL).strip()
                    if hidden_dims_input:
                        model_config['hidden_dims'] = [int(x.strip()) for x in hidden_dims_input.split(',')]
                    elif model_type == 'SimpleAutoencoder':
                        model_config['hidden_dims'] = [128, 64]
                    elif model_type == 'EnhancedAutoencoder':
                        model_config['hidden_dims'] = [256, 128, 64]
                    elif model_type == 'AutoencoderEnsemble':
                        model_config['hidden_dims'] = [192, 96, 48]
                    
                    dropout_input = input(Fore.YELLOW + Style.BRIGHT + "Dropout rates (comma-separated, e.g., 0.2,0.15,0.1): " + Style.RESET_ALL).strip()
                    if dropout_input:
                        model_config['dropout_rates'] = [float(x.strip()) for x in dropout_input.split(',')]
                    else:
                        hidden_len = len(model_config['hidden_dims'])
                        if hidden_len == 2:
                            model_config['dropout_rates'] = [0.2, 0.15]
                        elif hidden_len == 3:
                            model_config['dropout_rates'] = [0.2, 0.15, 0.1]
                        else:
                            model_config['dropout_rates'] = [0.2 - i*0.05 for i in range(hidden_len)]
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nActivation Functions:")
                    activations = ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu']
                    for i, act in enumerate(activations, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if act == model_config.get('activation', 'leaky_relu') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {act}{current}")
                    
                    while True:
                        try:
                            act_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect activation (1-{len(activations)}, default=2): " + Style.RESET_ALL).strip()
                            if act_choice in [''] + [str(i) for i in range(1, len(activations)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(activations)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    activation = activations[int(act_choice)-1] if act_choice else 'leaky_relu'
                    model_config['activation'] = activation
                    
                    if activation == 'leaky_relu':
                        activation_param = input(Fore.YELLOW + Style.BRIGHT + "Negative slope for LeakyReLU (0.2): " + Style.RESET_ALL).strip()
                        model_config['activation_param'] = float(activation_param) if activation_param else 0.2
                    elif activation == 'elu':
                        activation_param = input(Fore.YELLOW + Style.BRIGHT + "Alpha for ELU (1.0): " + Style.RESET_ALL).strip()
                        model_config['activation_param'] = float(activation_param) if activation_param else 1.0
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nNormalization:")
                    norm_options = ['none', 'batch', 'layer', 'instance', 'group']
                    for i, norm in enumerate(norm_options, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if norm == model_config.get('normalization', 'batch') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {norm}{current}")
                    
                    while True:
                        try:
                            norm_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect normalization (1-{len(norm_options)}, default=2): " + Style.RESET_ALL).strip()
                            if norm_choice in [''] + [str(i) for i in range(1, len(norm_options)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(norm_options)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    normalization = norm_options[int(norm_choice)-1] if norm_choice else 'batch'
                    model_config['normalization'] = normalization
                    
                    if normalization == 'batch':
                        model_config['use_batch_norm'] = True
                        model_config['use_layer_norm'] = False
                    elif normalization == 'layer':
                        model_config['use_batch_norm'] = False
                        model_config['use_layer_norm'] = True
                    else:
                        model_config['use_batch_norm'] = False
                        model_config['use_layer_norm'] = False
                    
                    bias = input(Fore.YELLOW + Style.BRIGHT + "Use bias in layers? (Y/n): " + Style.RESET_ALL).strip().lower()
                    model_config['bias'] = bias in ('', 'y', 'yes')
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nWeight Initialization:")
                    init_methods = ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'orthogonal']
                    for i, init in enumerate(init_methods, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if init == model_config.get('weight_init', 'xavier_uniform') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {init}{current}")
                    
                    while True:
                        try:
                            init_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect initialization (1-{len(init_methods)}, default=1): " + Style.RESET_ALL).strip()
                            if init_choice in [''] + [str(i) for i in range(1, len(init_methods)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(init_methods)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    weight_init = init_methods[int(init_choice)-1] if init_choice else 'xavier_uniform'
                    model_config['weight_init'] = weight_init
                    
                    if model_type in ['EnhancedAutoencoder', 'AutoencoderEnsemble']:
                        print(Fore.YELLOW + Style.BRIGHT + "\nEnhanced Features:")
                        attention = input(Fore.YELLOW + Style.BRIGHT + "Use attention mechanism? (Y/n): " + Style.RESET_ALL).strip().lower()
                        model_config['use_attention'] = attention in ('', 'y', 'yes')
                        
                        residual = input(Fore.YELLOW + Style.BRIGHT + "Use residual blocks? (Y/n): " + Style.RESET_ALL).strip().lower()
                        model_config['residual_blocks'] = residual in ('', 'y', 'yes')
                        
                        skip_conn = input(Fore.YELLOW + Style.BRIGHT + "Use skip connections? (Y/n): " + Style.RESET_ALL).strip().lower()
                        model_config['skip_connection'] = skip_conn in ('', 'y', 'yes')
                        
                        if model_type == 'EnhancedAutoencoder':
                            legacy = input(Fore.YELLOW + Style.BRIGHT + "Use legacy mode? (y/N): " + Style.RESET_ALL).strip().lower()
                            model_config['legacy_mode'] = legacy in ('y', 'yes')
                    
                    if model_type == 'AutoencoderEnsemble':
                        print(Fore.YELLOW + Style.BRIGHT + "\nEnsemble Configuration:")
                        num_models = input(Fore.YELLOW + Style.BRIGHT + "Number of ensemble models (3): " + Style.RESET_ALL).strip()
                        model_config['num_models'] = int(num_models) if num_models else 3
                        
                        diversity = input(Fore.YELLOW + Style.BRIGHT + "Diversity factor (0.3): " + Style.RESET_ALL).strip()
                        model_config['diversity_factor'] = float(diversity) if diversity else 0.3
                    
                    min_features = input(Fore.YELLOW + Style.BRIGHT + "Minimum features validation (5): " + Style.RESET_ALL).strip()
                    model_config['min_features'] = int(min_features) if min_features else 5
                    
                    print(Fore.GREEN + Style.BRIGHT + f"\nModel configuration updated for {model_type}")
                
                elif section_choice == '2':
                    print(Fore.MAGENTA + Style.BRIGHT + "\nTRAINING PARAMETERS CONFIGURATION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    training_config = final_config.setdefault('training', {})
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Basic Training Parameters:")
                    epochs = input(Fore.YELLOW + Style.BRIGHT + "Number of epochs (50): " + Style.RESET_ALL).strip()
                    training_config['epochs'] = int(epochs) if epochs else 50
                    
                    batch_size = input(Fore.YELLOW + Style.BRIGHT + "Batch size (64): " + Style.RESET_ALL).strip()
                    training_config['batch_size'] = int(batch_size) if batch_size else 64
                    
                    lr = input(Fore.YELLOW + Style.BRIGHT + "Learning rate (0.001): " + Style.RESET_ALL).strip()
                    training_config['learning_rate'] = float(lr) if lr else 0.001
                    
                    patience = input(Fore.YELLOW + Style.BRIGHT + "Early stopping patience (15): " + Style.RESET_ALL).strip()
                    training_config['patience'] = int(patience) if patience else 15
                    
                    validation_split = input(Fore.YELLOW + Style.BRIGHT + "Validation split (0.2): " + Style.RESET_ALL).strip()
                    training_config['validation_split'] = float(validation_split) if validation_split else 0.2
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nRegularization:")
                    weight_decay = input(Fore.YELLOW + Style.BRIGHT + "Weight decay (1e-4): " + Style.RESET_ALL).strip()
                    training_config['weight_decay'] = float(weight_decay) if weight_decay else 1e-4
                    
                    gradient_clip = input(Fore.YELLOW + Style.BRIGHT + "Gradient clipping threshold (1.0): " + Style.RESET_ALL).strip()
                    training_config['gradient_clip'] = float(gradient_clip) if gradient_clip else 1.0
                    
                    grad_accum = input(Fore.YELLOW + Style.BRIGHT + "Gradient accumulation steps (1): " + Style.RESET_ALL).strip()
                    training_config['gradient_accumulation_steps'] = int(grad_accum) if grad_accum else 1
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nOptimizer Configuration:")
                    optimizers = ['AdamW', 'Adam', 'SGD', 'RMSprop', 'Adagrad']
                    for i, opt in enumerate(optimizers, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if opt == training_config.get('optimizer', 'AdamW') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {opt}{current}")
                    
                    while True:
                        try:
                            opt_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect optimizer (1-{len(optimizers)}, default=1): " + Style.RESET_ALL).strip()
                            if opt_choice in [''] + [str(i) for i in range(1, len(optimizers)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(optimizers)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    optimizer = optimizers[int(opt_choice)-1] if opt_choice else 'AdamW'
                    training_config['optimizer'] = optimizer
                    
                    if optimizer in ['AdamW', 'Adam']:
                        adam_betas_input = input(Fore.YELLOW + Style.BRIGHT + "Adam betas (0.9,0.999): " + Style.RESET_ALL).strip()
                        if adam_betas_input:
                            betas = [float(x.strip()) for x in adam_betas_input.split(',')]
                            training_config['adam_betas'] = tuple(betas)
                        else:
                            training_config['adam_betas'] = (0.9, 0.999)
                        
                        adam_eps = input(Fore.YELLOW + Style.BRIGHT + "Adam epsilon (1e-8): " + Style.RESET_ALL).strip()
                        training_config['adam_eps'] = float(adam_eps) if adam_eps else 1e-8
                    elif optimizer == 'SGD':
                        momentum = input(Fore.YELLOW + Style.BRIGHT + "SGD momentum (0.9): " + Style.RESET_ALL).strip()
                        training_config['momentum'] = float(momentum) if momentum else 0.9
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nLearning Rate Scheduler:")
                    scheduler_options = ['None', 'ReduceLROnPlateau', 'StepLR', 'CosineAnnealingLR', 'ExponentialLR', 'OneCycleLR']
                    for i, sched in enumerate(scheduler_options, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if sched == training_config.get('scheduler', 'ReduceLROnPlateau') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {sched}{current}")
                    
                    while True:
                        try:
                            sched_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect scheduler (1-{len(scheduler_options)}, default=2): " + Style.RESET_ALL).strip()
                            if sched_choice in [''] + [str(i) for i in range(1, len(scheduler_options)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(scheduler_options)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    if sched_choice == '1':
                        training_config['scheduler'] = None
                    else:
                        scheduler_type = scheduler_options[int(sched_choice)-1] if sched_choice else 'ReduceLROnPlateau'
                        training_config['scheduler'] = scheduler_type
                        
                        scheduler_params = {}
                        if scheduler_type == 'ReduceLROnPlateau':
                            lr_patience = input(Fore.YELLOW + Style.BRIGHT + "LR scheduler patience (5): " + Style.RESET_ALL).strip()
                            lr_factor = input(Fore.YELLOW + Style.BRIGHT + "LR reduction factor (0.5): " + Style.RESET_ALL).strip()
                            min_lr = input(Fore.YELLOW + Style.BRIGHT + "Minimum learning rate (1e-6): " + Style.RESET_ALL).strip()
                            
                            scheduler_params = {
                                'patience': int(lr_patience) if lr_patience else 5,
                                'factor': float(lr_factor) if lr_factor else 0.5,
                                'min_lr': float(min_lr) if min_lr else 1e-6
                            }
                        elif scheduler_type == 'StepLR':
                            step_size = input(Fore.YELLOW + Style.BRIGHT + "Step size (30): " + Style.RESET_ALL).strip()
                            gamma = input(Fore.YELLOW + Style.BRIGHT + "Gamma (0.1): " + Style.RESET_ALL).strip()
                            
                            scheduler_params = {
                                'step_size': int(step_size) if step_size else 30,
                                'gamma': float(gamma) if gamma else 0.1
                            }
                        elif scheduler_type == 'CosineAnnealingLR':
                            t_max = input(Fore.YELLOW + Style.BRIGHT + f"T_max ({training_config['epochs']}): " + Style.RESET_ALL).strip()
                            eta_min = input(Fore.YELLOW + Style.BRIGHT + "Eta min (1e-7): " + Style.RESET_ALL).strip()
                            
                            scheduler_params = {
                                'T_max': int(t_max) if t_max else training_config['epochs'],
                                'eta_min': float(eta_min) if eta_min else 1e-7
                            }
                        elif scheduler_type == 'ExponentialLR':
                            gamma = input(Fore.YELLOW + Style.BRIGHT + "Gamma (0.95): " + Style.RESET_ALL).strip()
                            scheduler_params = {'gamma': float(gamma) if gamma else 0.95}
                        elif scheduler_type == 'OneCycleLR':
                            max_lr = input(Fore.YELLOW + Style.BRIGHT + f"Max LR ({training_config['learning_rate'] * 10}): " + Style.RESET_ALL).strip()
                            pct_start = input(Fore.YELLOW + Style.BRIGHT + "Percent start (0.3): " + Style.RESET_ALL).strip()
                            
                            scheduler_params = {
                                'max_lr': float(max_lr) if max_lr else training_config['learning_rate'] * 10,
                                'total_steps': training_config['epochs'],
                                'pct_start': float(pct_start) if pct_start else 0.3
                            }
                        
                        training_config['scheduler_params'] = scheduler_params
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nTraining Options:")
                    early_stopping = input(Fore.YELLOW + Style.BRIGHT + "Enable early stopping? (Y/n): " + Style.RESET_ALL).strip().lower()
                    training_config['early_stopping'] = early_stopping in ('', 'y', 'yes')
                    
                    shuffle = input(Fore.YELLOW + Style.BRIGHT + "Shuffle training data? (Y/n): " + Style.RESET_ALL).strip().lower()
                    training_config['shuffle'] = shuffle in ('', 'y', 'yes')
                    
                    if torch.cuda.is_available():
                        mixed_prec = input(Fore.YELLOW + Style.BRIGHT + "Use mixed precision training? (Y/n): " + Style.RESET_ALL).strip().lower()
                        training_config['mixed_precision'] = mixed_prec in ('', 'y', 'yes')
                    else:
                        training_config['mixed_precision'] = False
                    
                    print(Fore.GREEN + Style.BRIGHT + "\nTraining configuration updated")
                
                elif section_choice == '3':
                    print(Fore.MAGENTA + Style.BRIGHT + "\nDATA CONFIGURATION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    data_config = final_config.setdefault('data', {})
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Data Source:")
                    use_real_data = input(Fore.YELLOW + Style.BRIGHT + "Use real network data? (y/N): " + Style.RESET_ALL).strip().lower()
                    data_config['use_real_data'] = use_real_data in ('y', 'yes')
                    
                    if data_config['use_real_data']:
                        print(Fore.YELLOW + Style.BRIGHT + "\nReal Data Configuration:")
                        data_path = input(Fore.YELLOW + Style.BRIGHT + "Data file path (optional): " + Style.RESET_ALL).strip()
                        if data_path:
                            data_config['data_path'] = data_path
                        
                        artifacts_path = input(Fore.YELLOW + Style.BRIGHT + "Artifacts path (optional): " + Style.RESET_ALL).strip()
                        if artifacts_path:
                            data_config['artifacts_path'] = artifacts_path
                    else:
                        print(Fore.YELLOW + Style.BRIGHT + "\nSynthetic Data Configuration:")
                        normal_samples = input(Fore.YELLOW + Style.BRIGHT + "Normal samples (8000): " + Style.RESET_ALL).strip()
                        data_config['normal_samples'] = int(normal_samples) if normal_samples else 8000
                        
                        attack_samples = input(Fore.YELLOW + Style.BRIGHT + "Attack samples (2000): " + Style.RESET_ALL).strip()
                        data_config['attack_samples'] = int(attack_samples) if attack_samples else 2000
                        
                        features = input(Fore.YELLOW + Style.BRIGHT + "Number of features (20): " + Style.RESET_ALL).strip()
                        data_config['features'] = int(features) if features else 20
                        
                        anomaly_factor = input(Fore.YELLOW + Style.BRIGHT + "Anomaly factor (0.1): " + Style.RESET_ALL).strip()
                        data_config['anomaly_factor'] = float(anomaly_factor) if anomaly_factor else 0.1
                    
                    random_state = input(Fore.YELLOW + Style.BRIGHT + "Random state (42): " + Style.RESET_ALL).strip()
                    data_config['random_state'] = int(random_state) if random_state else 42
                    
                    test_split = input(Fore.YELLOW + Style.BRIGHT + "Test split ratio (0.2): " + Style.RESET_ALL).strip()
                    data_config['test_split'] = float(test_split) if test_split else 0.2
                    
                    stratified = input(Fore.YELLOW + Style.BRIGHT + "Use stratified split? (Y/n): " + Style.RESET_ALL).strip().lower()
                    data_config['stratified_split'] = stratified in ('', 'y', 'yes')
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nData Preprocessing:")
                    norm_options = ['standard', 'minmax', 'robust', 'none']
                    for i, norm in enumerate(norm_options, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if norm == data_config.get('data_normalization', 'standard') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {norm}{current}")
                    
                    while True:
                        try:
                            norm_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect normalization (1-{len(norm_options)}, default=1): " + Style.RESET_ALL).strip()
                            if norm_choice in [''] + [str(i) for i in range(1, len(norm_options)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(norm_options)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    normalization = norm_options[int(norm_choice)-1] if norm_choice else 'standard'
                    data_config['data_normalization'] = normalization
                    
                    preprocessing = input(Fore.YELLOW + Style.BRIGHT + "Enable data preprocessing? (Y/n): " + Style.RESET_ALL).strip().lower()
                    data_config['data_preprocessing'] = preprocessing in ('', 'y', 'yes')
                    
                    print(Fore.GREEN + Style.BRIGHT + "\nData configuration updated")
                
                elif section_choice == '4':
                    print(Fore.MAGENTA + Style.BRIGHT + "\nSYSTEM & HARDWARE CONFIGURATION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    hardware_config = final_config.setdefault('hardware', {})
                    system_config = final_config.setdefault('system', {})
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Device Configuration:")
                    device_options = ['auto', 'cpu', 'cuda']
                    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        device_options.append('mps')
                    
                    for i, dev in enumerate(device_options, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if dev == hardware_config.get('device', system_config.get('device', 'auto')) else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {dev}{current}")
                    
                    while True:
                        try:
                            dev_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect device (1-{len(device_options)}, default=1): " + Style.RESET_ALL).strip()
                            if dev_choice in [''] + [str(i) for i in range(1, len(device_options)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(device_options)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    device = device_options[int(dev_choice)-1] if dev_choice else 'auto'
                    hardware_config['device'] = device
                    
                    if torch.cuda.is_available():
                        cuda_opt = input(Fore.YELLOW + Style.BRIGHT + "Enable CUDA optimizations? (Y/n): " + Style.RESET_ALL).strip().lower()
                        hardware_config['cuda_optimizations'] = cuda_opt in ('', 'y', 'yes')
                    
                    memory_mgmt = input(Fore.YELLOW + Style.BRIGHT + "Enable memory management? (Y/n): " + Style.RESET_ALL).strip().lower()
                    hardware_config['memory_management'] = {'enable_memory_efficient': memory_mgmt in ('', 'y', 'yes')}
                    
                    gpu_memory = input(Fore.YELLOW + Style.BRIGHT + "Recommended GPU memory GB (4.0): " + Style.RESET_ALL).strip()
                    hardware_config['recommended_gpu_memory'] = float(gpu_memory) if gpu_memory else 4.0
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nPerformance Settings:")
                    num_workers = input(Fore.YELLOW + Style.BRIGHT + "Data loader workers (4): " + Style.RESET_ALL).strip()
                    system_config['num_workers'] = int(num_workers) if num_workers else 4
                    
                    pin_memory = input(Fore.YELLOW + Style.BRIGHT + "Pin memory for GPU? (Y/n): " + Style.RESET_ALL).strip().lower()
                    system_config['pin_memory'] = pin_memory in ('', 'y', 'yes') and torch.cuda.is_available()
                    
                    persistent_workers = input(Fore.YELLOW + Style.BRIGHT + "Use persistent workers? (Y/n): " + Style.RESET_ALL).strip().lower()
                    system_config['persistent_workers'] = persistent_workers in ('', 'y', 'yes')
                    
                    compile_model = input(Fore.YELLOW + Style.BRIGHT + "Compile model with torch.compile? (y/N): " + Style.RESET_ALL).strip().lower()
                    system_config['compile_model'] = compile_model in ('y', 'yes')
                    
                    benchmark_mode = input(Fore.YELLOW + Style.BRIGHT + "Enable benchmark mode? (y/N): " + Style.RESET_ALL).strip().lower()
                    system_config['benchmark_mode'] = benchmark_mode in ('y', 'yes')
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nReproducibility:")
                    reproducible = input(Fore.YELLOW + Style.BRIGHT + "Enable reproducible training? (Y/n): " + Style.RESET_ALL).strip().lower()
                    system_config['reproducible'] = reproducible in ('', 'y', 'yes')
                    
                    if system_config['reproducible']:
                        random_seed = input(Fore.YELLOW + Style.BRIGHT + "Random seed (42): " + Style.RESET_ALL).strip()
                        system_config['random_seed'] = int(random_seed) if random_seed else 42
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nDirectories:")
                    model_dir = input(Fore.YELLOW + Style.BRIGHT + f"Model directory ({DEFAULT_MODEL_DIR}): " + Style.RESET_ALL).strip()
                    system_config['model_dir'] = model_dir if model_dir else DEFAULT_MODEL_DIR
                    
                    log_dir = input(Fore.YELLOW + Style.BRIGHT + f"Log directory ({LOG_DIR}): " + Style.RESET_ALL).strip()
                    system_config['log_dir'] = log_dir if log_dir else LOG_DIR
                    
                    tb_dir = input(Fore.YELLOW + Style.BRIGHT + f"TensorBoard directory ({TB_DIR}): " + Style.RESET_ALL).strip()
                    system_config['tensorboard_dir'] = tb_dir if tb_dir else TB_DIR
                    
                    print(Fore.GREEN + Style.BRIGHT + "\nSystem & hardware configuration updated")
                
                elif section_choice == '5':
                    print(Fore.MAGENTA + Style.BRIGHT + "\nSECURITY & ANOMALY DETECTION CONFIGURATION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    security_config = final_config.setdefault('security', {})
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Anomaly Detection:")
                    percentile = input(Fore.YELLOW + Style.BRIGHT + "Anomaly threshold percentile (95.0): " + Style.RESET_ALL).strip()
                    security_config['percentile'] = float(percentile) if percentile else 95.0
                    
                    threshold_methods = ['percentile', 'adaptive', 'statistical', 'robust']
                    for i, method in enumerate(threshold_methods, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if method == security_config.get('anomaly_threshold_strategy', 'percentile') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {method}{current}")
                    
                    while True:
                        try:
                            method_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect threshold method (1-{len(threshold_methods)}, default=1): " + Style.RESET_ALL).strip()
                            if method_choice in [''] + [str(i) for i in range(1, len(threshold_methods)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(threshold_methods)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    threshold_method = threshold_methods[int(method_choice)-1] if method_choice else 'percentile'
                    security_config['anomaly_threshold_strategy'] = threshold_method
                    
                    enable_security = input(Fore.YELLOW + Style.BRIGHT + "Enable security metrics? (Y/n): " + Style.RESET_ALL).strip().lower()
                    security_config['enable_security_metrics'] = enable_security in ('', 'y', 'yes')
                    
                    adaptive_thresh = input(Fore.YELLOW + Style.BRIGHT + "Use adaptive thresholding? (Y/n): " + Style.RESET_ALL).strip().lower()
                    security_config['adaptive_threshold'] = adaptive_thresh in ('', 'y', 'yes')
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nAdvanced Security Options:")
                    attack_thresh = input(Fore.YELLOW + Style.BRIGHT + "Attack threshold (optional): " + Style.RESET_ALL).strip()
                    if attack_thresh:
                        security_config['attack_threshold'] = float(attack_thresh)
                    
                    false_neg_cost = input(Fore.YELLOW + Style.BRIGHT + "False negative cost (optional): " + Style.RESET_ALL).strip()
                    if false_neg_cost:
                        security_config['false_negative_cost'] = float(false_neg_cost)
                    
                    early_warning = input(Fore.YELLOW + Style.BRIGHT + "Early warning threshold (optional): " + Style.RESET_ALL).strip()
                    if early_warning:
                        security_config['early_warning_threshold'] = float(early_warning)
                    
                    confidence_interval = input(Fore.YELLOW + Style.BRIGHT + "Confidence interval (optional): " + Style.RESET_ALL).strip()
                    if confidence_interval:
                        security_config['confidence_interval'] = float(confidence_interval)
                    
                    threshold_validation = input(Fore.YELLOW + Style.BRIGHT + "Enable threshold validation? (Y/n): " + Style.RESET_ALL).strip().lower()
                    security_config['threshold_validation'] = threshold_validation in ('', 'y', 'yes')
                    
                    robust_detection = input(Fore.YELLOW + Style.BRIGHT + "Enable robust detection? (Y/n): " + Style.RESET_ALL).strip().lower()
                    security_config['robust_detection'] = robust_detection in ('', 'y', 'yes')
                    
                    fp_tolerance = input(Fore.YELLOW + Style.BRIGHT + "False positive tolerance (optional): " + Style.RESET_ALL).strip()
                    if fp_tolerance:
                        security_config['false_positive_tolerance'] = float(fp_tolerance)
                    
                    perf_optimized = input(Fore.YELLOW + Style.BRIGHT + "Performance optimized detection? (Y/n): " + Style.RESET_ALL).strip().lower()
                    security_config['performance_optimized_detection'] = perf_optimized in ('', 'y', 'yes')
                    
                    real_time = input(Fore.YELLOW + Style.BRIGHT + "Real-time monitoring? (y/N): " + Style.RESET_ALL).strip().lower()
                    security_config['real_time_monitoring'] = real_time in ('y', 'yes')
                    
                    model_type = final_config.get('model', {}).get('model_type', 'EnhancedAutoencoder')
                    if model_type == 'AutoencoderEnsemble':
                        print(Fore.YELLOW + Style.BRIGHT + "\nEnsemble Security:")
                        voting_methods = ['average', 'majority', 'weighted', 'max', 'min']
                        for i, voting in enumerate(voting_methods, 1):
                            current = Fore.GREEN + Style.BRIGHT + " (current)" if voting == security_config.get('ensemble_voting', 'average') else ""
                            print(Fore.WHITE + Style.BRIGHT + f"{i}. {voting}{current}")
                        
                        while True:
                            try:
                                voting_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect ensemble voting (1-{len(voting_methods)}, default=1): " + Style.RESET_ALL).strip()
                                if voting_choice in [''] + [str(i) for i in range(1, len(voting_methods)+1)]:
                                    break
                                print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(voting_methods)}")
                            except (EOFError, KeyboardInterrupt):
                                print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                                continue
                        
                        voting = voting_methods[int(voting_choice)-1] if voting_choice else 'average'
                        security_config['ensemble_voting'] = voting
                        
                        uncertainty = input(Fore.YELLOW + Style.BRIGHT + "Uncertainty threshold (optional): " + Style.RESET_ALL).strip()
                        if uncertainty:
                            security_config['uncertainty_threshold'] = float(uncertainty)
                    
                    print(Fore.GREEN + Style.BRIGHT + "\nSecurity configuration updated")
                
                elif section_choice == '6':
                    print(Fore.MAGENTA + Style.BRIGHT + "\nMONITORING & LOGGING CONFIGURATION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    monitoring_config = final_config.setdefault('monitoring', {})
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Logging Options:")
                    verbose = input(Fore.YELLOW + Style.BRIGHT + "Verbose output? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['verbose'] = verbose in ('', 'y', 'yes')
                    
                    debug = input(Fore.YELLOW + Style.BRIGHT + "Enable debug mode? (y/N): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['debug_mode'] = debug in ('y', 'yes')
                    
                    console_levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR']
                    for i, level in enumerate(console_levels, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if level == monitoring_config.get('console_logging_level', 'INFO') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {level}{current}")
                    
                    while True:
                        try:
                            level_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect console logging level (1-{len(console_levels)}, default=2): " + Style.RESET_ALL).strip()
                            if level_choice in [''] + [str(i) for i in range(1, len(console_levels)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(console_levels)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    console_level = console_levels[int(level_choice)-1] if level_choice else 'INFO'
                    monitoring_config['console_logging_level'] = console_level
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nProgress Tracking:")
                    tensorboard = input(Fore.YELLOW + Style.BRIGHT + "Enable TensorBoard logging? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['tensorboard_logging'] = tensorboard in ('', 'y', 'yes')
                    
                    progress_bar = input(Fore.YELLOW + Style.BRIGHT + "Show progress bars? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['progress_bar'] = progress_bar in ('', 'y', 'yes')
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nCheckpoints and Saving:")
                    checkpoints = input(Fore.YELLOW + Style.BRIGHT + "Save training checkpoints? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['save_checkpoints'] = checkpoints in ('', 'y', 'yes')
                    
                    if monitoring_config['save_checkpoints']:
                        checkpoint_freq = input(Fore.YELLOW + Style.BRIGHT + "Checkpoint frequency in epochs (10): " + Style.RESET_ALL).strip()
                        monitoring_config['checkpoint_frequency'] = int(checkpoint_freq) if checkpoint_freq else 10
                    
                    save_best = input(Fore.YELLOW + Style.BRIGHT + "Save best model? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['save_best_model'] = save_best in ('', 'y', 'yes')
                    
                    save_history = input(Fore.YELLOW + Style.BRIGHT + "Save model history? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['save_model_history'] = save_history in ('', 'y', 'yes')
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nMetrics and Frequencies:")
                    log_freq = input(Fore.YELLOW + Style.BRIGHT + "Log frequency (1): " + Style.RESET_ALL).strip()
                    monitoring_config['log_frequency'] = int(log_freq) if log_freq else 1
                    
                    metrics_freq = input(Fore.YELLOW + Style.BRIGHT + "Metrics frequency (1): " + Style.RESET_ALL).strip()
                    monitoring_config['metrics_frequency'] = int(metrics_freq) if metrics_freq else 1
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nMetrics to track (space-separated):")
                    print(Fore.GREEN + Style.BRIGHT + "Available: loss, reconstruction_error, anomaly_detection_rate, mse_statistics, memory_usage")
                    metrics_input = input(Fore.YELLOW + Style.BRIGHT + "Metrics (default: loss reconstruction_error): " + Style.RESET_ALL).strip()
                    if metrics_input:
                        monitoring_config['metrics_to_track'] = metrics_input.split()
                    else:
                        monitoring_config['metrics_to_track'] = ['loss', 'reconstruction_error']
                    
                    early_metric = input(Fore.YELLOW + Style.BRIGHT + "Early stopping metric (val_loss): " + Style.RESET_ALL).strip()
                    monitoring_config['early_stopping_metric'] = early_metric if early_metric else 'val_loss'
                    
                    log_summary = input(Fore.YELLOW + Style.BRIGHT + "Log model summary? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['log_model_summary'] = log_summary in ('', 'y', 'yes')
                    
                    stability_metrics = input(Fore.YELLOW + Style.BRIGHT + "Track stability metrics? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['stability_metrics'] = stability_metrics in ('', 'y', 'yes')
                    
                    performance_metrics = input(Fore.YELLOW + Style.BRIGHT + "Track performance metrics? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['performance_metrics'] = performance_metrics in ('', 'y', 'yes')
                    
                    profiling = input(Fore.YELLOW + Style.BRIGHT + "Enable profiling? (y/N): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['profiling_enabled'] = profiling in ('y', 'yes')
                    
                    print(Fore.GREEN + Style.BRIGHT + "\nMonitoring configuration updated")
                
                elif section_choice == '7':
                    print(Fore.MAGENTA + Style.BRIGHT + "\nADVANCED OPTIONS CONFIGURATION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    validation_config = final_config.setdefault('validation', {})
                    experimental_config = final_config.setdefault('experimental', {})
                    export_config = final_config.setdefault('export', {})
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Validation and Testing:")
                    detailed_metrics = input(Fore.YELLOW + Style.BRIGHT + "Calculate detailed metrics? (Y/n): " + Style.RESET_ALL).strip().lower()
                    validation_config['detailed_metrics'] = detailed_metrics in ('', 'y', 'yes')
                    
                    cross_validation = input(Fore.YELLOW + Style.BRIGHT + "Enable cross-validation? (y/N): " + Style.RESET_ALL).strip().lower()
                    if cross_validation in ('y', 'yes'):
                        cv_folds = input(Fore.YELLOW + Style.BRIGHT + "Number of CV folds (5): " + Style.RESET_ALL).strip()
                        validation_config['cross_validation'] = {
                            'enabled': True,
                            'folds': int(cv_folds) if cv_folds else 5
                        }
                    else:
                        validation_config['cross_validation'] = {'enabled': False}
                    
                    val_freq = input(Fore.YELLOW + Style.BRIGHT + "Validation frequency (1): " + Style.RESET_ALL).strip()
                    validation_config['validation_frequency'] = int(val_freq) if val_freq else 1
                    
                    save_val_results = input(Fore.YELLOW + Style.BRIGHT + "Save validation results? (Y/n): " + Style.RESET_ALL).strip().lower()
                    validation_config['save_validation_results'] = save_val_results in ('', 'y', 'yes')
                    
                    robustness = input(Fore.YELLOW + Style.BRIGHT + "Enable robustness testing? (y/N): " + Style.RESET_ALL).strip().lower()
                    validation_config['robustness_testing'] = robustness in ('y', 'yes')
                    
                    benchmarking = input(Fore.YELLOW + Style.BRIGHT + "Enable performance benchmarking? (y/N): " + Style.RESET_ALL).strip().lower()
                    validation_config['performance_benchmarking'] = benchmarking in ('y', 'yes')
                    
                    confidence_intervals = input(Fore.YELLOW + Style.BRIGHT + "Calculate confidence intervals? (y/N): " + Style.RESET_ALL).strip().lower()
                    validation_config['confidence_intervals'] = confidence_intervals in ('y', 'yes')
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nExport Options:")
                    save_model = input(Fore.YELLOW + Style.BRIGHT + "Save trained model? (Y/n): " + Style.RESET_ALL).strip().lower()
                    export_config['save_model'] = save_model in ('', 'y', 'yes')
                    
                    save_metadata = input(Fore.YELLOW + Style.BRIGHT + "Save training metadata? (Y/n): " + Style.RESET_ALL).strip().lower()
                    export_config['save_metadata'] = save_metadata in ('', 'y', 'yes')
                    
                    save_training_history = input(Fore.YELLOW + Style.BRIGHT + "Save training history? (Y/n): " + Style.RESET_ALL).strip().lower()
                    export_config['save_training_history'] = save_training_history in ('', 'y', 'yes')
                    
                    export_onnx = input(Fore.YELLOW + Style.BRIGHT + "Export to ONNX format? (y/N): " + Style.RESET_ALL).strip().lower()
                    export_config['export_onnx'] = export_onnx in ('y', 'yes')
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nExperimental Features:")
                    experimental_features = input(Fore.YELLOW + Style.BRIGHT + "Enable experimental features? (y/N): " + Style.RESET_ALL).strip().lower()
                    if experimental_features in ('y', 'yes'):
                        print(Fore.YELLOW + Style.BRIGHT + "\nAvailable experimental features:")
                        print(Fore.WHITE + Style.BRIGHT + "1. Advanced attention mechanisms")
                        print(Fore.WHITE + Style.BRIGHT + "2. Dynamic architecture adjustment")
                        print(Fore.WHITE + Style.BRIGHT + "3. Adaptive learning rates")
                        print(Fore.WHITE + Style.BRIGHT + "4. Novel regularization techniques")
                        print(Fore.WHITE + Style.BRIGHT + "5. Memory optimization")
                        
                        exp_features_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect features (space-separated numbers, optional): " + Style.RESET_ALL).strip()
                        if exp_features_input:
                            feature_map = {
                                '1': 'advanced_attention',
                                '2': 'dynamic_architecture',
                                '3': 'adaptive_lr',
                                '4': 'novel_regularization',
                                '5': 'memory_optimization'
                            }
                            selected_features = {}
                            for num in exp_features_input.split():
                                if num in feature_map:
                                    selected_features[feature_map[num]] = True
                            experimental_config['experimental_features'] = selected_features
                    
                    auto_optimize = input(Fore.YELLOW + Style.BRIGHT + "Enable automatic optimization? (y/N): " + Style.RESET_ALL).strip().lower()
                    experimental_config['auto_optimize'] = auto_optimize in ('y', 'yes')
                    
                    print(Fore.GREEN + Style.BRIGHT + "\nAdvanced options configuration updated")
                
                elif section_choice == '8':
                    print(Fore.MAGENTA + Style.BRIGHT + "\nCOMPLETE CONFIGURATION REVIEW")
                    print(Fore.CYAN + Style.BRIGHT + "=" * 40)
                    
                    model_config = final_config.get('model', {})
                    training_config = final_config.get('training', {})
                    data_config = final_config.get('data', {})
                    security_config = final_config.get('security', {})
                    system_config = final_config.get('system', {})
                    monitoring_config = final_config.get('monitoring', {})
                    hardware_config = final_config.get('hardware', {})
                    validation_config = final_config.get('validation', {})
                    experimental_config = final_config.get('experimental', {})
                    export_config = final_config.get('export', {})
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Model Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Type: " + Fore.GREEN + f"{model_config.get('model_type', 'EnhancedAutoencoder')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Encoding Dim: " + Fore.GREEN + f"{model_config.get('encoding_dim', 32)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Hidden Dims: " + Fore.GREEN + f"{model_config.get('hidden_dims', [256, 128, 64])}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Dropout Rates: " + Fore.GREEN + f"{model_config.get('dropout_rates', [0.2, 0.15, 0.1])}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Activation: " + Fore.GREEN + f"{model_config.get('activation', 'leaky_relu')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Normalization: " + Fore.GREEN + f"{model_config.get('normalization', 'batch')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Weight Init: " + Fore.GREEN + f"{model_config.get('weight_init', 'xavier_uniform')}")
                    
                    if model_config.get('model_type') == 'AutoencoderEnsemble':
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Ensemble Size: " + Fore.GREEN + f"{model_config.get('num_models', 3)}")
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Diversity Factor: " + Fore.GREEN + f"{model_config.get('diversity_factor', 0.3)}")
                    elif model_config.get('model_type') == 'EnhancedAutoencoder':
                        features = []
                        if model_config.get('use_attention', True):
                            features.append("Attention")
                        if model_config.get('residual_blocks', True):
                            features.append("Residual Blocks")
                        if model_config.get('skip_connection', True):
                            features.append("Skip Connections")
                        if features:
                            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Enhanced Features: " + Fore.GREEN + f"{', '.join(features)}")
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Legacy Mode: " + Fore.GREEN + f"{model_config.get('legacy_mode', False)}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nTraining Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.GREEN + f"{training_config.get('epochs', 50)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + f"{training_config.get('batch_size', 64)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + f"{training_config.get('learning_rate', 0.001)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Weight Decay: " + Fore.GREEN + f"{training_config.get('weight_decay', 1e-4)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Patience: " + Fore.GREEN + f"{training_config.get('patience', 15)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Optimizer: " + Fore.GREEN + f"{training_config.get('optimizer', 'AdamW')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Scheduler: " + Fore.GREEN + f"{training_config.get('scheduler', 'ReduceLROnPlateau')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Mixed Precision: " + Fore.GREEN + f"{training_config.get('mixed_precision', torch.cuda.is_available())}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Early Stopping: " + Fore.GREEN + f"{training_config.get('early_stopping', True)}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nData Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Use Real Data: " + Fore.GREEN + f"{data_config.get('use_real_data', False)}")
                    if not data_config.get('use_real_data', False):
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.GREEN + f"{data_config.get('normal_samples', 8000)}")
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.GREEN + f"{data_config.get('attack_samples', 2000)}")
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Features: " + Fore.GREEN + f"{data_config.get('features', 20)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Validation Split: " + Fore.GREEN + f"{training_config.get('validation_split', 0.2)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Test Split: " + Fore.GREEN + f"{data_config.get('test_split', 0.2)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Data Normalization: " + Fore.GREEN + f"{data_config.get('data_normalization', 'standard')}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nSecurity Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Anomaly Threshold: " + Fore.GREEN + f"{security_config.get('percentile', 95.0)}th percentile")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Threshold Method: " + Fore.GREEN + f"{security_config.get('anomaly_threshold_strategy', 'percentile')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Adaptive Threshold: " + Fore.GREEN + f"{security_config.get('adaptive_threshold', True)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Security Metrics: " + Fore.GREEN + f"{security_config.get('enable_security_metrics', True)}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nSystem Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Device: " + Fore.GREEN + f"{hardware_config.get('device', system_config.get('device', 'auto'))}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Reproducible: " + Fore.GREEN + f"{system_config.get('reproducible', True)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Workers: " + Fore.GREEN + f"{system_config.get('num_workers', 4)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Model Directory: " + Fore.GREEN + f"{system_config.get('model_dir', DEFAULT_MODEL_DIR)}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nMonitoring Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ TensorBoard: " + Fore.GREEN + f"{monitoring_config.get('tensorboard_logging', True)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Checkpoints: " + Fore.GREEN + f"{monitoring_config.get('save_checkpoints', True)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Metrics: " + Fore.GREEN + f"{monitoring_config.get('metrics_to_track', ['loss', 'reconstruction_error'])}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Verbose: " + Fore.GREEN + f"{monitoring_config.get('verbose', True)}")
                    
                    estimated_time = _estimate_training_time(
                        training_config.get('epochs', 50),
                        model_config.get('model_type', 'EnhancedAutoencoder'),
                        training_config.get('batch_size', 64)
                    )
                    print(Fore.YELLOW + Style.BRIGHT + "\nPerformance Estimate:")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Training Time: " + Fore.GREEN + f"~{estimated_time} minutes")
                    
                    
                    modify_more = input(Fore.YELLOW + Style.BRIGHT + "Continue modifying configuration? (y/N): " + Style.RESET_ALL).strip().lower()
                    if modify_more not in ('y', 'yes'):
                        continue
                
                elif section_choice == '9':
                    print(Fore.MAGENTA + Style.BRIGHT + "\nFINAL CONFIGURATION SUMMARY")
                    print(Fore.CYAN + Style.BRIGHT + "=" * 40)
                    
                    model_config = final_config.get('model', {})
                    training_config = final_config.get('training', {})
                    data_config = final_config.get('data', {})
                    hardware_config = final_config.get('hardware', {})
                    system_config = final_config.get('system', {})
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Final Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model: " + Fore.GREEN + f"{model_config.get('model_type', 'EnhancedAutoencoder')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training: " + Fore.GREEN + f"{training_config.get('epochs', 50)} epochs")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + f"{'Real Data' if data_config.get('use_real_data', False) else 'Synthetic Data'}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Device: " + Fore.GREEN + f"{final_config.get('hardware', {}).get('device', final_config.get('system', {}).get('device', 'auto'))}")
                    
                    estimated_time = _estimate_training_time(
                        training_config.get('epochs', 50),
                        model_config.get('model_type', 'EnhancedAutoencoder'),
                        training_config.get('batch_size', 64)
                    )
                    print(Fore.YELLOW + Style.BRIGHT + "\nPerformance:")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + f"~{estimated_time} minutes")
                    
                    confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart training with this configuration? (Y/n): " + Style.RESET_ALL).strip().lower()
                    if confirm in ('', 'y', 'yes'):
                        print(Fore.GREEN + Style.BRIGHT + "\nLaunching custom training...")
                        return _launch_training_with_config(final_config)
                    else:
                        save_config = input(Fore.YELLOW + Style.BRIGHT + "\nSave this configuration for later? (y/N): " + Style.RESET_ALL).strip().lower()
                        if save_config in ('y', 'yes'):
                            config_name = input(Fore.YELLOW + Style.BRIGHT + "\nConfiguration name: " + Style.RESET_ALL).strip()
                            if config_name:
                                try:
                                    config_path = Path(final_config.get('system', {}).get('config_dir', CONFIG_DIR)) / f"custom_config_{config_name}.json"
                                    config_path.parent.mkdir(parents=True, exist_ok=True)
                                    with open(config_path, 'w') as f:
                                        import json
                                        json.dump(final_config, f, indent=2, default=str)
                                    print(Fore.GREEN + Style.BRIGHT + f"\nConfiguration saved to: {config_path}")
                                except Exception as e:
                                    print(Fore.RED + Style.BRIGHT + f"\nFailed to save configuration: {e}")
                        
                        print(Fore.RED + Style.BRIGHT + "\nTraining cancelled")
                        return None
                
                else:
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-9.")
            
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                continue
    
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\n\nCustom training configuration interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"Custom training configuration failed: {e}", exc_info=True)
        message = (
            f"Error encountered during custom training configuration: {str(e)}\n"
            f"Context:\n"
            f"- Current Preset: {preset_name}\n"
            f"- Model Type: {model_type}\n"
            f"- Config Source: {config_source}\n\n"
            f"This could be due to:\n"
            f"- Configuration file corruption\n"
            f"- Invalid parameter combinations\n"
            f"- System resource issues\n"
            f"- Interactive input handling problems"
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="CUSTOM TRAINING CONFIGURATION ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        return None

def run_stability_test(
    # Test Configuration Parameters
    test_epochs: Optional[int] = None,
    test_batch_size: Optional[int] = None,
    test_learning_rate: Optional[float] = None,
    test_model_type: Optional[str] = None,
    test_encoding_dim: Optional[int] = None,
    test_normal_samples: Optional[int] = None,
    test_attack_samples: Optional[int] = None,
    test_features: Optional[int] = None,
    
    # Test Mode Parameters
    quick_test: Optional[bool] = None,
    comprehensive_test: Optional[bool] = None,
    stress_test: Optional[bool] = None,
    minimal_test: Optional[bool] = None,
    
    # System Parameters
    test_device: Optional[str] = None,
    test_mixed_precision: Optional[bool] = None,
    test_num_workers: Optional[int] = None,
    test_memory_efficient: Optional[bool] = None,
    
    # Output Parameters
    verbose: Optional[bool] = None,
    save_test_results: Optional[bool] = None,
    test_results_dir: Optional[Union[str, Path]] = None,
    interactive: Optional[bool] = None,
    
    # Error Handling Parameters
    continue_on_error: Optional[bool] = None,
    graceful_degradation: Optional[bool] = None,
    
    # Advanced Test Parameters
    test_all_models: Optional[bool] = None,
    test_data_sources: Optional[bool] = None,
    test_hardware_configs: Optional[bool] = None,
    performance_benchmarking: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    test_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """Full system stability test with context display and error handling."""
    try:
        # Clear screen and show banner with config
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        if config is None and banner_config is not None:
            config = banner_config
        else:
            config = get_current_config()
        
        # Extract configuration context with error handling
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu header with context
        print(Fore.MAGENTA + Style.BRIGHT + "SYSTEM STABILITY TEST")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Active Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")

        # Start timing
        start_time = datetime.now()
        test_start_time = time.time()
        
        # Initialize configuration
        if config is None:
            try:
                config = get_current_config() if 'get_current_config' in globals() else {}
            except Exception as e:
                logger.warning(f"Failed to load current config: {e}")
                console.print(
                    Panel.fit(
                        f"Failed to load current config, using defaults: {str(e)}",
                        title="WARNING",
                        style="bold red",
                        border_style="red",
                        padding=(1, 1),
                        box=box.ROUNDED
                    )
                )
                config = {}
        
        # Apply test-specific configuration
        if test_config:
            config.setdefault('stability_test', {}).update(test_config)
        
        # Apply all parameters to configuration
        final_config = config.copy()
        
        # Apply individual parameters
        params = locals().copy()
        params.update(kwargs)
        
        # Remove non-parameter items
        params_to_remove = {
            'config', 'test_config', 'kwargs', 'start_time', 'test_start_time', 'datetime', 'traceback', 'time', 'gc', 'warnings', 'Path'
        }
        
        cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
        
        # Set up defaults
        stability_config = final_config.setdefault('stability_test', {})
        
        # Test configuration defaults
        test_epochs = stability_config.setdefault('test_epochs', cleaned_params.get('test_epochs', 10))
        test_batch_size = stability_config.setdefault('test_batch_size', cleaned_params.get('test_batch_size', 32))
        test_learning_rate = stability_config.setdefault('test_learning_rate', cleaned_params.get('test_learning_rate', 0.01))
        test_model_type = stability_config.setdefault('test_model_type', cleaned_params.get('test_model_type', 'SimpleAutoencoder'))
        test_encoding_dim = stability_config.setdefault('test_encoding_dim', cleaned_params.get('test_encoding_dim', 8))
        test_normal_samples = stability_config.setdefault('test_normal_samples', cleaned_params.get('test_normal_samples', 8000))
        test_attack_samples = stability_config.setdefault('test_attack_samples', cleaned_params.get('test_attack_samples', 2000))
        test_features = stability_config.setdefault('test_features', cleaned_params.get('test_features', 20))
        
        # Test mode defaults
        quick_test = stability_config.setdefault('quick_test', cleaned_params.get('quick_test', True))
        comprehensive_test = stability_config.setdefault('comprehensive_test', cleaned_params.get('comprehensive_test', False))
        stress_test = stability_config.setdefault('stress_test', cleaned_params.get('stress_test', False))
        minimal_test = stability_config.setdefault('minimal_test', cleaned_params.get('minimal_test', False))
        
        # System defaults
        test_device = stability_config.setdefault('test_device', cleaned_params.get('test_device', 'auto'))
        test_mixed_precision = stability_config.setdefault('test_mixed_precision', cleaned_params.get('test_mixed_precision', False))
        test_num_workers = stability_config.setdefault('test_num_workers', cleaned_params.get('test_num_workers', 0))
        test_memory_efficient = stability_config.setdefault('test_memory_efficient', cleaned_params.get('test_memory_efficient', True))
        
        # Output defaults
        verbose = stability_config.setdefault('verbose', cleaned_params.get('verbose', True))
        save_test_results = stability_config.setdefault('save_test_results', cleaned_params.get('save_test_results', True))
        test_results_dir = stability_config.setdefault('test_results_dir', cleaned_params.get('results_dir', RESULTS_DIR / "stability_tests"))
        interactive = stability_config.setdefault('interactive', cleaned_params.get('interactive', not cleaned_params.get('non_interactive', False)))
        
        # Error handling defaults
        continue_on_error = stability_config.setdefault('continue_on_error', cleaned_params.get('continue_on_error', True))
        graceful_degradation = stability_config.setdefault('graceful_degradation', cleaned_params.get('graceful_degradation', True))
        
        # Advanced test defaults
        test_all_models = stability_config.setdefault('test_all_models', cleaned_params.get('test_all_models', False))
        test_data_sources = stability_config.setdefault('test_data_sources', cleaned_params.get('test_data_sources', False))
        test_hardware_configs = stability_config.setdefault('test_hardware_configs', cleaned_params.get('test_hardware_configs', False))
        performance_benchmarking = stability_config.setdefault('performance_benchmarking', cleaned_params.get('performance_benchmarking', False))

        # Set up logging
        if verbose:
            handlers_to_suppress = []
            for handler in logger.handlers:
                if isinstance(handler, logging.StreamHandler) and handler.stream.name in ['<stdout>', '<stderr>']:
                    handlers_to_suppress.append(handler)
                    handler.setLevel(logging.CRITICAL)  # Temporarily suppress console output
        
        # Initialize test results
        test_results = {
            'start_time': start_time.isoformat(),
            'test_configuration': stability_config,
            'system_info': {},
            'tests_run': [],
            'test_results': {},
            'overall_status': 'UNKNOWN',
            'recommendations': [],
            'warnings': [],
            'errors': [],
            'performance_metrics': {},
            'hardware_compatibility': {},
            'summary': {}
        }
        
        # Interactive prompt if enabled
        if interactive:
            print(Fore.YELLOW + Style.BRIGHT + "\nStability Test Validates:")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ System hardware compatibility")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Training pipeline functionality")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Data processing integrity")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Model training stability")
            print(Fore.GREEN + Style.BRIGHT + "  └─ Memory and performance characteristics")
            
            # Display test configuration
            print(Fore.YELLOW + Style.BRIGHT + "\nTest Mode Configuration:")
            print(Fore.WHITE + Style.BRIGHT + f"  1. Mode: " + Fore.GREEN + Style.BRIGHT + f"{'Comprehensive' if comprehensive_test else 'Stress' if stress_test else 'Minimal' if minimal_test else 'Quick'}")
            print(Fore.WHITE + Style.BRIGHT + f"  2. Model Type: " + Fore.GREEN + Style.BRIGHT + f"{test_model_type}")
            print(Fore.WHITE + Style.BRIGHT + f"  3. Epochs: " + Fore.GREEN + Style.BRIGHT + f"{test_epochs}")
            print(Fore.WHITE + Style.BRIGHT + f"  4. Batch Size: " + Fore.GREEN + Style.BRIGHT + f"{test_batch_size}")
            print(Fore.WHITE + Style.BRIGHT + f"  5. Learning Rate: " + Fore.GREEN + Style.BRIGHT + f"{test_learning_rate}")
            print(Fore.WHITE + Style.BRIGHT + f"  6. Samples: " + Fore.GREEN + Style.BRIGHT + f"{test_normal_samples} normal, {test_attack_samples} attack")
            print(Fore.WHITE + Style.BRIGHT + f"  7. Features: " + Fore.GREEN + Style.BRIGHT + f"{test_features}")
            print(Fore.WHITE + Style.BRIGHT + f"  8. Device: " + Fore.GREEN + Style.BRIGHT + f"{test_device}")
            print(Fore.WHITE + Style.BRIGHT + f"  9. Mixed Precision: " + Fore.GREEN + Style.BRIGHT + f"{test_mixed_precision}")
            
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nProceed with stability test? (Y/n): " + Style.RESET_ALL).lower().strip()
            if confirm not in ('', 'y', 'yes'):
                print(Fore.RED + Style.BRIGHT + "\nStability test cancelled by user")
                test_results['overall_status'] = 'CANCELLED'
                return test_results
        
        try:
            # Create test results directory
            test_results_dir = Path(test_results_dir)
            test_results_dir.mkdir(parents=True, exist_ok=True)
            
            # Test timestamp for unique identification
            test_timestamp = start_time.strftime("%Y%m%d_%H%M%S")
            test_session_id = f"stability_test_{test_timestamp}"
            
            test_results['test_session_id'] = test_session_id
            test_results['test_results_dir'] = str(test_results_dir)
            
            # System Information Collection
            try:
                system_info = {
                    'python_version': sys.version,
                    'pytorch_version': torch.__version__,
                    'platform': sys.platform,
                    'architecture': platform.architecture(),
                    'processor': platform.processor(),
                    'machine': platform.machine(),
                    'node': platform.node(),
                    'system': platform.system(),
                    'device': test_device if test_device != 'auto' else ('CUDA' if torch.cuda.is_available() else 'MPS' if (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()) else 'CPU'),
                    'timestamp': test_timestamp
                }
                
                # Hardware information
                try:
                    hw_info = check_hardware()
                    system_info.update(hw_info)
                except Exception as e:
                    logger.warning(f"Failed to get hardware info: {e}")
                    system_info['hardware_error'] = str(e)
                
                # Memory information
                try:
                    memory = psutil.virtual_memory()
                    system_info.update({
                        'total_memory_gb': memory.total / (1024**3),
                        'available_memory_gb': memory.available / (1024**3),
                        'memory_percent_used': memory.percent
                    })
                except ImportError:
                    logger.warning("psutil not available for memory monitoring")
                except Exception as e:
                    logger.warning(f"Memory info collection failed: {e}")
                
                # CUDA information
                if torch.cuda.is_available():
                    system_info.update({
                        'cuda_version': torch.version.cuda,
                        'cudnn_version': torch.backends.cudnn.version(),
                        'gpu_count': torch.cuda.device_count(),
                        'gpu_names': [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],
                        'gpu_memory': [torch.cuda.get_device_properties(i).total_memory / (1024**3) for i in range(torch.cuda.device_count())]
                    })
                
                test_results['system_info'] = system_info
                
                if verbose:
                    print(Fore.YELLOW + Style.BRIGHT + "\nSystem Information:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ System: " + Fore.YELLOW + Style.BRIGHT + f"{system_info.get('system', 'Unknown')} {system_info.get('machine', '')}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Python: " + Fore.YELLOW + Style.BRIGHT + f"{system_info.get('python_version', '').split()[0]}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ PyTorch: " + Fore.YELLOW + Style.BRIGHT + f"{system_info.get('pytorch_version', 'Unknown')}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Device: " + Fore.YELLOW + Style.BRIGHT + f"{system_info.get('device', 'Unknown')}")
                    if torch.cuda.is_available():
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA: " + Fore.YELLOW + Style.BRIGHT + f"{system_info.get('cuda_version', 'Unknown')}")
                        print(Fore.GREEN + Style.BRIGHT + f"  └─ GPUs: " + Fore.YELLOW + Style.BRIGHT + f"{system_info.get('gpu_count', 0)}")
                    print()
                
            except Exception as e:
                error_msg = f"System information collection failed: {e}"
                logger.error(error_msg)
                test_results['errors'].append(error_msg)
                test_results['system_info'] = {'error': str(e)}
            
            # Test 1: Basic Hardware Compatibility
            logger.info("Test 1: Hardware Compatibility")
            test_results['tests_run'].append('hardware_compatibility')
            
            try:
                hw_test_results = {
                    'test_name': 'hardware_compatibility',
                    'status': 'UNKNOWN',
                    'details': {},
                    'duration_seconds': 0,
                    'error': None
                }
                
                hw_start_time = time.time()
                
                # Device detection and validation
                if test_device == 'auto':
                    if torch.cuda.is_available():
                        device = torch.device('cuda')
                        hw_test_results['details']['cuda_available'] = True
                    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        device = torch.device('mps')
                        hw_test_results['details']['mps_available'] = True
                    else:
                        device = torch.device('cpu')
                    hw_test_results['details']['auto_device'] = str(device)
                else:
                    device = torch.device(test_device)
                    hw_test_results['details']['specified_device'] = str(device)
                
                # Test basic tensor operations
                test_tensor = torch.randn(100, test_features, device=device)
                test_output = torch.relu(test_tensor @ test_tensor.T)
                
                hw_test_results['details'].update({
                    'tensor_operations': 'SUCCESS',
                    'device_used': str(device),
                    'tensor_shape_test': test_tensor.shape,
                    'computation_result_shape': test_output.shape
                })
                
                # Test mixed precision if enabled
                if test_mixed_precision and device.type == 'cuda':
                    try:
                        autocast_context = get_autocast_context(device, True, True)
                        with autocast_context:
                            mp_test = torch.matmul(test_tensor, test_tensor.T)
                        hw_test_results['details']['mixed_precision'] = 'SUCCESS'
                    except Exception as mp_e:
                        hw_test_results['details']['mixed_precision'] = f'FAILED: {str(mp_e)}'
                
                hw_test_results['duration_seconds'] = time.time() - hw_start_time
                hw_test_results['status'] = 'PASSED'
                
                if verbose:
                    print(Fore.YELLOW + Style.BRIGHT + f"\nHardware compatibility test passed:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Device: " + Fore.YELLOW + Style.BRIGHT + f"{device}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Tensor operations: " + Fore.YELLOW + Style.BRIGHT + f"OK")
                    if test_mixed_precision and device.type == 'cuda':
                        print(Fore.GREEN + Style.BRIGHT + f"   Device: " + Fore.YELLOW + Style.BRIGHT + f"{device}")
                        print(Fore.GREEN + Style.BRIGHT + f"     └─ Mixed precision: " + Fore.YELLOW + Style.BRIGHT + f"{hw_test_results['details'].get('mixed_precision', 'N/A')}")
                    print("\n")
                
            except Exception as e:
                hw_test_results['status'] = 'FAILED'
                hw_test_results['error'] = str(e)
                hw_test_results['duration_seconds'] = time.time() - hw_start_time
                test_results['errors'].append(f"Hardware compatibility test failed: {e}")
                
                if verbose:
                    print(Fore.RED + Style.BRIGHT + f"\nHardware compatibility test failed: {e}")
                
                if not continue_on_error:
                    test_results['test_results']['hardware_compatibility'] = hw_test_results
                    test_results['overall_status'] = 'FAILED'
                    return test_results
            
            test_results['test_results']['hardware_compatibility'] = hw_test_results
            test_results['hardware_compatibility'] = hw_test_results['details']
            
            # Test 2: Data Pipeline Integrity
            logger.info("Test 2: Data Pipeline Integrity")
            test_results['tests_run'].append('data_pipeline')
            
            try:
                data_test_results = {
                    'test_name': 'data_pipeline',
                    'status': 'UNKNOWN',
                    'details': {},
                    'duration_seconds': 0,
                    'error': None
                }
                
                data_start_time = time.time()
                
                # Test synthetic data generation
                synthetic_data = generate_synthetic_data(
                    normal_samples=test_normal_samples,
                    attack_samples=test_attack_samples,
                    features=test_features,
                    random_state=42,
                    output_format='dict',
                    verbose=False,
                    config=final_config
                )
                
                data_test_results['details'].update({
                    'synthetic_data_generation': 'SUCCESS',
                    'train_samples': len(synthetic_data['X_train']),
                    'val_samples': len(synthetic_data['X_val']),
                    'test_samples': len(synthetic_data['X_test']),
                    'features': len(synthetic_data['feature_names']),
                    'data_shapes': {
                        'X_train': synthetic_data['X_train'].shape,
                        'X_val': synthetic_data['X_val'].shape,
                        'X_test': synthetic_data['X_test'].shape
                    }
                })
                
                # Test data validation
                validation_result = validate_data_integrity(synthetic_data, final_config)
                data_test_results['details']['data_validation'] = {
                    'passed': validation_result['passed'],
                    'quality_score': validation_result['quality_score'],
                    'warnings': len(validation_result['warnings']),
                    'errors': len(validation_result['errors'])
                }
                
                # Test DataLoader creation
                train_loader, val_loader, test_loader = create_dataloaders(
                    data=synthetic_data,
                    batch_size=test_batch_size,
                    num_workers=test_num_workers,
                    pin_memory=False,  # Disable for stability test
                    config=final_config
                )
                
                data_test_results['details']['dataloader_creation'] = {
                    'status': 'SUCCESS',
                    'train_batches': len(train_loader),
                    'val_batches': len(val_loader),
                    'test_batches': len(test_loader)
                }
                
                # Test batch iteration
                test_batch = next(iter(train_loader))
                data_test_results['details']['batch_iteration'] = {
                    'status': 'SUCCESS',
                    'batch_shape': test_batch.shape if isinstance(test_batch, torch.Tensor) else test_batch[0].shape,
                    'batch_dtype': str(test_batch.dtype if isinstance(test_batch, torch.Tensor) else test_batch[0].dtype)
                }
                
                data_test_results['duration_seconds'] = time.time() - data_start_time
                data_test_results['status'] = 'PASSED'
                
                if verbose:
                    print(Fore.YELLOW + Style.BRIGHT + f"\nData pipeline test passed:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Synthetic data: " + Fore.YELLOW + Style.BRIGHT + f"{test_normal_samples + test_attack_samples} samples, {test_features} features")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ DataLoaders: " + Fore.YELLOW + Style.BRIGHT + f"{len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test batches")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Data quality score: " + Fore.YELLOW + Style.BRIGHT + f"{validation_result['quality_score']:.2f}")
            
            except Exception as e:
                data_test_results['status'] = 'FAILED'
                data_test_results['error'] = str(e)
                data_test_results['duration_seconds'] = time.time() - data_start_time
                test_results['errors'].append(f"Data pipeline test failed: {e}")
                
                if verbose:
                    print(Fore.RED + Style.BRIGHT + f"\nData pipeline test failed: {e}")
                
                if not continue_on_error:
                    test_results['test_results']['data_pipeline'] = data_test_results
                    test_results['overall_status'] = 'FAILED'
                    return test_results
            
            test_results['test_results']['data_pipeline'] = data_test_results
            
            # Test 3: Model Initialization
            logger.info("Test 3: Model Initialization")
            test_results['tests_run'].append('model_initialization')
            
            try:
                model_test_results = {
                    'test_name': 'model_initialization',
                    'status': 'UNKNOWN',
                    'details': {},
                    'duration_seconds': 0,
                    'error': None
                }
                
                model_start_time = time.time()
                
                # Prepare model configuration
                model_config = {
                    'model': {
                        'model_type': test_model_type,
                        'input_dim': test_features,
                        'encoding_dim': test_encoding_dim,
                        'hidden_dims': [max(32, test_features // 2)],
                        'dropout_rates': [0.2],
                        'activation': 'leaky_relu',
                        'normalization': None if test_model_type == 'SimpleAutoencoder' else 'batch',
                        'skip_connection': False if test_model_type == 'SimpleAutoencoder' else True,
                        'residual_blocks': False if test_model_type == 'SimpleAutoencoder' else True,
                        'use_attention': False if test_model_type == 'SimpleAutoencoder' else True
                    },
                    'training': {
                        'mixed_precision': test_mixed_precision,
                        'batch_size': test_batch_size
                    },
                    'hardware': {
                        'device': str(device)
                    }
                }
                
                # Create model instance
                if test_model_type == 'SimpleAutoencoder':
                    model = SimpleAutoencoder(
                        input_dim=test_features,
                        encoding_dim=test_encoding_dim,
                        config=model_config
                    )
                elif test_model_type == 'EnhancedAutoencoder':
                    model = EnhancedAutoencoder(
                        input_dim=test_features,
                        encoding_dim=test_encoding_dim,
                        config=model_config
                    )
                elif test_model_type == 'AutoencoderEnsemble':
                    model = AutoencoderEnsemble(
                        input_dim=test_features,
                        encoding_dim=test_encoding_dim,
                        num_models=2,  # Reduced for stability test
                        config=model_config
                    )
                else:
                    raise ValueError(f"Unknown model type: {test_model_type}")
                
                model = model.to(device)
                
                # Test model forward pass
                test_input = torch.randn(test_batch_size, test_features, device=device)
                
                autocast_context = get_autocast_context(device, test_mixed_precision, True)
                with autocast_context:
                    test_output = model(test_input)
                
                # Model information
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                
                model_test_results['details'].update({
                    'model_creation': 'SUCCESS',
                    'model_type': test_model_type,
                    'model_class': type(model).__name__,
                    'total_parameters': total_params,
                    'trainable_parameters': trainable_params,
                    'model_size_mb': total_params * 4 / (1024 * 1024),
                    'forward_pass': 'SUCCESS',
                    'input_shape': test_input.shape,
                    'output_shape': test_output.shape,
                    'output_dtype': str(test_output.dtype),
                    'device_placement': str(next(model.parameters()).device)
                })
                
                model_test_results['duration_seconds'] = time.time() - model_start_time
                model_test_results['status'] = 'PASSED'
                
                if verbose:
                    print(Fore.YELLOW + Style.BRIGHT + f"Model initialization test passed:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{test_model_type} ({type(model).__name__})")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Parameters: " + Fore.YELLOW + Style.BRIGHT + f"{total_params:,} total, {trainable_params:,} trainable")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Forward pass: " + Fore.YELLOW + Style.BRIGHT + f"{test_input.shape} → {test_output.shape}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Device: " + Fore.YELLOW + Style.BRIGHT + f"{next(model.parameters()).device}")
            
            except Exception as e:
                model_test_results['status'] = 'FAILED'
                model_test_results['error'] = str(e)
                model_test_results['duration_seconds'] = time.time() - model_start_time
                test_results['errors'].append(f"Model initialization test failed: {e}")
                
                if verbose:
                    print(Fore.RED + Style.BRIGHT + f"\nModel initialization test failed: {e}")
                
                if not continue_on_error:
                    test_results['test_results']['model_initialization'] = model_test_results
                    test_results['overall_status'] = 'FAILED'
                    return test_results
            
            test_results['test_results']['model_initialization'] = model_test_results
            
            # Test 4: Training Stability
            logger.info("Test 4: Training Stability")
            test_results['tests_run'].append('training_stability')
            
            try:
                training_test_results = {
                    'test_name': 'training_stability',
                    'status': 'UNKNOWN',
                    'details': {},
                    'duration_seconds': 0,
                    'error': None
                }
                
                training_start_time = time.time()
                
                # Prepare training configuration
                stability_training_config = {
                    'model': {
                        'model_type': test_model_type,
                        'input_dim': test_features,
                        'encoding_dim': test_encoding_dim,
                        'hidden_dims': [max(32, test_features // 2)] if test_model_type == 'SimpleAutoencoder' else [128, 64],
                        'dropout_rates': [0.2] if test_model_type == 'SimpleAutoencoder' else [0.2, 0.15],
                        'activation': 'leaky_relu',
                        'normalization': None if test_model_type == 'SimpleAutoencoder' else 'batch'
                    },
                    'training': {
                        'batch_size': test_batch_size,
                        'epochs': test_epochs,
                        'learning_rate': test_learning_rate,
                        'patience': max(3, test_epochs // 3),
                        'weight_decay': 1e-4,
                        'gradient_clip': 1.0,
                        'mixed_precision': test_mixed_precision,
                        'optimizer_type': 'AdamW',
                        'scheduler_type': 'ReduceLROnPlateau',
                        'early_stopping': True,
                        'validation_split': 0.2
                    },
                    'data': {
                        'normal_samples': test_normal_samples,
                        'attack_samples': test_attack_samples,
                        'features': test_features,
                        'use_real_data': False,
                        'data_preprocessing': True
                    },
                    'system': {
                        'model_dir': test_results_dir / "training_test",
                        'device': str(device),
                        'random_seed': 42,
                        'reproducible': True
                    },
                    'monitoring': {
                        'verbose': False,
                        'debug_mode': False,
                        'tensorboard_logging': False,
                        'save_checkpoints': False,
                        'progress_bar': False
                    },
                    'export': {
                        'export_onnx': False,
                        'save_model': False,
                        'save_metadata': False,
                        'save_training_history': False
                    },
                    'advanced_training': {
                        'num_workers': test_num_workers,
                        'pin_memory': False,
                        'memory_efficient': test_memory_efficient
                    },
                    'error_handling': {
                        'error_handling': 'strict',
                        'graceful_degradation': graceful_degradation
                    }
                }
                
                # Run training
                training_results = train_model(config=stability_training_config)
                
                # Analyze training results
                if training_results and training_results.get('success', False):
                    final_metrics = training_results.get('final_metrics', {})
                    model_info = training_results.get('model_info', {})
                    training_time = training_results.get('training_time_minutes', 0)
                    
                    training_test_results['details'].update({
                        'training_completion': 'SUCCESS',
                        'epochs_completed': final_metrics.get('final_epoch', 0),
                        'best_val_loss': final_metrics.get('best_validation_loss', float('inf')),
                        'test_loss': final_metrics.get('test_loss', float('inf')),
                        'training_time_minutes': training_time,
                        'model_parameters': model_info.get('parameters', 0),
                        'threshold_calculated': final_metrics.get('threshold', 0),
                        'anomaly_rate': final_metrics.get('anomaly_detection_rate', 0)
                    })
                    
                    # Determine training quality
                    best_val_loss = final_metrics.get('best_validation_loss', float('inf'))
                    if best_val_loss < 0.05:
                        training_quality = 'EXCELLENT'
                    elif best_val_loss < 0.1:
                        training_quality = 'GOOD'
                    elif best_val_loss < 0.5:
                        training_quality = 'ACCEPTABLE'
                    else:
                        training_quality = 'POOR'
                    
                    training_test_results['details']['training_quality'] = training_quality
                    training_test_results['status'] = 'PASSED'
                    
                    if verbose:
                        print(Fore.YELLOW + Style.BRIGHT + f"Training stability test passed:")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('final_epoch', 0)}/{test_epochs}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Best validation loss: " + Fore.YELLOW + Style.BRIGHT + f"{best_val_loss:.6f}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Training quality: " + Fore.YELLOW + Style.BRIGHT + f"{training_quality}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Training time: " + Fore.YELLOW + Style.BRIGHT + f"{training_time:.1f} minutes")
                        print(Fore.GREEN + Style.BRIGHT + f"  └─ Threshold: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('threshold', 0):.6f}")
                
                else:
                    # Training failed or returned error
                    error_info = training_results.get('error', 'Unknown training error') if training_results else 'No training results returned'
                    training_test_results['details']['training_completion'] = 'FAILED'
                    training_test_results['details']['error_info'] = error_info
                    training_test_results['status'] = 'FAILED'
                    test_results['errors'].append(f"Training failed: {error_info}")
                    
                    if verbose:
                        print(Fore.RED + Style.BRIGHT + f"\nTraining stability test failed: {error_info}")
                
                training_test_results['duration_seconds'] = time.time() - training_start_time
            
            except Exception as e:
                training_test_results['status'] = 'FAILED'
                training_test_results['error'] = str(e)
                training_test_results['duration_seconds'] = time.time() - training_start_time
                test_results['errors'].append(f"Training stability test failed: {e}")
                
                if verbose:
                    print(Fore.RED + Style.BRIGHT + f"\nTraining stability test failed: {e}")
            
            test_results['test_results']['training_stability'] = training_test_results
            
            # Test 5: Performance Benchmarking (if enabled)
            if performance_benchmarking or comprehensive_test:
                logger.info("Test 5: Performance Benchmarking")
                test_results['tests_run'].append('performance_benchmarking')
                
                try:
                    perf_test_results = {
                        'test_name': 'performance_benchmarking',
                        'status': 'UNKNOWN',
                        'details': {},
                        'duration_seconds': 0,
                        'error': None
                    }
                    
                    perf_start_time = time.time()
                    
                    # Memory usage monitoring
                    if torch.cuda.is_available():
                        torch.cuda.reset_peak_memory_stats(device)
                        initial_memory = torch.cuda.memory_allocated(device)
                    
                    # Throughput testing
                    model.eval()
                    throughput_samples = 1000
                    throughput_data = torch.randn(throughput_samples, test_features, device=device)
                    
                    # Warmup
                    with torch.no_grad():
                        for _ in range(10):
                            _ = model(throughput_data[:32])
                    
                    # Actual throughput test
                    torch.cuda.synchronize() if torch.cuda.is_available() else None
                    throughput_start = time.time()
                    
                    with torch.no_grad():
                        throughput_output = model(throughput_data)
                    
                    torch.cuda.synchronize() if torch.cuda.is_available() else None
                    throughput_time = time.time() - throughput_start
                    
                    samples_per_second = throughput_samples / throughput_time
                    
                    perf_test_results['details'].update({
                        'throughput_test': 'SUCCESS',
                        'samples_per_second': samples_per_second,
                        'inference_time_ms': throughput_time * 1000,
                        'throughput_samples': throughput_samples
                    })
                    
                    # Memory usage
                    if torch.cuda.is_available():
                        peak_memory = torch.cuda.max_memory_allocated(device)
                        memory_usage_mb = (peak_memory - initial_memory) / (1024 * 1024)
                        perf_test_results['details'].update({
                            'memory_usage_mb': memory_usage_mb,
                            'peak_memory_mb': peak_memory / (1024 * 1024)
                        })
                    
                    perf_test_results['duration_seconds'] = time.time() - perf_start_time
                    perf_test_results['status'] = 'PASSED'
                    
                    # Store performance metrics
                    test_results['performance_metrics'] = {
                        'samples_per_second': samples_per_second,
                        'inference_time_ms': throughput_time * 1000,
                        'memory_usage_mb': perf_test_results['details'].get('memory_usage_mb', 0)
                    }
                    
                    if verbose:
                        print(Fore.YELLOW + Style.BRIGHT + f"\nPerformance benchmarking passed:")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Throughput: " + Fore.YELLOW + Style.BRIGHT + f"{samples_per_second:.1f} samples/sec")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Inference time: " + Fore.YELLOW + Style.BRIGHT + f"{throughput_time * 1000:.2f} ms")
                        if torch.cuda.is_available():
                            print(Fore.GREEN + Style.BRIGHT + f"  └─ Memory usage: " + Fore.YELLOW + Style.BRIGHT + f"{memory_usage_mb:.3f} MB")
                
                except Exception as e:
                    perf_test_results['status'] = 'FAILED'
                    perf_test_results['error'] = str(e)
                    perf_test_results['duration_seconds'] = time.time() - perf_start_time
                    test_results['warnings'].append(f"Performance benchmarking failed: {e}")
                    
                    if verbose:
                        print(Fore.RED + Style.BRIGHT + f"\nPerformance benchmarking failed: {e}")
                
                test_results['test_results']['performance_benchmarking'] = perf_test_results
            
            # Additional tests for comprehensive mode
            if comprehensive_test:
                # Test different model types
                if test_all_models:
                    logger.info("Test 6: Multiple Model Types")
                    test_results['tests_run'].append('multiple_models')
                    
                    model_types_to_test = ['SimpleAutoencoder', 'EnhancedAutoencoder']
                    multi_model_results = {}
                    
                    for mt in model_types_to_test:
                        if mt != test_model_type:  # Skip already tested model
                            try:
                                # Quick test for each model type
                                quick_config = stability_training_config.copy()
                                quick_config['model_architecture']['model_type'] = mt
                                quick_config['training_config']['epochs'] = 3
                                quick_config['training_config']['verbose'] = False
                                
                                result = train_model(config=quick_config)
                                success = result and result.get('success', False)
                                
                                multi_model_results[mt] = {
                                    'status': 'PASSED' if success else 'FAILED',
                                    'final_loss': result.get('final_metrics', {}).get('best_validation_loss', float('inf')) if result else float('inf')
                                }
                                
                            except Exception as e:
                                multi_model_results[mt] = {
                                    'status': 'FAILED',
                                    'error': str(e)
                                }
                    
                    test_results['test_results']['multiple_models'] = {
                        'test_name': 'multiple_models',
                        'status': 'PASSED' if all(r.get('status') == 'PASSED' for r in multi_model_results.values()) else 'PARTIAL',
                        'details': multi_model_results
                    }
                    
                    if verbose:
                        for mt, result in multi_model_results.items():
                            status_symbol = "✓" if result['status'] == 'PASSED' else "✗"
                            print(Fore.GREEN + Style.BRIGHT + f"\n  {status_symbol} {mt}: {result['status']}")
            
            # Stress test
            if stress_test:
                logger.info("Test 7: Stress Testing")
                test_results['tests_run'].append('stress_test')
                
                try:
                    stress_test_results = {
                        'test_name': 'stress_test',
                        'status': 'UNKNOWN',
                        'details': {},
                        'duration_seconds': 0,
                        'error': None
                    }
                    
                    stress_start_time = time.time()
                    
                    # Memory stress test
                    large_batch_size = min(512, test_batch_size * 8)
                    large_data = torch.randn(large_batch_size, test_features, device=device)
                    
                    # Repeated inference
                    stress_iterations = 100
                    successful_iterations = 0
                    
                    model.eval()
                    for i in range(stress_iterations):
                        try:
                            with torch.no_grad():
                                _ = model(large_data)
                            successful_iterations += 1
                        except Exception as e:
                            if i == 0:  # Fail immediately if first iteration fails
                                raise e
                            break
                    
                    stress_success_rate = successful_iterations / stress_iterations
                    
                    stress_test_results['details'].update({
                        'stress_iterations': stress_iterations,
                        'successful_iterations': successful_iterations,
                        'success_rate': stress_success_rate,
                        'large_batch_size': large_batch_size
                    })
                    
                    stress_test_results['duration_seconds'] = time.time() - stress_start_time
                    stress_test_results['status'] = 'PASSED' if stress_success_rate > 0.95 else 'PARTIAL' if stress_success_rate > 0.8 else 'FAILED'
                    
                    if verbose:
                        status_symbol = "✓" if stress_test_results['status'] == 'PASSED' else "⚠" if stress_test_results['status'] == 'PARTIAL' else "✗"
                        status_color = Fore.GREEN if stress_test_results['status'] == 'PASSED' else Fore.YELLOW if stress_test_results['status'] == 'PARTIAL' else Fore.RED
                        print(status_color + Style.BRIGHT + f"\n{status_symbol} Stress test: {successful_iterations}/{stress_iterations} iterations successful")
                
                except Exception as e:
                    stress_test_results['status'] = 'FAILED'
                    stress_test_results['error'] = str(e)
                    stress_test_results['duration_seconds'] = time.time() - stress_start_time
                    test_results['warnings'].append(f"Stress test failed: {e}")
                    
                    if verbose:
                        print(Fore.RED + Style.BRIGHT + f"\nStress test failed: {e}")
                
                test_results['test_results']['stress_test'] = stress_test_results
            
            # Calculate overall test duration
            total_test_time = time.time() - test_start_time
            test_results['total_duration_seconds'] = total_test_time
            
            # Determine overall status
            test_statuses = [result.get('status', 'UNKNOWN') for result in test_results['test_results'].values()]
            
            if all(status == 'PASSED' for status in test_statuses):
                overall_status = 'PASSED'
                status_description = "All tests passed successfully"
            elif any(status == 'FAILED' for status in test_statuses):
                if any(status == 'PASSED' for status in test_statuses):
                    overall_status = 'PARTIAL'
                    status_description = "Some tests failed"
                else:
                    overall_status = 'FAILED'
                    status_description = "Multiple tests failed"
            elif any(status == 'PARTIAL' for status in test_statuses):
                overall_status = 'PARTIAL'
                status_description = "Tests completed with warnings"
            else:
                overall_status = 'UNKNOWN'
                status_description = "Test status unclear"
            
            test_results['overall_status'] = overall_status
            test_results['status_description'] = status_description
            
            # Generate recommendations
            recommendations = []
            
            # Performance recommendations
            if test_results.get('performance_metrics', {}).get('samples_per_second', 0) < 100:
                recommendations.append("Consider using GPU acceleration for better performance")
            
            # Memory recommendations
            if test_results.get('performance_metrics', {}).get('memory_usage_mb', 0) > 1000:
                recommendations.append("High memory usage detected - consider reducing batch size")
            
            # Training quality recommendations
            training_result = test_results['test_results'].get('training_stability', {})
            if training_result.get('status') == 'PASSED':
                best_loss = training_result.get('details', {}).get('best_val_loss', float('inf'))
                if best_loss > 0.1:
                    recommendations.append("Training loss is high - consider adjusting learning rate or model architecture")
                elif best_loss > 0.05:
                    recommendations.append("Training performance is acceptable but could be improved")
            
            # Hardware recommendations
            if not torch.cuda.is_available():
                recommendations.append("CUDA not available - GPU acceleration would improve performance")
            
            # Error-based recommendations
            if test_results['errors']:
                recommendations.append("Errors detected - check system compatibility and dependencies")
            
            test_results['recommendations'] = recommendations
            
            # Summary statistics
            test_results['summary'] = {
                'total_tests_run': len(test_results['tests_run']),
                'tests_passed': sum(1 for result in test_results['test_results'].values() if result.get('status') == 'PASSED'),
                'tests_failed': sum(1 for result in test_results['test_results'].values() if result.get('status') == 'FAILED'),
                'tests_partial': sum(1 for result in test_results['test_results'].values() if result.get('status') == 'PARTIAL'),
                'total_errors': len(test_results['errors']),
                'total_warnings': len(test_results['warnings']),
                'total_duration_minutes': total_test_time / 60,
                'overall_status': overall_status,
                'system_stable': overall_status in ['PASSED', 'PARTIAL'] and len(test_results['errors']) == 0
            }
            
            # Save test results if requested
            if save_test_results:
                try:
                    results_file = test_results_dir / f"stability_test_results_{test_timestamp}.json"
                    with open(results_file, 'w') as f:
                        json.dump(test_results, f, indent=2, default=str)
                    test_results['results_file'] = str(results_file)
                    
                    if verbose:
                        print(Fore.GREEN + Style.BRIGHT + f"\nTest results saved to: {results_file}")
                
                except Exception as e:
                    logger.warning(f"Failed to save test results: {e}")
                    test_results['warnings'].append(f"Failed to save results: {e}")
            
            # Display comprehensive results
            if verbose:
                print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
                print(Fore.MAGENTA + Style.BRIGHT + "STABILITY TEST RESULTS")
                print(Fore.CYAN + Style.BRIGHT + "-"*40)
                
                # Overall status
                status_symbol = "✓" if overall_status == 'PASSED' else "⚠" if overall_status == 'PARTIAL' else "✗"
                status_color = Fore.GREEN if overall_status == 'PASSED' else Fore.YELLOW if overall_status == 'PARTIAL' else Fore.RED
                print(Fore.CYAN + Style.BRIGHT + f"Overall Status: " + status_color + Style.BRIGHT + f"{status_symbol} {overall_status} - {status_description}")
                
                # Summary
                summary = test_results['summary']
                print(Fore.YELLOW + Style.BRIGHT + f"Test Summary:")
                print(Fore.CYAN + Style.BRIGHT + f"- Total Tests: " + Fore.GREEN + Style.BRIGHT + f"{summary['total_tests_run']}")
                print(Fore.CYAN + Style.BRIGHT + f"- Passed: " + Fore.GREEN + Style.BRIGHT + f"{summary['tests_passed']}")
                print(Fore.CYAN + Style.BRIGHT + f"- Failed: " + Fore.GREEN + Style.BRIGHT + f"{summary['tests_failed']}")
                print(Fore.CYAN + Style.BRIGHT + f"- Partial: " + Fore.GREEN + Style.BRIGHT + f"{summary['tests_partial']}")
                print(Fore.CYAN + Style.BRIGHT + f"- Duration: " + Fore.GREEN + Style.BRIGHT + f"{summary['total_duration_minutes']:.1f} minutes")
                
                # System information
                if test_results['system_info']:
                    si = test_results['system_info']
                    print(Fore.YELLOW + Style.BRIGHT + f"\nSystem Information:")
                    print(Fore.CYAN + Style.BRIGHT + f"- Platform: " + Fore.GREEN + Style.BRIGHT + f"{si.get('system', 'Unknown')} {si.get('machine', '')}")
                    print(Fore.CYAN + Style.BRIGHT + f"- Python: " + Fore.GREEN + Style.BRIGHT + f"{si.get('python_version', '').split()[0]}")
                    print(Fore.CYAN + Style.BRIGHT + f"- PyTorch: " + Fore.GREEN + Style.BRIGHT + f"{si.get('pytorch_version', 'Unknown')}")
                    print(Fore.CYAN + Style.BRIGHT + f"- Device: " + Fore.GREEN + Style.BRIGHT + f"{si.get('device', 'Unknown')}")
                    if si.get('gpu_count', 0) > 0:
                        print(Fore.CYAN + Style.BRIGHT + f"- GPUs: " + Fore.GREEN + Style.BRIGHT + f"{si.get('gpu_count', 0)}")
                
                # Performance metrics
                if test_results.get('performance_metrics'):
                    pm = test_results['performance_metrics']
                    print(Fore.YELLOW + Style.BRIGHT + f"\nPerformance Metrics:")
                    print(Fore.CYAN + Style.BRIGHT + f"- Throughput: " + Fore.GREEN + Style.BRIGHT + f"{pm.get('samples_per_second', 0):.1f} samples/sec")
                    print(Fore.CYAN + Style.BRIGHT + f"- Inference Time: " + Fore.GREEN + Style.BRIGHT + f"{pm.get('inference_time_ms', 0):.2f} ms")
                    if pm.get('memory_usage_mb'):
                        print(Fore.CYAN + Style.BRIGHT + f"- Memory Usage: " + Fore.GREEN + Style.BRIGHT + f"{pm.get('memory_usage_mb', 0):.1f} MB")
                
                # Recommendations
                if test_results['recommendations']:
                    print(Fore.YELLOW + Style.BRIGHT + f"\nRecommendations:")
                    for i, rec in enumerate(test_results['recommendations'], 1):
                        print(Fore.CYAN + Style.BRIGHT + f"{i}. " + Fore.GREEN + Style.BRIGHT + f"{rec}")
                
                # Errors and warnings
                if test_results['errors']:
                    print(Fore.YELLOW + Style.BRIGHT + f"\nErrors ({len(test_results['errors'])}):")
                    for i, error in enumerate(test_results['errors'], 1):
                        print(Fore.YELLOW + Style.BRIGHT + f"{i}. " + Fore.RED + Style.BRIGHT + f"{error}")
                
                if test_results['warnings']:
                    print(Fore.YELLOW + Style.BRIGHT + f"\nWarnings ({len(test_results['warnings'])}):")
                    for i, warning in enumerate(test_results['warnings'], 1):
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. " + Fore.YELLOW + Style.BRIGHT + f"{warning}")
                
                # Final recommendation
                if overall_status == 'PASSED':
                    final_msg = "System is stable and ready for production use!"
                    final_color = Fore.GREEN
                elif overall_status == 'PARTIAL':
                    final_msg = "System is functional but may need attention for optimal performance."
                    final_color = Fore.YELLOW
                else:
                    final_msg = "System has stability issues that should be addressed before use."
                    final_color = Fore.RED
                
                print(final_color + Style.BRIGHT + f"\n{final_msg}")
            
            # Cleanup
            try:
                if 'model' in locals():
                    del model
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
            except Exception as cleanup_error:
                logger.warning(f"Cleanup failed: {cleanup_error}")
            
            return test_results
            
        except Exception as e:
            # Handle unexpected errors
            test_results['overall_status'] = 'ERROR'
            test_results['error'] = str(e)
            test_results['traceback'] = traceback.format_exc()
            test_results['total_duration_seconds'] = time.time() - test_start_time
            
            logger.error(f"Stability test encountered unexpected error: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            if verbose:
                print(Fore.RED + Style.BRIGHT + f"\nSTABILITY TEST ERROR: {e}")
                print(Fore.YELLOW + Style.BRIGHT + "Please check system configuration and dependencies.")
            
            # Save error results if possible
            if save_test_results:
                try:
                    error_file = test_results_dir / f"stability_test_error_{test_timestamp}.json"
                    with open(error_file, 'w') as f:
                        json.dump(test_results, f, indent=2, default=str)
                    test_results['error_file'] = str(error_file)
                except Exception as save_error:
                    logger.error(f"Failed to save error results: {save_error}")
            
            return test_results
        
        finally:
            # Restore logging level
            if verbose and 'original_level' in locals():
                try:
                    for handler in handlers_to_suppress:
                        handler.setLevel(logging.ERROR)
                except Exception:
                    pass
            
            # Final cleanup
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
            except Exception:
                pass
            
    except Exception as e:
        # Top-level error handling
        error_msg = f"Stability test initialization failed: {e}"
        logger.error(error_msg, exc_info=True)
        
        console.print(
            Panel.fit(
                f"{error_msg}",
                title="STABILITY TEST INITIALIZATION ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        return {
            'overall_status': 'FAILED',
            'errors': [error_msg],
            'tests_run': [],
            'test_results': {},
            'summary': {'total_tests': 0, 'passed_tests': 0, 'failed_tests': 0}
        }

def hyperparameter_search(
    # Data Parameters
    X_train: Optional[np.ndarray] = None,
    X_val: Optional[np.ndarray] = None,
    X_test: Optional[np.ndarray] = None,
    data: Optional[Dict[str, Any]] = None,
    use_real_data: Optional[bool] = None,
    data_path: Optional[Union[str, Path]] = None,
    artifacts_path: Optional[Union[str, Path]] = None,
    
    # Search Configuration Parameters
    n_trials: Optional[int] = None,
    timeout_seconds: Optional[float] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    
    # Model Type Selection
    model_types: Optional[List[str]] = None,
    search_all_models: Optional[bool] = None,
    
    # Cross-Validation Parameters
    cv_folds: Optional[int] = None,
    cv_shuffle: Optional[bool] = None,
    cv_random_state: Optional[int] = None,
    
    # Search Space Configuration
    search_space: Optional[Dict[str, Any]] = None,
    parameter_ranges: Optional[Dict[str, Any]] = None,
    fixed_params: Optional[Dict[str, Any]] = None,
    
    # Training Configuration
    trial_epochs: Optional[int] = None,
    trial_patience: Optional[int] = None,
    trial_batch_size: Optional[int] = None,
    
    # System Parameters
    device: Optional[str] = None,
    random_seed: Optional[int] = None,
    num_workers: Optional[int] = None,
    
    # Output Parameters
    verbose: Optional[bool] = None,
    save_study: Optional[bool] = None,
    study_dir: Optional[Union[str, Path]] = None,
    generate_plots: Optional[bool] = None,
    
    # Storage Parameters
    storage_url: Optional[str] = None,
    load_if_exists: Optional[bool] = None,
    
    # Optimization Callbacks
    early_stopping_patience: Optional[int] = None,
    early_stopping_min_trials: Optional[int] = None,
    
    # Advanced Parameters
    parallel_jobs: Optional[int] = None,
    memory_limit: Optional[str] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    hpo_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """
    Comprehensive hyperparameter search that integrates with the current train_model() implementation.
    
    Returns:
        Dictionary containing optimization results, best parameters, and study information.
    """
    
    # Start timing
    start_time = datetime.now()
    hpo_start_time = time.time()
    
    # Initialize configuration
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply HPO-specific configuration
    if hpo_config:
        config.setdefault('hyperparameter_optimization', {}).update(hpo_config)
    
    # Apply all parameters to configuration
    final_config = config.copy()
    
    # Apply individual parameters
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'hpo_config', 'kwargs', 'start_time', 'hpo_start_time',
        'datetime', 'traceback', 'time', 'gc', 'warnings', 'Path'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Set up HPO configuration with intelligent defaults
    hpo_config_section = final_config.setdefault('hyperparameter_optimization', {})
    
    # Core HPO parameters
    n_trials = hpo_config_section.setdefault('n_trials', cleaned_params.get('n_trials', 100))
    timeout_seconds = hpo_config_section.setdefault('timeout_seconds', cleaned_params.get('timeout_seconds', 0))
    study_name = hpo_config_section.setdefault('study_name', cleaned_params.get('study_name', f"autoencoder_hpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"))
    direction = hpo_config_section.setdefault('direction', cleaned_params.get('direction', 'minimize'))
    sampler_type = hpo_config_section.setdefault('sampler_type', cleaned_params.get('sampler_type', 'TPE'))
    pruner_type = hpo_config_section.setdefault('pruner_type', cleaned_params.get('pruner_type', 'MedianPruner'))
    
    # Model selection
    model_types = hpo_config_section.setdefault('model_types', cleaned_params.get('model_types', ['SimpleAutoencoder', 'EnhancedAutoencoder']))
    search_all_models = hpo_config_section.setdefault('search_all_models', cleaned_params.get('search_all_models', False))
    
    if search_all_models:
        model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
    
    # Cross-validation parameters
    cv_folds = hpo_config_section.setdefault('cv_folds', cleaned_params.get('cv_folds', 3))
    cv_shuffle = hpo_config_section.setdefault('cv_shuffle', cleaned_params.get('cv_shuffle', True))
    cv_random_state = hpo_config_section.setdefault('cv_random_state', cleaned_params.get('cv_random_state', 42))
    
    # Training parameters for trials
    trial_epochs = hpo_config_section.setdefault('trial_epochs', cleaned_params.get('trial_epochs', 20))
    trial_patience = hpo_config_section.setdefault('trial_patience', cleaned_params.get('trial_patience', 5))
    trial_batch_size = hpo_config_section.setdefault('trial_batch_size', cleaned_params.get('trial_batch_size', 64))
    
    # System parameters
    device = hpo_config_section.setdefault('device', cleaned_params.get('device', 'auto'))
    random_seed = hpo_config_section.setdefault('random_seed', cleaned_params.get('random_seed', 42))
    num_workers = hpo_config_section.setdefault('num_workers', cleaned_params.get('num_workers', 0))
    
    # Output parameters
    verbose = hpo_config_section.setdefault('verbose', cleaned_params.get('verbose', True))
    save_study = hpo_config_section.setdefault('save_study', cleaned_params.get('save_study', True))
    study_dir = hpo_config_section.setdefault('study_dir', cleaned_params.get('study_dir', DEFAULT_MODEL_DIR / "hpo_studies"))
    generate_plots = hpo_config_section.setdefault('generate_plots', cleaned_params.get('generate_plots', True))
    
    # Storage parameters
    storage_url = hpo_config_section.setdefault('storage_url', cleaned_params.get('storage_url', None))
    load_if_exists = hpo_config_section.setdefault('load_if_exists', cleaned_params.get('load_if_exists', False))
    
    # Early stopping
    early_stopping_patience = hpo_config_section.setdefault('early_stopping_patience', cleaned_params.get('early_stopping_patience', 10))
    early_stopping_min_trials = hpo_config_section.setdefault('early_stopping_min_trials', cleaned_params.get('early_stopping_min_trials', 20))
    
    # Advanced parameters
    parallel_jobs = hpo_config_section.setdefault('parallel_jobs', cleaned_params.get('parallel_jobs', 1))
    memory_limit = hpo_config_section.setdefault('memory_limit', cleaned_params.get('memory_limit', None))

    # Set up logging
    if verbose:
        original_level = logger.level
        logger.setLevel(logging.INFO)
    
    # Progress tracking data
    progress_data = {
        'current_stage': 'Starting...',
        'trials_completed': 0,
        'trials_failed': 0,
        'trials_pruned': 0,
        'best_value': float('inf'),
        'current_trial': None,
        'current_model_type': None
    }
    
    # Initialize results tracking
    hpo_results = {
        'start_time': start_time.isoformat(),
        'study_name': study_name,
        'configuration': hpo_config_section,
        'trials_completed': 0,
        'best_value': float('inf'),
        'best_params': {},
        'best_config': {},
        'optimization_history': [],
        'model_performance': {},
        'errors': [],
        'warnings': []
    }
    
    try:
        # Display HPO header
        if verbose:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION LAUNCH")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            print(Fore.YELLOW + Style.BRIGHT + "Optimization Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Study Name: " + Fore.YELLOW + Style.BRIGHT + f"{study_name}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Trials: " + Fore.YELLOW + Style.BRIGHT + f"{n_trials}")
            
            if timeout_seconds > 0:
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{timeout_seconds}s")
            else:
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"None")
            
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Types: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(model_types)}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ CV Folds: " + Fore.YELLOW + Style.BRIGHT + f"{cv_folds}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{sampler_type}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{pruner_type}")
            
            print(Fore.CYAN + Style.BRIGHT + "\n  Training Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Trial Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{trial_epochs}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Trial Patience: " + Fore.YELLOW + Style.BRIGHT + f"{trial_patience}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Batch Size Range: " + Fore.YELLOW + Style.BRIGHT + f"[32, 64, 128, 256]")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Learning Rate Range: " + Fore.YELLOW + Style.BRIGHT + f"[1e-4, 1e-2]")
            print(Fore.GREEN + Style.BRIGHT + f"    └─ Device: " + Fore.YELLOW + Style.BRIGHT + f"{device}")
            
            print(Fore.CYAN + Style.BRIGHT + "\n  System Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Random Seed: " + Fore.YELLOW + Style.BRIGHT + f"{random_seed}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Num Workers: " + Fore.YELLOW + Style.BRIGHT + f"{num_workers}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Parallel Jobs: " + Fore.YELLOW + Style.BRIGHT + f"{parallel_jobs}")
            print(Fore.GREEN + Style.BRIGHT + f"    └─ Memory Limit: " + Fore.YELLOW + Style.BRIGHT + f"{memory_limit or 'Auto'}")
            
            print(Fore.CYAN + Style.BRIGHT + "\n  Output Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Save Study: " + Fore.YELLOW + Style.BRIGHT + f"{save_study}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Generate Plots: " + Fore.YELLOW + Style.BRIGHT + f"{generate_plots}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Study Directory: " + Fore.YELLOW + Style.BRIGHT + f"{study_dir}")
            print(Fore.GREEN + Style.BRIGHT + f"    └─ Storage: " + Fore.YELLOW + Style.BRIGHT + f"{storage_url or 'In-Memory'}")
            
            print(Fore.MAGENTA + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.GREEN + Style.BRIGHT + "  Starting hyperparameter optimization process...")
            print(Fore.MAGENTA + Style.BRIGHT + "-"*40 + Style.RESET_ALL)
        
        # Calculate total stages for progress tracking
        total_stages = 6  # Preparation, Data, Setup, Optimization, Analysis, Finalization
        
        # STAGE 1: Preparation
        progress_data['current_stage'] = "Preparation"
        
        with alive_bar(1, title='Preparation\t\t', bar='smooth', spinner='dots') as preparation_bar:
            preparation_bar.text("Initializing hyperparameter search...")
            
            # Memory optimization at start
            _optimize_memory_if_needed(
                condition=True,
                hardware_data=None,
                aggressive=False,
                silent=not verbose
            )
            
            preparation_bar.text("Preparation complete")
            preparation_bar()
        
        # STAGE 2: Data Preparation
        progress_data['current_stage'] = "Data Preparation"
        
        with alive_bar(1, title='Data Preparation\t', bar='smooth', spinner='dots') as data_preparation_bar:
            data_preparation_bar.text("Preparing data...")
            
            # Prepare data
            if data is None:
                if X_train is not None:
                    # Create data dictionary from provided arrays
                    data = {
                        'X_train': X_train,
                        'X_val': X_val if X_val is not None else None,
                        'X_test': X_test if X_test is not None else None,
                        'feature_names': [f'feature_{i}' for i in range(X_train.shape[1])]
                    }
                else:
                    # Generate or load data based on configuration
                    if use_real_data:
                        try:
                            with alive_bar(1, title='Loading Real Data\t', bar='smooth', spinner='dots', length=20) as data_bar:
                                data_bar.text("Loading real dataset...")
                                data = load_and_validate_data(
                                    data_path=data_path,
                                    artifacts_path=artifacts_path,
                                    config=final_config
                                )
                                data_bar.text("Real data loaded")
                                data_bar()
                            if verbose:
                                logger.info("Loaded real data for HPO")
                        except Exception as e:
                            if verbose:
                                logger.warning(f"Failed to load real data: {e}, falling back to synthetic data")
                            with alive_bar(1, title='Generating Synthetic Data', bar='smooth', spinner='dots', length=20) as data_bar:
                                data_bar.text("Generating synthetic data...")
                                data = generate_synthetic_data(
                                    normal_samples=10000,
                                    attack_samples=2000,
                                    features=78,
                                    validation_split=0.2,
                                    random_state=random_seed,
                                    config=final_config
                                )
                                data_bar.text("Synthetic data generated")
                                data_bar()
                    else:
                        with alive_bar(1, title='Generating Synthetic Data', bar='smooth', spinner='dots', length=20) as data_bar:
                            data_bar.text("Generating synthetic data...")
                            data = generate_synthetic_data(
                                normal_samples=10000,
                                attack_samples=2000,
                                features=78,
                                validation_split=0.2,
                                random_state=random_seed,
                                config=final_config
                            )
                            data_bar.text("Synthetic data generated")
                            data_bar()
            
            # Validate data
            if not isinstance(data, dict) or 'X_train' not in data:
                raise ValueError("Invalid data format. Expected dictionary with 'X_train' key.")
            
            X_train = data['X_train']
            X_val = data.get('X_val')
            X_test = data.get('X_test')
            feature_names = data.get('feature_names', [f'feature_{i}' for i in range(X_train.shape[1])])
            input_dim = X_train.shape[1]
            
            # Update data info in results
            hpo_results['data_info'] = {
                'train_samples': len(X_train),
                'val_samples': len(X_val) if X_val is not None else 0,
                'test_samples': len(X_test) if X_test is not None else 0,
                'features': input_dim,
                'cv_folds': cv_folds
            }
            
            data_preparation_bar.text(f"Data: {X_train.shape[0]} samples, {input_dim} features")
            data_preparation_bar()
        
        # STAGE 3: Study Setup
        progress_data['current_stage'] = "Study Setup"
        
        with alive_bar(4, title='Study Setup\t\t', bar='smooth', spinner='dots') as study_setup_bar:
            study_setup_bar.text("Setting up cross-validation...")
            
            # Set up cross-validation
            if X_val is None:
                # Handle case when cv_folds=1 by using a simple train/validation split
                if cv_folds == 1:
                    # Use a single 80/20 train-validation split
                    from sklearn.model_selection import train_test_split
                    train_idx, val_idx = train_test_split(
                        np.arange(len(X_train)), 
                        test_size=0.2, 
                        random_state=cv_random_state,
                        shuffle=cv_shuffle
                    )
                    cv_splits = [(train_idx, val_idx)]
                else:
                    # Use k-fold cross-validation for cv_folds >= 2
                    kf = KFold(n_splits=cv_folds, shuffle=cv_shuffle, random_state=cv_random_state)
                    cv_splits = list(kf.split(X_train))
            else:
                # Use fixed train/validation split
                cv_splits = [(np.arange(len(X_train)), np.arange(len(X_val)))]
                cv_folds = 1
                if verbose:
                    logger.info("Using provided train/validation split")
            
            study_setup_bar.text("Creating study directory...")
            study_setup_bar()
            
            # Create study directory
            study_dir = Path(study_dir)
            study_dir.mkdir(parents=True, exist_ok=True)
            
            study_setup_bar.text("Setting up sampler...")
            study_setup_bar()
            
            # Set up Optuna sampler
            if sampler_type == 'TPE':
                sampler = TPESampler(
                    seed=random_seed,
                    consider_prior=True,
                    consider_magic_clip=True,
                    consider_endpoints=False,
                    n_startup_trials=min(10, n_trials // 10),
                    multivariate=True
                )
            elif sampler_type == 'Random':
                sampler = RandomSampler(seed=random_seed)
            elif sampler_type == 'CmaEs':
                sampler = CmaEsSampler(seed=random_seed)
            else:
                if verbose:
                    logger.warning(f"Unknown sampler type '{sampler_type}', using TPE")
                sampler = TPESampler(seed=random_seed)
            
            study_setup_bar.text("Setting up pruner...")
            study_setup_bar()
            
            # Set up Optuna pruner
            if pruner_type == 'MedianPruner':
                pruner = MedianPruner(
                    n_startup_trials=5,
                    n_warmup_steps=5,
                    interval_steps=1
                )
            elif pruner_type == 'HyperbandPruner':
                pruner = HyperbandPruner(
                    min_resource=1,
                    max_resource=trial_epochs,
                    reduction_factor=3
                )
            elif pruner_type == 'NopPruner' or pruner_type == 'None':
                pruner = NopPruner()
            else:
                if verbose:
                    logger.warning(f"Unknown pruner type '{pruner_type}', using MedianPruner")
                pruner = MedianPruner()
            
            study_setup_bar.text("Creating study...")
            
            # Set up storage if specified
            storage = None
            if storage_url:
                try:
                    storage = optuna.storages.RDBStorage(
                        url=storage_url,
                        heartbeat_interval=60,
                        grace_period=120
                    )
                except Exception as e:
                    if verbose:
                        logger.warning(f"Failed to setup storage: {e}")
            
            # Create study
            study = optuna.create_study(
                direction=direction,
                sampler=sampler,
                pruner=pruner,
                study_name=study_name,
                storage=storage,
                load_if_exists=load_if_exists
            )
            
            study_setup_bar.text("Study setup complete")
            study_setup_bar()
        
        # Define search space based on model types
        def get_search_space_for_model(trial, model_type, input_dim):
            """Define search space for specific model type"""
            
            if model_type == 'SimpleAutoencoder':
                # SimpleAutoencoder search space - FIXED: Ensure min <= max
                min_encoding_dim = max(4, min(8, input_dim // 8))  # More conservative minimum
                max_encoding_dim = max(min_encoding_dim + 4, min(64, input_dim // 2))  # Ensure max > min
                
                encoding_dim = trial.suggest_int('encoding_dim', min_encoding_dim, max_encoding_dim)
                
                # Hidden layer configurations
                hidden_choice = trial.suggest_categorical('hidden_choice', [
                    'single_small', 'single_medium', 'single_large', 'double_small', 'double_medium'
                ])
                
                if hidden_choice == 'single_small':
                    hidden_dims = [max(16, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif hidden_choice == 'single_medium':
                    hidden_dims = [max(32, encoding_dim * 4)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif hidden_choice == 'single_large':
                    hidden_dims = [max(64, encoding_dim * 8)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif hidden_choice == 'double_small':
                    hidden_dims = [max(32, encoding_dim * 4), max(16, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.1, 0.4),
                        trial.suggest_float('dropout_1', 0.05, 0.3)
                    ]
                else:  # double_medium
                    hidden_dims = [max(64, encoding_dim * 8), max(32, encoding_dim * 4)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.15, 0.4),
                        trial.suggest_float('dropout_1', 0.1, 0.3)
                    ]
                
                return {
                    'model_type': 'SimpleAutoencoder',
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'activation': trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'gelu', 'tanh']),
                    'normalization': trial.suggest_categorical('normalization', [None, 'batch']),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'weight_init': trial.suggest_categorical('weight_init', ['xavier_uniform', 'xavier_normal', 'kaiming_uniform']),
                    'bias': trial.suggest_categorical('bias', [True, False]),
                    'mixed_precision': trial.suggest_categorical('mixed_precision', [False, True])
                }
            
            elif model_type == 'EnhancedAutoencoder':
                # EnhancedAutoencoder search space - FIXED: Ensure min <= max
                min_encoding_dim = max(4, min(8, input_dim // 8))  # More conservative minimum
                max_encoding_dim = max(min_encoding_dim + 4, min(64, input_dim // 2))  # Ensure max > min
                
                encoding_dim = trial.suggest_int('encoding_dim', min_encoding_dim, max_encoding_dim)
                
                # Architecture choices
                arch_choice = trial.suggest_categorical('arch_choice', [
                    'compact', 'balanced', 'deep', 'wide'
                ])
                
                if arch_choice == 'compact':
                    hidden_dims = [max(32, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.3)]
                elif arch_choice == 'balanced':
                    hidden_dims = [max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.15, 0.35),
                        trial.suggest_float('dropout_1', 0.1, 0.25)
                    ]
                elif arch_choice == 'deep':
                    hidden_dims = [max(128, encoding_dim * 8), max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.2, 0.4),
                        trial.suggest_float('dropout_1', 0.15, 0.35),
                        trial.suggest_float('dropout_2', 0.1, 0.3)
                    ]
                else:  # wide
                    hidden_dims = [max(256, encoding_dim * 16), max(128, encoding_dim * 8)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.25, 0.45),
                        trial.suggest_float('dropout_1', 0.2, 0.4)
                    ]
                
                return {
                    'model_type': 'EnhancedAutoencoder',
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'activation': trial.suggest_categorical('activation', ['leaky_relu', 'gelu', 'relu', 'swish']),
                    'normalization': trial.suggest_categorical('normalization', ['batch', 'layer', 'instance']),
                    'use_attention': trial.suggest_categorical('use_attention', [False, True]),
                    'residual_blocks': trial.suggest_categorical('residual_blocks', [False, True]),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'weight_init': trial.suggest_categorical('weight_init', ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal']),
                    'bias': trial.suggest_categorical('bias', [True, False]),
                    'legacy_mode': trial.suggest_categorical('legacy_mode', [False, True]),
                    'mixed_precision': trial.suggest_categorical('mixed_precision', [False, True])
                }
            
            elif model_type == 'AutoencoderEnsemble':
                # AutoencoderEnsemble search space - FIXED: Ensure min <= max
                min_encoding_dim = max(4, min(8, input_dim // 8))  # More conservative minimum
                max_encoding_dim = max(min_encoding_dim + 8, min(48, input_dim // 3))  # Ensure max > min
                
                encoding_dim = trial.suggest_int('encoding_dim', min_encoding_dim, max_encoding_dim, step=8)
                num_models = trial.suggest_int('num_models', 3, 7, step=2)
                diversity_factor = trial.suggest_float('diversity_factor', 0.1, 0.5)
                
                # Ensemble architecture
                ensemble_arch = trial.suggest_categorical('ensemble_arch', ['small', 'medium', 'large'])
                
                if ensemble_arch == 'small':
                    hidden_dims = [max(32, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.15, 0.35)]
                elif ensemble_arch == 'medium':
                    hidden_dims = [max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.2, 0.4),
                        trial.suggest_float('dropout_1', 0.15, 0.35)
                    ]
                else:  # large
                    hidden_dims = [max(128, encoding_dim * 8), max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.25, 0.45),
                        trial.suggest_float('dropout_1', 0.2, 0.4),
                        trial.suggest_float('dropout_2', 0.15, 0.35)
                    ]
                
                return {
                    'model_type': 'AutoencoderEnsemble',
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'num_models': num_models,
                    'diversity_factor': diversity_factor,
                    'activation': trial.suggest_categorical('activation', ['leaky_relu', 'gelu', 'relu']),
                    'normalization': trial.suggest_categorical('normalization', ['batch', 'layer']),
                    'use_attention': trial.suggest_categorical('use_attention', [False, True]),
                    'residual_blocks': trial.suggest_categorical('residual_blocks', [False, True]),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'weight_init': trial.suggest_categorical('weight_init', ['xavier_uniform', 'kaiming_uniform']),
                    'bias': trial.suggest_categorical('bias', [True, False]),
                    'min_features': trial.suggest_int('min_features', 3, 10),
                    'mixed_precision': trial.suggest_categorical('mixed_precision', [False, True])
                }
            
            else:
                raise ValueError(f"Unknown model type: {model_type}")
        
        # Define objective function with nested progress for cross-validation
        def objective(trial):
            try:
                # Select model type
                model_type = trial.suggest_categorical('model_type', model_types)
                
                # Update progress tracking
                progress_data['current_trial'] = trial.number
                progress_data['current_model_type'] = model_type
                
                # Get model-specific parameters
                model_params = get_search_space_for_model(trial, model_type, input_dim)
                
                # Training parameters
                learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
                batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])
                weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)
                optimizer_type = trial.suggest_categorical('optimizer_type', ['Adam', 'AdamW', 'RMSprop'])
                scheduler_type = trial.suggest_categorical('scheduler_type', [None, 'ReduceLROnPlateau', 'StepLR', 'CosineAnnealingLR'])
                
                # Advanced training parameters
                gradient_clip = trial.suggest_categorical('gradient_clip', [None, 0.5, 1.0, 2.0])
                gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])
                
                # Scheduler parameters
                scheduler_params = {}
                if scheduler_type == 'ReduceLROnPlateau':
                    scheduler_params = {
                        'patience': trial.suggest_int('lr_patience', 3, 8),
                        'factor': trial.suggest_float('lr_factor', 0.3, 0.7),
                        'min_lr': trial.suggest_float('min_lr', 1e-8, 1e-5, log=True)
                    }
                elif scheduler_type == 'StepLR':
                    scheduler_params = {
                        'step_size': trial.suggest_int('step_size', 10, 30),
                        'gamma': trial.suggest_float('gamma', 0.1, 0.5)
                    }
                elif scheduler_type == 'CosineAnnealingLR':
                    scheduler_params = {
                        'T_max': trial_epochs,
                        'eta_min': trial.suggest_float('eta_min', 1e-8, 1e-5, log=True)
                    }
                
                # Build comprehensive configuration for train_model
                trial_config = {
                    'model': {
                        'model_type': model_params['model_type'],
                        'input_dim': input_dim,
                        'encoding_dim': model_params['encoding_dim'],
                        'hidden_dims': model_params['hidden_dims'],
                        'dropout_rates': model_params['dropout_rates'],
                        'activation': model_params['activation'],
                        'normalization': model_params['normalization'],
                        'skip_connection': model_params.get('skip_connection', True),
                        'residual_blocks': model_params.get('residual_blocks', True),
                        'use_attention': model_params.get('use_attention', True),
                        'legacy_mode': model_params.get('legacy_mode', False),
                        'num_models': model_params.get('num_models'),
                        'diversity_factor': model_params.get('diversity_factor'),
                        'weight_init': model_params.get('weight_init', 'xavier_uniform'),
                        'bias': model_params.get('bias', True),
                        'min_features': model_params.get('min_features', 5)
                    },
                    
                    'training': {
                        'batch_size': batch_size,
                        'epochs': trial_epochs,
                        'learning_rate': learning_rate,
                        'patience': trial_patience,
                        'weight_decay': weight_decay,
                        'gradient_clip': gradient_clip,
                        'gradient_accumulation_steps': gradient_accumulation_steps,
                        'mixed_precision': model_params.get('mixed_precision', False),
                        'optimizer_type': optimizer_type,
                        'scheduler_type': scheduler_type,
                        'scheduler_params': scheduler_params,
                        'early_stopping': True,
                        'validation_split': 0.2
                    },
                    
                    'data': {
                        'use_real_data': True,
                        'features': input_dim,
                        'data_preprocessing': True,
                        'normalization_method': 'standard'
                    },
                    
                    'system': {
                        'device': device,
                        'random_seed': random_seed,
                        'reproducible': True,
                        'model_dir': study_dir / f"trial_{trial.number}",
                        'log_dir': study_dir / f"trial_{trial.number}" / "logs"
                    },
                    
                    'monitoring': {
                        'verbose': False,
                        'debug_mode': False,
                        'tensorboard_logging': False,
                        'save_checkpoints': False,
                        'progress_bar': False,
                        'log_frequency': 999999
                    },
                    
                    'export': {
                        'export_onnx': False,
                        'save_model': False,
                        'save_metadata': False,
                        'save_training_history': False
                    },
                    
                    'advanced_training': {
                        'num_workers': num_workers,
                        'pin_memory': False,
                        'persistent_workers': False,
                        'memory_efficient': True
                    },
                    
                    'error_handling': {
                        'error_handling': 'continue',
                        'graceful_degradation': True,
                        'continue_on_error': True
                    }
                }
                
                # Perform cross-validation with nested progress
                fold_scores = []
                
                # Create fold progress bar
                with alive_bar(len(cv_splits), title=f'Trial {trial.number} CV Folds', bar='smooth', spinner='dots', length=20) as fold_bar:
                    for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
                        try:
                            fold_bar.text(f"Fold {fold_idx + 1}/{len(cv_splits)}")
                            
                            # Prepare fold data
                            if X_val is None:
                                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
                            else:
                                X_fold_train, X_fold_val = X_train, X_val
                            
                            # Create fold data dictionary
                            fold_data = {
                                'X_train': X_fold_train,
                                'X_val': X_fold_val,
                                'X_test': X_fold_val,
                                'y_train': np.zeros(len(X_fold_train)),
                                'y_val': np.zeros(len(X_fold_val)),
                                'y_test': np.zeros(len(X_fold_val)),
                                'feature_names': feature_names
                            }
                            
                            # Use the comprehensive train_model function
                            result = train_model(
                                config={
                                    **trial_config,
                                    'hpo_data': fold_data,
                                    'hpo_fold': fold_idx
                                }
                            )
                            
                            # Extract validation loss
                            if result and result.get('success', False):
                                final_metrics = result.get('final_metrics', {})
                                val_loss = final_metrics.get('best_validation_loss', float('inf'))
                                
                                test_loss = final_metrics.get('test_loss', float('inf'))
                                training_time = result.get('training_time_minutes', 0)
                                
                                fold_scores.append(val_loss)
                                
                                trial.set_user_attr(f'fold_{fold_idx}_val_loss', val_loss)
                                trial.set_user_attr(f'fold_{fold_idx}_test_loss', test_loss)
                                trial.set_user_attr(f'fold_{fold_idx}_training_time', training_time)
                                
                            else:
                                error_msg = result.get('error', 'Unknown error') if result else 'No result returned'
                                if verbose:
                                    logger.warning(f"Trial {trial.number}, Fold {fold_idx} failed: {error_msg}")
                                fold_scores.append(float('inf'))
                            
                            # Update fold progress
                            current_loss = fold_scores[-1] if fold_scores else float('inf')
                            fold_bar.text(f"Fold {fold_idx + 1}/{len(cv_splits)} - Loss: {current_loss:.6f}")
                            fold_bar()
                            
                            # Report intermediate value for pruning
                            if fold_idx == 0:
                                trial.report(fold_scores[0], fold_idx)
                                if trial.should_prune():
                                    raise optuna.TrialPruned()
                        
                        except optuna.TrialPruned:
                            raise
                        except Exception as e:
                            if verbose:
                                logger.warning(f"Trial {trial.number}, Fold {fold_idx} error: {str(e)}")
                            fold_scores.append(float('inf'))
                            fold_bar()
                
                # Calculate final score
                if fold_scores:
                    mean_score = np.mean(fold_scores)
                    std_score = np.std(fold_scores)
                    
                    trial.set_user_attr('mean_cv_score', mean_score)
                    trial.set_user_attr('std_cv_score', std_score)
                    trial.set_user_attr('individual_fold_scores', fold_scores)
                    trial.set_user_attr('model_config', model_params)
                    trial.set_user_attr('training_config', {
                        'learning_rate': learning_rate,
                        'batch_size': batch_size,
                        'weight_decay': weight_decay,
                        'optimizer_type': optimizer_type,
                        'scheduler_type': scheduler_type
                    })
                    trial.set_user_attr('complete_config', trial_config)
                    
                    return mean_score
                else:
                    return float('inf')
            
            except optuna.TrialPruned:
                raise
            except Exception as e:
                if verbose:
                    logger.error(f"Trial {trial.number} failed with error: {str(e)}")
                trial.set_user_attr('error', str(e))
                trial.set_user_attr('failed', True)
                return float('inf')
        
        # Set up callbacks with progress tracking
        callbacks = []
        
        def progress_callback(study, trial):
            if trial.state == optuna.trial.TrialState.COMPLETE:
                progress_data['trials_completed'] += 1
                if trial.value < progress_data['best_value']:
                    progress_data['best_value'] = trial.value
                
                if verbose:
                    print(f"Trial {trial.number:3d} complete | Value: {trial.value:.5f} | Best: {study.best_value:.5f}")
                
                hpo_results['trials_completed'] = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])
                hpo_results['optimization_history'].append({
                    'trial': trial.number,
                    'value': trial.value,
                    'best_value': study.best_value,
                    'params': trial.params
                })
                
                if trial.value < hpo_results['best_value']:
                    hpo_results['best_value'] = trial.value
                    hpo_results['best_params'] = trial.params.copy()
                    if 'complete_config' in trial.user_attrs:
                        hpo_results['best_config'] = trial.user_attrs['complete_config'].copy()
            
            elif trial.state == optuna.trial.TrialState.PRUNED:
                progress_data['trials_pruned'] += 1
                if verbose:
                    print(f"Trial {trial.number:3d} pruned")
            elif trial.state == optuna.trial.TrialState.FAIL:
                progress_data['trials_failed'] += 1
                if verbose:
                    print(f"Trial {trial.number:3d} failed")
                hpo_results['errors'].append(f"Trial {trial.number} failed")
        
        callbacks.append(progress_callback)
        
        # Early stopping callback
        def early_stopping_callback(study, trial):
            if len(study.trials) >= early_stopping_min_trials:
                recent_trials = study.trials[-early_stopping_patience:]
                recent_values = [t.value for t in recent_trials if t.state == optuna.trial.TrialState.COMPLETE]
                
                if len(recent_values) >= early_stopping_patience:
                    if min(recent_values) >= study.best_value:
                        if verbose:
                            print(f"Early stopping triggered after {len(study.trials)} trials")
                        study.stop()
        
        callbacks.append(early_stopping_callback)
        
        # STAGE 4: Optimization with alive-progress
        progress_data['current_stage'] = "Optimization"
        
        if verbose:
            print("Starting optimization...")
        
        # Run optimization with alive-progress bar
        with alive_bar(n_trials, title='Running Trials\t\t', bar='smooth', spinner='dots', stats=True, elapsed=True) as trial_bar:
            
            # Custom optimization loop to update progress bar
            def optimize_with_progress():
                for i in range(n_trials):
                    if study._stop_flag:
                        break
                    
                    trial = study.ask()
                    value = objective(trial)
                    study.tell(trial, value)
                    
                    # Update progress bar
                    completed = progress_data['trials_completed']
                    pruned = progress_data['trials_pruned']
                    failed = progress_data['trials_failed']
                    best_val = progress_data['best_value']
                    current_model = progress_data.get('current_model_type', 'N/A')
                    
                    trial_bar.text(f"Completed: {completed} | Pruned: {pruned} | Failed: {failed} | Best: {best_val:.5f} | Model: {current_model}")
                    trial_bar()
                    
                    # Memory optimization between trials
                    if i % 5 == 0:  # Every 5 trials
                        _optimize_memory_if_needed(
                            condition=True,
                            hardware_data=None,
                            aggressive=False,
                            silent=not verbose
                        )
            
            # Run the optimization
            if timeout_seconds > 0:
                # Run with timeout
                stop_event = threading.Event()
                
                def run_optimization():
                    optimize_with_progress()
                    stop_event.set()
                
                optimization_thread = threading.Thread(target=run_optimization)
                optimization_thread.start()
                optimization_thread.join(timeout=timeout_seconds)
                
                if optimization_thread.is_alive():
                    study.stop()
                    if verbose:
                        print(f"Optimization timeout after {timeout_seconds} seconds")
            else:
                # Run without timeout
                optimize_with_progress()
            
            trial_bar.text(f"Optimization complete | Best: {progress_data['best_value']:.5f}")
        
        # STAGE 5: Analysis
        progress_data['current_stage'] = "Analysis"
        
        with alive_bar(1, title='Analysis\t\t\t', bar='smooth', spinner='dots') as results_analysis_bar:
            results_analysis_bar.text("Analyzing results...")
            
            # Calculate total time
            total_time = time.time() - hpo_start_time
            
            # Finalize results
            hpo_results.update({
                'end_time': datetime.now().isoformat(),
                'total_time_seconds': total_time,
                'total_time_minutes': total_time / 60,
                'study': study,
                'n_trials_completed': progress_data['trials_completed'],
                'n_trials_pruned': progress_data['trials_pruned'],
                'n_trials_failed': progress_data['trials_failed']
            })
            
            if study.trials:
                best_trial = study.best_trial
                hpo_results.update({
                    'best_trial_number': best_trial.number,
                    'best_value': best_trial.value,
                    'best_params': best_trial.params,
                    'best_user_attrs': best_trial.user_attrs
                })
                
                if 'complete_config' in best_trial.user_attrs:
                    hpo_results['best_config'] = best_trial.user_attrs['complete_config']
            
            results_analysis_bar.text("Analysis complete")
            results_analysis_bar()
        
        # STAGE 6: Finalization
        progress_data['current_stage'] = "Finalization"
        
        with alive_bar(1, title='Finalization\t\t', bar='smooth', spinner='dots') as finalization_bar:
            finalization_bar.text("Finalizing hyperparameter search...")
            
            # Display results
            if verbose:
                print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
                print(Fore.MAGENTA + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION COMPLETED")
                print(Fore.CYAN + Style.BRIGHT + "-"*40)
                
                print(Fore.YELLOW + Style.BRIGHT + "Performance Summary:")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Time: " + Fore.YELLOW + Style.BRIGHT + f"{total_time/60:.1f} minutes")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials Completed: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_results['n_trials_completed']}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials Pruned: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_results['n_trials_pruned']}")
                print(Fore.GREEN + Style.BRIGHT + f"  └─ Trials Failed: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_results['n_trials_failed']}")
                
                # Calculate success rate
                total_trials = hpo_results['n_trials_completed'] + hpo_results['n_trials_pruned'] + hpo_results['n_trials_failed']
                if total_trials > 0:
                    success_rate = (hpo_results['n_trials_completed'] / total_trials) * 100
                    success_color = Fore.GREEN if success_rate > 90 else Fore.YELLOW if success_rate > 75 else Fore.RED
                    print(Fore.CYAN + Style.BRIGHT + f"  ┌─ Success Rate: " + success_color + Style.BRIGHT + f"{success_rate:.1f}%")
                
                if study.trials:
                    print(Fore.MAGENTA + Style.BRIGHT + "\n  Best Trial Results:")
                    print(Fore.GREEN + Style.BRIGHT + f"    ├─ Trial Number: " + Fore.YELLOW + Style.BRIGHT + f"#{best_trial.number}")
                    print(Fore.GREEN + Style.BRIGHT + f"    ├─ Objective Value: " + Fore.YELLOW + Style.BRIGHT + f"{best_trial.value:.6f}")
                    print(Fore.GREEN + Style.BRIGHT + f"    ├─ Model Type: " + Fore.YELLOW + Style.BRIGHT + f"{best_trial.params.get('model_type', 'Unknown')}")
                    print(Fore.GREEN + Style.BRIGHT + f"    ├─ Learning Rate: " + Fore.YELLOW + Style.BRIGHT + f"{best_trial.params.get('learning_rate', 'Unknown'):.6f}")
                    print(Fore.GREEN + Style.BRIGHT + f"    ├─ Batch Size: " + Fore.YELLOW + Style.BRIGHT + f"{best_trial.params.get('batch_size', 'Unknown')}")
                    print(Fore.GREEN + Style.BRIGHT + f"    └─ Encoding Dim: " + Fore.YELLOW + Style.BRIGHT + f"{best_trial.params.get('encoding_dim', 'Unknown')}")
                    
                    if 'mean_cv_score' in best_trial.user_attrs:
                        mean_cv = best_trial.user_attrs['mean_cv_score']
                        std_cv = best_trial.user_attrs.get('std_cv_score', 0)
                        consistency_color = Fore.GREEN if std_cv < 0.01 else Fore.YELLOW if std_cv < 0.05 else Fore.RED
                        print(Fore.CYAN + Style.BRIGHT + f"\n    Cross-Validation Performance:")
                        print(Fore.GREEN + Style.BRIGHT + f"      ├─ Mean CV Score: " + Fore.YELLOW + Style.BRIGHT + f"{mean_cv:.6f}")
                        print(Fore.GREEN + Style.BRIGHT + f"      └─ Std CV Score: " + consistency_color + Style.BRIGHT + f"±{std_cv:.6f}")
            
            # Save study and results with alive-progress
            if save_study:
                try:
                    with alive_bar(2, title='Saving Study Data\t', bar='smooth', spinner='dots', length=20) as save_bar:
                        save_bar.text("Saving study object...")
                        
                        # Save study object
                        study_path = study_dir / f"{study_name}_study.pkl"
                        joblib.dump(study, study_path)
                        
                        save_bar.text("Study object saved")
                        save_bar()
                        
                        save_bar.text("Saving results JSON...")
                        
                        # Save results
                        results_path = study_dir / f"{study_name}_results.json"
                        with open(results_path, 'w') as f:
                            json_results = hpo_results.copy()
                            json_results.pop('study', None)
                            json.dump(json_results, f, indent=2, default=str)
                        
                        save_bar.text("Results JSON saved")
                        save_bar()
                    
                    hpo_results['study_path'] = str(study_path)
                    hpo_results['results_path'] = str(results_path)
                    
                    if verbose:
                        print(Fore.GREEN + Style.BRIGHT + f"    ├─ Study Object: " + Fore.YELLOW + Style.BRIGHT + f"{study_path}")
                        print(Fore.GREEN + Style.BRIGHT + f"    └─ Results JSON: " + Fore.YELLOW + Style.BRIGHT + f"{results_path}")
                        print(Fore.GREEN + Style.BRIGHT + "    ┌─ " + Fore.CYAN + Style.BRIGHT + "Study data saved successfully!")
                
                except Exception as e:
                    if verbose:
                        print(Fore.RED + Style.BRIGHT + f"    └─ Failed to save study: {e}")
                    hpo_results['warnings'].append(f"Failed to save study: {e}")
            
            # Generate plots with alive-progress
            if generate_plots and study.trials:
                try:
                    plot_dir = study_dir / "plots"
                    plot_dir.mkdir(exist_ok=True)
                    
                    if verbose:
                        print(Fore.CYAN + Style.BRIGHT + "\n  Generating Optimization Plots:")
                        plot_types = ['optimization_history']
                        if len(study.trials) > 10:
                            plot_types.append('param_importances')
                        if len(study.trials) > 5:
                            plot_types.append('parallel_coordinate')
                    
                    with alive_bar(len(plot_types), title='Creating Plots\t\t', bar='smooth', spinner='dots', length=20) as plot_bar:
                        plot_bar.text("Creating optimization history plot...")
                        
                        # Optimization history
                        fig = vis.plot_optimization_history(study)
                        plot_path = plot_dir / "optimization_history.html"
                        fig.write_html(plot_path)
                        
                        plot_bar.text("Optimization history plot created")
                        plot_bar()
                        
                        # Parameter importances
                        if len(study.trials) > 10:
                            plot_bar.text("Creating parameter importances plot...")
                            
                            fig = vis.plot_param_importances(study)
                            plot_path = plot_dir / "param_importances.html"
                            fig.write_html(plot_path)
                            
                            plot_bar.text("Parameter importances plot created")
                            plot_bar()
                        
                        # Parallel coordinate plot
                        if len(study.trials) > 5:
                            plot_bar.text("Creating parallel coordinate plot...")
                            
                            fig = vis.plot_parallel_coordinate(study)
                            plot_path = plot_dir / "parallel_coordinate.html"
                            fig.write_html(plot_path)
                            
                            plot_bar.text("Parallel coordinate plot created")
                            plot_bar()
                    
                    hpo_results['plots_dir'] = str(plot_dir)
                    
                    if verbose:
                        print(Fore.GREEN + Style.BRIGHT + f"    ├─ Plots Directory: " + Fore.YELLOW + Style.BRIGHT + f"{plot_dir}")
                        print(Fore.GREEN + Style.BRIGHT + f"    └─ Generated Plots: " + Fore.YELLOW + Style.BRIGHT + f"{len(plot_types)} visualizations")
                        print(Fore.GREEN + Style.BRIGHT + "    ┌─ " + Fore.CYAN + Style.BRIGHT + "Plots generated successfully!")
                
                except Exception as e:
                    if verbose:
                        print(Fore.RED + Style.BRIGHT + f"    └─ Failed to generate plots: {e}")
                    hpo_results['warnings'].append(f"Failed to generate plots: {e}")
            
            # Calculate and display statistics
            if study.trials:
                with alive_bar(1, title='Analyzing Statistics\t', bar='smooth', spinner='dots', length=20) as stats_bar:
                    stats_bar.text("Calculating performance statistics...")
                    
                    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
                    if completed_trials:
                        values = [t.value for t in completed_trials]
                        best_value = min(values)
                        worst_value = max(values)
                        mean_value = np.mean(values)
                        median_value = np.median(values)
                        std_value = np.std(values)
                        improvement_ratio = (worst_value - best_value) / worst_value if worst_value > 0 else 0
                        
                        hpo_results['summary_statistics'] = {
                            'best_value': best_value,
                            'worst_value': worst_value,
                            'mean_value': mean_value,
                            'median_value': median_value,
                            'std_value': std_value,
                            'improvement_ratio': improvement_ratio
                        }
                        
                        if verbose:
                            stats_bar.text("Statistics calculated")
                            stats_bar()
                            
                            print(Fore.MAGENTA + Style.BRIGHT + "\n  Statistical Summary:")
                            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Best Value: " + Fore.YELLOW + Style.BRIGHT + f"{best_value:.6f}")
                            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Worst Value: " + Fore.YELLOW + Style.BRIGHT + f"{worst_value:.6f}")
                            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Mean Value: " + Fore.YELLOW + Style.BRIGHT + f"{mean_value:.6f}")
                            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Median Value: " + Fore.YELLOW + Style.BRIGHT + f"{median_value:.6f}")
                            
                            consistency_color = Fore.GREEN if std_value < 0.01 else Fore.YELLOW if std_value < 0.05 else Fore.RED
                            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Std Deviation: " + consistency_color + Style.BRIGHT + f"±{std_value:.6f}")
                            
                            improvement_color = Fore.GREEN if improvement_ratio > 0.5 else Fore.YELLOW if improvement_ratio > 0.2 else Fore.BLUE
                            print(Fore.GREEN + Style.BRIGHT + f"    └─ Improvement Ratio: " + improvement_color + Style.BRIGHT + f"{improvement_ratio:.1%}")
                    else:
                        stats_bar.text("No completed trials for statistics")
                        stats_bar()
            
            # Final memory optimization
            _optimize_memory_if_needed(
                condition=True,
                hardware_data=None,
                aggressive=True,
                silent=not verbose
            )
            
            finalization_bar.text("Hyperparameter search complete!")
            finalization_bar()
        
        # Final completion message
        if verbose:
            print(Fore.MAGENTA + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.GREEN + Style.BRIGHT + " HYPERPARAMETER SEARCH COMPLETE")
            print(Fore.MAGENTA + Style.BRIGHT + "-"*40)
            print(Fore.CYAN + Style.BRIGHT + "  Next Steps:")
            print(Fore.WHITE + Style.BRIGHT + "    ├─ Review optimization results and plots")
            print(Fore.WHITE + Style.BRIGHT + "    ├─ Use best parameters for production training")
            print(Fore.WHITE + Style.BRIGHT + "    ├─ Consider fine-tuning with narrowed search space")
            print(Fore.WHITE + Style.BRIGHT + "    └─ Validate best model on separate test set")
            print(Fore.MAGENTA + Style.BRIGHT + "-"*40 + Style.RESET_ALL)
        
        return hpo_results
    
    except Exception as e:
        error_msg = f"Hyperparameter optimization failed: {str(e)}"
        if verbose:
            logger.error(error_msg)
            logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Create error results
        error_results = {
            'success': False,
            'error': error_msg,
            'error_type': type(e).__name__,
            'start_time': start_time.isoformat(),
            'end_time': datetime.now().isoformat(),
            'total_time_seconds': time.time() - hpo_start_time,
            'study_name': study_name,
            'configuration': hpo_config_section,
            'trials_completed': progress_data.get('trials_completed', 0),
            'traceback': traceback.format_exc()
        }
        
        # Save error information
        if save_study:
            try:
                error_path = study_dir / f"{study_name}_error.json"
                with open(error_path, 'w') as f:
                    json.dump(error_results, f, indent=2, default=str)
                error_results['error_log_path'] = str(error_path)
            except Exception as save_error:
                if verbose:
                    logger.warning(f"Failed to save error log: {save_error}")
        
        return error_results
    
    finally:
        # Restore logging level
        if verbose and 'original_level' in locals():
            try:
                logger.setLevel(original_level)
            except Exception:
                pass
        
        # Final cleanup
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        except Exception:
            pass

def setup_hyperparameter_optimization(
    # Core HPO Parameters
    n_trials: Optional[int] = None,
    timeout_seconds: Optional[float] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    
    # Data Parameters
    use_real_data: Optional[bool] = None,
    data_path: Optional[Union[str, Path]] = None,
    artifacts_path: Optional[Union[str, Path]] = None,
    normal_samples: Optional[int] = None,
    attack_samples: Optional[int] = None,
    features: Optional[int] = None,
    
    # Model Selection
    model_types: Optional[List[str]] = None,
    search_all_models: Optional[bool] = None,
    
    # Cross-Validation
    cv_folds: Optional[int] = None,
    cv_shuffle: Optional[bool] = None,
    cv_random_state: Optional[int] = None,
    
    # Search Space Configuration
    search_space: Optional[Dict[str, Any]] = None,
    parameter_ranges: Optional[Dict[str, Any]] = None,
    fixed_params: Optional[Dict[str, Any]] = None,
    
    # Trial Configuration
    trial_epochs: Optional[int] = None,
    trial_patience: Optional[int] = None,
    trial_batch_size: Optional[int] = None,
    
    # System Parameters
    hardware_data: Optional[Dict[str, Any]] = None,
    device: Optional[str] = None,
    random_seed: Optional[int] = None,
    num_workers: Optional[int] = None,
    
    # Storage and Output
    storage_url: Optional[str] = None,
    load_if_exists: Optional[bool] = None,
    save_study: Optional[bool] = None,
    study_dir: Optional[Union[str, Path]] = None,
    generate_plots: Optional[bool] = None,
    
    # Early Stopping
    early_stopping_patience: Optional[int] = None,
    early_stopping_min_trials: Optional[int] = None,
    
    # Monitoring
    verbose: Optional[bool] = None,
    show_progress: Optional[bool] = None,
    interactive: Optional[bool] = None,
    log_level: Optional[str] = None,
    
    # Advanced Parameters
    parallel_jobs: Optional[int] = None,
    memory_limit: Optional[str] = None,
    
    # Express Setup Compatibility Parameters
    express_context: Optional[Dict[str, Any]] = None,
    optimization_focus: Optional[str] = None,
    system_class: Optional[str] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    hpo_config: Optional[Dict[str, Any]] = None,
    args: Optional[argparse.Namespace] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """
    Set up hyperparameter optimization that integrates seamlessly with express setup configurations
    and the current train_model() implementation.
    
    Enhanced to fully support express setup configurations including:
    - Optimization focus strategies (balanced/speed/accuracy/efficiency)
    - System-aware resource allocation
    - Express search space configurations
    - Hardware-aware optimizations
    - Preset compatibility
    
    Returns:
        Dictionary containing the configured study, optimization functions, and metadata.
    """
    
    # Start timing
    start_time = datetime.now()
    setup_start_time = time.time()
    
    # Initialize configuration
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Get hardware context for system-aware configuration if not provided
    if hardware_data is None:
        try:
            hardware_data = check_hardware(include_memory_usage=True)
        except Exception as e:
            logger.debug(f"Hardware detection failed: {e}")
            hardware_data = {}
    
    # Apply HPO-specific configuration
    if hpo_config:
        config.setdefault('hyperparameter_optimization', {}).update(hpo_config)
    
    # Handle legacy args parameter
    if args is not None:
        # Extract parameters from args object
        legacy_params = {}
        for attr_name in dir(args):
            if not attr_name.startswith('_'):
                value = getattr(args, attr_name)
                if value is not None:
                    legacy_params[attr_name] = value
        
        # Map args parameters to function parameters
        legacy_mapping = {
            'hpo_trials': 'n_trials',
            'hpo_timeout': 'timeout_seconds',
            'hpo_study_name': 'study_name',
            'hpo_sampler': 'sampler_type',
            'hpo_pruner': 'pruner_type',
            'hpo_cv_folds': 'cv_folds',
            'hpo_trial_epochs': 'trial_epochs',
            'hpo_generate_plots': 'generate_plots',
            'hpo_save_study': 'save_study'
        }
        
        for old_key, new_key in legacy_mapping.items():
            if old_key in legacy_params:
                kwargs.setdefault(new_key, legacy_params[old_key])
        
        # Apply other relevant args
        kwargs.update(legacy_params)
    
    # Apply all parameters to configuration
    final_config = config.copy()
    
    # Apply individual parameters
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'hpo_config', 'kwargs', 'start_time', 'setup_start_time', 'datetime', 'traceback', 'time', 'gc', 'warnings', 'Path', 'args'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Set up HPO configuration with intelligent defaults
    hpo_section = final_config.setdefault('hyperparameter_optimization', {})
    
    # Extract express setup configuration if available
    express_config = hpo_section.get('express_setup', {})
    if express_context:
        express_config.update(express_context)
    
    # Apply optimization focus from express setup
    optimization_focus = (hpo_section.get('optimization_focus') or cleaned_params.get('optimization_focus') or express_config.get('focus', 'balanced'))
    hpo_section['optimization_focus'] = optimization_focus

    express_optimization_space = hpo_section.get('optimization_space') or express_config.get('optimization_space')
    
    # Apply system class from express setup
    system_class = (hpo_section.get('system_class') or cleaned_params.get('system_class') or express_config.get('system_class', 'standard'))
    hpo_section['system_class'] = system_class
    
    # Apply express intensity if available
    express_intensity = express_config.get('intensity', 'Standard')
    hpo_section['express_intensity'] = express_intensity
    
    # Core HPO parameters with express-aware defaults
    n_trials = hpo_section.setdefault('n_trials', cleaned_params.get('n_trials', 100))
    timeout_seconds = hpo_section.setdefault('timeout_seconds', cleaned_params.get('timeout_seconds', 0))
    study_name = hpo_section.setdefault('study_name', cleaned_params.get('study_name', f"autoencoder_hpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"))
    direction = hpo_section.setdefault('direction', cleaned_params.get('direction', 'minimize'))
    sampler_type = hpo_section.setdefault('sampler_type', cleaned_params.get('sampler_type', 'TPE'))
    pruner_type = hpo_section.setdefault('pruner_type', cleaned_params.get('pruner_type', 'MedianPruner'))
    
    # Data parameters
    use_real_data = hpo_section.setdefault('use_real_data', cleaned_params.get('use_real_data', False))
    data_path = hpo_section.setdefault('data_path', cleaned_params.get('data_path', None))
    artifacts_path = hpo_section.setdefault('artifacts_path', cleaned_params.get('artifacts_path', None))
    normal_samples = hpo_section.setdefault('normal_samples', cleaned_params.get('normal_samples', 10000))
    attack_samples = hpo_section.setdefault('attack_samples', cleaned_params.get('attack_samples', 2000))
    features = hpo_section.setdefault('features', cleaned_params.get('features', 78))
    
    # Model selection
    model_types = hpo_section.setdefault('model_types', cleaned_params.get('model_types', ['SimpleAutoencoder', 'EnhancedAutoencoder']))
    search_all_models = hpo_section.setdefault('search_all_models', cleaned_params.get('search_all_models', False))
    
    if search_all_models:
        model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
    
    # Cross-validation parameters
    cv_folds = hpo_section.setdefault('cv_folds', cleaned_params.get('cv_folds', 3))
    cv_shuffle = hpo_section.setdefault('cv_shuffle', cleaned_params.get('cv_shuffle', True))
    cv_random_state = hpo_section.setdefault('cv_random_state', cleaned_params.get('cv_random_state', 42))
    
    # Trial configuration with express-aware adjustments
    trial_epochs = hpo_section.setdefault('trial_epochs', cleaned_params.get('trial_epochs', 20))
    trial_patience = hpo_section.setdefault('trial_patience', cleaned_params.get('trial_patience', 5))
    trial_batch_size = hpo_section.setdefault('trial_batch_size', cleaned_params.get('trial_batch_size', 64))
    
    # System parameters
    device = hpo_section.setdefault('device', cleaned_params.get('device', 'auto'))
    random_seed = hpo_section.setdefault('random_seed', cleaned_params.get('random_seed', 42))
    num_workers = hpo_section.setdefault('num_workers', cleaned_params.get('num_workers', 0))
    
    # Storage and output
    storage_url = hpo_section.setdefault('storage_url', cleaned_params.get('storage_url', None))
    load_if_exists = hpo_section.setdefault('load_if_exists', cleaned_params.get('load_if_exists', False))
    save_study = hpo_section.setdefault('save_study', cleaned_params.get('save_study', True))
    study_dir = hpo_section.setdefault('study_dir', cleaned_params.get('study_dir', DEFAULT_MODEL_DIR / "hpo_studies"))
    generate_plots = hpo_section.setdefault('generate_plots', cleaned_params.get('generate_plots', True))
    
    # Early stopping
    early_stopping_patience = hpo_section.setdefault('early_stopping_patience', cleaned_params.get('early_stopping_patience', 10))
    early_stopping_min_trials = hpo_section.setdefault('early_stopping_min_trials', cleaned_params.get('early_stopping_min_trials', 20))
    
    # Monitoring
    verbose = hpo_section.setdefault('verbose', cleaned_params.get('verbose', True))
    interactive = hpo_section.setdefault('interactive', cleaned_params.get('interactive', True))
    show_progress = hpo_section.setdefault('show_progress', cleaned_params.get('show_progress', True))
    log_level = hpo_section.setdefault('log_level', cleaned_params.get('log_level', 'INFO'))
    
    # Advanced parameters with express-aware system optimization
    parallel_jobs = hpo_section.setdefault('parallel_jobs', cleaned_params.get('parallel_jobs', 1))
    
    # Apply express parallel trials if available
    express_parallel_trials = hpo_section.get('parallel_trials')
    if express_parallel_trials and express_parallel_trials > parallel_jobs:
        parallel_jobs = express_parallel_trials
        hpo_section['parallel_jobs'] = parallel_jobs
    
    memory_limit = hpo_section.setdefault('memory_limit', cleaned_params.get('memory_limit', None))
    
    # Apply optimization focus strategies to adjust parameters
    original_trial_epochs = trial_epochs
    original_trial_patience = trial_patience
    
    if optimization_focus == 'speed':
        # Speed focus: reduce trial epochs, increase pruning aggressiveness
        trial_epochs = max(2, trial_epochs // 2)
        trial_patience = max(3, trial_patience // 2)
        if verbose:
            print(Fore.YELLOW + Style.BRIGHT + f"  Speed focus: Reduced trial epochs from {original_trial_epochs} to {trial_epochs}")
            
    elif optimization_focus == 'accuracy':
        # Accuracy focus: increase trial epochs, reduce pruning
        trial_epochs = min(50, trial_epochs * 2)
        trial_patience = min(15, trial_patience * 2)
        if verbose:
            print(Fore.YELLOW + Style.BRIGHT + f"  Accuracy focus: Increased trial epochs from {original_trial_epochs} to {trial_epochs}")
            
    elif optimization_focus == 'efficiency':
        # Efficiency focus: optimize for memory and resource usage
        trial_epochs = max(15, trial_epochs)
        # Use smaller batch sizes for memory efficiency
        if 'batch_size' in hpo_section.get('optimization_space', {}):
            hpo_section['optimization_space']['batch_size']['choices'] = [32, 64]
        if verbose:
            print(Fore.YELLOW + Style.BRIGHT + f"  Efficiency focus: Optimized for memory usage")
    
    # Update the configuration with focus-adjusted values
    hpo_section['trial_epochs'] = original_trial_epochs
    hpo_section['trial_patience'] = original_trial_patience
    
    # Adjust parameters based on system class
    if system_class == "limited":
        # Limited resources: conservative settings
        parallel_jobs = 1
        num_workers = 0
        if trial_epochs > 15:
            trial_epochs = 15
        if verbose:
            print(Fore.YELLOW + Style.BRIGHT + f"  Limited system: Using conservative resource settings")
            
    elif system_class == "enterprise":
        # Enterprise system: maximize resource usage
        parallel_jobs = min(4, parallel_jobs * 2)
        num_workers = min(8, num_workers * 2)
        if verbose:
            print(Fore.YELLOW + Style.BRIGHT + f"  Enterprise system: Maximizing resource usage")
    
    # Update system-optimized parameters
    hpo_section['parallel_jobs'] = parallel_jobs
    hpo_section['num_workers'] = num_workers

    # Set up logging
    if verbose:
        original_level = logger.level
        logger.setLevel(getattr(logging, log_level))
    
    total_setup_stages = 16

    # Initialize results tracking
    setup_results = {
        'start_time': start_time.isoformat(),
        'study_name': study_name,
        'configuration': hpo_section,
        'express_context': {
            'optimization_focus': optimization_focus,
            'system_class': system_class,
            'express_intensity': express_intensity,
            'express_config': express_config
        },
        'setup_stages_completed': 0,
        'total_setup_stages': total_setup_stages,
        'warnings': [],
        'errors': [],
        'setup_time_seconds': 0,
        'success': False,
        'study': None,
        'objective_function': None,
        'search_space_function': None,
        'analysis_functions': {},
        'callbacks': []
    }
    
    # Progress tracking data
    progress_data = {
        'current_stage': 'Starting...',
        'current_operation': None,
        'setup_steps_completed': 0,
        'total_setup_steps': total_setup_stages,
        'warnings': [],
        'errors': []
    }
    
    try:
        # Display setup header
        if interactive:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION SETUP")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            # Show express context if available
            if express_config:
                print(Fore.YELLOW + Style.BRIGHT + "Express Setup Context:")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Optimization Focus: " + Fore.YELLOW + Style.BRIGHT + f"{optimization_focus.title()}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Intensity: " + Fore.YELLOW + Style.BRIGHT + f"{express_intensity}")
                if express_config.get('preset_alignment'):
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Preset Alignment: " + Fore.YELLOW + Style.BRIGHT + f"{express_config['preset_alignment']}")
            
            print(Fore.YELLOW + Style.BRIGHT + "Setup Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Study Name: " + Fore.YELLOW + Style.BRIGHT + f"{study_name}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Trials: " + Fore.YELLOW + Style.BRIGHT + f"{n_trials}")
            
            if timeout_seconds > 0:
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{timeout_seconds}s")
            else:
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"None")
            
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Types: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(model_types)}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ CV Folds: " + Fore.YELLOW + Style.BRIGHT + f"{cv_folds}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{sampler_type}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{pruner_type}")
            
            print(Fore.CYAN + Style.BRIGHT + "\n  Data Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Data Type: " + Fore.YELLOW + Style.BRIGHT + f"{'Real Data' if use_real_data else 'Synthetic Data'}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Normal Samples: " + Fore.YELLOW + Style.BRIGHT + f"{normal_samples}")
            print(Fore.GREEN + Style.BRIGHT + f"    └─ Attack Samples: " + Fore.YELLOW + Style.BRIGHT + f"{attack_samples}")
            
            print(Fore.CYAN + Style.BRIGHT + "\n  System Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Device: " + Fore.YELLOW + Style.BRIGHT + f"{device}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Random Seed: " + Fore.YELLOW + Style.BRIGHT + f"{random_seed}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Num Workers: " + Fore.YELLOW + Style.BRIGHT + f"{num_workers}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Parallel Jobs: " + Fore.YELLOW + Style.BRIGHT + f"{parallel_jobs}")
            print(Fore.GREEN + Style.BRIGHT + f"    └─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
            
            print(Fore.CYAN + Style.BRIGHT + "\n  Output Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Save Study: " + Fore.YELLOW + Style.BRIGHT + f"{save_study}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Generate Plots: " + Fore.YELLOW + Style.BRIGHT + f"{generate_plots}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Study Directory: " + Fore.YELLOW + Style.BRIGHT + f"{study_dir}")
            print(Fore.GREEN + Style.BRIGHT + f"    └─ Storage: " + Fore.YELLOW + Style.BRIGHT + f"{storage_url or 'In-Memory'}")
            
            print(Fore.GREEN + Style.BRIGHT + "\nStarting hyperparameter optimization setup...\n")
        
        # STAGE 1: Initial Configuration Setup
        progress_data['current_stage'] = "Initial Configuration"
        setup_results['current_stage'] = "Initial Configuration"
        
        # Memory optimization at start
        _optimize_memory_if_needed(
            condition=True,
            hardware_data=hardware_data,
            aggressive=False,
            silent=not verbose
        )
        
        # Update results tracking
        setup_results['setup_stages_completed'] = 1
        progress_data['setup_steps_completed'] = 1
        
        # STAGE 2: Express Configuration Integration
        progress_data['current_stage'] = "Express Configuration"
        setup_results['current_stage'] = "Express Configuration"
        
        # Apply express search space if available
        express_search_space = hpo_section.get('optimization_space')
        if express_search_space and verbose:
            param_count = len(express_search_space)
            print(Fore.GREEN + Style.BRIGHT + f"Using express search space with {param_count} parameters")
        
        # Apply express trial patience if calculated
        express_trial_patience = hpo_section.get('trial_patience')
        if express_trial_patience and express_trial_patience != trial_patience:
            trial_patience = express_trial_patience
            if verbose:
                print(Fore.GREEN + Style.BRIGHT + f"Using express-calculated trial patience: {trial_patience}")
        
        # Update results tracking
        setup_results['setup_stages_completed'] = 2
        progress_data['setup_steps_completed'] = 2
        
        # STAGE 3: Study Directory Setup
        progress_data['current_stage'] = "Directory Setup"
        setup_results['current_stage'] = "Directory Setup"
        
        # Create study directory
        study_dir = Path(study_dir)
        study_dir.mkdir(parents=True, exist_ok=True)
        
        # Create subdirectories
        plots_dir = study_dir / "plots"
        logs_dir = study_dir / "logs"
        trials_dir = study_dir / "trials"
        
        plots_dir.mkdir(exist_ok=True)
        logs_dir.mkdir(exist_ok=True)
        trials_dir.mkdir(exist_ok=True)
        
        # Update results tracking
        setup_results['study_dir'] = str(study_dir)
        setup_results['setup_stages_completed'] = 3
        progress_data['setup_steps_completed'] = 3
        
        # STAGE 4: Data Configuration Validation
        progress_data['current_stage'] = "Data Configuration"
        setup_results['current_stage'] = "Data Configuration"
        
        if use_real_data and not data_path:
            warning_msg = "use_real_data is True but no data_path provided, falling back to synthetic data"
            progress_data['warnings'].append(warning_msg)
            setup_results['warnings'].append(warning_msg)
            if verbose:
                logger.warning(warning_msg)
            use_real_data = False
        
        # Validate data path if provided
        if use_real_data and data_path:
            data_path_obj = Path(data_path)
            if not data_path_obj.exists():
                warning_msg = f"Data path {data_path} does not exist, falling back to synthetic data"
                progress_data['warnings'].append(warning_msg)
                setup_results['warnings'].append(warning_msg)
                if verbose:
                    logger.warning(warning_msg)
                use_real_data = False
        
        # Update configuration with validated parameters
        hpo_section['use_real_data'] = use_real_data
        hpo_section['data_path'] = data_path
        
        # Update results tracking
        setup_results['data_config'] = {
            'use_real_data': use_real_data,
            'data_path': data_path,
            'normal_samples': normal_samples,
            'attack_samples': attack_samples,
            'features': features
        }
        setup_results['setup_stages_completed'] = 4
        progress_data['setup_steps_completed'] = 4
        
        # STAGE 5: Model Configuration Setup
        progress_data['current_stage'] = "Model Configuration"
        setup_results['current_stage'] = "Model Configuration"
        
        # Validate model types
        valid_model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
        invalid_models = [model for model in model_types if model not in valid_model_types]
        
        if invalid_models:
            warning_msg = f"Invalid model types: {invalid_models}, using valid models only"
            progress_data['warnings'].append(warning_msg)
            setup_results['warnings'].append(warning_msg)
            if verbose:
                logger.warning(warning_msg)
            model_types = [model for model in model_types if model in valid_model_types]
        
        if not model_types:
            model_types = ['SimpleAutoencoder']
            if verbose:
                logger.warning("No valid model types specified, using SimpleAutoencoder as default")
        
        # Update configuration
        hpo_section['model_types'] = model_types
        
        # Update results tracking
        setup_results['model_config'] = {
            'model_types': model_types,
            'search_all_models': search_all_models,
            'valid_models_count': len(model_types)
        }
        setup_results['setup_stages_completed'] = 5
        progress_data['setup_steps_completed'] = 5
        
        # STAGE 6: Sampler Configuration
        progress_data['current_stage'] = "Sampler Setup"
        setup_results['current_stage'] = "Sampler Setup"
        
        sampler_config = {
            'seed': random_seed,
            'n_startup_trials': min(10, n_trials // 10)
        }

        # Apply optimization focus to sampler configuration
        if optimization_focus == 'speed':
            sampler_config['n_startup_trials'] = min(5, n_trials // 20)
        elif optimization_focus == 'accuracy':
            sampler_config['n_startup_trials'] = min(15, n_trials // 5)

        # Sampler configuration parsing
        sampler_raw = hpo_section.get('sampler_type', 'TPE')
        if isinstance(sampler_raw, dict):
            sampler_type = sampler_raw.get('type', 'TPE')
            additional_config = {k: v for k, v in sampler_raw.items() if k != 'type'}
            sampler_config.update(additional_config)
        else:
            sampler_type = sampler_raw

        # Map common sampler variants to standard names
        sampler_mapping = {
            'Random': 'Random',
            'TPE': 'TPE',
            'CmaEs': 'CmaEs',
            'NSGAIISampler': 'NSGAII'
        }
        sampler_type = sampler_mapping.get(sampler_type, 'TPE')

        if sampler_type == 'TPE':
            sampler = TPESampler(
                seed=sampler_config['seed'],
                consider_prior=sampler_config.get('consider_prior', True),
                consider_magic_clip=sampler_config.get('consider_magic_clip', True),
                consider_endpoints=sampler_config.get('consider_endpoints', False),
                n_startup_trials=sampler_config['n_startup_trials'],
                multivariate=sampler_config.get('multivariate', True),
                warn_independent_sampling=False
            )
            sampler_description = f"TPE sampler (startup: {sampler_config['n_startup_trials']} trials)"
        elif sampler_type == 'Random':
            sampler = RandomSampler(seed=sampler_config['seed'])
            sampler_description = "Random sampler configured"
        elif sampler_type == 'CmaEs':
            sampler = CmaEsSampler(seed=sampler_config['seed'])
            sampler_description = "CMA-ES sampler configured"
        elif sampler_type == 'NSGAII':
            sampler = NSGAIISampler(seed=sampler_config['seed'])
            sampler_description = "NSGA-II sampler configured"
        else:
            warning_msg = f"Unknown sampler type '{sampler_type}', using TPE"
            progress_data['warnings'].append(warning_msg)
            setup_results['warnings'].append(warning_msg)
            if verbose:
                logger.warning(warning_msg)
            sampler = TPESampler(seed=sampler_config['seed'])
            sampler_description = "TPE sampler (fallback) configured"
        
        # Update results tracking
        setup_results['sampler_config'] = {
            'type': type(sampler).__name__,
            'description': sampler_description,
            'config': sampler_config
        }
        setup_results['setup_stages_completed'] = 6
        progress_data['setup_steps_completed'] = 6
        
        # STAGE 7: Pruner Configuration
        progress_data['current_stage'] = "Pruner Setup"
        setup_results['current_stage'] = "Pruner Setup"
        
        pruner_config = {
            'n_startup_trials': 5,
            'n_warmup_steps': 5
        }

        # Apply optimization focus to pruner configuration
        if optimization_focus == 'speed':
            pruner_config['n_startup_trials'] = 3
            pruner_config['n_warmup_steps'] = 3
        elif optimization_focus == 'accuracy':
            pruner_config['n_startup_trials'] = 8
            pruner_config['n_warmup_steps'] = 8

        # Pruner configuration parsing
        pruner_raw = hpo_section.get('pruner_type', 'MedianPruner')
        if isinstance(pruner_raw, dict):
            pruner_type = pruner_raw.get('type', 'MedianPruner')
            additional_config = {k: v for k, v in pruner_raw.items() if k != 'type'}
            pruner_config.update(additional_config)
        else:
            pruner_type = pruner_raw

        # Map common pruner variants to standard names
        pruner_mapping = {
            'Nop': 'NopPruner',
            'Median': 'MedianPruner', 
            'Hyperband': 'HyperbandPruner',
            'SuccessiveHalving': 'SuccessiveHalvingPruner'
        }
        pruner_type = pruner_mapping.get(pruner_type, 'MedianPruner')

        if pruner_type == 'MedianPruner':
            pruner = MedianPruner(
                n_startup_trials=pruner_config['n_startup_trials'],
                n_warmup_steps=pruner_config['n_warmup_steps'],
                interval_steps=pruner_config.get('interval_steps', 1)
            )
            pruner_description = f"Median pruner (startup: {pruner_config['n_startup_trials']})"
        elif pruner_type == 'HyperbandPruner':
            pruner = HyperbandPruner(
                min_resource=pruner_config.get('min_resource', 1),
                max_resource=pruner_config.get('max_resource', trial_epochs),
                reduction_factor=pruner_config.get('reduction_factor', 3)
            )
            pruner_description = "Hyperband pruner configured"
        elif pruner_type == 'SuccessiveHalvingPruner':
            pruner = SuccessiveHalvingPruner(
                min_resource=pruner_config.get('min_resource', 1),
                reduction_factor=pruner_config.get('reduction_factor', 3)
            )
            pruner_description = "Successive Halving pruner configured"
        elif pruner_type == 'NopPruner' or pruner_type == 'None':
            pruner = NopPruner()
            pruner_description = "No pruning configured"
        else:
            warning_msg = f"Unknown pruner type '{pruner_type}', using MedianPruner"
            progress_data['warnings'].append(warning_msg)
            setup_results['warnings'].append(warning_msg)
            if verbose:
                logger.warning(warning_msg)
            pruner = MedianPruner()
            pruner_description = "Median pruner (fallback) configured"
        
        # Update results tracking
        setup_results['pruner_config'] = {
            'type': type(pruner).__name__,
            'description': pruner_description,
            'config': pruner_config
        }
        setup_results['setup_stages_completed'] = 7
        progress_data['setup_steps_completed'] = 7
        
        # STAGE 8: Storage Configuration
        progress_data['current_stage'] = "Storage Setup"
        setup_results['current_stage'] = "Storage Setup"
        
        storage = None
        if storage_url:
            try:
                storage = optuna.storages.RDBStorage(
                    url=storage_url,
                    heartbeat_interval=60,
                    grace_period=120
                )
                storage_description = f"Persistent storage: {storage_url}"
                if verbose:
                    print(f"Using persistent storage: {storage_url}")
            except Exception as e:
                warning_msg = f"Failed to setup storage: {e}"
                progress_data['warnings'].append(warning_msg)
                setup_results['warnings'].append(warning_msg)
                if verbose:
                    logger.warning(warning_msg)
                storage_description = "Storage setup failed, using in-memory"
        else:
            storage_description = "Using in-memory storage"
        
        # Update results tracking
        setup_results['storage_config'] = {
            'type': 'RDBStorage' if storage else 'InMemoryStorage',
            'description': storage_description,
            'url': storage_url
        }
        setup_results['setup_stages_completed'] = 8
        progress_data['setup_steps_completed'] = 8
        
        # STAGE 9: Study Creation
        progress_data['current_stage'] = "Study Creation"
        setup_results['current_stage'] = "Study Creation"
        
        study = optuna.create_study(
            direction=direction,
            sampler=sampler,
            pruner=pruner,
            study_name=study_name,
            storage=storage,
            load_if_exists=load_if_exists
        )
        
        # Update results tracking with study information
        setup_results['study'] = study
        setup_results['study_name'] = study_name
        setup_results['setup_stages_completed'] = 9
        progress_data['setup_steps_completed'] = 9
        
        # STAGE 10: Search Space Definition
        progress_data['current_stage'] = "Search Space"
        setup_results['current_stage'] = "Search Space"
        
        # Define search space for express configuration
        def create_search_space(trial):
            """Create search space that uses express configurations when available"""
            
            # Use express search space if available, otherwise use default
            express_optimization_space = hpo_section.get('optimization_space')
            
            if express_optimization_space:
                # Use the structured search space from express setup
                params = {}
                
                # First, handle model type selection
                model_type = trial.suggest_categorical('model_type', model_types)
                params['model_type'] = model_type
                
                # Then apply express search space parameters
                for param_name, param_config in express_optimization_space.items():
                    # Skip model_type if already handled
                    if param_name == 'model_type':
                        continue
                        
                    param_type = param_config.get('type', 'float')
                    
                    try:
                        if param_type == 'float':
                            low = param_config.get('low', param_config.get('min', 0.0))
                            high = param_config.get('high', param_config.get('max', 1.0))
                            log = param_config.get('log', False)
                            params[param_name] = trial.suggest_float(param_name, low, high, log=log)
                            
                        elif param_type == 'int':
                            low = param_config.get('low', param_config.get('min', 0))
                            high = param_config.get('high', param_config.get('max', 100))
                            step = param_config.get('step', 1)
                            params[param_name] = trial.suggest_int(param_name, low, high, step=step)
                            
                        elif param_type == 'categorical':
                            choices = param_config.get('choices', [])
                            params[param_name] = trial.suggest_categorical(param_name, choices)
                            
                    except Exception as e:
                        logger.warning(f"Failed to suggest parameter {param_name}: {e}")
                        # Fall back to default parameter suggestion
                
                return params
            else:
                # Fall back to original search space logic
                return _create_default_search_space(trial, model_types, features, trial_epochs)
        
        # Define search space
        def _create_default_search_space(trial, model_types, features, trial_epochs):
            """Create search space that maps directly to train_model parameters"""

            # Model type selection
            model_type = trial.suggest_categorical('model_type', model_types)
            input_dim = features
            
            # Base model parameters that apply to all model types
            learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
            batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])
            weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)
            optimizer_type = trial.suggest_categorical('optimizer_type', ['Adam', 'AdamW', 'RMSprop'])
            
            # Scheduler configuration
            scheduler_type = trial.suggest_categorical('scheduler_type', [None, 'ReduceLROnPlateau', 'StepLR', 'CosineAnnealingLR'])
            scheduler_params = {}
            
            if scheduler_type == 'ReduceLROnPlateau':
                scheduler_params = {
                    'patience': trial.suggest_int('lr_patience', 3, 8),
                    'factor': trial.suggest_float('lr_factor', 0.3, 0.7),
                    'min_lr': trial.suggest_float('min_lr', 1e-8, 1e-5, log=True)
                }
            elif scheduler_type == 'StepLR':
                scheduler_params = {
                    'step_size': trial.suggest_int('step_size', 10, 30),
                    'gamma': trial.suggest_float('gamma', 0.1, 0.5)
                }
            elif scheduler_type == 'CosineAnnealingLR':
                scheduler_params = {
                    'T_max': trial_epochs,
                    'eta_min': trial.suggest_float('eta_min', 1e-8, 1e-5, log=True)
                }
            
            # Advanced training parameters
            gradient_clip = trial.suggest_categorical('gradient_clip', [None, 0.5, 1.0, 2.0])
            gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])
            mixed_precision = trial.suggest_categorical('mixed_precision', [False, True])
            
            # Model-specific parameters
            if model_type == 'SimpleAutoencoder':
                min_encoding_dim = max(4, min(8, input_dim // 8))  # More conservative minimum
                max_encoding_dim = max(min_encoding_dim + 4, min(64, input_dim // 2))  # Ensure max > min
                
                encoding_dim = trial.suggest_int('encoding_dim', min_encoding_dim, max_encoding_dim)
                
                # Architecture choices
                arch_choice = trial.suggest_categorical('hidden_arch', [
                    'single_small', 'single_medium', 'single_large', 
                    'double_small', 'double_medium', 'double_large'
                ])
                
                if arch_choice == 'single_small':
                    hidden_dims = [max(16, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif arch_choice == 'single_medium':
                    hidden_dims = [max(32, encoding_dim * 4)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif arch_choice == 'single_large':
                    hidden_dims = [max(64, encoding_dim * 8)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif arch_choice == 'double_small':
                    hidden_dims = [max(32, encoding_dim * 4), max(16, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.1, 0.4),
                        trial.suggest_float('dropout_1', 0.05, 0.3)
                    ]
                elif arch_choice == 'double_medium':
                    hidden_dims = [max(64, encoding_dim * 8), max(32, encoding_dim * 4)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.15, 0.4),
                        trial.suggest_float('dropout_1', 0.1, 0.3)
                    ]
                else:  # double_large
                    hidden_dims = [max(128, encoding_dim * 16), max(64, encoding_dim * 8)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.2, 0.45),
                        trial.suggest_float('dropout_1', 0.15, 0.35)
                    ]
                
                model_params = {
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'activation': trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'gelu', 'tanh']),
                    'normalization': trial.suggest_categorical('normalization', [None, 'batch']),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'residual_blocks': False,
                    'use_attention': False,
                    'legacy_mode': False
                }
            
            elif model_type == 'EnhancedAutoencoder':
                # Ensure min <= max for encoding dimensions
                min_encoding_dim = max(4, min(8, input_dim // 8))
                max_encoding_dim = max(min_encoding_dim + 8, min(64, input_dim // 2))
                
                encoding_dim = trial.suggest_int('encoding_dim', min_encoding_dim, max_encoding_dim, step=8)
                
                # Architecture choices
                arch_choice = trial.suggest_categorical('enhanced_arch', [
                    'compact', 'balanced', 'deep', 'wide', 'ultra_deep'
                ])
                
                if arch_choice == 'compact':
                    hidden_dims = [max(32, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.3)]
                elif arch_choice == 'balanced':
                    hidden_dims = [max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.15, 0.35),
                        trial.suggest_float('dropout_1', 0.1, 0.25)
                    ]
                elif arch_choice == 'deep':
                    hidden_dims = [max(128, encoding_dim * 8), max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.2, 0.4),
                        trial.suggest_float('dropout_1', 0.15, 0.35),
                        trial.suggest_float('dropout_2', 0.1, 0.3)
                    ]
                elif arch_choice == 'wide':
                    hidden_dims = [max(256, encoding_dim * 16), max(128, encoding_dim * 8)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.25, 0.45),
                        trial.suggest_float('dropout_1', 0.2, 0.4)
                    ]
                else:  # ultra_deep
                    hidden_dims = [
                        max(256, encoding_dim * 16), 
                        max(128, encoding_dim * 8), 
                        max(64, encoding_dim * 4), 
                        max(32, encoding_dim * 2)
                    ]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.25, 0.45),
                        trial.suggest_float('dropout_1', 0.2, 0.4),
                        trial.suggest_float('dropout_2', 0.15, 0.35),
                        trial.suggest_float('dropout_3', 0.1, 0.3)
                    ]
                
                model_params = {
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'activation': trial.suggest_categorical('activation', ['leaky_relu', 'gelu', 'relu', 'swish']),
                    'normalization': trial.suggest_categorical('normalization', ['batch', 'layer', 'instance']),
                    'use_attention': trial.suggest_categorical('use_attention', [False, True]),
                    'residual_blocks': trial.suggest_categorical('residual_blocks', [False, True]),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'legacy_mode': trial.suggest_categorical('legacy_mode', [False, True])
                }
            
            elif model_type == 'AutoencoderEnsemble':
                # Ensure min <= max for encoding dimensions
                min_encoding_dim = max(4, min(8, input_dim // 8))
                max_encoding_dim = max(min_encoding_dim + 8, min(48, input_dim // 3))
                
                encoding_dim = trial.suggest_int('encoding_dim', min_encoding_dim, max_encoding_dim, step=8)
                num_models = trial.suggest_int('num_models', 3, 7, step=2)
                diversity_factor = trial.suggest_float('diversity_factor', 0.1, 0.5)
                
                # Ensemble architecture
                ensemble_arch = trial.suggest_categorical('ensemble_arch', ['small', 'medium', 'large'])
                
                if ensemble_arch == 'small':
                    hidden_dims = [max(32, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.15, 0.35)]
                elif ensemble_arch == 'medium':
                    hidden_dims = [max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.2, 0.4),
                        trial.suggest_float('dropout_1', 0.15, 0.35)
                    ]
                else:  # large
                    hidden_dims = [max(128, encoding_dim * 8), max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.25, 0.45),
                        trial.suggest_float('dropout_1', 0.2, 0.4),
                        trial.suggest_float('dropout_2', 0.15, 0.35)
                    ]
                
                model_params = {
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'num_models': num_models,
                    'diversity_factor': diversity_factor,
                    'activation': trial.suggest_categorical('activation', ['leaky_relu', 'gelu', 'relu']),
                    'normalization': trial.suggest_categorical('normalization', ['batch', 'layer']),
                    'use_attention': trial.suggest_categorical('use_attention', [False, True]),
                    'residual_blocks': trial.suggest_categorical('residual_blocks', [False, True]),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'min_features': trial.suggest_int('min_features', 3, 10)
                }
            
            # Security parameters
            percentile = trial.suggest_float('percentile', 90.0, 99.0)
            threshold_method = trial.suggest_categorical('threshold_method', ['percentile', 'adaptive'])
            
            # Return parameters
            return {
                # Model architecture parameters
                'model_type': model_type,
                'input_dim': features,
                'encoding_dim': model_params['encoding_dim'],
                'hidden_dims': model_params['hidden_dims'],
                'dropout_rates': model_params['dropout_rates'],
                'activation': model_params['activation'],
                'normalization': model_params['normalization'],
                'skip_connection': model_params.get('skip_connection', True),
                'residual_blocks': model_params.get('residual_blocks', True),
                'use_attention': model_params.get('use_attention', True),
                'legacy_mode': model_params.get('legacy_mode', False),
                'num_models': model_params.get('num_models'),
                'diversity_factor': model_params.get('diversity_factor'),
                
                # Training configuration parameters
                'batch_size': batch_size,
                'epochs': trial_epochs,
                'learning_rate': learning_rate,
                'patience': trial_patience,
                'weight_decay': weight_decay,
                'gradient_clip': gradient_clip,
                'gradient_accumulation_steps': gradient_accumulation_steps,
                'mixed_precision': mixed_precision,
                'optimizer_type': optimizer_type,
                'scheduler_type': scheduler_type,
                'scheduler_params': scheduler_params,
                'early_stopping': True,
                'validation_split': 0.2,
                
                # Data configuration parameters
                'normal_samples': normal_samples,
                'attack_samples': attack_samples,
                'features': features,
                'use_real_data': use_real_data,
                'data_path': data_path,
                'artifacts_path': artifacts_path,
                'data_preprocessing': True,
                
                # Security configuration parameters
                'percentile': percentile,
                'threshold_method': threshold_method,
                'enable_security_metrics': True,
                
                # System configuration parameters
                'device': device,
                'random_seed': random_seed,
                'reproducible': True,
                
                # Monitoring configuration (minimal for HPO)
                'verbose': False,
                'debug_mode': False,
                'tensorboard_logging': False,
                'save_checkpoints': False,
                'progress_bar': False,
                
                # Export configuration (minimal for HPO)
                'export_onnx': False,
                'save_model': False,
                'save_metadata': False,
                'save_training_history': False,
                
                # Advanced training parameters
                'num_workers': num_workers,
                'pin_memory': False,  # Disabled for HPO stability
                'persistent_workers': False,
                'memory_efficient': True,
                
                # Error handling for HPO
                'error_handling': 'continue',
                'graceful_degradation': True,
                'continue_on_error': True
            }
        
        # Update results tracking
        setup_results['search_space_function'] = create_search_space
        setup_results['setup_stages_completed'] = 10
        progress_data['setup_steps_completed'] = 10
        
        # STAGE 11: Objective Function Setup
        progress_data['current_stage'] = "Objective Function"
        setup_results['current_stage'] = "Objective Function"
        
        # Define objective function
        def objective(trial):
            """Objective function that uses train_model for evaluation"""
            try:
                # Get trial parameters
                trial_params = create_search_space(trial)
                
                use_real_data_local = use_real_data
                input_dim = features

                # Initialize fold_config at the beginning to avoid UnboundLocalError
                # fold_config = None

                # Ensure all required parameters are available in trial_params
                required_params = {
                    # Core model parameters
                    'model_type': trial_params.get('model_type', 'SimpleAutoencoder'),
                    'input_dim': trial_params.get('input_dim', features),
                    'encoding_dim': trial_params.get('encoding_dim', max(8, features // 4)),
                    'hidden_dims': trial_params.get('hidden_dims', [max(32, features // 2)]),
                    'dropout_rates': trial_params.get('dropout_rates', [0.2]),
                    'activation': trial_params.get('activation', 'relu'),
                    'normalization': trial_params.get('normalization', 'batch'),
                    'skip_connection': trial_params.get('skip_connection', True),
                    'residual_blocks': trial_params.get('residual_blocks', True),
                    'use_attention': trial_params.get('use_attention', True),
                    'legacy_mode': trial_params.get('legacy_mode', False),
                    
                    # Training parameters
                    'batch_size': trial_params.get('batch_size', 64),
                    'epochs': trial_params.get('epochs', trial_epochs),
                    'learning_rate': trial_params.get('learning_rate', 1e-3),
                    'patience': trial_params.get('patience', trial_patience),
                    'weight_decay': trial_params.get('weight_decay', 1e-4),
                    'optimizer_type': trial_params.get('optimizer_type', 'Adam'),
                    'scheduler_type': trial_params.get('scheduler_type', None),
                    'scheduler_params': trial_params.get('scheduler_params', {}),
                    
                    # Advanced training parameters
                    'gradient_clip': trial_params.get('gradient_clip', None),
                    'gradient_accumulation_steps': trial_params.get('gradient_accumulation_steps', 1),
                    'mixed_precision': trial_params.get('mixed_precision', False),
                    
                    # Data parameters
                    'features': trial_params.get('features', features),
                    'normal_samples': trial_params.get('normal_samples', normal_samples),
                    'attack_samples': trial_params.get('attack_samples', attack_samples),
                    'use_real_data': trial_params.get('use_real_data', use_real_data),
                    'data_path': trial_params.get('data_path', data_path),
                    'artifacts_path': trial_params.get('artifacts_path', artifacts_path),
                    'data_preprocessing': trial_params.get('data_preprocessing', True),
                    
                    # Security parameters
                    'percentile': trial_params.get('percentile', 95.0),
                    'threshold_method': trial_params.get('threshold_method', 'percentile'),
                    'enable_security_metrics': trial_params.get('enable_security_metrics', True),
                    
                    # System parameters
                    'device': trial_params.get('device', device),
                    'random_seed': trial_params.get('random_seed', random_seed),
                    'reproducible': trial_params.get('reproducible', True),
                    'num_workers': trial_params.get('num_workers', num_workers),
                    
                    # Monitoring parameters
                    'verbose': trial_params.get('verbose', False),
                    'debug_mode': trial_params.get('debug_mode', False),
                    'tensorboard_logging': trial_params.get('tensorboard_logging', False),
                    'save_checkpoints': trial_params.get('save_checkpoints', False),
                    'progress_bar': trial_params.get('progress_bar', False),
                    
                    # Export parameters
                    'export_onnx': trial_params.get('export_onnx', False),
                    'save_model': trial_params.get('save_model', False),
                    'save_metadata': trial_params.get('save_metadata', False),
                    'save_training_history': trial_params.get('save_training_history', False),
                    
                    # Advanced training parameters
                    'pin_memory': trial_params.get('pin_memory', False),
                    'persistent_workers': trial_params.get('persistent_workers', False),
                    'memory_efficient': trial_params.get('memory_efficient', True),
                    
                    # Error handling parameters
                    'error_handling': trial_params.get('error_handling', 'continue'),
                    'graceful_degradation': trial_params.get('graceful_degradation', True),
                    'continue_on_error': trial_params.get('continue_on_error', True),
                    
                    # Training configuration
                    'early_stopping': trial_params.get('early_stopping', True),
                    'validation_split': trial_params.get('validation_split', 0.2)
                }
                
                # Update trial_params with any missing required parameters
                for key, default_value in required_params.items():
                    if key not in trial_params or trial_params[key] is None:
                        trial_params[key] = default_value
                
                # Handle model-specific parameters that might be missing
                # These are conditionally returned based on model_type in search space functions
                if trial_params['model_type'] == 'AutoencoderEnsemble':
                    ensemble_params = {
                        'num_models': trial_params.get('num_models', 3),
                        'diversity_factor': trial_params.get('diversity_factor', 0.3),
                        'min_features': trial_params.get('min_features', 5)
                    }
                    for key, default_value in ensemble_params.items():
                        if key not in trial_params or trial_params[key] is None:
                            trial_params[key] = default_value
                else:
                    # Ensure ensemble-specific params are not present for non-ensemble models
                    for key in ['num_models', 'diversity_factor', 'min_features']:
                        if key in trial_params and trial_params[key] is not None:
                            # Remove or set to None for non-ensemble models to avoid confusion
                            trial_params[key] = None
                
                # Set up cross-validation
                if use_real_data_local and data_path:
                    # Load real data once for all folds
                    try:
                        data = load_and_validate_data(
                            data_path=data_path,
                            artifacts_path=artifacts_path,
                            silent=True,
                            config=final_config
                        )
                        X_data = data['X_train']
                        features_actual = len(data['feature_names'])
                        input_dim = features_actual
                        
                        # Update input dimension if different
                        if features_actual != features:
                            trial_params['input_dim'] = features_actual
                            trial_params['features'] = features_actual
                            # Also update encoding_dim if it's now too large (respecting search space constraints)
                            if trial_params['encoding_dim'] > features_actual:
                                trial_params['encoding_dim'] = max(4, features_actual // 4)
                    except Exception as e:
                        logger.warning(f"Failed to load real data: {e}, using synthetic data")
                        use_real_data_local = False
                        trial_params['use_real_data'] = False
                
                if not use_real_data_local:
                    # Generate synthetic data
                    data = generate_synthetic_data(
                        normal_samples=normal_samples,
                        attack_samples=attack_samples,
                        features=features,
                        validation_split=0.1,
                        random_state=cv_random_state,
                        silent=True,
                        config=final_config
                    )
                    X_train_orig = data['X_train']
                    X_val_orig = data.get('X_val', np.array([]))
                    
                    # Combine train and validation data, but only keep normal samples
                    if len(X_val_orig) > 0:
                        X_combined = np.vstack([X_train_orig, X_val_orig])
                    else:
                        X_combined = X_train_orig
                    
                    # For autoencoders, we only use normal data (label 0)
                    y_combined = data.get('y_train', np.zeros(len(X_train_orig)))
                    if len(X_val_orig) > 0:
                        y_val = data.get('y_val', np.zeros(len(X_val_orig)))
                        y_combined = np.concatenate([y_combined, y_val])
                    
                    # Filter to only normal samples (label 0) for autoencoder training
                    normal_mask = y_combined == 0
                    X_data = X_combined[normal_mask]
                    
                    # Ensure we have enough data
                    if len(X_data) < cv_folds * 50:
                        logger.warning(f"Not enough normal samples ({len(X_data)}) for CV, generating more")
                        data = generate_synthetic_data(
                            normal_samples=max(normal_samples, cv_folds * 100),
                            attack_samples=attack_samples,
                            features=features,
                            validation_split=0.1,
                            random_state=cv_random_state,
                            silent=True,
                            config=final_config
                        )
                        X_train_orig = data['X_train']
                        X_val_orig = data.get('X_val', np.array([]))
                        y_train_orig = data.get('y_train', np.zeros(len(X_train_orig)))
                        y_val_orig = data.get('y_val', np.zeros(len(X_val_orig))) if len(X_val_orig) > 0 else np.array([])
                        
                        # Combine and filter normal samples
                        if len(X_val_orig) > 0:
                            X_combined = np.vstack([X_train_orig, X_val_orig])
                            y_combined = np.concatenate([y_train_orig, y_val_orig])
                        else:
                            X_combined = X_train_orig
                            y_combined = y_train_orig
                        
                        normal_mask = y_combined == 0
                        X_data = X_combined[normal_mask]
                
                # Perform cross-validation
                if cv_folds == 1:
                    # Use a single train/validation split when cv_folds=1
                    from sklearn.model_selection import train_test_split
                    train_idx, val_idx = train_test_split(
                        np.arange(len(X_data)),
                        test_size=0.2,
                        random_state=cv_random_state,
                        shuffle=cv_shuffle
                    )
                    fold_indices = [(train_idx, val_idx)]
                else:
                    # Use k-fold cross-validation for cv_folds >= 2
                    kf = KFold(n_splits=cv_folds, shuffle=cv_shuffle, random_state=cv_random_state)
                    fold_indices = list(kf.split(X_data))

                fold_scores = []

                for fold_idx, (train_idx, val_idx) in enumerate(fold_indices):
                    try:
                        trial.set_user_attr('current_fold', f"{fold_idx + 1}/{len(fold_indices)}")
                        
                        # Create fold-specific data
                        X_fold_train = X_data[train_idx]
                        X_fold_val = X_data[val_idx]
                        
                        # Ensure minimum fold size
                        if len(X_fold_train) < 10 or len(X_fold_val) < 5:
                            logger.warning(f"Fold {fold_idx} too small: train={len(X_fold_train)}, val={len(X_fold_val)}")
                            fold_scores.append(float('inf'))
                            continue
                        
                        # Create temporary directory for this fold
                        fold_dir = study_dir / f"trial_{trial.number}" / f"fold_{fold_idx}"
                        fold_dir.mkdir(parents=True, exist_ok=True)
                        
                        # Create fold_params with all required parameters
                        fold_params = trial_params.copy()
                        
                        # Add fold-specific parameters with safe defaults
                        fold_params.update({
                            'model_dir': fold_dir,
                            'log_dir': fold_dir / "logs",
                            'tensorboard_dir': fold_dir / "tensorboard",
                            'tb_dir': fold_dir / "tensorboard",
                            # Override data parameters - use actual fold sizes
                            'normal_samples': len(X_fold_train),
                            'attack_samples': 0,
                            'use_real_data': False,
                            # Use minimal settings for speed
                            'save_checkpoints': False,
                            'checkpoint_frequency': 999999,
                            'log_frequency': 999999,
                            'metrics_frequency': 999999,
                            # Ensure critical parameters are present (redundant but safe)
                            'input_dim': trial_params['input_dim'],
                            'features': trial_params['features'],
                            'model_type': trial_params['model_type']
                        })
                        
                        # Create fold data in the format expected by train_model
                        fold_data = {
                            'X_train': X_fold_train,
                            'X_val': X_fold_val,
                            'X_test': X_fold_val,
                            'y_train': np.zeros(len(X_fold_train)),
                            'y_val': np.zeros(len(X_fold_val)),
                            'y_test': np.zeros(len(X_fold_val)),
                            'feature_names': data.get('feature_names', [f'feature_{i}' for i in range(X_fold_train.shape[1])])
                        }
                        
                        # Create config for train_model
                        fold_config = {
                            'model': {
                                'model_type': fold_params['model_type'],
                                'input_dim': fold_params['input_dim'],
                                'encoding_dim': fold_params['encoding_dim'],
                                'hidden_dims': fold_params['hidden_dims'],
                                'dropout_rates': fold_params['dropout_rates'],
                                'activation': fold_params['activation'],
                                'normalization': fold_params['normalization'],
                                'skip_connection': fold_params['skip_connection'],
                                'residual_blocks': fold_params['residual_blocks'],
                                'use_attention': fold_params['use_attention'],
                                'legacy_mode': fold_params['legacy_mode'],
                                'num_models': fold_params.get('num_models'),
                                'diversity_factor': fold_params.get('diversity_factor'),
                                'weight_init': 'xavier_uniform',
                                'bias': True,
                                'min_features': fold_params.get('min_features', 5)
                            },
                            
                            'training': {
                                'batch_size': fold_params['batch_size'],
                                'epochs': fold_params['epochs'],
                                'learning_rate': fold_params['learning_rate'],
                                'patience': fold_params['patience'],
                                'weight_decay': fold_params['weight_decay'],
                                'gradient_clip': fold_params['gradient_clip'],
                                'gradient_accumulation_steps': fold_params['gradient_accumulation_steps'],
                                'mixed_precision': fold_params['mixed_precision'],
                                'optimizer_type': fold_params['optimizer_type'],
                                'scheduler_type': fold_params['scheduler_type'],
                                'scheduler_params': fold_params['scheduler_params'],
                                'early_stopping': fold_params['early_stopping'],
                                'validation_split': fold_params['validation_split']
                            },
                            
                            'data': {
                                'use_real_data': False,
                                'features': fold_params['features'],
                                'normal_samples': len(X_fold_train),
                                'attack_samples': attack_samples,
                                'data_preprocessing': fold_params['data_preprocessing'],
                                'normalization_method': 'standard'
                            },
                            
                            'system': {
                                'device': fold_params['device'],
                                'random_seed': fold_params['random_seed'],
                                'reproducible': fold_params['reproducible'],
                                'model_dir': fold_dir,
                                'log_dir': fold_dir / "logs"
                            },
                            
                            'monitoring': {
                                'verbose': fold_params['verbose'],
                                'debug_mode': fold_params['debug_mode'],
                                'tensorboard_logging': fold_params['tensorboard_logging'],
                                'save_checkpoints': fold_params['save_checkpoints'],
                                'progress_bar': fold_params['progress_bar'],
                                'log_frequency': 999999
                            },
                            
                            'export': {
                                'export_onnx': fold_params['export_onnx'],
                                'save_model': fold_params['save_model'],
                                'save_metadata': fold_params['save_metadata'],
                                'save_training_history': fold_params['save_training_history']
                            },
                            
                            'advanced_training': {
                                'num_workers': fold_params['num_workers'],
                                'pin_memory': fold_params['pin_memory'],
                                'persistent_workers': fold_params['persistent_workers'],
                                'memory_efficient': fold_params['memory_efficient']
                            },
                            
                            'error_handling': {
                                'error_handling': fold_params['error_handling'],
                                'graceful_degradation': fold_params['graceful_degradation'],
                                'continue_on_error': fold_params['continue_on_error']
                            },
                            
                            'security': {
                                'percentile': fold_params['percentile'],
                                'threshold_method': fold_params['threshold_method'],
                                'enable_security_metrics': fold_params['enable_security_metrics']
                            },
                            
                            'hpo_fold_data': fold_data,
                            'hpo_trial_number': trial.number,
                            'hpo_fold_number': fold_idx,
                            'hpo_direct_data': True
                        }
                        
                        # Prepare parameters for train_model
                        safe_params = {}
                        excluded_keys = ['model_dir', 'log_dir', 'tensorboard_dir', 'tb_dir', 'normal_samples', 'attack_samples', 'use_real_data']
                        
                        for key, value in fold_params.items():
                            if key not in excluded_keys:
                                safe_params[key] = value
                        
                        # Train model using the train_model function
                        result = train_model(
                            config=fold_config,
                            silent=True,
                            X_train=X_fold_train,
                            X_val=X_fold_val,
                            X_test=X_fold_val,
                            y_train=np.zeros(len(X_fold_train)),
                            y_val=np.zeros(len(X_fold_val)),
                            y_test=np.zeros(len(X_fold_val)),
                            **safe_params
                        )
                        
                        # Validation extraction
                        val_loss = float('inf')
                        
                        if result and isinstance(result, dict):
                            # Check if training was successful
                            if result.get('success', False):
                                # Try multiple paths to extract validation loss
                                final_metrics = result.get('final_metrics', {})
                                
                                # Priority order for extracting validation loss
                                val_loss_candidates = [
                                    final_metrics.get('best_validation_loss'),
                                    final_metrics.get('validation_loss'),
                                    final_metrics.get('final_validation_loss'),
                                    final_metrics.get('val_loss'),
                                    final_metrics.get('test_loss'),
                                ]
                                
                                # Also check training_history for last validation loss
                                training_history = result.get('training_history', {})
                                if isinstance(training_history, dict):
                                    val_losses = training_history.get('val_loss', [])
                                    if val_losses and len(val_losses) > 0:
                                        # Get the minimum validation loss from history
                                        val_loss_candidates.append(min(val_losses))
                                
                                # Find first valid (non-inf, non-nan) loss value
                                for candidate in val_loss_candidates:
                                    if candidate is not None and not np.isnan(candidate) and not np.isinf(candidate):
                                        val_loss = float(candidate)
                                        break
                                
                                # If still inf, try to calculate reconstruction loss directly
                                if val_loss == float('inf') or np.isnan(val_loss):
                                    logger.warning(f"Trial {trial.number}, Fold {fold_idx}: No valid validation loss found in result")
                                    # Try to get reconstruction error as fallback
                                    reconstruction_error = final_metrics.get('reconstruction_error')
                                    if reconstruction_error is not None and not np.isnan(reconstruction_error) and not np.isinf(reconstruction_error):
                                        val_loss = float(reconstruction_error)
                                        logger.info(f"Trial {trial.number}, Fold {fold_idx}: Using reconstruction_error as fallback: {val_loss}")
                            else:
                                error_msg = result.get('error', 'Training failed')
                                logger.warning(f"Trial {trial.number}, Fold {fold_idx}: Training unsuccessful: {error_msg}")
                        else:
                            logger.warning(f"Trial {trial.number}, Fold {fold_idx}: Invalid result object returned")
                        
                        # Store the loss (even if inf, we need to track it)
                        fold_scores.append(val_loss)
                        
                        # Store fold-specific attributes with additional diagnostics
                        trial.set_user_attr(f'fold_{fold_idx}_val_loss', val_loss)
                        trial.set_user_attr(f'fold_{fold_idx}_success', val_loss != float('inf'))
                        
                        if result and isinstance(result, dict):
                            test_loss = result.get('final_metrics', {}).get('test_loss', float('inf'))
                            training_time = result.get('training_time_minutes', 0)
                            trial.set_user_attr(f'fold_{fold_idx}_test_loss', test_loss)
                            trial.set_user_attr(f'fold_{fold_idx}_training_time', training_time)
                        
                        # Log validation loss for debugging
                        if val_loss != float('inf'):
                            logger.info(f"Trial {trial.number}, Fold {fold_idx}: Validation loss = {val_loss:.6f}")
                        else:
                            logger.warning(f"Trial {trial.number}, Fold {fold_idx}: Failed to obtain valid validation loss")
                        
                        # Report intermediate value for pruning (after first fold)
                        if fold_idx == 0 and fold_scores[-1] != float('inf'):
                            trial.report(fold_scores[0], fold_idx)
                            if trial.should_prune():
                                raise optuna.TrialPruned()
                        
                        # Cleanup fold directory
                        try:
                            shutil.rmtree(fold_dir, ignore_errors=True)
                        except Exception:
                            pass
                    
                    except optuna.TrialPruned:
                        raise
                    except Exception as e:
                        logger.error(f"Trial {trial.number}, Fold {fold_idx} error: {str(e)}")
                        logger.error(f"Traceback: {traceback.format_exc()}")
                        fold_scores.append(float('inf'))
                
                # Calculate final score with better handling
                if fold_scores and any(score != float('inf') for score in fold_scores):
                    # Filter out infinite scores for statistics
                    valid_scores = [score for score in fold_scores if score != float('inf') and not np.isnan(score)]
                    if valid_scores:
                        mean_score = np.mean(valid_scores)
                        std_score = np.std(valid_scores)
                        
                        # Log success
                        logger.info(f"Trial {trial.number}: Mean CV score = {mean_score:.6f} ± {std_score:.6f} ({len(valid_scores)}/{len(fold_scores)} valid folds)")
                    else:
                        mean_score = float('inf')
                        std_score = 0.0
                        logger.warning(f"Trial {trial.number}: All folds returned invalid scores")
                else:
                    mean_score = float('inf')
                    std_score = 0.0
                    logger.warning(f"Trial {trial.number}: No valid scores obtained from any fold")
                
                # Store trial results
                trial.set_user_attr('mean_cv_score', mean_score)
                trial.set_user_attr('std_cv_score', std_score)
                trial.set_user_attr('individual_fold_scores', fold_scores)
                trial.set_user_attr('valid_folds', len([s for s in fold_scores if s != float('inf') and not np.isnan(s)]))
                trial.set_user_attr('trial_parameters', trial_params)
                trial.set_user_attr('complete_config', fold_config)
                
                return mean_score
            
            except optuna.TrialPruned:
                logger.info(f"Trial {trial.number} pruned")
                raise
            except Exception as e:
                logger.error(f"Trial {trial.number} failed with error: {str(e)}")
                logger.error(f"Traceback: {traceback.format_exc()}")
                trial.set_user_attr('error', str(e))
                trial.set_user_attr('failed', True)
                return float('inf')
        
        # Set up callbacks for monitoring
        callbacks = []
        
        # Progress callback
        def progress_callback(study, trial):
            if trial.state == optuna.trial.TrialState.COMPLETE:
                if verbose:
                    print(f"Trial {trial.number:3d} complete | Value: {trial.value:.5f} | Best: {study.best_value:.5f}")
            elif trial.state == optuna.trial.TrialState.PRUNED:
                if verbose:
                    print(f"Trial {trial.number:3d} pruned")
            elif trial.state == optuna.trial.TrialState.FAIL:
                if verbose:
                    print(f"Trial {trial.number:3d} failed")
        
        callbacks.append(progress_callback)
        
        # Early stopping callback
        def early_stopping_callback(study, trial):
            if len(study.trials) >= early_stopping_min_trials:
                recent_trials = study.trials[-early_stopping_patience:]
                recent_values = [t.value for t in recent_trials if t.state == optuna.trial.TrialState.COMPLETE]
                
                if len(recent_values) >= early_stopping_patience:
                    if min(recent_values) >= study.best_value:
                        if verbose:
                            print(f"Early stopping triggered after {len(study.trials)} trials")
                        study.stop()
        
        callbacks.append(early_stopping_callback)
        
        # Update results tracking
        setup_results['objective_function'] = objective
        setup_results['callbacks'] = callbacks
        setup_results['setup_stages_completed'] = 11
        progress_data['setup_steps_completed'] = 11
        
        # STAGE 12: Final Configuration & Analysis Setup
        progress_data['current_stage'] = "Final Configuration"
        setup_results['current_stage'] = "Final Configuration"
        
        # Create study configuration
        study_config = {
            "study_name": study_name,
            "configuration": hpo_section,
            "express_context": {
                "optimization_focus": optimization_focus,
                "system_class": system_class,
                "express_intensity": express_intensity
            },
            "sampler": {
                "type": type(sampler).__name__,
                "config": sampler_config
            },
            "pruner": {
                "type": type(pruner).__name__,
                "config": pruner_config
            },
            "search_space": {
                "model_types": model_types,
                "trial_epochs": trial_epochs,
                "cv_folds": cv_folds,
                "uses_express_space": bool(express_optimization_space)
            },
            "data_config": {
                "use_real_data": use_real_data,
                "normal_samples": normal_samples,
                "attack_samples": attack_samples,
                "features": features
            },
            "optimization": {
                "n_trials": n_trials,
                "timeout_seconds": timeout_seconds,
                "direction": direction
            },
            "timestamp": datetime.now().isoformat()
        }
        
        # STAGE 12: Configuration Serialization
        progress_data['current_stage'] = "Saving Study"
        setup_results['current_stage'] = "Saving Study"
        
        # Save configuration
        config_path = study_dir / "study_config.json"
        try:
            # Create serializable configuration
            serializable_study_config = {}
            config_items_processed = 0
            total_config_items = len(study_config.items())
            
            for key, value in study_config.items():
                config_items_processed += 1
                
                if key == 'configuration':
                    # Handle configuration section carefully to avoid circular references
                    serializable_config = {}
                    config_subitems = len(value.items())
                    subitems_processed = 0
                    
                    for config_key, config_value in value.items():
                        subitems_processed += 1
                        
                        if isinstance(config_value, (str, int, float, bool, type(None))):
                            serializable_config[config_key] = config_value
                        elif isinstance(config_value, (list, tuple)):
                            serializable_config[config_key] = [
                                item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                                for item in config_value
                            ]
                        elif isinstance(config_value, dict):
                            serializable_config[config_key] = {
                                str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                                for k, v in config_value.items()
                            }
                        elif isinstance(config_value, (Path,)):
                            serializable_config[config_key] = str(config_value)
                        else:
                            serializable_config[config_key] = str(config_value)
                    
                    serializable_study_config[key] = serializable_config
                    
                elif isinstance(value, (str, int, float, bool, type(None))):
                    serializable_study_config[key] = value
                elif isinstance(value, (list, tuple)):
                    serializable_study_config[key] = [
                        item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                        for item in value
                    ]
                elif isinstance(value, dict):
                    serializable_study_config[key] = {
                        str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                        for k, v in value.items()
                    }
                elif isinstance(value, (Path,)):
                    serializable_study_config[key] = str(value)
                else:
                    serializable_study_config[key] = str(value)
            
            with open(config_path, "w") as f:
                json.dump(serializable_study_config, f, indent=2)
            
            setup_results['study_config_path'] = str(config_path)
            logger.info(f"Study configuration saved: {config_path}")
            
        except Exception as e:
            error_msg = f"Failed to save study configuration: {e}"
            logger.error(error_msg)
            setup_results['warnings'].append(error_msg)
        
        # Update results tracking
        setup_results['setup_stages_completed'] = 12
        progress_data['setup_steps_completed'] = 12
        
        # STAGE 13: Optimization Function Setup
        progress_data['current_stage'] = "Optimization Function"
        setup_results['current_stage'] = "Optimization Function"

        # Define optimization function
        def run_optimization():
            """Run the optimization process."""
            
            # Start timing
            optimization_start_time = time.time()
            start_datetime = datetime.now()
            
            # Initialize optimization tracking
            optimization_stats = {
                'start_time': start_datetime.isoformat(),
                'study_name': study_name,
                'n_trials_planned': n_trials,
                'timeout_seconds': timeout_seconds,
                'completed_trials': 0,
                'pruned_trials': 0,
                'failed_trials': 0,
                'best_value_history': [],
                'trial_durations': [],
                'memory_usage': [],
                'early_stopping_triggered': False,
                'timeout_triggered': False,
                'optimization_focus': optimization_focus,
                'system_class': system_class
            }
            
            # Callback system
            enhanced_callbacks = callbacks.copy() if callbacks else []
            
            # Alive progress bar instance
            progress_bar = None
            
            # Progress bar initialization
            def initialize_progress_bar():
                """Initialize the alive_progress bar"""
                nonlocal progress_bar
                if show_progress and verbose:
                    try:
                        # Calculate timeout minutes for display
                        timeout_minutes = timeout_seconds / 60 if timeout_seconds > 0 else None
                        
                        # Prepare bar title and stats
                        bar_title = "Hyperparameter Optimization\t"
                        bar_stats = f"Planned: {n_trials} trials"
                        if timeout_minutes:
                            bar_stats += f", Timeout: {timeout_minutes:.1f} min"
                        if parallel_jobs != 1:
                            bar_stats += f", Jobs: {parallel_jobs}"
                        
                        progress_bar = alive_bar(
                            total=n_trials if n_trials > 0 else None,
                            title=bar_title,
                            bar='smooth',
                            spinner='dots_waves2',
                            stats=True,
                            force_tty=True,
                            monitor=True,
                            stats_end=True,
                            elapsed=True,
                            receipt=False
                        )
                        progress_bar.__enter__()
                        return True
                    except ImportError:
                        logger.warning("alive_progress not available, using basic progress tracking")
                        return False
                return False
            
            # Progress tracking
            def enhanced_progress_callback(study, trial):
                """Progress tracking with alive_progress integration"""
                current_time = time.time()
                elapsed_time = current_time - optimization_start_time
                
                # Update optimization stats
                optimization_stats['completed_trials'] = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])
                optimization_stats['pruned_trials'] = len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])
                optimization_stats['failed_trials'] = len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])
                
                # Track best value history
                if study.trials and study.best_trial:
                    optimization_stats['best_value_history'].append({
                        'trial_number': study.best_trial.number,
                        'value': study.best_value,
                        'timestamp': current_time,
                        'elapsed_seconds': elapsed_time
                    })
                
                # Track trial duration
                if trial.datetime_start and trial.datetime_complete:
                    duration = (trial.datetime_complete - trial.datetime_start).total_seconds()
                    optimization_stats['trial_durations'].append(duration)
                
                # Memory monitoring
                try:
                    process = psutil.Process()
                    memory_mb = process.memory_info().rss / 1024 / 1024
                    optimization_stats['memory_usage'].append({
                        'trial_number': trial.number,
                        'memory_mb': memory_mb,
                        'timestamp': current_time
                    })
                except ImportError:
                    pass
                
                # Update alive_progress bar
                if progress_bar:
                    trials_completed = optimization_stats['completed_trials']
                    trials_total = n_trials
                    
                    # Calculate statistics
                    total_trials = len(study.trials)
                    current_time_elapsed = time.time() - optimization_start_time
                    trials_per_minute = trials_completed / (current_time_elapsed / 60) if current_time_elapsed > 0 else 0
                    success_rate = (trials_completed / total_trials * 100) if total_trials > 0 else 0
                    
                    # Calculate ETA
                    avg_duration = np.mean(optimization_stats['trial_durations'][-5:]) if optimization_stats['trial_durations'] else 0
                    eta_seconds = avg_duration * (trials_total - trials_completed) if avg_duration > 0 else 0
                    
                    # Prepare progress text
                    status_text = []
                    
                    # Trial status
                    if trial.state == optuna.trial.TrialState.COMPLETE:
                        status_prefix = "[BEST] " if trial.value == study.best_value else ""
                        status_text.append(f"{status_prefix}Trial {trial.number}")
                        status_text.append(f"Value: {trial.value:.6f}")
                    elif trial.state == optuna.trial.TrialState.PRUNED:
                        status_text.append(f"Trial {trial.number} [PRUNED]")
                    elif trial.state == optuna.trial.TrialState.FAIL:
                        status_text.append(f"Trial {trial.number} [FAILED]")
                    
                    # Best value and progress
                    if study.best_value != float('inf'):
                        status_text.append(f"Best: {study.best_value:.6f}")
                    
                    # Progress statistics
                    progress_pct = (trials_completed / trials_total) * 100 if trials_total > 0 else 0
                    status_text.append(f"Progress: {trials_completed}/{trials_total} ({progress_pct:.1f}%)")
                    
                    # Rate and success information
                    status_text.append(f"Success: {success_rate:.1f}%")
                    status_text.append(f"Rate: {trials_per_minute:.1f}/min")
                    
                    # ETA information
                    if eta_seconds > 0:
                        if eta_seconds > 3600:
                            eta_str = f"{eta_seconds/3600:.1f}h"
                        elif eta_seconds > 60:
                            eta_str = f"{eta_seconds/60:.1f}m"
                        else:
                            eta_str = f"{eta_seconds:.0f}s"
                        status_text.append(f"ETA: {eta_str}")
                    
                    # Memory usage
                    if optimization_stats['memory_usage']:
                        current_memory = optimization_stats['memory_usage'][-1]['memory_mb']
                        status_text.append(f"Memory: {current_memory:.0f}MB")
                    
                    # Update progress bar text
                    progress_bar.text = ' | '.join(status_text)
                    
                    # Only advance bar for completed trials (not pruned or failed)
                    # if trial.state == optuna.trial.TrialState.COMPLETE:
                    #     progress_bar()
                
                # Detailed logging for verbose mode without alive_progress
                elif verbose and trial.state == optuna.trial.TrialState.COMPLETE:
                    trials_completed = optimization_stats['completed_trials']
                    trials_total = n_trials
                    progress_pct = (trials_completed / trials_total) * 100 if trials_total > 0 else 0
                    
                    # Color coding based on trial value
                    if trial.value == study.best_value:
                        value_color = Fore.GREEN + Style.BRIGHT
                    elif trial.value <= study.best_value * 1.1:  # Within 10% of best
                        value_color = Fore.YELLOW + Style.BRIGHT
                    else:
                        value_color = Fore.WHITE + Style.BRIGHT
                    
                    title_color = Fore.CYAN + Style.BRIGHT
                    
                    print(f"{title_color}Trial {trial.number:3d} | Value: {value_color}{trial.value:.6f}{title_color} | Best: {study.best_value:.6f} | Progress: {trials_completed}/{trials_total} ({progress_pct:.1f}%){Style.RESET_ALL}")
                    
                    # Periodic status updates
                    if trials_completed % 10 == 0 or trials_completed == trials_total:
                        avg_duration = np.mean(optimization_stats['trial_durations'][-10:]) if optimization_stats['trial_durations'] else 0
                        eta_seconds = avg_duration * (trials_total - trials_completed) if avg_duration > 0 else 0
                        eta_str = f"{eta_seconds / 60:.1f} min" if eta_seconds > 60 else f"{eta_seconds:.0f} sec"
                        
                        print(f"{Fore.MAGENTA + Style.BRIGHT}Status: {trials_completed}/{trials_total} trials | Avg trial: {avg_duration:.1f}s | ETA: {eta_str}{Style.RESET_ALL}")
                
                elif verbose and trial.state == optuna.trial.TrialState.PRUNED:
                    print(f"{Fore.YELLOW}Trial {trial.number:3d} pruned early | Best: {study.best_value:.6f}{Style.RESET_ALL}")
                
                elif verbose and trial.state == optuna.trial.TrialState.FAIL:
                    error_msg = trial.user_attrs.get('error', 'Unknown error')
                    print(f"{Fore.RED}Trial {trial.number:3d} failed: {error_msg[:50]}...{Style.RESET_ALL}")
            
            # Early stopping callback
            def enhanced_early_stopping_callback(study, trial):
                """Early stopping with adaptive patience"""
                if len(study.trials) < early_stopping_min_trials:
                    return
                
                # Adaptive patience based on optimization progress
                current_progress = len(study.trials) / n_trials if n_trials > 0 else 0
                adaptive_patience = max(
                    early_stopping_patience // 2,
                    min(early_stopping_patience * 2, int(early_stopping_patience * (1 + current_progress)))
                )
                
                # Check for improvement in recent trials
                recent_trials = study.trials[-adaptive_patience:]
                recent_complete_trials = [t for t in recent_trials if t.state == optuna.trial.TrialState.COMPLETE]
                
                if len(recent_complete_trials) >= adaptive_patience // 2:
                    recent_values = [t.value for t in recent_complete_trials]
                    best_recent = min(recent_values) if direction == 'minimize' else max(recent_values)
                    
                    # Check if no improvement in recent trials
                    if (direction == 'minimize' and best_recent >= study.best_value) or \
                       (direction == 'maximize' and best_recent <= study.best_value):
                        
                        optimization_stats['early_stopping_triggered'] = True
                        if verbose:
                            stop_msg = f"Early stopping triggered after {len(study.trials)} trials (no improvement in {len(recent_complete_trials)} trials)"
                            if progress_bar:
                                progress_bar.text = stop_msg
                            else:
                                print(f"{Fore.YELLOW + Style.BRIGHT}{stop_msg}{Style.RESET_ALL}")
                        study.stop()
            
            # Timeout monitoring callback
            def timeout_monitor_callback(study, trial):
                """Monitor timeout and provide warnings"""
                if timeout_seconds > 0:
                    elapsed = time.time() - optimization_start_time
                    remaining = timeout_seconds - elapsed
                    
                    # Warn when 10% time remaining
                    if remaining > 0 and remaining < timeout_seconds * 0.1:
                        if verbose and int(remaining) % 30 == 0:  # Warn every 30 seconds
                            warning_msg = f"Timeout warning: {remaining:.0f} seconds remaining"
                            if progress_bar:
                                progress_bar.text = warning_msg
                            else:
                                print(f"{Fore.YELLOW + Style.BRIGHT}{warning_msg}{Style.RESET_ALL}")
                    
                    # Stop when timeout reached
                    if remaining <= 0:
                        optimization_stats['timeout_triggered'] = True
                        if verbose:
                            timeout_msg = f"Timeout reached after {elapsed:.0f} seconds"
                            if progress_bar:
                                progress_bar.text = timeout_msg
                            else:
                                print(f"{Fore.YELLOW + Style.BRIGHT}{timeout_msg}{Style.RESET_ALL}")
                        study.stop()
            
            # Resource optimization callback
            def resource_optimization_callback(study, trial):
                """Optimize resources during optimization"""
                # Periodic memory cleanup
                if trial.number % 5 == 0:
                    _optimize_memory_if_needed(
                        condition=True,
                        hardware_data=hardware_data,
                        aggressive=False,
                        silent=not verbose
                    )
                
                # Adaptive resource allocation based on system class
                if system_class == "limited" and trial.number % 10 == 0:
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
            
            # Add enhanced callbacks
            enhanced_callbacks.extend([
                enhanced_progress_callback,
                enhanced_early_stopping_callback,
                timeout_monitor_callback,
                resource_optimization_callback
            ])
            
            # Display optimization header
            if verbose:
                print(Fore.MAGENTA + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION STARTED")
                print(Fore.CYAN + Style.BRIGHT + "-"*40)
                print(Fore.GREEN + Style.BRIGHT + "Study: " + Fore.YELLOW + Style.BRIGHT + f"{study_name}")
                print(Fore.GREEN + Style.BRIGHT + "Target: " + Fore.YELLOW + Style.BRIGHT + f"{n_trials} trials" + (f" or {timeout_seconds}s" if timeout_seconds > 0 else ""))
                print(Fore.GREEN + Style.BRIGHT + "Focus: " + Fore.YELLOW + Style.BRIGHT + f"{optimization_focus.title()} | System: {system_class.upper()}")
                print(Fore.GREEN + Style.BRIGHT + "Started: " + Fore.YELLOW + Style.BRIGHT + f"{start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
                
                if show_progress:
                    print(Fore.MAGENTA + Style.BRIGHT + "Progress: Using alive_progress for real-time monitoring")
                
                print(Fore.CYAN + Style.BRIGHT + "-"*40 + Style.RESET_ALL)
            
            # Initialize progress bar
            use_alive_progress = initialize_progress_bar()
            
            try:
                # Pre-optimization memory optimization
                _optimize_memory_if_needed(
                    condition=True,
                    hardware_data=hardware_data,
                    aggressive=True,
                    silent=not verbose
                )
                
                # Run optimization
                study.optimize(
                    objective,
                    n_trials=n_trials,
                    timeout=timeout_seconds if timeout_seconds > 0 else None,
                    callbacks=enhanced_callbacks,
                    gc_after_trial=True,
                    show_progress_bar=False,
                    n_jobs=parallel_jobs
                )
                
                # Close progress bar if it was used
                if progress_bar:
                    try:
                        progress_bar.__exit__(None, None, None)
                    except Exception:
                        pass
                
                # Calculate optimization statistics
                optimization_time = time.time() - optimization_start_time
                completed_trials = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])
                pruned_trials = len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])
                failed_trials = len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])
                total_trials = len(study.trials)
                success_rate = (completed_trials / total_trials) * 100 if total_trials > 0 else 0
                trials_per_minute = completed_trials / (optimization_time / 60) if optimization_time > 0 else 0
                
                # Results compilation
                optimization_results = {
                    'success': True,
                    'study': study,
                    'study_name': study_name,
                    'n_trials_completed': completed_trials,
                    'n_trials_pruned': pruned_trials,
                    'n_trials_failed': failed_trials,
                    'n_trials_total': total_trials,
                    'success_rate': success_rate,
                    'optimization_time_seconds': optimization_time,
                    'trials_per_minute': trials_per_minute,
                    'efficiency_ratio': completed_trials / total_trials if total_trials > 0 else 0,
                    'best_value': study.best_value if study.trials else float('inf'),
                    'best_params': study.best_params if study.trials else {},
                    'best_trial': study.best_trial if study.trials else None,
                    'average_trial_time_seconds': np.mean(optimization_stats['trial_durations']) if optimization_stats['trial_durations'] else 0,
                    'early_stopping_triggered': optimization_stats['early_stopping_triggered'],
                    'timeout_triggered': optimization_stats['timeout_triggered'],
                    'completion_reason': 'completed' if completed_trials >= n_trials else 'timeout' if optimization_stats['timeout_triggered'] else 'early_stopping' if optimization_stats['early_stopping_triggered'] else 'interrupted',
                    'optimization_stats': optimization_stats,
                    'end_time': datetime.now().isoformat(),
                    'configuration': {
                        'optimization_focus': optimization_focus,
                        'system_class': system_class,
                        'n_trials_planned': n_trials,
                        'timeout_seconds': timeout_seconds,
                        'parallel_jobs': parallel_jobs,
                        'used_alive_progress': use_alive_progress
                    }
                }
                
                # Display optimization summary
                if verbose:
                    if use_alive_progress:
                        # Add some space after the progress bar
                        print("\n")
                    
                    print(Fore.MAGENTA + Style.BRIGHT + "OPTIMIZATION COMPLETED")
                    print(Fore.CYAN + Style.BRIGHT + "-"*40)
                    
                    # Results summary
                    best_value = optimization_results['best_value']
                    value_color = Fore.GREEN if best_value != float('inf') else Fore.RED
                    value_display = f"{best_value:.6f}" if best_value != float('inf') else "N/A"
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Results Summary:")
                    print(Fore.GREEN + Style.BRIGHT + "  ├─ Best Value: " + value_color + Style.BRIGHT + f"{value_display}")
                    print(Fore.GREEN + Style.BRIGHT + "  ├─ Trials Completed: " + Fore.GREEN + Style.BRIGHT + f"{completed_trials}/{n_trials}")
                    print(Fore.GREEN + Style.BRIGHT + "  ├─ Success Rate: " + Fore.GREEN + Style.BRIGHT + f"{success_rate:.1f}%")
                    print(Fore.GREEN + Style.BRIGHT + "  ├─ Optimization Time: " + Fore.GREEN + Style.BRIGHT + f"{optimization_time/60:.1f} minutes")
                    print(Fore.GREEN + Style.BRIGHT + "  ├─ Avg Trial Time: " + Fore.GREEN + Style.BRIGHT + f"{optimization_results['average_trial_time_seconds']:.1f}s")
                    print(Fore.GREEN + Style.BRIGHT + "  ├─ Trials per Minute: " + Fore.GREEN + Style.BRIGHT + f"{trials_per_minute:.1f}")
                    print(Fore.GREEN + Style.BRIGHT + "  └─ Completion Reason: " + Fore.GREEN + Style.BRIGHT + f"{optimization_results['completion_reason'].replace('_', ' ').title()}")
                    
                    # Best parameters preview
                    if study.best_params:
                        print(Fore.YELLOW + Style.BRIGHT + "\nBest Parameters Preview:")
                        param_count = 0
                        for key, value in list(study.best_params.items())[:5]:  # Show first 5 params
                            param_count += 1
                            if param_count <= 5:
                                print(Fore.GREEN + Style.BRIGHT + f"  ├─ {key}: " + Fore.GREEN + Style.BRIGHT + f"{value}")
                        if len(study.best_params) > 5:
                            print(Fore.GREEN + Style.BRIGHT + f"  └─ ... and {len(study.best_params) - 5} more parameters")
                    
                    print(Fore.CYAN + Style.BRIGHT + "-"*40 + Style.RESET_ALL)
                
                return optimization_results
            
            except Exception as e:
                # Close progress bar on error
                if progress_bar:
                    try:
                        progress_bar.__exit__(None, None, None)
                    except Exception:
                        pass
                
                # Calculate partial statistics
                optimization_time = time.time() - optimization_start_time
                completed_trials = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) if 'study' in locals() else 0
                total_trials = len(study.trials) if 'study' in locals() else 0
                
                error_results = {
                    'success': False,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'study': study if 'study' in locals() else None,
                    'study_name': study_name,
                    'n_trials_completed': completed_trials,
                    'n_trials_total': total_trials,
                    'optimization_time_seconds': optimization_time,
                    'best_value': study.best_value if 'study' in locals() and study.trials else float('inf'),
                    'best_params': study.best_params if 'study' in locals() and study.trials else {},
                    'best_trial': study.best_trial if 'study' in locals() and study.trials else None,
                    'optimization_stats': optimization_stats,
                    'end_time': datetime.now().isoformat(),
                    'traceback': traceback.format_exc(),
                    'used_alive_progress': use_alive_progress
                }
                
                logger.error(f"Optimization failed: {str(e)}")
                logger.error(f"Traceback: {traceback.format_exc()}")
                
                if verbose:
                    if use_alive_progress:
                        print("\n")
                    print(Fore.RED + Style.BRIGHT + f"\nOptimization failed after {optimization_time/60:.1f} minutes")
                    print(Fore.RED + Style.BRIGHT + "Error: " + Fore.YELLOW + Style.BRIGHT + f"{str(e)}")
                    if completed_trials > 0:
                        print(Fore.YELLOW + Style.BRIGHT + f"Partial results available from {completed_trials} completed trials")
                
                return error_results
            
            finally:
                # Ensure progress bar is closed
                if progress_bar:
                    try:
                        progress_bar.__exit__(None, None, None)
                    except Exception:
                        pass
                
                # Final cleanup
                try:
                    _optimize_memory_if_needed(
                        condition=True,
                        hardware_data=hardware_data,
                        aggressive=True,
                        silent=not verbose
                    )
                    
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    gc.collect()
                except Exception:
                    pass
        
        # Update results tracking
        setup_results['run_optimization'] = run_optimization
        setup_results['setup_stages_completed'] = 13
        progress_data['setup_steps_completed'] = 13
        
        # STAGE 14: Analysis Function Setup
        progress_data['current_stage'] = "Analysis Function"
        setup_results['current_stage'] = "Analysis Function"
        
        # Results analysis function
        def analyze_results():
            """Analyze optimization results and generate reports"""
            if not study.trials:
                return {'error': 'No trials completed'}
            
            try:
                best_trial = study.best_trial
                
                analysis = {
                    'study_summary': {
                        'study_name': study_name,
                        'n_trials': len(study.trials),
                        'n_complete_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
                        'n_pruned_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),
                        'n_failed_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]),
                        'best_value': study.best_value,
                        'best_trial_number': best_trial.number,
                        'optimization_direction': direction
                    },
                    'best_trial': {
                        'number': best_trial.number,
                        'value': best_trial.value,
                        'params': best_trial.params,
                        'user_attrs': best_trial.user_attrs
                    },
                    'parameter_importance': {},
                    'configuration': study_config,
                    'express_context': setup_results['express_context'],
                    'timestamp': datetime.now().isoformat()
                }
                
                # Calculate parameter importance if enough trials
                if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 10:
                    try:
                        importance = optuna.importance.get_param_importances(study)
                        analysis['parameter_importance'] = importance
                    except Exception as e:
                        logger.warning(f"Failed to calculate parameter importance: {e}")
                
                # Save analysis
                if save_study:
                    analysis_path = study_dir / f"{study_name}_analysis.json"
                    try:
                        # Create serializable analysis to avoid circular references
                        serializable_analysis = {}
                        analysis_items_processed = 0
                        total_analysis_items = len(analysis.items())
                        
                        for key, value in analysis.items():
                            analysis_items_processed += 1
                            
                            if key == 'configuration':
                                # Skip complex configuration in analysis
                                serializable_analysis[key] = 'configuration_saved_separately'
                            elif key == 'best_trial':
                                # Handle best_trial carefully
                                serializable_best_trial = {}
                                for trial_key, trial_value in value.items():
                                    if trial_key == 'user_attrs':
                                        # Skip user_attrs to avoid circular references
                                        serializable_best_trial[trial_key] = 'user_attrs_not_included'
                                    elif isinstance(trial_value, (str, int, float, bool, type(None))):
                                        serializable_best_trial[trial_key] = trial_value
                                    elif isinstance(trial_value, (list, tuple)):
                                        serializable_best_trial[trial_key] = [
                                            item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                                            for item in trial_value
                                        ]
                                    elif isinstance(trial_value, dict):
                                        serializable_best_trial[trial_key] = {
                                            str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                                            for k, v in trial_value.items()
                                        }
                                    else:
                                        serializable_best_trial[trial_key] = str(trial_value)
                                serializable_analysis[key] = serializable_best_trial
                            elif isinstance(value, (str, int, float, bool, type(None))):
                                serializable_analysis[key] = value
                            elif isinstance(value, (list, tuple)):
                                serializable_analysis[key] = [
                                    item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                                    for item in value
                                ]
                            elif isinstance(value, dict):
                                serializable_analysis[key] = {
                                    str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                                    for k, v in value.items()
                                }
                            else:
                                serializable_analysis[key] = str(value)
                        
                        with open(analysis_path, "w") as f:
                            json.dump(serializable_analysis, f, indent=2)
                        analysis['analysis_path'] = str(analysis_path)
                        
                        logger.info(f"Analysis saved: {analysis_path.name}")

                    except Exception as save_error:
                        logger.warning(f"Failed to save analysis: {save_error}")
                
                return analysis
            
            except Exception as e:
                logger.error(f"Results analysis failed: {str(e)}")
                return {'error': str(e)}
        
        # Update results tracking
        setup_results['analyze_results'] = analyze_results
        setup_results['setup_stages_completed'] = 14
        progress_data['setup_steps_completed'] = 14
        
        # STAGE 15: Plot Generation Setup
        progress_data['current_stage'] = "Plot Generation"
        setup_results['current_stage'] = "Plot Generation"
        
        # Plot generation function
        def generate_plots():
            """Generate optimization plots"""
            if not study.trials or not generate_plots:
                return {}
            
            try:
                plot_dir = study_dir / "plots"
                plot_dir.mkdir(exist_ok=True)
                
                plots = {}
                plots_generated = 0
                total_plot_types = 4  # We have 4 types of plots to generate
                
                # Optimization history
                try:
                    fig = vis.plot_optimization_history(study)
                    plot_path = plot_dir / "optimization_history.html"
                    fig.write_html(plot_path)
                    plots['optimization_history'] = str(plot_path)
                    plots_generated += 1
                except Exception as e:
                    logger.warning(f"Failed to generate optimization history plot: {e}")
                
                # Parameter importances
                if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 10:
                    try:
                        fig = vis.plot_param_importances(study)
                        plot_path = plot_dir / "param_importances.html"
                        fig.write_html(plot_path)
                        plots['param_importances'] = str(plot_path)
                        plots_generated += 1
                    except Exception as e:
                        logger.warning(f"Failed to generate parameter importance plot: {e}")
                
                # Parallel coordinate plot
                if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 5:
                    try:
                        fig = vis.plot_parallel_coordinate(study)
                        plot_path = plot_dir / "parallel_coordinate.html"
                        fig.write_html(plot_path)
                        plots['parallel_coordinate'] = str(plot_path)
                        plots_generated += 1
                    except Exception as e:
                        logger.warning(f"Failed to generate parallel coordinate plot: {e}")
                
                # Slice plot
                if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 5:
                    try:
                        fig = vis.plot_slice(study)
                        plot_path = plot_dir / "slice_plot.html"
                        fig.write_html(plot_path)
                        plots['slice_plot'] = str(plot_path)
                        plots_generated += 1
                    except Exception as e:
                        logger.warning(f"Failed to generate slice plot: {e}")
                
                if verbose and plots:
                    print(f"Plots saved to: {plot_dir}")
                
                return plots
            
            except Exception as e:
                logger.error(f"Plot generation failed: {str(e)}")
                return {'error': str(e)}
        
        # Update results tracking
        setup_results['generate_plots'] = generate_plots
        setup_results['setup_stages_completed'] = 15
        progress_data['setup_steps_completed'] = 15
        
        # STAGE 16: Study Data Persistence Setup
        progress_data['current_stage'] = "Data Persistence"
        setup_results['current_stage'] = "Data Persistence"
        
        # Save study function
        def save_study_data():
            """Save study data and metadata"""
            if not save_study:
                return {}
            
            try:
                saved_files = {}
                
                # Save study object
                study_path = study_dir / f"{study_name}_study.pkl"
                joblib.dump(study, study_path)
                saved_files['study_path'] = str(study_path)
                
                # Save study trials data
                trials_data = []
                total_trials = len(study.trials)
                trials_processed = 0
                
                for trial in study.trials:
                    trials_processed += 1
                    
                    trial_data = {
                        'number': trial.number,
                        'state': trial.state.name,
                        'value': trial.value,
                        'params': trial.params,
                        'user_attrs': trial.user_attrs,
                        'datetime_start': trial.datetime_start.isoformat() if trial.datetime_start else None,
                        'datetime_complete': trial.datetime_complete.isoformat() if trial.datetime_complete else None
                    }
                    trials_data.append(trial_data)
                
                trials_path = study_dir / f"{study_name}_trials.json"
                try:
                    # Create serializable trials data
                    serializable_trials_data = []
                    total_trial_data = len(trials_data)
                    data_processed = 0
                    
                    for trial_data in trials_data:
                        data_processed += 1
                        
                        serializable_trial = {}
                        for key, value in trial_data.items():
                            if key == 'user_attrs':
                                # Handle user_attrs carefully to avoid circular references
                                serializable_user_attrs = {}
                                for attr_key, attr_value in value.items():
                                    if isinstance(attr_value, (str, int, float, bool, type(None))):
                                        serializable_user_attrs[attr_key] = attr_value
                                    elif isinstance(attr_value, (list, tuple)):
                                        serializable_user_attrs[attr_key] = [
                                            item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                                            for item in attr_value
                                        ]
                                    elif isinstance(attr_value, dict):
                                        serializable_user_attrs[attr_key] = {
                                            str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                                            for k, v in attr_value.items()
                                        }
                                    else:
                                        serializable_user_attrs[attr_key] = str(attr_value)
                                serializable_trial[key] = serializable_user_attrs
                            elif isinstance(value, (str, int, float, bool, type(None))):
                                serializable_trial[key] = value
                            elif isinstance(value, (list, tuple)):
                                serializable_trial[key] = [
                                    item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                                    for item in value
                                ]
                            elif isinstance(value, dict):
                                serializable_trial[key] = {
                                    str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                                    for k, v in value.items()
                                }
                            else:
                                serializable_trial[key] = str(value)
                        serializable_trials_data.append(serializable_trial)
                    
                    with open(trials_path, "w") as f:
                        json.dump(serializable_trials_data, f, indent=2)
                    saved_files['trials_path'] = str(trials_path)
                    
                    logger.info(f"Trial data saved: {trials_path.name}")
                        
                except Exception as trials_error:
                    logger.warning(f"Failed to save trials data: {trials_error}")
                    saved_files['trials_error'] = str(trials_error)
                
                logger.info(f"Study data saved to: {study_dir}")
                
                return saved_files
            
            except Exception as e:
                logger.error(f"Failed to save study data: {str(e)}")
                return {'error': str(e)}
        
        # Calculate setup time
        setup_time = time.time() - setup_start_time
        
        # Final memory optimization
        _optimize_memory_if_needed(
            condition=True,
            hardware_data=hardware_data,
            aggressive=True,
            silent=not verbose
        )

        # Update final results tracking
        setup_results.update({
            'success': True,
            'setup_time_seconds': setup_time,
            'end_time': datetime.now().isoformat(),
            'setup_stages_completed': 16,
            'progress_data': progress_data,
            'objective_function': objective,
            'search_space_function': create_search_space,
            # Add functions at root level for direct access
            'run_optimization': run_optimization,
            'analyze_results': analyze_results,
            'generate_plots': generate_plots,
            'save_study_data': save_study_data,
            # Also keep in analysis_functions for backward compatibility
            'analysis_functions': {
                'run_optimization': run_optimization,
                'analyze_results': analyze_results,
                'generate_plots': generate_plots,
                'save_study_data': save_study_data
            },
            'callbacks': callbacks,
            'study_config': study_config,
            'study_config_path': str(config_path)
        })
        
        # Display setup completion message
        if verbose:
            print(Fore.MAGENTA + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION SETUP COMPLETED")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            print(Fore.YELLOW + Style.BRIGHT + "Setup Summary:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Setup Time: " + Fore.YELLOW + Style.BRIGHT + f"{setup_time:.1f} seconds")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Study Name: " + Fore.YELLOW + Style.BRIGHT + f"{study_name}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Study Directory: " + Fore.YELLOW + Style.BRIGHT + f"{study_dir}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Types: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(model_types)}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Trials: " + Fore.YELLOW + Style.BRIGHT + f"{n_trials}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Optimization Focus: " + Fore.YELLOW + Style.BRIGHT + f"{optimization_focus.title()}")
            
            # Show express search space usage
            if express_optimization_space:
                param_count = len(express_optimization_space)
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Express Search Space: " + Fore.GREEN + Style.BRIGHT + f"{param_count} parameters")
            
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Parallel Jobs: " + Fore.YELLOW + Style.BRIGHT + f"{parallel_jobs}")
            
            if progress_data['warnings']:
                warning_color = Fore.YELLOW if len(progress_data['warnings']) < 5 else Fore.RED
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Warnings Encountered: " + warning_color + Style.BRIGHT + f"{len(progress_data['warnings'])}")
                for warning in progress_data['warnings'][:3]:
                    print(Fore.WHITE + Style.BRIGHT + f"    - {warning}")
                if len(progress_data['warnings']) > 3:
                    print(Fore.WHITE + Style.BRIGHT + f"    ... and {len(progress_data['warnings']) - 3} more warnings")
            
            print(Fore.YELLOW + Style.BRIGHT + "\nReady to run hyperparameter optimization!")
            print(Fore.CYAN + Style.BRIGHT + "  └─ Next Steps:")
            print(Fore.WHITE + Style.BRIGHT + "    ├─ Call run_optimization() to start the optimization")
            print(Fore.WHITE + Style.BRIGHT + "    ├─ Use analyze_results() to examine results")
            print(Fore.WHITE + Style.BRIGHT + "    ├─ Call generate_plots() for visualization")
            print(Fore.WHITE + Style.BRIGHT + "    └─ Use save_study_data() to persist results")
            print(Fore.MAGENTA + Style.BRIGHT + "\n" + "-"*40 + Style.RESET_ALL)
        
        # Return setup results
        return setup_results
    
    except (EOFError, KeyboardInterrupt):
        error_msg = "HPO setup cancelled by user."
        logger.warning(error_msg)
        return None
    
    except Exception as e:
        error_msg = f"HPO setup failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Update error results tracking
        setup_results.update({
            'success': False,
            'error': error_msg,
            'error_type': type(e).__name__,
            'end_time': datetime.now().isoformat(),
            'setup_time_seconds': time.time() - setup_start_time,
            'traceback': traceback.format_exc(),
            'progress_data': progress_data
        })
        
        # Save error information
        if save_study:
            try:
                error_path = study_dir / f"{study_name}_setup_error.json"
                study_dir.mkdir(parents=True, exist_ok=True)
                
                # Create serializable error info to avoid circular references
                serializable_error_info = {}
                for key, value in setup_results.items():
                    if key == 'configuration':
                        # Skip complex configuration in error info
                        serializable_error_info[key] = 'configuration_not_included_in_error_file'
                    elif key == 'progress_data':
                        # Handle progress_data carefully
                        serializable_progress_data = {}
                        for progress_key, progress_value in value.items():
                            if isinstance(progress_value, (str, int, float, bool, type(None))):
                                serializable_progress_data[progress_key] = progress_value
                            elif isinstance(progress_value, (list, tuple)):
                                serializable_progress_data[progress_key] = [
                                    item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                                    for item in progress_value
                                ]
                            elif isinstance(progress_value, dict):
                                serializable_progress_data[progress_key] = {
                                    str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                                    for k, v in progress_value.items()
                                }
                            else:
                                serializable_progress_data[progress_key] = str(progress_value)
                        serializable_error_info[key] = serializable_progress_data
                    elif isinstance(value, (str, int, float, bool, type(None))):
                        serializable_error_info[key] = value
                    elif isinstance(value, (list, tuple)):
                        serializable_error_info[key] = [
                            item if isinstance(item, (str, int, float, bool, type(None))) else str(item)
                            for item in value
                        ]
                    elif isinstance(value, dict):
                        serializable_error_info[key] = {
                            str(k): v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
                            for k, v in value.items()
                        }
                    else:
                        serializable_error_info[key] = str(value)
                
                with open(error_path, 'w') as f:
                    json.dump(serializable_error_info, f, indent=2)
                setup_results['error_log_path'] = str(error_path)
            except Exception as save_error:
                logger.warning(f"Failed to save error log: {save_error}")
        
        return setup_results
    
    finally:
        # Restore logging level
        if verbose and 'original_level' in locals():
            try:
                logger.setLevel(original_level)
            except Exception:
                pass
        
        # Final cleanup
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        except Exception:
            pass

def run_hyperparameter_optimization(
    # Core HPO Parameters
    n_trials: Optional[int] = None,
    timeout_minutes: Optional[float] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    
    # Data Parameters
    use_real_data: Optional[bool] = None,
    data_path: Optional[Union[str, Path]] = None,
    artifacts_path: Optional[Union[str, Path]] = None,
    normal_samples: Optional[int] = None,
    attack_samples: Optional[int] = None,
    features: Optional[int] = None,
    
    # Model Selection
    model_types: Optional[List[str]] = None,
    search_all_models: Optional[bool] = None,
    
    # Optimization Configuration
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    cv_folds: Optional[int] = None,
    trial_epochs: Optional[int] = None,
    
    # System Parameters
    device: Optional[str] = None,
    random_seed: Optional[int] = None,
    num_workers: Optional[int] = None,
    
    # Output Parameters
    verbose: Optional[bool] = None,
    show_progress: Optional[bool] = None,
    interactive: Optional[bool] = None,
    save_study: Optional[bool] = None,
    study_dir: Optional[Union[str, Path]] = None,
    generate_plots: Optional[bool] = None,
    
    # Training Parameters
    train_best_model: Optional[bool] = None,
    save_best_config: Optional[bool] = None,
    
    # Express Setup Compatibility Parameters
    express_context: Optional[Dict[str, Any]] = None,
    optimization_focus: Optional[str] = None,
    system_class: Optional[str] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    hpo_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Run comprehensive hyperparameter optimization with full express setup compatibility.
    
    Enhanced to fully support express setup configurations including:
    - Optimization focus strategies (balanced/speed/accuracy/efficiency)
    - System-aware resource allocation
    - Express search space configurations
    - Hardware-aware optimizations
    - Preset compatibility
    
    Returns:
        Dictionary containing optimization results, best parameters, and study information, or None if cancelled.
    """
    
    # Start timing
    start_time = datetime.now()
    hpo_start_time = time.time()
    
    # Initialize configuration
    if config is None:
        try:
            # Use the interactive setup function to get configuration
            interactive = True
            config = run_hyperparameter_optimization_interactive(
                use_current_config=False,
                non_interactive=False,
                operation_mode='express',
                force_express=False,
                skip_prompt=False
            )
            
            # If interactive setup was cancelled, return None
            if config is None:
                print("Hyperparameter optimization cancelled.")
                return None
            
            # Check if interactive setup returned a configuration
            if isinstance(config, dict):
                # If it returned a configuration, use it
                if 'hyperparameter_optimization' in config or 'config' in config:
                    # This is a configuration object
                    if 'config' in config:
                        # Extract the actual config from wrapper
                        config = config['config']
                    else:
                        config = config
                    
                    print(Fore.GREEN + Style.BRIGHT + "\nInteractive configuration completed successfully!")
                    print(Fore.CYAN + Style.BRIGHT + "Proceeding with optimized HPO configuration...")
                    
                elif config.get('success') is not None:
                    # If interactive setup directly launched HPO and returned results, return them
                    print(Fore.YELLOW + Style.BRIGHT + "\nHPO already completed in interactive mode!")
                    return config
                else:
                    # Unknown return type, use as configuration
                    print(Fore.YELLOW + Style.BRIGHT + "\nUsing configuration returned from interactive setup...")
                    config = config
            else:
                # Interactive setup returned something unexpected
                print(Fore.YELLOW + Style.BRIGHT + "\nUnexpected return from interactive setup, using original configuration")
                config = {}
        except Exception:
            config = {}
    else:
        if config is None:
            try:
                config = get_current_config() if 'get_current_config' in globals() else {}
            except Exception:
                config = {}
    
    # Apply HPO-specific configuration
    if hpo_config:
        config.setdefault('hyperparameter_optimization', {}).update(hpo_config)
    
    # Apply all parameters to configuration
    final_config = config.copy()
    
    # Apply individual parameters
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {'config', 'hpo_config', 'kwargs', 'start_time', 'hpo_start_time', 'datetime', 'traceback', 'time', 'gc', 'warnings', 'Path'}
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Set up defaults with proper None handling
    hpo_section = final_config.setdefault('hyperparameter_optimization', {})
    
    # Extract express setup configuration if available
    express_config = hpo_section.get('express_setup', {})
    if express_context:
        express_config.update(express_context)
    
    # Apply optimization focus from express setup
    optimization_focus = (hpo_section.get('optimization_focus') or cleaned_params.get('optimization_focus') or express_config.get('focus', 'speed'))
    hpo_section['optimization_focus'] = optimization_focus
    
    # Apply system class from express setup
    system_class = (hpo_section.get('system_class') or cleaned_params.get('system_class') or express_config.get('system_class', 'standard'))
    hpo_section['system_class'] = system_class
    
    # Apply express intensity if available
    express_intensity = express_config.get('intensity', 'Standard')
    hpo_section['express_intensity'] = express_intensity
    
    # Apply express parallel trials if available
    express_parallel_trials = hpo_section.get('parallel_trials')
    
    # Core parameters with intelligent defaults and None checks
    n_trials = hpo_section.setdefault('n_trials', cleaned_params.get('n_trials', 50))
    
    # Proper handling of timeout_minutes with None checking
    timeout_minutes_param = cleaned_params.get('timeout_minutes')
    if timeout_minutes_param is not None:
        try:
            timeout_minutes = float(timeout_minutes_param)
        except (TypeError, ValueError):
            logger.warning(f"Invalid timeout_minutes value: {timeout_minutes_param}, using default")
            timeout_minutes = 0.0
    else:
        timeout_minutes = hpo_section.get('timeout_minutes', 0.0)
    
    # Ensure timeout_minutes is always a valid float
    if timeout_minutes is None:
        timeout_minutes = 0.0
    
    hpo_section['timeout_minutes'] = timeout_minutes
    
    study_name = hpo_section.setdefault('study_name', cleaned_params.get('study_name', f"autoencoder_hpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"))
    direction = hpo_section.setdefault('direction', cleaned_params.get('direction', 'minimize'))
    sampler_type = hpo_section.setdefault('sampler_type', cleaned_params.get('sampler_type', 'TPE'))
    pruner_type = hpo_section.setdefault('pruner_type', cleaned_params.get('pruner_type', 'MedianPruner'))
    
    # Data parameters with None checks
    use_real_data = hpo_section.setdefault('use_real_data', cleaned_params.get('use_real_data', None))
    data_path = hpo_section.setdefault('data_path', cleaned_params.get('data_path', None))
    artifacts_path = hpo_section.setdefault('artifacts_path', cleaned_params.get('artifacts_path', None))
    
    # Ensure numeric parameters are properly handled
    normal_samples = cleaned_params.get('normal_samples')
    if normal_samples is not None:
        try:
            normal_samples = int(normal_samples)
        except (TypeError, ValueError):
            logger.warning(f"Invalid normal_samples value: {normal_samples}, using default")
            normal_samples = 10000
    else:
        normal_samples = hpo_section.get('normal_samples', 10000)
    hpo_section['normal_samples'] = normal_samples
    
    attack_samples = cleaned_params.get('attack_samples')
    if attack_samples is not None:
        try:
            attack_samples = int(attack_samples)
        except (TypeError, ValueError):
            logger.warning(f"Invalid attack_samples value: {attack_samples}, using default")
            attack_samples = 2000
    else:
        attack_samples = hpo_section.get('attack_samples', 2000)
    hpo_section['attack_samples'] = attack_samples
    
    features = cleaned_params.get('features')
    if features is not None:
        try:
            features = int(features)
        except (TypeError, ValueError):
            logger.warning(f"Invalid features value: {features}, using default")
            features = 78
    else:
        features = hpo_section.get('features', 78)
    hpo_section['features'] = features
    
    # Model selection
    model_types = hpo_section.setdefault('model_types', cleaned_params.get('model_types', ['SimpleAutoencoder', 'EnhancedAutoencoder']))
    search_all_models = hpo_section.setdefault('search_all_models', cleaned_params.get('search_all_models', False))
    
    # Optimization configuration with None checks
    cv_folds = cleaned_params.get('cv_folds')
    if cv_folds is not None:
        try:
            cv_folds = int(cv_folds)
            # Ensure cv_folds is at least 1, but handle the case properly
            if cv_folds < 1:
                logger.warning(f"cv_folds must be >= 1, got {cv_folds}. Using default: 3")
                cv_folds = 3
        except (TypeError, ValueError):
            logger.warning(f"Invalid cv_folds value: {cv_folds}, using default")
            cv_folds = 3
    else:
        cv_folds = hpo_section.get('cv_folds', 3)
    hpo_section['cv_folds'] = cv_folds
    
    trial_epochs = cleaned_params.get('trial_epochs')
    if trial_epochs is not None:
        try:
            trial_epochs = int(trial_epochs)
        except (TypeError, ValueError):
            logger.warning(f"Invalid trial_epochs value: {trial_epochs}, using default")
            trial_epochs = 20
    else:
        trial_epochs = hpo_section.get('trial_epochs', 20)
    hpo_section['trial_epochs'] = trial_epochs
    
    # System parameters with express-aware optimizations
    device = hpo_section.setdefault('device', cleaned_params.get('device', 'auto'))
    
    random_seed = cleaned_params.get('random_seed')
    if random_seed is not None:
        try:
            random_seed = int(random_seed)
        except (TypeError, ValueError):
            logger.warning(f"Invalid random_seed value: {random_seed}, using default")
            random_seed = 42
    else:
        random_seed = hpo_section.get('random_seed', 42)
    hpo_section['random_seed'] = random_seed
    
    num_workers = cleaned_params.get('num_workers')
    if num_workers is not None:
        try:
            num_workers = int(num_workers)
        except (TypeError, ValueError):
            logger.warning(f"Invalid num_workers value: {num_workers}, using default")
            num_workers = 0
    else:
        num_workers = hpo_section.get('num_workers', 0)
    
    # Apply system-aware optimizations for num_workers
    if system_class == "limited":
        num_workers = 0  # Conservative for limited systems
    elif system_class == "enterprise":
        num_workers = min(8, num_workers * 2)  # Maximize for enterprise
    
    hpo_section['num_workers'] = num_workers
    
    # Output parameters
    verbose = hpo_section.setdefault('verbose', cleaned_params.get('verbose', True))
    show_progress = hpo_section.setdefault('show_progress', cleaned_params.get('show_progress', True))
    interactive = hpo_section.setdefault('interactive', cleaned_params.get('interactive', True))
    save_study = hpo_section.setdefault('save_study', cleaned_params.get('save_study', True))
    study_dir = hpo_section.setdefault('study_dir', cleaned_params.get('study_dir', DEFAULT_MODEL_DIR / "hpo_studies"))
    generate_plots = hpo_section.setdefault('generate_plots', cleaned_params.get('generate_plots', True))
    
    # Training parameters
    train_best_model = hpo_section.setdefault('train_best_model', cleaned_params.get('train_best_model', True))
    save_best_config = hpo_section.setdefault('save_best_config', cleaned_params.get('save_best_config', True))
    
    # Apply optimization focus strategies to adjust parameters
    original_trial_epochs = trial_epochs
    
    if optimization_focus == 'speed':
        # Speed focus: reduce trial epochs for faster iterations
        trial_epochs = max(2, trial_epochs // 2)
        if verbose:
            print(Fore.YELLOW + Style.BRIGHT + f"  Speed focus: Reduced trial epochs from {original_trial_epochs} to {trial_epochs}")
            
    elif optimization_focus == 'accuracy':
        # Accuracy focus: increase trial epochs for better convergence
        trial_epochs = min(50, trial_epochs * 2)
        if verbose:
            print(Fore.YELLOW + Style.BRIGHT + f"  Accuracy focus: Increased trial epochs from {original_trial_epochs} to {trial_epochs}")
    
    # Update the configuration with focus-adjusted values
    hpo_section['trial_epochs'] = trial_epochs

    total_stages = 5  # Setup, Optimization, Analysis, Final Training, Finalization
    
    # Initialize results tracking
    hpo_results = {
        'start_time': start_time.isoformat(),
        'study_name': study_name,
        'configuration': hpo_section,
        'express_context': {
            'optimization_focus': optimization_focus,
            'system_class': system_class,
            'express_intensity': express_intensity,
            'express_config': express_config
        },
        'total_stages': total_stages,
        'stages_completed': 0,
        'current_stage': 'Starting...',
        'trials_completed': 0,
        'trials_failed': 0,
        'trials_pruned': 0,
        'best_value': float('inf'),
        'best_params': {},
        'best_config': {},
        'optimization_history': [],
        'model_performance': {},
        'errors': [],
        'warnings': [],
        'recommendations': [],
        'saved_files': {},
        'plots': {},
        'success': False,
        'total_time_seconds': 0,
        'total_time_minutes': 0,
        'stage_timings': {},
        'interactive_mode': interactive,
        'setup_successful': False,
        'optimization_successful': False,
        'analysis_successful': False,
        'final_training_successful': False
    }
    
    # Progress tracking data
    progress_data = {
        'current_stage': 'Starting...',
        'current_operation': None,
        'trials_completed': 0,
        'trials_failed': 0,
        'trials_pruned': 0,
        'best_value': float('inf'),
        'current_trial': None,
        'current_model_type': None,
        'setup_completed': False,
        'optimization_completed': False,
        'analysis_completed': False,
        'final_training_completed': False
    }
    
    try:
        # Update final configuration
        final_config['hyperparameter_optimization'] = hpo_section
        
        # Update results tracking with final configuration
        hpo_results.update({
            'configuration': hpo_section,
            'model_types_optimized': model_types,
            'data_config': {
                'use_real_data': use_real_data,
                'normal_samples': normal_samples,
                'attack_samples': attack_samples,
                'features': features,
                'cv_folds': cv_folds
            }
        })
        
        # Display HPO header
        if interactive:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION LAUNCH")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            # Show express context if available
            if express_config:
                print(Fore.YELLOW + Style.BRIGHT + "Express Setup Context:")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Optimization Focus: " + Fore.YELLOW + Style.BRIGHT + f"{optimization_focus.title()}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Intensity: " + Fore.YELLOW + Style.BRIGHT + f"{express_intensity}")
                if express_config.get('preset_alignment'):
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Preset Alignment: " + Fore.YELLOW + Style.BRIGHT + f"{express_config['preset_alignment']}")
            
            print(Fore.YELLOW + Style.BRIGHT + "Optimization Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Study Name: " + Fore.YELLOW + Style.BRIGHT + f"{study_name}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Trials: " + Fore.YELLOW + Style.BRIGHT + f"{n_trials}")
            
            if timeout_minutes > 0:
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{timeout_minutes} minutes")
            else:
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"No Timeout")
            
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Types: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(model_types)}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ CV Folds: " + Fore.YELLOW + Style.BRIGHT + f"{cv_folds}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{sampler_type}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{pruner_type}")
            
            print(Fore.CYAN + Style.BRIGHT + "\n  Data Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Data Type: " + Fore.YELLOW + Style.BRIGHT + f"{'Real Data' if use_real_data else 'Synthetic Data'}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Normal Samples: " + Fore.YELLOW + Style.BRIGHT + f"{normal_samples}")
            print(Fore.GREEN + Style.BRIGHT + f"    └─ Attack Samples: " + Fore.YELLOW + Style.BRIGHT + f"{attack_samples}")
            
            print(Fore.CYAN + Style.BRIGHT + "\n  System Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Device: " + Fore.YELLOW + Style.BRIGHT + f"{device}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Random Seed: " + Fore.YELLOW + Style.BRIGHT + f"{random_seed}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Num Workers: " + Fore.YELLOW + Style.BRIGHT + f"{num_workers}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
            print(Fore.GREEN + Style.BRIGHT + f"    └─ Trial Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{trial_epochs}")
            
            print(Fore.CYAN + Style.BRIGHT + "\n  Output Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Save Study: " + Fore.YELLOW + Style.BRIGHT + f"{save_study}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Generate Plots: " + Fore.YELLOW + Style.BRIGHT + f"{generate_plots}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Train Best Model: " + Fore.YELLOW + Style.BRIGHT + f"{train_best_model}")
            print(Fore.GREEN + Style.BRIGHT + f"    ├─ Save Best Config: " + Fore.YELLOW + Style.BRIGHT + f"{save_best_config}")
            print(Fore.GREEN + Style.BRIGHT + f"    └─ Study Directory: " + Fore.YELLOW + Style.BRIGHT + f"{study_dir}")
            
            print(Fore.GREEN + Style.BRIGHT + "\nStarting hyperparameter optimization pipeline...\n")
        
        # Memory optimization at start
        _optimize_memory_if_needed(
            condition=True,
            hardware_data=None,
            aggressive=False,
            silent=not verbose
        )
        
        # Set up hyperparameter optimization using the setup function
        timeout_seconds = timeout_minutes * 60 if timeout_minutes and timeout_minutes > 0 else 0
        
        # STAGE 1: Setup
        setup_stage_start = time.time()
        hpo_results['current_stage'] = "Setup"
        progress_data['current_stage'] = "Setup"

        hpo_setup = setup_hyperparameter_optimization(
            n_trials=n_trials,
            timeout_seconds=timeout_seconds,
            study_name=study_name,
            direction=direction,
            sampler_type=sampler_type,
            pruner_type=pruner_type,
            use_real_data=use_real_data,
            data_path=data_path,
            artifacts_path=artifacts_path,
            normal_samples=normal_samples,
            attack_samples=attack_samples,
            features=features,
            model_types=model_types,
            search_all_models=search_all_models,
            cv_folds=cv_folds,
            trial_epochs=trial_epochs,
            device=device,
            random_seed=random_seed,
            num_workers=num_workers,
            verbose=verbose,
            show_progress=show_progress,
            save_study=save_study,
            study_dir=study_dir,
            generate_plots=generate_plots,
            express_context=hpo_results['express_context'],
            optimization_focus=optimization_focus,
            system_class=system_class,
            config=final_config
        )
        
        if not hpo_setup.get('success', False):
            error_msg = hpo_setup.get('error', 'Setup failed')
            
            # Update results tracking with setup failure
            hpo_results.update({
                'success': False,
                'error': error_msg,
                'setup_successful': False,
                'stages_completed': 0,
                'stage_timings': {
                    'setup': time.time() - setup_stage_start
                }
            })
            return hpo_results
        
        # Update results tracking with successful setup
        hpo_results.update({
            'setup_successful': True,
            'stages_completed': 1,
            'study': hpo_setup.get('study'),
            'study_dir': hpo_setup.get('study_dir'),
            'hpo_setup': hpo_setup,  # Store full setup results
            'stage_timings': {
                'setup': time.time() - setup_stage_start
            }
        })
        progress_data['setup_completed'] = True

        # STAGE 2: Optimization
        optimization_stage_start = time.time()
        hpo_results['current_stage'] = "Optimization"
        progress_data['current_stage'] = "Optimization"
        
        # Run optimization with progress tracking
        optimization_results = hpo_setup['run_optimization']()
        
        if not optimization_results.get('success', False):
            error_msg = optimization_results.get('error', 'Optimization failed')
            
            # Update results tracking with optimization failure
            hpo_results.update({
                'success': False,
                'error': error_msg,
                'optimization_successful': False,
                'stages_completed': 1,  # Only setup completed
                'stage_timings': {
                    'setup': hpo_results['stage_timings']['setup'],
                    'optimization': time.time() - optimization_stage_start
                }
            })
            return hpo_results
        
        # Get study and results
        study = optimization_results['study']
        n_trials_completed = optimization_results['n_trials_completed']
        best_value = optimization_results['best_value']
        best_params = optimization_results['best_params']
        best_trial = optimization_results['best_trial']
        
        # Update results tracking with optimization success
        hpo_results.update({
            'optimization_successful': True,
            'stages_completed': 2,
            'trials_completed': n_trials_completed,
            'best_value': best_value,
            'best_params': best_params,
            'best_trial_number': best_trial.number if best_trial else None,
            'n_trials_total': len(study.trials),
            'n_trials_pruned': len([t for t in study.trials if t.state.name == 'PRUNED']),
            'n_trials_failed': len([t for t in study.trials if t.state.name == 'FAIL']),
            'stage_timings': {
                'setup': hpo_results['stage_timings']['setup'],
                'optimization': time.time() - optimization_stage_start
            }
        })
        progress_data.update({
            'trials_completed': n_trials_completed,
            'best_value': best_value,
            'optimization_completed': True
        })
        
        # STAGE 3: Analysis
        analysis_stage_start = time.time()
        hpo_results['current_stage'] = "Analysis"
        progress_data['current_stage'] = "Analysis"
        
        with alive_bar(1, title='Results Analysis\t', bar='smooth', spinner='dots') as analysis_bar:
            analysis_bar.text = "Analyzing optimization results..."
        
            analysis = hpo_setup['analyze_results']()
            
            # Generate plots
            plots = {}
            if generate_plots:
                analysis_bar.text = "Generating optimization plots..."
                plots = hpo_setup['generate_plots']()
                analysis_bar.text("Plots generated")
            
            # Update results tracking with analysis results
            hpo_results.update({
                'analysis_successful': True,
                'stages_completed': 3,
                'analysis': analysis,
                'plots': plots,
                'stage_timings': {
                    'setup': hpo_results['stage_timings']['setup'],
                    'optimization': hpo_results['stage_timings']['optimization'],
                    'analysis': time.time() - analysis_stage_start
                }
            })
            progress_data['analysis_completed'] = True
            
            # Extract best configuration for train_model integration
            if best_trial and hasattr(best_trial, 'user_attrs'):
                best_config_raw = best_trial.user_attrs.get('complete_config', {})
                if best_config_raw:
                    hpo_results['best_config_for_training'] = best_config_raw
            
            analysis_bar.text("Analysis complete")
            analysis_bar()
        
        # Calculate total time so far
        total_time = time.time() - hpo_start_time
        
        # STAGE 4: Final Training
        final_training_stage_start = time.time()
        hpo_results['current_stage'] = "Final Training"
        progress_data['current_stage'] = "Final Training"
        
        with alive_bar(1, title='Final Model Training\t', bar='smooth', spinner='dots') as training_bar:
            training_bar.text = "Checking final training configuration..."
        
            # Save study data
            if save_study:
                training_bar.text = "Saving study data..."
                saved_files = hpo_setup['save_study_data']()
                hpo_results['saved_files'] = saved_files
                training_bar.text("Study data saved")
            
            # Offer to train final model with best parameters
            final_training_results = None
            if train_best_model and n_trials_completed > 0 and hpo_results.get('best_config_for_training'):
                if interactive:
                    train_final = input(Fore.YELLOW + Style.BRIGHT + "\nTrain final model with best HPO parameters? (Y/n): " + Style.RESET_ALL).lower().strip()
                    train_final = train_final not in ('n', 'no')
                else:
                    train_final = True
                
                if train_final:
                    try:
                        training_bar.text("Training final model with optimized parameters...")
                        
                        # Use the best configuration for training
                        best_config = hpo_results['best_config_for_training'].copy()
                        
                        # Apply express optimization focus to final training
                        if optimization_focus == 'speed':
                            # Speed focus: faster training with reduced epochs
                            best_config.setdefault('training', {})['epochs'] = 10
                            best_config.setdefault('training', {})['patience'] = 5
                        elif optimization_focus == 'accuracy':
                            # Accuracy focus: more thorough training
                            best_config.setdefault('training', {})['epochs'] = 150
                            best_config.setdefault('training', {})['patience'] = 25
                        elif optimization_focus == 'efficiency':
                            # Efficiency focus: memory-optimized training
                            best_config.setdefault('training', {})['batch_size'] = min(best_config.get('training', {}).get('batch_size', 64), 64)
                        
                        # Update with full training parameters (not trial parameters)
                        training_config = best_config.setdefault('training', {})
                        training_config.update({
                            'epochs': training_config.get('epochs', 100),  # Use focus-adjusted epochs
                            'patience': training_config.get('patience', 20),  # Use focus-adjusted patience
                            'verbose': True,
                            'tensorboard_logging': True,
                            'save_checkpoints': True,
                            'progress_bar': True
                        })
                        
                        # Update monitoring for full training
                        monitoring_config = best_config.setdefault('monitoring', {})
                        monitoring_config.update({
                            'verbose': True,
                            'tensorboard_logging': True,
                            'save_checkpoints': True,
                            'save_best_model': True,
                            'progress_bar': True
                        })
                        
                        # Update export for full training
                        export_config = best_config.setdefault('export', {})
                        export_config.update({
                            'save_model': True,
                            'save_metadata': True,
                            'save_training_history': True
                        })
                        
                        # Update system config for final training
                        system_config = best_config.setdefault('system', {})
                        system_config.update({
                            'model_dir': Path(study_dir) / "final_model",
                            'log_dir': Path(study_dir) / "final_model" / "logs"
                        })
                        
                        # Add express context to final training config
                        best_config['express_context'] = hpo_results['express_context']
                        
                        # Train final model
                        final_training_results = train_model(config=best_config)
                        
                        if final_training_results and final_training_results.get('success', False):
                            hpo_results['final_model_training'] = final_training_results
                            hpo_results['final_training_successful'] = True
                            progress_data['final_training_completed'] = True
                            training_bar.text = "Final model training completed successfully!"
                        else:
                            error_msg = final_training_results.get('error', 'Unknown error') if final_training_results else 'No results'
                            training_bar.text = f"Final training failed: {error_msg}"
                            hpo_results['final_model_training'] = {'success': False, 'error': error_msg}
                            hpo_results['final_training_successful'] = False
                    
                    except Exception as e:
                        error_msg = f"Final model training failed: {str(e)}"
                        training_bar.text = f"Final training failed: {error_msg}"
                        hpo_results['final_model_training'] = {'success': False, 'error': error_msg}
                        hpo_results['final_training_successful'] = False
                else:
                    training_bar.text = "Final training skipped by user"
                    hpo_results['final_training_successful'] = None  # Skipped
            else:
                training_bar.text = "Final training not configured"
                hpo_results['final_training_successful'] = None  # Not configured
            
            training_bar()
        
        # STAGE 5: Finalization
        finalization_stage_start = time.time()
        hpo_results['current_stage'] = "Finalization"
        progress_data['current_stage'] = "Finalization"
        
        with alive_bar(1, title='Results Finalization\t', bar='smooth', spinner='dots') as finalization_bar:
            finalization_bar.text = "Finalizing HPO results and generating recommendations..."
        
            # Generate express-aware recommendations
            recommendations = []
            
            # Performance recommendations based on optimization focus
            finalization_bar.text("Analyzing performance...")
            if n_trials_completed == 0:
                recommendations.append("No trials completed successfully - check configuration and data")
            elif n_trials_completed < n_trials * 0.5:
                recommendations.append("Many trials failed - consider simplifying search space or checking system resources")
            
            if best_value != float('inf'):
                if best_value < 0.01:
                    recommendations.append("Excellent optimization results - ready for production use")
                    if optimization_focus == 'speed':
                        recommendations.append("Speed focus achieved excellent results quickly")
                    elif optimization_focus == 'accuracy':
                        recommendations.append("Accuracy focus delivered outstanding performance")
                elif best_value < 0.05:
                    recommendations.append("Good optimization results - consider additional fine-tuning")
                    if optimization_focus == 'speed':
                        recommendations.append("Speed focus provided good results efficiently")
                elif best_value < 0.1:
                    recommendations.append("Acceptable results - may benefit from longer optimization or different search space")
                else:
                    recommendations.append("High objective value - consider adjusting search space or model architecture")
                    if optimization_focus == 'accuracy':
                        recommendations.append("Try increasing trial epochs or reducing pruning for accuracy focus")
            
            finalization_bar.text("Performance analysis complete")
            finalization_bar.text("Analyzing configuration...")
            
            # Configuration recommendations
            if len(model_types) == 1:
                recommendations.append("Consider optimizing multiple model types for comparison")
            
            if cv_folds < 3 and optimization_focus == 'accuracy':
                recommendations.append("Consider using more cross-validation folds for more robust results with accuracy focus")
            
            if trial_epochs < 20 and optimization_focus == 'accuracy':
                recommendations.append("Consider increasing trial epochs for better convergence with accuracy focus")
            
            finalization_bar.text("Configuration analysis complete")
            finalization_bar.text("Analyzing resources...")

            # System-aware resource recommendations
            if system_class == "limited" and n_trials_completed > 0:
                recommendations.append("Limited system performed well - consider upgrading resources for larger searches")
            elif system_class == "enterprise" and n_trials_completed < n_trials * 0.8:
                recommendations.append("Enterprise system underutilized - consider increasing trial count or complexity")
            
            # Optimization focus-specific recommendations
            if optimization_focus == 'speed' and n_trials_completed > 0:
                recommendations.append("Speed focus completed efficiently - consider balanced focus for next optimization")
            elif optimization_focus == 'accuracy' and best_value > 0.05:
                recommendations.append("Accuracy focus may need more resources - consider increasing timeout or trial count")
            elif optimization_focus == 'efficiency':
                recommendations.append("Efficiency focus optimized resource usage - validate performance meets requirements")
            
            finalization_bar.text("Resource analysis complete")
            finalization_bar.text("Planning next steps...")

            # Next steps recommendations
            if n_trials_completed > 0:
                recommendations.append("Use best parameters for production training and validation")
                if not hpo_results.get('final_model_training'):
                    recommendations.append("Train final model with optimized parameters for deployment")
                
                # Express-specific next steps
                if express_intensity in ['Quick Scan', 'Standard Optimization']:
                    recommendations.append(f"Consider {express_intensity} for future optimizations with similar requirements")
                elif express_intensity in ['Thorough Search', 'Exhaustive Search']:
                    recommendations.append(f"{express_intensity} completed - use results for critical deployments")
            
            finalization_bar.text("Next steps planned")

            hpo_results['recommendations'] = recommendations
            
            # Save best configuration to global config if requested
            if save_best_config and hpo_results.get('best_config_for_training'):
                try:
                    best_config = hpo_results['best_config_for_training']
                    current_config = get_current_config()
                    
                    # Sanitize best_config to remove None values that should have defaults
                    def sanitize_config_values(cfg):
                        """Remove or replace problematic None values."""
                        if not isinstance(cfg, dict):
                            return cfg
                        
                        sanitized = {}
                        for key, value in cfg.items():
                            if isinstance(value, dict):
                                sanitized[key] = sanitize_config_values(value)
                            elif value is not None:
                                sanitized[key] = value
                            # Skip None values - let update_global_config use defaults
                        
                        return sanitized
                    
                    best_config_sanitized = sanitize_config_values(best_config)
                    
                    finalization_bar.text("Updating Global Config...")

                    # Merge best configuration
                    config_sections = list(best_config_sanitized.keys())
                    for section in config_sections:
                        values = best_config_sanitized[section]
                        if isinstance(values, dict):
                            current_config.setdefault(section, {}).update(values)
                        else:
                            current_config[section] = values
                        
                        finalization_bar.text(f"Updating {section}...")
                    
                    # Add HPO metadata
                    current_config.setdefault('metadata', {}).update({
                        'optimized_with_hpo': True,
                        'hpo_study_name': study_name,
                        'hpo_best_value': best_value,
                        'hpo_optimization_date': datetime.now().isoformat(),
                        'hpo_trial_number': best_trial.number if best_trial else 0,
                        'hpo_express_context': hpo_results['express_context']
                    })
                    
                    # Add express setup information if available
                    if express_config:
                        current_config['metadata']['hpo_express_setup'] = {
                            'intensity': express_intensity,
                            'optimization_focus': optimization_focus,
                            'system_class': system_class,
                            'preset_alignment': express_config.get('preset_alignment')
                        }
                    
                    update_global_config(current_config)
                    
                except Exception as e:
                    logger.warning(f"Failed to save best configuration: {e}")
                    recommendations.append("Failed to save best configuration - manually apply best parameters")
            
            # Final memory optimization
            _optimize_memory_if_needed(
                condition=True,
                hardware_data=None,
                aggressive=True,
                silent=not verbose
            )
            
            # Update final results tracking
            hpo_results.update({
                'success': True,
                'stages_completed': 5,
                'progress_data': progress_data,
                'stage_timings': {
                    'setup': hpo_results['stage_timings']['setup'],
                    'optimization': hpo_results['stage_timings']['optimization'],
                    'analysis': hpo_results['stage_timings']['analysis'],
                    'final_training': time.time() - final_training_stage_start,
                    'finalization': time.time() - finalization_stage_start
                }
            })

            finalization_bar.text("HPO pipeline complete!")
            finalization_bar()
        
        # Update total time after all stages
        total_time = time.time() - hpo_start_time
        hpo_results.update({
            'total_time_seconds': total_time,
            'total_time_minutes': total_time / 60,
            'end_time': datetime.now().isoformat()
        })
        
        # Delegate ALL display to _display_hpo_results function
        if not verbose:
            _display_hpo_results(hpo_results)
        
        return hpo_results
    
    except (EOFError, KeyboardInterrupt):
        print(Fore.RED + Style.BRIGHT + "\nHyperparameter optimization interrupted by user!")
        return {
            'success': False,
            'error': 'Interrupted by user',
            'error_type': 'KeyboardInterrupt',
            'start_time': start_time.isoformat(),
            'end_time': datetime.now().isoformat(),
            'total_time_seconds': time.time() - hpo_start_time,
            'study_name': study_name,
            'configuration': hpo_section,
            'express_context': hpo_results.get('express_context', {}),
            'progress_data': progress_data
        }
    
    except Exception as e:
        error_msg = f"Hyperparameter optimization failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Update error results tracking
        hpo_results.update({
            'success': False,
            'error': error_msg,
            'error_type': type(e).__name__,
            'end_time': datetime.now().isoformat(),
            'total_time_seconds': time.time() - hpo_start_time,
            'traceback': traceback.format_exc(),
            'progress_data': progress_data
        })
        
        # Save error information
        if save_study:
            try:
                error_dir = Path(study_dir)
                error_dir.mkdir(parents=True, exist_ok=True)
                error_path = error_dir / f"hpo_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                
                with open(error_path, 'w') as f:
                    json.dump(hpo_results, f, indent=2, default=str)
                
                hpo_results['error_log_path'] = str(error_path)
                
            except Exception as save_error:
                logger.warning(f"Failed to save error log: {save_error}")
        
        # Delegate error display to _display_hpo_results
        if not verbose:
            _display_hpo_results(hpo_results)
        
        return hpo_results
    
    finally:
        # Final cleanup
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        except Exception:
            pass

def _launch_hpo_with_config(config: Dict[str, Any], **kwargs) -> Optional[Dict[str, Any]]:
    """
    Launch hyperparameter optimization with configuration support and error handling.
    
    Enhanced to work seamlessly with express HPO setup, providing consistent
    configuration handling, error reporting, and user experience.
    """
    try:
        # Clear screen and show banner
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use banner config if no base config provided
        if config is None and banner_config is not None:
            config = banner_config
        
        # Get hardware context for system-aware display
        try:
            hardware_data = check_hardware(include_memory_usage=True)
        except Exception as e:
            logger.debug(f"Hardware detection failed: {e}")
            hardware_data = {}
        
        # Extract comprehensive context for display
        hpo_config = config.get('hyperparameter_optimization', {}) if config else {}
        data_config = config.get('data', {}) if config else {}
        system_config = config.get('system', {}) if config else {}
        training_config = config.get('training', {}) if config else {}
        model_config = config.get('model', {}) if config else {}
        monitoring_config = config.get('monitoring', {}) if config else {}
        metadata = config.get('metadata', {}) if config else {}
        presets_section = config.get('presets', {}) if config else {}
        runtime_config = config.get('runtime', {}) if config else {}
        
        # Extract setup context with multiple fallbacks
        setup_method = metadata.get('setup_method', 'unknown')
        config_source = metadata.get('config_source', 'unknown')
        preset_name = "Custom/Default"
        
        # Extract preset name with multiple fallbacks
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = metadata.get("preset_used", "Custom/Default")
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type
        model_type = model_config.get('model_type', 'Unknown')
        
        # Determine system performance class
        cuda_available = hardware_data.get('cuda', {}).get('available', False)
        memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)
        
        if cuda_available and memory_gb >= 32 and cpu_cores >= 16:
            system_class = "enterprise"
        elif cuda_available and memory_gb >= 16 and cpu_cores >= 8:
            system_class = "high-end"
        elif cuda_available and memory_gb >= 8:
            system_class = "performance"
        elif memory_gb >= 8:
            system_class = "standard"
        elif memory_gb >= 4:
            system_class = "balanced"
        else:
            system_class = "limited"
        
        # Header with context
        print(Fore.MAGENTA + Style.BRIGHT + "LAUNCHING HYPERPARAMETER OPTIMIZATION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Context Summary Section
        print(Fore.YELLOW + Style.BRIGHT + "CONTEXT SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Setup Method: " + Fore.YELLOW + Style.BRIGHT + f"{setup_method.replace('_', ' ').title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Configuration Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Active Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Memory: " + Fore.YELLOW + Style.BRIGHT + f"{memory_gb:.1f}GB")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ CPU Cores: " + Fore.YELLOW + Style.BRIGHT + f"{cpu_cores}")
        
        # Extract and validate HPO parameters from configuration
        if not hpo_config:
            raise ValueError("No hyperparameter_optimization section in configuration")
        
        if not hpo_config.get('enabled', False):
            raise ValueError("Hyperparameter optimization is not enabled in configuration")
        
        # Prepare parameters for run_hyperparameter_optimization
        hpo_params = {}
        
        # Core HPO parameters with validation
        hpo_params['n_trials'] = hpo_config.get('n_trials', hpo_config.get('trial_count', 50))
        hpo_params['timeout'] = hpo_config.get('timeout', hpo_config.get('timeout_seconds', 3600))
        hpo_params['timeout_minutes'] = hpo_params['timeout'] // 60
        hpo_params['study_name'] = hpo_config.get('study_name', f"hpo_express_{int(time.time())}")
        hpo_params['direction'] = hpo_config.get('direction', 'minimize')
        hpo_params['objective_metric'] = hpo_config.get('objective_metric', 'validation_loss')
        
        # Optimization focus strategy (balanced/accuracy/speed/efficiency)
        hpo_params['optimization_focus'] = hpo_config.get('optimization_focus', 'speed')
        
        # System-aware parallel processing settings
        hpo_params['parallel_trials'] = hpo_config.get('parallel_trials', min(2, cpu_cores // 2) if cpu_cores > 4 else 1)
        
        # Trial-specific patience and early stopping
        hpo_params['trial_patience'] = hpo_config.get('trial_patience', max(5, hpo_config.get('trial_epochs', 20) // 4))
        
        # Storage and plot generation toggles
        storage_config = hpo_config.get('storage', {})
        hpo_params['enable_storage'] = storage_config.get('enabled', False)
        hpo_params['enable_plots'] = hpo_config.get('generate_plots', hpo_config.get('enable_plots', True))
        
        # Complete optimization space configuration
        optimization_space = hpo_config.get('optimization_space', {})
        hpo_params['optimization_space'] = optimization_space
        
        # Extract express setup context if available
        express_config = hpo_config.get('express_setup', {})
        if express_config:
            hpo_params['express_intensity'] = express_config.get('intensity', 'Standard')
            hpo_params['express_focus'] = express_config.get('focus', 'balanced')
            hpo_params['express_system_class'] = express_config.get('system_class', system_class)
            hpo_params['express_preset_alignment'] = express_config.get('preset_alignment', preset_name)
            hpo_params['express_estimated_duration'] = express_config.get('estimated_duration', 'Unknown')
            
            # Use express recommendations for system optimization
            if express_config.get('system_class'):
                system_class = express_config['system_class']
        
        # Sampling and pruning configuration with parsing
        sampler_raw = hpo_config.get('sampler', 'TPESampler')
        pruner_raw = hpo_config.get('pruner', 'MedianPruner')
        
        # Handle dictionary-based sampler configuration
        if isinstance(sampler_raw, dict):
            sampler_type = sampler_raw.get('type', 'TPESampler')
            sampler_config = sampler_raw
        else:
            sampler_type = sampler_raw
            sampler_config = hpo_config.get('sampler_config', {})
        
        # Handle dictionary-based pruner configuration  
        if isinstance(pruner_raw, dict):
            pruner_type = pruner_raw.get('type', 'MedianPruner')
            pruner_config = pruner_raw
        else:
            pruner_type = pruner_raw
            pruner_config = hpo_config.get('pruner_config', {})
        
        # Map common variants to standard names
        sampler_mapping = {
            'Random': 'RandomSampler',
            'TPE': 'TPESampler', 
            'CmaEs': 'CmaEsSampler',
            'Grid': 'GridSampler',
            'NSGAII': 'NSGAIISampler'
        }
        
        pruner_mapping = {
            'Nop': 'NopPruner',
            'Median': 'MedianPruner',
            'Hyperband': 'HyperbandPruner',
            'SuccessiveHalving': 'HyperbandPruner',
            'Percentile': 'PercentilePruner'
        }
        
        hpo_params['sampler_type'] = sampler_mapping.get(sampler_type, sampler_type)
        hpo_params['pruner_type'] = pruner_mapping.get(pruner_type, pruner_type)
        
        # Store configuration objects if available
        if sampler_config:
            hpo_params['sampler_config'] = sampler_config
        if pruner_config:
            hpo_params['pruner_config'] = pruner_config
        
        # Model search configuration
        model_search = hpo_config.get('model_search', {})
        if model_search.get('enabled', False) or express_config:
            hpo_params['model_types'] = model_search.get('model_types', ['EnhancedAutoencoder'])
            hpo_params['search_all_models'] = model_search.get('search_all_models', len(hpo_params['model_types']) > 1)
        else:
            hpo_params['model_types'] = [model_config.get('model_type', 'EnhancedAutoencoder')]
            hpo_params['search_all_models'] = False
        
        # Cross-validation configuration
        cv_config = hpo_config.get('cross_validation', {})
        hpo_params['cv_folds'] = cv_config.get('folds', 3)
        hpo_params['cv_shuffle'] = cv_config.get('shuffle', True)
        hpo_params['cv_random_state'] = cv_config.get('random_state', 42)
        
        # Trial configuration
        hpo_params['trial_epochs'] = hpo_config.get('trial_epochs', 20)
        hpo_params['trial_patience'] = hpo_config.get('trial_patience', max(5, hpo_params['trial_epochs'] // 4))
        hpo_params['trial_batch_size'] = hpo_config.get('trial_batch_size', training_config.get('batch_size', 64))
        
        # Early stopping configuration
        early_stopping = hpo_config.get('early_stopping', {})
        hpo_params['early_stopping_patience'] = early_stopping.get('patience', 10)
        hpo_params['early_stopping_min_trials'] = early_stopping.get('min_trials', 20)
        hpo_params['early_stopping_min_improvement'] = early_stopping.get('min_improvement', 1e-4)
        
        # Storage configuration
        storage_config = hpo_config.get('storage', {})
        if storage_config.get('enabled', False):
            hpo_params['storage_url'] = storage_config.get('url', f'sqlite:///hpo_express_{int(time.time())}.db')
            hpo_params['load_if_exists'] = storage_config.get('load_if_exists', False)
        else:
            hpo_params['storage_url'] = None
        
        # System and performance parameters
        hpo_params['verbose'] = hpo_config.get('verbose', system_config.get('verbose', True))
        hpo_params['save_study'] = hpo_config.get('save_study', True)
        hpo_params['generate_plots'] = hpo_config.get('generate_plots', hpo_config.get('enable_plots', True))
        hpo_params['cleanup_trials'] = hpo_config.get('cleanup_trials', True)
        hpo_params['interactive'] = kwargs.get('interactive', False)
        
        # Performance optimization based on system class
        if 'parallel_trials' in hpo_config:
            hpo_params['parallel_jobs'] = hpo_config['parallel_trials']
        else:
            # Use express setup system-aware logic
            if system_class in ["limited", "balanced"]:
                hpo_params['parallel_jobs'] = 1
            elif system_class == "standard":
                hpo_params['parallel_jobs'] = min(2, cpu_cores // 2)
            else:
                hpo_params['parallel_jobs'] = min(4, cpu_cores // 2)
        
        # Data parameters
        hpo_params['use_real_data'] = data_config.get('use_real_data', False)
        hpo_params['data_path'] = data_config.get('data_path')
        hpo_params['artifacts_path'] = data_config.get('artifacts_path')
        
        if not hpo_params['use_real_data']:
            hpo_params['normal_samples'] = data_config.get('normal_samples', 8000)
            hpo_params['attack_samples'] = data_config.get('attack_samples', 2000)
            hpo_params['features'] = data_config.get('features', 20)
            hpo_params['noise_factor'] = data_config.get('synthetic_generation', {}).get('noise_factor', 0.05)
        
        hpo_params['normalization_method'] = data_config.get('normalization', 'standard')
        hpo_params['validation_split'] = data_config.get('validation_split', 0.2)
        hpo_params['test_split'] = data_config.get('test_split', 0.2)
        hpo_params['random_state'] = data_config.get('random_state', 42)
        
        # System parameters with hardware awareness
        hpo_params['device'] = system_config.get('device', 'auto')
        hpo_params['random_seed'] = system_config.get('random_seed', 42)
        hpo_params['reproducible'] = system_config.get('reproducible', True)
        hpo_params['num_workers'] = training_config.get('num_workers', 0)  # Safe default for HPO
        
        # Study directory configuration
        hpo_params['study_dir'] = Path(system_config.get('model_dir', 'models')) / "hpo_studies"
        hpo_params['study_dir'].mkdir(parents=True, exist_ok=True)
        
        # Optimization space configuration
        optimization_space = hpo_config.get('optimization_space', {})
        if optimization_space:
            hpo_params['optimization_space'] = optimization_space
        
        # Metrics configuration
        metrics_config = hpo_config.get('metrics', {})
        if metrics_config:
            hpo_params['primary_metric'] = metrics_config.get('primary', 'validation_loss')
            hpo_params['secondary_metrics'] = metrics_config.get('secondary', ['reconstruction_error', 'training_time'])
            hpo_params['direction'] = metrics_config.get('direction', 'minimize')
        
        # Error handling configuration
        error_config = config.get('error_handling', {})
        hpo_params['error_handling'] = error_config.get('enabled', True)
        hpo_params['graceful_degradation'] = error_config.get('graceful_degradation', True)
        hpo_params['continue_on_error'] = error_config.get('continue_on_error', False)
        
        # Continuation mode handling (for express setup restarts)
        if hpo_config.get('continuation_mode', False):
            hpo_params['continuation_mode'] = True
            hpo_params['original_trials'] = hpo_config.get('original_trials', 0)
            hpo_params['original_completed'] = hpo_config.get('original_completed', 0)
        
        # Validate critical express parameters are set
        required_express_params = ['optimization_focus', 'parallel_trials', 'trial_patience']
        missing_params = [param for param in required_express_params if param not in hpo_params]
        
        if missing_params:
            logger.warning(f"Missing express HPO parameters: {missing_params}")
            # Set defaults for missing parameters
            for param in missing_params:
                if param == 'optimization_focus':
                    hpo_params[param] = 'balanced'
                elif param == 'parallel_trials':
                    hpo_params[param] = min(2, cpu_cores // 2)
                elif param == 'trial_patience':
                    hpo_params[param] = max(5, hpo_params['trial_epochs'] // 4)
        
        # Add the complete configuration for reference
        hpo_params['config'] = config
        
        # Add any additional kwargs passed to the function
        hpo_params.update({k: v for k, v in kwargs.items() if k not in hpo_params})
        
        # HPO CONFIGURATION SUMMARY
        print(Fore.YELLOW + Style.BRIGHT + "\nHPO CONFIGURATION SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Express Setup Context (if available)
        if express_config:
            print(Fore.MAGENTA + Style.BRIGHT + "Express Setup:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Intensity: " + Fore.YELLOW + Style.BRIGHT + f"{express_config.get('intensity', 'Standard')}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Focus: " + Fore.YELLOW + Style.BRIGHT + f"{express_config.get('focus', 'balanced').title()}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{express_config.get('system_class', system_class).upper()}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Preset Alignment: " + Fore.YELLOW + Style.BRIGHT + f"{express_config.get('preset_alignment', preset_name)}")
        
        # Core Optimization Parameters
        print(Fore.MAGENTA + Style.BRIGHT + "\nCore Optimization:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Study Name: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['study_name']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['n_trials']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['timeout_minutes']} minutes")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Direction: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['direction'].upper()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Objective: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['objective_metric']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['sampler_type']}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['pruner_type']}")
        
        # Express-specific parameters display
        print(Fore.MAGENTA + Style.BRIGHT + "\nExpress Configuration:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Optimization Focus: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['optimization_focus'].title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Parallel Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['parallel_trials']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trial Patience: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['trial_patience']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Storage Enabled: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['enable_storage']}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Plots Enabled: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['enable_plots']}")
        
        # Model Configuration
        print(Fore.MAGENTA + Style.BRIGHT + "\nModel Configuration:")
        if hpo_params['search_all_models']:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Search Strategy: " + Fore.YELLOW + Style.BRIGHT + f"Multi-Model Search")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Models: " + Fore.YELLOW + Style.BRIGHT + f"{len(hpo_params['model_types'])} models")
            for i, model in enumerate(hpo_params['model_types'], 1):
                prefix = "  └─" if i == len(hpo_params['model_types']) else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {model}")
        else:
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Single Model: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['model_types'][0]}")
        
        # Data Configuration
        print(Fore.MAGENTA + Style.BRIGHT + "\nData Configuration:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{'Real Data' if hpo_params['use_real_data'] else 'Synthetic Data'}")
        if hpo_params['use_real_data']:
            if hpo_params['data_path']:
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data Path: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['data_path']}")
            if hpo_params['artifacts_path']:
                print(Fore.GREEN + Style.BRIGHT + f"  └─ Artifacts Path: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['artifacts_path']}")
        else:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params.get('normal_samples', 8000):,}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params.get('attack_samples', 2000):,}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params.get('features', 20)}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Noise Factor: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params.get('noise_factor', 0.05)}")
        
        # Validation Configuration
        print(Fore.MAGENTA + Style.BRIGHT + "\nValidation Configuration:")
        if hpo_params['cv_folds'] > 1:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Method: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['cv_folds']}-Fold Cross Validation")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Shuffle: " + Fore.YELLOW + Style.BRIGHT + f"{'Enabled' if hpo_params['cv_shuffle'] else 'Disabled'}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Random State: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['cv_random_state']}")
        else:
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Method: " + Fore.YELLOW + Style.BRIGHT + f"Single Split Validation")
        
        # Trial Configuration
        print(Fore.MAGENTA + Style.BRIGHT + "\nTrial Configuration:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs per Trial: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['trial_epochs']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Early Stopping: " + Fore.YELLOW + Style.BRIGHT + f"Patience {hpo_params['trial_patience']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['trial_batch_size']}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ HPO Early Stopping: " + Fore.YELLOW + Style.BRIGHT + f"Patience {hpo_params['early_stopping_patience']}")
        
        # Search Space Configuration
        print(Fore.MAGENTA + Style.BRIGHT + "\nSearch Space Parameters:")
        if optimization_space:
            param_count = 0
            for param, config in optimization_space.items():
                if param_count >= 6:  # Show first 6 parameters
                    break
                if isinstance(config, dict):
                    param_type = config.get('type', 'unknown')
                    if param_type == 'float' or ('low' in config and 'high' in config):
                        min_val = config.get('low', config.get('min', '?'))
                        max_val = config.get('high', config.get('max', '?'))
                        log_scale = " (log)" if config.get('log', False) else ""
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ {param}: " + Fore.YELLOW + Style.BRIGHT + f"{min_val} to {max_val}{log_scale}")
                    elif param_type == 'categorical' or 'choices' in config:
                        choices = config.get('choices', [])
                        if len(choices) <= 3:
                            choices_str = ", ".join(map(str, choices))
                        else:
                            choices_str = f"{choices[0]}, {choices[1]}, ..., {choices[-1]} ({len(choices)} options)"
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ {param}: " + Fore.YELLOW + Style.BRIGHT + f"{choices_str}")
                    elif param_type == 'int' or ('low' in config and 'high' in config):
                        min_val = config.get('low', config.get('min', '?'))
                        max_val = config.get('high', config.get('max', '?'))
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ {param}: " + Fore.YELLOW + Style.BRIGHT + f"{min_val} to {max_val}")
                    else:
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ {param}: " + Fore.YELLOW + Style.BRIGHT + f"Custom configuration")
                    param_count += 1
            
            if len(optimization_space) > 6:
                print(Fore.GREEN + Style.BRIGHT + f"  └─ ... and {len(optimization_space) - 6} more parameters")
        else:
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Using default search space")
        
        # System Configuration
        print(Fore.MAGENTA + Style.BRIGHT + "\nSystem Configuration:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Device: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['device'].upper()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Parallel Jobs: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['parallel_jobs']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Workers: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['num_workers']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Random Seed: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['random_seed']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Reproducible: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['reproducible']}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
        
        # Output Configuration
        print(Fore.MAGENTA + Style.BRIGHT + "\nOutput Configuration:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Verbose: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['verbose']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Generate Plots: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['generate_plots']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Save Study: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['save_study']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Storage: " + Fore.YELLOW + Style.BRIGHT + f"{'Enabled' if hpo_params.get('storage_url') else 'Disabled'}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Cleanup Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['cleanup_trials']}")
        
        # Performance Estimation
        print(Fore.MAGENTA + Style.BRIGHT + "\nPerformance Estimation:")
        estimated_time = _estimate_hpo_time(
            n_trials=hpo_params['n_trials'],
            trial_epochs=hpo_params['trial_epochs'],
            n_model_types=len(hpo_params['model_types']),
            cv_folds=hpo_params['cv_folds'],
            hardware_info=hardware_data,
            system_class=system_class
        )
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Estimated Duration: " + Fore.YELLOW + Style.BRIGHT + f"{estimated_time}")
        
        # Calculate total training operations
        total_ops = hpo_params['n_trials'] * hpo_params['trial_epochs'] * hpo_params['cv_folds'] * len(hpo_params['model_types'])
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Training Ops: " + Fore.YELLOW + Style.BRIGHT + f"{total_ops:,}")
        
        # Resource level assessment
        if total_ops < 1000:
            resource_level = "Light"
            resource_color = Fore.GREEN
        elif total_ops < 5000:
            resource_level = "Moderate" 
            resource_color = Fore.YELLOW
        else:
            resource_level = "Heavy"
            resource_color = Fore.RED
            
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Resource Level: " + resource_color + Style.BRIGHT + f"{resource_level}")
        
        # Express setup estimated duration if available
        if express_config:
            express_estimated = express_config.get('estimated_duration')
            if express_estimated and express_estimated != 'Unknown':
                print(Fore.GREEN + Style.BRIGHT + f"  └─ Express Estimate: " + Fore.CYAN + Style.BRIGHT + f"{express_estimated}")
        
        # Continuation Info (if applicable)
        if hpo_config.get('continuation_mode', False):
            print(Fore.MAGENTA + Style.BRIGHT + "\nContinuation Information:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Mode: " + Fore.YELLOW + Style.BRIGHT + f"Continuing Existing Study")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Original Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params.get('original_trials', 0)}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Completed Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params.get('original_completed', 0)}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Additional Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_params['n_trials']}")
        
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        
        # Log launch parameters for debugging
        logger.info(f"Launching HPO with configuration:")
        logger.info(f"  - Study: {hpo_params['study_name']}")
        logger.info(f"  - Trials: {hpo_params['n_trials']}")
        logger.info(f"  - Timeout: {hpo_params['timeout_minutes']} minutes")
        logger.info(f"  - Models: {', '.join(hpo_params['model_types'])}")
        logger.info(f"  - Sampler: {hpo_params['sampler_type']}")
        logger.info(f"  - Pruner: {hpo_params['pruner_type']}")
        logger.info(f"  - Data: {'Real' if hpo_params['use_real_data'] else 'Synthetic'}")
        logger.info(f"  - CV Folds: {hpo_params['cv_folds']}")
        logger.info(f"  - System Class: {system_class}")
        logger.info(f"  - Optimization Focus: {hpo_params['optimization_focus']}")
        logger.info(f"  - Parallel Trials: {hpo_params['parallel_trials']}")
        
        # Final confirmation for interactive mode
        interactive = kwargs.get('interactive', True)
        if interactive and not kwargs.get('skip_prompt', False):
            try:
                confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart hyperparameter optimization? (Y/n): " + Style.RESET_ALL).strip().lower()
                if confirm not in ('', 'y', 'yes'):
                    print(Fore.RED + Style.BRIGHT + "\nHPO launch cancelled by user")
                    return {
                        'success': False,
                        'cancelled': True,
                        'message': 'HPO launch cancelled by user',
                        'study_name': hpo_params['study_name'],
                        'configuration': config
                    }
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nHPO launch cancelled by user")
                return {
                    'success': False,
                    'cancelled': True,
                    'message': 'HPO launch cancelled by user',
                    'study_name': hpo_params['study_name'],
                    'configuration': config
                }
        
        print(Fore.GREEN + Style.BRIGHT + "\nStarting hyperparameter optimization...")
        
        # Call the main HPO function with parameters
        try:
            results = run_hyperparameter_optimization(**hpo_params)
        except Exception as hpo_error:
            logger.error(f"HPO execution failed: {hpo_error}")
            
            # Error display - delegate to _display_hpo_results for consistency
            error_results = {
                'success': False,
                'error': str(hpo_error),
                'error_type': type(hpo_error).__name__,
                'study_name': hpo_params['study_name'],
                'n_trials_total': hpo_params['n_trials'],
                'n_trials_completed': 0,
                'configuration': config,
                'start_time': datetime.now().isoformat(),
                'end_time': datetime.now().isoformat(),
                'recommendations': [
                    'Check system resources and configuration',
                    'Verify data availability and format',
                    'Consider reducing trial count or complexity',
                    'Review log files for detailed error information',
                    'Try using synthetic data for testing configuration'
                ],
                'launch_config': {
                    'launch_method': 'interactive_hpo',
                    'setup_method': setup_method,
                    'config_source': config_source,
                    'preset_used': preset_name,
                    'system_class': system_class,
                    'express_setup': bool(express_config),
                    'launch_timestamp': datetime.now().isoformat(),
                    'parameters_used': {
                        'n_trials': hpo_params['n_trials'],
                        'timeout_minutes': hpo_params['timeout_minutes'],
                        'model_types': hpo_params['model_types'],
                        'sampler_type': hpo_params['sampler_type'],
                        'pruner_type': hpo_params['pruner_type'],
                        'cv_folds': hpo_params['cv_folds'],
                        'data_source': 'real' if hpo_params['use_real_data'] else 'synthetic',
                        'trial_epochs': hpo_params['trial_epochs']
                    }
                }
            }
            
            # Use the dedicated display function for consistent error reporting
            _display_hpo_results(error_results)
            return error_results
        
        # Process and enhance results
        if results:
            # Add configuration metadata to results (run_hyperparameter_optimization now handles success flag)
            results['launch_config'] = {
                'launch_method': 'interactive_hpo',
                'setup_method': setup_method,
                'config_source': config_source,
                'preset_used': preset_name,
                'system_class': system_class,
                'express_setup': bool(express_config),
                'launch_timestamp': datetime.now().isoformat(),
                'parameters_used': {
                    'n_trials': hpo_params['n_trials'],
                    'timeout_minutes': hpo_params['timeout_minutes'],
                    'model_types': hpo_params['model_types'],
                    'sampler_type': hpo_params['sampler_type'],
                    'pruner_type': hpo_params['pruner_type'],
                    'cv_folds': hpo_params['cv_folds'],
                    'data_source': 'real' if hpo_params['use_real_data'] else 'synthetic',
                    'trial_epochs': hpo_params['trial_epochs']
                }
            }
            
            # Delegate ALL display to _display_hpo_results function for consistency
            _display_hpo_results(results)
        
        else:
            # Handle case where no results are returned
            no_results_response = {
                'success': False,
                'error': 'No results returned from optimization function',
                'error_type': 'ExecutionError',
                'study_name': hpo_params['study_name'],
                'n_trials_total': hpo_params['n_trials'],
                'n_trials_completed': 0,
                'start_time': datetime.now().isoformat(),
                'end_time': datetime.now().isoformat(),
                'recommendations': [
                    'Check HPO function implementation',
                    'Verify all required dependencies are available',
                    'Review system logs for more information',
                    'Try running with verbose logging enabled'
                ],
                'launch_config': {
                    'launch_method': 'interactive_hpo',
                    'setup_method': setup_method,
                    'config_source': config_source,
                    'preset_used': preset_name,
                    'system_class': system_class,
                    'express_setup': bool(express_config),
                    'launch_timestamp': datetime.now().isoformat(),
                    'parameters_used': {
                        'n_trials': hpo_params['n_trials'],
                        'timeout_minutes': hpo_params['timeout_minutes'],
                        'model_types': hpo_params['model_types'],
                        'sampler_type': hpo_params['sampler_type'],
                        'pruner_type': hpo_params['pruner_type'],
                        'cv_folds': hpo_params['cv_folds'],
                        'data_source': 'real' if hpo_params['use_real_data'] else 'synthetic',
                        'trial_epochs': hpo_params['trial_epochs']
                    }
                }
            }
            
            _display_hpo_results(no_results_response)
            results = no_results_response
        
        return results
        
    except (EOFError, KeyboardInterrupt):
        # Handle user interruption with consistent result structure
        study_name = "unknown"
        n_trials = 0
        try:
            if 'hpo_params' in locals():
                study_name = hpo_params.get('study_name', 'unknown')
                n_trials = hpo_params.get('n_trials', 0)
        except:
            pass
        
        interrupt_result = {
            'success': False,
            'error': 'HPO launch interrupted by user',
            'error_type': 'KeyboardInterrupt',
            'interrupted': True,
            'study_name': study_name,
            'n_trials_total': n_trials,
            'n_trials_completed': 0,
            'start_time': datetime.now().isoformat(),
            'end_time': datetime.now().isoformat(),
            'configuration': config,
            'interrupt_stage': 'launch',
            'recovery_info': {
                'can_resume': False,
                'data_lost': False,
                'message': 'HPO was cancelled before starting - no data to recover'
            },
            'recommendations': [
                'HPO can be restarted safely from the beginning',
                'Consider reducing trial count if time is limited',
                'All configuration has been preserved for restart'
            ],
            'launch_config': {
                'launch_method': 'interactive_hpo',
                'setup_method': setup_method,
                'config_source': config_source,
                'preset_used': preset_name,
                'system_class': system_class,
                'express_setup': bool(express_config),
                'launch_timestamp': datetime.now().isoformat(),
                'interrupted': True
            }
        }
        
        _display_hpo_results(interrupt_result)
        return interrupt_result
        
    except ValueError as ve:
        logger.error(f"HPO configuration validation failed: {ve}")
        config_error_result = {
            'success': False,
            'error': str(ve),
            'error_type': 'ConfigurationError',
            'study_name': hpo_params.get('study_name', 'unknown'),
            'n_trials_total': hpo_params.get('n_trials', 0),
            'n_trials_completed': 0,
            'start_time': datetime.now().isoformat(),
            'end_time': datetime.now().isoformat(),
            'recommendations': ['Fix configuration issues and try again'],
            'launch_config': {
                'launch_method': 'interactive_hpo',
                'setup_method': setup_method,
                'config_source': config_source,
                'preset_used': preset_name,
                'system_class': system_class,
                'express_setup': bool(express_config),
                'launch_timestamp': datetime.now().isoformat(),
                'configuration_error': True
            }
        }
        
        _display_hpo_results(config_error_result)
        return config_error_result
        
    except Exception as e:
        logger.error(f"HPO launch failed with unexpected error: {e}", exc_info=True)
        unexpected_error_result = {
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__,
            'study_name': hpo_params.get('study_name', 'unknown'),
            'n_trials_total': hpo_params.get('n_trials', 0),
            'n_trials_completed': 0,
            'start_time': datetime.now().isoformat(),
            'end_time': datetime.now().isoformat(),
            'recommendations': [
                'Check system resources and environment',
                'Verify configuration file integrity', 
                'Review error logs for more details',
                'Consider restarting the system'
            ],
            'launch_config': {
                'launch_method': 'interactive_hpo',
                'setup_method': setup_method,
                'config_source': config_source,
                'preset_used': preset_name,
                'system_class': system_class,
                'express_setup': bool(express_config),
                'launch_timestamp': datetime.now().isoformat(),
                'unexpected_error': True
            }
        }
        
        _display_hpo_results(unexpected_error_result)
        return unexpected_error_result

def hpo_training_menu(config: Optional[Dict[str, Any]] = None, **kwargs):
    """Menu for hyperparameter optimization options with context display and error handling."""
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None and banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Get hardware context for system-aware configuration
        try:
            hardware_data = check_hardware(include_memory_usage=True)
        except Exception as e:
            logger.debug(f"Hardware detection failed: {e}")
            hardware_data = {}
        
        # Extract configuration sections with error handling
        hpo_config = config.get('hyperparameter_optimization', {})
        data_config = config.get('data', {})
        system_config = config.get('system', {})
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        metadata = config.get('metadata', {})
        presets_section = config.get('presets', {})
        
        # Context extraction using multiple fallbacks with preset compatibility
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Determine preset name from multiple sources
        # Method 1: Check presets section
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        else:
            config_source = "Unknown"
        
        # HPO-specific context extraction
        hpo_data_path = data_config.get('data_path', 'Default')
        preset_count = len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        hpo_preset_count = len([p for p in PRESET_CONFIGS.values() if p.get('hyperparameter_optimization', {}).get('enabled', False)]) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        hpo_epoch_count = training_config.get('epochs', 100)
        hpo_batch_count = training_config.get('batch_size', 64)
        hpo_normal_samples_count = data_config.get('normal_samples', 8000)
        hpo_attack_samples_count = data_config.get('attack_samples', 2000)
        hpo_trials_count = hpo_config.get('n_trials', 50)
        hpo_strategy = hpo_config.get('strategy', 'optuna')
        hpo_timeout = hpo_config.get('timeout', 3600)
        hpo_enabled = hpo_config.get('enabled', False)
        
        # Hardware-aware system class detection
        cuda_available = hardware_data.get('cuda', {}).get('available', False)
        gpu_count = hardware_data.get('cuda', {}).get('gpu_count', 0)
        memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)
        
        # Determine system performance class
        if cuda_available and memory_gb >= 16 and cpu_cores >= 8:
            system_class = "high_performance"
        elif cuda_available and memory_gb >= 8:
            system_class = "performance"
        elif memory_gb >= 4:
            system_class = "standard"
        else:
            system_class = "limited"
        
        # Menu display with enhanced context
        print(Fore.MAGENTA + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION MENU")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        print(Fore.YELLOW + Style.BRIGHT + "Active HPO Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name.title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Strategy: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_strategy}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_trials_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_timeout}s")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Data: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_data_path}")
        
        # Hardware context display
        print(Fore.CYAN + Style.BRIGHT + "\nHardware Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
        if cuda_available:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ GPU Count: " + Fore.YELLOW + Style.BRIGHT + f"{gpu_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Memory: " + Fore.YELLOW + Style.BRIGHT + f"{memory_gb:.1f}GB")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ CPU Cores: " + Fore.YELLOW + Style.BRIGHT + f"{cpu_cores}")
        
        # Display any additional kwargs context if provided
        if kwargs:
            print(Fore.CYAN + Style.BRIGHT + f"\nAdditional Parameters:")
            for i, (key, value) in enumerate(kwargs.items()):
                if key not in ['config', 'use_real_data', 'use_current_config', 'preset', 'non_interactive']:
                    prefix = "  └─" if i == len(kwargs) - 1 else "  ├─"
                    print(Fore.GREEN + Style.BRIGHT + f"{prefix} {key}: " + Fore.YELLOW + Style.BRIGHT + f"{value}")
        
        # Enhanced menu options with context-aware descriptions
        print(Fore.YELLOW + Style.BRIGHT + "\nOptimization Options:")
        print(Fore.WHITE + Style.BRIGHT + "1. HPO with Current Configuration " + Fore.GREEN + Style.BRIGHT + f"(Preset: {preset_name.title()}, Model: {model_type})")
        print(Fore.WHITE + Style.BRIGHT + "2. HPO with Synthetic Data " + Fore.GREEN + Style.BRIGHT + f"(Samples: {hpo_normal_samples_count}N/{hpo_attack_samples_count}A)")
        print(Fore.WHITE + Style.BRIGHT + "3. HPO with Real Data " + Fore.GREEN + Style.BRIGHT + f"(Source: {hpo_data_path})")
        print(Fore.WHITE + Style.BRIGHT + "4. Custom HPO Configuration " + Fore.GREEN + Style.BRIGHT + "(Full Interactive Setup & Advanced Options)")
        print(Fore.WHITE + Style.BRIGHT + "5. Select HPO Preset " + Fore.GREEN + Style.BRIGHT + f"(Available: {hpo_preset_count} HPO presets)")
        print(Fore.WHITE + Style.BRIGHT + "6. Continue Existing Study " + Fore.GREEN + Style.BRIGHT + "(Resume & Extend Previous Optimization)")
        print(Fore.WHITE + Style.BRIGHT + "7. Quick HPO Test " + Fore.GREEN + Style.BRIGHT + f"(Hardware-aware validation - {system_class} system)")
        print(Fore.WHITE + Style.BRIGHT + "8. HPO Model Comparison " + Fore.GREEN + Style.BRIGHT + f"(Compare {len(MODEL_VARIANTS)} model types)")
        print(Fore.RED + Style.BRIGHT + "0. Back to Main Menu")
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-8): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                return
        
        try:
            if choice == "1":
                try:
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching HPO with Current Configuration...")
                    print(Fore.CYAN + Style.BRIGHT + f"Using preset '{preset_name.title()}' with model '{model_type}'")
                    
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=True,
                        config=config,
                        operation_mode='express',
                        data_mode='synthetic',
                        optimization_focus='balanced',
                        hardware_data=hardware_data,
                        skip_prompt=False,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Current Configuration HPO")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during Current Configuration HPO: {str(e)}\n\n"
                        f"Context:\n"
                        f"  ├─ Model: {model_type}\n"
                        f"  ├─ Preset: {preset_name.title()}\n" 
                        f"  ├─ System Class: {system_class}\n"
                        f"  ├─ Config Source: {config_source}\n"
                        f"  └─ Additional Parameters: {kwargs if kwargs else 'None'}\n\n"
                        f"Please check:\n"
                        f"  ├─ Current configuration validity\n"
                        f"  ├─ Model compatibility with HPO\n"
                        f"  ├─ Configuration file integrity\n"
                        f"  └─ System resource availability"
                    )
                    print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.RED + Style.BRIGHT + "CURRENT CONFIG HPO ERROR")
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    print(Fore.WHITE + Style.BRIGHT + message)
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    
            elif choice == "2":
                try:
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching HPO with Synthetic Data...")
                    print(Fore.CYAN + Style.BRIGHT + f"Generating {hpo_normal_samples_count} normal + {hpo_attack_samples_count} attack samples")
                    
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=False,
                        config=config,
                        operation_mode='express',
                        data_mode='synthetic',
                        optimization_focus='accuracy',
                        hardware_data=hardware_data,
                        skip_prompt=False,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Synthetic Data HPO")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during Synthetic Data HPO: {str(e)}\n\n"
                        f"Context:\n"
                        f"  ├─ Model: {model_type}\n"
                        f"  ├─ Synthetic Samples: {hpo_normal_samples_count}N/{hpo_attack_samples_count}A\n"
                        f"  ├─ System Class: {system_class}\n"
                        f"  └─ Additional Parameters: {kwargs if kwargs else 'None'}\n\n"
                        f"Please verify:\n"
                        f"  ├─ Data generation configuration\n"
                        f"  ├─ Model compatibility with synthetic data\n"
                        f"  ├─ System memory availability\n"
                        f"  └─ Feature dimension settings"
                    )
                    print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.RED + Style.BRIGHT + "SYNTHETIC DATA HPO ERROR")
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    print(Fore.WHITE + Style.BRIGHT + message)
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    
            elif choice == "3":
                try:
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching HPO with Real Data...")
                    print(Fore.CYAN + Style.BRIGHT + f"Using data from: {hpo_data_path}")
                    
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=False,
                        config=config,
                        operation_mode='express', 
                        data_mode='real',
                        optimization_focus='accuracy',
                        hardware_data=hardware_data,
                        skip_prompt=False,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Real Data HPO")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during Real Data HPO: {str(e)}\n\n"
                        f"Context:\n"
                        f"  ├─ Data Path: {hpo_data_path}\n"
                        f"  ├─ Model: {model_type}\n"
                        f"  ├─ System Class: {system_class}\n"
                        f"  └─ Additional Parameters: {kwargs if kwargs else 'None'}\n\n"
                        f"Please verify:\n"
                        f"  ├─ Data file exists and is accessible\n"
                        f"  ├─ Data format is compatible\n"
                        f"  ├─ Sufficient disk space\n"
                        f"  ├─ Data preprocessing requirements\n"
                        f"  └─ File permissions and paths"
                    )
                    print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.RED + Style.BRIGHT + "REAL DATA HPO ERROR")
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    print(Fore.WHITE + Style.BRIGHT + message)
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    
            elif choice == "4":
                try:
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching Custom HPO Configuration...")
                    print(Fore.CYAN + Style.BRIGHT + "Full interactive setup with advanced options")
                    
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=False,
                        config=config,
                        operation_mode='custom',
                        data_mode='auto',
                        optimization_focus='balanced', 
                        hardware_data=hardware_data,
                        skip_prompt=False,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Custom HPO Configuration")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during Custom HPO setup: {str(e)}\n\n"
                        f"Context:\n"
                        f"  ├─ Model: {model_type}\n"
                        f"  ├─ Preset: {preset_name.title()}\n"
                        f"  ├─ System Class: {system_class}\n"
                        f"  └─ Additional Parameters: {kwargs if kwargs else 'None'}\n\n"
                        f"Please check:\n"
                        f"  ├─ Parameter validation rules\n"
                        f"  ├─ Configuration file permissions\n"
                        f"  ├─ Interactive input handling\n"
                        f"  ├─ Search space definitions\n"
                        f"  └─ Advanced option compatibility"
                    )
                    print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.RED + Style.BRIGHT + "CUSTOM HPO ERROR")
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    print(Fore.WHITE + Style.BRIGHT + message)
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    
            elif choice == "5":
                try:
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching HPO Preset Selection...")
                    print(Fore.CYAN + Style.BRIGHT + f"Available: {hpo_preset_count} HPO-enabled presets")
                    
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=False,
                        config=config,
                        operation_mode='preset',
                        data_mode='auto',
                        optimization_focus='balanced',
                        hardware_data=hardware_data,
                        skip_prompt=False,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Preset HPO Selection")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during HPO preset selection: {str(e)}\n\n"
                        f"Context:\n"
                        f"  ├─ Available Presets: {hpo_preset_count}\n"
                        f"  ├─ Current Preset: {preset_name.title()}\n"
                        f"  ├─ System Class: {system_class}\n"
                        f"  └─ Additional Parameters: {kwargs if kwargs else 'None'}\n\n"
                        f"Please verify:\n"
                        f"  ├─ HPO preset configuration files\n"
                        f"  ├─ Preset validation logic\n"
                        f"  ├─ Configuration loading mechanisms\n"
                        f"  ├─ Preset compatibility checks\n"
                        f"  └─ Available system resources"
                    )
                    print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.RED + Style.BRIGHT + "HPO PRESET SELECTION ERROR")
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    print(Fore.WHITE + Style.BRIGHT + message)
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    
            elif choice == "6":
                try:
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching Continue Existing Study...")
                    print(Fore.CYAN + Style.BRIGHT + "Resume and extend previous optimization")
                    
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=False,
                        config=config,
                        operation_mode='continue',
                        data_mode='auto',
                        optimization_focus='balanced',
                        hardware_data=hardware_data,
                        skip_prompt=False,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Continued Study HPO")
                    
                except Exception as e:
                    message = (
                        f"Error encountered while continuing existing study: {str(e)}\n\n"
                        f"Context:\n"
                        f"  ├─ Model: {model_type}\n"
                        f"  ├─ Strategy: {hpo_strategy}\n"
                        f"  ├─ System Class: {system_class}\n"
                        f"  └─ Additional Parameters: {kwargs if kwargs else 'None'}\n\n"
                        f"Please check:\n"
                        f"  ├─ Existing study files exist\n"
                        f"  ├─ Study database accessibility\n"
                        f"  ├─ Study compatibility with current configuration\n"
                        f"  ├─ Storage backend availability\n"
                        f"  └─ Study metadata integrity"
                    )
                    print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.RED + Style.BRIGHT + "CONTINUE STUDY ERROR")
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    print(Fore.WHITE + Style.BRIGHT + message)
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    
            elif choice == "7":
                try:
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching Quick HPO Test...")
                    print(Fore.CYAN + Style.BRIGHT + f"Hardware-aware validation for {system_class} system")
                    
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=False,
                        config=config,
                        operation_mode='quick_test',
                        data_mode='synthetic',
                        optimization_focus='speed',
                        hardware_data=hardware_data,
                        #skip_prompt=True,
                        skip_prompt=False,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Quick HPO Test")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during quick HPO test: {str(e)}\n\n"
                        f"Context:\n"
                        f"  ├─ Model: {model_type}\n"
                        f"  ├─ Test Type: Hardware-aware validation\n"
                        f"  ├─ System Class: {system_class}\n"
                        f"  └─ Additional Parameters: {kwargs if kwargs else 'None'}\n\n"
                        f"This may indicate:\n"
                        f"  ├─ Basic HPO functionality issues\n"
                        f"  ├─ Model initialization problems\n"
                        f"  ├─ Resource allocation failures\n"
                        f"  ├─ System compatibility issues\n"
                        f"  └─ Dependency configuration problems"
                    )
                    print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.RED + Style.BRIGHT + "QUICK HPO TEST ERROR")
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    print(Fore.WHITE + Style.BRIGHT + message)
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    
            elif choice == "8":
                try:
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching HPO Model Comparison...")
                    print(Fore.CYAN + Style.BRIGHT + f"Comparing {len(MODEL_VARIANTS)} model architectures")
                    
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=False,
                        config=config,
                        operation_mode='model_comparison',
                        data_mode='synthetic',
                        optimization_focus='accuracy',
                        hardware_data=hardware_data,
                        #skip_prompt=True,
                        skip_prompt=False,
                        **kwargs
                    )
                    _handle_hpo_result(result, "HPO Model Comparison")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during HPO model comparison: {str(e)}\n\n"
                        f"Context:\n"
                        f"  ├─ Model: {model_type}\n"
                        f"  ├─ Comparison Type: Multi-model HPO\n"
                        f"  ├─ Models Available: {len(MODEL_VARIANTS)}\n"
                        f"  ├─ System Class: {system_class}\n"
                        f"  └─ Additional Parameters: {kwargs if kwargs else 'None'}\n\n"
                        f"Please ensure:\n"
                        f"  ├─ Multiple models are configured\n"
                        f"  ├─ Comparison data is available\n"
                        f"  ├─ Sufficient system resources\n"
                        f"  ├─ Model variant compatibility\n"
                        f"  └─ Parallel execution configuration"
                    )
                    print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.RED + Style.BRIGHT + "HPO MODEL COMPARISON ERROR")
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    print(Fore.WHITE + Style.BRIGHT + message)
                    print(Fore.RED + Style.BRIGHT + "-"*40)
                    
            elif choice == "0":
                print(Fore.YELLOW + Style.BRIGHT + "\nReturning to main menu...")
                return
            else:
                print(Fore.RED + Style.BRIGHT + f"\nInvalid selection '{choice}'. Please enter a number from 0-8.")
        
        except KeyboardInterrupt:
            print(Fore.YELLOW + Style.BRIGHT + "\nHPO operation interrupted by user")
            _handle_hpo_result(None, "User Interrupted HPO")
        except Exception as e:
            logger.error(f"HPO menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in HPO menu: {str(e)}\n\n"
                f"Context:\n"
                f"  ├─ Selected Option: {choice}\n"
                f"  ├─ Current Preset: {preset_name.title()}\n"
                f"  ├─ Model Type: {model_type}\n"
                f"  ├─ System Class: {system_class}\n"
                f"  ├─ Config Source: {config_source}\n"
                f"  └─ Additional Parameters: {kwargs if kwargs else 'None'}\n\n"
                f"This could indicate:\n"
                f"  ├─ System resource exhaustion\n"
                f"  ├─ HPO configuration corruption\n"
                f"  ├─ Dependency conflicts\n"
                f"  ├─ Optimization algorithm issues\n"
                f"  ├─ Hardware compatibility problems\n"
                f"  └─ Memory allocation failures\n\n"
                f"Please check the logs for detailed information."
            )
            print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.RED + Style.BRIGHT + "HPO MENU ERROR")
            print(Fore.RED + Style.BRIGHT + "-"*40)
            print(Fore.WHITE + Style.BRIGHT + message)
            print(Fore.RED + Style.BRIGHT + "-"*40)
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                break

def run_hyperparameter_optimization_interactive(
    use_real_data: Optional[bool] = None,
    use_current_config: bool = False,
    preset: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None,
    non_interactive: bool = False,
    operation_mode: Optional[str] = None,
    data_mode: Optional[str] = None,
    optimization_focus: Optional[str] = None,
    trial_count: Optional[int] = None,
    timeout_seconds: Optional[int] = None,
    study_name: Optional[str] = None,
    storage_url: Optional[str] = None,
    model_types: Optional[List[str]] = None,
    force_express: bool = False,
    skip_prompt: bool = False,
    hardware_data: Optional[Dict[str, Any]] = None,
    enable_storage: Optional[bool] = None,
    enable_plots: Optional[bool] = None,
    custom_search_space: Optional[Dict[str, Any]] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Enhanced interactive HPO setup with full preset configuration compatibility.
    
    Args:
        use_real_data: Whether to use real data (legacy parameter)
        use_current_config: Whether to use current configuration as base
        preset: Specific preset to apply from PRESET_CONFIGS
        config: Base configuration to use
        non_interactive: Run without user prompts
        operation_mode: Specific operation mode ('express', 'custom', 'preset', 'continue', 'quick_test', 'model_comparison')
        data_mode: Data source mode ('synthetic', 'real', 'auto')
        optimization_focus: Optimization objective focus ('accuracy', 'speed', 'balanced')
        trial_count: Override number of trials
        timeout_seconds: Override timeout in seconds
        study_name: Specific study name for continuation
        storage_url: Storage URL for study continuation
        model_types: List of model types for comparison
        force_express: Force express mode even if other options specified
        skip_prompt: Skip confirmation prompts
        hardware_data: Pre-fetched hardware data for optimization
        enable_storage: Override storage settings from preset
        enable_plots: Override plot generation settings from preset
        custom_search_space: Override search space from preset
        sampler_type: Override sampler type from preset
        pruner_type: Override pruner type from preset
        **kwargs: Additional parameters
        
    Returns:
        Dictionary with HPO results or None if cancelled/failed
    """
    try:
        # Check if we're being called from quick_test mode to prevent infinite recursion
        recursion_depth = kwargs.get('_recursion_depth', 0)
        max_recursion_depth = 0

        # Define operation modes that could cause recursion
        recursive_modes = ['express', 'custom', 'preset', 'continue', 'quick_test', 'model_comparison']

        if recursion_depth > max_recursion_depth and operation_mode in recursive_modes:
            logger.warning(f"Recursion depth {recursion_depth} reached for mode '{operation_mode}'. Bypassing interactive setup.")
            print(Fore.YELLOW + Style.BRIGHT + f"\nRecursion Guard: Bypassing interactive setup (depth: {recursion_depth})")
            print(Fore.GREEN + Style.BRIGHT + f"Launching HPO directly with {operation_mode} mode configuration...")
            
            # Ensure we have a valid config
            if config is None:
                config = get_current_config()
            
            # Apply any parameter overrides directly to config
            hpo_config = config.setdefault('hyperparameter_optimization', {})
            
            if trial_count is not None:
                hpo_config['n_trials'] = trial_count
            if timeout_seconds is not None:
                hpo_config['timeout'] = timeout_seconds
                hpo_config['timeout_seconds'] = timeout_seconds
            if sampler_type:
                hpo_config['sampler'] = sampler_type
            if pruner_type:
                hpo_config['pruner'] = pruner_type
            if enable_storage is not None:
                storage_config = hpo_config.setdefault('storage', {})
                storage_config['enabled'] = enable_storage
            if enable_plots is not None:
                hpo_config['generate_plots'] = enable_plots
            if custom_search_space:
                hpo_config['optimization_space'] = custom_search_space
            if study_name:
                hpo_config['study_name'] = study_name
            
            # Set operation mode metadata
            metadata = config.setdefault('metadata', {})
            metadata['recursion_bypass'] = True
            metadata['recursion_depth'] = recursion_depth
            metadata['operation_mode'] = operation_mode
            
            # Launch HPO directly without further interactive setup
            return _launch_hpo_with_config(config=config, **kwargs)
        
        # Increment recursion depth for tracking
        kwargs['_recursion_depth'] = recursion_depth + 1
        
        # Clear screen and show banner
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None and banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Get hardware context for system-aware configuration
        if hardware_data is None:
            try:
                hardware_data = check_hardware(include_memory_usage=True)
            except Exception as e:
                logger.debug(f"Hardware detection failed: {e}")
                hardware_data = {}
        
        # Extract configuration context for display
        hpo_config = config.get('hyperparameter_optimization', {})
        data_config = config.get('data', {})
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        metadata = config.get('metadata', {})
        presets_section = config.get('presets', {})
        
        # Context extraction with preset compatibility
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Determine preset name from multiple sources
        # Method 1: Check presets section
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        else:
            config_source = "Unknown"
        
        # HPO-specific context from current preset configuration
        hpo_strategy = hpo_config.get('strategy', 'optuna')
        hpo_trials = hpo_config.get('n_trials', 50)
        hpo_timeout = hpo_config.get('timeout', 3600)
        hpo_enabled = hpo_config.get('enabled', False)
        hpo_sampler = hpo_config.get('sampler', 'TPESampler')
        hpo_pruner = hpo_config.get('pruner', 'MedianPruner')
        
        # Training configuration context
        epochs = training_config.get('epochs', 100)
        batch_size = training_config.get('batch_size', 64)
        learning_rate = training_config.get('learning_rate', 0.001)
        
        # Data configuration context
        normal_samples = data_config.get('normal_samples', 8000)
        attack_samples = data_config.get('attack_samples', 2000)
        features = data_config.get('features', 20)
        data_path = data_config.get('data_path', 'Default')
        use_real_data_config = data_config.get('use_real_data', False)
        
        # Resolve use_real_data parameter with preset compatibility
        if use_real_data is None:
            use_real_data = use_real_data_config
        else:
            use_real_data_config = use_real_data
        
        # Resolve data mode from use_real_data parameter
        if use_real_data is True:
            data_mode = 'real'
        elif use_real_data is False:
            data_mode = 'synthetic'
        elif data_mode is None:
            data_mode = 'auto'
        
        # Apply parameter overrides with preset validation
        if trial_count is not None:
            hpo_trials = trial_count
        else:
            trial_count = hpo_trials

        if timeout_seconds is not None:
            hpo_timeout = timeout_seconds
        else:
            timeout_seconds = hpo_timeout
        
        # Hardware-aware system class detection
        cuda_available = hardware_data.get('cuda', {}).get('available', False)
        gpu_count = hardware_data.get('cuda', {}).get('gpu_count', 0)
        memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)
        
        # Determine system performance class
        if cuda_available and memory_gb >= 16 and cpu_cores >= 8:
            system_class = "high_performance"
        elif cuda_available and memory_gb >= 8:
            system_class = "performance"
        elif memory_gb >= 4:
            system_class = "standard"
        else:
            system_class = "limited"
        
        # Header with context awareness
        header_title = "INTERACTIVE HYPERPARAMETER OPTIMIZATION SETUP"
        if operation_mode == 'quick_test':
            header_title = "QUICK HPO TEST LAUNCHER"
        elif operation_mode == 'model_comparison':
            header_title = "MODEL COMPARISON HPO LAUNCHER"
        elif operation_mode == 'continue':
            header_title = "STUDY CONTINUATION SETUP"
        
        print(Fore.MAGENTA + Style.BRIGHT + header_title)
        print(Fore.CYAN + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        
        # Display recursion depth if applicable (for debugging)
        if recursion_depth > 0:
            print(Fore.BLUE + Style.BRIGHT + f"Recursion Depth: {recursion_depth}/{max_recursion_depth}")
        
        # Display current configuration context
        print(Fore.YELLOW + Style.BRIGHT + "Current Configuration Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name.title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Config Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        
        # HPO Configuration from current preset
        print(Fore.CYAN + Style.BRIGHT + "\nHPO Configuration (from preset):")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ HPO Enabled: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_enabled}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Strategy: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_strategy}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Default Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_trials}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_timeout}s")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_sampler}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_pruner}")
        
        # Training Configuration from current preset
        print(Fore.BLUE + Style.BRIGHT + "\nTraining Configuration (from preset):")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{epochs}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.YELLOW + Style.BRIGHT + f"{batch_size}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Learning Rate: " + Fore.YELLOW + Style.BRIGHT + f"{learning_rate}")
        
        # Data Configuration from current preset
        print(Fore.CYAN + Style.BRIGHT + "\nData Configuration (from preset):")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.YELLOW + Style.BRIGHT + f"{normal_samples}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.YELLOW + Style.BRIGHT + f"{attack_samples}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Data Source: " + Fore.YELLOW + Style.BRIGHT + f"{'Real' if use_real_data_config else 'Synthetic'}")
        
        # Display extended parameters if provided
        extended_params = []
        if operation_mode:
            extended_params.append(f"Operation Mode: {operation_mode}")
            if operation_mode == 'quick_test':
                extended_params.append(f"Test Type: Quick HPO (Hardware-aware optimization)")
            elif operation_mode == 'model_comparison':
                extended_params.append(f"Comparison: Multiple model architectures")
        
        if data_mode and data_mode != 'auto':
            extended_params.append(f"Data Mode: {data_mode}")
        if optimization_focus:
            extended_params.append(f"Optimization Focus: {optimization_focus}")
        if trial_count and trial_count != hpo_trials:
            extended_params.append(f"Trial Count Override: {trial_count}")
        if timeout_seconds and timeout_seconds != hpo_timeout:
            extended_params.append(f"Timeout Override: {timeout_seconds}s")
        if study_name:
            extended_params.append(f"Study Name: {study_name}")
        if storage_url:
            extended_params.append(f"Storage URL: {storage_url}")
        if sampler_type and sampler_type != hpo_sampler:
            extended_params.append(f"Sampler Override: {sampler_type}")
        if pruner_type and pruner_type != hpo_pruner:
            extended_params.append(f"Pruner Override: {pruner_type}")
        if enable_storage is not None:
            extended_params.append(f"Storage Override: {enable_storage}")
        if enable_plots is not None:
            extended_params.append(f"Plots Override: {enable_plots}")
        if custom_search_space:
            extended_params.append("Custom Search Space: Provided")
        if model_types:
            extended_params.append(f"Model Types: {', '.join(model_types)}")
        
        if extended_params:
            print(Fore.CYAN + Style.BRIGHT + "\nParameter Overrides:")
            for i, param in enumerate(extended_params):
                prefix = "  └─" if i == len(extended_params) - 1 else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {param}")
        
        # Display hardware context
        print(Fore.MAGENTA + Style.BRIGHT + "\nHardware Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
        if cuda_available:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ GPU Count: " + Fore.YELLOW + Style.BRIGHT + f"{gpu_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Memory: " + Fore.YELLOW + Style.BRIGHT + f"{memory_gb:.1f}GB")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ CPU Cores: " + Fore.YELLOW + Style.BRIGHT + f"{cpu_cores}")
        
        # Configuration loading logic with preset validation
        if config:
            base_config = deepcopy(config)
            config_source_desc = "provided configuration"
            if operation_mode == 'quick_test':
                config_source_desc = "quick test configuration"
            print(Fore.GREEN + Style.BRIGHT + f"\nUsing {config_source_desc} as base:")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        elif use_current_config:
            try:
                base_config = get_current_config()
                print(Fore.GREEN + Style.BRIGHT + "\nUsing current system configuration:")
                print(Fore.YELLOW + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
            except Exception as e:
                logger.warning(f"Failed to load current config: {e}")
                base_config = deepcopy(PRESET_CONFIGS.get('default', _create_minimal_fallback_config('standard')))
                print(Fore.RED + Style.BRIGHT + "\nFailed to load current config, using default preset:")
                print(Fore.YELLOW + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        else:
            # Use current preset as base
            base_config = deepcopy(PRESET_CONFIGS.get(preset_name, PRESET_CONFIGS.get('default', _create_minimal_fallback_config('standard'))))
            print(Fore.CYAN + Style.BRIGHT + f"\nUsing {preset_name} preset as base configuration:")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        
        # Preset application with validation
        if preset and preset != preset_name:
            try:
                if preset in PRESET_CONFIGS:
                    preset_config = deepcopy(PRESET_CONFIGS[preset])
                    
                    # Deep merge the preset configuration into base_config
                    for section, values in preset_config.items():
                        if section not in base_config:
                            base_config[section] = values
                        elif isinstance(values, dict) and isinstance(base_config[section], dict):
                            base_config[section] = deep_update(base_config[section], values)
                        else:
                            base_config[section] = values
                    
                    # Update preset tracking
                    if 'presets' not in base_config:
                        base_config['presets'] = {}
                    base_config['presets']['current_preset'] = preset
                    
                    # Extract and display HPO settings from the new preset
                    hpo_settings = preset_config.get('hyperparameter_optimization', {})
                    print(Fore.GREEN + Style.BRIGHT + f"\nSuccessfully applied preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset}")
                    print(Fore.YELLOW + Style.BRIGHT + f"\nUpdated HPO Configuration:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Enabled: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_settings.get('enabled', False)}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Strategy: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_settings.get('strategy', 'optuna')}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_settings.get('n_trials', 50)}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_settings.get('sampler', 'TPESampler')}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_settings.get('pruner', 'MedianPruner')}")
                    
                    # Update local variables with new preset values
                    preset_name = preset
                    hpo_trials = hpo_settings.get('n_trials', hpo_trials)
                    hpo_timeout = hpo_settings.get('timeout', hpo_timeout)
                    hpo_enabled = hpo_settings.get('enabled', hpo_enabled)
                    
                else:
                    available_presets = list(PRESET_CONFIGS.keys())
                    print(Fore.RED + Style.BRIGHT + f"\nPreset '{preset}' not found in PRESET_CONFIGS.")
                    print(Fore.GREEN + Style.BRIGHT + f"Available presets: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(available_presets)}")
                    
            except Exception as e:
                logger.warning(f"Failed to apply preset '{preset}': {e}")
                print(Fore.RED + Style.BRIGHT + f"\nError applying preset '{preset}': {str(e)}")
        
        # Apply parameter overrides to configuration
        if base_config.get('hyperparameter_optimization'):
            hpo_section = base_config['hyperparameter_optimization']
            
            # Apply overrides with validation using tree structure
            overrides_applied = []
            
            if trial_count is not None and trial_count != hpo_section.get('n_trials'):
                hpo_section['n_trials'] = trial_count
                overrides_applied.append(f"Trial count: {trial_count}")
            
            if timeout_seconds is not None and timeout_seconds != hpo_section.get('timeout'):
                hpo_section['timeout'] = timeout_seconds
                overrides_applied.append(f"Timeout: {timeout_seconds}s")
            
            if sampler_type and sampler_type != hpo_section.get('sampler'):
                hpo_section['sampler'] = sampler_type
                overrides_applied.append(f"Sampler: {sampler_type}")
            
            if pruner_type and pruner_type != hpo_section.get('pruner'):
                hpo_section['pruner'] = pruner_type
                overrides_applied.append(f"Pruner: {pruner_type}")
            
            if enable_storage is not None:
                storage_config = hpo_section.get('storage', {})
                storage_config['enabled'] = enable_storage
                hpo_section['storage'] = storage_config
                overrides_applied.append(f"Storage: {enable_storage}")
            
            if enable_plots is not None:
                hpo_section['generate_plots'] = enable_plots
                overrides_applied.append(f"Plots: {enable_plots}")
            
            if custom_search_space:
                hpo_section['optimization_space'].update(custom_search_space)
                overrides_applied.append("Custom search space parameters")
            
            if overrides_applied:
                print(Fore.CYAN + Style.BRIGHT + "\nApplied Parameter Overrides:")
                for i, override in enumerate(overrides_applied):
                    prefix = "  └─" if i == len(overrides_applied) - 1 else "  ├─"
                    print(Fore.GREEN + Style.BRIGHT + f"{prefix} {override}")
        
        # Handle operation_mode-based auto-routing
        if operation_mode and not force_express:
            print(Fore.CYAN + Style.BRIGHT + f"\nAuto-routing based on operation mode: {operation_mode.upper()}")
            
            if operation_mode == 'express':
                # Pass all additional parameters to express setup function
                return _interactive_hpo_express_setup(
                    base_config,
                    use_current_config=use_current_config,
                    data_mode=data_mode,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    operation_mode=operation_mode,
                    optimization_focus=optimization_focus,
                    non_interactive=non_interactive,
                    skip_prompt=skip_prompt,
                    **kwargs
                )
            elif operation_mode == 'custom':
                return _interactive_hpo_custom_setup(
                    base_config,
                    data_mode=data_mode,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    study_name=study_name,
                    storage_url=storage_url,
                    non_interactive=non_interactive,
                    skip_prompt=skip_prompt,
                    operation_mode=operation_mode,
                    use_current_config=use_current_config,
                    force_express=force_express,
                    model_types=model_types,
                    **kwargs
                )
            elif operation_mode == 'preset':
                return _interactive_hpo_preset_setup(
                    base_config,
                    data_mode=data_mode,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    study_name=study_name,
                    storage_url=storage_url,
                    non_interactive=non_interactive,
                    skip_prompt=skip_prompt,
                    operation_mode=operation_mode,
                    use_current_config=use_current_config,
                    force_express=force_express,
                    model_types=model_types,
                    **kwargs
                )
            elif operation_mode == 'continue':
                return _interactive_hpo_continue_setup(
                    base_config,
                    study_name=study_name,
                    storage_url=storage_url,
                    hardware_data=hardware_data,
                    data_mode=data_mode,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    non_interactive=non_interactive,
                    skip_prompt=skip_prompt,
                    operation_mode=operation_mode,
                    use_current_config=use_current_config,
                    force_express=force_express,
                    model_types=model_types,
                    **kwargs
                )
            elif operation_mode == 'quick_test':
                # Use hardware-aware quick test parameters
                return _run_quick_hpo_test(
                    config=base_config,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    data_mode=data_mode,
                    operation_mode=operation_mode,
                    use_current_config=use_current_config,
                    non_interactive=non_interactive,
                    skip_prompt=skip_prompt,
                    force_express=force_express,
                    study_name=study_name,
                    model_types=model_types,
                    optimization_focus=optimization_focus,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    **kwargs
                )
            elif operation_mode == 'model_comparison':
                comparison_models = model_types or list(MODEL_VARIANTS.keys())
                # Pass all additional parameters to model comparison function
                return _run_hpo_model_comparison(
                    config=base_config,
                    model_types=comparison_models,
                    non_interactive=non_interactive,
                    skip_prompt=skip_prompt,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    data_mode=data_mode,
                    optimization_focus=optimization_focus,
                    operation_mode=operation_mode,
                    **kwargs
                )
        
        # Handle non-interactive mode with feedback
        if non_interactive or (use_current_config and skip_prompt):
            mode_description = "Non-interactive Mode"
            if operation_mode == 'quick_test':
                mode_description = "Quick Test Mode"
            elif operation_mode == 'model_comparison':
                mode_description = "Model Comparison Mode"
                
            print(Fore.GREEN + Style.BRIGHT + f"\n{mode_description} - Using Configured Defaults.")
            print(Fore.YELLOW + Style.BRIGHT + f"\nConfiguration Summary:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ HPO Strategy: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_strategy}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials: " + Fore.YELLOW + Style.BRIGHT + f"{trial_count or hpo_trials}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{timeout_seconds or hpo_timeout}s")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class}")
            
            # Apply data mode if specified
            if data_mode and data_mode != 'auto':
                if 'data' not in base_config:
                    base_config['data'] = {}
                if data_mode == 'synthetic':
                    base_config['data']['use_real_data'] = False
                elif data_mode == 'real':
                    base_config['data']['use_real_data'] = True
            
            # Handle specific operation modes in non-interactive
            if operation_mode == 'express':
                return _interactive_hpo_express_setup(
                    config=base_config,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    data_mode=data_mode,
                    use_current_config=True,
                    non_interactive=True,
                    skip_prompt=True,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    **kwargs
                )
            elif operation_mode == 'quick_test':
                return _run_quick_hpo_test(
                    config=base_config,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    data_mode=data_mode,
                    non_interactive=True,
                    skip_prompt=True,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    **kwargs
                )
            elif operation_mode == 'model_comparison':
                comparison_models = model_types or list(MODEL_VARIANTS.keys())
                # Pass all additional parameters to model comparison function
                return _run_hpo_model_comparison(
                    config=base_config,
                    model_types=comparison_models,
                    non_interactive=True,
                    skip_prompt=True,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    data_mode=data_mode,
                    optimization_focus=optimization_focus,
                    operation_mode=operation_mode,
                    **kwargs
                )
            else:
                return _launch_hpo_with_config(config=base_config, hardware_data=hardware_data, **kwargs)
        
        # Interactive menu with preset-aware options
        quick_start_options = [
            "1. Express HPO " + Fore.GREEN + Style.BRIGHT + f"(Quick optimization for {model_type})",
            "2. Custom HPO Configuration " + Fore.GREEN + Style.BRIGHT + "(Full control & advanced options)",
            "3. Use Different HPO Preset " + Fore.GREEN + Style.BRIGHT + f"(Available: {len(PRESET_CONFIGS)} presets)",
            "4. Continue Existing Study " + Fore.GREEN + Style.BRIGHT + "(Resume previous optimization)",
            "5. Quick HPO Test " + Fore.GREEN + Style.BRIGHT + f"(Hardware-aware validation - optimized for {system_class})"
        ]
        
        # Add model comparison option if multiple models available
        if len(MODEL_VARIANTS) > 1:
            quick_start_options.append("6. Model Comparison HPO " + Fore.GREEN + Style.BRIGHT + f"(Compare {len(MODEL_VARIANTS)} model types)")
        
        print(Fore.YELLOW + Style.BRIGHT + "\nQuick Start Options:")
        for option in quick_start_options:
            print(Fore.WHITE + Style.BRIGHT + option)
        
        # Add data mode options if not already specified
        if data_mode == 'auto':
            option_number = len(quick_start_options) + 1
            print(Fore.WHITE + Style.BRIGHT + f"{option_number}. Synthetic Data HPO " + Fore.GREEN + Style.BRIGHT + f"(Generate {normal_samples} samples)")
            option_number += 1
            print(Fore.WHITE + Style.BRIGHT + f"{option_number}. Real Data HPO " + Fore.GREEN + Style.BRIGHT + f"(Use: {data_path})")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and Return to Previous Menu")
            choice_range = f"0-{option_number}"
            max_choice = option_number
        else:
            print(Fore.RED + Style.BRIGHT + "0. Cancel and Return to Previous Menu")
            choice_range = f"0-{len(quick_start_options)}"
            max_choice = len(quick_start_options)
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect option ({choice_range}): ").strip()
                
                if not choice:
                    continue
                
                try:
                    choice_num = int(choice)
                    if choice_num < 0 or choice_num > max_choice:
                        print(Fore.RED + Style.BRIGHT + f"\nInvalid choice. Please select {choice_range}.")
                        choice = None
                        continue
                except ValueError:
                    print(Fore.RED + Style.BRIGHT + f"\nInvalid input. Please enter a number {choice_range}.")
                    choice = None
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nHPO setup interrupted by user!")
                return None
        
        choice_num = int(choice)
        
        # Option handling with preset awareness
        if choice_num == 1:
            # Pass all additional parameters to express setup function
            return _interactive_hpo_express_setup(
                base_config,
                data_mode=data_mode,
                hardware_data=hardware_data,
                enable_storage=enable_storage,
                enable_plots=enable_plots,
                custom_search_space=custom_search_space,
                sampler_type=sampler_type,
                pruner_type=pruner_type,
                trial_count=trial_count,
                timeout_seconds=timeout_seconds,
                optimization_focus=optimization_focus,
                **kwargs
            )
            
        elif choice_num == 2:
            return _interactive_hpo_custom_setup(
                base_config,
                data_mode=data_mode,
                hardware_data=hardware_data,
                enable_storage=enable_storage,
                enable_plots=enable_plots,
                custom_search_space=custom_search_space,
                sampler_type=sampler_type,
                pruner_type=pruner_type,
                trial_count=trial_count,
                timeout_seconds=timeout_seconds,
                optimization_focus=optimization_focus,
                study_name=study_name,
                storage_url=storage_url,
                non_interactive=non_interactive,
                skip_prompt=skip_prompt,
                operation_mode=operation_mode,
                use_current_config=use_current_config,
                force_express=force_express,
                model_types=model_types,
                **kwargs
            )
            
        elif choice_num == 3:
            return _interactive_hpo_preset_setup(
                base_config,
                data_mode=data_mode,
                hardware_data=hardware_data,
                enable_storage=enable_storage,
                enable_plots=enable_plots,
                custom_search_space=custom_search_space,
                sampler_type=sampler_type,
                pruner_type=pruner_type,
                trial_count=trial_count,
                timeout_seconds=timeout_seconds,
                optimization_focus=optimization_focus,
                study_name=study_name,
                storage_url=storage_url,
                non_interactive=non_interactive,
                skip_prompt=skip_prompt,
                operation_mode=operation_mode,
                use_current_config=use_current_config,
                force_express=force_express,
                model_types=model_types,
                **kwargs
            )
            
        elif choice_num == 4:
            return _interactive_hpo_continue_setup(
                base_config,
                study_name=study_name,
                storage_url=storage_url,
                hardware_data=hardware_data,
                data_mode=data_mode,
                enable_storage=enable_storage,
                enable_plots=enable_plots,
                custom_search_space=custom_search_space,
                sampler_type=sampler_type,
                pruner_type=pruner_type,
                trial_count=trial_count,
                timeout_seconds=timeout_seconds,
                optimization_focus=optimization_focus,
                non_interactive=non_interactive,
                skip_prompt=skip_prompt,
                operation_mode=operation_mode,
                use_current_config=use_current_config,
                force_express=force_express,
                model_types=model_types,
                **kwargs
            )
            
        elif choice_num == 5:
            return _run_quick_hpo_test(
                config=base_config,
                data_mode=data_mode,
                trial_count=trial_count,
                timeout_seconds=timeout_seconds,
                hardware_data=hardware_data,
                enable_storage=enable_storage,
                enable_plots=enable_plots,
                custom_search_space=custom_search_space,
                sampler_type=sampler_type,
                pruner_type=pruner_type,
                **kwargs
            )
            
        elif choice_num == 6 and len(MODEL_VARIANTS) > 1:
            comparison_models = model_types or list(MODEL_VARIANTS.keys())
            # Pass all additional parameters to model comparison function
            return _run_hpo_model_comparison(
                config=base_config,
                model_types=comparison_models,
                hardware_data=hardware_data,
                enable_storage=enable_storage,
                enable_plots=enable_plots,
                custom_search_space=custom_search_space,
                sampler_type=sampler_type,
                pruner_type=pruner_type,
                trial_count=trial_count,
                timeout_seconds=timeout_seconds,
                data_mode=data_mode,
                optimization_focus=optimization_focus,
                **kwargs
            )
            
        elif data_mode == 'auto':
            # Handle data mode shortcuts
            if choice_num == len(quick_start_options) + 1:
                # Synthetic data shortcut: Pass all parameters
                return _interactive_hpo_express_setup(
                    base_config,
                    data_mode='synthetic',
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    **kwargs
                )
            elif choice_num == len(quick_start_options) + 2:
                # Real data shortcut: Pass all parameters
                return _interactive_hpo_express_setup(
                    base_config,
                    data_mode='real',
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    **kwargs
                )
            
        elif choice_num == 0:
            print(Fore.RED + Style.BRIGHT + "\nHPO setup cancelled by user!")
            return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\nHPO setup interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"Interactive HPO setup failed: {e}", exc_info=True)
        
        # error context
        error_context = {
            "Preset": preset_name if 'preset_name' in locals() else 'Unknown',
            "Model": model_type if 'model_type' in locals() else 'Unknown',
            "Operation Mode": operation_mode if operation_mode else 'Not specified',
            "Data Mode": data_mode if data_mode else 'Not specified',
            "System Class": system_class if 'system_class' in locals() else 'Unknown',
            "Use Current Config": use_current_config,
            "Hardware Available": bool(hardware_data) if 'hardware_data' in locals() else False,
            "CUDA Available": cuda_available if 'cuda_available' in locals() else False,
            "Preset Configs Available": len(PRESET_CONFIGS),
            "Model Variants Available": len(MODEL_VARIANTS)
        }
        
        message = (
            f"Interactive HPO setup failed: {str(e)}\n\n"
            f"Context:\n" +
            "\n".join([f"├─ {key}: {value}" for key, value in list(error_context.items())[:-1]]) +
            f"\n└─ {list(error_context.items())[-1][0]}: {list(error_context.items())[-1][1]}" +
            f"\n\nThis could be due to:\n"
            f"├─ Configuration file corruption\n"
            f"├─ Missing or invalid preset definitions\n"
            f"├─ System resource constraints\n"
            f"├─ Invalid parameter combinations\n"
            f"├─ Preset compatibility issues\n"
            f"└─ Model variant initialization failure"
        )
        
        print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
        print(Fore.RED + Style.BRIGHT + "HPO SETUP ERROR")
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + message)
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        
        return None

def _run_quick_hpo_test(
    config: Dict[str, Any],
    trial_count: Optional[int] = None,
    timeout_seconds: Optional[int] = None,
    data_mode: Optional[str] = None,
    operation_mode: Optional[str] = None,
    use_current_config: bool = False,
    non_interactive: bool = False,
    skip_prompt: bool = False,
    force_express: bool = False,
    study_name: Optional[str] = None,
    model_types: Optional[List[str]] = None,
    optimization_focus: Optional[str] = None,
    hardware_data: Optional[Dict[str, Any]] = None,
    enable_storage: Optional[bool] = None,
    enable_plots: Optional[bool] = None,
    custom_search_space: Optional[Dict[str, Any]] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Run a quick HPO test with comprehensive menu options and enhanced user interaction.
    
    Provides a streamlined HPO testing experience with optimized parameters for rapid
    experimentation and system validation.
    """
    try:
        # Clear screen and show banner with config
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None and banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Get hardware context for system-aware configuration
        if hardware_data is None:
            try:
                hardware_data = check_hardware(include_memory_usage=True)
            except Exception as e:
                logger.debug(f"Hardware detection failed: {e}")
                hardware_data = {}
        
        # Extract configuration context for display
        hpo_config = config.get('hyperparameter_optimization', {})
        data_config = config.get('data', {})
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        presets_section = config.get('presets', {})
        metadata = config.get('metadata', {})
        
        # Context extraction with preset compatibility
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Extract preset name with multiple fallbacks
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        else:
            config_source = "Unknown"
        
        # Hardware context extraction
        cuda_available = hardware_data.get('cuda', {}).get('available', False)
        gpu_count = hardware_data.get('cuda', {}).get('gpu_count', 0)
        memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)
        
        # Determine system performance class
        if cuda_available and memory_gb >= 16 and cpu_cores >= 8:
            system_class = "high_performance"
            base_quick_trials = 15
        elif cuda_available and memory_gb >= 8:
            system_class = "performance" 
            base_quick_trials = 12
        elif memory_gb >= 4:
            system_class = "standard"
            base_quick_trials = 10
        else:
            system_class = "limited"
            base_quick_trials = 8
        
        # Apply parameter overrides with system awareness
        quick_trial_count = trial_count if trial_count is not None else base_quick_trials
        quick_timeout = timeout_seconds if timeout_seconds is not None else 900
        
        # Ensure quick test limits are respected
        quick_trial_count = min(quick_trial_count, 20)
        quick_timeout = min(quick_timeout, 1800)
        
        # Resolve data mode
        use_real_data_config = data_config.get('use_real_data', False)
        if data_mode is None:
            if use_real_data_config is True:
                quick_data_mode = 'real'
            elif use_real_data_config is False:
                quick_data_mode = 'synthetic'
            else:
                quick_data_mode = 'synthetic'
        else:
            quick_data_mode = data_mode
        
        # Header with context display
        print(Fore.MAGENTA + Style.BRIGHT + "QUICK HPO TEST CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        print(Fore.YELLOW + Style.BRIGHT + "Base Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Data Mode: " + Fore.YELLOW + Style.BRIGHT + f"{quick_data_mode}")
        
        # Display hardware context
        print(Fore.MAGENTA + Style.BRIGHT + "\nHardware Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
        if cuda_available:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ GPU Count: " + Fore.YELLOW + Style.BRIGHT + f"{gpu_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Memory: " + Fore.YELLOW + Style.BRIGHT + f"{memory_gb:.1f}GB")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ CPU Cores: " + Fore.YELLOW + Style.BRIGHT + f"{cpu_cores}")
        
        # Display parameter overrides if provided
        override_params = []
        if trial_count is not None:
            override_params.append(f"Trial Count: {trial_count}")
        if timeout_seconds is not None:
            override_params.append(f"Timeout: {timeout_seconds}s")
        if sampler_type:
            override_params.append(f"Sampler: {sampler_type}")
        if pruner_type:
            override_params.append(f"Pruner: {pruner_type}")
        if optimization_focus:
            override_params.append(f"Optimization Focus: {optimization_focus}")
        if enable_storage is not None:
            override_params.append(f"Storage: {enable_storage}")
        if enable_plots is not None:
            override_params.append(f"Plots: {enable_plots}")
        if custom_search_space:
            override_params.append("Custom Search Space: Provided")
        
        if override_params:
            print(Fore.CYAN + Style.BRIGHT + "\nParameter Overrides:")
            for i, param in enumerate(override_params):
                prefix = "  └─" if i == len(override_params) - 1 else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {param}")
        
        # Quick HPO test features
        print(Fore.YELLOW + Style.BRIGHT + "\nQuick HPO Test Features:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Optimized parameters for rapid testing")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System-aware configuration")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Performance metrics and validation")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Resource-efficient execution")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Immediate feedback with optimization results")
        
        # Default configuration display
        print(Fore.YELLOW + Style.BRIGHT + "\nDefault Quick HPO Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Trials: " + Fore.GREEN + Style.BRIGHT + f"{quick_trial_count}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.GREEN + Style.BRIGHT + f"{quick_timeout}s")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data Mode: " + Fore.GREEN + Style.BRIGHT + f"{quick_data_mode.title()}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Epochs per Trial: " + Fore.GREEN + Style.BRIGHT + f"5")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Cross-Validation: " + Fore.GREEN + Style.BRIGHT + f"3-fold")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Early Stopping: " + Fore.GREEN + Style.BRIGHT + f"Enabled")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.GREEN + Style.BRIGHT + f"{sampler_type or 'TPESampler'}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Pruner: " + Fore.GREEN + Style.BRIGHT + f"{pruner_type or 'MedianPruner'}")
        
        estimated_time = (quick_trial_count * 0.5) if cuda_available else (quick_trial_count * 2.0)
        print(Fore.YELLOW + Style.BRIGHT + "\nPerformance Estimates:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Estimated Time: " + Fore.GREEN + Style.BRIGHT + f"~{estimated_time:.1f} minutes")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Trials per Minute: " + Fore.GREEN + Style.BRIGHT + f"~{quick_trial_count / (estimated_time + 0.1):.1f}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ System Optimization: " + Fore.GREEN + Style.BRIGHT + f"{system_class.upper()}")
        
        # Handle non-interactive mode
        if non_interactive or skip_prompt:
            print(Fore.GREEN + Style.BRIGHT + f"\nNon-interactive Mode - Using default quick HPO configuration...")
            quick_config = None  # Will be created below
        else:
            # Enhanced customization options
            print(Fore.YELLOW + Style.BRIGHT + f"\nCustomization Options:")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            print(Fore.WHITE + Style.BRIGHT + "1. Use default quick settings " + Fore.GREEN + Style.BRIGHT + "(Recommended)")
            print(Fore.WHITE + Style.BRIGHT + "2. Adjust trial count " + Fore.GREEN + Style.BRIGHT + f"(Current: {quick_trial_count})")
            print(Fore.WHITE + Style.BRIGHT + "3. Adjust timeout " + Fore.GREEN + Style.BRIGHT + f"(Current: {quick_timeout}s)")
            print(Fore.WHITE + Style.BRIGHT + "4. Modify data source " + Fore.GREEN + Style.BRIGHT + f"(Current: {quick_data_mode})")
            print(Fore.WHITE + Style.BRIGHT + "5. Modify sampler/pruner " + Fore.GREEN + Style.BRIGHT + "(Advanced)")
            print(Fore.WHITE + Style.BRIGHT + "6. Advanced quick setup " + Fore.GREEN + Style.BRIGHT + "(Full Control)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            custom_choice = None
            while not custom_choice:
                try:
                    custom_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-6): " + Style.RESET_ALL).strip()
                    
                    if not custom_choice:
                        continue
                        
                    if custom_choice not in ['1', '2', '3', '4', '5', '6', '0']:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-6.")
                        custom_choice = None
                        continue
                        
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nCustomization selection cancelled")
                    return None
            
            if custom_choice == '0':
                print(Fore.RED + Style.BRIGHT + "\nQuick HPO test cancelled")
                return None
            
            # Apply customizations based on user choice
            if custom_choice in ['2', '3', '4', '5', '6']:
                # Trial count adjustment
                if custom_choice in ['2', '3', '4', '5', '6']:
                    print(Fore.MAGENTA + Style.BRIGHT + "\nTRIAL COUNT ADJUSTMENT")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    current_trials = quick_trial_count
                    
                    print(Fore.CYAN + Style.BRIGHT + f"Current: " + Fore.GREEN + Style.BRIGHT + f"{current_trials} trials")
                    print(Fore.CYAN + Style.BRIGHT + "Quick: 5 trials | Standard: 10 trials | Thorough: 15+ trials")
                    
                    try:
                        new_trials = input(Fore.YELLOW + Style.BRIGHT + f"New trial count (Enter for {current_trials}, 'c' to cancel): " + Style.RESET_ALL).strip()
                        
                        if new_trials.lower() == 'c':
                            print(Fore.RED + Style.BRIGHT + "\nTrial count adjustment cancelled")
                            return None
                        elif new_trials:
                            new_trials_int = int(new_trials)
                            if new_trials_int > 0:
                                quick_trial_count = min(new_trials_int, 20)
                                print(Fore.GREEN + Style.BRIGHT + f"\nUpdated: {quick_trial_count} trials")
                            else:
                                print(Fore.RED + Style.BRIGHT + "\nInvalid trial count. Must be positive.")
                                print(Fore.GREEN + Style.BRIGHT + f"\nKeeping: {current_trials} trials")
                        else:
                            print(Fore.GREEN + Style.BRIGHT + f"\nKeeping: {current_trials} trials")
                            
                    except ValueError:
                        print(Fore.RED + Style.BRIGHT + f"\nInvalid input, keeping {current_trials} trials")
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nTrial count adjustment cancelled")
                        return None
                
                # Timeout adjustment
                if custom_choice in ['3', '4', '5', '6']:
                    print(Fore.MAGENTA + Style.BRIGHT + "\nTIMEOUT ADJUSTMENT")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    current_timeout = quick_timeout
                    
                    print(Fore.CYAN + Style.BRIGHT + f"Current: " + Fore.GREEN + Style.BRIGHT + f"{current_timeout}s ({current_timeout//60} minutes)")
                    print(Fore.CYAN + Style.BRIGHT + "Quick: 300s | Standard: 900s | Thorough: 1800s")
                    
                    try:
                        new_timeout_input = input(Fore.YELLOW + Style.BRIGHT + f"New timeout in seconds (Enter for {current_timeout}): ").strip()
                        
                        if not new_timeout_input:
                            print(Fore.GREEN + Style.BRIGHT + "\nKeeping current timeout.")
                        else:
                            new_timeout = int(new_timeout_input)
                            if new_timeout > 0:
                                quick_timeout = min(new_timeout, 1800)
                                print(Fore.GREEN + Style.BRIGHT + f"\nUpdated timeout to {quick_timeout}s ({quick_timeout//60} minutes)")
                            else:
                                print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
                    except ValueError:
                        print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nTimeout adjustment cancelled")
                        return None
                
                # Data source customization
                if custom_choice in ['4', '5', '6']:
                    print(Fore.MAGENTA + Style.BRIGHT + "\nDATA SOURCE SELECTION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    print(Fore.WHITE + Style.BRIGHT + "1. Synthetic data " + Fore.GREEN + Style.BRIGHT + "(testing/development)")
                    print(Fore.WHITE + Style.BRIGHT + "2. Real network data " + Fore.GREEN + Style.BRIGHT + "(production validation)")
                    print(Fore.RED + Style.BRIGHT + "0. Cancel and keep current")
                    
                    while True:
                        try:
                            data_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect data source (0-2, current: {quick_data_mode}): " + Style.RESET_ALL).strip()
                            if data_choice in ['1', '2', '0']:
                                break
                            print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                            return None
                    
                    if data_choice == '1':
                        quick_data_mode = 'synthetic'
                        print(Fore.GREEN + Style.BRIGHT + f"\nSelected: Synthetic Data")
                    elif data_choice == '2':
                        quick_data_mode = 'real'
                        print(Fore.GREEN + Style.BRIGHT + f"\nSelected: Real Data")
                    else:
                        print(Fore.GREEN + Style.BRIGHT + f"\nKeeping: {quick_data_mode.title()} Data")
                
                # Sampler and pruner customization
                if custom_choice in ['5', '6']:
                    print(Fore.MAGENTA + Style.BRIGHT + "\nOPTIMIZATION ALGORITHM")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    current_sampler = sampler_type or 'TPESampler'
                    current_pruner = pruner_type or 'MedianPruner'
                    
                    print(Fore.YELLOW + Style.BRIGHT + f"Current: " + Fore.CYAN + Style.BRIGHT + f"{current_sampler} + {current_pruner}")
                    print(Fore.WHITE + Style.BRIGHT + "1. Keep current algorithm")
                    print(Fore.WHITE + Style.BRIGHT + "2. TPESampler + MedianPruner " + Fore.GREEN + Style.BRIGHT + "(efficient, recommended)")
                    print(Fore.WHITE + Style.BRIGHT + "3. RandomSampler + HyperbandPruner " + Fore.GREEN + Style.BRIGHT + "(fast exploration)")
                    print(Fore.WHITE + Style.BRIGHT + "4. CmaEsSampler + MedianPruner " + Fore.GREEN + Style.BRIGHT + "(advanced optimization)")
                    print(Fore.RED + Style.BRIGHT + "0. Cancel and return")
                    
                    try:
                        algo_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect algorithm (0-4): " + Style.RESET_ALL).strip()
                        
                        if algo_choice == '0':
                            print(Fore.GREEN + Style.BRIGHT + f"\nKeeping current algorithm")
                        elif algo_choice in ['2', '3', '4']:
                            algo_configs = {
                                '2': ('TPESampler', 'MedianPruner'),
                                '3': ('RandomSampler', 'HyperbandPruner'),
                                '4': ('CmaEsSampler', 'MedianPruner')
                            }
                            sampler_type, pruner_type = algo_configs[algo_choice]
                            print(Fore.GREEN + Style.BRIGHT + f"\nChanged algorithm to: {sampler_type} + {pruner_type}")
                        else:
                            print(Fore.GREEN + Style.BRIGHT + f"\nKeeping: {current_sampler} + {current_pruner}")
                            
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nAlgorithm customization cancelled")
                        return None
                
                # Advanced customization fallback
                if custom_choice == '6':
                    print(Fore.GREEN + Style.BRIGHT + "\nSwitching to advanced quick setup...")
                    return _interactive_hpo_express_setup(
                        config,
                        data_mode=quick_data_mode,
                        hardware_data=hardware_data,
                        enable_storage=enable_storage,
                        enable_plots=enable_plots,
                        custom_search_space=custom_search_space,
                        sampler_type=sampler_type,
                        pruner_type=pruner_type,
                        trial_count=quick_trial_count,
                        timeout_seconds=quick_timeout,
                        optimization_focus=optimization_focus,
                        operation_mode='quick_test',
                        **kwargs
                    )
            
            # Final confirmation with updated configuration
            print(Fore.MAGENTA + Style.BRIGHT + f"\nFinal Quick HPO Configuration:")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            print(Fore.YELLOW + Style.BRIGHT + "Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Trials: " + Fore.GREEN + Style.BRIGHT + f"{quick_trial_count}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.GREEN + Style.BRIGHT + f"{quick_timeout}s ({quick_timeout//60} minutes)")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data Mode: " + Fore.GREEN + Style.BRIGHT + f"{quick_data_mode.title()}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.GREEN + Style.BRIGHT + f"{sampler_type or 'TPESampler'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Pruner: " + Fore.GREEN + Style.BRIGHT + f"{pruner_type or 'MedianPruner'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ System Class: " + Fore.GREEN + Style.BRIGHT + f"{system_class.upper()}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ CUDA Available: " + Fore.GREEN + Style.BRIGHT + f"{cuda_available}")
            
            estimated_time = (quick_trial_count * 0.5) if cuda_available else (quick_trial_count * 2.0)
            print(Fore.YELLOW + Style.BRIGHT + "\nPerformance Estimates:")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + Style.BRIGHT + f"~{estimated_time:.1f} minutes")
            
            if custom_choice != '1':
                print(Fore.YELLOW + Style.BRIGHT + f"\nCustomizations Applied:")
                customizations = []
                if custom_choice in ['2', '3', '4', '5', '6']:
                    customizations.append("Trial count")
                if custom_choice in ['3', '4', '5', '6']:
                    customizations.append("Timeout")
                if custom_choice in ['4', '5', '6']:
                    customizations.append("Data source")
                if custom_choice in ['5', '6']:
                    customizations.append("Optimization algorithm")
                
                for i, customization in enumerate(customizations):
                    prefix = "  └─" if i == len(customizations) - 1 else "  ├─"
                    print(Fore.CYAN + Style.BRIGHT + f"{prefix} {customization}")
            
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart quick HPO test with this configuration? (Y/n): " + Style.RESET_ALL).lower().strip()
            if confirm not in ('', 'y', 'yes'):
                print(Fore.RED + Style.BRIGHT + "\nQuick HPO test cancelled by user")
                return None
        
        # Generate study name if not provided
        if not study_name:
            study_name = f"quick_hpo_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Create quick test configuration directly in the main function
        quick_config = deepcopy(config) if config else {}
        
        # HPO configuration for quick test
        hpo_config = quick_config.setdefault('hyperparameter_optimization', {})
        hpo_config.update({
            'enabled': True,
            'n_trials': quick_trial_count,
            'timeout': quick_timeout,
            'strategy': 'optuna',
            'trial_epochs': 5,
            'trial_patience': 3,
            'model_search': {
                'enabled': False,
                'model_types': model_types or ['EnhancedAutoencoder']
            },
            'cross_validation': {
                'enabled': True,
                'folds': 3,
                'shuffle': True,
                'random_state': 42
            },
            'early_stopping': {
                'enabled': True,
                'patience': 5,
                'min_trials': min(5, quick_trial_count),
                'improvement_threshold': 0.01
            },
            'sampling': {
                'sampler': sampler_type or 'TPESampler',
                'n_startup_trials': min(5, quick_trial_count),
                'n_ei_candidates': 24
            },
            'pruning': {
                'pruner': pruner_type or 'MedianPruner',
                'n_startup_trials': min(5, quick_trial_count),
                'n_warmup_steps': 3
            },
            'generate_plots': enable_plots if enable_plots is not None else True,
            'save_study': enable_storage if enable_storage is not None else True,
            'study_name': study_name,
            'test_mode': True,
            'quick_test': True,
            'optimization_focus': optimization_focus or 'balanced'
        })
        
        # Apply custom search space if provided
        if custom_search_space:
            optimization_space = hpo_config.setdefault('optimization_space', {})
            optimization_space.update(custom_search_space)
        
        # Data configuration
        data_config = quick_config.setdefault('data', {})
        if quick_data_mode == 'synthetic':
            data_config.update({
                'use_real_data': False,
                'use_synthetic': True,
                'normal_samples': 5000,
                'attack_samples': 1000,
                'features': 20,
                'test_size': 0.2,
                'validation_size': 0.1,
                'random_state': 42,
                'synthetic_data': {
                    'enabled': True,
                    'complexity': 'medium',
                    'noise_level': 0.05,
                    'correlation_strength': 0.3
                },
                'quick_test_mode': True
            })
        else:
            data_config.update({
                'use_real_data': True,
                'use_synthetic': False,
                'test_size': 0.3,
                'validation_size': 0.1,
                'random_state': 42,
                'quick_test_mode': True,
                'data_loading': {
                    'max_samples': 10000,
                    'shuffle': True,
                    'random_seed': 42
                }
            })
        
        # System configuration
        system_config = quick_config.setdefault('system', {})
        system_config.update({
            'quick_test': True,
            'resource_limits': {
                'max_memory_usage': '4GB',
                'max_cpu_usage': 80,
                'max_gpu_usage': 70 if cuda_available else 0
            },
            'hardware_optimization': {
                'enabled': True,
                'system_class': system_class,
                'cuda_available': cuda_available
            }
        })
        
        # Add metadata for tracking
        metadata_config = quick_config.setdefault('metadata', {})
        metadata_config.update({
            'test_type': 'quick_hpo',
            'start_time': datetime.now().isoformat(),
            'quick_test': True,
            'operation_mode': 'quick_test',
            'hardware_context': {
                'system_class': system_class,
                'cuda_available': cuda_available,
                'memory_gb': memory_gb,
                'cpu_cores': cpu_cores
            }
        })
        
        print(Fore.GREEN + Style.BRIGHT + f"\nLaunching Quick HPO Test...")
        print(Fore.CYAN + Style.BRIGHT + f"Study Name: " + Fore.YELLOW + Style.BRIGHT + f"{study_name}")
        
        # Small delay to ensure user sees the configuration
        time.sleep(1)
        
        # Execute the HPO test
        kwargs['_recursion_depth'] = kwargs.get('_recursion_depth', 0) + 1
        result = run_hyperparameter_optimization_interactive(
            config=quick_config,
            use_current_config=False,
            operation_mode='quick_test',
            data_mode=quick_data_mode,
            trial_count=quick_trial_count,
            timeout_seconds=quick_timeout,
            non_interactive=True,
            skip_prompt=True,
            force_express=True,
            study_name=study_name,
            optimization_focus=optimization_focus or 'balanced',
            model_types=model_types,
            hardware_data=hardware_data,
            enable_storage=enable_storage,
            enable_plots=enable_plots,
            custom_search_space=custom_search_space,
            sampler_type=sampler_type,
            pruner_type=pruner_type,
            **kwargs
        )
        
        # Result handling
        if result:
            if '_handle_hpo_result' in globals():
                _handle_hpo_result(result, "Quick HPO Test")
            return result
        else:
            return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\nQuick HPO test interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"Quick HPO test setup failed: {e}", exc_info=True)
        
        error_context = {
            "Preset": preset_name if 'preset_name' in locals() else 'Unknown',
            "Model": model_type if 'model_type' in locals() else 'Unknown',
            "System Class": system_class if 'system_class' in locals() else 'Unknown',
            "Data Mode": quick_data_mode if 'quick_data_mode' in locals() else 'Unknown',
            "Trial Count": quick_trial_count if 'quick_trial_count' in locals() else 'Unknown',
            "Timeout": quick_timeout if 'quick_timeout' in locals() else 'Unknown',
            "CUDA Available": cuda_available if 'cuda_available' in locals() else False
        }
        
        message = (
            f"Quick HPO test setup failed: {str(e)}\n\n"
            f"Context:\n" +
            "\n".join([f"├─ {key}: {value}" for key, value in list(error_context.items())[:-1]]) +
            f"\n└─ {list(error_context.items())[-1][0]}: {list(error_context.items())[-1][1]}" +
            f"\n\nThis could be due to:\n"
            f"├─ Configuration conflicts\n"
            f"├─ System resource constraints\n"
            f"├─ Missing dependencies\n"
            f"├─ Data generation issues\n"
            f"├─ Model initialization problems\n"
            f"└─ Preset compatibility issues"
        )
        
        print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
        print(Fore.RED + Style.BRIGHT + "QUICK HPO TEST SETUP ERROR")
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + message)
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        
        return None

def _run_hpo_model_comparison(
    config: Dict[str, Any],
    model_types: Optional[List[str]] = None,
    non_interactive: bool = False,
    skip_prompt: bool = False,
    hardware_data: Optional[Dict[str, Any]] = None,
    enable_storage: Optional[bool] = None,
    enable_plots: Optional[bool] = None,
    custom_search_space: Optional[Dict[str, Any]] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    trial_count: Optional[int] = None,
    timeout_seconds: Optional[int] = None,
    data_mode: Optional[str] = None,
    optimization_focus: Optional[str] = None,
    operation_mode: Optional[str] = None,
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Run HPO comparing all available model types with context display and error handling.
    
    Args:
        config: Base configuration to use
        model_types: Specific model types to compare (None = all available)
        non_interactive: Run without user prompts
        skip_prompt: Skip confirmation prompts
        hardware_data: Pre-fetched hardware data for optimization
        enable_storage: Override storage settings
        enable_plots: Override plot generation settings
        custom_search_space: Custom search space parameters
        sampler_type: Override sampler type
        pruner_type: Override pruner type
        trial_count: Override number of trials
        timeout_seconds: Override timeout in seconds
        data_mode: Data source mode for comparison
        optimization_focus: Optimization focus for comparison
        operation_mode: Operation mode compatibility
        **kwargs: Additional parameters
        
    Returns:
        Dictionary with comparison results or None if cancelled/failed
    """
    try:
        # Clear screen and show banner
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None and banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Get hardware context for system-aware configuration if not provided
        if hardware_data is None:
            try:
                hardware_data = check_hardware(include_memory_usage=True)
            except Exception as e:
                logger.debug(f"Hardware detection failed: {e}")
                hardware_data = {}
        
        # Extract configuration context for display
        hpo_config = config.get('hyperparameter_optimization', {})
        data_config = config.get('data', {})
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        presets_section = config.get('presets', {})
        metadata = config.get('metadata', {})
        
        # Context extraction with preset compatibility
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        else:
            config_source = "Unknown"
        
        # Get available models for comparison
        if model_types:
            available_models = model_types
        else:
            available_models = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
        
        # Ensure we have at least 2 models for comparison
        if len(available_models) < 2:
            print(Fore.RED + Style.BRIGHT + f"\nInsufficient models for comparison. Available: {len(available_models)}")
            print(Fore.YELLOW + Style.BRIGHT + f"Available models: {list(MODEL_VARIANTS.keys())}")
            return None
        
        # Hardware-aware system class detection
        cuda_available = hardware_data.get('cuda', {}).get('available', False)
        memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)
        
        # Determine system performance class for resource optimization
        if cuda_available and memory_gb >= 16 and cpu_cores >= 8:
            system_class = "high_performance"
        elif cuda_available and memory_gb >= 8:
            system_class = "performance"
        elif memory_gb >= 4:
            system_class = "standard"
        else:
            system_class = "limited"
        
        # Header with context display
        print(Fore.MAGENTA + Style.BRIGHT + "MODEL COMPARISON HPO LAUNCHER")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        
        # Display current configuration context with preset information
        print(Fore.YELLOW + Style.BRIGHT + "Comparison Configuration Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name.title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Config Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        
        # Model comparison specific parameters
        print(Fore.CYAN + Style.BRIGHT + "\nModel Comparison Parameters:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Models to Compare: " + Fore.YELLOW + Style.BRIGHT + f"{len(available_models)}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Types: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(available_models)}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Comparison Mode: " + Fore.YELLOW + Style.BRIGHT + f"Comprehensive HPO")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Fair Comparison: " + Fore.YELLOW + Style.BRIGHT + f"Identical data splits & resources")
        
        # Display hardware context
        print(Fore.MAGENTA + Style.BRIGHT + "\nHardware Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
        if cuda_available:
            gpu_count = hardware_data.get('cuda', {}).get('gpu_count', 0)
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ GPU Count: " + Fore.YELLOW + Style.BRIGHT + f"{gpu_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Memory: " + Fore.YELLOW + Style.BRIGHT + f"{memory_gb:.1f}GB")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ CPU Cores: " + Fore.YELLOW + Style.BRIGHT + f"{cpu_cores}")
        
        # Display any additional parameter overrides
        extended_params = []
        if optimization_focus:
            extended_params.append(f"Optimization Focus: {optimization_focus}")
        if enable_storage is not None:
            extended_params.append(f"Storage: {enable_storage}")
        if enable_plots is not None:
            extended_params.append(f"Plots: {enable_plots}")
        if custom_search_space:
            extended_params.append("Custom search space parameters")
        if sampler_type:
            extended_params.append(f"Sampler: {sampler_type}")
        if pruner_type:
            extended_params.append(f"Pruner: {pruner_type}")
        if trial_count:
            extended_params.append(f"Trial Count Override: {trial_count}")
        if timeout_seconds:
            extended_params.append(f"Timeout Override: {timeout_seconds}s")
        if data_mode and data_mode != 'auto':
            extended_params.append(f"Data Mode: {data_mode}")
        
        if extended_params:
            print(Fore.CYAN + Style.BRIGHT + "\nParameter Overrides:")
            for i, param in enumerate(extended_params):
                prefix = "  └─" if i == len(extended_params) - 1 else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {param}")
        
        # Comparison description with context-aware information
        message = (
            f"HPO Model Comparison Overview\n"
            f"Purpose: Comprehensive optimization and comparison of model architectures\n\n"
            f"Comparison Configuration:\n"
            f"├─ Models Being Compared: {len(available_models)} architectures\n"
            f"├─ System Class: {system_class} (hardware-aware optimization)\n"
            f"├─ Fair Comparison: Identical data splits and resources\n"
            f"├─ Optimization: Independent HPO for each model\n"
            f"├─ Analysis: Comprehensive performance metrics\n"
            f"└─ Output: Detailed comparison report and visualizations\n\n"
            f"Models Being Compared:\n"
        )
        
        # Add model descriptions
        model_descriptions = {
            'SimpleAutoencoder': 'Basic architecture for baseline performance',
            'EnhancedAutoencoder': 'Advanced features for improved accuracy', 
            'AutoencoderEnsemble': 'Multiple models for robust performance',
            'VariationalAutoencoder': 'Probabilistic approach for uncertainty estimation',
            'ConvAutoencoder': 'Convolutional architecture for spatial data'
        }
        
        for i, model in enumerate(available_models, 1):
            desc = model_descriptions.get(model, 'Custom model configuration')
            message += f"{['├─','├─','└─'][min(i-1, len(available_models)-1)]} {model}: {desc}\n"
        
        message += (
            f"\nExpected Outcomes:\n"
            f"├─ Identification of best-performing model for your data\n"
            f"├─ Optimal hyperparameters for each architecture\n"
            f"├─ Performance trade-off analysis (accuracy vs speed vs complexity)\n"
            f"├─ Resource utilization comparison\n"
            f"└─ Model selection recommendations with confidence metrics"
        )
        
        print(Fore.YELLOW + Style.BRIGHT + f"\n{message}")
        print(Fore.MAGENTA + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        
        # Skip interactive configuration if non-interactive mode
        if non_interactive or skip_prompt:
            print(Fore.GREEN + Style.BRIGHT + "\nNon-interactive mode - using standard comparison configuration...")
            n_trials = trial_count if trial_count is not None else 50
            trial_epochs = 20
            timeout_minutes = (timeout_seconds // 60) if timeout_seconds else 120
            cv_folds = 5
            config_name = "Standard"
            config_description = "Non-interactive standard comparison"
            config_color = Fore.CYAN
        else:
            # Configuration options with descriptions
            print(Fore.YELLOW + Style.BRIGHT + "\nComparison Configuration Options:")
            print(Fore.WHITE + Style.BRIGHT + "1. Quick Comparison " + Fore.GREEN + Style.BRIGHT + "(10 trials/model, ~30-60 minutes)")
            print(Fore.WHITE + Style.BRIGHT + "2. Standard Comparison " + Fore.GREEN + Style.BRIGHT + "(50 trials/model, ~2-4 hours)")
            print(Fore.WHITE + Style.BRIGHT + "3. Thorough Comparison " + Fore.GREEN + Style.BRIGHT + "(100 trials/model, ~4-8 hours)")
            print(Fore.WHITE + Style.BRIGHT + "4. Custom Configuration " + Fore.GREEN + Style.BRIGHT + "(Full parameter control)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and Return to HPO Menu")
            
            # Input handling with retry logic
            choice = None
            while not choice:
                try:
                    choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect configuration (0-4): ").strip()
                    
                    # If empty input, retry
                    if not choice:
                        continue
                        
                    if choice not in ['0', '1', '2', '3', '4']:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-4.")
                        choice = None
                        continue
                        
                except (EOFError, KeyboardInterrupt):
                    print(Fore.YELLOW + Style.BRIGHT + "\nModel comparison cancelled by user")
                    return None
            
            if choice == "0":
                print(Fore.YELLOW + Style.BRIGHT + "\nModel comparison cancelled")
                return None
            
            # Set parameters based on choice with configurations
            trial_configs = {
                '1': {
                    'trials': 10, 
                    'epochs': 5, 
                    'timeout': 60, 
                    'name': 'Quick',
                    'description': 'Fast comparison for initial assessment',
                    'cv_folds': 3,
                    'color': Fore.GREEN
                },
                '2': {
                    'trials': 50, 
                    'epochs': 20, 
                    'timeout': 120, 
                    'name': 'Standard',
                    'description': 'Balanced comparison for reliable results',
                    'cv_folds': 5,
                    'color': Fore.CYAN
                }, 
                '3': {
                    'trials': 100, 
                    'epochs': 25, 
                    'timeout': 180, 
                    'name': 'Thorough',
                    'description': 'Comprehensive comparison for production use',
                    'cv_folds': 5,
                    'color': Fore.MAGENTA
                },
                '4': None  # Custom configuration
            }
            
            if choice in trial_configs and trial_configs[choice] is not None:
                trial_config = trial_configs[choice]
                n_trials = trial_count if trial_count is not None else trial_config['trials']
                trial_epochs = trial_config['epochs']
                timeout_minutes = (timeout_seconds // 60) if timeout_seconds else trial_config['timeout']
                config_name = trial_config['name']
                config_description = trial_config['description']
                cv_folds = trial_config['cv_folds']
                config_color = trial_config['color']
            elif choice == '4':
                # Custom configuration with validation
                print(Fore.WHITE + Style.BRIGHT + "\nCustom Comparison Configuration")
                print(Fore.YELLOW + Style.BRIGHT + "-"*40)
                print(Fore.WHITE + Style.BRIGHT + "Set parameters for your custom model comparison.")
                print(Fore.WHITE + Style.BRIGHT + "Press Enter to use default values shown in parentheses.")
                print(Fore.YELLOW + Style.BRIGHT + "-"*40)
                
                try:
                    n_trials_input = input(Fore.YELLOW + Style.BRIGHT + f"Trials per model (default 50): ").strip()
                    base_trials = int(n_trials_input) if n_trials_input else 50
                    n_trials = trial_count if trial_count is not None else base_trials
                    
                    trial_epochs_input = input(Fore.YELLOW + Style.BRIGHT + f"Epochs per trial (default 20): ").strip()
                    trial_epochs = int(trial_epochs_input) if trial_epochs_input else 20
                    
                    if timeout_seconds:
                        timeout_minutes = timeout_seconds // 60
                    else:
                        timeout_input = input(Fore.YELLOW + Style.BRIGHT + f"Timeout in minutes (default 120): ").strip()
                        timeout_minutes = int(timeout_input) if timeout_input else 120
                    
                    cv_folds_input = input(Fore.YELLOW + Style.BRIGHT + f"Cross-validation folds (default 5): ").strip()
                    cv_folds = int(cv_folds_input) if cv_folds_input else 5
                    
                    config_name = "Custom"
                    config_description = "User-defined comparison parameters"
                    config_color = Fore.BLUE
                    
                except ValueError as ve:
                    print(Fore.RED + Style.BRIGHT + f"\nInvalid input value: {str(ve)}")
                    print(Fore.YELLOW + Style.BRIGHT + "\nUsing standard configuration as fallback.")
                    n_trials = trial_count if trial_count is not None else 50
                    trial_epochs, timeout_minutes, cv_folds = 20, 120, 5
                    config_name = "Standard"
                    config_description = "Fallback configuration after input error"
                    config_color = Fore.YELLOW
            else:
                print(Fore.YELLOW + Style.BRIGHT + "\nInvalid choice selected. Using standard configuration.")
                n_trials = trial_count if trial_count is not None else 50
                trial_epochs, timeout_minutes, cv_folds = 20, 120, 5
                config_name = "Standard"
                config_description = "Fallback configuration"
                config_color = Fore.YELLOW
        
        # Calculate total resources and display configuration summary
        total_trials = n_trials * len(available_models)
        estimated_hours = (total_trials * trial_epochs * 0.5) / 60  # Rough estimate
        
        print(config_color + Style.BRIGHT + f"\n{config_name} MODEL COMPARISON CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(config_color + Style.BRIGHT + "Configuration Summary:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset Mode: " + Fore.YELLOW + Style.BRIGHT + f"{config_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Description: " + Fore.YELLOW + Style.BRIGHT + f"{config_description}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Models to Compare: " + Fore.YELLOW + Style.BRIGHT + f"{len(available_models)} architectures")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials per Model: " + Fore.YELLOW + Style.BRIGHT + f"{n_trials}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Optimization Trials: " + Fore.YELLOW + Style.BRIGHT + f"{total_trials}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs per Trial: " + Fore.YELLOW + Style.BRIGHT + f"{trial_epochs}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Cross-Validation Folds: " + Fore.YELLOW + Style.BRIGHT + f"{cv_folds}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Maximum Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{timeout_minutes} minutes")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Estimated Duration: " + Fore.YELLOW + Style.BRIGHT + f"{estimated_hours:.1f} hours")
        
        print(Fore.CYAN + Style.BRIGHT + "\nCurrent Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Active Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Configuration Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Skip confirmation if in non-interactive mode
        if non_interactive or skip_prompt:
            print(Fore.GREEN + Style.BRIGHT + "\nNon-interactive mode - starting model comparison automatically...")
        else:
            # Confirmation with resource requirements
            confirm = None
            while not confirm:
                try:
                    confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart model comparison with this configuration? (Y/n): ").strip().lower()
                    if not confirm:
                        continue
                except (EOFError, KeyboardInterrupt):
                    print(Fore.YELLOW + Style.BRIGHT + "\nModel comparison cancelled by user")
                    return None
            
            if confirm not in ('', 'y', 'yes'):
                print(Fore.YELLOW + Style.BRIGHT + "\nModel comparison cancelled")
                return None
        
        # Progress display
        print("\033c", end="")
        show_banner()
        
        print(Fore.GREEN + Style.BRIGHT + "HPO MODEL COMPARISON INITIATED")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + "Comparison Parameters:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Configuration: " + Fore.YELLOW + Style.BRIGHT + f"{config_name} Mode")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Models: " + Fore.YELLOW + Style.BRIGHT + f"{len(available_models)} architectures")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials per Model: " + Fore.YELLOW + Style.BRIGHT + f"{n_trials}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Trials: " + Fore.YELLOW + Style.BRIGHT + f"{total_trials}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs per Trial: " + Fore.YELLOW + Style.BRIGHT + f"{trial_epochs}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Cross-Validation: " + Fore.YELLOW + Style.BRIGHT + f"{cv_folds}-fold")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{timeout_minutes} minutes")
        
        print(Fore.YELLOW + Style.BRIGHT + f"\nExpected Completion: {estimated_hours:.1f} hours")
        
        print(Fore.CYAN + Style.BRIGHT + "\nProcess Overview:")
        for i, model in enumerate(available_models, 1):
            prefix = "  └─" if i == len(available_models) else "  ├─"
            print(Fore.WHITE + Style.BRIGHT + f"{prefix} Optimizing {model} architecture")
        print(Fore.WHITE + Style.BRIGHT + "  ├─ Generating comparative analysis")
        print(Fore.WHITE + Style.BRIGHT + "  └─ Creating visualization reports")
        
        print(Fore.MAGENTA + Style.BRIGHT + "\nMonitoring:")
        print(Fore.WHITE + Style.BRIGHT + "  ├─ Progress bars for each model optimization")
        print(Fore.WHITE + Style.BRIGHT + "  ├─ Real-time performance metrics")
        print(Fore.WHITE + Style.BRIGHT + "  ├─ Resource utilization tracking")
        print(Fore.WHITE + Style.BRIGHT + "  └─ Intermediate result saving")
        
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Create comparison configuration
        comparison_config = deepcopy(config) if config else {}
        
        # HPO configuration for model comparison
        hpo_config = comparison_config.setdefault('hyperparameter_optimization', {})
        hpo_config.update({
            'enabled': True,
            'n_trials': n_trials,
            'timeout': timeout_minutes * 60,
            'strategy': 'optuna',
            'trial_epochs': trial_epochs,
            'trial_patience': max(5, trial_epochs // 4),
            'model_search': {
                'enabled': True,
                'model_types': available_models,
                'search_all_models': True,
                'comparison_mode': True,
                'save_individual_studies': True
            },
            'cross_validation': {
                'enabled': True,
                'folds': cv_folds,
                'shuffle': True,
                'random_state': 42,
                'stratified': True
            },
            'early_stopping': {
                'enabled': True,
                'patience': max(10, n_trials // 10),
                'min_trials': max(5, n_trials // 5),
                'improvement_threshold': 0.001
            },
            'sampling': {
                'sampler': sampler_type or 'TPESampler',
                'n_startup_trials': max(5, n_trials // 10),
                'n_ei_candidates': 24
            },
            'pruning': {
                'pruner': pruner_type or 'HyperbandPruner',
                'min_resource': 1,
                'max_resource': trial_epochs,
                'reduction_factor': 3
            },
            'generate_plots': enable_plots if enable_plots is not None else True,
            'save_study': enable_storage if enable_storage is not None else True,
            'comparison_analysis': True,
            'study_name': f"model_comparison_{config_name.lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'model_comparison': True
        })
        
        # Apply custom search space if provided
        if custom_search_space:
            optimization_space = hpo_config.setdefault('optimization_space', {})
            optimization_space.update(custom_search_space)
        
        # Data configuration for fair comparison
        data_config = comparison_config.setdefault('data', {})
        data_config.update({
            'comparison_mode': True,
            'consistent_splits': True,
            'random_state': 42,
            'use_real_data': data_mode == 'real' if data_mode else data_config.get('use_real_data', False)
        })
        
        # System configuration for resource management
        system_config = comparison_config.setdefault('system', {})
        system_config.update({
            'model_comparison': True,
            'resource_management': {
                'max_concurrent_trials': 2,
                'memory_monitoring': True,
                'gpu_utilization_optimization': cuda_available
            },
            'hardware_context': hardware_data
        })
        
        # Add metadata for tracking and analysis
        metadata_config = comparison_config.setdefault('metadata', {})
        metadata_config.update({
            'comparison_type': 'hpo_model_comparison',
            'configuration_mode': config_name,
            'start_time': datetime.now().isoformat(),
            'models_compared': available_models,
            'total_trials': total_trials,
            'preset_used': preset_name,
            'system_class': system_class,
            'optimization_focus': optimization_focus or 'balanced'
        })
        
        # Add runtime information
        runtime_config = comparison_config.setdefault('runtime', {})
        runtime_config.update({
            'operation_mode': 'model_comparison',
            'config_source': f"model_comparison_{config_source}",
            'start_time': datetime.now().isoformat(),
            'hardware_data': hardware_data
        })
        
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.WHITE + Style.BRIGHT + "Comparison Configuration Applied")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Mode: " + Fore.YELLOW + Style.BRIGHT + f"{config_name} Comparison")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Models: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(available_models)}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials per Model: " + Fore.YELLOW + Style.BRIGHT + f"{n_trials}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Trials: " + Fore.YELLOW + Style.BRIGHT + f"{total_trials}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Cross-Validation: " + Fore.YELLOW + Style.BRIGHT + f"{cv_folds}-fold")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Study Name: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_config['study_name']}")
        
        print(Fore.GREEN + Style.BRIGHT + "\nStarting comprehensive model comparison...")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Small delay to ensure user sees the configuration
        time.sleep(3)
        kwargs['_recursion_depth'] = kwargs.get('_recursion_depth', 0) + 1
        # Run the model comparison using the main interactive function with all parameters
        result = run_hyperparameter_optimization_interactive(
            config=comparison_config,
            operation_mode='model_comparison',
            non_interactive=True,
            skip_prompt=True,
            model_types=available_models,
            trial_count=n_trials,
            timeout_seconds=timeout_minutes * 60,
            data_mode=data_mode,
            optimization_focus=optimization_focus,
            hardware_data=hardware_data,
            enable_storage=enable_storage,
            enable_plots=enable_plots,
            custom_search_space=custom_search_space,
            sampler_type=sampler_type,
            pruner_type=pruner_type,
            **kwargs
        )
        
        # Result handling with comparison-specific feedback
        if result:
            # Use the result handler if available
            if '_handle_hpo_result' in globals():
                _handle_hpo_result(result, f"{config_name} Model Comparison HPO")
            
            # Additional comparison-specific analysis if successful
            if result.get('success', False):
                # print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
                # print(Fore.GREEN + Style.BRIGHT + "MODEL COMPARISON COMPLETE")
                # print(Fore.CYAN + Style.BRIGHT + "-"*40)
                # print(Fore.GREEN + Style.BRIGHT + "Model Comparison Completed Successfully!")
                # print(Fore.WHITE + Style.BRIGHT + "\nComparison Summary:")
                # print(Fore.GREEN + Style.BRIGHT + f"  ├─ Configuration: " + Fore.YELLOW + Style.BRIGHT + f"{config_name} Mode")
                # print(Fore.GREEN + Style.BRIGHT + f"  ├─ Models Compared: " + Fore.YELLOW + Style.BRIGHT + f"{len(available_models)}")
                # print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Trials: " + Fore.YELLOW + Style.BRIGHT + f"{result.get('total_trials', 'N/A')}")
                # print(Fore.GREEN + Style.BRIGHT + f"  ├─ Best Performing Model: " + Fore.YELLOW + Style.BRIGHT + f"{result.get('best_model', 'N/A')}")
                # #print(Fore.GREEN + Style.BRIGHT + f"  ├─ Best Score: " + Fore.YELLOW + Style.BRIGHT + f"{result.get('best_score', 'N/A'):.4f}")
                # print(Fore.GREEN + Style.BRIGHT + f"  ├─ Best Score: " + Fore.YELLOW + Style.BRIGHT + f"{result.get('best_score', 'N/A')}")
                # print(Fore.GREEN + Style.BRIGHT + f"  └─ Duration: " + Fore.YELLOW + Style.BRIGHT + f"{result.get('duration', 'N/A')}")
                # print(Fore.GREEN + Style.BRIGHT + "\nDetailed comparison report and visualizations have been generated.")
                
                # print(Fore.CYAN + Style.BRIGHT + "-"*40)
                
                # Display model comparison summary if available
                if 'model_comparison' in result:
                    _display_model_comparison_summary(result, models=available_models)
                    
                return result
            else:
                # print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
                # print(Fore.YELLOW + Style.BRIGHT + "Model Comparison Completed with Notes")
                # print(Fore.YELLOW + Style.BRIGHT + "-"*40)
                # print(Fore.WHITE + Style.BRIGHT + "The comparison completed but encountered some issues.")
                # print(Fore.WHITE + Style.BRIGHT + "Check the detailed results above for more information.")
                # print(Fore.YELLOW + Style.BRIGHT + "\nPartial results may still be available for analysis.")
                # print(Fore.YELLOW + Style.BRIGHT + "-"*40)
                return result
        else:
            # print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
            # print(Fore.RED + Style.BRIGHT + "Model Comparison Failed to Return Results")
            # print(Fore.RED + Style.BRIGHT + "-"*40)
            # print(Fore.WHITE + Style.BRIGHT + "The comparison may have encountered an unexpected issue.")
            # print(Fore.WHITE + Style.BRIGHT + "Check the logs for detailed error information.")
            # print(Fore.RED + Style.BRIGHT + "-"*40)
            return None
    
    except KeyboardInterrupt:
        print(Fore.YELLOW + Style.BRIGHT + "\nModel comparison interrupted by user")
        return None
    except Exception as e:
        logger.error(f"HPO model comparison error: {e}", exc_info=True)
        
        # error context
        error_context = {
            "Preset": preset_name if 'preset_name' in locals() else 'Unknown',
            "Current Model": model_type if 'model_type' in locals() else 'Unknown',
            "Operation Mode": operation_mode if operation_mode else 'Not specified',
            "System Class": system_class if 'system_class' in locals() else 'Unknown',
            "Models to Compare": len(available_models) if 'available_models' in locals() else 0,
            "Hardware Available": bool(hardware_data) if 'hardware_data' in locals() else False,
            "CUDA Available": cuda_available if 'cuda_available' in locals() else False,
            "Model Variants Available": len(MODEL_VARIANTS)
        }
        
        message = (
            f"Model comparison failed: {str(e)}\n\n"
            f"Context:\n" +
            "\n".join([f"├─ {key}: {value}" for key, value in list(error_context.items())[:-1]]) +
            f"\n└─ {list(error_context.items())[-1][0]}: {list(error_context.items())[-1][1]}" +
            f"\n\nThis could be due to:\n"
            f"├─ System resource exhaustion\n"
            f"├─ Configuration conflicts between models\n"
            f"├─ Missing model dependencies\n"
            f"├─ Data compatibility issues\n"
            f"├─ Optimization algorithm failures\n"
            f"└─ Model variant initialization problems"
        )
        
        print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
        print(Fore.RED + Style.BRIGHT + "MODEL COMPARISON ERROR")
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + message)
        print(Fore.RED + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        
        return None

def _display_model_comparison_summary(result: Dict[str, Any], models: List[str]) -> None:
    """
    Display comprehensive summary of model comparison results with context.
    
    Args:
        result: HPO results dictionary containing comparison data
        models: List of models that were compared
    """
    try:
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "DETAILED MODEL COMPARISON SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Extract metadata and context from result
        metadata = result.get('metadata', {})
        runtime_info = result.get('runtime', {})
        comparison_config = result.get('comparison_config', {})
        
        # Display comparison context
        print(Fore.YELLOW + Style.BRIGHT + "Comparison Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Configuration Mode: " + Fore.YELLOW + Style.BRIGHT + f"{metadata.get('configuration_mode', 'Standard')}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Optimization Focus: " + Fore.YELLOW + Style.BRIGHT + f"{metadata.get('optimization_focus', 'Balanced')}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{metadata.get('system_class', 'Unknown')}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset Used: " + Fore.YELLOW + Style.BRIGHT + f"{metadata.get('preset_used', 'Default')}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Total Models: " + Fore.YELLOW + Style.BRIGHT + f"{len(models)}")
        
        # Extract study and trials data
        study = result.get('study')
        best_trial = result.get('best_trial')
        best_model = result.get('best_model', 'Unknown')
        best_score = result.get('best_score', 0.0)
        total_trials = result.get('total_trials', 0)
        
        if not study or not hasattr(study, 'trials'):
            print(Fore.YELLOW + Style.BRIGHT + "\nNo detailed comparison data available in study.")
            
            # Fallback to basic result information
            if best_model and best_score:
                print(Fore.GREEN + Style.BRIGHT + f"\nBasic Results:")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Best Model: " + Fore.YELLOW + Style.BRIGHT + f"{best_model}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Best Score: " + Fore.YELLOW + Style.BRIGHT + f"{best_score:.6f}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Models Compared: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(models)}")
                print(Fore.GREEN + Style.BRIGHT + f"  └─ Total Trials: " + Fore.YELLOW + Style.BRIGHT + f"{total_trials}")
            return
        
        # Analyze trials by model type
        model_results = {}
        completed_trials = [t for t in study.trials if t.state.name == 'COMPLETE']
        pruned_trials = [t for t in study.trials if t.state.name == 'PRUNED']
        failed_trials = [t for t in study.trials if t.state.name == 'FAIL']
        
        print(Fore.WHITE + Style.BRIGHT + f"\nTrial Statistics:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Completed Trials: " + Fore.YELLOW + Style.BRIGHT + f"{len(completed_trials)}")
        print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Pruned Trials: " + Fore.YELLOW + Style.BRIGHT + f"{len(pruned_trials)}")
        print(Fore.RED + Style.BRIGHT + f"  ├─ Failed Trials: " + Fore.YELLOW + Style.BRIGHT + f"{len(failed_trials)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Total Trials: " + Fore.YELLOW + Style.BRIGHT + f"{len(study.trials)}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Models Compared: " + Fore.YELLOW + Style.BRIGHT + f"{len(models)}")
        
        # Collect model-specific results
        for trial in completed_trials:
            model_type = trial.params.get('model_type', 'Unknown')
            if model_type not in model_results:
                model_results[model_type] = {
                    'trials': [],
                    'best_value': float('inf'),
                    'worst_value': float('-inf'),
                    'values': [],
                    'params': [],
                    'durations': [],
                    'pruned_count': 0,
                    'failed_count': 0
                }
            
            if trial.value is not None:
                model_results[model_type]['trials'].append(trial)
                model_results[model_type]['values'].append(trial.value)
                model_results[model_type]['params'].append(trial.params)
                
                # Track durations if available
                if hasattr(trial, 'duration') and trial.duration:
                    model_results[model_type]['durations'].append(trial.duration.total_seconds())
                
                if trial.value < model_results[model_type]['best_value']:
                    model_results[model_type]['best_value'] = trial.value
                    model_results[model_type]['best_params'] = trial.params
                if trial.value > model_results[model_type]['worst_value']:
                    model_results[model_type]['worst_value'] = trial.value
        
        # Count pruned and failed trials per model
        for trial in pruned_trials:
            model_type = trial.params.get('model_type', 'Unknown')
            if model_type in model_results:
                model_results[model_type]['pruned_count'] += 1
        
        for trial in failed_trials:
            model_type = trial.params.get('model_type', 'Unknown')
            if model_type in model_results:
                model_results[model_type]['failed_count'] += 1
        
        # Calculate statistics for each model
        for model_type, data in model_results.items():
            if data['values']:
                data['avg_value'] = sum(data['values']) / len(data['values'])
                data['std_value'] = np.std(data['values']) if len(data['values']) > 1 else 0.0
                data['trials_count'] = len(data['values'])
                data['success_rate'] = (len(data['values']) / (len(data['values']) + data['pruned_count'] + data['failed_count'])) * 100
                
                # Calculate average duration
                if data['durations']:
                    data['avg_duration'] = sum(data['durations']) / len(data['durations'])
                else:
                    data['avg_duration'] = 0
        
        # Sort models by performance (best average)
        sorted_models = sorted(
            [(model, data) for model, data in model_results.items() if data['values']],
            key=lambda x: x[1]['avg_value']
        )
        
        # Display performance ranking
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "MODEL PERFORMANCE RANKING")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        for i, (model_type, data) in enumerate(sorted_models, 1):
            trials_count = data['trials_count']
            best_val = data['best_value']
            avg_val = data['avg_value']
            std_val = data['std_value']
            success_rate = data['success_rate']
            avg_duration = data['avg_duration']
            
            is_last_model = i == len(sorted_models)
            prefix = "  └─" if is_last_model else "  ├─"
            
            print(Fore.WHITE + Style.BRIGHT + f"\n{prefix} {i}. {model_type}")
            
            # Performance metrics
            print(Fore.CYAN + Style.BRIGHT + f"  {'  ' if is_last_model else '  '}├─ Completed Trials: " + Fore.YELLOW + Style.BRIGHT + f"{trials_count}")
            print(Fore.GREEN + Style.BRIGHT + f"  {'  ' if is_last_model else '  '}├─ Best Value: " + Fore.YELLOW + Style.BRIGHT + f"{best_val:.6f}")
            print(Fore.YELLOW + Style.BRIGHT + f"  {'  ' if is_last_model else '  '}├─ Average Value: " + Fore.YELLOW + Style.BRIGHT + f"{avg_val:.6f}")
            
            if std_val > 0:
                consistency_color = Fore.GREEN if std_val < 0.01 else Fore.YELLOW if std_val < 0.05 else Fore.RED
                print(Fore.BLUE + Style.BRIGHT + f"  {'  ' if is_last_model else '  '}├─ Std Deviation: " + consistency_color + Style.BRIGHT + f"±{std_val:.6f}")
            
            # Success rate
            success_color = Fore.GREEN if success_rate > 90 else Fore.YELLOW if success_rate > 75 else Fore.RED
            print(Fore.CYAN + Style.BRIGHT + f"  {'  ' if is_last_model else '  '}├─ Success Rate: " + success_color + Style.BRIGHT + f"{success_rate:.1f}%")
            
            # Average duration if available
            if avg_duration > 0:
                duration_color = Fore.GREEN if avg_duration < 30 else Fore.YELLOW if avg_duration < 60 else Fore.RED
                print(Fore.CYAN + Style.BRIGHT + f"  {'  ' if is_last_model else '  '}├─ Avg Trial Time: " + duration_color + Style.BRIGHT + f"{avg_duration:.1f}s")
            
            # Performance assessment
            performance_prefix = "  └─" if is_last_model else "  ├─"
            if i == 1:
                print(Fore.GREEN + Style.BRIGHT + f"  {'  ' if is_last_model else '  '}└─ Status: " + Fore.GREEN + Style.BRIGHT + "BEST PERFORMER" + Style.RESET_ALL)
            elif i == len(sorted_models):
                print(Fore.RED + Style.BRIGHT + f"  {'  ' if is_last_model else '  '}└─ Status: " + Fore.RED + Style.BRIGHT + "Needs Improvement" + Style.RESET_ALL)
            else:
                print(Fore.YELLOW + Style.BRIGHT + f"  {'  ' if is_last_model else '  '}└─ Status: " + Fore.YELLOW + Style.BRIGHT + "Good Performance" + Style.RESET_ALL)
        
        # Overall recommendations and insights
        if sorted_models:
            best_model_type = sorted_models[0][0]
            best_avg_value = sorted_models[0][1]['avg_value']
            best_single_value = sorted_models[0][1]['best_value']
            best_std_value = sorted_models[0][1]['std_value']
            best_success_rate = sorted_models[0][1]['success_rate']
            
            print(Fore.MAGENTA + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.GREEN + Style.BRIGHT + "FINAL RECOMMENDATION")
            print(Fore.MAGENTA + Style.BRIGHT + "-"*40)
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Recommended Model: " + Fore.YELLOW + Style.BRIGHT + f"{best_model_type}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Best Objective Value: " + Fore.YELLOW + Style.BRIGHT + f"{best_single_value:.6f}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Average Performance: " + Fore.YELLOW + Style.BRIGHT + f"{best_avg_value:.6f}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Success Rate: " + Fore.YELLOW + Style.BRIGHT + f"{best_success_rate:.1f}%")
            
            # Performance gap analysis
            if len(sorted_models) > 1:
                second_best_avg = sorted_models[1][1]['avg_value']
                performance_gap = ((second_best_avg - best_avg_value) / second_best_avg * 100)
                
                print(Fore.CYAN + Style.BRIGHT + f"\n  Performance Advantage:")
                print(Fore.WHITE + Style.BRIGHT + f"    ├─ {best_model_type} is {performance_gap:.1f}% better than {sorted_models[1][0]}")
                
                if performance_gap > 10:
                    print(Fore.GREEN + Style.BRIGHT + f"    ├─ Significant performance advantage")
                    print(Fore.GREEN + Style.BRIGHT + f"    └─ Strong recommendation for production use")
                elif performance_gap > 5:
                    print(Fore.YELLOW + Style.BRIGHT + f"    ├─ Moderate performance advantage") 
                    print(Fore.YELLOW + Style.BRIGHT + f"    └─ Consider additional validation")
                else:
                    print(Fore.BLUE + Style.BRIGHT + f"    ├─ Marginal performance advantage")
                    print(Fore.BLUE + Style.BRIGHT + f"    └─ Evaluate based on other criteria")
            
            # Consistency analysis
            print(Fore.CYAN + Style.BRIGHT + f"\n  Consistency Analysis:")
            if best_std_value < 0.01:
                print(Fore.GREEN + Style.BRIGHT + f"    ├─ Excellent consistency across trials")
                print(Fore.GREEN + Style.BRIGHT + f"    └─ Highly reliable performance")
            elif best_std_value < 0.05:
                print(Fore.YELLOW + Style.BRIGHT + f"    ├─ Good consistency across trials")
                print(Fore.YELLOW + Style.BRIGHT + f"    └─ Reliable performance")
            else:
                print(Fore.RED + Style.BRIGHT + f"    ├─ Variable performance across trials")
                print(Fore.RED + Style.BRIGHT + f"    └─ Consider additional validation")
            
            # Success rate analysis
            print(Fore.CYAN + Style.BRIGHT + f"\n  Optimization Stability:")
            if best_success_rate > 90:
                print(Fore.GREEN + Style.BRIGHT + f"    ├─ Excellent optimization stability")
                print(Fore.GREEN + Style.BRIGHT + f"    └─ Robust hyperparameter search")
            elif best_success_rate > 75:
                print(Fore.YELLOW + Style.BRIGHT + f"    ├─ Good optimization stability")
                print(Fore.YELLOW + Style.BRIGHT + f"    └─ Reliable search process")
            else:
                print(Fore.RED + Style.BRIGHT + f"    ├─ Optimization stability needs improvement")
                print(Fore.RED + Style.BRIGHT + f"    └─ Consider adjusting search space")
        
        # Display key hyperparameters for best model
        best_params = None
        if best_trial and 'params' in best_trial:
            best_params = best_trial['params']
        elif sorted_models and 'best_params' in sorted_models[0][1]:
            best_params = sorted_models[0][1]['best_params']
        
        if best_params:
            print(Fore.MAGENTA + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.CYAN + Style.BRIGHT + "OPTIMAL HYPERPARAMETERS")
            print(Fore.MAGENTA + Style.BRIGHT + "-"*40)
            
            # Categorize parameters
            architecture_params = ['hidden_size', 'num_layers', 'encoding_dim', 'latent_dim']
            training_params = ['learning_rate', 'batch_size', 'weight_decay', 'dropout_rate']
            optimization_params = ['patience', 'min_delta', 'factor', 'cooldown']
            
            displayed_count = 0
            
            # Display architecture parameters
            arch_found = [p for p in architecture_params if p in best_params]
            if arch_found:
                print(Fore.YELLOW + Style.BRIGHT + "  Architecture Parameters:")
                for i, param in enumerate(arch_found, 1):
                    value = best_params[param]
                    prefix = "    └─" if i == len(arch_found) else "    ├─"
                    print(Fore.WHITE + Style.BRIGHT + f"{prefix} {param}: " + Fore.YELLOW + Style.BRIGHT + f"{value}")
                    displayed_count += 1
            
            # Display training parameters
            train_found = [p for p in training_params if p in best_params]
            if train_found:
                print(Fore.YELLOW + Style.BRIGHT + "  Training Parameters:")
                for i, param in enumerate(train_found, 1):
                    value = best_params[param]
                    prefix = "    └─" if i == len(train_found) else "    ├─"
                    print(Fore.WHITE + Style.BRIGHT + f"{prefix} {param}: " + Fore.YELLOW + Style.BRIGHT + f"{value}")
                    displayed_count += 1
            
            # Display optimization parameters
            opt_found = [p for p in optimization_params if p in best_params]
            if opt_found:
                print(Fore.YELLOW + Style.BRIGHT + "  Optimization Parameters:")
                for i, param in enumerate(opt_found, 1):
                    value = best_params[param]
                    prefix = "    └─" if i == len(opt_found) else "    ├─"
                    print(Fore.WHITE + Style.BRIGHT + f"{prefix} {param}: " + Fore.YELLOW + Style.BRIGHT + f"{value}")
                    displayed_count += 1
            
            if displayed_count == 0:
                print(Fore.YELLOW + Style.BRIGHT + "  └─ No key hyperparameters available in results")
        
        # Resource utilization summary
        if 'duration' in result or any('avg_duration' in data for _, data in model_results.items()):
            print(Fore.MAGENTA + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.CYAN + Style.BRIGHT + "RESOURCE UTILIZATION SUMMARY")
            print(Fore.MAGENTA + Style.BRIGHT + "-"*40)
            
            print(Fore.WHITE + Style.BRIGHT + "  Resource Metrics:")
            if 'duration' in result:
                print(Fore.WHITE + Style.BRIGHT + f"    ├─ Total Duration: " + Fore.YELLOW + Style.BRIGHT + f"{result['duration']}")
            
            print(Fore.WHITE + Style.BRIGHT + f"    ├─ Total Trials: " + Fore.YELLOW + Style.BRIGHT + f"{len(completed_trials)}")
            
            # Calculate efficiency metrics
            if len(completed_trials) > 0 and 'duration' in result:
                try:
                    duration_seconds = result.get('duration_seconds', 0)
                    if duration_seconds > 0:
                        time_per_trial = duration_seconds / len(completed_trials)
                        efficiency_color = Fore.GREEN if time_per_trial < 30 else Fore.YELLOW if time_per_trial < 60 else Fore.RED
                        print(Fore.WHITE + Style.BRIGHT + f"    ├─ Average Time per Trial: " + efficiency_color + Style.BRIGHT + f"{time_per_trial:.1f}s")
                        
                        # Trials per hour
                        trials_per_hour = (len(completed_trials) / duration_seconds) * 3600
                        print(Fore.WHITE + Style.BRIGHT + f"    └─ Trial Rate: " + Fore.YELLOW + Style.BRIGHT + f"{trials_per_hour:.1f} trials/hour")
                except:
                    print(Fore.WHITE + Style.BRIGHT + f"    └─ Efficiency metrics: " + Fore.YELLOW + Style.BRIGHT + f"Not available")
            
            # Model-specific resource usage
            if len(sorted_models) > 1:
                print(Fore.YELLOW + Style.BRIGHT + "\n  Model Efficiency Ranking:")
                efficiency_ranking = sorted(
                    [(model, data) for model, data in model_results.items() if data.get('avg_duration', 0) > 0],
                    key=lambda x: x[1]['avg_duration']
                )
                
                for i, (model, data) in enumerate(efficiency_ranking[:3], 1):  # Top 3 most efficient
                    duration = data['avg_duration']
                    prefix = "    └─" if i == len(efficiency_ranking[:3]) else "    ├─"
                    efficiency_status = "Most Efficient" if i == 1 else "Efficient" if i == 2 else "Average Efficiency"
                    efficiency_color = Fore.GREEN if i == 1 else Fore.YELLOW if i == 2 else Fore.BLUE
                    print(Fore.WHITE + Style.BRIGHT + f"{prefix} " + efficiency_color + Style.BRIGHT + f"{efficiency_status}: " + Fore.YELLOW + Style.BRIGHT + f"{model} ({duration:.1f}s per trial)")
        
        # Additional insights and recommendations
        print(Fore.MAGENTA + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "ADDITIONAL INSIGHTS")
        print(Fore.MAGENTA + Style.BRIGHT + "-"*40)
        
        if sorted_models:
            best_model_data = sorted_models[0][1]
            
            print(Fore.WHITE + Style.BRIGHT + "  Performance Confidence:")
            # Performance confidence
            if best_model_data['std_value'] < 0.01 and best_model_data['success_rate'] > 90:
                print(Fore.GREEN + Style.BRIGHT + f"    ├─ High confidence in recommended model")
                print(Fore.GREEN + Style.BRIGHT + f"    └─ Production-ready with minimal risk")
            elif best_model_data['std_value'] < 0.05 and best_model_data['success_rate'] > 75:
                print(Fore.YELLOW + Style.BRIGHT + f"    ├─ Moderate confidence in recommended model")
                print(Fore.YELLOW + Style.BRIGHT + f"    └─ Suitable for most applications")
            else:
                print(Fore.RED + Style.BRIGHT + f"    ├─ Limited confidence in recommended model")
                print(Fore.RED + Style.BRIGHT + f"    └─ Additional validation recommended")
            
            # Resource recommendations
            total_pruned = len(pruned_trials)
            pruning_rate = (total_pruned / len(study.trials)) * 100 if study.trials else 0
            print(Fore.WHITE + Style.BRIGHT + f"\n  Search Efficiency:")
            if pruning_rate > 50:
                print(Fore.YELLOW + Style.BRIGHT + f"    ├─ High pruning rate ({pruning_rate:.1f}%)")
                print(Fore.YELLOW + Style.BRIGHT + f"    └─ Consider adjusting search space bounds")
            elif pruning_rate < 10:
                print(Fore.GREEN + Style.BRIGHT + f"    ├─ Optimal pruning rate ({pruning_rate:.1f}%)")
                print(Fore.GREEN + Style.BRIGHT + f"    └─ Efficient search space configuration")
            else:
                print(Fore.BLUE + Style.BRIGHT + f"    ├─ Moderate pruning rate ({pruning_rate:.1f}%)")
                print(Fore.BLUE + Style.BRIGHT + f"    └─ Balanced search efficiency")
            
            # Next steps
            print(Fore.CYAN + Style.BRIGHT + f"\n  Recommended Next Steps:")
            print(Fore.WHITE + Style.BRIGHT + f"    ├─ Validate best model on separate test set")
            print(Fore.WHITE + Style.BRIGHT + f"    ├─ Consider ensemble with top 2-3 performers")
            print(Fore.WHITE + Style.BRIGHT + f"    ├─ Fine-tune best model with narrowed search space")
            if len(models) > 3:
                print(Fore.WHITE + Style.BRIGHT + f"    ├─ Remove consistently poor performers from future comparisons")
                print(Fore.WHITE + Style.BRIGHT + f"    └─ Document comparison methodology for reproducibility")
            else:
                print(Fore.WHITE + Style.BRIGHT + f"    └─ Document comparison methodology for reproducibility")
        
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + "  ├─ Comparison summary complete.")
        print(Fore.GREEN + Style.BRIGHT + "  └─ Check generated reports for detailed analysis and visualizations.")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
    except Exception as e:
        logger.error(f"Error displaying comparison summary: {e}", exc_info=True)
        print(Fore.RED + Style.BRIGHT + f"\n  ├─ Could not display detailed comparison summary: {str(e)}")
        print(Fore.YELLOW + Style.BRIGHT + f"  └─ Basic results are still available above.")

def _handle_hpo_result(result: Optional[Dict[str, Any]], hpo_type: str) -> None:
    """
    Handle and display HPO results with formatting and context awareness.
    
    Args:
        result: HPO results dictionary or None if cancelled/failed
        hpo_type: Type of HPO operation for context-aware display
    """
    try:
        # Handle cancellation case with context-aware messaging
        if result is None:
            print(Fore.RED + Style.BRIGHT + "OPERATION CANCELLED")
            
            # Context-aware cancellation messages
            if 'quick' in hpo_type.lower():
                print(Fore.YELLOW + Style.BRIGHT + "Quick HPO test was cancelled by user.")
                print(Fore.WHITE + Style.BRIGHT + "No optimization was performed.")
            elif 'comparison' in hpo_type.lower():
                print(Fore.YELLOW + Style.BRIGHT + "Model comparison was cancelled by user.")
                print(Fore.WHITE + Style.BRIGHT + "No models were optimized or compared.")
            elif 'continue' in hpo_type.lower():
                print(Fore.YELLOW + Style.BRIGHT + "Study continuation was cancelled by user.")
                print(Fore.WHITE + Style.BRIGHT + "Existing study remains unchanged.")
            else:
                print(Fore.YELLOW + Style.BRIGHT + "Hyperparameter optimization was cancelled by user.")
                print(Fore.WHITE + Style.BRIGHT + "No trials were executed.")
            
            return
        
        # Extract success status
        success = result.get('success', False)
        
        # Display operation context header
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + f"HPO OPERATION: {hpo_type.upper()}")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Extract operation context for quick summary
        metadata = result.get('metadata', {})
        runtime_info = result.get('runtime', {})
        operation_mode = runtime_info.get('operation_mode', metadata.get('setup_method', 'standard'))
        preset_used = metadata.get('preset_used', metadata.get('hpo_preset_used', 'custom'))
        system_class = metadata.get('system_class_used', 'unknown')
        
        # Quick context summary
        print(Fore.YELLOW + Style.BRIGHT + "Operation Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Type: " + Fore.WHITE + Style.BRIGHT + f"{hpo_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Mode: " + Fore.WHITE + Style.BRIGHT + f"{operation_mode.replace('_', ' ').title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.WHITE + Style.BRIGHT + f"{preset_used.title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ System Class: " + Fore.WHITE + Style.BRIGHT + f"{system_class.upper()}")
        
        # Quick status summary
        if success:
            n_completed = result.get('n_trials_completed', result.get('total_trials', 0))
            best_value = result.get('best_value', result.get('best_score', 'N/A'))
            
            print(Fore.GREEN + Style.BRIGHT + "\nQuick Results:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Status: " + Fore.GREEN + Style.BRIGHT + f"SUCCESS")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Trials Completed: " + Fore.GREEN + Style.BRIGHT + f"{n_completed}")
            
            if isinstance(best_value, (int, float)) and best_value != float('inf'):
                if abs(best_value) < 0.001:
                    best_value_str = f"{best_value:.2e}"
                else:
                    best_value_str = f"{best_value:.6f}"
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Best Value: " + Fore.GREEN + Style.BRIGHT + f"{best_value_str}")
            
            # Context-specific quick highlights
            if 'comparison' in hpo_type.lower():
                best_model = result.get('best_model', result.get('best_model_type', 'N/A'))
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Best Model: " + Fore.GREEN + Style.BRIGHT + f"{best_model}")
            else:
                total_time = result.get('total_time_minutes', result.get('duration_minutes', 0))
                if isinstance(total_time, (int, float)) and total_time > 0:
                    if total_time < 1:
                        time_str = f"{total_time*60:.1f} seconds"
                    elif total_time < 60:
                        time_str = f"{total_time:.1f} minutes"
                    else:
                        hours = total_time / 60
                        time_str = f"{hours:.1f} hours"
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Duration: " + Fore.GREEN + Style.BRIGHT + f"{time_str}")
        else:
            error = result.get('error', 'Unknown error occurred during optimization')
            error_type = result.get('error_type', 'OptimizationError')
            n_completed = result.get('n_trials_completed', result.get('completed_trials', 0))
            
            print(Fore.RED + Style.BRIGHT + "\nQuick Results:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Status: " + Fore.RED + Style.BRIGHT + f"FAILED")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Error Type: " + Fore.RED + Style.BRIGHT + f"{error_type}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Trials Completed: " + Fore.YELLOW + Style.BRIGHT + f"{n_completed}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Error: " + Fore.RED + Style.BRIGHT + f"{error}")
        
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        
        # Delegate comprehensive display to _display_hpo_results
        #_display_hpo_results(result)
        
        # Add context-specific final messages
        if success:
            print(Fore.GREEN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.GREEN + Style.BRIGHT + f"HPO {hpo_type.upper()} COMPLETED SUCCESSFULLY")
            print(Fore.GREEN + Style.BRIGHT + "-"*40)
            
            # Context-specific completion messages
            if 'quick' in hpo_type.lower():
                print(Fore.CYAN + Style.BRIGHT + "✓ Quick test validated system compatibility")
                print(Fore.CYAN + Style.BRIGHT + "✓ Configuration ready for full optimization")
            elif 'comparison' in hpo_type.lower():
                print(Fore.CYAN + Style.BRIGHT + "✓ Model comparison completed successfully")
                print(Fore.CYAN + Style.BRIGHT + "✓ Best performing model identified")
            elif 'continue' in hpo_type.lower():
                print(Fore.CYAN + Style.BRIGHT + "✓ Study continuation completed successfully")
                print(Fore.CYAN + Style.BRIGHT + "✓ Additional optimization progress achieved")
            else:
                print(Fore.CYAN + Style.BRIGHT + "✓ Hyperparameter optimization completed")
                print(Fore.CYAN + Style.BRIGHT + "✓ Optimal configuration found")
            
            print(Fore.GREEN + Style.BRIGHT + "-"*40)
        else:
            print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.RED + Style.BRIGHT + f"HPO {hpo_type.upper()} ENCOUNTERED ISSUES")
            print(Fore.RED + Style.BRIGHT + "-"*40)
            
            # Context-specific failure guidance
            if 'quick' in hpo_type.lower():
                print(Fore.YELLOW + Style.BRIGHT + "⚠ Quick test revealed system configuration issues")
                print(Fore.YELLOW + Style.BRIGHT + "⚠ Review system setup before proceeding")
            elif 'comparison' in hpo_type.lower():
                print(Fore.YELLOW + Style.BRIGHT + "⚠ Model comparison could not be completed")
                print(Fore.YELLOW + Style.BRIGHT + "⚠ Check individual model configurations")
            else:
                print(Fore.YELLOW + Style.BRIGHT + "⚠ Optimization did not complete successfully")
                print(Fore.YELLOW + Style.BRIGHT + "⚠ Review recommendations for recovery steps")
            
            print(Fore.RED + Style.BRIGHT + "-"*40)
    
    except Exception as e:
        # Error handling for result processing itself
        logger.error(f"Error handling HPO result: {e}", exc_info=True)
        
        print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.RED + Style.BRIGHT + "RESULT PROCESSING ERROR")
        print(Fore.RED + Style.BRIGHT + "-"*40)
        print(Fore.WHITE + Style.BRIGHT + f"Error processing HPO results for: {hpo_type}")
        print(Fore.RED + Style.BRIGHT + f"Error: {str(e)}")
        
        # Try to display basic result information as fallback
        if result and isinstance(result, dict):
            try:
                print(Fore.YELLOW + Style.BRIGHT + "\nBASIC RESULT INFORMATION:")
                
                # Status
                if 'success' in result:
                    status = "SUCCESS" if result['success'] else "ISSUES ENCOUNTERED"
                    status_color = Fore.GREEN if result['success'] else Fore.RED
                    print(status_color + Style.BRIGHT + f"  ├─ Status: {status}")
                
                # Trials
                trials_keys = ['n_trials_completed', 'total_trials', 'completed_trials']
                for key in trials_keys:
                    if key in result:
                        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Trials Completed: {result[key]}")
                        break
                
                # Best value
                value_keys = ['best_value', 'best_score', 'objective_value']
                for key in value_keys:
                    if key in result and isinstance(result[key], (int, float)):
                        if abs(result[key]) < 0.001:
                            value_str = f"{result[key]:.2e}"
                        else:
                            value_str = f"{result[key]:.6f}"
                        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Best Value: {value_str}")
                        break
                
                # Study name
                if 'study_name' in result:
                    print(Fore.WHITE + Style.BRIGHT + f"  └─ Study: {result['study_name']}")
                
            except Exception as fallback_error:
                logger.debug(f"Fallback result display also failed: {fallback_error}")
                print(Fore.RED + Style.BRIGHT + "\nCould not extract any result information")
        
        print(Fore.RED + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + "The optimization likely completed successfully despite display issues.")
        print(Fore.YELLOW + Style.BRIGHT + "Check logs and output directories for detailed results.")
        print(Fore.RED + Style.BRIGHT + "-"*40)

def _interactive_hpo_express_setup(
    base_config: Dict[str, Any],
    data_mode: Optional[str] = None,
    hardware_data: Optional[Dict[str, Any]] = None,
    enable_storage: Optional[bool] = None,
    enable_plots: Optional[bool] = None,
    custom_search_space: Optional[Dict[str, Any]] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    trial_count: Optional[int] = None,
    timeout_seconds: Optional[int] = None,
    optimization_focus: Optional[str] = None,
    study_name: Optional[str] = None,
    storage_url: Optional[str] = None,
    non_interactive: bool = False,
    skip_prompt: bool = False,
    operation_mode: Optional[str] = None,
    use_current_config: bool = False,
    force_express: bool = False,
    model_types: Optional[List[str]] = None,
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Interactive express HPO setup for quick hyperparameter optimization configuration.
    
    Provides a streamlined HPO setup process with smart defaults while maintaining
    full compatibility with the centralized configuration system and preset configurations.
    """
    try:
        # Clear screen and show banner
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner for context if available
        if base_config is None and banner_config is not None:
            base_config = banner_config
        
        # Get hardware context for system-aware configuration if not provided
        if hardware_data is None:
            try:
                hardware_data = check_hardware(include_memory_usage=True)
            except Exception as e:
                logger.debug(f"Hardware detection failed: {e}")
                hardware_data = {}
        
        # Extract configuration context
        hpo_config = base_config.get('hyperparameter_optimization', {}) if base_config else {}
        data_config = base_config.get('data', {}) if base_config else {}
        model_config = base_config.get('model', {}) if base_config else {}
        training_config = base_config.get('training', {}) if base_config else {}
        presets_section = base_config.get('presets', {}) if base_config else {}
        metadata = base_config.get('metadata', {}) if base_config else {}
        
        # Context extraction
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        if base_config:
            # Preset detection
            if isinstance(presets_section, dict):
                preset_name = presets_section.get("current_preset", "Custom/Default")
            
            # Check metadata for preset_used
            if preset_name in ["Custom/Default", None, ""]:
                metadata = base_config.get("metadata", {})
                if isinstance(metadata, dict):
                    preset_name = metadata.get("preset_used", "Custom/Default")
            
            # Check legacy _preset_name field
            if preset_name in ["Custom/Default", None, ""]:
                preset_name = base_config.get("_preset_name", "Custom/Default")
            
            # Check runtime information
            if preset_name in ["Custom/Default", None, ""]:
                runtime = base_config.get("runtime", {})
                if isinstance(runtime, dict):
                    preset_name = runtime.get("active_preset", "Custom/Default")
            
            # Clean up preset name display
            if preset_name in ["Custom/Default", None, "", "none"]:
                preset_name = "Custom/Default"
            elif isinstance(preset_name, str):
                preset_name = preset_name.title()
            
            # Extract model type with error handling
            if isinstance(model_config, dict):
                model_type = model_config.get('model_type', 'Unknown')
            
            # Extract config source with fallbacks
            if "runtime" in base_config and isinstance(base_config["runtime"], dict):
                config_source = base_config["runtime"].get("config_source", "Unknown")
            elif "metadata" in base_config and isinstance(base_config["metadata"], dict):
                config_source = base_config["metadata"].get("config_source", "Unknown")
            else:
                config_source = "Unknown"
        
        # Hardware-aware system class detection
        cuda_available = hardware_data.get('cuda', {}).get('available', False)
        memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)
        
        # Determine system performance class for resource optimization
        if cuda_available and memory_gb >= 32 and cpu_cores >= 16:
            system_class = "enterprise"
            recommended_preset = "advanced"
        elif cuda_available and memory_gb >= 16 and cpu_cores >= 8:
            system_class = "high-end"
            recommended_preset = "performance"
        elif cuda_available and memory_gb >= 8:
            system_class = "performance"
            recommended_preset = "performance"
        elif memory_gb >= 8:
            system_class = "standard"
            recommended_preset = "baseline"
        elif memory_gb >= 4:
            system_class = "balanced"
            recommended_preset = "default"
        else:
            system_class = "limited"
            recommended_preset = "lightweight"
        
        # Extract current HPO configuration summary
        current_trials = hpo_config.get('n_trials', hpo_config.get('n_trials', 50))
        current_trial_epochs = hpo_config.get('trial_epochs', 30)
        current_timeout = hpo_config.get('timeout_seconds', hpo_config.get('timeout', 3600))
        current_sampler = hpo_config.get('sampler', 'TPESampler')
        current_pruner = hpo_config.get('pruner', 'MedianPruner')
        current_epochs = training_config.get('epochs', 100)
        current_batch_size = training_config.get('batch_size', 64)
        current_use_real_data = data_config.get('use_real_data', False)
        current_noise_factor = data_config.get('synthetic_generation', {}).get('noise_factor', 0.05)
        current_hpo_status = hpo_config.get('enabled', True)

        # enable hpo if not already
        if current_hpo_status is False:
            hpo_config['enabled'] = True

        if timeout_seconds is None:
            timeout_seconds = current_timeout
        
        if trial_count is None:
            trial_count = current_trials
        
        # Show Header
        print(Fore.MAGENTA + Style.BRIGHT + "EXPRESS HPO SETUP")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        print(Fore.YELLOW + Style.BRIGHT + "Base Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Recommended Preset: " + Fore.YELLOW + Style.BRIGHT + f"{recommended_preset}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Mode: " + Fore.YELLOW + Style.BRIGHT + f"Express HPO Setup")
        
        # Hardware Context Display
        print(Fore.YELLOW + Style.BRIGHT + "\nHardware Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
        if cuda_available:
            gpu_count = hardware_data.get('cuda', {}).get('gpu_count', 0)
            gpu_memory = hardware_data.get('cuda', {}).get('gpu_memory_gb', 0)
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ GPU Count: " + Fore.YELLOW + Style.BRIGHT + f"{gpu_count}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ GPU Memory: " + Fore.YELLOW + Style.BRIGHT + f"{gpu_memory}GB")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Memory: " + Fore.YELLOW + Style.BRIGHT + f"{memory_gb:.1f}GB")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ CPU Cores: " + Fore.YELLOW + Style.BRIGHT + f"{cpu_cores}")
        
        # Display preset-specific recommendations
        preset_compatibility = PRESET_CONFIGS.get(preset_name.lower(), {}).get('metadata', {}).get('compatibility', [])
        if preset_compatibility:
            print(Fore.CYAN + Style.BRIGHT + f"\nPreset Compatibility: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(preset_compatibility)}")
        
        # Display any additional parameter overrides
        extended_params = []
        if optimization_focus:
            extended_params.append(f"Optimization Focus: {optimization_focus}")
        if enable_storage is not None:
            extended_params.append(f"Storage: {enable_storage}")
        if enable_plots is not None:
            extended_params.append(f"Plots: {enable_plots}")
        if custom_search_space:
            extended_params.append("Custom search space parameters")
        if sampler_type:
            extended_params.append(f"Sampler: {sampler_type}")
        if pruner_type:
            extended_params.append(f"Pruner: {pruner_type}")
        if trial_count:
            extended_params.append(f"Trial Count Override: {trial_count}")
        if timeout_seconds:
            extended_params.append(f"Timeout Override: {timeout_seconds}s")
        if data_mode and data_mode != 'auto':
            extended_params.append(f"Data Mode: {data_mode}")
        
        if extended_params:
            print(Fore.CYAN + Style.BRIGHT + "\nParameter Overrides:")
            for i, param in enumerate(extended_params):
                prefix = "  └─" if i == len(extended_params) - 1 else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {param}")
        
        # Show detailed summary and confirmation
        if use_current_config and not (non_interactive or skip_prompt):
            print(Fore.MAGENTA + Style.BRIGHT + "\nCURRENT HPO CONFIGURATION SUMMARY")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            print(Fore.YELLOW + Style.BRIGHT + "HPO Core Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Enabled: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_config.get('enabled', True)}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials: " + Fore.YELLOW + Style.BRIGHT + f"{current_trials}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{current_timeout//60} minutes")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{current_sampler}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{current_pruner}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Data Source: " + Fore.YELLOW + Style.BRIGHT + f"{'Real Data' if current_use_real_data else 'Synthetic Data'}")
            
            # Training configuration
            print(Fore.YELLOW + Style.BRIGHT + "\nTraining Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{current_epochs}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.YELLOW + Style.BRIGHT + f"{current_batch_size}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.YELLOW + Style.BRIGHT + f"{training_config.get('learning_rate', 0.001)}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Validation Split: " + Fore.YELLOW + Style.BRIGHT + f"{training_config.get('validation_split', 0.2)}")
            
            # Data configuration
            if current_use_real_data:
                data_path = data_config.get('data_path', 'Default')
                print(Fore.YELLOW + Style.BRIGHT + "\nData Configuration:")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data Path: " + Fore.YELLOW + Style.BRIGHT + f"{data_path}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{data_config.get('features', 'Auto')}")
            else:
                normal_samples = data_config.get('normal_samples', 8000)
                attack_samples = data_config.get('attack_samples', 2000)
                features = data_config.get('features', 20)
                print(Fore.YELLOW + Style.BRIGHT + "\nData Configuration:")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.YELLOW + Style.BRIGHT + f"{normal_samples}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.YELLOW + Style.BRIGHT + f"{attack_samples}")
                print(Fore.GREEN + Style.BRIGHT + f"  └─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
            
            # Calculate estimated time for current configuration
            estimated_time = _estimate_hpo_time(
                n_trials=current_trials,
                trial_epochs=current_trial_epochs,
                n_model_types=1,  # Assume single model type for current config
                cv_folds=3,  # Default CV folds
                hardware_info=hardware_data,
                system_class=system_class
            )
            
            print(Fore.YELLOW + Style.BRIGHT + "\nSystem Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Estimated Duration: " + Fore.YELLOW + Style.BRIGHT + f"{estimated_time}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Parallel Trials: " + Fore.YELLOW + Style.BRIGHT + f"{min(2, cpu_cores // 2) if cpu_cores > 4 else 1}")

            # Initial menu options
            print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration Options:")
            print(Fore.WHITE + Style.BRIGHT + "1. Run HPO with current settings")
            print(Fore.WHITE + Style.BRIGHT + "2. Modify HPO configurations")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            while True:
                try:
                    initial_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-2): " + Style.RESET_ALL).strip()
                    if initial_choice in ['0', '1', '2']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                except (EOFError, KeyboardInterrupt):
                    return None
            
            if initial_choice == '0':
                return None
            
            elif initial_choice == '1':
                # Confirmation prompt to run with current settings
                try:
                    confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart hyperparameter optimization with these current settings? (Y/n): " + Style.RESET_ALL).lower().strip()
                except (EOFError, KeyboardInterrupt):
                    return None
                
                if confirm not in ('', 'y', 'yes'):
                    return None
                
                # Use current configuration as-is
                print(Fore.GREEN + Style.BRIGHT + "\nLaunching hyperparameter optimization with current configuration...")
                return _launch_hpo_with_config(config=base_config, **kwargs)
            
            elif initial_choice == '2':
                # Continue with full configuration process
                print(Fore.GREEN + Style.BRIGHT + "\nProceeding to full HPO configuration...")
                print(Fore.MAGENTA + Style.BRIGHT + "EXPRESS HPO SETUP - CUSTOM CONFIGURATION")
                print(Fore.CYAN + Style.BRIGHT + "-"*40)

        # Data Source Selection
        if data_mode is not None and not (non_interactive or skip_prompt):
            print(Fore.MAGENTA + Style.BRIGHT + "\nDATA SOURCE SELECTION")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            # Check current preset's data configuration
            current_preset_data = PRESET_CONFIGS.get(preset_name.lower(), {}).get('data', {})
            preset_use_real_data = current_preset_data.get('use_real_data', False)
            
            print(Fore.GREEN + Style.BRIGHT + f"Current preset recommends: " + Fore.YELLOW + Style.BRIGHT + f"{'Real Data' if preset_use_real_data else 'Synthetic Data'}")
            
            print(Fore.WHITE + Style.BRIGHT + "1. Real network data " + Fore.GREEN + Style.BRIGHT + "(recommended for production HPO)")
            print(Fore.WHITE + Style.BRIGHT + "2. Synthetic data " + Fore.GREEN + Style.BRIGHT + "(faster iterations, good for testing)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            while True:
                try:
                    data_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect data source (0-2): " + Style.RESET_ALL).strip()
                    if data_choice in ['1', '2', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                except (EOFError, KeyboardInterrupt):
                    return None
            
            if data_choice == '0':
                return None
                
            data_mode = 'real' if data_choice == '1' else 'synthetic'
            print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {'Real Data' if data_mode == 'real' else 'Synthetic Data'}")
            
            # Data configuration based on selection
            if data_mode == 'real':
                use_real_data = True
                print(Fore.MAGENTA + Style.BRIGHT + "\nREAL DATA CONFIGURATION")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                # Data file path configuration
                default_data_path = data_config.get('data_path', 'data/network_data.csv')
                print(Fore.YELLOW + Style.BRIGHT + f"Selected: Real Network Data")
                print(Fore.CYAN + Style.BRIGHT + "  └─ Using actual network traffic data for realistic optimization")
                print(Fore.YELLOW + Style.BRIGHT + f"\nData file path (default): " + Fore.GREEN + Style.BRIGHT + f"{default_data_path}")
                print(Fore.CYAN + Style.BRIGHT + "  ├─ Path to your network traffic data file")
                print(Fore.CYAN + Style.BRIGHT + "  └─ Leave empty to use default or provide custom path")
                
                user_data_path = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter data file path or 0 to cancel: " + Style.RESET_ALL).strip()

                if user_data_path == '0':
                    return None

                data_path = user_data_path if user_data_path else default_data_path
                
                # Validate path exists
                if not Path(data_path).exists():
                    print(Fore.YELLOW + Style.BRIGHT + f"\nWarning: Path '{data_path}' does not exist.")
                    create_confirm = input(Fore.YELLOW + Style.BRIGHT + "\nContinue anyway? (y/N): " + Style.RESET_ALL).strip().lower()
                    if create_confirm not in ('y', 'yes'):
                        data_path = default_data_path
                        Path(data_path).parent.mkdir(parents=True, exist_ok=True)
                        print(Fore.GREEN + Style.BRIGHT + f"\nUsing default data path: {data_path}")

                # Data format selection
                print(Fore.YELLOW + Style.BRIGHT + f"\nData format (default): " + Fore.GREEN + Style.BRIGHT + "auto")
                print(Fore.CYAN + Style.BRIGHT + "  └─ Format of your data file")
                print(Fore.WHITE + Style.BRIGHT + "1. CSV " + Fore.GREEN + Style.BRIGHT + "(Comma-separated values)")
                print(Fore.WHITE + Style.BRIGHT + "2. Parquet " + Fore.GREEN + Style.BRIGHT + "(Columnar storage)")
                print(Fore.WHITE + Style.BRIGHT + "3. JSON " + Fore.GREEN + Style.BRIGHT + "(JavaScript Object Notation)")
                print(Fore.WHITE + Style.BRIGHT + "4. Auto-detect " + Fore.GREEN + Style.BRIGHT + "(Detect from file extension)")
                print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
                
                format_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect data format (0-4): " + Style.RESET_ALL).strip()
                format_map = {'1': 'csv', '2': 'parquet', '3': 'json', '4': 'auto'}

                if format_choice == '0':
                    return None

                data_format = format_map.get(format_choice, 'auto')

                # Artifacts file path configuration
                default_artifacts_path = data_config.get('artifacts_path', 'data/artifacts.pkl')
                print(Fore.YELLOW + Style.BRIGHT + f"\nArtifacts path (default): " + Fore.GREEN + Style.BRIGHT + f"{default_artifacts_path}")
                print(Fore.CYAN + Style.BRIGHT + "  ├─ Path to your preprocessing artifacts data file")
                print(Fore.CYAN + Style.BRIGHT + "  ├─ Directory for storing preprocessing artifacts")
                print(Fore.CYAN + Style.BRIGHT + "  └─ Leave empty to use default or provide custom path")
                
                user_artifacts_path = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter artifacts file path or 0 to cancel: " + Style.RESET_ALL).strip()

                if user_artifacts_path == '0':
                    return None
                
                artifacts_path = user_artifacts_path if user_artifacts_path else default_artifacts_path
                
                # Validate artifacts path exists
                if not Path(artifacts_path).exists():
                    print(Fore.YELLOW + Style.BRIGHT + f"\nWarning: Path '{artifacts_path}' does not exist.")
                    create_confirm = input(Fore.YELLOW + Style.BRIGHT + "\nContinue anyway? (y/N): " + Style.RESET_ALL).strip().lower()
                    if create_confirm not in ('y', 'yes'):
                        artifacts_path = default_data_path
                        Path(artifacts_path).parent.mkdir(parents=True, exist_ok=True)
                        print(Fore.GREEN + Style.BRIGHT + f"\nUsing default artifacts path: {artifacts_path}")
                
                # Feature configuration
                preset_features = current_preset_data.get('features', 20)
                print(Fore.YELLOW + Style.BRIGHT + f"\nNumber of features (default): " + Fore.GREEN + Style.BRIGHT + f"{preset_features}")
                print(Fore.CYAN + Style.BRIGHT + "  ├─ Number of input features or 'auto' to detect")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Preset recommends {preset_features} features")

                # Use preset features or allow customization
                user_features = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of features or 'c' to cancel: " + Style.RESET_ALL).strip()

                if user_features == 'c':
                    return None
                
                if user_features and user_features.lower() != 'auto':
                    try:
                        features = int(user_features)
                        print(Fore.GREEN + Style.BRIGHT + f"\nUsing {features} features")
                    except ValueError:
                        features = preset_features
                        print(Fore.YELLOW + Style.BRIGHT + "\nInvalid input, using preset recommended features")
                else:
                    features = preset_features if user_features != 'auto' else 'auto'
                    print(Fore.GREEN + Style.BRIGHT + f"\nUsing {'auto feature detection' if features == 'auto' else 'preset recommended features'}")

                print(Fore.GREEN + Style.BRIGHT + f"\nReal network data configured:")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data file path: " + Fore.YELLOW + Style.BRIGHT + f"{data_path}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data format: " + Fore.YELLOW + Style.BRIGHT + f"{data_format}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Artifacts path: " + Fore.YELLOW + Style.BRIGHT + f"{artifacts_path}")
                print(Fore.GREEN + Style.BRIGHT + f"  └─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
                
            else:  # Synthetic data
                use_real_data = False
                print(Fore.MAGENTA + Style.BRIGHT + "\nSYNTHETIC DATA CONFIGURATION")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                # Use preset synthetic data configuration as baseline
                preset_normal_samples = current_preset_data.get('normal_samples', 8000)
                preset_attack_samples = current_preset_data.get('attack_samples', 2000)
                preset_features = current_preset_data.get('features', 20)
                preset_noise_factor = current_preset_data.get('synthetic_generation', {}).get('noise_factor', 0.05)
                
                print(Fore.GREEN + Style.BRIGHT + f"Selected: Synthetic Data")
                print(Fore.CYAN + Style.BRIGHT + "  ├─ Using generated data for faster iteration cycles")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Preset defaults: {preset_normal_samples} normal, {preset_attack_samples} attack samples, {preset_features} features")
                
                # Data complexity level
                print(Fore.YELLOW + Style.BRIGHT + "\nData complexity level:")
                print(Fore.CYAN + Style.BRIGHT + "  └─ Controls the realism and complexity of generated data")
                print(Fore.WHITE + Style.BRIGHT + "1. Simple " + Fore.GREEN + Style.BRIGHT + "(Basic patterns, fast generation)")
                print(Fore.WHITE + Style.BRIGHT + "2. Moderate " + Fore.GREEN + Style.BRIGHT + "(Balanced complexity - preset default)")
                print(Fore.WHITE + Style.BRIGHT + "3. Complex " + Fore.GREEN + Style.BRIGHT + "(Realistic patterns, slower generation)")
                print(Fore.WHITE + Style.BRIGHT + "4. Custom " + Fore.GREEN + Style.BRIGHT + "(Configure all parameters manually)")
                print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
                
                complexity_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect complexity level (0-4): " + Style.RESET_ALL).strip()

                if complexity_choice == '0':
                    return None
                
                if complexity_choice == '4':
                    # Custom synthetic data configuration
                    print(Fore.MAGENTA + Style.BRIGHT + "\nCUSTOM SYNTHETIC DATA CONFIGURATION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    # Normal samples configuration
                    print(Fore.CYAN + Style.BRIGHT + "Number of normal samples to generate:")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{preset_normal_samples}")
                    user_normal = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of normal samples or 'c' to cancel: " + Style.RESET_ALL).strip()

                    if user_normal == 'c':
                        return None

                    normal_samples = int(user_normal) if user_normal else preset_normal_samples
                    
                    # Attack samples configuration
                    print(Fore.CYAN + Style.BRIGHT + "\nNumber of attack samples to generate:")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{preset_attack_samples}")
                    user_attack = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of attack samples or 'c' to cancel: " + Style.RESET_ALL).strip()

                    if user_attack == 'c':
                        return None

                    attack_samples = int(user_attack) if user_attack else preset_attack_samples
                    
                    # Features configuration
                    print(Fore.CYAN + Style.BRIGHT + "\nNumber of features to generate:")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{preset_features}")
                    user_features = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of features or 'c' to cancel: " + Style.RESET_ALL).strip()

                    if user_features == 'c':
                        return None

                    features = int(user_features) if user_features else preset_features
                    
                    # Noise level configuration
                    print(Fore.CYAN + Style.BRIGHT + "\nNoise level for data generation (0.0-1.0):")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{preset_noise_factor}")
                    user_noise = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter noise level or 'c' to cancel: " + Style.RESET_ALL).strip()

                    if user_noise == 'c':
                        return None

                    noise_factor = float(user_noise) if user_noise else preset_noise_factor
                    
                    print(Fore.GREEN + Style.BRIGHT + f"\nCustom synthetic data configured:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Normal samples: " + Fore.YELLOW + Style.BRIGHT + f"{normal_samples}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Attack samples: " + Fore.YELLOW + Style.BRIGHT + f"{attack_samples}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Noise Factor: " + Fore.YELLOW + Style.BRIGHT + f"{noise_factor}")
                    
                else:
                    # Predefined complexity levels
                    complexity_configs = {
                        '1': {
                            'normal_samples': max(1000, preset_normal_samples // 2),
                            'attack_samples': max(200, preset_attack_samples // 2),
                            'features': max(10, preset_features // 2),
                            'noise_factor': preset_noise_factor * 0.5,
                            'name': 'Simple'
                        },
                        '2': {
                            'normal_samples': preset_normal_samples,
                            'attack_samples': preset_attack_samples,
                            'features': preset_features,
                            'noise_factor': preset_noise_factor,
                            'name': 'Moderate'
                        },
                        '3': {
                            'normal_samples': preset_normal_samples * 2,
                            'attack_samples': preset_attack_samples * 2,
                            'features': min(100, preset_features * 2),
                            'noise_factor': preset_noise_factor * 1.5,
                            'name': 'Complex'
                        }
                    }
                    
                    config = complexity_configs.get(complexity_choice, complexity_configs['2'])
                    normal_samples = config['normal_samples']
                    attack_samples = config['attack_samples']
                    features = config['features']
                    noise_factor = config['noise_factor']
                    config_name = config['name']
                    
                    print(Fore.GREEN + Style.BRIGHT + f"\nCustom synthetic data configured:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config name: " + Fore.YELLOW + Style.BRIGHT + f"{config_name}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Normal samples: " + Fore.YELLOW + Style.BRIGHT + f"{normal_samples}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Attack samples: " + Fore.YELLOW + Style.BRIGHT + f"{attack_samples}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Noise level: " + Fore.YELLOW + Style.BRIGHT + f"{noise_factor}")
        
        # Resolve data mode
        use_real_data_config = data_config.get('use_real_data', False)
        if data_mode is None:
            # Auto-detect from existing config
            use_real_data = use_real_data_config
        else:
            # Use provided data_mode
            use_real_data = data_mode == 'real'
        
        # Skip interactive optimization goal if in non-interactive mode
        if non_interactive or skip_prompt:
            # Use provided trial_count or preset-based defaults
            n_trials = trial_count if trial_count is not None else current_trials
            trial_epochs = current_trial_epochs if current_trial_epochs is not None else 20
            timeout_minutes = (timeout_seconds // 60) if timeout_seconds else 60
            goal_name = "Express Optimization"
            print(Fore.GREEN + Style.BRIGHT + f"\nNon-interactive mode - using {n_trials} trials, {timeout_minutes} minutes timeout")
        else:
            # HPO Intensity Selection with system-aware recommendations
            print(Fore.MAGENTA + Style.BRIGHT + "\nHPO INTENSITY SELECTION")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            # System-aware intensity recommendations based on preset capabilities
            if system_class == "limited":
                recommended_intensity = '1'
                print(Fore.YELLOW + Style.BRIGHT + "  └─ System recommendation: Quick Scan (limited resources)")
            elif system_class == "enterprise":
                recommended_intensity = '4'
                print(Fore.YELLOW + Style.BRIGHT + "  └─ System recommendation: Exhaustive Search (high-performance system)")
            elif preset_name.lower() in ['performance', 'advanced']:
                recommended_intensity = '3'
                print(Fore.YELLOW + Style.BRIGHT + "  └─ System recommendation: Thorough Search (performance preset)")
            else:
                recommended_intensity = '2'
                print(Fore.YELLOW + Style.BRIGHT + "  └─ System recommendation: Standard Optimization (balanced approach)")
            
            # Intensity configurations
            intensity_configs = {
                '1': {
                    'n_trials': 20, 'trial_epochs': 15, 'timeout_minutes': 30, 'name': 'Quick Scan',
                    'description': 'Fast exploration for initial parameter ranges',
                    'best_for': 'Limited resources, quick experiments, parameter sensitivity analysis',
                    'aligned_presets': ['debug', 'lightweight']
                },
                '2': {
                    'n_trials': 50, 'trial_epochs': 20, 'timeout_minutes': 120, 'name': 'Standard Optimization',
                    'description': 'Balanced approach for most use cases',
                    'best_for': 'General optimization, production model tuning, balanced performance',
                    'aligned_presets': ['default', 'baseline', 'stability']
                },
                '3': {
                    'n_trials': 100, 'trial_epochs': 25, 'timeout_minutes': 240, 'name': 'Thorough Search',
                    'description': 'Comprehensive search for optimal performance',
                    'best_for': 'High-performance systems, final model tuning, research experiments',
                    'aligned_presets': ['performance']
                },
                '4': {
                    'n_trials': 200, 'trial_epochs': 30, 'timeout_minutes': 480, 'name': 'Exhaustive Search',
                    'description': 'Maximum coverage of parameter space',
                    'best_for': 'Enterprise systems, research papers, critical deployments',
                    'aligned_presets': ['advanced']
                }
            }
            
            # Display options with estimated times
            for key, config in intensity_configs.items():
                estimated_time = _estimate_hpo_time(
                    n_trials=config['n_trials'],
                    trial_epochs=config['trial_epochs'],
                    n_model_types=1,
                    cv_folds=3,
                    hardware_info=hardware_data,
                    system_class=system_class
                )
                recommendation_indicator = " " + Fore.GREEN + Style.BRIGHT + "*" + Style.RESET_ALL if key == recommended_intensity else ""
                preset_alignment = f" [Compatible presets: {', '.join(config['aligned_presets'])}]" if config['aligned_presets'] else ""
                
                print(Fore.WHITE + Style.BRIGHT + f"{key}. {config['name']}{recommendation_indicator}")
                print(Fore.CYAN + Style.BRIGHT + f"   ├─ {config['description']}{preset_alignment}")
                print(Fore.CYAN + Style.BRIGHT + f"   ├─ Best for: {config['best_for']}")
                print(Fore.CYAN + Style.BRIGHT + f"   ├─ Estimated Time: {estimated_time}")
                print(Fore.CYAN + Style.BRIGHT + f"   └─ Trials: {config['n_trials']}, Epochs: {config['trial_epochs']}, Timeout: {config['timeout_minutes']}m\n")
            
            print(Fore.WHITE + Style.BRIGHT + "\n5. Custom Configuration " + Fore.GREEN + Style.BRIGHT + "(Configure all parameters individually)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            while True:
                try:
                    intensity_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect HPO intensity (0-5) [Recommended: {recommended_intensity}]: " + Style.RESET_ALL).strip()
                    if intensity_choice in ['1', '2', '3', '4', '5', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-5.")
                except (EOFError, KeyboardInterrupt):
                    return None
            
            if intensity_choice == '0':
                return None
            
            if intensity_choice == '5':
                # Custom HPO configuration
                print(Fore.MAGENTA + Style.BRIGHT + "\nCUSTOM HPO CONFIGURATION")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                # Get preset-based defaults for custom configuration
                preset_hpo = PRESET_CONFIGS.get(preset_name.lower(), {}).get('hyperparameter_optimization', {})
                preset_trials = preset_hpo.get('n_trials', 50)
                preset_trial_epochs = preset_hpo.get('trial_epochs', 20)
                preset_timeout = preset_hpo.get('timeout_seconds', 3600) // 60
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Configure all parameters individually")
                
                # Number of trials
                print(Fore.CYAN + Style.BRIGHT + f"\nDefault trials: " + Fore.GREEN + Style.BRIGHT + f"{preset_trials}")
                user_trials = input(Fore.YELLOW + Style.BRIGHT + f"  └─ Enter number of trials or 'c' to cancel: " + Style.RESET_ALL).strip()

                if user_trials == 'c':
                    return None

                n_trials = int(user_trials) if user_trials else preset_trials
                
                # Trial epochs
                print(Fore.CYAN + Style.BRIGHT + f"\nDefault trial epochs: " + Fore.GREEN + Style.BRIGHT + f"{preset_trial_epochs}")
                user_epochs = input(Fore.YELLOW + Style.BRIGHT + f"  └─ Enter trial epochs or 'c' to cancel: " + Style.RESET_ALL).strip()

                if user_epochs == 'c':
                    return None

                trial_epochs = int(user_epochs) if user_epochs else preset_trial_epochs
                
                # Timeout configuration
                print(Fore.CYAN + Style.BRIGHT + f"\nDefault timeout (minutes): " + Fore.GREEN + Style.BRIGHT + f"{preset_timeout}")
                user_timeout = input(Fore.YELLOW + Style.BRIGHT + f"  └─ Enter timeout in minutes or 'c' to cancel: " + Style.RESET_ALL).strip()

                if user_timeout == 'c':
                    return None

                timeout_minutes = float(user_timeout) if user_timeout else preset_timeout
                
                timeout_seconds = timeout_minutes * 60
                goal_name = "Custom configuration"
                
            else:
                goal_config = intensity_configs[intensity_choice]
                n_trials = goal_config['n_trials']
                trial_epochs = goal_config['trial_epochs']
                timeout_minutes = goal_config['timeout_minutes']
                timeout_seconds = timeout_minutes * 60
                goal_name = goal_config['name']
            
            # Show final estimated time for selected configuration
            final_estimated_time = _estimate_hpo_time(
                n_trials=n_trials,
                trial_epochs=trial_epochs,
                n_model_types=1,
                cv_folds=3,
                hardware_info=hardware_data,
                system_class=system_class
            )
            
            print(Fore.GREEN + Style.BRIGHT + f"\nSelected: " + Fore.YELLOW + Style.BRIGHT + f"{goal_name}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials: " + Fore.YELLOW + Style.BRIGHT + f"{n_trials}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trial epochs: " + Fore.YELLOW + Style.BRIGHT + f"{trial_epochs}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{timeout_minutes} minutes")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Estimated duration: " + Fore.YELLOW + Style.BRIGHT + f"{final_estimated_time}")
        
        # Optimization Focus Selection
        if optimization_focus is None and not (non_interactive or skip_prompt):
            print(Fore.MAGENTA + Style.BRIGHT + "\nOPTIMIZATION FOCUS SELECTION")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            # Focus recommendation based on preset and system
            focus_recommendation = '3'  # Balanced by default
            print(Fore.YELLOW + Style.BRIGHT + "  ├─ Default recommendation: Speed - optimized training with limited resources (performance/debug/lightweight)")
            
            if preset_name.lower() in ['default', 'baseline', 'stability']:
                focus_recommendation = '1'  # Speed for performance preset
                print(Fore.YELLOW + Style.BRIGHT + "  └─ Preset recommendation: Balanced - for optimized training (default/baseline/stability presets)")
            elif preset_name.lower() == 'advanced':
                focus_recommendation = '2'  # Accuracy for advanced preset
                print(Fore.YELLOW + Style.BRIGHT + "  └─ Preset recommendation: Accuracy - for high-performance system (advanced preset)")
            elif preset_name.lower() in ['performance','debug', 'lightweight']:
                focus_recommendation = '3'  # Speed for resource-constrained presets
                print(Fore.YELLOW + Style.BRIGHT + " └─ Preset recommendation: Speed - optimized training with limited resources (performance/debug/lightweight)")
            elif system_class in ["limited", "standard"]:
                focus_recommendation = '4'  # Speed for limited systems
                print(Fore.YELLOW + Style.BRIGHT + " └─ System recommendation: Speed - for limited resources/standard systems")
            
            focus_configs = {
                '1': {
                    'name': 'Balanced', 'description': 'Optimizes for both accuracy and training efficiency',
                    'priorities': ['Model performance', 'Training speed', 'Resource usage'],
                    'best_for': 'Most use cases, general optimization, production systems',
                    'aligned_presets': ['default', 'baseline', 'stability']
                },
                '2': {
                    'name': 'Accuracy', 'description': 'Prioritizes model performance over training speed',
                    'priorities': ['Model performance', 'Generalization', 'Robustness'],
                    'best_for': 'Critical applications, final model tuning, accuracy-sensitive tasks',
                    'aligned_presets': ['advanced']
                },
                '3': {
                    'name': 'Speed', 'description': 'Prioritizes training efficiency and resource usage',
                    'priorities': ['Training speed', 'Resource efficiency', 'Quick iterations'],
                    'best_for': 'Limited resources, development phase, rapid prototyping',
                    'aligned_presets': ['performance', 'debug', 'lightweight']
                },
                '4': {
                    'name': 'Efficiency', 'description': 'Focuses on resource efficiency and model size',
                    'priorities': ['Memory usage', 'Inference speed', 'Model size'],
                    'best_for': 'Edge deployment, resource-constrained environments, mobile applications',
                    'aligned_presets': ['lightweight']
                }
            }
            
            # Display focus options
            for key, config in focus_configs.items():
                recommendation_indicator = " " + Fore.GREEN + "*" + Style.RESET_ALL if key == focus_recommendation else ""
                preset_alignment = f" [Compatible presets: {', '.join(config['aligned_presets'])}]" if config['aligned_presets'] else ""
                
                print(Fore.WHITE + Style.BRIGHT + f"{key}. {config['name']}{recommendation_indicator}")
                print(Fore.CYAN + Style.BRIGHT + f"   ├─ {config['description']}{preset_alignment}")
                print(Fore.GREEN + Style.BRIGHT + f"   ├─ Priorities: {', '.join(config['priorities'])}")
                print(Fore.YELLOW + Style.BRIGHT + f"   └─ Best for: {config['best_for']}")
            
            print(Fore.RED + Style.BRIGHT + "\n0. Cancel and return to previous menu")
            
            while True:
                try:
                    focus_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect optimization focus (0-4) [Recommended: {focus_recommendation}]: " + Style.RESET_ALL).strip()
                    if not focus_choice and focus_recommendation:
                        focus_choice = focus_recommendation
                        print(Fore.GREEN + Style.BRIGHT + f"\nUsing recommended focus: {focus_choice}")
                    if focus_choice in ['1', '2', '3', '4', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-4.")
                except (EOFError, KeyboardInterrupt):
                    return None
            
            if focus_choice == '0':
                return None
                
            focus_map = {'1': 'balanced', '2': 'accuracy', '3': 'speed', '4': 'efficiency'}
            optimization_focus = focus_map.get(focus_choice, 'speed')
            focus_config = focus_configs[focus_choice]
            
            print(Fore.GREEN + Style.BRIGHT + f"\nSelected: " + Fore.YELLOW + Style.BRIGHT + f"{focus_config['name']} Focus")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Description: " + Fore.YELLOW + Style.BRIGHT + f"{focus_config['description']}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Priorities: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(focus_config['priorities'])}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Best for: " + Fore.YELLOW + Style.BRIGHT + f"{focus_config['best_for']}")
        
        # Algorithm Selection
        if (sampler_type is None or pruner_type is None) and not (non_interactive or skip_prompt):
            print(Fore.MAGENTA + Style.BRIGHT + "\nOPTIMIZATION ALGORITHM SELECTION")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            # Get preset HPO configuration for algorithm defaults
            preset_hpo = PRESET_CONFIGS.get(preset_name.lower(), {}).get('hyperparameter_optimization', {})
            preset_sampler = preset_hpo.get('sampler', 'TPESampler')
            preset_pruner = preset_hpo.get('pruner', 'MedianPruner')
            
            # Algorithm recommendations based on system, goal, and preset
            if n_trials > 10 <= 100 or preset_name.lower() in ['default', 'baseline', 'stability'] or system_class in ["standard", "balanced"]:
                algo_recommendation = '1'  # TPE + Median for default presets
                print(Fore.YELLOW + Style.BRIGHT + " └─ Default recommendation: TPE + Median Pruner - for default/baseline/stability presets")
            elif n_trials > 100 or preset_name.lower() in ['performance', 'advanced'] or system_class in ["performance", "high-end", "enterprise"]:
                algo_recommendation = '3'  # CmaEs for large search spaces
                print(Fore.YELLOW + Style.BRIGHT + " ├─ Trials recommendation: CMA-ES - for large search spaces (>100 trials)")
            elif n_trials <= 10 or system_class in ["limited", "standard"] or preset_name.lower() in ['debug', 'lightweight']:
                algo_recommendation = '2'  # Random + Hyperband for faster exploration
                print(Fore.YELLOW + Style.BRIGHT + " └─ System recommendation: Random + Hyperband - for limited/standard systems")
            elif preset_name.lower() in ['advanced'] or system_class in ["enterprise", "high-end"]:
                algo_recommendation = '4'  # NSGAIISampler for advanced presets
                print(Fore.YELLOW + Style.BRIGHT + " └─ Preset recommendation: NSGAIISampler + HyperbandPruner - for advanced preset")
            
            algo_configs = {
                '1': {
                    'sampler': 'TPESampler', 'pruner': 'MedianPruner', 'name': 'TPE + Median Pruner',
                    'description': 'Tree-structured Parzen Estimator with median-based pruning',
                    'strengths': ['Efficient for medium-sized spaces', 'Good convergence', 'Robust performance'],
                    'weaknesses': ['Slower for very large spaces', 'More memory usage'],
                    'best_for': 'Most use cases, balanced optimization, general hyperparameter tuning',
                    'aligned_presets': ['default', 'baseline', 'stability']
                },
                '2': {
                    'sampler': 'RandomSampler', 'pruner': 'HyperbandPruner', 'name': 'Random + Hyperband',
                    'description': 'Random sampling with aggressive early stopping',
                    'strengths': ['Very fast exploration', 'Good for large spaces', 'Memory efficient'],
                    'weaknesses': ['May miss optimal regions', 'Less thorough'],
                    'best_for': 'Large search spaces, limited resources, quick exploration',
                    'aligned_presets': ['debug', 'lightweight']
                },
                '3': {
                    'sampler': 'CmaEsSampler', 'pruner': 'MedianPruner', 'name': 'CMA-ES + Median Pruner',
                    'description': 'Covariance Matrix Adaptation Evolution Strategy',
                    'strengths': ['Excellent for complex spaces', 'Good convergence properties', 'Handles dependencies well'],
                    'weaknesses': ['Slower initial progress', 'More computational overhead'],
                    'best_for': 'Complex optimization landscapes, research, thorough searches',
                    'aligned_presets': ['performance', 'advanced']
                },
                '4': {
                    'sampler': 'NSGAIISampler', 'pruner': 'HyperbandPruner', 'name': 'NSGA-II + Hyperband',
                    'description': 'Multi-objective optimization with fast pruning',
                    'strengths': ['Multi-objective optimization', 'Pareto front discovery', 'Good for trade-off analysis'],
                    'weaknesses': ['More complex configuration', 'Higher computational cost'],
                    'best_for': 'Multi-objective optimization, trade-off analysis, advanced users',
                    'aligned_presets': ['advanced']
                }
            }
            
            # Display algorithm options
            for key, config in algo_configs.items():
                recommendation_indicator = " " + Fore.GREEN + "*" + Style.RESET_ALL if key == algo_recommendation else ""
                preset_alignment = f" [Compatible presets: {', '.join(config['aligned_presets'])}]" if config['aligned_presets'] else ""
                
                print(Fore.WHITE + Style.BRIGHT + f"{key}. {config['name']}{recommendation_indicator}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ " + Fore.CYAN + Style.BRIGHT + f"{config['description']}{preset_alignment}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ Strengths: " + Fore.GREEN + Style.BRIGHT + f"{', '.join(config['strengths'])}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ Weaknesses: " + Fore.MAGENTA + Style.BRIGHT + f"{', '.join(config['weaknesses'])}")
                print(Fore.WHITE + Style.BRIGHT + f"   └─ Best for: " + Fore.GREEN + Style.BRIGHT + f"{config['best_for']}")

            print(Fore.WHITE + Style.BRIGHT + f"\n5. Use preset defaults " + Fore.GREEN + Style.BRIGHT + f"({preset_sampler} + {preset_pruner})")
            print(Fore.WHITE + Style.BRIGHT + "6. Custom Algorithm " + Fore.GREEN + Style.BRIGHT + "(configure sampler and pruner individually)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            while True:
                try:
                    algo_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect algorithm (0-6) [Recommended: {algo_recommendation}]: " + Style.RESET_ALL).strip()
                    if algo_choice in ['1', '2', '3', '4', '5', '6', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-6.")
                except (EOFError, KeyboardInterrupt):
                    return None
            
            if algo_choice == '0':
                return None
            
            if algo_choice == '5':
                # Use preset defaults
                sampler_type = preset_sampler
                pruner_type = preset_pruner
                algo_name = f"Preset Defaults ({sampler_type} + {pruner_type})"
                print(Fore.GREEN + Style.BRIGHT + f"\nUsing preset defaults: {sampler_type} + {pruner_type}")
                
            elif algo_choice == '6':
                # Custom algorithm configuration
                print(Fore.MAGENTA + Style.BRIGHT + "\nCUSTOM ALGORITHM CONFIGURATION")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                # Sampler selection
                available_samplers = ['TPESampler', 'RandomSampler', 'CmaEsSampler', 'GridSampler', 'NSGAIISampler']
                print(Fore.YELLOW + Style.BRIGHT + "\nAvailable samplers:")
                print(Fore.CYAN + Style.BRIGHT + "  └─ Default sampler: " + Fore.GREEN + Style.BRIGHT + f"{preset_sampler}")
                for i, sampler in enumerate(available_samplers, 1):
                    print(Fore.WHITE + Style.BRIGHT + f"  {i}. {sampler}")
                
                sampler_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect sampler (1-5): " + Style.RESET_ALL).strip()

                if sampler_choice == 'c':
                    return None

                if sampler_choice and sampler_choice.isdigit() and 1 <= int(sampler_choice) <= len(available_samplers):
                    sampler_type = available_samplers[int(sampler_choice) - 1]
                else:
                    sampler_type = preset_sampler
                
                # Pruner selection
                available_pruners = ['MedianPruner', 'HyperbandPruner', 'NopPruner', 'PercentilePruner', 'SuccessiveHalvingPruner']
                print(Fore.YELLOW + Style.BRIGHT + "\nAvailable pruners:")
                print(Fore.CYAN + Style.BRIGHT + "  └─ Default pruner: " + Fore.GREEN + Style.BRIGHT + f"{preset_pruner}")
                for i, pruner in enumerate(available_pruners, 1):
                    print(Fore.WHITE + Style.BRIGHT + f"  {i}. {pruner}")
                
                pruner_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect pruner (1-5): " + Style.RESET_ALL).strip()

                if pruner_choice == 'c':
                    return None

                if pruner_choice and pruner_choice.isdigit() and 1 <= int(pruner_choice) <= len(available_pruners):
                    pruner_type = available_pruners[int(pruner_choice) - 1]
                else:
                    pruner_type = preset_pruner
                
                algo_name = f"Custom configuration"
                print(Fore.GREEN + Style.BRIGHT + f"\nCustom algorithm configured: {sampler_type} + {pruner_type}")
                
            else:
                algo_config = algo_configs[algo_choice]
                sampler_type = algo_config['sampler']
                pruner_type = algo_config['pruner']
                algo_name = algo_config['name']
            
            print(Fore.GREEN + Style.BRIGHT + f"\nSelected: " + Fore.YELLOW + Style.BRIGHT + f"{algo_name}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{sampler_type}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{pruner_type}")
        
        # Model types selection
        if model_types is None and not (non_interactive or skip_prompt):
            print(Fore.MAGENTA + Style.BRIGHT + "\nMODEL TYPES TO OPTIMIZE")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            # Get preset model compatibility
            preset_compatibility = PRESET_CONFIGS.get(preset_name.lower(), {}).get('metadata', {}).get('compatibility', [])
            available_models = preset_compatibility if preset_compatibility else ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
            
            # Model selection recommendation based on preset and resources
            model_recommendation = '2'  # Enhanced + Ensemble by default
            
            if system_class in ["limited", "standard"]:
                model_recommendation = '4'  # Enhanced only for limited systems
            elif goal_name in ["Quick Scan"]:
                model_recommendation = '5'  # Simple only for quick scans
            elif 'AutoencoderEnsemble' not in available_models:
                model_recommendation = '4'  # Enhanced only if ensemble not available
            
            model_type_configs = {
                '1': {
                    'models': available_models, 'name': 'All compatible models',
                    'description': 'Comprehensive search across all compatible model architectures',
                    'coverage': 'Full architecture space exploration', 'time_impact': 'High',
                    'best_for': 'Research, architecture comparison, maximum coverage'
                },
                '2': {
                    'models': [m for m in ['EnhancedAutoencoder', 'AutoencoderEnsemble'] if m in available_models], 
                    'name': 'Enhanced + Ensemble',
                    'description': 'Focus on high-performance models with ensemble capability',
                    'coverage': 'Best performing architectures', 'time_impact': 'Medium-High',
                    'best_for': 'Production optimization, best performance focus'
                },
                '3': {
                    'models': [m for m in ['SimpleAutoencoder', 'EnhancedAutoencoder'] if m in available_models], 
                    'name': 'Simple + Enhanced',
                    'description': 'Balanced approach with baseline and enhanced models',
                    'coverage': 'Performance vs complexity trade-off', 'time_impact': 'Medium',
                    'best_for': 'Balanced optimization, performance-complexity analysis'
                },
                '4': {
                    'models': ['EnhancedAutoencoder'] if 'EnhancedAutoencoder' in available_models else available_models[:1], 
                    'name': 'Enhanced only',
                    'description': 'Focus on the most capable single model architecture',
                    'coverage': 'Single high-performance architecture', 'time_impact': 'Low-Medium',
                    'best_for': 'Standard optimization, resource-constrained environments'
                },
                '5': {
                    'models': ['SimpleAutoencoder'] if 'SimpleAutoencoder' in available_models else available_models[:1], 
                    'name': 'Simple only',
                    'description': 'Fast optimization of basic autoencoder architecture',
                    'coverage': 'Basic architecture only', 'time_impact': 'Low',
                    'best_for': 'Quick experiments, baseline establishment, limited resources'
                },
                '6': {
                    'models': ['AutoencoderEnsemble'] if 'AutoencoderEnsemble' in available_models else available_models[:1], 
                    'name': 'Ensemble only',
                    'description': 'Optimize ensemble configurations and combinations',
                    'coverage': 'Ensemble-specific parameters', 'time_impact': 'Medium',
                    'best_for': 'Ensemble specialization, combination optimization'
                }
            }
            
            # Filter out invalid configurations
            valid_model_configs = {}
            for key, config in model_type_configs.items():
                if config['models']:  # Only include configurations with valid models
                    valid_model_configs[key] = config
            
            # Display options with estimated time impact
            for key, config in valid_model_configs.items():
                n_model_types = len(config['models'])
                time_impact = _estimate_hpo_time(
                    n_trials=n_trials,
                    trial_epochs=trial_epochs,
                    n_model_types=n_model_types,
                    cv_folds=3,
                    hardware_info=hardware_data,
                    system_class=system_class
                )
                recommendation_indicator = " " + Fore.GREEN + "*" + Style.RESET_ALL if key == model_recommendation else ""
                print(Fore.WHITE + Style.BRIGHT + f"{key}. {config['name']}{recommendation_indicator}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ " + Fore.CYAN + Style.BRIGHT + f"{config['description']}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ {n_model_types} Model{'s' if n_model_types > 1 else ''}: " + Fore.GREEN + Style.BRIGHT + f"{', '.join(config['models'])}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ Coverage: " + Fore.GREEN + Style.BRIGHT + f"{config['coverage']}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ Time Impact: " + Fore.MAGENTA + Style.BRIGHT + f"{config['time_impact']} - {time_impact}")
                print(Fore.WHITE + Style.BRIGHT + f"   └─ Best for: " + Fore.GREEN + Style.BRIGHT + f"{config['best_for']}")
            
            print(Fore.RED + Style.BRIGHT + "\n0. Cancel and return to previous menu")
            
            model_choice = None
            while not model_choice:
                try:
                    model_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect model types (0-{len(valid_model_configs)}) [Recommended: {model_recommendation}]: ").strip()
                    if not model_choice and model_recommendation in valid_model_configs:
                        model_choice = model_recommendation
                        print(Fore.GREEN + Style.BRIGHT + f"\nUsing recommended model selection: {model_choice}")

                    if model_choice not in list(valid_model_configs.keys()) + ['0']:
                        print(Fore.RED + Style.BRIGHT + f"\nInvalid choice. Please select 0-{len(valid_model_configs)}.")
                        model_choice = None
                        continue
                except (EOFError, KeyboardInterrupt):
                    return None
            
            if model_choice == '0':
                return None
            
            model_config = valid_model_configs[model_choice]
            model_types = model_config['models']
            
            print(Fore.GREEN + Style.BRIGHT + f"\nSelected: " + Fore.YELLOW + Style.BRIGHT + f"{model_config['name']}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Models: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(model_types)}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Description: " + Fore.YELLOW + Style.BRIGHT + f"{model_config['description']}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Coverage: " + Fore.YELLOW + Style.BRIGHT + f"{model_config['coverage']}")
        
        # Cross-validation selection
        cv_folds = 3  # Default
        cv_name = "3-fold CV"
        if not (non_interactive or skip_prompt):
            print(Fore.MAGENTA + Style.BRIGHT + "\nVALIDATION STRATEGY")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            # Get preset validation configuration
            preset_validation = PRESET_CONFIGS.get(preset_name.lower(), {}).get('validation', {})
            preset_cv = preset_validation.get('cross_validation', {})
            preset_cv_folds = preset_cv.get('folds', 3) if preset_cv.get('enabled', True) else 1
            
            # CV recommendation based on data and resources
            cv_recommendation = '2'  # 5-fold CV by default
            
            if n_trials > 50 or system_class in ["limited", "standard"]:
                cv_recommendation = '1'  # 3-fold for larger trials or limited resources
            elif data_mode == 'synthetic':
                cv_recommendation = '3'  # Single split for synthetic data
            elif preset_cv_folds == 1:
                cv_recommendation = '3'  # Match preset if it uses single split
            
            cv_configs = {
                '1': {
                    'cv_folds': 3, 'name': '3-fold Cross Validation',
                    'description': 'Balanced approach between robustness and speed',
                    'robustness': 'Good', 'variance': 'Medium',
                    'best_for': 'Most use cases, balanced validation, general optimization'
                },
                '2': {
                    'cv_folds': 5, 'name': '5-fold Cross Validation',
                    'description': 'More robust validation with better performance estimation',
                    'robustness': 'High', 'variance': 'Low',
                    'best_for': 'Production optimization, reliable performance estimation'
                },
                '3': {
                    'cv_folds': 1, 'name': 'Single split validation',
                    'description': 'Fastest validation with single train-test split',
                    'robustness': 'Basic', 'variance': 'High',
                    'best_for': 'Quick experiments, large datasets, synthetic data'
                },
                '4': {
                    'cv_folds': 10, 'name': '10-fold Cross Validation',
                    'description': 'Maximum robustness for performance estimation',
                    'robustness': 'Very High', 'variance': 'Very Low',
                    'best_for': 'Small datasets, critical applications, research papers'
                }
            }
            
            # Display options with estimated time impact
            for key, config in cv_configs.items():
                time_impact = _estimate_hpo_time(
                    n_trials=n_trials,
                    trial_epochs=trial_epochs,
                    n_model_types=len(model_types) if model_types else 1,
                    cv_folds=config['cv_folds'],
                    hardware_info=hardware_data,
                    system_class=system_class
                )
                recommendation_indicator = " " + Fore.GREEN + Style.BRIGHT + "*" + Style.RESET_ALL if key == cv_recommendation else ""
                preset_indicator = " [Preset default]" if config['cv_folds'] == preset_cv_folds else ""
                
                print(Fore.WHITE + Style.BRIGHT + f"{key}. {config['name']}{recommendation_indicator}{preset_indicator}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ " + Fore.CYAN + Style.BRIGHT + f"{config['description']}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ Robustness: " + Fore.GREEN + Style.BRIGHT + f"{config['robustness']}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ Variance: " + Fore.GREEN + Style.BRIGHT + f"{config['variance']}")
                print(Fore.WHITE + Style.BRIGHT + f"   ├─ Time Impact: " + Fore.MAGENTA + Style.BRIGHT + f"{time_impact}")
                print(Fore.WHITE + Style.BRIGHT + f"   └─ Best for: " + Fore.GREEN + Style.BRIGHT + f"{config['best_for']}")
            
            print(Fore.WHITE + Style.BRIGHT + f"\n5. Use preset default " + Fore.GREEN + Style.BRIGHT + f"({preset_cv_folds}-fold)")
            print(Fore.WHITE + Style.BRIGHT + "6. Custom folds " + Fore.GREEN + Style.BRIGHT + "(Specify number of folds)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            cv_choice = None
            while not cv_choice:
                try:
                    cv_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect validation strategy (0-6) [Recommended: {cv_recommendation}]: ").strip()
                    if not cv_choice and cv_recommendation:
                        cv_choice = cv_recommendation
                        print(Fore.GREEN + Style.BRIGHT + f"\nUsing recommended validation: {cv_choice}")
                    if cv_choice not in ['1', '2', '3', '4', '5', '6', '0']:
                        print(Fore.RED + Style.BRIGHT + f"\nInvalid choice. Please select 0-6.")
                        cv_choice = None
                        continue
                except (EOFError, KeyboardInterrupt):
                    return None
            
            if cv_choice == '0':
                return None
            
            if cv_choice == '5':
                # Use preset default
                cv_folds = preset_cv_folds
                cv_name = f"{cv_folds}-fold Cross Validation"
                print(Fore.GREEN + Style.BRIGHT + f"\nUsing preset default: {cv_name}")
                
                # Set cv_robustness and cv_best_for based on preset_cv_folds
                if preset_cv_folds == 1:
                    cv_robustness = "Basic"
                    cv_best_for = "Quick experiments, large datasets, synthetic data"
                elif preset_cv_folds == 3:
                    cv_robustness = "Good"
                    cv_best_for = "Most use cases, balanced validation, general optimization"
                elif preset_cv_folds == 5:
                    cv_robustness = "High"
                    cv_best_for = "Production optimization, reliable performance estimation"
                elif preset_cv_folds == 10:
                    cv_robustness = "Very High"
                    cv_best_for = "Small datasets, critical applications, research papers"
                else:
                    cv_robustness = "Custom"
                    cv_best_for = "Custom fold configuration"
                
            elif cv_choice == '6':
                # Custom cross-validation configuration
                print(Fore.MAGENTA + Style.BRIGHT + "\nCUSTOM CROSS-VALIDATION CONFIGURATION")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                print(Fore.CYAN + Style.BRIGHT + "  └─ Default cross-validation folds: " + Fore.GREEN + Style.BRIGHT + f"{preset_cv_folds}")
                
                user_folds = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of cross-validation folds (1-10): " + Style.RESET_ALL).strip()

                if user_folds == 'c':
                    return None

                if user_folds:
                    cv_folds = int(user_folds)
                else:
                    cv_folds = preset_cv_folds
                
                cv_name = f"{cv_folds}-fold Cross Validation"
                cv_robustness = "Unknown"
                cv_best_for = "Custom configuration"
                print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {cv_name}")
                
            else:
                cv_config = cv_configs[cv_choice]
                cv_folds = cv_config['cv_folds']
                cv_name = cv_config['name']
                cv_robustness = cv_config['robustness']
                cv_best_for = cv_config['best_for']
            
            print(Fore.GREEN + Style.BRIGHT + f"\nSelected: " + Fore.YELLOW + Style.BRIGHT + f"{cv_name}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Folds: " + Fore.YELLOW + Style.BRIGHT + f"{cv_folds}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Robustness: " + Fore.YELLOW + Style.BRIGHT + f"{cv_robustness}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Best for: " + Fore.YELLOW + Style.BRIGHT + f"{cv_best_for}")
        
        # Performance Configuration
        print(Fore.MAGENTA + Style.BRIGHT + "\nPERFORMANCE CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Plot Generation Configuration
        if enable_plots is None and not (non_interactive or skip_prompt):
            # Get preset monitoring configuration
            preset_monitoring = PRESET_CONFIGS.get(preset_name.lower(), {}).get('monitoring', {})
            preset_tensorboard = preset_monitoring.get('tensorboard_logging', False)
            
            print(Fore.YELLOW + Style.BRIGHT + "Plot Generation")
            print(Fore.CYAN + Style.BRIGHT + "  └─ Generate optimization plots and visualizations")
            print(Fore.CYAN + Style.BRIGHT + f"     └─ Preset has tensorboard logging: {'Enabled' if preset_tensorboard else 'Disabled'}")
            
            while True:
                try:
                    plot_choice = input(Fore.YELLOW + Style.BRIGHT + "\nEnable plot generation? (Y/n): " + Style.RESET_ALL).strip().lower()
                    if not plot_choice or plot_choice in ('y', 'yes'):
                        enable_plots = True
                        print(Fore.GREEN + Style.BRIGHT + "\nPlot generation enabled")
                        break
                    elif plot_choice in ('n', 'no'):
                        enable_plots = False
                        print(Fore.MAGENTA + Style.BRIGHT + "\nPlot generation disabled")
                        break
                    else:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please enter Y or N.")
                except (EOFError, KeyboardInterrupt):
                    return None
        
        # Result Storage Configuration
        if enable_storage is None and not (non_interactive or skip_prompt):
            # Get preset HPO storage configuration
            preset_hpo_storage = PRESET_CONFIGS.get(preset_name.lower(), {}).get('hyperparameter_optimization', {}).get('storage', {})
            preset_storage_enabled = preset_hpo_storage.get('enabled', False)
            
            print(Fore.YELLOW + Style.BRIGHT + "\nResult Storage")
            print(Fore.CYAN + Style.BRIGHT + "  └─ Save HPO results for future analysis")
            print(Fore.CYAN + Style.BRIGHT + f"     └─ Preset has storage: {'Enabled' if preset_storage_enabled else 'Disabled'}")
            
            while True:
                try:
                    storage_choice = input(Fore.YELLOW + Style.BRIGHT + "\nEnable result storage? (Y/n): " + Style.RESET_ALL).strip().lower()
                    if not storage_choice or storage_choice in ('y', 'yes'):
                        enable_storage = True
                        print(Fore.GREEN + Style.BRIGHT + "\nResult storage enabled")
                        break
                    elif storage_choice in ('n', 'no'):
                        enable_storage = False
                        print(Fore.MAGENTA + Style.BRIGHT + "\nResult storage disabled")
                        break
                    else:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please enter Y or N.")
                except (EOFError, KeyboardInterrupt):
                    return None
        
        # Set defaults for express setup
        if enable_plots is None:
            enable_plots = True  # Default to enabled for express setup
        if enable_storage is None:
            enable_storage = True  # Default to enabled for express setup
        
        # Initialize variables that may not be set in all code paths
        features = features if 'features' in locals() else data_config.get('features', 20)
        normal_samples = normal_samples if 'normal_samples' in locals() else data_config.get('normal_samples', 8000)
        attack_samples = attack_samples if 'attack_samples' in locals() else data_config.get('attack_samples', 2000)
        goal_name = goal_name if 'goal_name' in locals() else "Express Optimization"
        timeout_minutes = timeout_minutes if 'timeout_minutes' in locals() else (timeout_seconds // 60) if timeout_seconds else 60
        
        # Validate all required variables are set
        required_vars = {
            'features': features,
            'normal_samples': normal_samples,
            'attack_samples': attack_samples,
            'goal_name': goal_name,
            'timeout_minutes': timeout_minutes,
            'n_trials': n_trials if 'n_trials' in locals() else trial_count if trial_count is not None else current_trials,
            'trial_epochs': trial_epochs if 'trial_epochs' in locals() else current_trial_epochs,
            'timeout_seconds': timeout_seconds,
            'optimization_focus': optimization_focus or 'balanced',
            'model_types': model_types if model_types and len(model_types) > 0 else ['EnhancedAutoencoder'],
            'cv_folds': cv_folds,
            'use_real_data': use_real_data if 'use_real_data' in locals() else current_use_real_data
        }
        
        # Log any missing variables for debugging
        for var_name, var_value in required_vars.items():
            if var_value is None:
                logger.warning(f"Required variable '{var_name}' is None, using fallback")
        
        # Calculate final estimated time with all parameters
        final_estimated_time = _estimate_hpo_time(
            n_trials=required_vars['n_trials'],
            trial_epochs=required_vars['trial_epochs'],
            n_model_types=len(required_vars['model_types']),
            cv_folds=required_vars['cv_folds'],
            hardware_info=hardware_data,
            system_class=system_class
        )
        
        # Final configuration summary
        print(Fore.MAGENTA + Style.BRIGHT + "\nFINAL EXPRESS HPO CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        print(Fore.YELLOW + Style.BRIGHT + "Configuration Summary:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Intensity: " + Fore.YELLOW + Style.BRIGHT + f"{required_vars['goal_name']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Focus: " + Fore.YELLOW + Style.BRIGHT + f"{required_vars['optimization_focus'].title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Algorithm: " + Fore.YELLOW + Style.BRIGHT + f"{sampler_type or 'TPESampler'} + {pruner_type or 'MedianPruner'}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Models: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(required_vars['model_types'])}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Validation: " + Fore.YELLOW + Style.BRIGHT + f"{cv_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data: " + Fore.YELLOW + Style.BRIGHT + f"{'Real' if required_vars['use_real_data'] else 'Synthetic'}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials: " + Fore.YELLOW + Style.BRIGHT + f"{required_vars['n_trials']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trial Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{required_vars['trial_epochs']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{required_vars['timeout_minutes']} minutes")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Estimated Duration: " + Fore.YELLOW + Style.BRIGHT + f"{final_estimated_time}")
        
        # Resource optimization tips
        print(Fore.CYAN + Style.BRIGHT + "\nResource Optimization Tips:")
        if system_class in ["limited", "standard"]:
            print(Fore.YELLOW + Style.BRIGHT + "  ├─ Consider reducing trial epochs for faster iterations")
            print(Fore.YELLOW + Style.BRIGHT + "  ├─ Use single split validation to save time")
            print(Fore.YELLOW + Style.BRIGHT + "  └─ Focus on fewer model types for limited resources")
        elif system_class in ["performance", "high-end", "enterprise"]:
            print(Fore.GREEN + Style.BRIGHT + "  ├─ System can handle more parallel trials")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Consider increasing CV folds for better robustness")
            print(Fore.GREEN + Style.BRIGHT + "  └─ Can optimize multiple model types simultaneously")
        
        # Build final configuration with smart HPO defaults
        final_config = base_config.copy() if base_config else {}
        
        # If no base_config was provided, use appropriate preset as foundation
        if not base_config:
            if recommended_preset in PRESET_CONFIGS:
                final_config = deepcopy(PRESET_CONFIGS[recommended_preset])
                print(Fore.GREEN + Style.BRIGHT + f"\nUsing {recommended_preset} preset as foundation for express HPO")
            elif 'default' in PRESET_CONFIGS:
                final_config = deepcopy(PRESET_CONFIGS['default'])
                print(Fore.GREEN + Style.BRIGHT + f"\nUsing default preset as foundation for express HPO")
            else:
                logger.warning("No suitable preset available, creating minimal config")
                final_config = {}
        
        # Apply final parameter values using the validated variables
        final_sampler = sampler_type or 'TPESampler'
        final_pruner = pruner_type or 'MedianPruner'
        final_trial_count = required_vars['n_trials']
        final_timeout = required_vars['timeout_seconds']
        final_optimization_focus = required_vars['optimization_focus']
        final_model_types = required_vars['model_types']
        final_cv_folds = required_vars['cv_folds']
        final_trial_epochs = required_vars['trial_epochs']
        final_use_real_data = required_vars['use_real_data']
        
        # Get preset-based search space if custom not provided
        if not custom_search_space:
            preset_hpo_space = PRESET_CONFIGS.get(preset_name.lower(), {}).get('hyperparameter_optimization', {}).get('optimization_space', {})
            if preset_hpo_space:
                custom_search_space = preset_hpo_space
                print(Fore.GREEN + Style.BRIGHT + f"\nUsing preset-compatible search space from {preset_name}")
        
        # Hyperparameter Optimization Configuration
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        
        # Update HPO configuration
        hpo_config.update({
            'enabled': True,
            'n_trials': final_trial_count,
            'timeout': final_timeout,
            'timeout_seconds': final_timeout,
            'strategy': 'optuna',
            'sampler': final_sampler,
            'pruner': final_pruner,
            'optimization_focus': final_optimization_focus,
            'generate_plots': enable_plots,
            'parallel_trials': min(2, cpu_cores // 2) if cpu_cores > 4 else 1,
            'show_progress_bar': True,
            'verbose': True,
            
            # Express-specific settings
            'trial_epochs': final_trial_epochs,
            'trial_patience': max(5, final_trial_epochs // 4),
            
            'storage': {
                'enabled': enable_storage,
                'url': storage_url or f'sqlite:///hpo_express_{int(time.time())}.db'
            },
            
            'optimization_space': custom_search_space or {
                'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 1e-2, 'log': True},
                'batch_size': {'type': 'categorical', 'choices': [32, 64, 128, 256]},
                'encoding_dim': {'type': 'int', 'low': 8, 'high': 64},
                'dropout_rate': {'type': 'float', 'low': 0.1, 'high': 0.5},
                'weight_decay': {'type': 'float', 'low': 1e-6, 'high': 1e-3, 'log': True}
            },
            
            'metrics': {
                'primary': 'validation_loss',
                'secondary': ['reconstruction_error', 'training_time', 'memory_usage'],
                'direction': 'minimize'
            },
            
            'early_stopping': {
                'enabled': True,
                'patience': 10,
                'min_delta': 1e-4
            },
            
            # Model search configuration
            'model_search': {
                'enabled': len(final_model_types) > 1,
                'model_types': final_model_types,
                'search_all_models': len(final_model_types) >= 3
            },
            
            # Cross-validation settings
            'cross_validation': {
                'enabled': final_cv_folds > 1,
                'folds': final_cv_folds,
                'shuffle': True,
                'random_state': 42
            },
            
            # Express setup metadata
            'express_setup': {
                'intensity': required_vars['goal_name'],
                'focus': final_optimization_focus,
                'estimated_duration': final_estimated_time,
                'system_class': system_class,
                'preset_alignment': preset_name
            }
        })
        
        # Model Configuration
        model_config = final_config.setdefault('model', {})
        model_config.update({
            'model_type': model_type,
            'hpo_optimized': True,
            # HPO will optimize these parameters based on preset ranges
            'encoding_dim': model_config.get('encoding_dim', 32),
            'hidden_dims': model_config.get('hidden_dims', [256, 128, 64]),
            'dropout_rates': model_config.get('dropout_rates', [0.2, 0.15, 0.1]),
        })
        
        # Training Configuration
        training_config = final_config.setdefault('training', {})
        training_config.update({
            'epochs': training_config.get('epochs', 100),
            'batch_size': training_config.get('batch_size', 64),
            'learning_rate': training_config.get('learning_rate', 0.001),
            'patience': training_config.get('patience', 15),
            'mixed_precision': training_config.get('mixed_precision', torch.cuda.is_available()),
            'early_stopping': training_config.get('early_stopping', True),
            'validation_split': training_config.get('validation_split', 0.2),
            'weight_decay': training_config.get('weight_decay', 1e-4),
            'hpo_mode': True
        })
        
        # Data Configuration - FIXED: Simplified and reliable
        data_config = final_config.setdefault('data', {})
        data_config.update({
            'use_real_data': final_use_real_data,
            'features': features,
            'normal_samples': normal_samples if not final_use_real_data else None,
            'attack_samples': attack_samples if not final_use_real_data else None,
            'test_split': data_config.get('test_split', 0.2),
            'random_state': data_config.get('random_state', 42),
            'stratified_split': data_config.get('stratified_split', True),
            'hpo_optimized': True
        })
        
        # System Configuration
        system_config = final_config.setdefault('system', {})
        system_config.update({
            'reproducible': system_config.get('reproducible', True),
            'random_seed': system_config.get('random_seed', 42),
            'non_interactive': non_interactive,
            'hpo_optimized': True,
            'system_class': system_class,
            'parallel_processing': system_config.get('parallel_processing', False),
        })
        
        # Monitoring Configuration
        monitoring_config = final_config.setdefault('monitoring', {})
        monitoring_config.update({
            'verbose': monitoring_config.get('verbose', True),
            'tensorboard_logging': monitoring_config.get('tensorboard_logging', True),
            'save_best_model': monitoring_config.get('save_best_model', True),
            'metrics_to_track': monitoring_config.get('metrics_to_track', ['loss', 'validation_loss', 'reconstruction_error', 'learning_rate']),
            'hpo_progress_tracking': True,
            'plot_generation': enable_plots,
        })
        
        # Runtime Configuration
        runtime_config = final_config.setdefault('runtime', {})
        runtime_config.update({
            'config_loaded_at': datetime.now().isoformat(),
            'config_source': 'interactive_hpo_express_setup',
            'runtime_id': f"hpo_express_{int(time.time())}",
            'process_id': os.getpid(),
            'system_analysis_completed': True,
            'system_class': system_class,
            'preset_context': preset_name,
            'hpo_setup': {
                'intensity': required_vars['goal_name'],
                'focus': final_optimization_focus,
                'algorithm': f"{final_sampler}+{final_pruner}",
                'models': final_model_types,
                'validation': f"{final_cv_folds}-fold",
                'data_source': 'real' if final_use_real_data else 'synthetic',
                'trial_epochs': final_trial_epochs,
                'estimated_duration': final_estimated_time
            },
            'resource_status': {
                'gpu_available': torch.cuda.is_available(),
                'memory_gb': memory_gb,
                'cpu_cores': cpu_cores
            }
        })
        
        # Update metadata
        metadata = final_config.setdefault('metadata', {})
        metadata.update({
            'last_modified': datetime.now().isoformat(),
            'setup_method': 'express_hpo',
            'preset_used': preset_name,
            'model_type': model_type,
            'express_configuration': {
                'goal': required_vars['goal_name'].lower().replace(' ', '_'),
                'intensity': required_vars['goal_name'].lower().replace(' ', '_'),
                'focus': final_optimization_focus,
                'data_source': 'real' if final_use_real_data else 'synthetic',
                'model_types': final_model_types,
                'validation_strategy': cv_name.lower().replace(' ', '_'),
                'algorithm': f"{final_sampler}_{final_pruner}".lower(),
                'estimated_duration': final_estimated_time,
                'system_class': system_class,
                'preset_alignment': preset_name.lower()
            }
        })
        
        # Skip confirmation if in non-interactive mode
        if non_interactive or skip_prompt:
            print(Fore.GREEN + Style.BRIGHT + "\nNon-interactive mode - launching express HPO automatically...")
            return _launch_hpo_with_config(config=final_config, **kwargs)
        
        # Final confirmation
        try:
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart hyperparameter optimization with these express settings? (Y/n/c to cancel): " + Style.RESET_ALL).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print(Fore.RED + Style.BRIGHT + "\nHPO configuration cancelled by user")
            return None
        
        if confirm in ('', 'y', 'yes'):
            print(Fore.GREEN + Style.BRIGHT + "\nLaunching hyperparameter optimization with express configuration...")
            return _launch_hpo_with_config(config=final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print(Fore.RED + Style.BRIGHT + "\nHPO cancelled")
            return None
        else:
            # Fallback options
            print(Fore.YELLOW + Style.BRIGHT + "\nWould you like to?")
            print(Fore.WHITE + Style.BRIGHT + "1. Try express HPO setup again with different settings")
            print(Fore.WHITE + Style.BRIGHT + "2. Switch to custom HPO configuration for full control")
            print(Fore.WHITE + Style.BRIGHT + "3. Switch to preset-based HPO configuration")
            print(Fore.RED + Style.BRIGHT + "0. Return to previous menu")
            
            while True:
                try:
                    retry_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-3): " + Style.RESET_ALL).strip()
                    if retry_choice in ['1', '2', '3', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                    return None
            
            if retry_choice == '1':
                print(Fore.GREEN + Style.BRIGHT + "\nRestarting express HPO setup...")
                return _interactive_hpo_express_setup(base_config, data_mode=data_mode, hardware_data=hardware_data, **kwargs)
            elif retry_choice == '2':
                print(Fore.GREEN + Style.BRIGHT + "\nSwitching to custom HPO configuration...")
                return _interactive_hpo_custom_setup(
                    base_config,
                    data_mode=data_mode,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    **kwargs
                )
            elif retry_choice == '3':
                print(Fore.GREEN + Style.BRIGHT + "\nSwitching to preset HPO configuration...")
                return _interactive_hpo_preset_setup(
                    base_config,
                    data_mode=data_mode,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    **kwargs
                )
            else:
                print(Fore.RED + Style.BRIGHT + "\nReturning to previous menu")
                return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\nExpress HPO setup interrupted by user")
        return None
    except Exception as e:
        logger.error(f"Express HPO setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during express HPO setup: {str(e)}\n"
            f"Context:\n"
            f"- Preset: {preset_name}\n"
            f"- Data Mode: {data_mode}\n"
            f"- Trial Count: {trial_count}\n"
            f"- System Class: {system_class}\n"
            f"- CUDA Available: {cuda_available}\n\n"
            f"This could be due to:\n"
            f"- HPO configuration structure issues\n"
            f"- Missing required HPO parameters\n"
            f"- System resource constraints\n"
            f"- Invalid search space definitions\n"
            f"- Preset compatibility issues"
        )
        print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
        print(Fore.RED + Style.BRIGHT + "EXPRESS HPO SETUP ERROR")
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + message)
        print(Fore.RED + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        return None

def _interactive_hpo_preset_setup(
    base_config: Dict[str, Any],
    data_mode: Optional[str] = None,
    hardware_data: Optional[Dict[str, Any]] = None,
    enable_storage: Optional[bool] = None,
    enable_plots: Optional[bool] = None,
    custom_search_space: Optional[Dict[str, Any]] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    trial_count: Optional[int] = None,
    timeout_seconds: Optional[int] = None,
    optimization_focus: Optional[str] = None,
    study_name: Optional[str] = None,
    storage_url: Optional[str] = None,
    non_interactive: bool = False,
    skip_prompt: bool = False,
    operation_mode: Optional[str] = None,
    use_current_config: bool = False,
    force_express: bool = False,
    model_types: Optional[List[str]] = None,
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Interactive HPO preset configuration setup with comprehensive context integration.
    
    Provides a streamlined HPO preset selection experience while maintaining full
    compatibility with the centralized configuration system.
    """
    try:
        # Clear screen and show banner for consistency
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if base_config is None and banner_config is not None:
            base_config = banner_config
        else:
            base_config = base_config or {}
        
        # Get hardware context if not provided
        if hardware_data is None:
            try:
                hardware_data = check_hardware(include_memory_usage=True)
            except Exception as e:
                logger.debug(f"Hardware detection failed: {e}")
                hardware_data = {}
        
        # Extract context for display
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = base_config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
            current_preset = preset_name
        
        # Extract model type
        model_section = base_config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
            current_model_type = model_type
        
        # Extract config source with fallbacks
        if "runtime" in base_config and isinstance(base_config["runtime"], dict):
            config_source = base_config["runtime"].get("config_source", "Unknown")
        elif "metadata" in base_config and isinstance(base_config["metadata"], dict):
            config_source = base_config["metadata"].get("config_source", "Unknown")
        
        # Hardware context extraction
        cuda_available = hardware_data.get('cuda', {}).get('available', False)
        gpu_count = hardware_data.get('cuda', {}).get('gpu_count', 0)
        memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)

        # Determine system performance class
        if cuda_available and memory_gb >= 32 and cpu_cores >= 16:
            system_class = "enterprise"
            recommended_preset = "advanced"
        elif cuda_available and memory_gb >= 16 and cpu_cores >= 8:
            system_class = "high-end"
            recommended_preset = "performance"
        elif cuda_available and memory_gb >= 8:
            system_class = "performance"
            recommended_preset = "performance"
        elif memory_gb >= 8:
            system_class = "standard"
            recommended_preset = "baseline"
        elif memory_gb >= 4:
            system_class = "balanced"
            recommended_preset = "default"
        else:
            system_class = "limited"
            recommended_preset = "lightweight"
        
        # Resolve data_mode from use_real_data parameter if needed
        data_config = base_config.get('data', {})
        use_real_data_config = data_config.get('use_real_data', False)
        if data_mode is None:
            if use_real_data_config is True:
                data_mode = 'real'
            elif use_real_data_config is False:
                data_mode = 'synthetic'
            else:
                data_mode = 'auto'
        
        # Header with context display
        header_title = "HPO PRESET CONFIGURATION SETUP"
        if operation_mode == 'quick_test':
            header_title = "QUICK HPO PRESET SELECTION"
        elif operation_mode == 'model_comparison':
            header_title = "MODEL COMPARISON PRESET SELECTION"
        
        # Clear screen and show context
        print(Fore.MAGENTA + Style.BRIGHT + header_title)
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        print(Fore.YELLOW + Style.BRIGHT + f"Base Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.YELLOW + Style.BRIGHT + f"{current_preset}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Model: " + Fore.YELLOW + Style.BRIGHT + f"{current_model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Data Mode: " + Fore.YELLOW + Style.BRIGHT + f"{data_mode}")
        
        # Display hardware context
        print(Fore.MAGENTA + Style.BRIGHT + "\nHardware Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
        if cuda_available:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ GPU Count: " + Fore.YELLOW + Style.BRIGHT + f"{gpu_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Memory: " + Fore.YELLOW + Style.BRIGHT + f"{memory_gb:.1f}GB")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ CPU Cores: " + Fore.YELLOW + Style.BRIGHT + f"{cpu_cores}")
        
        # Display parameter overrides if provided
        override_params = []
        if trial_count is not None:
            override_params.append(f"Trial Count: {trial_count}")
        if timeout_seconds is not None:
            override_params.append(f"Timeout: {timeout_seconds}s")
        if sampler_type:
            override_params.append(f"Sampler: {sampler_type}")
        if pruner_type:
            override_params.append(f"Pruner: {pruner_type}")
        if optimization_focus:
            override_params.append(f"Optimization Focus: {optimization_focus}")
        if enable_storage is not None:
            override_params.append(f"Storage: {enable_storage}")
        if enable_plots is not None:
            override_params.append(f"Plots: {enable_plots}")
        if custom_search_space:
            override_params.append("Custom Search Space: Provided")
        
        if override_params:
            print(Fore.CYAN + Style.BRIGHT + "\nParameter Overrides:")
            for i, param in enumerate(override_params):
                prefix = "  └─" if i == len(override_params) - 1 else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {param}")
        
        print(Fore.YELLOW + Style.BRIGHT + "\nAvailable HPO Preset Configurations:\n")
        
        # Get available presets and their HPO configurations from PRESET_CONFIGS
        available_presets = []
        hpo_enabled_presets = []
        system_optimized_presets = []
        
        for preset_name, preset_config in PRESET_CONFIGS.items():
            # Validate preset configuration first
            required_sections = ['hyperparameter_optimization', 'training', 'model', 'data']
            missing_sections = []
            
            for section in required_sections:
                if section not in preset_config:
                    missing_sections.append(section)
            
            is_valid = len(missing_sections) == 0
            if not is_valid:
                logger.warning(f"Preset '{preset_name}' is missing sections: {missing_sections}")
                continue
            
            hpo_config = preset_config.get('hyperparameter_optimization', {})
            training_config = preset_config.get('training', {})
            data_config = preset_config.get('data', {})
            model_config = preset_config.get('model', {})
            system_config = preset_config.get('system', {})
            metadata = preset_config.get('metadata', {})
            preset_model_config = preset_config.get('model', {})
            
            # Check system compatibility
            hw_req = metadata.get('recommended_hardware', {})
            preset_ram_gb = hw_req.get('ram_gb', 0)
            preset_cpu_cores = hw_req.get('cpu_cores', 0)
            
            # Determine system compatibility
            system_compatible = True
            if preset_ram_gb > 0 and memory_gb < preset_ram_gb:
                system_compatible = False
            if preset_cpu_cores > 0 and cpu_cores < preset_cpu_cores:
                system_compatible = False
            
            # Model compatibility
            compatibility = metadata.get('compatibility', [])
            model_compatible = not compatibility or current_model_type in compatibility
            
            # System optimization level
            system_optimized = system_compatible and model_compatible

            # Extract current HPO configuration summary
            current_trials = hpo_config.get('n_trials', hpo_config.get('n_trials', 50))
            current_trial_epochs = hpo_config.get('trial_epochs', 30)
            current_timeout = hpo_config.get('timeout_seconds', hpo_config.get('timeout', 3600))
            current_sampler = hpo_config.get('sampler', 'TPESampler')
            current_pruner = hpo_config.get('pruner', 'MedianPruner')
            current_epochs = training_config.get('epochs', 100)
            current_batch_size = training_config.get('batch_size', 64)
            current_use_real_data = data_config.get('use_real_data', False)
            current_noise_factor = data_config.get('synthetic_generation', {}).get('noise_factor', 0.05)
            
            preset_info = {
                'name': preset_name,
                'description': metadata.get('description', f'{preset_name.title()} preset'),
                'enabled': hpo_config.get('enabled', False),
                'n_trials': current_trials,
                'trial_epochs': current_trial_epochs,
                'epochs': current_epochs,
                'batch_size': current_batch_size,
                'timeout': current_timeout,
                'timeout_minutes': current_timeout // 60 if current_timeout else 30,
                'timeout_seconds': current_timeout,
                'strategy': hpo_config.get('strategy', 'optuna'),
                'sampler': current_sampler,
                'pruner': current_pruner,
                'recommended_hardware': hw_req,
                'compatibility': compatibility,
                'preset_data': preset_config,
                'model_type': preset_model_config.get('model_type', 'Unknown'),
                'system_compatible': system_compatible,
                'model_compatible': model_compatible,
                'system_optimized': system_optimized,
                'optimization_focus': metadata.get('optimization_focus', 'balanced'),
                'generate_plots': hpo_config.get('generate_plots', True),
                'storage_enabled': hpo_config.get('storage', {}).get('enabled', True),
                'config_valid': is_valid,
                'missing_sections': missing_sections,
                'use_real_data': current_use_real_data,
                'noise_factor': current_noise_factor
            }
            
            available_presets.append(preset_info)
            if preset_info['enabled']:
                hpo_enabled_presets.append(preset_info)
            if preset_info['system_optimized']:
                system_optimized_presets.append(preset_info)
        
        if not available_presets:
            print(Fore.RED + Style.BRIGHT + "\nNo HPO presets available! Using express HPO setup instead.")
            return _interactive_hpo_express_setup(
                base_config,
                data_mode=data_mode,
                hardware_data=hardware_data,
                enable_storage=enable_storage,
                enable_plots=enable_plots,
                custom_search_space=custom_search_space,
                sampler_type=sampler_type,
                pruner_type=pruner_type,
                trial_count=trial_count,
                timeout_seconds=timeout_seconds,
                optimization_focus=optimization_focus,
                **kwargs
            )
        
        available_presets_count = len(available_presets)
        hpo_enabled_presets_count = len(hpo_enabled_presets)
        system_optimized_count = len(system_optimized_presets)
        
        # Preset display
        print(Fore.YELLOW + Style.BRIGHT + f"\nPreset Availability Summary:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Presets: " + Fore.YELLOW + Style.BRIGHT + f"{available_presets_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ HPO-enabled Presets: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_enabled_presets_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System-optimized Presets: " + Fore.YELLOW + Style.BRIGHT + f"{system_optimized_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Current System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Display presets with validation status
        for i, preset in enumerate(available_presets, 1):
            # Color coding based on multiple factors including validation
            if preset['system_optimized'] and preset['enabled'] and preset['config_valid']:
                status_icon = Fore.GREEN + Style.BRIGHT + "✓"
            elif preset['system_optimized'] and preset['config_valid']:
                status_icon = Fore.YELLOW + Style.BRIGHT + "⚠"
            else:
                status_icon = Fore.RED + Style.BRIGHT + "✗"
            
            # Validation status
            validation_status = Fore.GREEN + Style.BRIGHT + "VALID" if preset['config_valid'] else Fore.RED + Style.BRIGHT + "INVALID"
            # Compatibility status
            compatibility_status = Fore.GREEN + Style.BRIGHT + "PASS" if preset['model_compatible'] else Fore.RED + Style.BRIGHT + "FAIL"
            # All models compatibility
            model_compatibility = Fore.YELLOW + Style.BRIGHT + "All models" if not preset['compatibility'] else Fore.YELLOW + Style.BRIGHT + ", ".join(preset['compatibility'])
            # HPO status
            hpo_status = Fore.GREEN + Style.BRIGHT + "ENABLED" if preset['enabled'] else Fore.RED + Style.BRIGHT + "DISABLED"
            # System optimization status
            system_status = Fore.GREEN + Style.BRIGHT + "OPTIMIZED" if preset['system_optimized'] else Fore.RED + Style.BRIGHT + "NOT OPTIMIZED"

            header_color = Fore.WHITE + Style.BRIGHT
            title_color = Fore.CYAN + Style.BRIGHT
            
            print(f"{header_color}{i}. {preset['name'].upper()} {status_icon}{Style.RESET_ALL}")
            print(f"   {title_color}Description: {preset['description']}{Style.RESET_ALL}")
            print(f"   {title_color}Config: {validation_status}{title_color} | HPO: {hpo_status}{title_color} | System: {system_status}{Style.RESET_ALL}")
            print(f"   {title_color}Trials: {preset['n_trials']} | Timeout: {preset['timeout_minutes']} minutes{Style.RESET_ALL}")
            print(f"   {title_color}Strategy: {preset['strategy']} | Sampler: {preset['sampler']} | Pruner: {preset['pruner']}{Style.RESET_ALL}")
            print(f"   {title_color}Model Type: {preset['model_type']}{Style.RESET_ALL}")
            print(f"   {title_color}Focus: {preset['optimization_focus'].title()}{Style.RESET_ALL}")
            print(f"   {title_color}Compatibility: {compatibility_status} {model_compatibility}{Style.RESET_ALL}")
            
            # Show validation issues if any
            if not preset['config_valid']:
                print(f"   {Fore.RED + Style.BRIGHT}Missing sections: {', '.join(preset['missing_sections'])}{Style.RESET_ALL}")
            
            # Hardware requirements with color coding
            hw_req = preset['recommended_hardware']
            if hw_req:
                hw_color = Fore.GREEN if hw_req.get('ram_gb', 0) <= memory_gb else Fore.RED
                cpu_color = Fore.GREEN if hw_req.get('cpu_cores', 0) <= cpu_cores else Fore.RED
                print(f"   {hw_color}Hardware: {hw_req.get('cpu_cores', 'N/A')} cores, "
                      f"{hw_req.get('ram_gb', 'N/A')}GB RAM{Style.RESET_ALL}")
            print()
        
        # Navigation options
        print(Fore.YELLOW + Style.BRIGHT + "\nNavigation Options:")
        print(Fore.WHITE + Style.BRIGHT + f"{len(available_presets)+1}. " + Fore.GREEN + Style.BRIGHT + "Switch to Express HPO Setup")
        print(Fore.WHITE + Style.BRIGHT + f"{len(available_presets)+2}. " + Fore.GREEN + Style.BRIGHT + "Switch to Custom HPO Configuration")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        # Handle non-interactive mode
        if non_interactive:
            print(Fore.GREEN + Style.BRIGHT + f"\nNon-interactive Mode - Using current preset: {current_preset}")
            selected_preset_name = current_preset
        else:
            # Preset selection with robust error handling
            while True:
                try:
                    choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect HPO preset (1-{len(available_presets)}) or 0 to cancel: " + Style.RESET_ALL).strip()
                    
                    if not choice:
                        continue
                        
                    choice_num = int(choice)
                    
                    if 1 <= choice_num <= len(available_presets):
                        selected_preset = available_presets[choice_num-1]
                        selected_preset_name = selected_preset['name']
                        
                        # Validate selected preset configuration
                        if not selected_preset['config_valid']:
                            print(Fore.RED + Style.BRIGHT + f"\nWarning: Selected preset '{selected_preset_name}' has configuration issues!")
                            print(Fore.YELLOW + Style.BRIGHT + f"Missing sections: {', '.join(selected_preset['missing_sections'])}")
                            confirm_invalid = input(Fore.YELLOW + Style.BRIGHT + "Continue anyway? (y/N): " + Style.RESET_ALL).strip().lower()
                            if confirm_invalid not in ('y', 'yes'):
                                continue
                        break
                    elif choice_num == 0:
                        print(Fore.RED + Style.BRIGHT + "\nHPO preset selection cancelled")
                        return None
                    elif choice_num == len(available_presets) + 1:
                        print(Fore.GREEN + Style.BRIGHT + "\nSwitching to express HPO setup...")
                        return _interactive_hpo_express_setup(
                            base_config,
                            data_mode=data_mode,
                            hardware_data=hardware_data,
                            enable_storage=enable_storage,
                            enable_plots=enable_plots,
                            custom_search_space=custom_search_space,
                            sampler_type=sampler_type,
                            pruner_type=pruner_type,
                            trial_count=trial_count,
                            timeout_seconds=timeout_seconds,
                            optimization_focus=optimization_focus,
                            **kwargs
                        )
                    elif choice_num == len(available_presets) + 2:
                        print(Fore.GREEN + Style.BRIGHT + "\nSwitching to custom HPO configuration...")
                        return _interactive_hpo_custom_setup(
                            base_config,
                            data_mode=data_mode,
                            hardware_data=hardware_data,
                            enable_storage=enable_storage,
                            enable_plots=enable_plots,
                            custom_search_space=custom_search_space,
                            sampler_type=sampler_type,
                            pruner_type=pruner_type,
                            trial_count=trial_count,
                            timeout_seconds=timeout_seconds,
                            optimization_focus=optimization_focus,
                            **kwargs
                        )
                    else:
                        print(Fore.RED + Style.BRIGHT + f"\nInvalid choice. Please select 1-{len(available_presets)+2} or 0 to cancel.")
                        
                except ValueError:
                    print(Fore.RED + Style.BRIGHT + f"\nInvalid input. Please enter a number 1-{len(available_presets)+2} or 0 to cancel.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nHPO preset selection cancelled")
                    return None
        
        # Selection confirmation
        selected_preset_info = next((p for p in available_presets if p['name'] == selected_preset_name), None)
        if not selected_preset_info:
            print(Fore.RED + Style.BRIGHT + f"\nPreset '{selected_preset_name}' not found!")
            return None
        
        print(Fore.GREEN + Style.BRIGHT + f"\nSelected preset: " + Fore.YELLOW + Style.BRIGHT + f"{selected_preset_name.upper()}")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Load the full preset configuration from PRESET_CONFIGS with validation
        preset_config = PRESET_CONFIGS[selected_preset_name]
        
        # Validate preset configuration
        required_sections = ['hyperparameter_optimization', 'training', 'model', 'data']
        missing_sections = []
        
        for section in required_sections:
            if section not in preset_config:
                missing_sections.append(section)
        
        is_valid = len(missing_sections) == 0
        
        if not is_valid:
            print(Fore.RED + Style.BRIGHT + f"\nError: Preset '{selected_preset_name}' has invalid configuration!")
            print(Fore.YELLOW + Style.BRIGHT + f"Missing sections: {', '.join(missing_sections)}")
            print(Fore.RED + Style.BRIGHT + "Cannot proceed with invalid preset configuration.")
            return None
        
        # Use safe merge instead of full deep_update to preserve preset configuration
        if not base_config:
            final_config = deepcopy(preset_config)
        else:
            # Create a deep copy of the preset config
            final_config = deepcopy(preset_config)
            
            # Only merge safe sections that shouldn't override preset configuration
            safe_sections = ['metadata', 'runtime', 'system', 'presets']
            
            for section in safe_sections:
                if section in base_config and base_config[section]:
                    if section not in final_config:
                        final_config[section] = {}
                    final_config[section] = deep_update(
                        final_config[section],
                        base_config[section]
                    )
        
        # Apply parameter overrides to the configuration
        if final_config.get('hyperparameter_optimization'):
            hpo_section = final_config['hyperparameter_optimization']
            
            # Apply overrides with validation
            overrides_applied = []
            
            if trial_count is not None and trial_count != hpo_section.get('n_trials'):
                hpo_section['n_trials'] = trial_count
                overrides_applied.append(f"Trial count: {trial_count}")
            
            if timeout_seconds is not None and timeout_seconds != hpo_section.get('timeout'):
                hpo_section['timeout'] = timeout_seconds
                hpo_section['timeout_seconds'] = timeout_seconds
                overrides_applied.append(f"Timeout: {timeout_seconds}s")
            
            if sampler_type and sampler_type != hpo_section.get('sampler'):
                hpo_section['sampler'] = sampler_type
                overrides_applied.append(f"Sampler: {sampler_type}")
            
            if pruner_type and pruner_type != hpo_section.get('pruner'):
                hpo_section['pruner'] = pruner_type
                overrides_applied.append(f"Pruner: {pruner_type}")
            
            if enable_storage is not None:
                storage_config = hpo_section.get('storage', {})
                storage_config['enabled'] = enable_storage
                hpo_section['storage'] = storage_config
                overrides_applied.append(f"Storage: {enable_storage}")
            
            if enable_plots is not None:
                hpo_section['generate_plots'] = enable_plots
                overrides_applied.append(f"Plots: {enable_plots}")
            
            if custom_search_space:
                hpo_section['optimization_space'] = custom_search_space
                overrides_applied.append("Custom search space parameters")
            
            if optimization_focus:
                # Update optimization focus in metadata
                metadata_section = final_config.setdefault('metadata', {})
                metadata_section['optimization_focus'] = optimization_focus
                overrides_applied.append(f"Optimization focus: {optimization_focus}")
            
            if overrides_applied:
                print(Fore.CYAN + Style.BRIGHT + "\nApplied Parameter Overrides:")
                for i, override in enumerate(overrides_applied):
                    prefix = "  └─" if i == len(overrides_applied) - 1 else "  ├─"
                    print(Fore.GREEN + Style.BRIGHT + f"{prefix} {override}")
        
        # Complete data source configuration
        if data_mode and data_mode != 'auto':
            data_config = final_config.setdefault('data', {})
            preset_data_config = selected_preset_info['preset_data'].get('data', {})
            
            if data_mode == 'real':
                # Configure complete real data setup
                data_config.update({
                    'use_real_data': True,
                    'data_path': preset_data_config.get('data_path', 'data/network_data.csv'),
                    'artifacts_path': preset_data_config.get('artifacts_path', 'data/artifacts.pkl'),
                    'features': preset_data_config.get('features', 20),
                    'data_format': preset_data_config.get('data_format', 'csv'),
                    'validation_split': preset_data_config.get('validation_split', 0.2),
                    'test_split': preset_data_config.get('test_split', 0.1)
                })
            else:  # synthetic
                # Configure complete synthetic data setup
                synthetic_defaults = {
                    'use_real_data': False,
                    'normal_samples': preset_data_config.get('normal_samples', 8000),
                    'attack_samples': preset_data_config.get('attack_samples', 2000),
                    'features': preset_data_config.get('features', 20),
                    'validation_split': preset_data_config.get('validation_split', 0.2),
                    'test_split': preset_data_config.get('test_split', 0.1)
                }
                
                # Add synthetic generation configuration
                synthetic_gen_defaults = {
                    'noise_factor': preset_data_config.get('synthetic_generation', {}).get('noise_factor', 0.05),
                    'correlation_strength': preset_data_config.get('synthetic_generation', {}).get('correlation_strength', 0.3),
                    'anomaly_ratio': preset_data_config.get('synthetic_generation', {}).get('anomaly_ratio', 0.1)
                }
                
                data_config.update(synthetic_defaults)
                data_config.setdefault('synthetic_generation', {}).update(synthetic_gen_defaults)
        
        # Ensure HPO is enabled and configure it properly
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        hpo_config['enabled'] = True
        
        # Update study name to include preset and timestamp
        original_study_name = hpo_config.get('study_name', 'autoencoder_hpo')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        hpo_config['study_name'] = f"{selected_preset_name}_{original_study_name}_{timestamp}"
        
        # Update storage configuration with preset-specific path
        if 'storage' in hpo_config:
            storage_config = hpo_config['storage']
            if 'url' in storage_config:
                # Update storage URL to include preset name
                original_url = storage_config['url']
                storage_config['url'] = original_url.replace('study.db', f'{selected_preset_name}_study.db')
        
        # Update metadata to reflect preset usage and HPO setup
        metadata = final_config.setdefault('metadata', {})
        metadata.update({
            'last_modified': datetime.now().isoformat(),
            'setup_method': 'preset_hpo',
            'hpo_preset_used': selected_preset_name,
            'hpo_enabled_at': datetime.now().isoformat(),
            'original_hpo_enabled': selected_preset_info['enabled'],
            'preset_compatibility': selected_preset_info['compatibility'],
            'preset_hardware_recommendations': selected_preset_info['recommended_hardware'],
            'system_class_used': system_class,
            'optimization_focus': selected_preset_info['optimization_focus'],
            'config_source': f'preset_{selected_preset_name}',
            'preset_config_valid': is_valid,
            'base_config_merged': bool(base_config)
        })
        
        # Update presets section
        presets_section = final_config.setdefault('presets', {})
        presets_section['current_preset'] = selected_preset_name
        presets_section['previous_preset'] = current_preset
        
        # Configuration summary
        print(Fore.MAGENTA + Style.BRIGHT + f"\nHPO PRESET CONFIGURATION SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{selected_preset_name.upper()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Description: " + Fore.YELLOW + Style.BRIGHT + f"{selected_preset_info['description']}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data Source: " + Fore.YELLOW + Style.BRIGHT + f"{data_mode}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ HPO Status: " + Fore.YELLOW + Style.BRIGHT + f"{'Enabled (default)' if selected_preset_info['enabled'] else 'Enabled (activated)'}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_config.get('n_trials', 50)}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_config.get('timeout', 3600) // 60 if hpo_config.get('timeout') else 30} minutes")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Strategy: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_config.get('strategy', 'optuna')}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_config.get('sampler', 'TPESampler')}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_config.get('pruner', 'MedianPruner')}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Compatibility: " + Fore.YELLOW + Style.BRIGHT + f"{'OPTIMAL' if selected_preset_info['system_optimized'] else 'SUBOPTIMAL'}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Validation: " + Fore.YELLOW + Style.BRIGHT + f"{'PASS' if is_valid else 'FAIL'}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Study Name: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_config['study_name']}")

        # Show hardware recommendations
        hw_req = selected_preset_info['recommended_hardware']
        if hw_req:
            hw_color = Fore.GREEN if hw_req.get('ram_gb', 0) <= memory_gb else Fore.RED
            cpu_color = Fore.GREEN if hw_req.get('cpu_cores', 0) <= cpu_cores else Fore.RED
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Hardware Compatibility:")
            if hw_req.get('gpu_memory_gb'):
                print(Fore.GREEN + Style.BRIGHT + f"      ├─ GPU Memory: " + Fore.YELLOW + Style.BRIGHT + f"{hw_req.get('gpu_memory_gb', 'N/A')}GB")
            print(Fore.GREEN + Style.BRIGHT + f"      ├─ CPU Cores: " + cpu_color + Style.BRIGHT + f"{hw_req.get('cpu_cores', 'N/A')} (available: {cpu_cores})")
            print(Fore.GREEN + Style.BRIGHT + f"      └─ RAM: " + hw_color + Style.BRIGHT + f"{hw_req.get('ram_gb', 'N/A')}GB (available: {memory_gb:.1f}GB)")
        
        # Handle skip_prompt for non-interactive flows
        if skip_prompt or non_interactive:
            print(Fore.GREEN + Style.BRIGHT + f"\nAuto-launching HPO with {selected_preset_name} preset...")
            
            # Final validation before launch
            required_sections_final = ['hyperparameter_optimization', 'training', 'model', 'data']
            final_missing = []
            
            for section in required_sections_final:
                if section not in final_config:
                    final_missing.append(section)
            
            is_final_valid = len(final_missing) == 0
            
            if not is_final_valid:
                print(Fore.RED + Style.BRIGHT + f"\nError: Final configuration is invalid!")
                print(Fore.YELLOW + Style.BRIGHT + f"Missing sections: {', '.join(final_missing)}")
                return None
            
            # Ensure system configuration is HPO-friendly
            system_config = final_config.setdefault('system', {})
            system_config.update({
                'non_interactive': non_interactive,
                'verbose': True,
                'hpo_optimized': True,
                'system_class': system_class
            })
            
            # Ensure training configuration is optimized for HPO
            training_config = final_config.setdefault('training', {})
            training_config.update({
                'num_workers': 0,  # Safer for HPO
                'pin_memory': False,  # Disabled for HPO stability
                'persistent_workers': False,
                'hpo_mode': True
            })
            
            # Update runtime information
            runtime_config = final_config.setdefault('runtime', {})
            runtime_config.update({
                'setup_method': 'preset_hpo',
                'active_preset': selected_preset_name,
                'hpo_launch_time': datetime.now().isoformat(),
                'hardware_context': {
                    'system_class': system_class,
                    'cuda_available': cuda_available,
                    'memory_gb': memory_gb,
                    'cpu_cores': cpu_cores
                },
                'config_validation': {
                    'preset_valid': is_valid,
                    'final_valid': is_final_valid,
                    'base_config_used': bool(base_config)
                }
            })
            
            return _launch_hpo_with_config(config=final_config, **kwargs)
        
        # CUSTOMIZATION OPTIONS
        print(Fore.YELLOW + Style.BRIGHT + f"\nCustomization Options:")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + "1. Use preset as-is " + Fore.GREEN + Style.BRIGHT + "(Recommended)")
        print(Fore.WHITE + Style.BRIGHT + "2. Customize data source only " + Fore.GREEN + Style.BRIGHT + "(keep preset, change data)")
        print(Fore.WHITE + Style.BRIGHT + "3. Adjust trial count " + Fore.GREEN + Style.BRIGHT + f"(Current: {hpo_config.get('n_trials', 50)})")
        print(Fore.WHITE + Style.BRIGHT + "4. Adjust timeout " + Fore.GREEN + Style.BRIGHT + f"(Current: {hpo_config.get('timeout', 3600) // 60 if hpo_config.get('timeout') else 60} min)")
        print(Fore.WHITE + Style.BRIGHT + "5. Modify sampler " + Fore.GREEN + Style.BRIGHT + f"(Current: {hpo_config.get('sampler', 'TPESampler')})")
        print(Fore.WHITE + Style.BRIGHT + "6. Modify pruner " + Fore.GREEN + Style.BRIGHT + f"(Current: {hpo_config.get('pruner', 'MedianPruner')})")
        print(Fore.WHITE + Style.BRIGHT + "7. Advanced custom setup " + Fore.GREEN + Style.BRIGHT + "(Full Control)")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        custom_choice = None
        while not custom_choice:
            try:
                custom_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect customization option (0-7): " + Style.RESET_ALL).strip()
                
                if not custom_choice:
                    continue
                    
                if custom_choice not in ['1', '2', '3', '4', '5', '6', '7', '0']:
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-7.")
                    custom_choice = None
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nCustomization selection cancelled")
                return None
        
        if custom_choice == '0':
            print(Fore.RED + Style.BRIGHT + "\nHPO preset selection cancelled")
            return None
        
        # Data source customization
        if custom_choice in ['2', '3', '4', '5', '6', '7'] or data_mode is None:
            if data_mode is None:
                print(Fore.MAGENTA + Style.BRIGHT + "\nDATA SOURCE SELECTION")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                print(Fore.WHITE + Style.BRIGHT + "1. Real network data " + Fore.GREEN + Style.BRIGHT + "(production use)")
                print(Fore.WHITE + Style.BRIGHT + "2. Synthetic data " + Fore.GREEN + Style.BRIGHT + "(testing/development)")
                print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
                
                while True:
                    try:
                        data_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect data source (0-2): " + Style.RESET_ALL).strip()
                        if data_choice in ['1', '2', '0']:
                            break
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                        return None
                
                if data_choice == '0':
                    print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                    return None
                    
                data_mode = 'real' if data_choice == '1' else 'synthetic'
                print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {'Real Data' if data_mode == 'real' else 'Synthetic Data'}")
                
                # Data configuration based on selection
                if data_mode == 'real':
                    use_real_data = True
                    final_config.setdefault('data', {})['use_real_data'] = True
                    print(Fore.MAGENTA + Style.BRIGHT + "\nREAL DATA CONFIGURATION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    # Data file path configuration
                    default_data_path = final_config.get('data', {}).get('data_path', 'data/network_data.csv')
                    print(Fore.YELLOW + Style.BRIGHT + f"Selected: Real Network Data")
                    print(Fore.CYAN + Style.BRIGHT + "  └─ Using actual network traffic data for realistic optimization")
                    print(Fore.YELLOW + Style.BRIGHT + f"\nData file path (default): " + Fore.GREEN + Style.BRIGHT + f"{default_data_path}")
                    print(Fore.CYAN + Style.BRIGHT + "  ├─ Path to your network traffic data file")
                    print(Fore.CYAN + Style.BRIGHT + "  └─ Leave empty to use default or provide custom path")
                    
                    user_data_path = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter data file path or 0 to cancel: " + Style.RESET_ALL).strip()

                    if user_data_path == '0':
                        return None

                    data_path = user_data_path if user_data_path else default_data_path
                    final_config.setdefault('data', {})['data_path'] = data_path
                    
                    # Validate path exists
                    if not Path(data_path).exists():
                        print(Fore.YELLOW + Style.BRIGHT + f"\nWarning: Path '{data_path}' does not exist.")
                        create_confirm = input(Fore.YELLOW + Style.BRIGHT + "\nContinue anyway? (y/N): " + Style.RESET_ALL).strip().lower()
                        if create_confirm not in ('y', 'yes'):
                            data_path = default_data_path
                            Path(data_path).parent.mkdir(parents=True, exist_ok=True)
                            final_config.setdefault('data', {})['data_path'] = data_path
                            print(Fore.GREEN + Style.BRIGHT + f"\nUsing default data path: {data_path}")

                    # Artifacts file path configuration
                    default_artifacts_path = final_config.get('data', {}).get('artifacts_path', 'data/artifacts.pkl')
                    print(Fore.YELLOW + Style.BRIGHT + f"\nArtifacts path (default): " + Fore.GREEN + Style.BRIGHT + f"{default_artifacts_path}")
                    print(Fore.CYAN + Style.BRIGHT + "  ├─ Path to your preprocessing artifacts data file")
                    print(Fore.CYAN + Style.BRIGHT + "  ├─ Directory for storing preprocessing artifacts")
                    print(Fore.CYAN + Style.BRIGHT + "  └─ Leave empty to use default or provide custom path")
                    
                    user_artifacts_path = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter artifacts file path or 0 to cancel: " + Style.RESET_ALL).strip()

                    if user_artifacts_path == '0':
                        return None
                    
                    artifacts_path = user_artifacts_path if user_artifacts_path else default_artifacts_path
                    final_config.setdefault('data', {})['artifacts_path'] = artifacts_path
                    
                    # Validate artifacts path exists
                    if not Path(artifacts_path).exists():
                        print(Fore.YELLOW + Style.BRIGHT + f"\nWarning: Path '{artifacts_path}' does not exist.")
                        create_confirm = input(Fore.YELLOW + Style.BRIGHT + "\nContinue anyway? (y/N): " + Style.RESET_ALL).strip().lower()
                        if create_confirm not in ('y', 'yes'):
                            artifacts_path = default_data_path
                            Path(artifacts_path).parent.mkdir(parents=True, exist_ok=True)
                            final_config.setdefault('data', {})['artifacts_path'] = artifacts_path
                            print(Fore.GREEN + Style.BRIGHT + f"\nUsing default artifacts path: {artifacts_path}")
                    
                    # Feature configuration
                    preset_features = final_config.get('data', {}).get('features', 20)
                    print(Fore.YELLOW + Style.BRIGHT + f"\nNumber of features (default): " + Fore.GREEN + Style.BRIGHT + f"{preset_features}")
                    print(Fore.CYAN + Style.BRIGHT + "  ├─ Number of input features or 'auto' to detect")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Preset recommends {preset_features} features")

                    # Use preset features or allow customization
                    user_features = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of features or 'c' to cancel: " + Style.RESET_ALL).strip()

                    if user_features == 'c':
                        return None
                    
                    if user_features and user_features.lower() != 'auto':
                        try:
                            features = int(user_features)
                            final_config.setdefault('data', {})['features'] = features
                            print(Fore.GREEN + Style.BRIGHT + f"\nUsing {features} features")
                        except ValueError:
                            features = preset_features
                            final_config.setdefault('data', {})['features'] = features
                            print(Fore.YELLOW + Style.BRIGHT + "\nInvalid input, using preset recommended features")
                    else:
                        features = preset_features if user_features != 'auto' else 'auto'
                        final_config.setdefault('data', {})['features'] = features
                        print(Fore.GREEN + Style.BRIGHT + f"\nUsing {'auto feature detection' if features == 'auto' else 'preset recommended features'}")

                    print(Fore.GREEN + Style.BRIGHT + f"\nReal network data configured:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data file path: " + Fore.YELLOW + Style.BRIGHT + f"{data_path}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Artifacts path: " + Fore.YELLOW + Style.BRIGHT + f"{artifacts_path}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
                    
                else:  # Synthetic data
                    use_real_data = False
                    final_config.setdefault('data', {})['use_real_data'] = False
                    print(Fore.MAGENTA + Style.BRIGHT + "\nSYNTHETIC DATA CONFIGURATION")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    # Use preset synthetic data configuration as baseline
                    preset_normal_samples = final_config.get('data', {}).get('normal_samples', 8000)
                    preset_attack_samples = final_config.get('data', {}).get('attack_samples', 2000)
                    preset_features = final_config.get('data', {}).get('features', 20)
                    preset_noise_factor = final_config.get('data', {}).get('synthetic_generation', {}).get('noise_factor', 0.05)
                    
                    print(Fore.GREEN + Style.BRIGHT + f"Selected: Synthetic Data")
                    print(Fore.CYAN + Style.BRIGHT + "  ├─ Using generated data for faster iteration cycles")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Preset defaults: {preset_normal_samples} normal, {preset_attack_samples} attack samples, {preset_features} features")
                    
                    # Data complexity level
                    print(Fore.YELLOW + Style.BRIGHT + "\nData complexity level:")
                    print(Fore.CYAN + Style.BRIGHT + "  └─ Controls the realism and complexity of generated data")
                    print(Fore.WHITE + Style.BRIGHT + "1. Simple " + Fore.GREEN + Style.BRIGHT + "(Basic patterns, fast generation)")
                    print(Fore.WHITE + Style.BRIGHT + "2. Moderate " + Fore.GREEN + Style.BRIGHT + "(Balanced complexity - preset default)")
                    print(Fore.WHITE + Style.BRIGHT + "3. Complex " + Fore.GREEN + Style.BRIGHT + "(Realistic patterns, slower generation)")
                    print(Fore.WHITE + Style.BRIGHT + "4. Custom " + Fore.GREEN + Style.BRIGHT + "(Configure all parameters manually)")
                    print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
                    
                    complexity_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect complexity level (0-4): " + Style.RESET_ALL).strip()

                    if complexity_choice == '0':
                        return None
                    
                    if complexity_choice == '4':
                        # Custom synthetic data configuration
                        print(Fore.MAGENTA + Style.BRIGHT + "\nCUSTOM SYNTHETIC DATA CONFIGURATION")
                        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                        
                        # Normal samples configuration
                        print(Fore.CYAN + Style.BRIGHT + "Number of normal samples to generate:")
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{preset_normal_samples}")
                        user_normal = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of normal samples or 'c' to cancel: " + Style.RESET_ALL).strip()

                        if user_normal == 'c':
                            return None

                        normal_samples = int(user_normal) if user_normal else preset_normal_samples
                        final_config.setdefault('data', {})['normal_samples'] = normal_samples
                        
                        # Attack samples configuration
                        print(Fore.CYAN + Style.BRIGHT + "\nNumber of attack samples to generate:")
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{preset_attack_samples}")
                        user_attack = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of attack samples or 'c' to cancel: " + Style.RESET_ALL).strip()

                        if user_attack == 'c':
                            return None

                        attack_samples = int(user_attack) if user_attack else preset_attack_samples
                        final_config.setdefault('data', {})['attack_samples'] = attack_samples
                        
                        # Features configuration
                        print(Fore.CYAN + Style.BRIGHT + "\nNumber of features to generate:")
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{preset_features}")
                        user_features = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of features or 'c' to cancel: " + Style.RESET_ALL).strip()

                        if user_features == 'c':
                            return None

                        features = int(user_features) if user_features else preset_features
                        final_config.setdefault('data', {})['features'] = features
                        
                        # Noise level configuration
                        print(Fore.CYAN + Style.BRIGHT + "\nNoise level for data generation (0.0-1.0):")
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{preset_noise_factor}")
                        user_noise = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter noise level or 'c' to cancel: " + Style.RESET_ALL).strip()

                        if user_noise == 'c':
                            return None

                        noise_factor = float(user_noise) if user_noise else preset_noise_factor
                        final_config.setdefault('data', {}).setdefault('synthetic_generation', {})['noise_factor'] = noise_factor
                        
                        print(Fore.GREEN + Style.BRIGHT + f"\nCustom synthetic data configured:")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Normal samples: " + Fore.YELLOW + Style.BRIGHT + f"{normal_samples}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Attack samples: " + Fore.YELLOW + Style.BRIGHT + f"{attack_samples}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
                        print(Fore.GREEN + Style.BRIGHT + f"  └─ Noise Factor: " + Fore.YELLOW + Style.BRIGHT + f"{noise_factor}")
                        
                    else:
                        # Predefined complexity levels
                        complexity_configs = {
                            '1': {
                                'normal_samples': max(1000, preset_normal_samples // 2),
                                'attack_samples': max(200, preset_attack_samples // 2),
                                'features': max(10, preset_features // 2),
                                'noise_factor': preset_noise_factor * 0.5,
                                'name': 'Simple'
                            },
                            '2': {
                                'normal_samples': preset_normal_samples,
                                'attack_samples': preset_attack_samples,
                                'features': preset_features,
                                'noise_factor': preset_noise_factor,
                                'name': 'Moderate'
                            },
                            '3': {
                                'normal_samples': preset_normal_samples * 2,
                                'attack_samples': preset_attack_samples * 2,
                                'features': min(100, preset_features * 2),
                                'noise_factor': preset_noise_factor * 1.5,
                                'name': 'Complex'
                            }
                        }
                        
                        config = complexity_configs.get(complexity_choice, complexity_configs['2'])
                        normal_samples = config['normal_samples']
                        attack_samples = config['attack_samples']
                        features = config['features']
                        noise_factor = config['noise_factor']
                        config_name = config['name']

                        final_config.setdefault('data', {})['normal_samples'] = normal_samples
                        final_config.setdefault('data', {})['attack_samples'] = attack_samples
                        final_config.setdefault('data', {})['features'] = features
                        final_config.setdefault('data', {}).setdefault('synthetic_generation', {})['noise_factor'] = noise_factor
                        
                        print(Fore.GREEN + Style.BRIGHT + f"\nCustom synthetic data configured:")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config name: " + Fore.YELLOW + Style.BRIGHT + f"{config_name}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Normal samples: " + Fore.YELLOW + Style.BRIGHT + f"{normal_samples}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Attack samples: " + Fore.YELLOW + Style.BRIGHT + f"{attack_samples}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
                        print(Fore.GREEN + Style.BRIGHT + f"  └─ Noise level: " + Fore.YELLOW + Style.BRIGHT + f"{noise_factor}")
            
            final_config.setdefault('data', {})['use_real_data'] = data_mode == 'real'
        
        # Trial count adjustment
        if custom_choice in ['3', '4', '5', '6', '7']:
            print(Fore.MAGENTA + Style.BRIGHT + "\nTRIAL COUNT ADJUSTMENT")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            current_trials = hpo_config.get('n_trials', 50)
            
            print(Fore.CYAN + Style.BRIGHT + f"Current: " + Fore.GREEN + Style.BRIGHT + f"{current_trials} trials")
            print(Fore.CYAN + Style.BRIGHT + "Quick: 20 trials | Standard: 50 trials | Thorough: 100+ trials")
            
            try:
                new_trials = input(Fore.YELLOW + Style.BRIGHT + f"\nNew trial count (Enter for {current_trials}, 'c' to cancel): " + Style.RESET_ALL).strip()
                
                if new_trials.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nTrial count adjustment cancelled")
                    return None
                elif new_trials:
                    new_trials_int = int(new_trials)
                    if new_trials_int > 0:
                        final_config.setdefault('hyperparameter_optimization', {})['n_trials'] = new_trials_int
                        print(Fore.GREEN + Style.BRIGHT + f"\nUpdated: {new_trials_int} trials")
                    else:
                        final_config.setdefault('hyperparameter_optimization', {})['n_trials'] = current_trials
                        print(Fore.RED + Style.BRIGHT + "\nInvalid trial count. Must be positive.")
                        print(Fore.GREEN + Style.BRIGHT + f"\nKeeping: {current_trials} trials")
                else:
                    final_config.setdefault('hyperparameter_optimization', {})['n_trials'] = current_trials
                    print(Fore.GREEN + Style.BRIGHT + f"\nKeeping: {current_trials} trials")
                    
            except ValueError:
                final_config.setdefault('hyperparameter_optimization', {})['n_trials'] = current_trials
                print(Fore.RED + Style.BRIGHT + f"\nInvalid input, keeping {current_trials} trials")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nTrial count adjustment cancelled")
                return None
        
        # Timeout adjustment
        if custom_choice in ['4', '5', '6', '7']:
            print(Fore.MAGENTA + Style.BRIGHT + "\nTIMEOUT ADJUSTMENT")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            current_timeout = hpo_config.get('timeout_seconds', hpo_config.get('timeout', 3600))
            current_timeout_minutes = current_timeout // 60 if current_timeout else 30
            
            print(Fore.CYAN + Style.BRIGHT + f"Current: " + Fore.GREEN + Style.BRIGHT + f"{current_timeout_minutes} minutes")
            print(Fore.CYAN + Style.BRIGHT + "Quick: 30 min | Standard: 60 min | Thorough: 120+ min")
            
            try:
                new_timeout_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter timeout in minutes (current: {current_timeout_minutes}, recommended 30-480): ").strip()
                
                if not new_timeout_input:
                    final_config.setdefault('hyperparameter_optimization', {})['timeout'] = current_timeout_minutes * 60
                    final_config.setdefault('hyperparameter_optimization', {})['timeout_seconds'] = current_timeout_minutes * 60
                    print(Fore.GREEN + Style.BRIGHT + "\nKeeping current timeout.")
                else:
                    new_timeout_minutes = int(new_timeout_input)
                    if new_timeout_minutes > 0:
                        final_config.setdefault('hyperparameter_optimization', {})['timeout'] = new_timeout_minutes * 60
                        final_config.setdefault('hyperparameter_optimization', {})['timeout_seconds'] = new_timeout_minutes * 60
                        print(Fore.GREEN + Style.BRIGHT + f"\nUpdated timeout to {new_timeout_minutes} minutes")
                    else:
                        final_config.setdefault('hyperparameter_optimization', {})['timeout'] = current_timeout_minutes *60
                        final_config.setdefault('hyperparameter_optimization', {})['timeout_seconds'] = current_timeout_minutes *60
                        print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
            except ValueError:
                final_config.setdefault('hyperparameter_optimization', {})['timeout'] = current_timeout_minutes *60
                final_config.setdefault('hyperparameter_optimization', {})['timeout_seconds'] = current_timeout_minutes *60
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nTimeout adjustment cancelled")
                return None
        
        # Sampler selection
        if custom_choice in ['5', '6', '7']:
            print(Fore.MAGENTA + Style.BRIGHT + "\nSAMPLER SELECTION")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            current_sampler = hpo_config.get('sampler', 'TPESampler')
            
            print(Fore.YELLOW + Style.BRIGHT + f"Current sampler: " + Fore.CYAN + Style.BRIGHT + f"{current_sampler}")
            print(Fore.WHITE + Style.BRIGHT + "1. TPESampler " + Fore.GREEN + Style.BRIGHT + "(Tree-structured Parzen Estimator - Recommended)")
            print(Fore.WHITE + Style.BRIGHT + "2. RandomSampler " + Fore.GREEN + Style.BRIGHT + "(Random Sampling - Fast exploration)")
            print(Fore.WHITE + Style.BRIGHT + "3. CmaEsSampler " + Fore.GREEN + Style.BRIGHT + "(CMA-ES - Advanced optimization)")
            print(Fore.WHITE + Style.BRIGHT + "4. GridSampler " + Fore.GREEN + Style.BRIGHT + "(Grid Search - Exhaustive)")
            print(Fore.WHITE + Style.BRIGHT + "5. NSGAIISampler " + Fore.GREEN + Style.BRIGHT + "(Multi-objective optimization)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and keep current")
            
            try:
                sampler_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect sampler (0-5): " + Style.RESET_ALL).strip()
                
                if sampler_choice == '0':
                    final_config.setdefault('hyperparameter_optimization', {})['sampler'] = current_sampler
                    print(Fore.GREEN + Style.BRIGHT + f"\nKeeping current sampler: {current_sampler}")
                elif sampler_choice in ['1', '2', '3', '4', '5']:
                    sampler_configs = {
                        '1': 'TPESampler',
                        '2': 'RandomSampler', 
                        '3': 'CmaEsSampler',
                        '4': 'GridSampler',
                        '5': 'NSGAIISampler'
                    }
                    new_sampler = sampler_configs[sampler_choice]
                    final_config.setdefault('hyperparameter_optimization', {})['sampler'] = new_sampler
                    print(Fore.GREEN + Style.BRIGHT + f"\nChanged sampler to: {new_sampler}")
                else:
                    final_config.setdefault('hyperparameter_optimization', {})['sampler'] = current_sampler
                    print(Fore.GREEN + Style.BRIGHT + f"\nKeeping current sampler: {current_sampler}")
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSampler selection cancelled")
                return None
        
        # Pruner selection
        if custom_choice in ['6', '7']:
            print(Fore.MAGENTA + Style.BRIGHT + "\nPRUNER SELECTION")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            current_pruner = hpo_config.get('pruner', 'MedianPruner')
            
            print(Fore.YELLOW + Style.BRIGHT + f"Current pruner: " + Fore.CYAN + Style.BRIGHT + f"{current_pruner}")
            print(Fore.WHITE + Style.BRIGHT + "1. MedianPruner " + Fore.GREEN + Style.BRIGHT + "(Robust and Effective)")
            print(Fore.WHITE + Style.BRIGHT + "2. HyperbandPruner " + Fore.GREEN + Style.BRIGHT + "(Adaptive Resource Allocation)")
            print(Fore.WHITE + Style.BRIGHT + "3. NopPruner " + Fore.GREEN + Style.BRIGHT + "(No Pruning - Full Trials)")
            print(Fore.WHITE + Style.BRIGHT + "4. PercentilePruner " + Fore.GREEN + Style.BRIGHT + "(Percentile-based Pruning)")
            print(Fore.WHITE + Style.BRIGHT + "5. SuccessiveHalvingPruner " + Fore.GREEN + Style.BRIGHT + "(Resource-efficient)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and keep current")
            
            try:
                pruner_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect pruner (0-3): " + Style.RESET_ALL).strip()
                
                if pruner_choice == '0':
                    final_config.setdefault('hyperparameter_optimization', {})['pruner'] = current_pruner
                    print(Fore.GREEN + Style.BRIGHT + f"\nKeeping current pruner: {current_pruner}")
                elif pruner_choice in ['1', '2', '3']:
                    pruner_configs = {
                        '1': 'MedianPruner',
                        '2': 'HyperbandPruner',
                        '3': 'NopPruner',
                        '4': 'PercentilePruner',
                        '5': 'SuccessiveHalvingPruner'
                    }
                    new_pruner = pruner_configs[pruner_choice]
                    final_config.setdefault('hyperparameter_optimization', {})['pruner'] = new_pruner
                    print(Fore.GREEN + Style.BRIGHT + f"\nChanged pruner to: {new_pruner}")
                else:
                    final_config.setdefault('hyperparameter_optimization', {})['pruner'] = current_pruner
                    print(Fore.GREEN + Style.BRIGHT + f"\nKeeping current pruner: {current_pruner}")
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nPruner selection cancelled")
                return None
        
        # Full customization fallback
        if custom_choice == '7':
            print(Fore.GREEN + Style.BRIGHT + "\nSwitching to full HPO customization mode...")
            return _interactive_hpo_custom_setup(
                final_config,
                data_mode=data_mode,
                hardware_data=hardware_data,
                enable_storage=enable_storage,
                enable_plots=enable_plots,
                custom_search_space=custom_search_space,
                sampler_type=sampler_type,
                pruner_type=pruner_type,
                trial_count=trial_count,
                timeout_seconds=timeout_seconds,
                optimization_focus=optimization_focus,
                **kwargs
            )
        
        # Final confirmation with summary
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "FINAL HPO CONFIGURATION SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        final_model_type = final_config.get('model', {}).get('model_type', current_model_type)
        final_trials = final_config.get('hyperparameter_optimization', {}).get('n_trials', selected_preset_info['n_trials'])
        final_timeout = final_config.get('hyperparameter_optimization', {}).get('timeout', selected_preset_info['timeout'])
        final_timeout_minutes = final_timeout // 60 if final_timeout else final_config.get('hyperparameter_optimization', {}).get('timeout_minutes', selected_preset_info['timeout_minutes'])
        final_sampler = final_config.get('hyperparameter_optimization', {}).get('sampler', selected_preset_info['sampler'])
        final_pruner = final_config.get('hyperparameter_optimization', {}).get('pruner', selected_preset_info['pruner'])
        final_data_source = final_config.get('data', {}).get('use_real_data', selected_preset_info['use_real_data'])
        if final_data_source is None:
            # Try to get from preset data config
            preset_data_config = selected_preset_info.get('preset_data', {}).get('data', {})
            final_data_source = preset_data_config.get('use_real_data', False)
        
        print(Fore.YELLOW + Style.BRIGHT + "Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Preset: " + Fore.GREEN + Style.BRIGHT + f"{selected_preset_name}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model: " + Fore.GREEN + Style.BRIGHT + f"{final_model_type}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + Style.BRIGHT + f"{'Real' if final_data_source else 'Synthetic'}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Trials: " + Fore.GREEN + Style.BRIGHT + f"{final_trials}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.GREEN + Style.BRIGHT + f"{final_timeout_minutes} minutes")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.GREEN + Style.BRIGHT + f"{final_sampler}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Pruner: " + Fore.GREEN + Style.BRIGHT + f"{final_pruner}")
        
        if custom_choice != '1':
            print(Fore.YELLOW + Style.BRIGHT + f"\nCustomizations Applied:")
            customizations = []
            if custom_choice in ['2', '3', '4', '5', '6', '7']:
                customizations.append("Data source")
            if custom_choice in ['3', '4', '5', '6', '7']:
                customizations.append("Trial count")
            if custom_choice in ['4', '5', '6', '7']:
                customizations.append("Timeout")
            if custom_choice in ['5', '6', '7']:
                customizations.append("Sampler")
            if custom_choice in ['6', '7']:
                customizations.append("Pruner")
            
            for i, customization in enumerate(customizations):
                prefix = "  └─" if i == len(customizations) - 1 else "  ├─"
                print(Fore.CYAN + Style.BRIGHT + f"{prefix} {customization}")
        
        # Confirmation with fallback options
        try:
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart hyperparameter optimization with this configuration? (Y/n/c to cancel): " + Style.RESET_ALL).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print(Fore.RED + Style.BRIGHT + "\nHPO confirmation cancelled")
            return None
        
        if confirm in ('', 'y', 'yes'):
            print(Fore.GREEN + Style.BRIGHT + "\nLaunching hyperparameter optimization with preset configuration...")
            
            # Final validation before launch
            required_sections_final = ['hyperparameter_optimization', 'training', 'model', 'data']
            final_missing = []
            
            for section in required_sections_final:
                if section not in final_config:
                    final_missing.append(section)
            
            is_final_valid = len(final_missing) == 0
            
            if not is_final_valid:
                print(Fore.RED + Style.BRIGHT + f"\nError: Final configuration is invalid!")
                print(Fore.YELLOW + Style.BRIGHT + f"Missing sections: {', '.join(final_missing)}")
                return None
            
            # Ensure system configuration is HPO-friendly
            system_config = final_config.setdefault('system', {})
            system_config.update({
                'non_interactive': non_interactive,
                'verbose': True,
                'hpo_optimized': True,
                'system_class': system_class
            })
            
            # Ensure training configuration is optimized for HPO
            training_config = final_config.setdefault('training', {})
            training_config.update({
                'num_workers': 0,  # Safer for HPO
                'pin_memory': False,  # Disabled for HPO stability
                'persistent_workers': False,
                'hpo_mode': True
            })
            
            # Update runtime information
            runtime_config = final_config.setdefault('runtime', {})
            runtime_config.update({
                'setup_method': 'preset_hpo',
                'active_preset': selected_preset_name,
                'hpo_launch_time': datetime.now().isoformat(),
                'hardware_context': {
                    'system_class': system_class,
                    'cuda_available': cuda_available,
                    'memory_gb': memory_gb,
                    'cpu_cores': cpu_cores
                },
                'config_validation': {
                    'preset_valid': is_valid,
                    'final_valid': is_final_valid,
                    'base_config_used': bool(base_config)
                }
            })
            
            return _launch_hpo_with_config(config=final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print(Fore.RED + Style.BRIGHT + "\nHPO cancelled")
            return None
        else:
            # Enhanced fallback options
            print(Fore.YELLOW + Style.BRIGHT + "\nWould you like to?")
            print(Fore.WHITE + Style.BRIGHT + "1. Try HPO preset selection again")
            print(Fore.WHITE + Style.BRIGHT + "2. Switch to express HPO setup")
            print(Fore.WHITE + Style.BRIGHT + "3. Switch to custom HPO configuration")
            print(Fore.RED + Style.BRIGHT + "0. Return to previous menu")
            
            while True:
                try:
                    retry_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-3): " + Style.RESET_ALL).strip()
                    if retry_choice in ['1', '2', '3', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                    return None
            
            if retry_choice == '1':
                print(Fore.GREEN + Style.BRIGHT + "\nRestarting HPO preset selection...")
                return _interactive_hpo_preset_setup(
                    base_config,
                    data_mode=data_mode,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    **kwargs
                )
            elif retry_choice == '2':
                print(Fore.GREEN + Style.BRIGHT + "\nSwitching to express HPO setup...")
                return _interactive_hpo_express_setup(
                    base_config,
                    data_mode=data_mode,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    **kwargs
                )
            elif retry_choice == '3':
                print(Fore.GREEN + Style.BRIGHT + "\nSwitching to custom HPO configuration...")
                return _interactive_hpo_custom_setup(
                    base_config,
                    data_mode=data_mode,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    **kwargs
                )
            else:
                print(Fore.RED + Style.BRIGHT + "\nReturning to previous menu")
                return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\nHPO preset setup interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"HPO preset setup failed: {e}", exc_info=True)
        
        # Error context
        error_context = {
            "Current Preset": current_preset if 'current_preset' in locals() else 'Unknown',
            "Selected Preset": selected_preset_name if 'selected_preset_name' in locals() else 'Unknown',
            "Model Type": current_model_type if 'current_model_type' in locals() else 'Unknown',
            "System Class": system_class if 'system_class' in locals() else 'Unknown',
            "Data Mode": data_mode if data_mode else 'Not specified',
            "Operation Mode": operation_mode if operation_mode else 'Not specified',
            "CUDA Available": cuda_available if 'cuda_available' in locals() else False,
            "Preset Configs Available": len(PRESET_CONFIGS),
            "Trial Count Override": trial_count if trial_count else 'Not specified',
            "Timeout Override": f"{timeout_seconds}s" if timeout_seconds else 'Not specified'
        }
        
        message = (
            f"HPO preset setup failed: {str(e)}\n\n"
            f"Context:\n" +
            "\n".join([f"├─ {key}: {value}" for key, value in list(error_context.items())[:-1]]) +
            f"\n└─ {list(error_context.items())[-1][0]}: {list(error_context.items())[-1][1]}" +
            f"\n\nThis could be due to:\n"
            f"├─ Preset configuration corruption\n"
            f"├─ Configuration merge conflicts\n"
            f"├─ System resource constraints\n"
            f"├─ Invalid parameter combinations\n"
            f"├─ Preset compatibility issues\n"
            f"└─ Model variant initialization failure"
        )
        
        print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
        print(Fore.RED + Style.BRIGHT + "HPO PRESET SETUP ERROR")
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + message)
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        
        return None

def _interactive_hpo_custom_setup(
    base_config: Dict[str, Any],
    data_mode: Optional[str] = None,
    hardware_data: Optional[Dict[str, Any]] = None,
    enable_storage: Optional[bool] = None,
    enable_plots: Optional[bool] = None,
    custom_search_space: Optional[Dict[str, Any]] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    trial_count: Optional[int] = None,
    timeout_seconds: Optional[int] = None,
    optimization_focus: Optional[str] = None,
    study_name: Optional[str] = None,
    storage_url: Optional[str] = None,
    non_interactive: bool = False,
    skip_prompt: bool = False,
    operation_mode: Optional[str] = None,
    use_current_config: bool = False,
    force_express: bool = False,
    model_types: Optional[List[str]] = None,
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Enhanced full custom HPO configuration with complete parameter compatibility.
    Updated to align with preset configurations: STABILITY, PERFORMANCE, BASELINE, 
    DEBUG, LIGHTWEIGHT, ADVANCED, and DEFAULT presets.
    
    Args:
        base_config: Base configuration to use
        data_mode: Data source mode ('synthetic', 'real', 'auto')
        hardware_data: Pre-fetched hardware data for optimization
        enable_storage: Override storage settings from preset
        enable_plots: Override plot generation settings from preset
        custom_search_space: Override search space from preset
        sampler_type: Override sampler type from preset
        pruner_type: Override pruner type from preset
        trial_count: Override number of trials
        timeout_seconds: Override timeout in seconds
        optimization_focus: Optimization objective focus ('accuracy', 'speed', 'balanced')
        study_name: Specific study name for continuation
        storage_url: Storage URL for study continuation
        non_interactive: Run without user prompts
        skip_prompt: Skip confirmation prompts
        operation_mode: Specific operation mode
        use_current_config: Whether to use current configuration as base
        force_express: Force express mode even if other options specified
        model_types: List of model types for comparison
        **kwargs: Additional parameters for forward compatibility
        
    Returns:
        Dictionary with HPO results or None if cancelled/failed
    """
    try:
        # Clear screen and show banner with config retrieval
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if base_config is None and banner_config is not None:
            base_config = banner_config
        elif base_config is None:
            base_config = get_current_config()
        
        # Get hardware context if not provided
        if hardware_data is None:
            try:
                hardware_data = check_hardware(include_memory_usage=True)
            except Exception as e:
                logger.debug(f"Hardware detection failed: {e}")
                hardware_data = {}
        
        # Extract configuration context for display
        current_hpo_config = base_config.get('hyperparameter_optimization', {})
        data_config = base_config.get('data', {})
        model_config = base_config.get('model', {})
        training_config = base_config.get('training', {})
        metadata = base_config.get('metadata', {})
        presets_section = base_config.get('presets', {})
        validation_config = base_config.get('validation', {})
        cross_validation_config = validation_config.get('cross_validation', {})
        search_space_config = current_hpo_config.get('search_space', {})
        current_system_config = base_config.get('system', {})
        early_stopping_config = current_hpo_config.get('early_stopping', {})
        current_storage_config = current_hpo_config.get('storage', {})
        current_hardware_config = base_config.get('hardware', {})
        
        # Context extraction using multiple fallbacks
        current_preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        if isinstance(presets_section, dict):
            current_preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if current_preset_name in ["Custom/Default", None, ""]:
            metadata = base_config.get("metadata", {})
            if isinstance(metadata, dict):
                current_preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if current_preset_name in ["Custom/Default", None, ""]:
            current_preset_name = base_config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if current_preset_name in ["Custom/Default", None, ""]:
            runtime = base_config.get("runtime", {})
            if isinstance(runtime, dict):
                current_preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if current_preset_name in ["Custom/Default", None, "", "none"]:
            current_preset_name = "Custom/Default"
        elif isinstance(current_preset_name, str):
            current_preset_name = current_preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in base_config and isinstance(base_config["runtime"], dict):
            config_source = base_config["runtime"].get("config_source", "runtime")
        elif "metadata" in base_config and isinstance(base_config["metadata"], dict):
            config_source = base_config["metadata"].get("config_source", "metadata")
        else:
            config_source = "Unknown"
        
        # Hardware-aware system class detection with preset alignment
        cuda_available = hardware_data.get('cuda', {}).get('available', False)
        gpu_count = hardware_data.get('cuda', {}).get('gpu_count', 0)
        memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)
        
        # Determine system performance class
        if cuda_available and memory_gb >= 32 and cpu_cores >= 16:
            system_class = "advanced"
        elif cuda_available and memory_gb >= 16 and cpu_cores >= 8:
            system_class = "performance"
        elif cuda_available and memory_gb >= 8:
            system_class = "baseline"
        elif memory_gb >= 4:
            system_class = "stability"
        elif memory_gb >= 2:
            system_class = "lightweight"
        else:
            system_class = "debug"
        
        # Map system class to preset-based defaults
        preset_defaults = {
            "debug": {
                "trials": 10, "timeout": 600, "epochs": 5, "batch_size": 16,
                "encoding_min": 2, "encoding_max": 8, "lr_min": 1e-3, "lr_max": 1e-1,
                "dropout_min": 0.0, "dropout_max": 0.1, "workers": 1
            },
            "lightweight": {
                "trials": 20, "timeout": 1800, "epochs": 20, "batch_size": 8,
                "encoding_min": 4, "encoding_max": 12, "lr_min": 1e-4, "lr_max": 1e-2,
                "dropout_min": 0.0, "dropout_max": 0.3, "workers": 1
            },
            "stability": {
                "trials": 50, "timeout": 1800, "epochs": 50, "batch_size": 32,
                "encoding_min": 6, "encoding_max": 12, "lr_min": 1e-4, "lr_max": 1e-2,
                "dropout_min": 0.1, "dropout_max": 0.4, "workers": 2
            },
            "baseline": {
                "trials": 100, "timeout": 3600, "epochs": 50, "batch_size": 64,
                "encoding_min": 8, "encoding_max": 24, "lr_min": 1e-5, "lr_max": 1e-1,
                "dropout_min": 0.1, "dropout_max": 0.4, "workers": 4
            },
            "performance": {
                "trials": 200, "timeout": 7200, "epochs": 80, "batch_size": 128,
                "encoding_min": 12, "encoding_max": 32, "lr_min": 1e-5, "lr_max": 1e-2,
                "dropout_min": 0.0, "dropout_max": 0.2, "workers": 8
            },
            "advanced": {
                "trials": 500, "timeout": 14400, "epochs": 100, "batch_size": 256,
                "encoding_min": 16, "encoding_max": 64, "lr_min": 1e-6, "lr_max": 1e-3,
                "dropout_min": 0.0, "dropout_max": 0.3, "workers": 16
            }
        }
        
        system_defaults = preset_defaults.get(system_class, preset_defaults["baseline"])
        
        # Resolve data_mode from use_real_data parameter if needed
        use_real_data_config = data_config.get('use_real_data', False)
        use_real_data = None
        if data_mode is None:
            if use_real_data_config is True:
                data_mode = 'real'
            elif use_real_data_config is False:
                data_mode = 'synthetic'
            else:
                data_mode = 'auto'
        else:
            # Convert data_mode to use_real_data for backward compatibility
            if data_mode == 'real':
                use_real_data = True
            elif data_mode == 'synthetic':
                use_real_data = False
        
        # Header with context display
        header_title = "CUSTOM HYPERPARAMETER OPTIMIZATION CONFIGURATION"
        if operation_mode == 'quick_test':
            header_title = "QUICK CUSTOM HPO CONFIGURATION"
        elif operation_mode == 'model_comparison':
            header_title = "MODEL COMPARISON CUSTOM CONFIGURATION"
            
        print(Fore.MAGENTA + Style.BRIGHT + header_title)
        print(Fore.CYAN + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        
        # Display current context
        print(Fore.YELLOW + Style.BRIGHT + "Current Configuration Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.YELLOW + Style.BRIGHT + f"{current_preset_name.title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Data Mode: " + Fore.YELLOW + Style.BRIGHT + f"{data_mode}")
        
        # Display parameter overrides if provided
        override_params = []
        if trial_count is not None:
            override_params.append(f"Trial Count: {trial_count}")
        if timeout_seconds is not None:
            override_params.append(f"Timeout: {timeout_seconds}s")
        if sampler_type:
            override_params.append(f"Sampler: {sampler_type}")
        if pruner_type:
            override_params.append(f"Pruner: {pruner_type}")
        if optimization_focus:
            override_params.append(f"Optimization Focus: {optimization_focus}")
        if enable_storage is not None:
            override_params.append(f"Storage: {enable_storage}")
        if enable_plots is not None:
            override_params.append(f"Plots: {enable_plots}")
        if custom_search_space:
            override_params.append("Custom Search Space: Provided")
        if study_name:
            override_params.append(f"Study Name: {study_name}")
        if storage_url:
            override_params.append(f"Storage URL: {storage_url}")
        if model_types:
            override_params.append(f"Model Types: {len(model_types)} models")
        
        if override_params:
            print(Fore.CYAN + Style.BRIGHT + "\nParameter Overrides:")
            for i, param in enumerate(override_params):
                prefix = "  └─" if i == len(override_params) - 1 else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {param}")
        
        # Display hardware context
        print(Fore.MAGENTA + Style.BRIGHT + "\nHardware Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
        if cuda_available:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ GPU Count: " + Fore.YELLOW + Style.BRIGHT + f"{gpu_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Memory: " + Fore.YELLOW + Style.BRIGHT + f"{memory_gb:.1f}GB")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CPU Cores: " + Fore.YELLOW + Style.BRIGHT + f"{cpu_cores}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Recommended Preset: " + Fore.CYAN + Style.BRIGHT + f"{system_class.title()}")
        
        print(Fore.CYAN + Style.BRIGHT + "\nFull custom HPO configuration with preset-aligned defaults")
        print(Fore.YELLOW + Style.BRIGHT + f"System-recommended defaults shown in parentheses for {system_class} class")
        print(Fore.RED + Style.BRIGHT + "Enter 'c' at any time to cancel and return to previous menu\n")
        
        # Handle non-interactive mode with parameter overrides
        if non_interactive and skip_prompt:
            print(Fore.GREEN + Style.BRIGHT + f"\nNon-interactive Mode - Using custom configuration with overrides")
            
            # Apply parameter overrides directly
            final_config = deepcopy(base_config) if base_config else {}
            if not final_config:
                final_config = deepcopy(PRESET_CONFIGS.get('default', {}))
            
            # Apply all parameter overrides
            hpo_config = final_config.setdefault('hyperparameter_optimization', {})
            
            if trial_count is not None:
                hpo_config['n_trials'] = trial_count
            if timeout_seconds is not None:
                hpo_config['timeout'] = timeout_seconds
                hpo_config['timeout_seconds'] = timeout_seconds
            if sampler_type:
                hpo_config['sampler'] = sampler_type
            if pruner_type:
                hpo_config['pruner'] = pruner_type
            if enable_storage is not None:
                storage_config = hpo_config.get('storage', {})
                storage_config['enabled'] = enable_storage
                hpo_config['storage'] = storage_config
            if enable_plots is not None:
                hpo_config['generate_plots'] = enable_plots
            if custom_search_space:
                hpo_config['optimization_space'] = custom_search_space
            if study_name:
                hpo_config['study_name'] = study_name
            if optimization_focus:
                metadata_section = final_config.setdefault('metadata', {})
                metadata_section['optimization_focus'] = optimization_focus
            
            # Apply data mode
            data_config_section = final_config.setdefault('data', {})
            if data_mode and data_mode != 'auto':
                if data_mode == 'synthetic':
                    data_config_section['use_real_data'] = False
                elif data_mode == 'real':
                    data_config_section['use_real_data'] = True
            
            # Apply model types if provided
            if model_types:
                model_search = hpo_config.setdefault('model_search', {})
                model_search['model_types'] = model_types
                model_search['enabled'] = len(model_types) > 1
            
            # Update metadata
            metadata_section = final_config.setdefault('metadata', {})
            metadata_section.update({
                'last_modified': datetime.now().isoformat(),
                'setup_method': 'custom_hpo_non_interactive',
                'custom_configuration': True,
                'parameter_overrides_applied': len(override_params) if override_params else 0,
                'system_class_used': system_class
            })
            
            print(Fore.GREEN + Style.BRIGHT + f"\nApplied {len(override_params)} parameter overrides in non-interactive mode")
            return _launch_hpo_with_config(config=final_config, **kwargs)
        
        # Deep copy base config to avoid mutations
        final_config = deepcopy(base_config) if base_config else {}
        
        # If no base config, use system-class appropriate preset as foundation
        if not final_config:
            if system_class in PRESET_CONFIGS:
                final_config = deepcopy(PRESET_CONFIGS[system_class])
                print(Fore.GREEN + Style.BRIGHT + f"\nUsing {system_class} preset as foundation for custom HPO")
            else:
                final_config = deepcopy(PRESET_CONFIGS.get('default', {}))
                print(Fore.GREEN + Style.BRIGHT + "\nUsing default preset as foundation for custom HPO")
        
        # Apply initial parameter overrides to configuration
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        applied_overrides = []
        
        if trial_count is not None:
            hpo_config['n_trials'] = trial_count
            applied_overrides.append(f"Trial count: {trial_count}")
        if timeout_seconds is not None:
            hpo_config['timeout'] = timeout_seconds
            hpo_config['timeout_seconds'] = timeout_seconds
            applied_overrides.append(f"Timeout: {timeout_seconds}s")
        if sampler_type:
            hpo_config['sampler'] = sampler_type
            applied_overrides.append(f"Sampler: {sampler_type}")
        if pruner_type:
            hpo_config['pruner'] = pruner_type
            applied_overrides.append(f"Pruner: {pruner_type}")
        if enable_storage is not None:
            storage_config = hpo_config.get('storage', {})
            storage_config['enabled'] = enable_storage
            hpo_config['storage'] = storage_config
            applied_overrides.append(f"Storage: {enable_storage}")
        if enable_plots is not None:
            hpo_config['generate_plots'] = enable_plots
            applied_overrides.append(f"Plots: {enable_plots}")
        if custom_search_space:
            hpo_config['optimization_space'] = custom_search_space
            applied_overrides.append("Custom search space parameters")
        if optimization_focus:
            metadata_section = final_config.setdefault('metadata', {})
            metadata_section['optimization_focus'] = optimization_focus
            applied_overrides.append(f"Optimization focus: {optimization_focus}")
        
        if applied_overrides:
            print(Fore.CYAN + Style.BRIGHT + "\nApplied Initial Parameter Overrides:")
            for i, override in enumerate(applied_overrides):
                prefix = "  └─" if i == len(applied_overrides) - 1 else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {override}")
        
        print(Fore.MAGENTA + Style.BRIGHT + "OPTIMIZATION STRATEGY")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Study configuration with input handling and parameter override support
        study_name_input = study_name  # Start with provided study_name
        if study_name_input is None:
            while not study_name_input:
                try:
                    study_name_prompt = input(Fore.YELLOW + Style.BRIGHT + f"Study name (auto-generated): ").strip()
                    if study_name_prompt.lower() == 'c':
                        print(Fore.RED + Style.BRIGHT + "\nStudy configuration cancelled")
                        return None
                    if not study_name_prompt:
                        study_name_input = f"custom_hpo_{system_class}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                        print(Fore.GREEN + Style.BRIGHT + f"\nUsing auto-generated study name: {study_name_input}")
                    else:
                        study_name_input = study_name_prompt
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nStudy configuration interrupted!")
                    return None
        else:
            print(Fore.GREEN + Style.BRIGHT + f"\nUsing provided study name: {study_name_input}")
        
        print(Fore.YELLOW + Style.BRIGHT + "\nOptimization direction:")
        print(Fore.WHITE + Style.BRIGHT + "1. Minimize " + Fore.GREEN + Style.BRIGHT + "(lower values are better - e.g., loss)")
        print(Fore.WHITE + Style.BRIGHT + "2. Maximize " + Fore.GREEN + Style.BRIGHT + "(higher values are better - e.g., accuracy)")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        direction_choice = None
        while not direction_choice:
            try:
                direction_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect direction (0-2, default=1): ").strip()
                
                if not direction_choice:
                    direction_choice = '1'
                    break
                    
                if direction_choice == '0':
                    print(Fore.RED + Style.BRIGHT + "\nDirection selection cancelled")
                    return None
                    
                if direction_choice not in ['1', '2']:
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                    direction_choice = None
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nDirection selection interrupted!")
                return None
        
        direction = 'minimize' if direction_choice == '1' else 'maximize'
        
        # Trial configuration with validation and parameter override support
        n_trials = trial_count if trial_count is not None else None
        while n_trials is None:
            try:
                default_trials = system_defaults["trials"]
                n_trials_input = input(Fore.YELLOW + Style.BRIGHT + f"\nNumber of trials ({default_trials}): ").strip()
                if n_trials_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nTrial configuration cancelled")
                    return None
                n_trials = int(n_trials_input) if n_trials_input else default_trials
                if n_trials <= 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
                    n_trials = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nTrial configuration interrupted!")
                return None
        
        timeout_minutes_input = timeout_seconds // 60 if timeout_seconds is not None else None
        while timeout_minutes_input is None:
            try:
                default_timeout_min = system_defaults["timeout"] // 60
                timeout_input = input(Fore.YELLOW + Style.BRIGHT + f"\nTimeout in minutes ({default_timeout_min} for system class, 0 for no timeout): ").strip()
                if timeout_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nTimeout configuration cancelled")
                    return None
                timeout_minutes_input = int(timeout_input) if timeout_input else default_timeout_min
                if timeout_minutes_input < 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a non-negative number.")
                    timeout_minutes_input = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nTimeout configuration interrupted!")
                return None
        
        # Convert back to seconds for configuration
        timeout_seconds_final = timeout_minutes_input * 60 if timeout_minutes_input > 0 else 0
        
        print(Fore.MAGENTA + Style.BRIGHT + "SAMPLING AND PRUNING")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Use provided sampler_type or prompt for selection
        if sampler_type is None:
            print(Fore.YELLOW + Style.BRIGHT + "Sampling strategy:")
            samplers = [
                ('TPESampler', 'Tree-structured Parzen Estimator (Recommended for most cases)'),
                ('RandomSampler', 'Random sampling (Good baseline, fast)'),
                ('CmaEsSampler', 'Covariance Matrix Adaptation (Advanced, mathematical)'),
                ('GridSampler', 'Grid search (Systematic, exhaustive)'),
                ('NSGAIISampler', 'Multi-objective optimization (For multiple objectives)')
            ]
            for i, (sampler, desc) in enumerate(samplers, 1):
                print(f"{Fore.WHITE + Style.BRIGHT}{i}. {sampler}{Style.RESET_ALL} - {Fore.GREEN + Style.BRIGHT}{desc}{Style.RESET_ALL}")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            sampler_choice = None
            while not sampler_choice:
                try:
                    sampler_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect sampler (0-{len(samplers)}, default=1): ").strip()
                    
                    if not sampler_choice:
                        sampler_choice = '1'
                        break
                        
                    if sampler_choice == '0':
                        print(Fore.RED + Style.BRIGHT + "\nSampler selection cancelled")
                        return None
                        
                    if not sampler_choice.isdigit() or not (1 <= int(sampler_choice) <= len(samplers)):
                        print(Fore.RED + Style.BRIGHT + f"\nInvalid choice. Please select 0-{len(samplers)}.")
                        sampler_choice = None
                        continue
                        
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nSampler selection interrupted!")
                    return None
            
            sampler_idx = int(sampler_choice) - 1
            sampler_type = samplers[sampler_idx][0]
        else:
            print(Fore.GREEN + Style.BRIGHT + f"\nUsing provided sampler: {sampler_type}")
        
        # Use provided pruner_type or prompt for selection
        if pruner_type is None:
            print(Fore.YELLOW + Style.BRIGHT + "\nPruning strategy:")
            pruners = [
                ('MedianPruner', 'Prune trials below median performance (Balanced)'),
                ('HyperbandPruner', 'Successive halving algorithm (Aggressive)'),
                ('NopPruner', 'No pruning - complete all trials (Debug/Comprehensive)'),
                ('PercentilePruner', 'Prune trials below a certain percentile (Customizable)'),
                ('SuccessiveHalvingPruner', 'Iteratively prune worst-performing trials (Efficient)')
            ]
            for i, (pruner, desc) in enumerate(pruners, 1):
                print(f"{Fore.WHITE + Style.BRIGHT}{i}. {pruner}{Style.RESET_ALL} - {Fore.GREEN + Style.BRIGHT}{desc}{Style.RESET_ALL}")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            pruner_choice = None
            while not pruner_choice:
                try:
                    pruner_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect pruner (0-{len(pruners)}, default=1): ").strip()
                    
                    if not pruner_choice:
                        pruner_choice = '1'
                        break
                        
                    if pruner_choice == '0':
                        print(Fore.RED + Style.BRIGHT + "\nPruner selection cancelled")
                        return None
                        
                    if not pruner_choice.isdigit() or not (1 <= int(pruner_choice) <= len(pruners)):
                        print(Fore.RED + Style.BRIGHT + f"\nInvalid choice. Please select 0-{len(pruners)}.")
                        pruner_choice = None
                        continue
                        
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nPruner selection interrupted!")
                    return None
            
            pruner_idx = int(pruner_choice) - 1
            pruner_type = pruners[pruner_idx][0]
        else:
            print(Fore.GREEN + Style.BRIGHT + f"\nUsing provided pruner: {pruner_type}")
        
        print(Fore.MAGENTA + Style.BRIGHT + "MODEL SELECTION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Use provided model_types or prompt for selection
        if model_types is None:
            print(Fore.YELLOW + Style.BRIGHT + "Model types to optimize:")
            available_models = list(MODEL_VARIANTS.keys())
            selected_models = []
            
            for i, model in enumerate(available_models, 1):
                descriptions = {
                    'SimpleAutoencoder': 'Fast, lightweight model for quick iterations',
                    'EnhancedAutoencoder': 'Advanced features, good balance of performance and speed',
                    'AutoencoderEnsemble': 'Best performance, more complex and resource-intensive'
                }
                while True:
                    try:
                        include = input(Fore.YELLOW + Style.BRIGHT + f"\n{i}. Include {model}? ({descriptions.get(model, 'Custom model')}) (Y/n): ").strip().lower()
                        if include == 'c':
                            print(Fore.RED + Style.BRIGHT + "\nModel selection cancelled")
                            return None
                        if include in ('', 'y', 'yes', 'n', 'no'):
                            break
                        print(Fore.RED + Style.BRIGHT + "\nPlease enter Y or N")
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nModel selection interrupted!")
                        return None
                
                if include in ('', 'y', 'yes'):
                    selected_models.append(model)
                    print(Fore.GREEN + Style.BRIGHT + f"\n   Added {model}")
            
            if not selected_models:
                print(Fore.YELLOW + Style.BRIGHT + "\nNo models selected, using EnhancedAutoencoder as default")
                selected_models = ['EnhancedAutoencoder']
            
            model_types = selected_models
        else:
            print(Fore.GREEN + Style.BRIGHT + f"\nUsing provided model types: {', '.join(model_types)}")
            selected_models = model_types
        
        search_all_models = len(selected_models) >= 3
        
        print(Fore.MAGENTA + Style.BRIGHT + "DATA CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Data source configuration with parameter override support
        if data_mode == 'auto' or use_real_data is None:
            print(Fore.YELLOW + Style.BRIGHT + "\nData source:")
            print(Fore.WHITE + Style.BRIGHT + "1. Real network data " + Fore.GREEN + Style.BRIGHT + "(Recommended for production)")
            print(Fore.WHITE + Style.BRIGHT + "2. Synthetic data " + Fore.GREEN + Style.BRIGHT + "(Recommended for testing/development)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            data_choice = None
            while not data_choice:
                try:
                    data_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect data source (0-2): ").strip()
                    
                    if data_choice == '0':
                        print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                        return None
                        
                    if data_choice not in ['1', '2']:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                        data_choice = None
                        continue
                        
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nData selection interrupted!")
                    return None
                
            if data_choice == '1':
                use_real_data = True
                data_mode = 'real'
                print(Fore.GREEN + Style.BRIGHT + "\nUsing real network data for HPO")
            else:
                use_real_data = False
                data_mode = 'synthetic'
                print(Fore.GREEN + Style.BRIGHT + "\nUsing synthetic data for HPO")
        else:
            # Use the resolved data_mode
            if data_mode == 'real':
                use_real_data = True
            elif data_mode == 'synthetic':
                use_real_data = False
            print(Fore.GREEN + Style.BRIGHT + f"\nUsing {data_mode} data mode")
        
        # Data path, samples, features configuration with preset alignment
        data_path = data_config.get('data_path', 'data/network_data.csv')
        artifacts_path = data_config.get('artifacts_path', 'data/artifacts.pkl')
        
        # Get preset-appropriate sample sizes
        if use_real_data:
            normal_samples = data_config.get('normal_samples', 8000)
            attack_samples = data_config.get('attack_samples', 2000)
            features = data_config.get('features', 20)
        else:
            # Use system-class appropriate synthetic data sizes
            sample_multiplier = {
                "debug": 0.1, "lightweight": 0.25, "stability": 0.5,
                "baseline": 1.0, "performance": 1.25, "advanced": 1.5
            }.get(system_class, 1.0)
            
            normal_samples = int(data_config.get('normal_samples', 8000) * sample_multiplier)
            attack_samples = int(data_config.get('attack_samples', 2000) * sample_multiplier)
            features = data_config.get('features', 20)
        
        noise_factor = data_config.get('synthetic_generation', {}).get('noise_factor', 0.05)
        
        if use_real_data:
            print(Fore.YELLOW + Style.BRIGHT + "\nReal data configuration:")
            print(Fore.CYAN + Style.BRIGHT + "  └─ Using actual network traffic data for realistic optimization")
            try:
                print(Fore.YELLOW + Style.BRIGHT + f"\nData file path (default): " + Fore.GREEN + Style.BRIGHT + f"{data_path}")
                print(Fore.CYAN + Style.BRIGHT + "  ├─ Path to your network traffic data file")
                print(Fore.CYAN + Style.BRIGHT + "  └─ Leave empty to use default or provide custom path")
                
                data_path_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter data file path or 'c' to cancel: ").strip()
                if data_path_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nData configuration cancelled")
                    return None
                data_path = data_path_input if data_path_input else data_path
                
                print(Fore.YELLOW + Style.BRIGHT + f"\nArtifacts path (default): " + Fore.GREEN + Style.BRIGHT + f"{artifacts_path}")
                print(Fore.CYAN + Style.BRIGHT + "  ├─ Path to your preprocessing artifacts data file")
                print(Fore.CYAN + Style.BRIGHT + "  ├─ Directory for storing preprocessing artifacts")
                print(Fore.CYAN + Style.BRIGHT + "  └─ Leave empty to use default or provide custom path")
                
                artifacts_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter artifacts file path or 'c' to cancel: ").strip()
                if artifacts_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nData configuration cancelled")
                    return None
                artifacts_path = artifacts_input if artifacts_input else artifacts_path
                
                print(Fore.YELLOW + Style.BRIGHT + f"\nNumber of features (default): " + Fore.GREEN + Style.BRIGHT + f"{features}")
                print(Fore.CYAN + Style.BRIGHT + "  ├─ Number of input features or 'auto' to detect")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Preset recommends {features} features")
                
                features_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter number of features or 'c' to cancel: ").strip()
                if features_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nFeatures configuration cancelled")
                    return None
                features = int(features_input) if features_input else features
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nFeatures configuration interrupted!")
                return None
        else:
            print(Fore.YELLOW + Style.BRIGHT + "\nSynthetic data configuration:")
            print(Fore.CYAN + Style.BRIGHT + "  └─ Using generated data for faster iteration cycles")
            try:
                print(Fore.CYAN + Style.BRIGHT + "\nNumber of normal samples to generate:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{normal_samples:,} (based on {system_class} preset)")
                normal_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter number of normal samples or 'c' to cancel: ").strip()
                if normal_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nData configuration cancelled")
                    return None
                normal_samples = int(normal_input) if normal_input else normal_samples
                
                print(Fore.CYAN + Style.BRIGHT + "\nNumber of attack samples to generate:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{attack_samples:,} (based on {system_class} preset)")
                attack_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter number of attack samples or 'c' to cancel: ").strip()
                if attack_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nData configuration cancelled")
                    return None
                attack_samples = int(attack_input) if attack_input else attack_samples
                
                print(Fore.CYAN + Style.BRIGHT + "\nNumber of features to generate:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{features}")
                features_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter number of features or 'c' to cancel: ").strip()
                if features_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nData configuration cancelled")
                    return None
                features = int(features_input) if features_input else features
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nData configuration interrupted!")
                return None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nInvalid input. Using default values.")
        
        print(Fore.MAGENTA + Style.BRIGHT + "VALIDATION STRATEGY")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Cross-validation configuration
        cv_folds = None
        default_cv_folds = validation_config.get('cross_validation', {}).get('folds', 3)
        default_cv_folds_status = Fore.GREEN + Style.BRIGHT + f"{default_cv_folds}" if default_cv_folds else Fore.YELLOW + Style.BRIGHT + "not set"
        while cv_folds is None:
            try:
                print(Fore.CYAN + Style.BRIGHT + "\nNumber of cross-validation folds:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: {default_cv_folds_status}")
                cv_folds_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of cv-folds or 'c' to cancel: ").strip()
                if cv_folds_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nValidation configuration cancelled")
                    return None
                cv_folds = int(cv_folds_input) if cv_folds_input else default_cv_folds
                if cv_folds <= 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
                    cv_folds = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nValidation configuration interrupted!")
                return None
        
        cv_shuffle = None
        default_cv_shuffle = validation_config.get('cross_validation', {}).get('shuffle', True)
        default_cv_shuffle_status = Fore.GREEN + Style.BRIGHT + f"{default_cv_shuffle}" if default_cv_shuffle else Fore.YELLOW + Style.BRIGHT + "not set"
        while cv_shuffle is None:
            try:
                print(Fore.CYAN + Style.BRIGHT + "\nShuffle cross-validation splits:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: {default_cv_shuffle_status}")
                cv_shuffle_input = input(Fore.YELLOW + Style.BRIGHT + "\nShuffle CV splits? (Y/n): ").strip().lower()
                if cv_shuffle_input == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nValidation configuration cancelled")
                    return None
                cv_shuffle = cv_shuffle_input in ('', 'y', 'yes')
                break
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nValidation configuration interrupted!")
                return None
        
        cv_random_state = None
        default_cv_random_state = validation_config.get('cross_validation', {}).get('random_state', 42)
        default_cv_random_state_status = Fore.GREEN + Style.BRIGHT + f"{default_cv_random_state}" if default_cv_random_state else Fore.YELLOW + Style.BRIGHT + "not set"
        while cv_random_state is None:
            try:
                print(Fore.CYAN + Style.BRIGHT + "\nCross-validation random state:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: {default_cv_random_state_status}")
                cv_random_state_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter CV random state or 'c' to cancel: ").strip()
                if cv_random_state_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nValidation configuration cancelled")
                    return None
                cv_random_state = int(cv_random_state_input) if cv_random_state_input else 42
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nValidation configuration interrupted!")
                return None
        
        print(Fore.MAGENTA + Style.BRIGHT + "TRIAL CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Trial configuration
        trial_epochs = None
        while trial_epochs is None:
            try:
                default_epochs = system_defaults["epochs"]
                print(Fore.CYAN + Style.BRIGHT + f"\nTrial epochs:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_epochs}")
                trial_epochs_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter epochs per trial or 'c' to cancel: ").strip()
                if trial_epochs_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nTrial configuration cancelled")
                    return None
                trial_epochs = int(trial_epochs_input) if trial_epochs_input else default_epochs
                if trial_epochs <= 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
                    trial_epochs = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nTrial configuration interrupted!")
                return None
        
        trial_patience = None
        while trial_patience is None:
            try:
                default_trial_patience = hpo_config.get('trial_patience', 10)
                print(Fore.CYAN + Style.BRIGHT + f"\nTrial patience:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_trial_patience}")
                trial_patience_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter early stopping patience per trial or 'c' to cancel: ").strip()
                if trial_patience_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nTrial configuration cancelled")
                    return None
                trial_patience = int(trial_patience_input) if trial_patience_input else default_trial_patience
                if trial_patience < 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a non-negative number.")
                    trial_patience = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nTrial configuration interrupted!")
                return None
        
        trial_batch_size = None
        while trial_batch_size is None:
            try:
                default_batch_size = system_defaults["batch_size"]
                print(Fore.CYAN + Style.BRIGHT + f"\nBatch size:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_batch_size}")
                trial_batch_size_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter batch size for trials or 'c' to cancel: ").strip()
                if trial_batch_size_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nTrial configuration cancelled")
                    return None
                trial_batch_size = int(trial_batch_size_input) if trial_batch_size_input else default_batch_size
                if trial_batch_size <= 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
                    trial_batch_size = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nTrial configuration interrupted!")
                return None
        
        print(Fore.MAGENTA + Style.BRIGHT + "SEARCH SPACE CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        print(Fore.YELLOW + Style.BRIGHT + "Define hyperparameter search ranges:")
        
        # Learning rate range
        lr_min = None
        while lr_min is None:
            try:
                default_lr_min = system_defaults["lr_min"]
                print(Fore.CYAN + Style.BRIGHT + f"\nLearning rate minimum:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_lr_min:.0e}")
                lr_min_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter lr_min or 'c' to cancel: ").strip()
                if lr_min_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSearch space configuration cancelled")
                    return None
                lr_min = float(lr_min_input) if lr_min_input else default_lr_min
                if lr_min <= 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
                    lr_min = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSearch space configuration interrupted!")
                return None
        
        lr_max = None
        while lr_max is None:
            try:
                default_lr_max = system_defaults["lr_max"]
                print(Fore.CYAN + Style.BRIGHT + f"\nLearning rate maximum:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_lr_max:.0e}")
                lr_max_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter lr_max or 'c' to cancel: ").strip()
                if lr_max_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSearch space configuration cancelled")
                    return None
                lr_max = float(lr_max_input) if lr_max_input else default_lr_max
                if lr_max <= lr_min:
                    print(Fore.RED + Style.BRIGHT + f"\nMaximum must be greater than minimum ({lr_min})")
                    lr_max = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSearch space configuration interrupted!")
                return None
        
        # Batch size options
        batch_sizes = None
        while batch_sizes is None:
            try:
                default_batch_sizes = [16, 32, 64, 128, 256]
                # Filter batch sizes based on system class
                if system_class == "debug":
                    default_batch_sizes = [8, 16, 32]
                elif system_class == "lightweight":
                    default_batch_sizes = [8, 16, 32]
                elif system_class == "stability":
                    default_batch_sizes = [16, 32, 64]
                elif system_class == "baseline":
                    default_batch_sizes = [32, 64, 128]
                elif system_class == "performance":
                    default_batch_sizes = [64, 128, 256]
                elif system_class == "advanced":
                    default_batch_sizes = [64, 128, 256, 512]
                
                batch_sizes_str = ", ".join(map(str, default_batch_sizes))
                print(Fore.CYAN + Style.BRIGHT + f"\nBatch sizes:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{batch_sizes_str}")
                batch_sizes_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter batch sizes (comma-separated) or 'c' to cancel: ").strip()
                if batch_sizes_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSearch space configuration cancelled")
                    return None
                if batch_sizes_input:
                    batch_sizes = [int(x.strip()) for x in batch_sizes_input.split(',')]
                    if any(bs <= 0 for bs in batch_sizes):
                        print(Fore.RED + Style.BRIGHT + "\nAll batch sizes must be positive numbers.")
                        batch_sizes = None
                else:
                    batch_sizes = default_batch_sizes
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter valid numbers separated by commas.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSearch space configuration interrupted!")
                return None
        
        # Encoding dimension range
        encoding_dim_min = None
        while encoding_dim_min is None:
            try:
                default_encoding_min = system_defaults["encoding_min"]
                print(Fore.CYAN + Style.BRIGHT + f"\nEncoding dimension minimum:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_encoding_min}")
                encoding_dim_min_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter encoding_dim_min or 'c' to cancel: ").strip()
                if encoding_dim_min_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSearch space configuration cancelled")
                    return None
                encoding_dim_min = int(encoding_dim_min_input) if encoding_dim_min_input else default_encoding_min
                if encoding_dim_min <= 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
                    encoding_dim_min = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSearch space configuration interrupted!")
                return None
        
        encoding_dim_max = None
        while encoding_dim_max is None:
            try:
                default_encoding_max = system_defaults["encoding_max"]
                print(Fore.CYAN + Style.BRIGHT + f"\nEncoding dimension maximum:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_encoding_max}")
                encoding_dim_max_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter encoding_dim_max or 'c' to cancel: ").strip()
                if encoding_dim_max_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSearch space configuration cancelled")
                    return None
                encoding_dim_max = int(encoding_dim_max_input) if encoding_dim_max_input else default_encoding_max
                if encoding_dim_max <= encoding_dim_min:
                    print(Fore.RED + Style.BRIGHT + f"\nMaximum must be greater than minimum ({encoding_dim_min})")
                    encoding_dim_max = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSearch space configuration interrupted!")
                return None
        
        # Dropout rate range
        dropout_min = None
        while dropout_min is None:
            try:
                default_dropout_min = system_defaults["dropout_min"]
                print(Fore.CYAN + Style.BRIGHT + f"\nDropout rate minimum:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_dropout_min}")
                dropout_min_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter dropout_min or 'c' to cancel: ").strip()
                if dropout_min_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSearch space configuration cancelled")
                    return None
                dropout_min = float(dropout_min_input) if dropout_min_input else default_dropout_min
                if dropout_min < 0 or dropout_min > 1:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a value between 0 and 1.")
                    dropout_min = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSearch space configuration interrupted!")
                return None
        
        dropout_max = None
        while dropout_max is None:
            try:
                default_dropout_max = system_defaults["dropout_max"]
                print(Fore.CYAN + Style.BRIGHT + f"\nDropout rate maximum:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_dropout_max}")
                dropout_max_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter dropout_max or 'c' to cancel: ").strip()
                if dropout_max_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSearch space configuration cancelled")
                    return None
                dropout_max = float(dropout_max_input) if dropout_max_input else default_dropout_max
                if dropout_max < dropout_min or dropout_max > 1:
                    print(Fore.RED + Style.BRIGHT + f"\nMaximum must be between {dropout_min} and 1")
                    dropout_max = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSearch space configuration interrupted!")
                return None
        
        # Weight decay range
        weight_decay_min = None
        while weight_decay_min is None:
            try:
                default_weight_decay_min = search_space_config.get('weight_decay_min', 1e-6)
                print(Fore.CYAN + Style.BRIGHT + f"\nWeight decay minimum:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_weight_decay_min}")
                weight_decay_min_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter weight_decay_min or 'c' to cancel: ").strip()
                if weight_decay_min_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSearch space configuration cancelled")
                    return None
                weight_decay_min = float(weight_decay_min_input) if weight_decay_min_input else default_weight_decay_min
                if weight_decay_min <= 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
                    weight_decay_min = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSearch space configuration interrupted!")
                return None
        
        weight_decay_max = None
        while weight_decay_max is None:
            try:
                default_weight_decay_max = search_space_config.get('weight_decay_max', 1e-2)
                print(Fore.CYAN + Style.BRIGHT + f"\nWeight decay maximum:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_weight_decay_max}")
                weight_decay_max_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter weight_decay_max or 'c' to cancel: ").strip()
                if weight_decay_max_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSearch space configuration cancelled")
                    return None
                weight_decay_max = float(weight_decay_max_input) if weight_decay_max_input else default_weight_decay_max
                if weight_decay_max <= weight_decay_min:
                    print(Fore.RED + Style.BRIGHT + f"\nMaximum must be greater than minimum ({weight_decay_min})")
                    weight_decay_max = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSearch space configuration interrupted!")
                return None
        
        print(Fore.MAGENTA + Style.BRIGHT + "SYSTEM CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # System configuration
        device = None
        while device is None:
            try:
                default_device = 'cuda' if cuda_available else 'cpu'
                print(Fore.CYAN + Style.BRIGHT + f"\nDevice type configuration:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_device}")
                device_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter device type or 'c' to cancel: ").strip()
                if device_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSystem configuration cancelled")
                    return None
                device = device_input if device_input else default_device
                if device not in ['auto', 'cpu', 'cuda']:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter 'auto', 'cpu', or 'cuda'")
                    device = None
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSystem configuration interrupted!")
                return None
        
        random_seed = None
        while random_seed is None:
            try:
                default_random_seed = current_system_config.get('random_seed', 42)
                print(Fore.CYAN + Style.BRIGHT + f"\nRandom seed configuration:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_random_seed}")
                random_seed_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter random seed or 'c' to cancel: ").strip()
                if random_seed_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSystem configuration cancelled")
                    return None
                random_seed = int(random_seed_input) if random_seed_input else default_random_seed
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSystem configuration interrupted!")
                return None
        
        num_workers = None
        while num_workers is None:
            try:
                default_workers = system_defaults["workers"]
                print(Fore.CYAN + Style.BRIGHT + f"\nNumber of workers:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default recommended for HPO: " + Fore.GREEN + Style.BRIGHT + f"{default_workers}")
                num_workers_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter number of workers or 'c' to cancel: ").strip()
                if num_workers_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nSystem configuration cancelled")
                    return None
                num_workers = int(num_workers_input) if num_workers_input else default_workers
                if num_workers < 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a non-negative number.")
                    num_workers = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSystem configuration interrupted!")
                return None
        
        print(Fore.MAGENTA + Style.BRIGHT + "EARLY STOPPING AND PRUNING")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Early stopping configuration
        early_stopping_patience = None
        while early_stopping_patience is None:
            try:
                default_early_stopping_patience = early_stopping_config.get('patience', 10)
                print(Fore.CYAN + Style.BRIGHT + f"\nEarly stopping patience:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_early_stopping_patience}")
                early_stopping_patience_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter HPO early_stopping_patience or 'c' to cancel: ").strip()
                if early_stopping_patience_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nEarly stopping configuration cancelled")
                    return None
                early_stopping_patience = int(early_stopping_patience_input) if early_stopping_patience_input else default_early_stopping_patience
                if early_stopping_patience < 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a non-negative number.")
                    early_stopping_patience = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nEarly stopping configuration interrupted!")
                return None
        
        early_stopping_min_trials = None
        while early_stopping_min_trials is None:
            try:
                early_stopping_min_trials_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter minimum trials before early stopping (20) or 'c' to cancel: ").strip()
                if early_stopping_min_trials_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nEarly stopping configuration cancelled")
                    return None
                early_stopping_min_trials = int(early_stopping_min_trials_input) if early_stopping_min_trials_input else 20
                if early_stopping_min_trials < 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a non-negative number.")
                    early_stopping_min_trials = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nEarly stopping configuration interrupted!")
                return None
        
        print(Fore.MAGENTA + Style.BRIGHT + "OUTPUT AND STORAGE")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Output configuration
        verbose = None
        while verbose is None:
            try:
                verbose_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter verbose output? (Y/n) or 'c' to cancel: ").strip().lower()
                if verbose_input == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nOutput configuration cancelled")
                    return None
                verbose = verbose_input in ('', 'y', 'yes')
                break
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nOutput configuration interrupted!")
                return None
        
        save_study = enable_storage if enable_storage is not None else None
        while save_study is None:
            try:
                save_study_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter save study results? (Y/n) or 'c' to cancel: ").strip().lower()
                if save_study_input == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nStorage configuration cancelled")
                    return None
                save_study = save_study_input in ('', 'y', 'yes')
                break
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nStorage configuration interrupted!")
                return None
        
        generate_plots = enable_plots if enable_plots is not None else None
        while generate_plots is None:
            try:
                generate_plots_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter generate optimization plots? (Y/n) or 'c' to cancel: ").strip().lower()
                if generate_plots_input == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nPlotting configuration cancelled")
                    return None
                generate_plots = generate_plots_input in ('', 'y', 'yes')
                break
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nPlotting configuration interrupted!")
                return None
        
        study_dir = None
        while study_dir is None:
            try:
                default_study_dir = Path(DEFAULT_MODEL_DIR / 'hpo_studies')
                print(Fore.CYAN + Style.BRIGHT + f"\nStudy directory:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_study_dir}")
                study_dir_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter study directory or 'c' to cancel: ").strip()
                if study_dir_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nDirectory configuration cancelled")
                    return None
                study_dir = Path(study_dir_input) if study_dir_input else DEFAULT_MODEL_DIR / "hpo_studies"
                break
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nDirectory configuration interrupted!")
                return None
        
        print(Fore.MAGENTA + Style.BRIGHT + "ADVANCED OPTIONS")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Advanced options
        storage_url_input = storage_url  # Start with provided storage_url
        if storage_url_input is None:
            while storage_url_input is None:
                try:
                    default_storage_url = current_storage_config.get('url', f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/study.db")
                    print(Fore.CYAN + Style.BRIGHT + f"\nStorage URL:")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_storage_url}")
                    storage_url_prompt = input(Fore.YELLOW + Style.BRIGHT + "\nEnter database storage URL (optional) or 'c' to cancel: ").strip()
                    if storage_url_prompt.lower() == 'c':
                        print(Fore.RED + Style.BRIGHT + "\nStorage configuration cancelled")
                        return None
                    storage_url_input = storage_url_prompt if storage_url_prompt else default_storage_url
                    break
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nStorage configuration interrupted!")
                    return None
        
        load_if_exists = None
        while load_if_exists is None:
            try:
                default_load_if_exists = current_storage_config.get('load_if_exists', False)
                print(Fore.CYAN + Style.BRIGHT + f"\nLoad existing study:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_load_if_exists}")
                load_if_exists_input = input(Fore.YELLOW + Style.BRIGHT + "\nLoad existing study if found? (y/N) or 'c' to cancel: ").strip().lower()
                if load_if_exists_input == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nStudy loading configuration cancelled")
                    return None
                load_if_exists = load_if_exists_input in ('y', 'yes')
                break
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nStudy loading configuration interrupted!")
                return None
        
        parallel_jobs = None
        while parallel_jobs is None:
            try:
                default_parallel = min(4, max(1, cpu_cores // 2))  # Conservative default
                print(Fore.CYAN + Style.BRIGHT + f"\nParallel jobs:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_parallel}")
                parallel_jobs_input = input(Fore.YELLOW + Style.BRIGHT + f"\nEnter parallel_jobs or 'c' to cancel: ").strip()
                if parallel_jobs_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nParallel configuration cancelled")
                    return None
                parallel_jobs = int(parallel_jobs_input) if parallel_jobs_input else default_parallel
                if parallel_jobs < 1:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
                    parallel_jobs = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nParallel configuration interrupted!")
                return None
        
        memory_limit = None
        while memory_limit is None:
            try:
                default_memory_limit = current_hardware_config.get('memory_management', {}).get('memory_limit', None)
                print(Fore.CYAN + Style.BRIGHT + f"\nMemory limit:")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Default: " + Fore.GREEN + Style.BRIGHT + f"{default_memory_limit}")
                memory_limit_input = input(Fore.YELLOW + Style.BRIGHT + "\nEnter memory limit (optional, e.g., '8GB') or 'c' to cancel: ").strip()
                if memory_limit_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nMemory configuration cancelled")
                    return None
                memory_limit = memory_limit_input if memory_limit_input else default_memory_limit
                break
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nMemory configuration interrupted!")
                return None
        
        # Build HPO configuration
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        
        # Update with custom settings
        hpo_config.update({
            'enabled': True,
            'strategy': hpo_config.get('strategy', 'optuna'),
            'n_trials': n_trials,
            'timeout': timeout_seconds_final,
            'timeout_seconds': timeout_seconds_final,
            'study_name': study_name_input,
            'direction': direction,
            'sampler': sampler_type,
            'pruner': pruner_type,
            'objective_metric': hpo_config.get('objective_metric', 'validation_loss'),
            
            # Custom trial configuration
            'trial_epochs': trial_epochs,
            'trial_patience': trial_patience,
            'trial_batch_size': trial_batch_size,
            
            # Model search configuration
            'model_search': {
                'enabled': len(selected_models) > 1,
                'model_types': selected_models,
                'search_all_models': search_all_models
            },
            
            # Cross-validation settings
            'cross_validation': {
                'enabled': cv_folds > 1,
                'folds': cv_folds,
                'shuffle': cv_shuffle,
                'random_state': cv_random_state
            },
            
            # optimization space
            'optimization_space': custom_search_space or {
                'learning_rate': {'type': 'float', 'low': lr_min, 'high': lr_max, 'log': True},
                'batch_size': {'type': 'categorical', 'choices': batch_sizes},
                'encoding_dim': {'type': 'int', 'low': encoding_dim_min, 'high': encoding_dim_max},
                'dropout_rate': {'type': 'float', 'low': dropout_min, 'high': dropout_max},
                'weight_decay': {'type': 'float', 'low': weight_decay_min, 'high': weight_decay_max, 'log': True}
            },
            
            # Early stopping configuration
            'early_stopping': {
                'enabled': True,
                'patience': early_stopping_patience,
                'min_improvement': 1e-4,
                'min_trials': early_stopping_min_trials
            },
            
            # Advanced settings
            'cleanup_trials': True,
            'generate_plots': generate_plots,
            
            # search space for compatibility
            'search_space': {
                'encoding_dim_min': encoding_dim_min,
                'encoding_dim_max': encoding_dim_max,
                'hidden_layers_min': 1,
                'hidden_layers_max': 3,
                'lr_min': lr_min,
                'lr_max': lr_max,
                'batch_sizes': batch_sizes,
                'weight_decay_min': weight_decay_min,
                'weight_decay_max': weight_decay_max,
                'dropout_min': dropout_min,
                'dropout_max': dropout_max,
                'activations': ["relu", "leaky_relu", "gelu", "tanh"],
                'normalizations': [None, "batch", "layer"],
                'percentile_min': 90,
                'percentile_max': 99
            },
            
            # Sampler configuration
            'sampler_config': {
                'type': sampler_type,
                'seed': random_seed,
                'consider_prior': sampler_type == 'TPESampler',
                'prior_weight': 1.0,
                'consider_magic_clip': sampler_type == 'TPESampler',
                'consider_endpoints': False,
                'n_startup_trials': 10 if sampler_type == 'TPESampler' else 5,
                'n_ei_candidates': 24 if sampler_type == 'TPESampler' else 10,
                'multivariate': sampler_type == 'TPESampler'
            },
            
            # Pruner configuration
            'pruner_config': {
                'type': pruner_type,
                'n_startup_trials': 5 if pruner_type != 'NopPruner' else 0,
                'n_warmup_steps': 10 if pruner_type == 'MedianPruner' else 5,
                'interval_steps': 1,
                'min_resource': 1 if pruner_type == 'HyperbandPruner' else None,
                'max_resource': 'auto' if pruner_type == 'HyperbandPruner' else None,
                'reduction_factor': 3 if pruner_type == 'HyperbandPruner' else None
            },
            
            # Scoring configuration
            'scoring': {
                'use_composite_score': False,
                'validation_weight': 0.7,
                'test_weight': 0.2,
                'complexity_weight': 0.1,
                'max_params_penalty': 100000
            },
            
            # Storage configuration
            'storage': {
                'enabled': save_study,
                'url': storage_url_input or f"sqlite:///{study_dir}/custom_study_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db",
                'load_if_exists': load_if_exists,
                'heartbeat_interval': 60,
                'grace_period': 120
            }
        })
        
        # Data configuration
        data_config_section = final_config.setdefault('data', {})
        data_config_section.update({
            'use_real_data': use_real_data,
            'data_path': data_path,
            'artifacts_path': artifacts_path,
            'normal_samples': normal_samples if not use_real_data else data_config_section.get('normal_samples'),
            'attack_samples': attack_samples if not use_real_data else data_config_section.get('attack_samples'),
            'features': features if not use_real_data else data_config_section.get('features'),
            'validation_split': data_config_section.get('validation_split', 0.2),
            'test_split': data_config_section.get('test_split', 0.2),
            'normalization': data_config_section.get('normalization', 'standard'),
            'random_state': cv_random_state
        })
        
        # Training configuration adjustments for HPO
        training_config = final_config.setdefault('training', {})
        training_config.update({
            'num_workers': num_workers,
            'pin_memory': False,  # Disabled for HPO stability
            'persistent_workers': False
        })
        
        # System configuration
        system_config = final_config.setdefault('system', {})
        system_config.update({
            'device': device,
            'random_seed': random_seed,
            'reproducible': True,
            'non_interactive': non_interactive,
            'verbose': verbose,
            'parallel_processing': parallel_jobs > 1,
            'max_workers': parallel_jobs,
            'memory_limit': memory_limit
        })
        
        # Update metadata to reflect custom setup
        metadata_section = final_config.setdefault('metadata', {})
        metadata_section.update({
            'last_modified': datetime.now().isoformat(),
            'setup_method': 'custom_hpo',
            'custom_configuration': {
                'optimization_goal': 'custom',
                'data_source': 'real' if use_real_data else 'synthetic',
                'model_types': selected_models,
                'validation_strategy': f"{cv_folds}_fold_cv" if cv_folds > 1 else "single_split",
                'sampler': sampler_type,
                'pruner': pruner_type,
                'search_space_customized': True,
                'estimated_duration_minutes': timeout_minutes_input if timeout_minutes_input > 0 else 'unlimited',
                'system_class_aligned': system_class,
                'preset_compatibility': current_preset_name
            },
            'parameter_overrides_applied': len(applied_overrides),
            'optimization_focus': optimization_focus or metadata_section.get('optimization_focus', 'balanced'),
            'system_class_used': system_class,
            'hardware_context': {
                'cuda_available': cuda_available,
                'gpu_count': gpu_count,
                'memory_gb': memory_gb,
                'cpu_cores': cpu_cores
            }
        })
        
        # Update presets section
        presets_section = final_config.setdefault('presets', {})
        presets_section['current_preset'] = 'custom_configuration'
        presets_section['custom_parameters'] = {
            'trial_count': n_trials,
            'timeout_seconds': timeout_seconds_final,
            'sampler_type': sampler_type,
            'pruner_type': pruner_type,
            'data_mode': data_mode,
            'model_types': selected_models,
            'system_class': system_class,
            'base_preset': current_preset_name
        }
        
        # HPO Configuration Review
        print(Fore.MAGENTA + Style.BRIGHT + "\nCUSTOM HPO CONFIGURATION SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Study Configuration Section
        print(Fore.MAGENTA + Style.BRIGHT + "Study Configuration:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Study Name: " + Fore.YELLOW + Style.BRIGHT + f"{study_name_input}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Optimization: " + Fore.YELLOW + Style.BRIGHT + f"{direction.upper()} over {n_trials} trials")
        if timeout_minutes_input > 0:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{timeout_minutes_input} minutes ({timeout_seconds_final} seconds)")
        else:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"No limit - run all {n_trials} trials")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Objective Metric: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_config.get('objective_metric', 'validation_loss')}")
        
        # Optimization Strategy Section
        print(Fore.MAGENTA + Style.BRIGHT + "\nOptimization Strategy:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{sampler_type}")
        sampler_desc = {
            'TPESampler': 'Tree-structured Parzen Estimator (Recommended)',
            'RandomSampler': 'Random Sampling (Baseline)',
            'CmaEsSampler': 'Covariance Matrix Adaptation (Advanced)',
            'GridSampler': 'Grid Search (Exhaustive)',
            'NSGAIISampler': 'Multi-objective optimization (For multiple objectives)'
        }
        print(Fore.GREEN + Style.BRIGHT + f"  │  └─ " + Fore.CYAN + Style.BRIGHT + f"{sampler_desc.get(sampler_type, 'Custom')}")
        
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{pruner_type}")
        pruner_desc = {
            'MedianPruner': 'Median-based Pruning (Balanced)',
            'HyperbandPruner': 'Successive Halving (Aggressive)',
            'NopPruner': 'No Pruning (Complete All Trials)',
            'PercentilePruner': 'Prune trials below a certain percentile (Customizable)',
            'SuccessiveHalvingPruner': 'Iteratively prune worst-performing trials (Efficient)'
        }
        print(Fore.GREEN + Style.BRIGHT + f"  │  └─ " + Fore.CYAN + Style.BRIGHT + f"{pruner_desc.get(pruner_type, 'Custom')}")
        
        # Early stopping details
        if pruner_type != 'NopPruner':
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Early Stopping: " + Fore.YELLOW + Style.BRIGHT + f"Enabled (patience: {early_stopping_patience})")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Min Trials: " + Fore.YELLOW + Style.BRIGHT + f"{early_stopping_min_trials} before early stopping")
        else:
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Early Stopping: " + Fore.YELLOW + Style.BRIGHT + f"Disabled (No Pruning)")
        
        # Model Configuration Section
        print(Fore.MAGENTA + Style.BRIGHT + "\nModel Configuration:")
        model_descriptions = {
            'SimpleAutoencoder': 'Fast & Lightweight',
            'EnhancedAutoencoder': 'Balanced Performance',
            'AutoencoderEnsemble': 'High Performance'
        }
        
        if len(selected_models) == 1:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Single Model: " + Fore.YELLOW + Style.BRIGHT + f"{selected_models[0]}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Type: " + Fore.CYAN + Style.BRIGHT + f"{model_descriptions.get(selected_models[0], 'Custom')}")
        else:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Multi-Model Search: " + Fore.YELLOW + Style.BRIGHT + f"Enabled")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Models: " + Fore.YELLOW + Style.BRIGHT + f"{len(selected_models)} models")
            for i, model in enumerate(selected_models, 1):
                status = "✓" if model in selected_models else "✗"
                print(Fore.GREEN + Style.BRIGHT + f"  │  {i}. {status} {model}" + 
                      Fore.CYAN + Style.BRIGHT + f" - {model_descriptions.get(model, 'Custom')}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Search Strategy: " + 
                  Fore.YELLOW + Style.BRIGHT + f"{'Comprehensive' if search_all_models else 'Targeted'}")
        
        # Data Configuration Section
        print(Fore.MAGENTA + Style.BRIGHT + "\nData Configuration:")
        if use_real_data:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"Real Network Data")
            if data_path:
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data Path: " + Fore.YELLOW + Style.BRIGHT + f"{data_path}")
            if artifacts_path:
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Artifacts Path: " + Fore.YELLOW + Style.BRIGHT + f"{artifacts_path}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Type: " + Fore.CYAN + Style.BRIGHT + f"Production-ready dataset")
        else:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"Synthetic Data")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.YELLOW + Style.BRIGHT + f"{normal_samples:,}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.YELLOW + Style.BRIGHT + f"{attack_samples:,}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Features: " + Fore.YELLOW + Style.BRIGHT + f"{features}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Type: " + Fore.CYAN + Style.BRIGHT + f"Generated test dataset")
        
        # Validation Strategy Section
        print(Fore.MAGENTA + Style.BRIGHT + "\nValidation Strategy:")
        if cv_folds > 1:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Method: " + Fore.YELLOW + Style.BRIGHT + f"{cv_folds}-Fold Cross-Validation")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Shuffle: " + Fore.YELLOW + Style.BRIGHT + f"{'Enabled' if cv_shuffle else 'Disabled'}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Random State: " + Fore.YELLOW + Style.BRIGHT + f"{cv_random_state}")
            robustness = "High" if cv_folds >= 5 else "Medium" if cv_folds >= 3 else "Basic"
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Robustness: " + Fore.CYAN + Style.BRIGHT + f"{robustness}")
        else:
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Method: " + Fore.YELLOW + Style.BRIGHT + f"Single Split Validation")
        
        # Trial Configuration Section
        print(Fore.MAGENTA + Style.BRIGHT + "\nTrial Configuration:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs per Trial: " + Fore.YELLOW + Style.BRIGHT + f"{trial_epochs}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Early Stopping: " + Fore.YELLOW + Style.BRIGHT + f"Patience {trial_patience}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.YELLOW + Style.BRIGHT + f"{trial_batch_size}")
        
        # Calculate trial complexity
        trial_complexity = "Low"
        if trial_epochs > 50:
            trial_complexity = "Very High"
        elif trial_epochs > 30:
            trial_complexity = "High"
        elif trial_epochs > 15:
            trial_complexity = "Medium"
        
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Complexity: " + Fore.CYAN + Style.BRIGHT + f"{trial_complexity}")
        
        # Search Space Configuration Section
        print(Fore.MAGENTA + Style.BRIGHT + "\nSearch Space Configuration:")
        
        # Learning Rate
        lr_range = f"{lr_min:.2e} to {lr_max:.2e}"
        lr_log = " (log scale)" if hpo_config['optimization_space']['learning_rate'].get('log', True) else ""
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.YELLOW + Style.BRIGHT + f"{lr_range}{lr_log}")
        
        # Batch Sizes
        batch_size_str = ", ".join(map(str, batch_sizes))
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Batch Sizes: " + Fore.YELLOW + Style.BRIGHT + f"[{batch_size_str}]")
        
        # Encoding Dimension
        encoding_range = f"{encoding_dim_min} to {encoding_dim_max}"
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Encoding Dim: " + Fore.YELLOW + Style.BRIGHT + f"{encoding_range}")
        
        # Dropout Rate
        dropout_range = f"{dropout_min} to {dropout_max}"
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Dropout Rate: " + Fore.YELLOW + Style.BRIGHT + f"{dropout_range}")
        
        # Weight Decay
        wd_range = f"{weight_decay_min:.2e} to {weight_decay_max:.2e}"
        wd_log = " (log scale)" if hpo_config['optimization_space']['weight_decay'].get('log', True) else ""
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Weight Decay: " + Fore.YELLOW + Style.BRIGHT + f"{wd_range}{wd_log}")
        
        # Calculate search space size
        param_combinations = len(batch_sizes) * (encoding_dim_max - encoding_dim_min + 1)
        search_space_size = "Small" if param_combinations < 50 else "Medium" if param_combinations < 200 else "Large"
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Search Space: " + Fore.CYAN + Style.BRIGHT + f"{search_space_size} ({param_combinations}+ combinations)")
        
        # System Configuration Section
        print(Fore.MAGENTA + Style.BRIGHT + "\nSystem Configuration:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Device: " + Fore.YELLOW + Style.BRIGHT + f"{device.upper()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Workers: " + Fore.YELLOW + Style.BRIGHT + f"{num_workers}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Random Seed: " + Fore.YELLOW + Style.BRIGHT + f"{random_seed}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Parallel Jobs: " + Fore.YELLOW + Style.BRIGHT + f"{parallel_jobs}")
        
        if memory_limit:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Memory Limit: " + Fore.YELLOW + Style.BRIGHT + f"{memory_limit}")
        
        # System capability assessment
        capability = "Balanced"
        if parallel_jobs > 4:
            capability = "High Performance"
        elif parallel_jobs == 1:
            capability = "Single-threaded"
        
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Capability: " + Fore.CYAN + Style.BRIGHT + f"{capability}")
        
        # Storage and Output Section
        print(Fore.MAGENTA + Style.BRIGHT + "\nStorage and Output:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Study Storage: " + Fore.YELLOW + Style.BRIGHT + f"{'Enabled' if save_study else 'Disabled'}")
        if save_study and storage_url_input:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Database: " + Fore.YELLOW + Style.BRIGHT + f"{storage_url_input}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Load Existing: " + Fore.YELLOW + Style.BRIGHT + f"{'Yes' if load_if_exists else 'No'}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Generate Plots: " + Fore.YELLOW + Style.BRIGHT + f"{'Yes' if generate_plots else 'No'}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Verbose Output: " + Fore.YELLOW + Style.BRIGHT + f"{'Yes' if verbose else 'No'}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Study Directory: " + Fore.YELLOW + Style.BRIGHT + f"{study_dir}")
        
        # Performance and Resource Estimation Section
        print(Fore.MAGENTA + Style.BRIGHT + "\nPerformance and Resource Estimation:")
        
        # Calculate estimated duration
        estimated_time = _estimate_hpo_time(n_trials=n_trials, trial_epochs=trial_epochs, n_model_types=len(selected_models), cv_folds=cv_folds, hardware_info=hardware_data, system_class=system_class)
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Estimated Duration: " + Fore.YELLOW + Style.BRIGHT + f"~{estimated_time}")
        
        # Calculate resource requirements
        total_training_ops = n_trials * trial_epochs * cv_folds * len(selected_models)
        resource_level = "Light" if total_training_ops < 1000 else "Moderate" if total_training_ops < 5000 else "Heavy"
        
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Training Ops: " + Fore.YELLOW + Style.BRIGHT + f"{total_training_ops:,}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Resource Level: " + Fore.CYAN + Style.BRIGHT + f"{resource_level}")
        
        # Success probability estimation
        success_prob = "High" if n_trials >= 100 and cv_folds >= 3 else "Medium" if n_trials >= 50 else "Exploratory"
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Success Probability: " + Fore.CYAN + Style.BRIGHT + f"{success_prob}")
        
        # Configuration Summary Flags
        print(Fore.MAGENTA + Style.BRIGHT + "\nConfiguration Summary Flags:")
        
        flags = []
        if n_trials >= 100:
            flags.append("Comprehensive Search")
        elif n_trials <= 30:
            flags.append("Quick Exploration")
            
        if cv_folds >= 5:
            flags.append("Robust Validation")
            
        if len(selected_models) > 1:
            flags.append("Multi-Model")
            
        if timeout_minutes_input == 0:
            flags.append("No Time Limit")
            
        if parallel_jobs > 1:
            flags.append(f"Parallel ({parallel_jobs}x)")
            
        if not use_real_data:
            flags.append("Synthetic Data")
            
        if system_class != "baseline":
            flags.append(f"{system_class.title()} Optimized")
            
        if not flags:
            flags.append("Balanced Configuration")
            
        flags_display = " - ".join(flags)
        print(Fore.CYAN + Style.BRIGHT + f"  {flags_display}")
        
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-" * 40)
        
        # Final confirmation with options
        confirm = None
        while not confirm:
            try:
                confirm_input = input(Fore.YELLOW + Style.BRIGHT + "\nStart hyperparameter optimization with this configuration? (Y/n/c to cancel): ").strip().lower()
                
                if confirm_input in ('', 'y', 'yes'):
                    print(Fore.GREEN + Style.BRIGHT + "\nLaunching comprehensive hyperparameter optimization...")
                    
                    # Ensure HPO-friendly system settings
                    system_config.update({
                        'non_interactive': non_interactive,
                        'verbose': verbose,
                        'hpo_optimized': True,
                        'system_class': system_class
                    })
                    
                    # Update runtime information
                    runtime_config = final_config.setdefault('runtime', {})
                    runtime_config.update({
                        'setup_method': 'custom_hpo',
                        'active_preset': 'custom_configuration',
                        'hpo_launch_time': datetime.now().isoformat(),
                        'parameter_context': {
                            'operation_mode': operation_mode,
                            'use_current_config': use_current_config,
                            'force_express': force_express,
                            'skip_prompt': skip_prompt
                        }
                    })
                    
                    return _launch_hpo_with_config(config=final_config, **kwargs)
                elif confirm_input in ('c', 'cancel'):
                    print(Fore.RED + Style.BRIGHT + "HPO cancelled")
                    return None
                elif confirm_input in ('n', 'no'):
                    print(Fore.YELLOW + Style.BRIGHT + "\nWould you like to:")
                    print(Fore.WHITE + Style.BRIGHT + "1. Try again with different settings")
                    print(Fore.WHITE + Style.BRIGHT + "2. Switch to preset configuration")
                    print(Fore.WHITE + Style.BRIGHT + "3. Switch to express setup")
                    print(Fore.RED + Style.BRIGHT + "0. Return to previous menu")
                    
                    retry_choice = None
                    while not retry_choice:
                        try:
                            retry_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-3): ").strip()
                            
                            if retry_choice not in ['1', '2', '3', '0']:
                                print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
                                retry_choice = None
                                continue
                                
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOption selection interrupted!")
                            return None
                    
                    if retry_choice == '1':
                        return _interactive_hpo_custom_setup(
                            base_config,
                            data_mode=data_mode,
                            hardware_data=hardware_data,
                            enable_storage=enable_storage,
                            enable_plots=enable_plots,
                            custom_search_space=custom_search_space,
                            sampler_type=sampler_type,
                            pruner_type=pruner_type,
                            trial_count=trial_count,
                            timeout_seconds=timeout_seconds,
                            optimization_focus=optimization_focus,
                            study_name=study_name,
                            storage_url=storage_url,
                            non_interactive=non_interactive,
                            skip_prompt=skip_prompt,
                            operation_mode=operation_mode,
                            use_current_config=use_current_config,
                            force_express=force_express,
                            model_types=model_types,
                            **kwargs
                        )
                    elif retry_choice == '2':
                        return _interactive_hpo_preset_setup(
                            base_config,
                            data_mode=data_mode,
                            hardware_data=hardware_data,
                            enable_storage=enable_storage,
                            enable_plots=enable_plots,
                            custom_search_space=custom_search_space,
                            sampler_type=sampler_type,
                            pruner_type=pruner_type,
                            trial_count=trial_count,
                            timeout_seconds=timeout_seconds,
                            optimization_focus=optimization_focus,
                            study_name=study_name,
                            storage_url=storage_url,
                            non_interactive=non_interactive,
                            skip_prompt=skip_prompt,
                            operation_mode=operation_mode,
                            use_current_config=use_current_config,
                            force_express=force_express,
                            model_types=model_types,
                            **kwargs
                        )
                    elif retry_choice == '3':
                        return _interactive_hpo_express_setup(
                            base_config,
                            data_mode=data_mode,
                            hardware_data=hardware_data,
                            enable_storage=enable_storage,
                            enable_plots=enable_plots,
                            custom_search_space=custom_search_space,
                            sampler_type=sampler_type,
                            pruner_type=pruner_type,
                            trial_count=trial_count,
                            timeout_seconds=timeout_seconds,
                            optimization_focus=optimization_focus,
                            study_name=study_name,
                            storage_url=storage_url,
                            non_interactive=non_interactive,
                            skip_prompt=skip_prompt,
                            operation_mode=operation_mode,
                            use_current_config=use_current_config,
                            force_express=force_express,
                            model_types=model_types,
                            **kwargs
                        )
                    else:
                        print(Fore.RED + Style.BRIGHT + "\nReturning to previous menu")
                        return None
                else:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter Y, N, or C")
                    confirm = None
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nConfirmation interrupted!")
                return None
                
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\nCustom HPO setup interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"Custom HPO setup failed: {e}", exc_info=True)
        
        # Error context
        error_context = {
            "Current Preset": current_preset_name if 'current_preset_name' in locals() else 'Unknown',
            "Model Type": model_type if 'model_type' in locals() else 'Unknown',
            "System Class": system_class if 'system_class' in locals() else 'Unknown',
            "Data Mode": data_mode if data_mode else 'Not specified',
            "Operation Mode": operation_mode if operation_mode else 'Not specified',
            "Trial Count Override": trial_count if trial_count else 'Not specified',
            "Timeout Override": f"{timeout_seconds}s" if timeout_seconds else 'Not specified',
            "Sampler Override": sampler_type if sampler_type else 'Not specified',
            "Pruner Override": pruner_type if pruner_type else 'Not specified',
            "Model Types": len(model_types) if model_types else 'Not specified'
        }
        
        message = (
            f"Custom HPO setup failed: {str(e)}\n\n"
            f"Context:\n" +
            "\n".join([f"├─ {key}: {value}" for key, value in list(error_context.items())[:-1]]) +
            f"\n└─ {list(error_context.items())[-1][0]}: {list(error_context.items())[-1][1]}" +
            f"\n\nThis could be due to:\n"
            f"├─ Invalid input parameters\n"
            f"├─ Configuration merge conflicts\n"
            f"├─ System resource constraints\n"
            f"├─ Data configuration issues\n"
            f"├─ Parameter override conflicts\n"
            f"└─ Model compatibility issues"
        )
        
        print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
        print(Fore.RED + Style.BRIGHT + "CUSTOM HPO SETUP ERROR")
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + message)
        print(Fore.RED + Style.BRIGHT + "-" * 40)
        
        return None

def _interactive_hpo_continue_setup(
    base_config: Dict[str, Any],
    study_name: Optional[str] = None,
    storage_url: Optional[str] = None,
    hardware_data: Optional[Dict[str, Any]] = None,
    data_mode: Optional[str] = None,
    enable_storage: Optional[bool] = None,
    enable_plots: Optional[bool] = None,
    custom_search_space: Optional[Dict[str, Any]] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    trial_count: Optional[int] = None,
    timeout_seconds: Optional[int] = None,
    optimization_focus: Optional[str] = None,
    non_interactive: bool = False,
    skip_prompt: bool = False,
    operation_mode: Optional[str] = None,
    use_current_config: bool = False,
    force_express: bool = False,
    model_types: Optional[List[str]] = None,
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Enhanced setup to continue an existing HPO study with full parameter compatibility.
    
    Args:
        base_config: Base configuration to use
        study_name: Specific study name for continuation
        storage_url: Storage URL for study continuation
        hardware_data: Pre-fetched hardware data for optimization
        data_mode: Data source mode ('synthetic', 'real', 'auto')
        enable_storage: Override storage settings from preset
        enable_plots: Override plot generation settings from preset
        custom_search_space: Override search space from preset
        sampler_type: Override sampler type from preset
        pruner_type: Override pruner type from preset
        trial_count: Override number of trials
        timeout_seconds: Override timeout in seconds
        optimization_focus: Optimization objective focus ('accuracy', 'speed', 'balanced')
        non_interactive: Run without user prompts
        skip_prompt: Skip confirmation prompts
        operation_mode: Specific operation mode
        use_current_config: Whether to use current configuration as base
        force_express: Force express mode even if other options specified
        model_types: List of model types for comparison
        **kwargs: Additional parameters passed from main function
        
    Returns:
        Dictionary with HPO results or None if cancelled/failed
    """
    try:
        # Clear screen and show banner with config retrieval
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if base_config is None and banner_config is not None:
            base_config = banner_config
        elif base_config is None:
            base_config = get_current_config()
        
        # Get hardware context if not provided
        if hardware_data is None:
            try:
                hardware_data = check_hardware(include_memory_usage=True)
            except Exception as e:
                logger.debug(f"Hardware detection failed: {e}")
                hardware_data = {}
        
        # Extract configuration context for display
        current_hpo_config = base_config.get('hyperparameter_optimization', {})
        data_config = base_config.get('data', {})
        model_config = base_config.get('model', {})
        training_config = base_config.get('training', {})
        metadata = base_config.get('metadata', {})
        presets_section = base_config.get('presets', {})
        
        # Context extraction using multiple fallbacks with preset compatibility
        current_preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        if isinstance(presets_section, dict):
            current_preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if current_preset_name in ["Custom/Default", None, ""]:
            metadata = base_config.get("metadata", {})
            if isinstance(metadata, dict):
                current_preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if current_preset_name in ["Custom/Default", None, ""]:
            current_preset_name = base_config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if current_preset_name in ["Custom/Default", None, ""]:
            runtime = base_config.get("runtime", {})
            if isinstance(runtime, dict):
                current_preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if current_preset_name in ["Custom/Default", None, "", "none"]:
            current_preset_name = "Custom/Default"
        elif isinstance(current_preset_name, str):
            current_preset_name = current_preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in base_config and isinstance(base_config["runtime"], dict):
            config_source = base_config["runtime"].get("config_source", "runtime")
        elif "metadata" in base_config and isinstance(base_config["metadata"], dict):
            config_source = base_config["metadata"].get("config_source", "metadata")
        else:
            config_source = "Unknown"
        
        # HPO-specific context from current preset configuration
        hpo_strategy = current_hpo_config.get('strategy', 'optuna')
        hpo_trials = current_hpo_config.get('n_trials', 50)
        hpo_timeout = current_hpo_config.get('timeout', 3600)
        hpo_enabled = current_hpo_config.get('enabled', False)
        hpo_sampler = current_hpo_config.get('sampler', 'TPESampler')
        hpo_pruner = current_hpo_config.get('pruner', 'MedianPruner')
        
        # Training configuration context
        epochs = training_config.get('epochs', 100)
        batch_size = training_config.get('batch_size', 64)
        learning_rate = training_config.get('learning_rate', 0.001)
        
        # Data configuration context
        normal_samples = data_config.get('normal_samples', 8000)
        attack_samples = data_config.get('attack_samples', 2000)
        features = data_config.get('features', 20)
        data_path = data_config.get('data_path', 'Default')
        use_real_data_config = data_config.get('use_real_data', False)
        
        # Resolve data mode from parameters
        if data_mode is None:
            if use_real_data_config:
                data_mode = 'real'
            else:
                data_mode = 'synthetic'
        
        # Apply parameter overrides with preset validation
        if trial_count is not None:
            hpo_trials = trial_count
        else:
            trial_count = hpo_trials

        if timeout_seconds is not None:
            hpo_timeout = timeout_seconds
        else:
            timeout_seconds = hpo_timeout
        
        # Hardware-aware system class detection
        cuda_available = hardware_data.get('cuda', {}).get('available', False)
        gpu_count = hardware_data.get('cuda', {}).get('gpu_count', 0)
        memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)
        
        # Determine system performance class
        if cuda_available and memory_gb >= 16 and cpu_cores >= 8:
            system_class = "high_performance"
        elif cuda_available and memory_gb >= 8:
            system_class = "performance"
        elif memory_gb >= 4:
            system_class = "standard"
        else:
            system_class = "limited"
        
        # Header with context awareness
        header_title = "CONTINUE EXISTING HPO STUDY"
        if operation_mode == 'continue':
            header_title = "STUDY CONTINUATION SETUP"
        
        print(Fore.MAGENTA + Style.BRIGHT + header_title)
        print(Fore.CYAN + Style.BRIGHT + "-" * 40 + Style.RESET_ALL)
        
        # Display current configuration context
        print(Fore.YELLOW + Style.BRIGHT + "Current Configuration Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.YELLOW + Style.BRIGHT + f"{current_preset_name.title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data Source: " + Fore.YELLOW + Style.BRIGHT + f"{data_path}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ HPO Strategy: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_strategy}")
        
        # Display extended parameters if provided
        extended_params = []
        if operation_mode:
            extended_params.append(f"Operation Mode: {operation_mode}")
        if data_mode and data_mode != 'auto':
            extended_params.append(f"Data Mode: {data_mode}")
        if optimization_focus:
            extended_params.append(f"Optimization Focus: {optimization_focus}")
        if trial_count and trial_count != hpo_trials:
            extended_params.append(f"Trial Count Override: {trial_count}")
        if timeout_seconds and timeout_seconds != hpo_timeout:
            extended_params.append(f"Timeout Override: {timeout_seconds}s")
        if study_name:
            extended_params.append(f"Study Name: {study_name}")
        if storage_url:
            extended_params.append(f"Storage URL: {storage_url}")
        if sampler_type and sampler_type != hpo_sampler:
            extended_params.append(f"Sampler Override: {sampler_type}")
        if pruner_type and pruner_type != hpo_pruner:
            extended_params.append(f"Pruner Override: {pruner_type}")
        if enable_storage is not None:
            extended_params.append(f"Storage Override: {enable_storage}")
        if enable_plots is not None:
            extended_params.append(f"Plots Override: {enable_plots}")
        if custom_search_space:
            extended_params.append("Custom Search Space: Provided")
        if model_types:
            extended_params.append(f"Model Types: {', '.join(model_types)}")
        
        if extended_params:
            print(Fore.CYAN + Style.BRIGHT + "\nParameter Overrides:")
            for i, param in enumerate(extended_params):
                prefix = "  └─" if i == len(extended_params) - 1 else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {param}")
        
        # Display hardware context
        print(Fore.MAGENTA + Style.BRIGHT + "\nHardware Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
        if cuda_available:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ GPU Count: " + Fore.YELLOW + Style.BRIGHT + f"{gpu_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Memory: " + Fore.YELLOW + Style.BRIGHT + f"{memory_gb:.1f}GB")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ CPU Cores: " + Fore.YELLOW + Style.BRIGHT + f"{cpu_cores}")
        
        print(Fore.CYAN + Style.BRIGHT + "\nContinue existing hyperparameter optimization study")
        print(Fore.YELLOW + Style.BRIGHT + "Press Enter for defaults shown in parentheses")
        print(Fore.RED + Style.BRIGHT + "Enter 'c' at any time to cancel and return to previous menu\n")
        
        # Handle non-interactive mode for study continuation
        if non_interactive:
            if study_name and storage_url:
                print(Fore.GREEN + Style.BRIGHT + f"\nNon-interactive Mode - Continuing study: {study_name}")
                print(Fore.YELLOW + Style.BRIGHT + f"Using storage: {storage_url}")
                
                # Build configuration for non-interactive continuation
                final_config = deepcopy(base_config)
                
                # Apply parameter overrides
                hpo_config = final_config.setdefault('hyperparameter_optimization', {})
                hpo_config.update({
                    'enabled': True,
                    'strategy': 'optuna',
                    'n_trials': trial_count or 50,
                    'timeout': timeout_seconds or 3600,
                    'study_name': study_name,
                    'load_if_exists': True,
                    'storage': {
                        'enabled': enable_storage if enable_storage is not None else True,
                        'url': storage_url,
                        'load_if_exists': True
                    }
                })
                
                # Apply other overrides
                if sampler_type:
                    hpo_config['sampler'] = sampler_type
                if pruner_type:
                    hpo_config['pruner'] = pruner_type
                if enable_plots is not None:
                    hpo_config['generate_plots'] = enable_plots
                if custom_search_space:
                    hpo_config.setdefault('optimization_space', {}).update(custom_search_space)
                
                return _launch_hpo_with_config(config=final_config, **kwargs)
            else:
                print(Fore.RED + Style.BRIGHT + "\nNon-interactive continuation requires study_name and storage_url")
                print(Fore.YELLOW + Style.BRIGHT + "Switching to interactive study selection...")
        
        # Look for existing studies in multiple locations
        study_base_dir = Path(DEFAULT_MODEL_DIR) / "hpo_studies"
        alternative_dirs = [
            Path("studies"),
            Path("optuna_studies"),
            Path("hpo"),
            CONFIG_DIR / "studies",
            Path.cwd() / "hpo_studies"
        ]
        
        # Check primary directory
        if not study_base_dir.exists():
            print(Fore.YELLOW + Style.BRIGHT + f"\nPrimary study directory not found at {study_base_dir}")
            
            # Check alternative directories
            found_dir = None
            for alt_dir in alternative_dirs:
                if alt_dir.exists() and (list(alt_dir.glob("*.db")) or list(alt_dir.glob("*_study.pkl"))):
                    found_dir = alt_dir
                    print(Fore.GREEN + Style.BRIGHT + f"\nFound studies in alternative directory: {found_dir}")
                    study_base_dir = found_dir
                    break
            
            if not found_dir:
                print(Fore.RED + Style.BRIGHT + "\nNo study directories found.")
                print(Fore.YELLOW + Style.BRIGHT + "\nSwitching to express setup...")
                return _interactive_hpo_express_setup(
                    base_config,
                    data_mode=data_mode,
                    hardware_data=hardware_data,
                    enable_storage=enable_storage,
                    enable_plots=enable_plots,
                    custom_search_space=custom_search_space,
                    sampler_type=sampler_type,
                    pruner_type=pruner_type,
                    trial_count=trial_count,
                    timeout_seconds=timeout_seconds,
                    optimization_focus=optimization_focus,
                    **kwargs
                )
        
        # Find study files (both database and pickle formats)
        study_db_files = list(study_base_dir.glob("*.db"))
        study_pkl_files = list(study_base_dir.glob("*_study.pkl"))
        
        # Combine and prioritize database files
        all_study_files = study_db_files + study_pkl_files
        
        if not all_study_files:
            print(Fore.RED + Style.BRIGHT + "\nNo existing studies found")
            print(Fore.YELLOW + Style.BRIGHT + "\nSwitching to express setup...")
            return _interactive_hpo_express_setup(
                base_config,
                data_mode=data_mode,
                hardware_data=hardware_data,
                enable_storage=enable_storage,
                enable_plots=enable_plots,
                custom_search_space=custom_search_space,
                sampler_type=sampler_type,
                pruner_type=pruner_type,
                trial_count=trial_count,
                timeout_seconds=timeout_seconds,
                optimization_focus=optimization_focus,
                **kwargs
            )
        
        print(Fore.GREEN + Style.BRIGHT + f"\nFound {len(all_study_files)} existing study files:")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        studies_info = []
        
        for i, study_file in enumerate(all_study_files, 1):
            try:
                study_info = {
                    'index': i,
                    'file': study_file,
                    'study': None,
                    'name': study_file.stem,
                    'file_type': 'database' if study_file.suffix == '.db' else 'pickle',
                    'trials': 0,
                    'complete': 0,
                    'pruned': 0,
                    'failed': 0,
                    'best_value': "No trials",
                    'error': None,
                    'accessible': False
                }
                
                # Try to load the study based on file type
                if study_file.suffix == '.db':
                    # Database format - try Optuna direct loading
                    try:
                        import optuna
                        storage_url = f"sqlite:///{study_file}"
                        
                        # Try to get study names from this database
                        try:
                            storage = optuna.storages.get_storage(storage_url)
                            study_names = storage.get_all_study_names()
                            
                            if study_names:
                                # Use the first study name (most common case)
                                study_name = study_names[0]
                                study = optuna.load_study(study_name=study_name, storage=storage_url)
                                
                                study_info.update({
                                    'study': study,
                                    'name': study_name,
                                    'trials': len(study.trials),
                                    'complete': len([t for t in study.trials if t.state.name == 'COMPLETE']),
                                    'pruned': len([t for t in study.trials if t.state.name == 'PRUNED']),
                                    'failed': len([t for t in study.trials if t.state.name == 'FAIL']),
                                    'best_value': study.best_value if study.trials and study_info['complete'] > 0 else "No completed trials",
                                    'accessible': True
                                })
                                
                                if len(study_names) > 1:
                                    study_info['multiple_studies'] = study_names
                                    
                            else:
                                study_info['error'] = "Database contains no studies"
                                
                        except Exception as load_error:
                            study_info['error'] = f"Could not load from database: {str(load_error)}"
                            
                    except ImportError:
                        study_info['error'] = "Optuna not available for database loading"
                    except Exception as e:
                        study_info['error'] = f"Database access failed: {str(e)}"
                        
                else:
                    # Pickle format - legacy support
                    try:
                        study = joblib.load(study_file)
                        study_info.update({
                            'study': study,
                            'name': getattr(study, 'study_name', study_file.stem),
                            'trials': len(study.trials),
                            'complete': len([t for t in study.trials if t.state.name == 'COMPLETE']),
                            'pruned': len([t for t in study.trials if t.state.name == 'PRUNED']),
                            'failed': len([t for t in study.trials if t.state.name == 'FAIL']),
                            'best_value': study.best_value if hasattr(study, 'best_value') and study.trials and study_info['complete'] > 0 else "No completed trials",
                            'accessible': True
                        })
                    except Exception as e:
                        study_info['error'] = f"Pickle loading failed: {str(e)}"
                
                # Add file metadata
                try:
                    file_stat = study_file.stat()
                    study_info.update({
                        'file_size_mb': file_stat.st_size / (1024 * 1024),
                        'last_modified': datetime.fromtimestamp(file_stat.st_mtime),
                        'creation_time': datetime.fromtimestamp(file_stat.st_ctime) if hasattr(file_stat, 'st_ctime') else None
                    })
                except Exception:
                    pass
                
                studies_info.append(study_info)
                
                # Display study information with color coding
                status_color = Fore.GREEN if study_info['accessible'] else Fore.RED
                status_icon = "✓" if study_info['accessible'] else "✗"
                
                print(f"{Fore.WHITE + Style.BRIGHT}{i}. {study_info['name']} {status_color}({study_info['file_type'].upper()}) {status_icon}{Style.RESET_ALL}")
                
                if study_info['accessible']:
                    completion_rate = (study_info['complete'] / study_info['trials']) * 100 if study_info['trials'] > 0 else 0
                    completion_color = Fore.GREEN if completion_rate > 75 else Fore.YELLOW if completion_rate > 50 else Fore.RED
                    
                    print(f"   {Fore.CYAN}Trials:{Style.RESET_ALL} {study_info['complete']}/{study_info['trials']} complete "
                          f"({completion_color}{completion_rate:.1f}%{Style.RESET_ALL}), "
                          f"{study_info['pruned']} pruned, {study_info['failed']} failed")
                    print(f"   {Fore.CYAN}Best Value:{Style.RESET_ALL} {Fore.YELLOW + Style.BRIGHT}{study_info['best_value']}{Style.RESET_ALL}")
                    
                    if 'multiple_studies' in study_info:
                        print(f"   {Fore.YELLOW}Multiple studies in database: {len(study_info['multiple_studies'])}{Style.RESET_ALL}")
                    
                    print(f"   {Fore.CYAN}Last Modified:{Style.RESET_ALL} {study_info['last_modified'].strftime('%Y-%m-%d %H:%M:%S')}")
                    if study_info['file_size_mb'] > 0:
                        size_color = Fore.GREEN if study_info['file_size_mb'] < 10 else Fore.YELLOW if study_info['file_size_mb'] < 50 else Fore.RED
                        print(f"   {Fore.CYAN}File Size:{Style.RESET_ALL} {size_color}{study_info['file_size_mb']:.1f} MB{Style.RESET_ALL}")
                else:
                    print(f"   {Fore.RED}Error: {study_info['error']}{Style.RESET_ALL}")
                
                print()
                
            except Exception as e:
                study_info = {
                    'index': i,
                    'file': study_file,
                    'name': study_file.name,
                    'error': str(e),
                    'accessible': False
                }
                studies_info.append(study_info)
                print(f"{Fore.WHITE + Style.BRIGHT}{i}. {study_file.name} {Fore.RED}(Error: {e}){Style.RESET_ALL}")
                print()
        
        # Filter accessible studies
        accessible_studies = [s for s in studies_info if s['accessible']]
        
        if not accessible_studies:
            print(Fore.RED + Style.BRIGHT + "\nNo accessible studies found. All studies have errors.")
            print(Fore.YELLOW + Style.BRIGHT + "\nAvailable options:")
            print(Fore.WHITE + Style.BRIGHT + "1. Try to repair database files")
            print(Fore.WHITE + Style.BRIGHT + "2. Switch to express setup")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            choice = None
            while not choice:
                try:
                    choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-2): ").strip()
                    
                    if choice == '0':
                        print(Fore.RED + Style.BRIGHT + "\nStudy continuation cancelled")
                        return None
                    elif choice == '1':
                        print(Fore.YELLOW + Style.BRIGHT + "\nDatabase repair not implemented. Switching to express setup...")
                    elif choice == '2':
                        return _interactive_hpo_express_setup(
                            base_config,
                            data_mode=data_mode,
                            hardware_data=hardware_data,
                            enable_storage=enable_storage,
                            enable_plots=enable_plots,
                            custom_search_space=custom_search_space,
                            sampler_type=sampler_type,
                            pruner_type=pruner_type,
                            trial_count=trial_count,
                            timeout_seconds=timeout_seconds,
                            optimization_focus=optimization_focus,
                            **kwargs
                        )
                    else:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                        choice = None
                        continue
                        
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOption selection interrupted!")
                    return None
            
            return _interactive_hpo_express_setup(
                base_config,
                data_mode=data_mode,
                hardware_data=hardware_data,
                enable_storage=enable_storage,
                enable_plots=enable_plots,
                custom_search_space=custom_search_space,
                sampler_type=sampler_type,
                pruner_type=pruner_type,
                trial_count=trial_count,
                timeout_seconds=timeout_seconds,
                optimization_focus=optimization_focus,
                **kwargs
            )
        
        print(Fore.WHITE + Style.BRIGHT + f"{len(studies_info)+1}. Back to express setup")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        # Input handling with retry logic
        selected_study_info = None
        while selected_study_info is None:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect study to continue (1-{len(studies_info)}, {len(studies_info)+1} for express setup, 0 to cancel): ").strip()
                
                if choice.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nStudy continuation cancelled")
                    return None
                    
                if not choice:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid choice.")
                    continue
                
                choice_num = int(choice)
                
                if choice_num == 0:
                    print(Fore.RED + Style.BRIGHT + "\nStudy continuation cancelled")
                    return None
                elif choice_num == len(studies_info) + 1:
                    return _interactive_hpo_express_setup(
                        base_config,
                        data_mode=data_mode,
                        hardware_data=hardware_data,
                        enable_storage=enable_storage,
                        enable_plots=enable_plots,
                        custom_search_space=custom_search_space,
                        sampler_type=sampler_type,
                        pruner_type=pruner_type,
                        trial_count=trial_count,
                        timeout_seconds=timeout_seconds,
                        optimization_focus=optimization_focus,
                        **kwargs
                    )
                elif 1 <= choice_num <= len(studies_info):
                    selected_study_info = studies_info[choice_num-1]
                    if not selected_study_info['accessible']:
                        print(Fore.RED + Style.BRIGHT + f"\nCannot continue study due to error: {selected_study_info['error']}")
                        selected_study_info = None
                        continue
                else:
                    print(Fore.RED + Style.BRIGHT + f"\nInvalid choice. Please select 0-{len(studies_info)+1}.")
                    
            except ValueError:
                print(Fore.RED + Style.BRIGHT + f"\nInvalid input. Please enter a number 0-{len(studies_info)+1}.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nStudy selection interrupted!")
                return None
        
        study = selected_study_info['study']
        study_name = selected_study_info['name']
        file_type = selected_study_info['file_type']
        
        print(Fore.GREEN + Style.BRIGHT + f"\nSelected study: {study_name} ({file_type})")
        print(Fore.CYAN + Style.BRIGHT + f"Current status: {selected_study_info['complete']} completed trials out of {selected_study_info['trials']} total")
        
        # Display study configuration if available
        try:
            if hasattr(study, 'user_attrs'):
                config_info = study.user_attrs.get('configuration', {})
                if config_info:
                    print(Fore.YELLOW + Style.BRIGHT + "\nOriginal study configuration:")
                    if 'sampler_type' in config_info:
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{config_info['sampler_type']}")
                    if 'pruner_type' in config_info:
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{config_info['pruner_type']}")
                    if 'model_types' in config_info:
                        print(Fore.GREEN + Style.BRIGHT + f"  └─ Models: " + Fore.YELLOW + Style.BRIGHT + f"{', '.join(config_info['model_types'])}")
        except Exception:
            pass
        
        # Continuation configuration
        print(Fore.MAGENTA + Style.BRIGHT + "CONTINUATION CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        
        # Additional trials with input handling
        additional_trials = None
        while additional_trials is None:
            try:
                additional_trials_input = input(Fore.YELLOW + Style.BRIGHT + f"Additional trials to run ({trial_count or 50}): ").strip()
                if additional_trials_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nStudy continuation cancelled")
                    return None
                additional_trials = int(additional_trials_input) if additional_trials_input else (trial_count or 50)
                if additional_trials <= 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a positive number.")
                    additional_trials = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nTrial configuration interrupted!")
                return None
        
        # Additional timeout with input handling
        additional_timeout = None
        while additional_timeout is None:
            try:
                default_timeout = (timeout_seconds or 3600) // 60
                additional_timeout_input = input(Fore.YELLOW + Style.BRIGHT + f"Additional timeout in minutes ({default_timeout}, 0 for no limit): ").strip()
                if additional_timeout_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nStudy continuation cancelled")
                    return None
                additional_timeout = int(additional_timeout_input) if additional_timeout_input else default_timeout
                if additional_timeout < 0:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a non-negative number.")
                    additional_timeout = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nTimeout configuration interrupted!")
                return None
        
        # Advanced continuation options with input handling
        print(Fore.YELLOW + Style.BRIGHT + "\nAdvanced continuation options:")
        print(Fore.WHITE + Style.BRIGHT + "1. Continue with same configuration " + Fore.GREEN + Style.BRIGHT + "(Recommended)")
        print(Fore.WHITE + Style.BRIGHT + "2. Modify search space " + Fore.GREEN + Style.BRIGHT + "(Expand parameter ranges)")
        print(Fore.WHITE + Style.BRIGHT + "3. Change pruning strategy " + Fore.GREEN + Style.BRIGHT + "(Adjust optimization behavior)")
        print(Fore.WHITE + Style.BRIGHT + "4. Add new model types " + Fore.GREEN + Style.BRIGHT + "(Include additional architectures)")
        
        advanced_choice = None
        while advanced_choice is None:
            try:
                advanced_choice_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (1-4, default=1): ").strip()
                if advanced_choice_input.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nStudy continuation cancelled")
                    return None
                advanced_choice = int(advanced_choice_input) if advanced_choice_input in ['2', '3', '4'] else 1
                if advanced_choice not in [1, 2, 3, 4]:
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 1-4.")
                    advanced_choice = None
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nAdvanced option selection interrupted!")
                return None
        
        # Build configuration for continuation
        final_config = deepcopy(base_config) if base_config else {}
        
        # If no base config, try to extract from study or use default
        if not final_config:
            if 'default' in PRESET_CONFIGS:
                final_config = deepcopy(PRESET_CONFIGS['default'])
                print(Fore.GREEN + Style.BRIGHT + "\nUsing default preset as base for continuation")
            else:
                logger.warning("No base config available for continuation")
                final_config = {}
        
        # Apply data mode if specified
        if data_mode and data_mode != 'auto':
            if 'data' not in final_config:
                final_config['data'] = {}
            if data_mode == 'synthetic':
                final_config['data']['use_real_data'] = False
            elif data_mode == 'real':
                final_config['data']['use_real_data'] = True
        
        # HPO configuration for continuation
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        hpo_config.update({
            'enabled': True,
            'strategy': 'optuna',
            'n_trials': additional_trials,
            'timeout': additional_timeout * 60 if additional_timeout > 0 else 0,
            'timeout_seconds': additional_timeout * 60 if additional_timeout > 0 else 0,
            'study_name': study_name,
            'direction': 'minimize',  # Will be overridden by existing study
            'load_if_exists': True,
            'verbose': True,
            'cleanup_trials': True,
            'generate_plots': enable_plots if enable_plots is not None else True,
            
            # Storage configuration
            'storage': {
                'enabled': enable_storage if enable_storage is not None else True,
                'url': f"sqlite:///{selected_study_info['file']}" if file_type == 'database' else None,
                'load_if_exists': True,
                'heartbeat_interval': 60,
                'grace_period': 120
            }
        })
        
        # Apply parameter overrides
        if sampler_type:
            hpo_config['sampler'] = sampler_type
        if pruner_type:
            hpo_config['pruner'] = pruner_type
        
        # Apply advanced modifications based on user choice
        if advanced_choice == 2:  # Modify search space
            print(Fore.YELLOW + Style.BRIGHT + "\nModifying search space...")
            print(Fore.GREEN + Style.BRIGHT + "Current optimization space will be expanded")
            
            # Get existing optimization space from study if available
            existing_space = {}
            try:
                if hasattr(study, 'user_attrs') and 'optimization_space' in study.user_attrs:
                    existing_space = study.user_attrs['optimization_space']
            except Exception:
                pass
            
            # Use custom search space if provided, otherwise expand default
            if custom_search_space:
                expanded_space = custom_search_space
                print(Fore.GREEN + Style.BRIGHT + "\nUsing provided custom search space")
            else:
                # Expand the search space
                expanded_space = {
                    'learning_rate': {'type': 'float', 'low': 1e-6, 'high': 1e-1, 'log': True},
                    'batch_size': {'type': 'categorical', 'choices': [16, 32, 64, 128, 256]},
                    'encoding_dim': {'type': 'int', 'low': 4, 'high': 64},
                    'dropout_rate': {'type': 'float', 'low': 0.0, 'high': 0.6},
                    'weight_decay': {'type': 'float', 'low': 1e-7, 'high': 1e-2, 'log': True}
                }
            
            # Merge with existing space
            for key, value in existing_space.items():
                if key not in expanded_space:
                    expanded_space[key] = value
            
            hpo_config['optimization_space'] = expanded_space
            print(Fore.GREEN + Style.BRIGHT + "\nSearch space expanded with broader ranges")
            
        elif advanced_choice == 3:  # Change pruning strategy
            print(Fore.YELLOW + Style.BRIGHT + "\nChanging pruning strategy...")
            print(Fore.WHITE + Style.BRIGHT + "1. MedianPruner " + Fore.GREEN + Style.BRIGHT + "(Aggressive pruning)")
            print(Fore.WHITE + Style.BRIGHT + "2. HyperbandPruner " + Fore.GREEN + Style.BRIGHT + "(Successive halving)")
            print(Fore.WHITE + Style.BRIGHT + "3. NopPruner " + Fore.GREEN + Style.BRIGHT + "(No pruning - complete all trials)")
            
            pruner_choice = None
            while pruner_choice is None:
                try:
                    pruner_choice_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect pruner (1-3): ").strip()
                    if pruner_choice_input.lower() == 'c':
                        print(Fore.RED + Style.BRIGHT + "\nStudy continuation cancelled")
                        return None
                    pruner_choice = int(pruner_choice_input)
                    if pruner_choice not in [1, 2, 3]:
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 1-3.")
                        pruner_choice = None
                except ValueError:
                    print(Fore.RED + Style.BRIGHT + "\nPlease enter a valid number.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nPruner selection interrupted!")
                    return None
            
            if pruner_choice == 1:
                hpo_config['pruner'] = 'MedianPruner'
            elif pruner_choice == 2:
                hpo_config['pruner'] = 'HyperbandPruner'
            elif pruner_choice == 3:
                hpo_config['pruner'] = 'NopPruner'
            
            print(Fore.GREEN + Style.BRIGHT + f"\nPruner set to {hpo_config['pruner']}")
            
        elif advanced_choice == 4:  # Add new model types
            print(Fore.YELLOW + Style.BRIGHT + "\nAdding new model types...")
            available_models = model_types or list(MODEL_VARIANTS.keys())
            
            # Try to get existing models from study
            existing_models = []
            try:
                if hasattr(study, 'user_attrs') and 'model_types' in study.user_attrs:
                    existing_models = study.user_attrs['model_types']
            except Exception:
                pass
            
            if existing_models:
                print(Fore.CYAN + Style.BRIGHT + f"Current models: {', '.join(existing_models)}")
            
            new_models = []
            for model in available_models:
                if model not in existing_models:
                    include = None
                    while include is None:
                        try:
                            include_input = input(Fore.YELLOW + Style.BRIGHT + f"\nAdd {model}? (y/N): ").strip().lower()
                            if include_input == 'c':
                                print(Fore.RED + Style.BRIGHT + "\nStudy continuation cancelled")
                                return None
                            include = include_input in ('y', 'yes')
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nModel selection interrupted!")
                            return None
                    
                    if include:
                        new_models.append(model)
            
            if new_models:
                all_models = list(set(existing_models + new_models))
                hpo_config['model_search'] = {
                    'enabled': len(all_models) > 1,
                    'model_types': all_models,
                    'search_all_models': len(all_models) >= 3
                }
                print(Fore.GREEN + Style.BRIGHT + f"\nAdded models: {', '.join(new_models)}")
            else:
                print(Fore.YELLOW + Style.BRIGHT + "\nNo new models added")
        
        # Try to extract and preserve original study configuration
        try:
            if hasattr(study, 'user_attrs') and 'configuration' in study.user_attrs:
                original_config = study.user_attrs['configuration']
                
                # Preserve important original settings
                for section, values in original_config.items():
                    if section in ['data', 'model', 'training'] and isinstance(values, dict):
                        section_config = final_config.setdefault(section, {})
                        for key, value in values.items():
                            if key not in section_config:
                                section_config[key] = value
                
                print(Fore.GREEN + Style.BRIGHT + "\nPreserved original study configuration")
        except Exception as e:
            print(Fore.YELLOW + Style.BRIGHT + f"\nCould not extract original study configuration: {e}")
            print(Fore.YELLOW + Style.BRIGHT + "\nUsing default configuration for missing settings")
        
        # System configuration for continuation
        system_config = final_config.setdefault('system', {})
        system_config.update({
            'non_interactive': non_interactive,
            'verbose': True,
            'random_seed': 42,
            'reproducible': True,
            'hpo_optimized': True
        })
        
        # Update metadata to reflect continuation
        metadata = final_config.setdefault('metadata', {})
        metadata.update({
            'last_modified': datetime.now().isoformat(),
            'setup_method': 'continue_hpo',
            'continuation_info': {
                'original_study': study_name,
                'original_trials': selected_study_info['trials'],
                'completed_trials': selected_study_info['complete'],
                'additional_trials': additional_trials,
                'additional_timeout_minutes': additional_timeout if additional_timeout > 0 else 'unlimited',
                'file_type': file_type,
                'advanced_modifications': advanced_choice > 1,
                'continuation_timestamp': datetime.now().isoformat(),
                'optimization_focus': optimization_focus,
                'data_mode': data_mode
            }
        })
        
        # Update runtime information
        runtime_config = final_config.setdefault('runtime', {})
        runtime_config.update({
            'setup_method': 'continue_hpo',
            'active_preset': 'Continued Study',
            'hpo_launch_time': datetime.now().isoformat(),
            'original_study': study_name,
            'operation_mode': operation_mode or 'continue'
        })
        
        # Continuation Summary
        print(Fore.MAGENTA + Style.BRIGHT + "CONTINUATION SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        print(Fore.MAGENTA + Style.BRIGHT + "\nStudy Continuation Details:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Study: " + Fore.YELLOW + Style.BRIGHT + f"{study_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ File Type: " + Fore.YELLOW + Style.BRIGHT + f"{file_type.upper()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Progress: " + Fore.YELLOW + Style.BRIGHT + f"{selected_study_info['complete']} completed trials")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Additional Trials: " + Fore.YELLOW + Style.BRIGHT + f"{additional_trials}")
        
        if additional_timeout > 0:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Additional Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{additional_timeout} minutes")
        else:
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Additional Timeout: " + Fore.YELLOW + Style.BRIGHT + f"No limit")
        
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Advanced Modifications: " + Fore.YELLOW + Style.BRIGHT + f"{'Yes' if advanced_choice > 1 else 'No'}")
        
        if selected_study_info['best_value'] != "No completed trials" and selected_study_info['best_value'] != "No trials":
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Current Best Value: " + Fore.YELLOW + Style.BRIGHT + f"{selected_study_info['best_value']}")
        
        # Estimate additional time
        try:
            model_count = len(hpo_config.get('model_search', {}).get('model_types', ['EnhancedAutoencoder']))
            trial_epochs = hpo_config.get('trial_epochs', 20)
            cv_folds = hpo_config.get('cross_validation', {}).get('folds', 3)
            
            estimated_time = _estimate_hpo_time(n_trials=additional_trials, trial_epochs=trial_epochs, n_model_types=model_count, cv_folds=cv_folds, hardware_info=hardware_data, system_class=system_class)
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Estimated Additional Time: " + Fore.YELLOW + Style.BRIGHT + f"~{estimated_time}")
        except Exception:
            pass
        
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        
        # Handle skip_prompt mode
        if skip_prompt:
            print(Fore.GREEN + Style.BRIGHT + f"\nSkipping confirmation - Continuing study: {study_name}...")
            
            # Additional setup for continuation
            hpo_config.update({
                'continuation_mode': True,
                'original_trials': selected_study_info['trials'],
                'original_completed': selected_study_info['complete']
            })
            
            return _launch_hpo_with_config(config=final_config, **kwargs)
        
        # Confirmation with options
        confirm = None
        while not confirm:
            try:
                confirm_input = input(Fore.YELLOW + Style.BRIGHT + "\nContinue hyperparameter optimization with these settings? (Y/n/c to cancel): ").strip().lower()
                
                if confirm_input in ('', 'y', 'yes'):
                    print(Fore.GREEN + Style.BRIGHT + f"\nContinuing hyperparameter optimization study: {study_name}...")
                    
                    # Additional setup for continuation
                    hpo_config.update({
                        'continuation_mode': True,
                        'original_trials': selected_study_info['trials'],
                        'original_completed': selected_study_info['complete']
                    })
                    
                    return _launch_hpo_with_config(config=final_config, **kwargs)
                elif confirm_input in ('c', 'cancel'):
                    print(Fore.RED + Style.BRIGHT + "\nStudy continuation cancelled")
                    return None
                elif confirm_input in ('n', 'no'):
                    print(Fore.YELLOW + Style.BRIGHT + "\nWould you like to:")
                    print(Fore.WHITE + Style.BRIGHT + "1. Try with different settings")
                    print(Fore.WHITE + Style.BRIGHT + "2. Switch to express setup")
                    print(Fore.RED + Style.BRIGHT + "0. Return to previous menu")
                    
                    retry_choice = None
                    while not retry_choice:
                        try:
                            retry_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-2): ").strip()
                            
                            if retry_choice not in ['1', '2', '0']:
                                print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                                retry_choice = None
                                continue
                                
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOption selection interrupted!")
                            return None
                    
                    if retry_choice == '1':
                        return _interactive_hpo_continue_setup(
                            base_config,
                            study_name=study_name,
                            storage_url=storage_url,
                            hardware_data=hardware_data,
                            data_mode=data_mode,
                            enable_storage=enable_storage,
                            enable_plots=enable_plots,
                            custom_search_space=custom_search_space,
                            sampler_type=sampler_type,
                            pruner_type=pruner_type,
                            trial_count=trial_count,
                            timeout_seconds=timeout_seconds,
                            optimization_focus=optimization_focus,
                            non_interactive=non_interactive,
                            skip_prompt=skip_prompt,
                            operation_mode=operation_mode,
                            use_current_config=use_current_config,
                            force_express=force_express,
                            model_types=model_types,
                            **kwargs
                        )
                    elif retry_choice == '2':
                        return _interactive_hpo_express_setup(
                            base_config,
                            data_mode=data_mode,
                            hardware_data=hardware_data,
                            enable_storage=enable_storage,
                            enable_plots=enable_plots,
                            custom_search_space=custom_search_space,
                            sampler_type=sampler_type,
                            pruner_type=pruner_type,
                            trial_count=trial_count,
                            timeout_seconds=timeout_seconds,
                            optimization_focus=optimization_focus,
                            **kwargs
                        )
                    else:
                        print(Fore.RED + Style.BRIGHT + "\nReturning to previous menu")
                        return None
                else:
                    print(Fore.RED + Style.BRIGHT + "\nInvalid input. Please enter Y/n/c.")
                    confirm = None
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nConfirmation interrupted!")
                return None
                
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\nStudy continuation setup interrupted")
        return None
    except Exception as e:
        logger.error(f"Study continuation setup failed: {e}")
        print(Fore.RED + Style.BRIGHT + f"\nStudy continuation setup failed: {str(e)}")
        
        # Error recovery with user options
        print(Fore.YELLOW + Style.BRIGHT + "\nError recovery options:")
        print(Fore.WHITE + Style.BRIGHT + "1. Retry continuation setup")
        print(Fore.WHITE + Style.BRIGHT + "2. Switch to express setup")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to main menu")
        
        recovery_choice = None
        while not recovery_choice:
            try:
                recovery_choice_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect recovery option (0-2): ").strip()
                
                if recovery_choice_input == '0':
                    print(Fore.RED + Style.BRIGHT + "\nStudy continuation cancelled")
                    return None
                elif recovery_choice_input == '1':
                    print(Fore.YELLOW + Style.BRIGHT + "\nRetrying continuation setup...")
                    return _interactive_hpo_continue_setup(
                        base_config,
                        study_name=study_name,
                        storage_url=storage_url,
                        hardware_data=hardware_data,
                        data_mode=data_mode,
                        enable_storage=enable_storage,
                        enable_plots=enable_plots,
                        custom_search_space=custom_search_space,
                        sampler_type=sampler_type,
                        pruner_type=pruner_type,
                        trial_count=trial_count,
                        timeout_seconds=timeout_seconds,
                        optimization_focus=optimization_focus,
                        non_interactive=non_interactive,
                        skip_prompt=skip_prompt,
                        operation_mode=operation_mode,
                        use_current_config=use_current_config,
                        force_express=force_express,
                        model_types=model_types,
                        **kwargs
                    )
                elif recovery_choice_input == '2':
                    print(Fore.YELLOW + Style.BRIGHT + "\nSwitching to express setup...")
                    return _interactive_hpo_express_setup(
                        base_config,
                        data_mode=data_mode,
                        hardware_data=hardware_data,
                        enable_storage=enable_storage,
                        enable_plots=enable_plots,
                        custom_search_space=custom_search_space,
                        sampler_type=sampler_type,
                        pruner_type=pruner_type,
                        trial_count=trial_count,
                        timeout_seconds=timeout_seconds,
                        optimization_focus=optimization_focus,
                        **kwargs
                    )
                else:
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                    recovery_choice = None
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nRecovery option selection interrupted!")
                return None



def _estimate_hpo_time(
    n_trials: int,
    trial_epochs: int,
    n_model_types: int,
    cv_folds: int,
    hardware_info: Optional[Dict[str, Any]] = None,
    system_class: str = 'unknown'
) -> str:
    """
    Enhanced HPO completion time estimation with hardware awareness and detailed analysis.
    
    Args:
        n_trials: Number of optimization trials
        trial_epochs: Epochs per trial
        n_model_types: Number of model types to optimize
        cv_folds: Cross-validation folds
        hardware_info: Optional hardware information from system analysis
        system_class: System performance class (e.g., 'high-end', 'standard', 'limited')
    
    Returns:
        Human-readable time estimate string
    """
    try:
        # Base time per trial epoch in seconds (conservative estimate)
        base_epoch_time = 15  # seconds per epoch on average hardware
        
        # Get hardware information if not provided
        if hardware_info is None:
            try:
                hardware_info = check_hardware(min_disk_gb=1.0, include_memory_usage=False)
            except Exception:
                hardware_info = {}
        
        # Hardware performance factors
        cpu_factor = 1.0
        memory_factor = 1.0
        gpu_factor = 1.0
        storage_factor = 1.0
        
        # CPU analysis
        cpu_info = hardware_info.get('cpu_cores', {})
        if cpu_info.get('available', False):
            logical_cores = cpu_info.get('logical_cores', 1)
            
            if logical_cores >= 16:
                cpu_factor = 0.4  # High-end CPU
            elif logical_cores >= 8:
                cpu_factor = 0.6  # Good CPU
            elif logical_cores >= 4:
                cpu_factor = 0.8  # Standard CPU
            elif logical_cores >= 2:
                cpu_factor = 1.0  # Basic CPU
            else:
                cpu_factor = 1.5  # Limited CPU
            
            # Consider CPU frequency if available
            cpu_capacity = cpu_info.get('capacity', {})
            frequency_ghz = cpu_capacity.get('frequency_ghz', 0)
            if frequency_ghz > 3.5:
                cpu_factor *= 0.85  # High frequency bonus
            elif frequency_ghz < 2.0 and frequency_ghz > 0:
                cpu_factor *= 1.15  # Low frequency penalty
        
        # Memory analysis
        ram_info = hardware_info.get('system_ram', {})
        if ram_info.get('available', False):
            ram_gb = ram_info.get('ram_total_gb', 4.0)
            ram_available_gb = ram_info.get('ram_available_gb', ram_gb * 0.7)
            
            if ram_gb >= 32:
                memory_factor = 0.7  # Plenty of memory
            elif ram_gb >= 16:
                memory_factor = 0.8  # Good memory
            elif ram_gb >= 8:
                memory_factor = 0.9  # Standard memory
            elif ram_gb >= 4:
                memory_factor = 1.0  # Basic memory
            else:
                memory_factor = 1.3  # Limited memory
            
            # Consider available memory ratio
            if ram_available_gb / ram_gb < 0.3:  # Less than 30% available
                memory_factor *= 1.2  # Memory pressure penalty
        
        # GPU analysis
        cuda_info = hardware_info.get('cuda', {})
        if cuda_info.get('available', False):
            gpu_count = cuda_info.get('gpu_count', 0)
            gpus = cuda_info.get('gpus', [])
            
            if gpus:
                # Analyze the best GPU
                best_gpu = max(gpus, key=lambda g: g.get('memory_gb', 0))
                gpu_memory_gb = best_gpu.get('memory_gb', 0)
                compute_capability = float(best_gpu.get('compute_capability', '0.0'))
                
                if gpu_memory_gb >= 16 and compute_capability >= 8.0:
                    gpu_factor = 0.15  # High-end modern GPU (RTX 40xx series)
                elif gpu_memory_gb >= 12 and compute_capability >= 7.5:
                    gpu_factor = 0.2   # High-end GPU (RTX 30xx series)
                elif gpu_memory_gb >= 8 and compute_capability >= 7.0:
                    gpu_factor = 0.25  # Good GPU (RTX 20xx series)
                elif gpu_memory_gb >= 6 and compute_capability >= 6.1:
                    gpu_factor = 0.35  # Mid-range GPU (GTX 16xx series)
                elif gpu_memory_gb >= 4:
                    gpu_factor = 0.45  # Entry-level GPU
                else:
                    gpu_factor = 0.6   # Low-end GPU
                
                # Multi-GPU bonus (limited for HPO)
                if gpu_count > 1:
                    gpu_factor *= 0.9  # Slight improvement for multi-GPU
            else:
                gpu_factor = 1.0  # CUDA available but no GPU info
        else:
            gpu_factor = 1.0  # CPU-only execution
        
        # Storage analysis (affects data loading)
        disk_info = hardware_info.get('disk_space', {})
        if disk_info.get('available', False):
            # This is basic - in real implementation, would check SSD vs HDD
            free_gb = disk_info.get('free_gb', 10)
            if free_gb < 5:
                storage_factor = 1.2  # Low disk space penalty
            else:
                storage_factor = 1.0
        
        # System class adjustment
        system_class_factors = {
            'high-end': 0.6,
            'performance': 0.7, 
            'standard': 1.0,
            'baseline': 1.1,
            'limited': 1.4,
            'debug': 1.6,
            'lightweight': 1.3,
            'unknown': 1.0
        }
        system_class_factor = system_class_factors.get(system_class.lower(), 1.0)
        
        # Model complexity factor
        model_complexity_factor = 1.0
        if n_model_types == 1:
            model_complexity_factor = 1.0
        elif n_model_types == 2:
            model_complexity_factor = 1.6  # Not quite 2x due to shared overhead
        elif n_model_types == 3:
            model_complexity_factor = 2.2
        else:
            model_complexity_factor = n_model_types * 0.8  # Diminishing returns
        
        # Cross-validation factor
        cv_factor = max(1, cv_folds)
        if cv_folds > 5:
            # Diminishing returns for very high CV
            cv_factor = 5 + (cv_folds - 5) * 0.5
        
        # HPO overhead factor (trial setup, parameter sampling, etc.)
        hpo_overhead_factor = 1.25
        if n_trials > 200:
            hpo_overhead_factor = 1.3  # More overhead for large studies
        elif n_trials < 20:
            hpo_overhead_factor = 1.4  # Proportionally more overhead for small studies
        
        # Progressive trial factor (later trials may be faster due to pruning)
        if n_trials > 50:
            progressive_factor = 0.9  # Some trials will be pruned early
        else:
            progressive_factor = 1.0
        
        # Calculate total time
        base_trial_time = trial_epochs * base_epoch_time
        
        # Apply all factors
        hardware_adjusted_time = (base_trial_time * cpu_factor * memory_factor * 
                                min(gpu_factor, 1.0) * storage_factor * system_class_factor)
        
        total_seconds = (n_trials * hardware_adjusted_time * model_complexity_factor * 
                        cv_factor * hpo_overhead_factor * progressive_factor)
        
        # Add setup and teardown time
        setup_time = 60  # 1 minute setup
        teardown_time = 30  # 30 seconds teardown per trial average
        total_seconds += setup_time + (n_trials * teardown_time * 0.1)  # Reduced teardown per trial
        
        # Convert to minutes
        total_minutes = total_seconds / 60.0
        
        # Generate human-readable estimate with confidence interval
        if total_minutes < 1:
            base_estimate = "< 1 minute"
            range_estimate = "< 2 minutes"
        elif total_minutes < 60:
            minutes = int(total_minutes)
            base_estimate = f"{minutes} minute{'s' if minutes != 1 else ''}"
            range_estimate = f"{int(minutes * 0.7)}-{int(minutes * 1.3)} minutes"
        elif total_minutes < 1440:  # Less than 24 hours
            hours = total_minutes / 60
            if hours < 2:
                base_hours = int(hours)
                remaining_minutes = int((hours - base_hours) * 60)
                base_estimate = f"{base_hours}h {remaining_minutes}m"
                range_estimate = f"{int(hours * 0.7 * 60)}m - {int(hours * 1.3 * 60)}m"
            else:
                base_hours = int(hours)
                base_estimate = f"{base_hours} hour{'s' if base_hours != 1 else ''}"
                range_estimate = f"{int(base_hours * 0.7)}-{int(base_hours * 1.3)} hours"
        else:
            days = total_minutes / 1440
            base_days = int(days)
            remaining_hours = int((days - base_days) * 24)
            if remaining_hours == 0:
                base_estimate = f"{base_days} day{'s' if base_days != 1 else ''}"
            else:
                base_estimate = f"{base_days}d {remaining_hours}h"
            range_estimate = f"{int(days * 0.7)}-{int(days * 1.3)} days"
        
        # Add hardware context to estimate
        hardware_context = []
        if cuda_info.get('available', False):
            best_gpu = cuda_info.get('gpus', [{}])[0] if cuda_info.get('gpus') else {}
            gpu_name = best_gpu.get('name', 'GPU')
            hardware_context.append(f"GPU: {gpu_name}")
        else:
            hardware_context.append("CPU-only")
        
        cpu_cores = hardware_info.get('cpu_cores', {}).get('logical_cores', 'unknown')
        if cpu_cores != 'unknown':
            hardware_context.append(f"{cpu_cores} cores")
        
        ram_gb = hardware_info.get('system_ram', {}).get('ram_total_gb', 0)
        if ram_gb > 0:
            hardware_context.append(f"{ram_gb:.0f}GB RAM")
        
        # Build final estimate string
        if hardware_context:
            context_str = f" ({', '.join(hardware_context)})"
        else:
            context_str = ""
        
        # Return estimate with context
        final_estimate = f"{base_estimate}{context_str}"
        
        # Add confidence and range information for debugging
        logger.debug(f"HPO time estimation details:")
        logger.debug(f"  Base time per trial: {base_trial_time:.1f}s")
        logger.debug(f"  Hardware factors: CPU={cpu_factor:.2f}, Memory={memory_factor:.2f}, GPU={gpu_factor:.2f}")
        logger.debug(f"  Model complexity factor: {model_complexity_factor:.2f}")
        logger.debug(f"  CV factor: {cv_factor:.1f}")
        logger.debug(f"  Total estimated minutes: {total_minutes:.1f}")
        logger.debug(f"  Confidence range: {range_estimate}")
        
        return final_estimate
        
    except Exception as e:
        logger.debug(f"Time estimation failed: {e}")
        
        # Fallback estimation based on simple heuristics
        try:
            simple_minutes = n_trials * trial_epochs * n_model_types * cv_folds * 0.5  # 30 seconds per epoch average
            
            if torch.cuda.is_available():
                simple_minutes *= 0.3  # GPU acceleration
            
            if simple_minutes < 60:
                return f"~{int(simple_minutes)} minutes"
            elif simple_minutes < 1440:
                hours = int(simple_minutes // 60)
                return f"~{hours} hour{'s' if hours != 1 else ''}"
            else:
                days = int(simple_minutes // 1440)
                return f"~{days} day{'s' if days != 1 else ''}"
                
        except Exception:
            # Ultimate fallback
            return "Several hours (estimation failed)"

def _display_hpo_results(results: Dict[str, Any]) -> None:
    """
    Display comprehensive HPO results in a user-friendly format with complete integrated functionality.
    Handles all result types from updated HPO functions with rich formatting and detailed analysis.
    """
    try:
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION RESULTS SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Extract core result information with fallback handling
        success = results.get('success', False)
        start_time = results.get('start_time', 'Unknown')
        end_time = results.get('end_time', 'Unknown')
        study_name = results.get('study_name', 'Unknown')

        # Extract setup context with multiple fallbacks
        launch_config = results.get('launch_config', {})
        setup_method = launch_config.get('setup_method', results.get('setup_method', 'unknown'))
        config_source = launch_config.get('config_source', results.get('config_source', 'unknown'))
        preset_used = launch_config.get('preset_used', results.get('preset_used', 'Custom/Default'))
        system_class = launch_config.get('system_class', results.get('system_class', 'standard'))

        # Extract express context if available
        express_context = results.get('express_context', {})
        optimization_focus = express_context.get('optimization_focus', results.get('optimization_focus', 'balanced'))
        express_intensity = express_context.get('express_intensity', results.get('express_intensity', 'Standard'))
        
        # Display basic status information
        status_color = Fore.GREEN if success else Fore.RED
        status_icon = "✓" if success else "✗"
        
        print(Fore.YELLOW + Style.BRIGHT + "Optimization Overview:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Status: " + status_color + Style.BRIGHT + f"{status_icon} {'SUCCESS' if success else 'FAILED'}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Study Name: " + Fore.YELLOW + Style.BRIGHT + f"{study_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Setup Method: " + Fore.YELLOW + Style.BRIGHT + f"{setup_method.replace('_', ' ').title()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Configuration Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Active Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_used}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Started: " + Fore.YELLOW + Style.BRIGHT + f"{start_time}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Completed: " + Fore.YELLOW + Style.BRIGHT + f"{end_time}")
        
        # Calculate and display duration if available
        duration_str = None
        try:
            if isinstance(start_time, str) and isinstance(end_time, str) and start_time != 'Unknown' and end_time != 'Unknown':
                start_dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                end_dt = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
                duration = end_dt - start_dt
                total_time_minutes = duration.total_seconds() / 60
                if total_time_minutes > 0:
                    hours = int(total_time_minutes // 60)
                    minutes = int(total_time_minutes % 60)
                    seconds = int((total_time_minutes * 60) % 60)
                    
                    if hours > 0:
                        duration_str = f"{hours}h {minutes}m {seconds}s"
                    elif minutes > 0:
                        duration_str = f"{minutes}m {seconds}s"
                    else:
                        duration_str = f"{seconds}s"
            else:
                total_time_seconds = results.get('total_time_seconds', 0)
                total_time_minutes = results.get('total_time_minutes', total_time_seconds / 60 if total_time_seconds > 0 else 0)
                if total_time_minutes > 0:
                    hours = int(total_time_minutes // 60)
                    minutes = int(total_time_minutes % 60)
                    seconds = int((total_time_minutes * 60) % 60)
                    
                    if hours > 0:
                        duration_str = f"{hours}h {minutes}m {seconds}s"
                    elif minutes > 0:
                        duration_str = f"{minutes}m {seconds}s"
                    else:
                        duration_str = f"{seconds}s"
        except Exception:
            total_time_seconds = results.get('total_time_seconds', 0)
            total_time_minutes = results.get('total_time_minutes', total_time_seconds / 60 if total_time_seconds > 0 else 0)
            if total_time_minutes > 0:
                hours = int(total_time_minutes // 60)
                minutes = int(total_time_minutes % 60)
                seconds = int((total_time_minutes * 60) % 60)
                
                if hours > 0:
                    duration_str = f"{hours}h {minutes}m {seconds}s"
                elif minutes > 0:
                    duration_str = f"{minutes}m {seconds}s"
                else:
                    duration_str = f"{seconds}s"
        
        if duration_str:
            print(Fore.YELLOW + Style.BRIGHT + "\nDuration Information:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Total Duration: " + Fore.YELLOW + Style.BRIGHT + f"{duration_str}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Total Minutes: " + Fore.YELLOW + Style.BRIGHT + f"{total_time_minutes:.1f}")
        
        # Express setup context display
        if express_context:
            print(Fore.YELLOW + Style.BRIGHT + "\nExpress Setup Context:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Intensity: " + Fore.YELLOW + Style.BRIGHT + f"{express_intensity}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Focus: " + Fore.YELLOW + Style.BRIGHT + f"{optimization_focus.title()}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class.upper()}")

        # Handle failure cases with error information
        if not success:
            error_msg = results.get('error', 'Unknown error occurred')
            error_type = results.get('error_type', 'UnknownError')
            interrupted = results.get('interrupted', False)
            cancelled = results.get('cancelled', False)
            
            print(Fore.RED + Style.BRIGHT + "\nError Details:")
            if interrupted or cancelled:
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Type: " + Fore.YELLOW + Style.BRIGHT + f"User Interruption")
                print(Fore.WHITE + Style.BRIGHT + f"  └─ Message: " + Fore.YELLOW + Style.BRIGHT + f"Operation cancelled by user")
            else:
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Type: " + Fore.YELLOW + Style.BRIGHT + f"{error_type}")
                print(Fore.WHITE + Style.BRIGHT + f"  └─ Message: " + Fore.YELLOW + Style.BRIGHT + f"{error_msg}")
            
            # Show stage completion information for partial results
            stages_completed = results.get('stages_completed', 0)
            total_stages = results.get('total_stages', 5)
            current_stage = results.get('current_stage', 'Unknown')
            
            if stages_completed > 0:
                print(Fore.CYAN + Style.BRIGHT + "\nProgress Before Failure:")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Stages Completed: " + Fore.GREEN + Style.BRIGHT + f"{stages_completed}/{total_stages}")
                print(Fore.WHITE + Style.BRIGHT + f"  └─ Failed During: " + Fore.RED + Style.BRIGHT + f"{current_stage}")

            # Show continuation information if this was a continuation attempt
            continuation_info = results.get('launch_config', {}).get('continuation_info')
            if continuation_info:
                print(Fore.CYAN + Style.BRIGHT + f"\nContinuation Context:")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Original Study: " + Fore.GREEN + Style.BRIGHT + f"{continuation_info.get('original_study', 'Unknown')}")
                print(Fore.WHITE + Style.BRIGHT + f"  └─ Previous Trials: " + Fore.YELLOW + Style.BRIGHT + f"{continuation_info.get('completed_trials', 0)}")
            
            # Show partial results if any trials completed
            n_completed = results.get('n_trials_completed', results.get('trials_completed', 0))
            n_total = results.get('n_trials_total', results.get('total_trials', 0))
            
            if n_completed > 0:
                print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
                print(Fore.MAGENTA + Style.BRIGHT + "PARTIAL OPTIMIZATION RESULTS")
                print(Fore.CYAN + Style.BRIGHT + "-"*40)
                
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Trials Completed: " + Fore.GREEN + Style.BRIGHT + f"{n_completed}/{n_total}")
                
                best_value = results.get('best_value')
                if best_value is not None and best_value != float('inf'):
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Best Value Found: " + Fore.GREEN + Style.BRIGHT + f"{best_value:.6f}")
                    
                    best_params = results.get('best_params', {})
                    if best_params:
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Best Parameters (Top 10):")
                        # Display top 10 parameters in organized format
                        param_items = list(best_params.items())[:10]
                        for i, (param, value) in enumerate(param_items, 1):
                            prefix = "    └─" if i == len(param_items) else "    ├─"
                            if isinstance(value, float):
                                if abs(value) < 0.001:
                                    formatted_value = f"{value:.2e}"
                                else:
                                    formatted_value = f"{value:.4f}"
                            elif isinstance(value, (list, tuple)):
                                formatted_value = str(list(value) if isinstance(value, tuple) else value)
                            else:
                                formatted_value = str(value)
                            
                            print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.GREEN + Style.BRIGHT + f"{param}: {formatted_value}")
                        
                        if len(best_params) > 10:
                            print(Fore.CYAN + Style.BRIGHT + f"    └─ ... and " + Fore.GREEN + Style.BRIGHT + f"{len(best_params) - 10}" + Fore.CYAN + Style.BRIGHT + " more parameters")
                
                # Show trial statistics for partial results
                n_pruned = results.get('n_trials_pruned', 0)
                n_failed = results.get('n_trials_failed', 0)
                
                if n_pruned > 0 or n_failed > 0:
                    print(Fore.CYAN + Style.BRIGHT + f"\nTrial Breakdown:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Completed: " + Fore.GREEN + Style.BRIGHT + f"{n_completed}")
                    if n_pruned > 0:
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Pruned: " + Fore.YELLOW + Style.BRIGHT + f"{n_pruned}")
                    if n_failed > 0:
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Failed: " + Fore.RED + Style.BRIGHT + f"{n_failed}")
            
            # Display recovery recommendations
            recommendations = results.get('recommendations', [])
            if recommendations:
                print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
                print(Fore.MAGENTA + Style.BRIGHT + "RECOVERY RECOMMENDATIONS")
                print(Fore.CYAN + Style.BRIGHT + "-"*40)
                
                for i, rec in enumerate(recommendations, 1):
                    prefix = "  └─" if i == len(recommendations) else "  ├─"
                    print(Fore.WHITE + Style.BRIGHT + f"{prefix} {i}. " + Fore.YELLOW + Style.BRIGHT + f"{rec}")
            else:
                # Provide default recommendations based on error type and context
                default_recommendations = []
                if error_type in ['ConfigurationError', 'ValueError']:
                    default_recommendations.extend([
                        'Review and validate configuration parameters',
                        'Check data paths and file availability',
                        'Verify model architecture compatibility'
                    ])
                elif error_type in ['RuntimeError', 'CUDA_ERROR']:
                    default_recommendations.extend([
                        'Check GPU memory availability and reduce batch size if needed',
                        'Verify CUDA installation and compatibility',
                        'Consider switching to CPU device for testing'
                    ])
                elif error_type in ['MemoryError', 'OutOfMemoryError']:
                    default_recommendations.extend([
                        'Reduce number of trials or model complexity',
                        'Decrease batch size and number of workers',
                        'Enable memory optimization settings'
                    ])
                else:
                    default_recommendations.extend([
                        'Check system resources (CPU, memory, disk space)',
                        'Review error logs for detailed information',
                        'Consider using a simpler configuration or preset',
                        'Verify all required dependencies are installed'
                    ])
                
                if default_recommendations:
                    print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.MAGENTA + Style.BRIGHT + "GENERAL RECOMMENDATIONS")
                    print(Fore.CYAN + Style.BRIGHT + "-"*40)
                    
                    for i, rec in enumerate(default_recommendations, 1):
                        prefix = "  └─" if i == len(default_recommendations) else "  ├─"
                        print(Fore.WHITE + Style.BRIGHT + f"{prefix} {i}. " + Fore.YELLOW + Style.BRIGHT + f"{rec}")
            
            # Show error log path if available
            error_log_path = results.get('error_log_path')
            if error_log_path:
                print(Fore.CYAN + Style.BRIGHT + f"\nAdditional Information:")
                print(Fore.RED + Style.BRIGHT + f"  └─ Detailed error information saved to: " + Fore.YELLOW + Style.BRIGHT + f"{error_log_path}")
            
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            return
        
        # SUCCESS CASE - Display results
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "TRIAL STATISTICS")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        n_trials_total = results.get('n_trials_total', results.get('total_trials', 0))
        n_trials_completed = results.get('n_trials_completed', results.get('trials_completed', 0))
        n_trials_pruned = results.get('n_trials_pruned', results.get('trials_pruned', 0))
        n_trials_failed = results.get('n_trials_failed', results.get('trials_failed', 0))
        
        # Calculate completion rate and efficiency metrics
        completion_rate = (n_trials_completed / n_trials_total * 100) if n_trials_total > 0 else 0
        pruning_rate = (n_trials_pruned / n_trials_total * 100) if n_trials_total > 0 else 0
        failure_rate = (n_trials_failed / n_trials_total * 100) if n_trials_total > 0 else 0
        
        print(Fore.YELLOW + Style.BRIGHT + "Trial Overview:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Total Trials: " + Fore.WHITE + Style.BRIGHT + f"{n_trials_total}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Completed: " + Fore.GREEN + Style.BRIGHT + f"{n_trials_completed} ({completion_rate:.1f}%)")
        if n_trials_pruned > 0:
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Pruned: " + Fore.YELLOW + Style.BRIGHT + f"{n_trials_pruned} ({pruning_rate:.1f}%)")
        if n_trials_failed > 0:
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Failed: " + Fore.RED + Style.BRIGHT + f"{n_trials_failed} ({failure_rate:.1f}%)")
        
        # Display efficiency assessment
        if completion_rate >= 90:
            efficiency = "Excellent"
            efficiency_color = Fore.GREEN
        elif completion_rate >= 75:
            efficiency = "Very Good"
            efficiency_color = Fore.GREEN
        elif completion_rate >= 60:
            efficiency = "Good"
            efficiency_color = Fore.YELLOW
        elif completion_rate >= 40:
            efficiency = "Fair"
            efficiency_color = Fore.YELLOW
        else:
            efficiency = "Needs Improvement"
            efficiency_color = Fore.RED
        
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Optimization Efficiency: " + efficiency_color + Style.BRIGHT + f"{efficiency}")
        
        if n_trials_completed == 0:
            print(Fore.YELLOW + Style.BRIGHT + "\nOptimization Status:")
            print(Fore.CYAN + Style.BRIGHT + "  ├─ Result: " + Fore.RED + Style.BRIGHT + "No trials completed successfully")
            print(Fore.CYAN + Style.BRIGHT + "  └─ Recommendation: " + Fore.YELLOW + Style.BRIGHT + "Please check your configuration and system resources.")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            return
        
        # BEST TRIAL RESULTS
        best_value = results.get('best_value')
        best_params = results.get('best_params', {})
        best_trial_number = results.get('best_trial_number')
        
        if best_value is not None and best_value != float('inf'):
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "BEST TRIAL RESULTS")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            print(Fore.GREEN + Style.BRIGHT + "Performance Summary:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Best Objective Value: " + Fore.GREEN + Style.BRIGHT + f"{best_value:.6f}")
            if best_trial_number is not None:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Best Trial Number: " + Fore.GREEN + Style.BRIGHT + f"{best_trial_number}")
            
            # Performance assessment based on objective value and direction
            study = results.get('study')
            direction = 'minimize'  # Default
            if study and hasattr(study, 'direction'):
                direction = study.direction.name.lower()
            else:
                direction = results.get('direction', 'minimize').lower()
            
            if direction == 'maximize':
                if best_value > 0.95:
                    performance = "Outstanding"
                    performance_color = Fore.GREEN
                elif best_value > 0.85:
                    performance = "Excellent"
                    performance_color = Fore.GREEN
                elif best_value > 0.75:
                    performance = "Very Good"
                    performance_color = Fore.YELLOW
                elif best_value > 0.65:
                    performance = "Good"
                    performance_color = Fore.YELLOW
                else:
                    performance = "Needs Improvement"
                    performance_color = Fore.RED
            else:  # minimize direction
                if best_value < 0.01:
                    performance = "Outstanding"
                    performance_color = Fore.GREEN
                elif best_value < 0.05:
                    performance = "Excellent"
                    performance_color = Fore.GREEN
                elif best_value < 0.1:
                    performance = "Very Good"
                    performance_color = Fore.YELLOW
                elif best_value < 0.2:
                    performance = "Good"
                    performance_color = Fore.YELLOW
                else:
                    performance = "Needs Improvement"
                    performance_color = Fore.RED
            
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Performance Assessment: " + performance_color + Style.BRIGHT + f"{performance}")

            # Optimization focus achievement
            if optimization_focus == 'speed' and total_time_minutes < (n_trials_total * 2):
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Speed Focus: " + Fore.GREEN + Style.BRIGHT + f"Achieved")
            elif optimization_focus == 'accuracy' and performance in ['Outstanding', 'Excellent']:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Accuracy Focus: " + Fore.GREEN + Style.BRIGHT + f"Achieved")
            elif optimization_focus == 'balanced' and efficiency in ['Excellent', 'Very Good']:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Balanced Focus: " + Fore.GREEN + Style.BRIGHT + f"Achieved")
            
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Parameters Optimized: " + Fore.GREEN + Style.BRIGHT + f"{len(best_params)}")
            
            # Display best parameters with intelligent categorization
            if best_params:
                print(Fore.GREEN + Style.BRIGHT + "\nOptimal Hyperparameters:")
                
                # Define parameter categories for organized display
                param_categories = {
                    'Model Architecture': {
                        'params': ['model_type', 'encoding_dim', 'hidden_arch', 'enhanced_arch', 'ensemble_arch', 
                                 'activation', 'normalization', 'num_models', 'diversity_factor'],
                        'color': Fore.GREEN
                    },
                    'Training Configuration': {
                        'params': ['learning_rate', 'batch_size', 'weight_decay', 'optimizer_type', 'scheduler_type', 
                                 'gradient_clip', 'gradient_accumulation_steps'],
                        'color': Fore.BLUE
                    },
                    'Regularization': {
                        'params': ['dropout_0', 'dropout_1', 'dropout_2', 'dropout_3', 'dropout_rate'],
                        'color': Fore.MAGENTA
                    },
                    'Advanced Features': {
                        'params': ['use_attention', 'residual_blocks', 'skip_connection', 'legacy_mode', 
                                 'mixed_precision', 'use_batch_norm', 'use_layer_norm'],
                        'color': Fore.CYAN
                    },
                    'Scheduler Parameters': {
                        'params': ['lr_patience', 'lr_factor', 'min_lr', 'step_size', 'gamma', 'eta_min'],
                        'color': Fore.YELLOW
                    },
                    'Detection Thresholds': {
                        'params': ['percentile', 'threshold_method', 'attack_threshold'],
                        'color': Fore.WHITE
                    }
                }
                
                displayed_params = set()
                
                # Display categorized parameters
                for category_name, category_info in param_categories.items():
                    category_params = {}
                    for param in category_info['params']:
                        if param in best_params:
                            category_params[param] = best_params[param]
                            displayed_params.add(param)
                    
                    if category_params:
                        print(category_info['color'] + Style.BRIGHT + f"  ├─ {category_name}:")
                        param_items = list(category_params.items())
                        for i, (param, value) in enumerate(param_items, 1):
                            prefix = "  │   └─" if i == len(param_items) else "  │   ├─"
                            # Format value based on type and magnitude
                            if isinstance(value, float):
                                if abs(value) < 0.001:
                                    formatted_value = f"{value:.2e}"
                                elif abs(value) < 0.01:
                                    formatted_value = f"{value:.4f}"
                                else:
                                    formatted_value = f"{value:.3f}"
                            elif isinstance(value, (list, tuple)):
                                formatted_value = str(list(value) if isinstance(value, tuple) else value)
                            elif isinstance(value, bool):
                                formatted_value = "✓" if value else "✗"
                            else:
                                formatted_value = str(value)
                            
                            print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.GREEN + Style.BRIGHT + f"{param}: {formatted_value}")
                
                # Display any remaining uncategorized parameters
                remaining_params = {k: v for k, v in best_params.items() if k not in displayed_params}
                if remaining_params:
                    print(Fore.WHITE + Style.BRIGHT + f"  └─ Other Parameters:")
                    remaining_items = list(remaining_params.items())
                    for i, (param, value) in enumerate(remaining_items, 1):
                        prefix = "      └─" if i == len(remaining_items) else "      ├─"
                        if isinstance(value, float):
                            if abs(value) < 0.001:
                                formatted_value = f"{value:.2e}"
                            else:
                                formatted_value = f"{value:.4f}"
                        else:
                            formatted_value = str(value)
                        print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.GREEN + Style.BRIGHT + f"{param}: {formatted_value}")
        
        # OPTIMIZATION CONFIGURATION
        configuration = results.get('configuration', {})
        optimization_config = results.get('optimization_config', {})
        model_types_optimized = results.get('model_types_optimized', [])
        launch_config = results.get('launch_config', {})

        if configuration or optimization_config or model_types_optimized or launch_config:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "OPTIMIZATION CONFIGURATION")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            # Display model types if available
            if model_types_optimized:
                print(Fore.GREEN + Style.BRIGHT + "Model Configuration:")
                model_count = len(model_types_optimized)
                if model_count == 1:
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + Style.BRIGHT + f"{model_types_optimized[0]}")
                else:
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Types: " + Fore.GREEN + Style.BRIGHT + f"{model_count} models")
                    for i, model_type in enumerate(model_types_optimized):
                        prefix = "  └─" if i == model_count - 1 else "  ├─"
                        print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.GREEN + Style.BRIGHT + f"{model_type}")
            
            # Display key configuration parameters from main configuration
            if configuration:
                print(Fore.GREEN + Style.BRIGHT + "\nOptimization Parameters:")
                config_to_display = {
                    'n_trials': 'Total Trials',
                    'timeout_minutes': 'Timeout (minutes)',
                    'cv_folds': 'Cross-Validation Folds',
                    'trial_epochs': 'Trial Epochs',
                    'sampler_type': 'Sampler',
                    'pruner_type': 'Pruner',
                    'optimization_focus': 'Optimization Focus'
                }
                
                config_items = [(display_name, configuration.get(config_key)) for config_key, display_name in config_to_display.items() if configuration.get(config_key) is not None]
                
                for i, (display_name, value) in enumerate(config_items):
                    prefix = "  └─" if i == len(config_items) - 1 else "  ├─"
                    print(Fore.CYAN + Style.BRIGHT + f"{prefix} {display_name}: " + Fore.GREEN + Style.BRIGHT + f"{value}")
            
            # Display additional configuration details from optimization_config and launch_config
            config_details = {}
            if optimization_config:
                config_details.update(optimization_config)
            if 'parameters_used' in launch_config:
                config_details.update(launch_config['parameters_used'])
            
            # Filter out already displayed parameters and model types
            excluded_keys = ['model_types', 'model_types_optimized'] + list(config_to_display.keys() if 'config_to_display' in locals() else [])
            additional_config_items = [(k, v) for k, v in config_details.items() if k not in excluded_keys and v is not None]
            
            if additional_config_items:
                print(Fore.GREEN + Style.BRIGHT + "\nAdditional Configuration:")
                for i, (key, value) in enumerate(additional_config_items, 1):
                    prefix = "  └─" if i == len(additional_config_items) else "  ├─"
                    formatted_key = key.replace('_', ' ').title()
                    print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.YELLOW + Style.BRIGHT + f"{formatted_key}: " + Fore.GREEN + Style.BRIGHT + f"{value}")

        # CROSS-VALIDATION RESULTS
        study = results.get('study')
        cv_results_displayed = False
        
        if study and hasattr(study, 'best_trial') and study.best_trial:
            best_trial = study.best_trial
            if hasattr(best_trial, 'user_attrs') and best_trial.user_attrs:
                user_attrs = best_trial.user_attrs
                
                # Cross-validation scores
                mean_cv_score = user_attrs.get('mean_cv_score')
                std_cv_score = user_attrs.get('std_cv_score')
                individual_scores = user_attrs.get('individual_fold_scores', [])
                valid_folds = user_attrs.get('valid_folds')
                
                if mean_cv_score is not None:
                    print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
                    print(Fore.MAGENTA + Style.BRIGHT + "CROSS-VALIDATION RESULTS")
                    print(Fore.CYAN + Style.BRIGHT + "-"*40)
                    cv_results_displayed = True
                    
                    print(Fore.GREEN + Style.BRIGHT + "Cross-Validation Performance:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Mean CV Score: " + Fore.GREEN + Style.BRIGHT + f"{mean_cv_score:.6f}")
                    if std_cv_score is not None:
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Standard Deviation: " + Fore.GREEN + Style.BRIGHT + f"±{std_cv_score:.6f}")
                        # Calculate confidence interval
                        confidence_lower = mean_cv_score - 1.96 * std_cv_score
                        confidence_upper = mean_cv_score + 1.96 * std_cv_score
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ 95% Confidence Interval: " + Fore.GREEN + Style.BRIGHT + f"[{confidence_lower:.6f}, {confidence_upper:.6f}]")
                    
                    if valid_folds is not None and individual_scores:
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Valid Folds: " + Fore.GREEN + Style.BRIGHT + f"{valid_folds}/{len(individual_scores)}")
                        
                        if len(individual_scores) > 1:
                            valid_scores = [s for s in individual_scores if s != float('inf') and s is not None]
                            if valid_scores:
                                print(Fore.CYAN + Style.BRIGHT + f"  └─ Individual Fold Scores:")
                                for i, score in enumerate(individual_scores):
                                    prefix = "      └─" if i == len(individual_scores) - 1 else "      ├─"
                                    if score != float('inf') and score is not None:
                                        score_color = Fore.GREEN if score == min(valid_scores) else Fore.YELLOW if score == max(valid_scores) else Fore.WHITE
                                        print(Fore.WHITE + Style.BRIGHT + f"{prefix} " + score_color + Style.BRIGHT + f"Fold {i+1}: {score:.6f}")
                                    else:
                                        print(Fore.WHITE + Style.BRIGHT + f"{prefix} " + Fore.RED + Style.BRIGHT + f"Fold {i+1}: Failed")
                                
                                # Calculate fold consistency metrics
                                if len(valid_scores) > 1:
                                    score_range = max(valid_scores) - min(valid_scores)
                                    relative_std = (std_cv_score / abs(mean_cv_score)) * 100 if mean_cv_score != 0 and std_cv_score else 0
                                    
                                    print(Fore.GREEN + Style.BRIGHT + f"\nFold Consistency Analysis:")
                                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Score Range: " + Fore.GREEN + Style.BRIGHT + f"{score_range:.6f}")
                                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Relative Std Dev: " + Fore.GREEN + Style.BRIGHT + f"{relative_std:.1f}%")
                                    
                                    if relative_std < 5:
                                        consistency = "Excellent"
                                        consistency_color = Fore.GREEN
                                    elif relative_std < 10:
                                        consistency = "Good"
                                        consistency_color = Fore.YELLOW
                                    elif relative_std < 20:
                                        consistency = "Fair"
                                        consistency_color = Fore.BLUE
                                    else:
                                        consistency = "Poor"
                                        consistency_color = Fore.RED
                                    
                                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Consistency Rating: " + consistency_color + Style.BRIGHT + f"{consistency}")
                
                # Training time information
                fold_times = []
                for i in range(10):  # Check up to 10 folds
                    time_key = f'fold_{i}_training_time'
                    if time_key in user_attrs:
                        fold_times.append(user_attrs[time_key])
                
                if fold_times and cv_results_displayed:
                    avg_fold_time = sum(fold_times) / len(fold_times)
                    total_fold_time = sum(fold_times)
                    print(Fore.GREEN + Style.BRIGHT + f"\nTraining Time Analysis:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Average per Fold: " + Fore.GREEN + Style.BRIGHT + f"{avg_fold_time:.1f} minutes")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Total Training Time: " + Fore.GREEN + Style.BRIGHT + f"{total_fold_time:.1f} minutes")
                    
                    # Estimate time for different fold counts
                    if avg_fold_time > 0:
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Estimated Time for 3-fold: " + Fore.GREEN + Style.BRIGHT + f"{avg_fold_time * 3:.1f} minutes")
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Estimated Time for 5-fold: " + Fore.GREEN + Style.BRIGHT + f"{avg_fold_time * 5:.1f} minutes")
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time for 10-fold: " + Fore.GREEN + Style.BRIGHT + f"{avg_fold_time * 10:.1f} minutes")
        
        # STAGE COMPLETION AND TIMING INFORMATION
        stage_timings = results.get('stage_timings', {})
        stages_completed = results.get('stages_completed', 0)
        
        if stage_timings and stages_completed > 0:
            print(Fore.MAGENTA + Style.BRIGHT + "\nStage Completion & Timing:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Stages Completed: " + Fore.YELLOW + Style.BRIGHT + f"{stages_completed}/5")
            
            stage_names = {
                'setup': 'Setup',
                'optimization': 'Optimization',
                'analysis': 'Analysis',
                'final_training': 'Final Training',
                'finalization': 'Finalization'
            }
            
            stage_items = []
            for stage_key, stage_name in stage_names.items():
                if stage_key in stage_timings:
                    stage_time = stage_timings[stage_key]
                    minutes = stage_time / 60
                    stage_items.append((stage_name, minutes))
            
            for i, (stage_name, minutes) in enumerate(stage_items):
                prefix = "  └─" if i == len(stage_items) - 1 else "  ├─"
                print(Fore.GREEN + Style.BRIGHT + f"{prefix} {stage_name}: " + Fore.YELLOW + Style.BRIGHT + f"{minutes:.1f}m")

        # DATA CONFIGURATION
        data_config = results.get('data_config', {})
        if data_config:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "DATA CONFIGURATION")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            print(Fore.GREEN + Style.BRIGHT + "Dataset Information:")
            use_real_data = data_config.get('use_real_data', False)
            data_source = 'Real Network Data' if use_real_data else 'Synthetic Data'
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data Source: " + Fore.GREEN + Style.BRIGHT + f"{data_source}")
            
            if not use_real_data:
                normal_samples = data_config.get('normal_samples', 0)
                attack_samples = data_config.get('attack_samples', 0)
                if normal_samples > 0 or attack_samples > 0:
                    total_samples = normal_samples + attack_samples
                    attack_ratio = (attack_samples / total_samples * 100) if total_samples > 0 else 0
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Dataset Size: " + Fore.GREEN + Style.BRIGHT + f"{total_samples:,} samples")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.GREEN + Style.BRIGHT + f"{normal_samples:,}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.GREEN + Style.BRIGHT + f"{attack_samples:,}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Attack Ratio: " + Fore.GREEN + Style.BRIGHT + f"{attack_ratio:.1f}%")
            
            features = data_config.get('features', 0)
            if features > 0:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Features: " + Fore.GREEN + Style.BRIGHT + f"{features}")
            
            normalization = data_config.get('normalization', 'N/A')
            if normalization != 'N/A':
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Normalization: " + Fore.GREEN + Style.BRIGHT + f"{normalization}")
            
            cv_folds = data_config.get('cv_folds', 3)
            if cv_folds >= 1:
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Cross-Validation Folds: " + Fore.GREEN + Style.BRIGHT + f"{cv_folds}")
        
        # PERFORMANCE ANALYSIS
        analysis = results.get('analysis', {})
        if analysis:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "PERFORMANCE ANALYSIS")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            # Display study summary metrics
            study_summary = analysis.get('study_summary', {})
            if study_summary:
                print(Fore.GREEN + Style.BRIGHT + "Study Summary:")
                summary_items = list(study_summary.items())
                for i, (metric, value) in enumerate(summary_items, 1):
                    prefix = "  └─" if i == len(summary_items) else "  ├─"
                    formatted_metric = metric.replace('_', ' ').title()
                    if isinstance(value, float):
                        print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.YELLOW + Style.BRIGHT + f"{formatted_metric}: " + Fore.GREEN + Style.BRIGHT + f"{value:.4f}")
                    else:
                        print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.YELLOW + Style.BRIGHT + f"{formatted_metric}: " + Fore.GREEN + Style.BRIGHT + f"{value}")
            
            # Parameter importance analysis
            param_importance = analysis.get('parameter_importance', {})
            if param_importance:
                print(Fore.GREEN + Style.BRIGHT + f"\nParameter Importance Analysis:")
                sorted_importance = sorted(param_importance.items(), key=lambda x: x[1], reverse=True)
                
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Top 10 Most Important Parameters:")
                for i, (param, importance) in enumerate(sorted_importance[:10], 1):
                    # Create importance bar visualization
                    bar_length = int(importance * 20)  # Scale to 20 characters
                    bar = "█" * bar_length + "░" * (20 - bar_length)
                    prefix = "  │   └─" if i == len(sorted_importance[:10]) else "  │   ├─"
                    importance_color = Fore.GREEN if importance > 0.1 else Fore.YELLOW if importance > 0.05 else Fore.RED
                    print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + importance_color + Style.BRIGHT + f"{param:<25}  │{bar}│ {importance:.4f}")
                
                if len(sorted_importance) > 10:
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ ... and " + Fore.GREEN + Style.BRIGHT + f"{len(sorted_importance) - 10}" + Fore.CYAN + Style.BRIGHT + " more parameters")
            
            # Optimization efficiency metrics
            if n_trials_total > 0:
                efficiency_metrics = {
                    'Trial Completion Rate': (n_trials_completed / n_trials_total) * 100,
                    'Trial Success Rate': ((n_trials_completed + n_trials_pruned) / n_trials_total) * 100 if n_trials_total > 0 else 0
                }
                
                if n_trials_pruned > 0:
                    efficiency_metrics['Pruning Effectiveness'] = (n_trials_pruned / n_trials_total) * 100
                
                print(Fore.GREEN + Style.BRIGHT + f"\nOptimization Efficiency Metrics:")
                efficiency_items = list(efficiency_metrics.items())
                for i, (metric, value) in enumerate(efficiency_items, 1):
                    prefix = "  └─" if i == len(efficiency_items) else "  ├─"
                    value_color = Fore.GREEN if value > 80 else Fore.YELLOW if value > 60 else Fore.RED
                    print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.GREEN + Style.BRIGHT + f"{metric}: " + value_color + Style.BRIGHT + f"{value:.1f}%")
        
        # CONVERGENCE ANALYSIS
        optimization_history = results.get('optimization_history', [])
        if optimization_history and len(optimization_history) > 5:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "CONVERGENCE ANALYSIS")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            # Extract valid values from optimization history
            valid_entries = [(i, entry) for i, entry in enumerate(optimization_history) 
                           if entry.get('value') is not None and entry.get('value') != float('inf')]
            
            if valid_entries:
                values = [entry[1]['value'] for entry in valid_entries]
                indices = [entry[0] for entry in valid_entries]
                
                if len(values) > 1:
                    initial_best = values[0]
                    final_best = min(values)
                    
                    # Calculate improvement
                    if initial_best != 0:
                        improvement = abs((final_best - initial_best) / initial_best) * 100
                        improvement_direction = "improvement" if final_best < initial_best else "degradation"
                    else:
                        improvement = 0
                        improvement_direction = "stable"
                    
                    print(Fore.GREEN + Style.BRIGHT + "Convergence Metrics:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Initial Best Value: " + Fore.GREEN + Style.BRIGHT + f"{initial_best:.6f}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Final Best Value: " + Fore.GREEN + Style.BRIGHT + f"{final_best:.6f}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Total Change: " + Fore.GREEN + Style.BRIGHT + f"{improvement:.1f}% {improvement_direction}")
                    
                    # Find convergence point
                    best_value_final = min(values)
                    best_trial_idx = None
                    for i, value in enumerate(values):
                        if value == best_value_final:
                            best_trial_idx = indices[i]
                            break
                    
                    if best_trial_idx is not None:
                        convergence_point = (best_trial_idx + 1) / len(optimization_history) * 100
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Best Value Found: " + Fore.GREEN + Style.BRIGHT + f"Trial {best_trial_idx + 1} ({convergence_point:.1f}% through optimization)")
                        
                        # Assess convergence quality
                        if convergence_point < 25:
                            convergence_quality = "Early (Excellent search efficiency)"
                            convergence_color = Fore.GREEN
                        elif convergence_point < 50:
                            convergence_quality = "Mid-stage (Good exploration)"
                            convergence_color = Fore.YELLOW
                        elif convergence_point < 75:
                            convergence_quality = "Late-stage (Thorough search)"
                            convergence_color = Fore.BLUE
                        else:
                            convergence_quality = "Very late (May need more trials)"
                            convergence_color = Fore.RED
                        
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Convergence Assessment: " + convergence_color + Style.BRIGHT + f"{convergence_quality}")
                    
                    # Calculate convergence stability
                    if len(values) > 10:
                        last_10_values = values[-10:]
                        recent_variance = np.var(last_10_values) if len(last_10_values) > 1 else 0
                        stability_color = Fore.GREEN if recent_variance < 0.001 else Fore.YELLOW if recent_variance < 0.01 else Fore.RED
                        print(Fore.CYAN + Style.BRIGHT + f"\nRecent Optimization Stability: " + stability_color + Style.BRIGHT + f"{recent_variance:.6f} (lower is better)")
        
        # SYSTEM CONFIGURATION
        system_config = results.get('system_config', {})
        if system_config:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "SYSTEM CONFIGURATION")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            print(Fore.GREEN + Style.BRIGHT + "System Overview:")
            device = system_config.get('device', 'N/A')
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Compute Device: " + Fore.GREEN + Style.BRIGHT + f"{device}")
            
            if system_config.get('random_seed') is not None:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Random Seed: " + Fore.GREEN + Style.BRIGHT + f"{system_config['random_seed']}")
            
            if system_config.get('num_workers') is not None:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data Loading Workers: " + Fore.GREEN + Style.BRIGHT + f"{system_config['num_workers']}")
            
            if system_config.get('parallel_jobs', 1) > 1:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Parallel Jobs: " + Fore.GREEN + Style.BRIGHT + f"{system_config['parallel_jobs']}")
            
            # Display hardware utilization if available
            hardware_info = system_config.get('hardware_info', {})
            if hardware_info:
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Hardware Utilization:")
                hardware_items = list(hardware_info.items())
                for i, (component, info) in enumerate(hardware_items, 1):
                    prefix = "      └─" if i == len(hardware_items) else "      ├─"
                    if isinstance(info, dict) and 'utilization' in info:
                        util_value = info['utilization']
                        util_color = Fore.GREEN if util_value < 70 else Fore.YELLOW if util_value < 90 else Fore.RED
                        print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.YELLOW + Style.BRIGHT + f"{component.title()}: " + util_color + Style.BRIGHT + f"{util_value}%")
        
        # SAVED ARTIFACTS AND FILES
        saved_files = results.get('saved_files', {})
        plots = results.get('plots', {})
        
        if saved_files or plots:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "GENERATED ARTIFACTS")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            if saved_files:
                print(Fore.WHITE + Style.BRIGHT + "Study Files:")
                file_items = list(saved_files.items())
                for i, (file_type, file_path) in enumerate(file_items, 1):
                    prefix = "  └─" if i == len(file_items) else "  ├─"
                    file_name = file_type.replace('_', ' ').title()
                    print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.YELLOW + Style.BRIGHT + f"{file_name}: " + Fore.GREEN + Style.BRIGHT + f"{file_path}")
                    
                    # Display file size if available
                    try:
                        if isinstance(file_path, (str, Path)):
                            file_size = Path(file_path).stat().st_size
                            if file_size > 1024 * 1024:
                                size_str = f"({file_size / (1024 * 1024):.1f} MB)"
                            elif file_size > 1024:
                                size_str = f"({file_size / 1024:.1f} KB)"
                            else:
                                size_str = f"({file_size} bytes)"
                            print(Fore.CYAN + Style.BRIGHT + f"    └─ Size: " + Fore.GREEN + Style.BRIGHT + f"{size_str}")
                    except Exception:
                        pass
            
            if plots:
                print(Fore.GREEN + Style.BRIGHT + "\nVisualization Plots:")
                plot_items = list(plots.items())
                for i, (plot_type, plot_path) in enumerate(plot_items, 1):
                    prefix = "  └─" if i == len(plot_items) else "  ├─"
                    plot_name = plot_type.replace('_', ' ').title()
                    print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.YELLOW + Style.BRIGHT + f"{plot_name}: " + Fore.GREEN + Style.BRIGHT + f"{plot_path}")
        
        # RECOMMENDATIONS
        recommendations = results.get('recommendations', [])
        
        # Generate intelligent recommendations based on results
        auto_recommendations = []
        
        if best_value is not None and best_value != float('inf'):
            if hasattr(results.get('study'), 'direction') and results.get('study').direction.name == 'MAXIMIZE':
                if best_value < 0.7:
                    auto_recommendations.append("Consider expanding search space or increasing trial count for better performance")
            else:  # Minimize direction
                if best_value > 0.1:
                    auto_recommendations.append("High objective value suggests need for search space refinement or more trials")
                elif best_value < 0.01:
                    auto_recommendations.append("Excellent results achieved - ready for production deployment")
        
        if completion_rate < 60:
            auto_recommendations.append("Low completion rate - consider simplifying model or increasing resources")
        
        if n_trials_failed > n_trials_completed // 2:
            auto_recommendations.append("High failure rate detected - review error logs and system resources")
        
        # Combine user recommendations with auto-generated ones
        all_recommendations = list(recommendations) + auto_recommendations
        
        if all_recommendations:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "RECOMMENDATIONS")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            for i, rec in enumerate(all_recommendations, 1):
                prefix = "  └─" if i == len(all_recommendations) else "  ├─"
                print(Fore.WHITE + Style.BRIGHT + f"{prefix} {i}. " + Fore.YELLOW + Style.BRIGHT + f"{rec}")
        
        # FINAL MODEL TRAINING RESULTS
        final_model_training = results.get('final_model_training', {})
        if final_model_training:
            print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
            print(Fore.MAGENTA + Style.BRIGHT + "FINAL MODEL TRAINING")
            print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            if final_model_training.get('success', False):
                print(Fore.CYAN + Style.BRIGHT + "  ├─ Status: " + Fore.GREEN + Style.BRIGHT + f"SUCCESS")
                
                training_time = final_model_training.get('training_time_minutes', 0)
                if training_time > 0:
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training Time: " + Fore.GREEN + Style.BRIGHT + f"{training_time:.1f} minutes")
                
                final_metrics = final_model_training.get('final_metrics', {})
                if final_metrics:
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Final Model Performance:")
                    metric_items = list(final_metrics.items())
                    for i, (metric, value) in enumerate(metric_items, 1):
                        prefix = "  │   └─" if i == len(metric_items) else "  │   ├─"
                        metric_name = metric.replace('_', ' ').title()
                        if isinstance(value, float):
                            print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.YELLOW + Style.BRIGHT + f"{metric_name}: " + Fore.GREEN + Style.BRIGHT + f"{value:.6f}")
                        else:
                            print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.YELLOW + Style.BRIGHT + f"{metric_name}: " + Fore.GREEN + Style.BRIGHT + f"{value}")
                
                artifacts = final_model_training.get('artifacts', {})
                if artifacts:
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Model Artifacts:")
                    artifact_items = list(artifacts.items())
                    for i, (artifact_type, path) in enumerate(artifact_items, 1):
                        prefix = "      └─" if i == len(artifact_items) else "      ├─"
                        artifact_name = artifact_type.replace('_', ' ').title()
                        print(Fore.CYAN + Style.BRIGHT + f"{prefix} " + Fore.YELLOW + Style.BRIGHT + f"{artifact_name}: " + Fore.GREEN + Style.BRIGHT + f"{path}")
            else:
                print(Fore.RED + Style.BRIGHT + f"  ├─ Status: " + Fore.YELLOW + Style.BRIGHT + "FAILED")
                error = final_model_training.get('error', 'Unknown error')
                print(Fore.RED + Style.BRIGHT + f"  └─ Error: " + Fore.RED + Style.BRIGHT + f"{error}")
        
        # OPTIMIZATION SUMMARY AND NEXT STEPS
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "OPTIMIZATION SUMMARY & NEXT STEPS")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        if n_trials_completed > 0 and best_value is not None and best_value != float('inf'):
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Hyperparameter optimization completed successfully!")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Found optimal configuration with objective value: " + Fore.YELLOW + Style.BRIGHT + f"{best_value:.6f}")
            
            trials_completed_status = Fore.YELLOW + Style.BRIGHT + f"{n_trials_completed}"
            total_time_minutes_status = Fore.YELLOW + Style.BRIGHT + f"{total_time_minutes:.1f}"
            duration_msg = f"Completed {trials_completed_status} trials in {total_time_minutes_status} minutes" if 'total_time_minutes' in locals() else f"Completed {trials_completed_status} trials"
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ {duration_msg}")
            
            # Performance assessment and recommendations
            objective_quality = "Unknown"
            objective_color = Fore.WHITE
            if hasattr(results.get('study'), 'direction') and results.get('study').direction.name == 'MAXIMIZE':
                if best_value > 0.9:
                    objective_quality = "Outstanding"
                    objective_color = Fore.GREEN
                elif best_value > 0.8:
                    objective_quality = "Excellent"
                    objective_color = Fore.GREEN
                elif best_value > 0.7:
                    objective_quality = "Good"
                    objective_color = Fore.YELLOW
                else:
                    objective_quality = "Needs Improvement"
                    objective_color = Fore.RED
            else:  # Minimize direction
                if best_value < 0.01:
                    objective_quality = "Outstanding"
                    objective_color = Fore.GREEN
                elif best_value < 0.05:
                    objective_quality = "Excellent"
                    objective_color = Fore.GREEN
                elif best_value < 0.1:
                    objective_quality = "Good"
                    objective_color = Fore.YELLOW
                else:
                    objective_quality = "Needs Improvement"
                    objective_color = Fore.RED
            
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Performance Quality: " + objective_color + Style.BRIGHT + f"{objective_quality}")
            
            print(Fore.CYAN + Style.BRIGHT + "  └─ Recommended Next Steps:")
            if objective_quality in ["Outstanding", "Excellent"]:
                print(Fore.GREEN + Style.BRIGHT + f"    ├─ Configuration is production-ready")
                print(Fore.GREEN + Style.BRIGHT + f"    ├─ Deploy model with optimized parameters")
                print(Fore.GREEN + Style.BRIGHT + f"    ├─ Monitor performance on real-world data")
                print(Fore.GREEN + Style.BRIGHT + f"    ├─ Set up automated retraining pipeline")
            elif objective_quality == "Good":
                print(Fore.YELLOW + Style.BRIGHT + f"    ├─ Review optimization plots for insights")
                print(Fore.YELLOW + Style.BRIGHT + f"    ├─ Consider extended optimization with more trials")
                print(Fore.YELLOW + Style.BRIGHT + f"    ├─ Fine-tune search space based on parameter importance")
                print(Fore.YELLOW + Style.BRIGHT + f"    ├─ Test configuration on validation dataset")
            else:
                print(Fore.RED + Style.BRIGHT + f"    ├─ Analyze failed and pruned trials for patterns")
                print(Fore.RED + Style.BRIGHT + f"    ├─ Expand search space or adjust ranges")
                print(Fore.RED + Style.BRIGHT + f"    ├─ Check system resources and configuration")
                print(Fore.RED + Style.BRIGHT + f"    ├─ Consider different optimization strategy")
            
            if final_model_training and final_model_training.get('success'):
                print(Fore.GREEN + Style.BRIGHT + f"    └─ Final trained model is ready for deployment")
            elif not final_model_training:
                print(Fore.YELLOW + Style.BRIGHT + f"    └─ Train final model using optimized parameters")
            
            if plots:
                print(Fore.CYAN + Style.BRIGHT + f"    └─ Review generated optimization visualizations")
        else:
            print(Fore.YELLOW + Style.BRIGHT + "  ├─ Hyperparameter optimization completed with limited success:")
            if n_trials_completed == 0:
                print(Fore.RED + Style.BRIGHT + f"    ├─ No trials completed successfully")
                print(Fore.RED + Style.BRIGHT + f"    └─ Check configuration, data paths, and system resources")
            else:
                n_trials_completed_status = Fore.GREEN + Style.BRIGHT + f"{n_trials_completed}"
                n_trials_total_status = Fore.GREEN + Style.BRIGHT + f"{n_trials_total}"
                print(Fore.YELLOW + Style.BRIGHT + f"    ├─ Only {n_trials_completed_status} out of {n_trials_total_status} trials completed")
                print(Fore.YELLOW + Style.BRIGHT + f"    └─ Consider adjusting search space or increasing timeout")
            
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Troubleshooting Steps:")
            print(Fore.WHITE + Style.BRIGHT + f"    ├─ Review error logs and failed trial information")
            print(Fore.WHITE + Style.BRIGHT + f"    ├─ Simplify search space or reduce model complexity")
            print(Fore.WHITE + Style.BRIGHT + f"    ├─ Check data availability and preprocessing")
            print(Fore.WHITE + Style.BRIGHT + f"    ├─ Verify system resources (CPU, memory, GPU)")
            print(Fore.WHITE + Style.BRIGHT + f"    └─ Try using preset configurations for stable baseline")
        
        # Study continuation information
        study_dir = results.get('study_dir')
        if study_dir and saved_files:
            print(Fore.GREEN + Style.BRIGHT + f"\nStudy Continuation:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Study data saved to: " + Fore.YELLOW + Style.BRIGHT + f"{study_dir}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Use 'Continue Existing Study' option to resume with additional trials")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Saved configurations can be loaded for similar optimizations")
        
        # Final summary statistics
        if 'total_time_minutes' in locals() and total_time_minutes > 0:
            trials_per_minute = n_trials_completed / total_time_minutes if total_time_minutes > 0 else 0
            print(Fore.GREEN + Style.BRIGHT + f"\nPerformance Statistics:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Throughput: " + Fore.GREEN + Style.BRIGHT + f"{trials_per_minute:.2f} trials per minute")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Total Operations: " + Fore.GREEN + Style.BRIGHT + f"{n_trials_completed} trials")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Total Time: " + Fore.GREEN + Style.BRIGHT + f"{total_time_minutes:.1f} minutes")
            
            if best_value is not None and best_value != float('inf') and n_trials_completed > 0:
                value_improvement_rate = abs(best_value) / total_time_minutes if total_time_minutes > 0 else 0
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Optimization Rate: " + Fore.GREEN + Style.BRIGHT + f"{value_improvement_rate:.6f} improvement per minute")
        
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + "  ├─ HPO results summary complete.")
        print(Fore.GREEN + Style.BRIGHT + "  └─ Check generated artifacts for detailed analysis and visualizations.")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
    except Exception as e:
        logger.error(f"Error displaying HPO results: {e}", exc_info=True)
        print(Fore.RED + Style.BRIGHT + f"\n  ├─ Could not display detailed HPO results summary: {str(e)}")
        print(Fore.YELLOW + Style.BRIGHT + f"  └─ Basic results are still available above.")
        
        # Fallback display for critical information
        try:
            print(Fore.GREEN + Style.BRIGHT + f"\nBasic Results Summary:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Success: " + Fore.GREEN + Style.BRIGHT + f"{results.get('success', 'Unknown')}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Study Name: " + Fore.GREEN + Style.BRIGHT + f"{results.get('study_name', 'Unknown')}")
            
            n_completed = results.get('n_trials_completed', 0)
            if n_completed > 0:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Trials Completed: " + Fore.GREEN + Style.BRIGHT + f"{n_completed}")
                best_value = results.get('best_value')
                if best_value and best_value != float('inf'):
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Best Value: " + Fore.GREEN + Style.BRIGHT + f"{best_value:.6f}")
            
            error = results.get('error')
            if error:
                print(Fore.RED + Style.BRIGHT + f"  ├─ Error: " + Fore.YELLOW + Style.BRIGHT + f"{error}")
            
            # Show saved files if available
            saved_files = results.get('saved_files', {})
            if saved_files:
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Saved Files:")
                for file_type, path in saved_files.items():
                    print(Fore.CYAN + Style.BRIGHT + f"    └─ {file_type}: " + Fore.GREEN + Style.BRIGHT + f"{path}")
                    
        except Exception as fallback_error:
            logger.error(f"Fallback display also failed: {fallback_error}")
            print(Fore.RED + Style.BRIGHT + "  ├─ Unable to display results due to formatting errors")
            print(Fore.RED + Style.BRIGHT + "  ├─ Check the raw results dictionary for detailed information")
            print(Fore.RED + Style.BRIGHT + "  └─ Review log files for error details and debugging information")

def _hpo_preset_selection_menu(config: Dict[str, Any], **kwargs) -> None:
    """
    Interactive menu for selecting HPO presets with full parameter compatibility.
    
    Args:
        config: Base configuration to use
        **kwargs: Additional parameters passed from main function for full compatibility
    """
    while True:
        try:
            # Clear screen and show banner
            print("\033c", end="")
            banner_config = show_banner(return_config=True)
            
            # Use the config returned from show_banner or fallback
            if config is None and banner_config is not None:
                config = banner_config
            elif config is None:
                config = get_current_config()
            
            # Extract all relevant parameters from kwargs for context display
            use_real_data = kwargs.get('use_real_data')
            use_current_config = kwargs.get('use_current_config', False)
            operation_mode = kwargs.get('operation_mode')
            data_mode = kwargs.get('data_mode')
            optimization_focus = kwargs.get('optimization_focus')
            trial_count = kwargs.get('trial_count')
            timeout_seconds = kwargs.get('timeout_seconds')
            study_name = kwargs.get('study_name')
            storage_url = kwargs.get('storage_url')
            model_types = kwargs.get('model_types')
            force_express = kwargs.get('force_express', False)
            skip_prompt = kwargs.get('skip_prompt', False)
            hardware_data = kwargs.get('hardware_data')
            enable_storage = kwargs.get('enable_storage')
            enable_plots = kwargs.get('enable_plots')
            custom_search_space = kwargs.get('custom_search_space')
            sampler_type = kwargs.get('sampler_type')
            pruner_type = kwargs.get('pruner_type')
            
            # Extract configuration sections with error handling
            hpo_config = config.get('hyperparameter_optimization', {})
            presets_config = config.get('presets', {})
            model_config = config.get('model', {})
            data_config = config.get('data', {})
            training_config = config.get('training', {})
            metadata = config.get('metadata', {})
            
            # Context extraction using multiple fallbacks
            preset_name = "default"
            model_type = "SimpleAutoencoder"
            config_source = "unknown"
            
            # Determine preset name from multiple sources
            if isinstance(presets_config, dict):
                preset_name = presets_config.get("current_preset", "default")
            
            if preset_name in ["default", None, "", "none"]:
                preset_name = metadata.get("preset_used", "default")
            
            if preset_name in ["default", None, "", "none"]:
                preset_name = config.get("_preset_name", "default")
            
            # Validate preset exists in PRESET_CONFIGS
            if preset_name not in PRESET_CONFIGS:
                logger.warning(f"Preset '{preset_name}' not found in PRESET_CONFIGS, using default")
                preset_name = "default"
            
            # Extract model type with validation
            if isinstance(model_config, dict):
                model_type = model_config.get('model_type', 'SimpleAutoencoder')
                # Validate model type against available variants
                if model_type not in MODEL_VARIANTS:
                    logger.warning(f"Model type '{model_type}' not available, using SimpleAutoencoder")
                    model_type = 'SimpleAutoencoder'
            
            # Extract config source with fallbacks
            if "runtime" in config and isinstance(config["runtime"], dict):
                config_source = config["runtime"].get("config_source", "runtime")
            elif "metadata" in config and isinstance(config["metadata"], dict):
                config_source = config["metadata"].get("config_source", "metadata")
            else:
                config_source = "loaded_config"
            
            # HPO-specific context extraction
            hpo_strategy = hpo_config.get('strategy', 'optuna')
            hpo_trials = hpo_config.get('n_trials', 50)
            hpo_timeout = hpo_config.get('timeout', 3600)
            hpo_enabled = hpo_config.get('enabled', False)
            hpo_sampler = hpo_config.get('sampler', 'TPESampler')
            hpo_pruner = hpo_config.get('pruner', 'MedianPruner')
            
            # Data configuration context
            normal_samples = data_config.get('normal_samples', 8000)
            attack_samples = data_config.get('attack_samples', 2000)
            features = data_config.get('features', 20)
            data_path = data_config.get('data_path', 'Default')
            use_real_data_config = data_config.get('use_real_data', False)
            
            # Resolve data mode from parameters
            if data_mode is None:
                if use_real_data_config:
                    data_mode = 'real'
                else:
                    data_mode = 'synthetic'
            
            # Hardware context extraction if available
            if hardware_data is None:
                try:
                    hardware_data = check_hardware(include_memory_usage=True)
                except Exception as e:
                    logger.debug(f"Hardware detection failed: {e}")
                    hardware_data = {}
            
            # Hardware-aware system class detection
            cuda_available = hardware_data.get('cuda', {}).get('available', False)
            memory_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
            cpu_cores = hardware_data.get('cpu_cores', {}).get('logical_cores', 4)
            
            # Determine system performance class
            if cuda_available and memory_gb >= 16 and cpu_cores >= 8:
                system_class = "high_performance"
            elif cuda_available and memory_gb >= 8:
                system_class = "performance"
            elif memory_gb >= 4:
                system_class = "standard"
            else:
                system_class = "limited"
            
            # Get available HPO-enabled presets with filtering
            hpo_presets = {}
            for name, preset_config in PRESET_CONFIGS.items():
                preset_hpo_config = preset_config.get('hyperparameter_optimization', {})
                preset_metadata = preset_config.get('metadata', {})
                preset_model_config = preset_config.get('model', {})
                
                # Check system compatibility
                hw_req = preset_metadata.get('recommended_hardware', {})
                preset_ram_gb = hw_req.get('ram_gb', 0)
                preset_cpu_cores = hw_req.get('cpu_cores', 0)
                
                # Determine system compatibility
                system_compatible = True
                if preset_ram_gb > 0 and memory_gb < preset_ram_gb:
                    system_compatible = False
                if preset_cpu_cores > 0 and cpu_cores < preset_cpu_cores:
                    system_compatible = False
                
                # Model compatibility
                compatibility = preset_metadata.get('compatibility', [])
                model_compatible = not compatibility or model_type in compatibility
                
                hpo_presets[name] = {
                    'config': preset_config,
                    'hpo_config': preset_hpo_config,
                    'description': preset_metadata.get('description', f'{name.title()} preset'),
                    'trials': preset_hpo_config.get('n_trials', 50),
                    'strategy': preset_hpo_config.get('strategy', 'optuna'),
                    'model_type': preset_model_config.get('model_type', 'Unknown'),
                    'timeout': preset_hpo_config.get('timeout', 3600),
                    'sampler': preset_hpo_config.get('sampler', 'TPESampler'),
                    'pruner': preset_hpo_config.get('pruner', 'MedianPruner'),
                    'enabled': preset_hpo_config.get('enabled', False),
                    'recommended_hardware': hw_req,
                    'compatibility': compatibility,
                    'system_compatible': system_compatible,
                    'model_compatible': model_compatible,
                    'optimization_focus': preset_metadata.get('optimization_focus', 'balanced'),
                    'generate_plots': preset_hpo_config.get('generate_plots', True),
                    'storage_enabled': preset_hpo_config.get('storage', {}).get('enabled', True)
                }
            
            # Header display with enhanced context
            print(Fore.MAGENTA + Style.BRIGHT + "HPO PRESET SELECTION MENU")
            print(Fore.CYAN + Style.BRIGHT + "-" * 50)
            print(Fore.YELLOW + Style.BRIGHT + "Current Configuration Context:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name.title()}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Config Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ Available HPO Presets: " + Fore.YELLOW + Style.BRIGHT + f"{len(hpo_presets)}")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ Data Mode: " + Fore.YELLOW + Style.BRIGHT + f"{data_mode}")
            
            # Display hardware context
            print(Fore.MAGENTA + Style.BRIGHT + "\nHardware Context:")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ CUDA Available: " + Fore.YELLOW + Style.BRIGHT + f"{cuda_available}")
            if cuda_available:
                gpu_count = hardware_data.get('cuda', {}).get('gpu_count', 0)
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ GPU Count: " + Fore.YELLOW + Style.BRIGHT + f"{gpu_count}")
            print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Memory: " + Fore.YELLOW + Style.BRIGHT + f"{memory_gb:.1f}GB")
            print(Fore.GREEN + Style.BRIGHT + f"  └─ CPU Cores: " + Fore.YELLOW + Style.BRIGHT + f"{cpu_cores}")
            
            # Display parameter overrides if provided
            override_params = []
            if trial_count is not None:
                override_params.append(f"Trial Count: {trial_count}")
            if timeout_seconds is not None:
                override_params.append(f"Timeout: {timeout_seconds}s")
            if sampler_type:
                override_params.append(f"Sampler: {sampler_type}")
            if pruner_type:
                override_params.append(f"Pruner: {pruner_type}")
            if optimization_focus:
                override_params.append(f"Optimization Focus: {optimization_focus}")
            if enable_storage is not None:
                override_params.append(f"Storage: {enable_storage}")
            if enable_plots is not None:
                override_params.append(f"Plots: {enable_plots}")
            if custom_search_space:
                override_params.append("Custom Search Space: Provided")
            if model_types:
                override_params.append(f"Model Types: {', '.join(model_types)}")
            if study_name:
                override_params.append(f"Study Name: {study_name}")
            if storage_url:
                override_params.append(f"Storage URL: {storage_url}")
            
            if override_params:
                print(Fore.CYAN + Style.BRIGHT + "\nParameter Overrides:")
                for i, param in enumerate(override_params):
                    prefix = "  └─" if i == len(override_params) - 1 else "  ├─"
                    print(Fore.GREEN + Style.BRIGHT + f"{prefix} {param}")
            
            # Handle no presets case with feedback
            if not hpo_presets:
                print(Fore.RED + Style.BRIGHT + f"\nNo HPO-enabled presets found in configuration.")
                print(Fore.YELLOW + Style.BRIGHT + f"\nCurrent HPO Configuration:")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ HPO Enabled: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_enabled}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Strategy: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_strategy}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Default Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_trials}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_timeout}s")
                print(Fore.GREEN + Style.BRIGHT + f"  └─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class}")
                
                print(Fore.YELLOW + Style.BRIGHT + f"\nRecommendations:")
                print(Fore.MAGENTA + Style.BRIGHT + f"  ├─ 1. Use Express HPO setup with intelligent defaults")
                print(Fore.MAGENTA + Style.BRIGHT + f"  ├─ 2. Create custom HPO configuration")
                print(Fore.MAGENTA + Style.BRIGHT + f"  ├─ 3. Check preset configuration files")
                print(Fore.MAGENTA + Style.BRIGHT + f"  └─ 4. Use current configuration with HPO enabled")
                
                # Handle non-interactive mode
                if skip_prompt:
                    kwargs['_recursion_depth'] = kwargs.get('_recursion_depth', 0) + 1
                    print(Fore.GREEN + Style.BRIGHT + f"\nNon-interactive mode - proceeding with express setup...")
                    result = run_hyperparameter_optimization_interactive(
                        config=config,
                        operation_mode='express',
                        non_interactive=True,
                        skip_prompt=True,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Express HPO")
                    return
                
                # Interactive input handling with retry logic
                confirm = None
                while not confirm:
                    try:
                        confirm = input(Fore.YELLOW + Style.BRIGHT + "\nProceed with express setup instead? (Y/n/c for custom): ").strip().lower()
                        if not confirm:
                            continue
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nReturning to HPO menu...")
                        return
                
                if confirm in ('', 'y', 'yes'):
                    kwargs['_recursion_depth'] = kwargs.get('_recursion_depth', 0) + 1
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching Express HPO setup...")
                    result = run_hyperparameter_optimization_interactive(
                        config=config,
                        operation_mode='express',
                        data_mode=data_mode,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Express HPO")
                    return
                elif confirm in ('c', 'custom'):
                    kwargs['_recursion_depth'] = kwargs.get('_recursion_depth', 0) + 1
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching Custom HPO setup...")
                    result = run_hyperparameter_optimization_interactive(
                        config=config,
                        operation_mode='custom',
                        data_mode=data_mode,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Custom HPO")
                    return
                else:
                    print(Fore.YELLOW + Style.BRIGHT + f"\nExpress setup cancelled.")
                    continue
            
            # Preset listing with context-aware display
            print(Fore.YELLOW + Style.BRIGHT + f"\nAvailable HPO Presets ({len(hpo_presets)}):")
            print(Fore.CYAN + Style.BRIGHT + "-" * 50)
            
            preset_list = list(hpo_presets.items())
            for i, (name, info) in enumerate(preset_list, 1):
                # Color coding based on multiple factors
                if info['system_compatible'] and info['model_compatible'] and info['enabled']:
                    status_color = Fore.GREEN
                    status_icon = "[OPTIMAL]"
                elif info['system_compatible'] and info['model_compatible']:
                    status_color = Fore.YELLOW
                    status_icon = "[COMPATIBLE]"
                elif info['system_compatible']:
                    status_color = Fore.BLUE
                    status_icon = "[SYSTEM OK]"
                else:
                    status_color = Fore.RED
                    status_icon = "[CHECK SYSTEM]"
                
                # Strategy color coding
                strategy_color = Fore.GREEN if info['strategy'] == 'optuna' else Fore.CYAN
                
                print(Fore.CYAN + Style.BRIGHT + f"{i}. {name.upper()} {status_color}{status_icon}{Style.RESET_ALL}")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Description: " + Fore.GREEN + Style.BRIGHT + f"{info['description']}")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Status: " + Fore.YELLOW + Style.BRIGHT + f"{'Enabled' if info['enabled'] else 'Available'}")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Trials: " + Fore.CYAN + Style.BRIGHT + f"{info['trials']}")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Strategy: " + strategy_color + Style.BRIGHT + f"{info['strategy'].upper()}")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{info['model_type']}")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Timeout: " + Fore.MAGENTA + Style.BRIGHT + f"{info['timeout']}s")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Focus: " + Fore.BLUE + Style.BRIGHT + f"{info['optimization_focus'].title()}")
                print(Fore.WHITE + Style.BRIGHT + f"  ├─ Sampler: " + Fore.BLUE + Style.BRIGHT + f"{info['sampler']}")
                print(Fore.WHITE + Style.BRIGHT + f"  └─ Pruner: " + Fore.BLUE + Style.BRIGHT + f"{info['pruner']}")
                
                # System compatibility information
                if not info['system_compatible']:
                    hw_req = info['recommended_hardware']
                    if hw_req:
                        print(Fore.RED + Style.BRIGHT + f"   [!] System: Requires {hw_req.get('cpu_cores', 'N/A')} cores, {hw_req.get('ram_gb', 'N/A')}GB RAM")
                
                # Model compatibility information
                if not info['model_compatible'] and info['compatibility']:
                    print(Fore.YELLOW + Style.BRIGHT + f"   [!] Model: Compatible with {', '.join(info['compatibility'])}")
                
                print()
            
            print(Fore.WHITE + Style.BRIGHT + f"{len(preset_list) + 1}. Express HPO Setup")
            print(Fore.WHITE + Style.BRIGHT + f"{len(preset_list) + 2}. Custom HPO Setup")
            print(Fore.RED + Style.BRIGHT + "0. Back to HPO Menu")
            
            # Input handling with retry logic
            choice = None
            while not choice:
                try:
                    choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect option (0-{len(preset_list) + 2}): ").strip()
                    
                    # If empty input, retry
                    if not choice:
                        continue
                        
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nReturning to HPO menu...")
                    return
            
            try:
                choice_num = int(choice)
                
                if choice_num == 0:
                    return
                elif choice_num == len(preset_list) + 1:
                    # Express setup option
                    kwargs['_recursion_depth'] = kwargs.get('_recursion_depth', 0) + 1
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching Express HPO setup...")
                    result = run_hyperparameter_optimization_interactive(
                        config=config,
                        operation_mode='express',
                        data_mode=data_mode,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Express HPO")
                    return
                elif choice_num == len(preset_list) + 2:
                    # Custom setup option
                    kwargs['_recursion_depth'] = kwargs.get('_recursion_depth', 0) + 1
                    print(Fore.GREEN + Style.BRIGHT + f"\nLaunching Custom HPO setup...")
                    result = run_hyperparameter_optimization_interactive(
                        config=config,
                        operation_mode='custom',
                        data_mode=data_mode,
                        **kwargs
                    )
                    _handle_hpo_result(result, "Custom HPO")
                    return
                elif 1 <= choice_num <= len(preset_list):
                    selected_name, selected_info = preset_list[choice_num - 1]
                    
                    # Confirmation with preset information
                    print(Fore.YELLOW + Style.BRIGHT + f"\nSelected HPO Preset: " + Fore.GREEN + Style.BRIGHT + f"{selected_name.upper()}")
                    print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                    
                    print(Fore.MAGENTA + Style.BRIGHT + "Preset Configuration Details:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Description: " + Fore.YELLOW + Style.BRIGHT + f"{selected_info['description']}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Optimization Strategy: " + Fore.YELLOW + Style.BRIGHT + f"{selected_info['strategy'].upper()}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Number of Trials: " + Fore.YELLOW + Style.BRIGHT + f"{selected_info['trials']}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Timeout: " + Fore.YELLOW + Style.BRIGHT + f"{selected_info['timeout']} seconds")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Target Model: " + Fore.YELLOW + Style.BRIGHT + f"{selected_info['model_type']}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Optimization Focus: " + Fore.YELLOW + Style.BRIGHT + f"{selected_info['optimization_focus'].title()}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Sampler: " + Fore.YELLOW + Style.BRIGHT + f"{selected_info['sampler']}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Pruner: " + Fore.YELLOW + Style.BRIGHT + f"{selected_info['pruner']}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Plots: " + Fore.YELLOW + Style.BRIGHT + f"{selected_info['generate_plots']}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Storage: " + Fore.YELLOW + Style.BRIGHT + f"{selected_info['storage_enabled']}")
                    
                    print(Fore.MAGENTA + Style.BRIGHT + "\nCurrent Context:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Active Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ System Class: " + Fore.YELLOW + Style.BRIGHT + f"{system_class}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data Mode: " + Fore.YELLOW + Style.BRIGHT + f"{data_mode}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Config Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
                    
                    # Display parameter overrides that will be applied
                    if override_params:
                        print(Fore.CYAN + Style.BRIGHT + f"\nParameter Overrides to Apply:")
                        for i, param in enumerate(override_params):
                            prefix = "  └─" if i == len(override_params) - 1 else "  ├─"
                            print(Fore.GREEN + Style.BRIGHT + f"{prefix} {param}")
                    
                    # Compatibility checks
                    compatibility_issues = []
                    
                    if not selected_info['model_compatible'] and selected_info['model_type'] != 'Unknown':
                        compatibility_issues.append(f"Model type mismatch: preset for {selected_info['model_type']}, current is {model_type}")
                    
                    if not selected_info['system_compatible']:
                        hw_req = selected_info['recommended_hardware']
                        if hw_req:
                            compatibility_issues.append(f"System requirements: {hw_req.get('cpu_cores', 'N/A')} cores, {hw_req.get('ram_gb', 'N/A')}GB RAM")
                    
                    if compatibility_issues:
                        print(Fore.RED + Style.BRIGHT + f"\nCompatibility Notes:")
                        for issue in compatibility_issues:
                            print(Fore.YELLOW + Style.BRIGHT + f"   {issue}")
                    
                    # Handle skip_prompt mode
                    if skip_prompt:
                        print(Fore.GREEN + Style.BRIGHT + f"\nNon-interactive mode - proceeding with {selected_name} preset...")
                        kwargs['_recursion_depth'] = kwargs.get('_recursion_depth', 0) + 1
                        result = run_hyperparameter_optimization_interactive(
                            config=config,
                            preset=selected_name,
                            operation_mode='preset',
                            non_interactive=True,
                            skip_prompt=True,
                            **kwargs
                        )
                        _handle_hpo_result(result, f"{selected_name.title()} Preset HPO")
                        return
                    
                    # Confirmation input with options
                    confirm = None
                    while not confirm:
                        try:
                            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nProceed with this preset? (Y/n/m for modify): ").strip().lower()
                            if not confirm:
                                continue
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nPreset selection cancelled.")
                            break
                    
                    if confirm in ('', 'y', 'yes'):
                        print(Fore.GREEN + Style.BRIGHT + f"\nLaunching HPO with {selected_name} preset...")
                        kwargs['_recursion_depth'] = kwargs.get('_recursion_depth', 0) + 1
                        result = run_hyperparameter_optimization_interactive(
                            config=config,
                            preset=selected_name,
                            operation_mode='preset',
                            data_mode=data_mode,
                            **kwargs
                        )
                        _handle_hpo_result(result, f"{selected_name.title()} Preset HPO")
                        return
                    elif confirm == 'm':
                        kwargs['_recursion_depth'] = kwargs.get('_recursion_depth', 0) + 1
                        # Modified preset setup with custom parameters
                        print(Fore.GREEN + Style.BRIGHT + f"\nLaunching modified preset setup for {selected_name}...")
                        result = run_hyperparameter_optimization_interactive(
                            config=config,
                            preset=selected_name,
                            operation_mode='custom',  # Use custom mode for modifications
                            data_mode=data_mode,
                            **kwargs
                        )
                        _handle_hpo_result(result, f"Modified {selected_name.title()} Preset HPO")
                        return
                    else:
                        print(Fore.RED + Style.BRIGHT + "\nPreset selection cancelled.")
                        continue
                    
                else:
                    print(Fore.RED + Style.BRIGHT + f"\nInvalid selection '{choice}'. Please enter a number from 0-{len(preset_list) + 2}.")
            
            except ValueError:
                print(Fore.RED + Style.BRIGHT + f"\nInvalid input '{choice}'. Please enter a valid number.")
        
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nHPO preset selection interrupted by user!")
            break
        except Exception as e:
            logger.error(f"HPO preset selection error: {e}", exc_info=True)
            
            # Error context
            error_context = {
                "Current Preset": preset_name,
                "Model Type": model_type,
                "System Class": system_class,
                "Available Presets": len(hpo_presets),
                "Config Source": config_source,
                "Data Mode": data_mode,
                "Operation Mode": operation_mode or "Not specified",
                "Parameter Overrides": len(override_params) if override_params else 0
            }
            
            message = (
                f"HPO preset selection failed: {str(e)}\n\n"
                f"Context:\n" +
                "\n".join([f"├─ {key}: {value}" for key, value in list(error_context.items())[:-1]]) +
                f"\n└─ {list(error_context.items())[-1][0]}: {list(error_context.items())[-1][1]}" +
                f"\n\nThis could be due to:\n"
                f"├─ Configuration file corruption\n"
                f"├─ Missing or invalid preset definitions\n"
                f"├─ System resource constraints\n"
                f"├─ Invalid parameter combinations\n"
                f"├─ Preset compatibility issues\n"
                f"└─ Model variant initialization failure"
            )
            
            print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
            print(Fore.RED + Style.BRIGHT + "HPO PRESET SELECTION ERROR")
            print(Fore.RED + Style.BRIGHT + "-" * 40)
            print(Fore.WHITE + Style.BRIGHT + message)
            print(Fore.RED + Style.BRIGHT + "-" * 40)
        
        # Only continue if we're not returning to previous menu
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to HPO menu...")
                break

def export_to_onnx(
    model: nn.Module,
    input_dim: int,
    device: torch.device,
    model_dir: Path = None,
    opset_version: int = None,
    config: Optional[Dict] = None
) -> Optional[Path]:
    """
    Export model to ONNX format with memory protection and comprehensive error handling.
    
    Args:
        model: PyTorch model to export
        input_dim: Input dimension 
        device: Device to use
        model_dir: Directory to save ONNX model
        opset_version: ONNX opset version
        config: Configuration dict with export settings
    
    Returns:
        Path to saved ONNX model or None if export fails
    """
    # Initialize memory tracking
    memory_stats = {
        'initial_ram': psutil.virtual_memory().used / (1024**3),
        'initial_vram': torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0
    }
    
    def log_memory_usage(stage: str):
        """Log current memory usage at different stages"""
        mem = psutil.virtual_memory()
        memory_stats[f'{stage}_ram'] = mem.used / (1024**3)
        if torch.cuda.is_available():
            memory_stats[f'{stage}_vram'] = torch.cuda.memory_allocated() / (1024**3)
        logger.debug(f"Memory at {stage}: RAM {memory_stats[f'{stage}_ram']:.2f}GB | "
                    f"VRAM {memory_stats.get(f'{stage}_vram', 0):.2f}GB")

    try:
        # Load configuration with memory limits
        if config is None:
            try:
                config = get_current_config()
            except Exception:
                config = {}
        
        system_config = config.get('system', {})
        export_config = system_config.get('onnx_export', {})
        
        # Apply memory-aware configuration 
        max_ram_usage = export_config.get('max_ram_gb', 8)
        max_vram_usage = export_config.get('max_vram_gb', 2) if torch.cuda.is_available() else 0
        chunk_size = export_config.get('chunk_size', min(128, input_dim))

        # Verify system resources before starting
        current_ram = psutil.virtual_memory().used / (1024**3)
        current_vram = torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0
        
        if current_ram > max_ram_usage * 0.7:
            raise MemoryError(f"High RAM usage detected: {current_ram:.1f}/{max_ram_usage}GB")
        if current_vram > max_vram_usage * 0.8:
            raise MemoryError(f"High VRAM usage detected: {current_vram:.1f}/{max_vram_usage}GB")

        # Apply configuration with parameter precedence
        if model_dir is None:
            model_dir = Path(system_config.get('model_dir', DEFAULT_MODEL_DIR))
        if opset_version is None:
            opset_version = export_config.get('opset_version', 14)
        
        model.eval()
        onnx_path = model_dir / "autoencoder_ids.onnx"
        
        logger.info(f"Exporting model to ONNX format at {onnx_path}")
        log_memory_usage('pre_export')

        # Memory protection context
        with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):
            # Prepare directory and inputs with memory cleanup
            try:
                model_dir.mkdir(parents=True, exist_ok=True)
                
                # Create dummy input in chunks to reduce memory spikes
                dummy_input = None
                try:
                    chunks = []
                    for i in range(0, input_dim, chunk_size):
                        chunk = torch.randn(1, min(chunk_size, input_dim - i), device='cpu')
                        chunks.append(chunk)
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None
                    
                    dummy_input = torch.cat(chunks, dim=1).to(device)
                    del chunks
                except Exception as e:
                    if dummy_input is not None:
                        del dummy_input
                    raise MemoryError(f"Input creation failed: {str(e)}")

                log_memory_usage('post_input_creation')

                # Export with memory monitoring
                try:
                    torch.onnx.export(
                        model,
                        dummy_input,
                        onnx_path,
                        opset_version=opset_version,
                        input_names=["input"],
                        output_names=["output"],
                        dynamic_axes=export_config.get('dynamic_axes', {'input': {0: 'batch_size'}}),
                        do_constant_folding=export_config.get('constant_folding', True),
                        export_params=True,
                        verbose=export_config.get('verbose', False),
                        training=torch.onnx.TrainingMode.EVAL,
                        operator_export_type=torch.onnx.OperatorExportTypes.ONNX
                    )
                finally:
                    del dummy_input
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None

                log_memory_usage('post_export')

                # Validation with memory protection
                validation_result = None
                if export_config.get('runtime_validation', True) and ONNXRUNTIME_AVAILABLE:
                    validation_result = validate_onnx_model(
                        model, onnx_path, device, 
                        tolerance=export_config.get('validation_tolerance', 1e-5),
                        strict=export_config.get('strict_validation', False)
                    )
                
                # Generate metadata with memory info
                metadata = create_export_metadata(
                    model, onnx_path, config, validation_result, memory_stats
                )
                
                save_metadata(metadata, model_dir / "onnx_export_metadata.json")
                logger.info(f"Export completed: {onnx_path}")
                return onnx_path

            except Exception as e:
                # Cleanup any partial files
                if onnx_path.exists():
                    try:
                        onnx_path.unlink()
                    except:
                        pass
                raise

    except MemoryError as e:
        logger.error(f"Memory error during export: {str(e)}")
        log_memory_usage('error')
        if export_config.get('fail_silently', False):
            return None
        raise
    except Exception as e:
        logger.error(f"Export failed: {str(e)}")
        if export_config.get('fail_silently', False):
            return None
        raise RuntimeError(f"ONNX export failed: {str(e)}") from e
    finally:
        # Force cleanup
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

def validate_onnx_model(
    model: nn.Module, 
    onnx_path: Path, 
    device: torch.device,
    tolerance: float = 1e-5,
    strict: bool = False
) -> Dict:
    """
    Memory-safe ONNX model validation with comprehensive error handling.
    
    Args:
        model: PyTorch model to validate against
        onnx_path: Path to ONNX model file
        device: Device to use for validation
        tolerance: Maximum allowed difference between outputs
        strict: Whether to raise error on validation failure
    
    Returns:
        Dict containing validation results and status
    """
    validation_result = {
        'status': 'skipped',
        'max_difference': None,
        'error': None
    }
    
    try:
        # Load in memory-safe way
        with warnings.catch_warnings(), torch.no_grad():
            # 1. Verify ONNX model structure
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)
            
            # 2. Runtime validation
            ort_session = ort.InferenceSession(
                str(onnx_path),
                providers=['CPUExecutionProvider']
            )
            
            # Create test input in chunks to reduce memory usage
            input_shape = onnx_model.graph.input[0].type.tensor_type.shape.dim[1].dim_value
            test_input = torch.randn(1, input_shape, device='cpu').numpy()
            
            # Run comparison with memory safety
            ort_output = ort_session.run(None, {'input': test_input})[0]
            
            # Move model output to CPU and detach gradients 
            with torch.no_grad():
                torch_output = model(torch.from_numpy(test_input).to(device))
                torch_output = torch_output.cpu().numpy()
            
            # Validate shapes before numerical comparison
            if ort_output.shape != torch_output.shape:
                error_msg = f"Shape mismatch: ONNX {ort_output.shape} vs PyTorch {torch_output.shape}"
                if strict:
                    raise RuntimeError(error_msg)
                validation_result.update({
                    'status': 'failed',
                    'error': error_msg
                })
            else:
                # Convert to numpy arrays of same dtype for comparison
                ort_output = ort_output.astype(np.float32)
                torch_output = torch_output.astype(np.float32)
                
                # Calculate max difference safely
                max_diff = np.abs(ort_output - torch_output).max()
                validation_result['max_difference'] = float(max_diff)
                
                if max_diff > tolerance:
                    error_msg = f"Numerical difference {max_diff:.2e} > tolerance {tolerance:.1e}"
                    if strict:
                        raise RuntimeError(error_msg)
                    validation_result.update({
                        'status': 'warning',
                        'error': error_msg
                    })
                else:
                    validation_result['status'] = 'passed'
    
    except Exception as e:
        validation_result.update({
            'status': 'failed',
            'error': str(e)
        })
        if strict:
            raise
    
    return validation_result

def create_export_metadata(
    model: nn.Module,
    onnx_path: Path,
    config: Dict,
    validation_result: Optional[Dict],
    memory_stats: Dict
) -> Dict:
    """Generate comprehensive export metadata"""
    return {
        "export_timestamp": datetime.now().isoformat(),
        "model_type": type(model).__name__,
        "input_dim": getattr(model, 'input_dim', 'unknown'),
        "opset_version": config.get('system', {}).get('onnx_export', {}).get('opset_version', 14),
        "file_size_mb": onnx_path.stat().st_size / (1024**2),
        "memory_usage": memory_stats,
        "validation": validation_result or {'status': 'not_performed'},
        "system": {
            "python_version": sys.version,
            "pytorch_version": torch.__version__,
            "onnx_version": onnx.__version__,
            "onnxruntime_available": ONNXRUNTIME_AVAILABLE,
            "device": str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
        },
        "config": config.get('system', {}).get('onnx_export', {})
    }

def save_metadata(metadata: Dict, path: Path) -> None:
    """Safely save metadata with error handling"""
    try:
        with open(path, 'w') as f:
            json.dump(metadata, f, indent=2)
        logger.info(f"Metadata saved to {path}")
    except Exception as e:
        logger.error(f"Failed to save metadata: {str(e)}")

def save_tensorboard_data(
    writer: SummaryWriter,
    save_dir: Path = None,
    filename_suffix: str = "",
    config: Optional[Dict] = None
) -> Dict[str, Path]:
    """Save TensorBoard event data in multiple readable formats with configuration integration."""
    # Load configuration
    if config is None:
        try:
            config = get_current_config()
        except Exception:
            config = {}
    
    monitoring_config = config.get('monitoring', {})
    tensorboard_config = monitoring_config.get('tensorboard', {})
    
    # Apply configuration with parameter precedence
    if save_dir is None:
        save_dir = Path(monitoring_config.get('tensorboard_dir', TB_DIR))
    
    export_formats = tensorboard_config.get('export_formats', ['json', 'csv'])
    include_histograms = tensorboard_config.get('include_histograms', False)
    include_images = tensorboard_config.get('include_images', False)
    
    logger.info(f"Saving TensorBoard data to: {save_dir}")
    logger.info(f"Export formats: {export_formats}")
    
    saved_files = {}
    
    try:
        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
    except ImportError:
        logger.warning("Could not import EventAccumulator - skipping TensorBoard data export")
        return saved_files

    try:
        # Ensure save directory exists
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # Get the event file path from the SummaryWriter
        event_files = list(Path(writer.log_dir).glob('events.out.tfevents.*'))
        if not event_files:
            logger.warning("No TensorBoard event files found")
            return saved_files
        
        # Use the most recent event file
        event_file = max(event_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"Processing event file: {event_file}")
        
        # Load the event data with size guidance
        size_guidance = {
            'scalars': tensorboard_config.get('max_scalars', 1000),
            'histograms': tensorboard_config.get('max_histograms', 100) if include_histograms else 0,
            'images': tensorboard_config.get('max_images', 10) if include_images else 0,
        }
        
        event_acc = EventAccumulator(str(event_file), size_guidance=size_guidance)
        event_acc.Reload()
        
        # Extract scalar data
        tb_data = {
            'metadata': {
                'export_timestamp': datetime.now().isoformat(),
                'source_file': str(event_file),
                'tensorflow_version': getattr(event_acc, '_tensorflow_version', 'unknown'),
                'export_config': tensorboard_config
            },
            'scalars': {},
            'tags_metadata': {}
        }
        
        # Process scalar data
        scalar_tags = event_acc.Tags().get('scalars', [])
        logger.info(f"Found {len(scalar_tags)} scalar tags")
        
        for tag in scalar_tags:
            try:
                events = event_acc.Scalars(tag)
                tb_data['scalars'][tag] = {
                    'steps': [e.step for e in events],
                    'values': [e.value for e in events],
                    'wall_times': [e.wall_time for e in events],
                    'count': len(events)
                }
                tb_data['tags_metadata'][tag] = {
                    'type': 'scalar',
                    'first_step': events[0].step if events else 0,
                    'last_step': events[-1].step if events else 0,
                    'min_value': min(e.value for e in events) if events else 0,
                    'max_value': max(e.value for e in events) if events else 0
                }
            except Exception as e:
                logger.warning(f"Failed to process scalar tag '{tag}': {str(e)}")
        
        # Process histograms if requested
        if include_histograms:
            histogram_tags = event_acc.Tags().get('histograms', [])
            if histogram_tags:
                tb_data['histograms'] = {}
                logger.info(f"Processing {len(histogram_tags)} histogram tags")
                
                for tag in histogram_tags:
                    try:
                        events = event_acc.Histograms(tag)
                        tb_data['histograms'][tag] = [
                            {
                                'step': e.step,
                                'wall_time': e.wall_time,
                                'bucket_limits': e.histogram_value.bucket_limit,
                                'bucket_counts': e.histogram_value.bucket,
                                'min': e.histogram_value.min,
                                'max': e.histogram_value.max,
                                'sum': e.histogram_value.sum,
                                'count': e.histogram_value.num
                            }
                            for e in events
                        ]
                    except Exception as e:
                        logger.warning(f"Failed to process histogram tag '{tag}': {str(e)}")
        
        # Save in requested formats
        base_filename = f"tensorboard_data{filename_suffix}"
        
        # JSON format
        if 'json' in export_formats:
            json_path = save_dir / f"{base_filename}.json"
            with open(json_path, 'w') as f:
                json.dump(tb_data, f, indent=2, default=str)
            saved_files['json'] = json_path
            logger.info(f"[INFO] Saved JSON data: {json_path}")
        
        # CSV format for scalars
        if 'csv' in export_formats and tb_data['scalars']:
            csv_path = save_dir / f"{base_filename}.csv"
            try:
                # Create a flattened DataFrame
                csv_data = []
                for tag, values in tb_data['scalars'].items():
                    for step, value, wall_time in zip(values['steps'], values['values'], values['wall_times']):
                        csv_data.append({
                            'tag': tag,
                            'step': step,
                            'value': value,
                            'wall_time': wall_time,
                            'timestamp': datetime.fromtimestamp(wall_time).isoformat()
                        })
                
                df = pd.DataFrame(csv_data)
                df.to_csv(csv_path, index=False)
                saved_files['csv'] = csv_path
                logger.info(f"[INFO] Saved CSV data: {csv_path}")
            except Exception as e:
                logger.warning(f"Could not save CSV format: {str(e)}")
        
        # Parquet format for efficient storage
        if 'parquet' in export_formats and tb_data['scalars']:
            try:
                parquet_path = save_dir / f"{base_filename}.parquet"
                # Similar to CSV but save as Parquet
                parquet_data = []
                for tag, values in tb_data['scalars'].items():
                    for step, value, wall_time in zip(values['steps'], values['values'], values['wall_times']):
                        parquet_data.append({
                            'tag': tag,
                            'step': step,
                            'value': value,
                            'wall_time': wall_time
                        })
                
                df = pd.DataFrame(parquet_data)
                df.to_parquet(parquet_path, index=False)
                saved_files['parquet'] = parquet_path
                logger.info(f"[INFO] Saved Parquet data: {parquet_path}")
            except Exception as e:
                logger.warning(f"Could not save Parquet format: {str(e)}")
        
        # Save summary statistics
        if tensorboard_config.get('save_summary', True):
            summary = {
                'export_summary': {
                    'timestamp': datetime.now().isoformat(),
                    'total_scalar_tags': len(tb_data['scalars']),
                    'total_data_points': sum(len(v['values']) for v in tb_data['scalars'].values()),
                    'file_size_mb': event_file.stat().st_size / 1024 / 1024,
                    'time_range': {
                        'first_event': min(
                            min(v['wall_times']) for v in tb_data['scalars'].values() 
                            if v['wall_times']
                        ) if tb_data['scalars'] else 0,
                        'last_event': max(
                            max(v['wall_times']) for v in tb_data['scalars'].values() 
                            if v['wall_times']
                        ) if tb_data['scalars'] else 0
                    },
                    'exported_formats': list(saved_files.keys()),
                    'tags_summary': tb_data['tags_metadata']
                }
            }
            
            summary_path = save_dir / f"{base_filename}_summary.json"
            with open(summary_path, 'w') as f:
                json.dump(summary, f, indent=2, default=str)
            saved_files['summary'] = summary_path
            logger.info(f"[INFO] Saved summary: {summary_path}")
        
        logger.info(f"[INFO] TensorBoard data export complete ({len(saved_files)} files)")
        return saved_files
        
    except Exception as e:
        logger.error(f"Failed to save TensorBoard data: {str(e)}")
        return saved_files

def prompt_user(prompt: str, default: bool = True) -> bool:
    """Interactive user prompt with default handling."""
    while True:
        response = input(f"{prompt} [{'Y/n' if default else 'y/N'}]: ").strip().lower()
        if not response:
            return default
        if response in ('y', 'yes'):
            return True
        if response in ('n', 'no'):
            return False
        print("Please answer yes/y or no/n")

def show_banner(return_config=False) -> None:
    """Display the application banner"""
    # ASCII art banner
    console.print("\n" , Panel.fit(
        """
                            
⠀⠀⠀⠀⠀⠀⠀⢀⣠⣤⣠⣶⠚⠛⠿⠷⠶⣤⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⢀⣴⠟⠉⠀⠀⢠⡄⠀⠀⠀⠀⠀⠉⠙⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⢀⡴⠛⠁⠀⠀⠀⠀⠘⣷⣴⠏⠀⠀⣠⡄⠀⠀⢨⡇⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠺⣇⠀⠀⠀⠀⠀⠀⠀⠘⣿⠀⠀⠘⣻⣻⡆⠀⠀⠙⠦⣄⣀⠀⠀⠀⠀
⠀⠀⠀⢰⡟⢷⡄⠀⠀⠀⠀⠀⠀⢸⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⢻⠶⢤⡀
⠀⠀⠀⣾⣇⠀⠻⣄⠀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣀⣴⣿
⠀⠀⢸⡟⠻⣆⠀⠈⠳⢄⡀⠀⠀⡼⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠶⠶⢤⣬⡿⠁
⠀⢀⣿⠃⠀⠹⣆⠀⠀⠀⠙⠓⠿⢧⡀⠀⢠⡴⣶⣶⣒⣋⣀⣀⣤⣶⣶⠟⠁⠀
⠀⣼⡏⠀⠀⠀⠙⠀⠀⠀⠀⠀⠀⠀⠙⠳⠶⠤⠵⣶⠒⠚⠻⠿⠋⠁⠀⠀⠀⠀
⢰⣿⡇⠀⠀⠀⠀⠀⠀⠀⣆⠀⠀⠀⠀⠀⠀⠀⢠⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⢿⡿⠁⠀⠀⠀⠀⠀⠀⠀⠘⣦⡀⠀⠀⠀⠀⠀⢸⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣷⡄⠀⠀⠀⠀⣿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢷⡀⠀⠀⠀⢸⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣿⠇⠀⠀⠀⠀⠀⠀⠀

    """,
        style="bold cyan", 
        title="[bold yellow]GreyChamp | IDS[/]", 
        subtitle="[magenta]DEEP LEARNING SUITE[/]",
        border_style="bold blue",
        box=box.DOUBLE,
        padding=(1, 2)
    ))
    
    print(Fore.CYAN + Style.BRIGHT + "\n" + "=" *40)
    print(Fore.GREEN + Style.BRIGHT + "  - Interactive Mode -  ".center(40))
    print(Fore.CYAN + Style.BRIGHT + "="*40 + Style.RESET_ALL)
    
    try:
        # Get configuration status with error handling
        config = get_current_config()
        metadata_config = config.get('metadata', {})
        training_config = config.get('training', {})
        model_config = config.get('model', {})
        security_config = config.get('security', {})
        data_config = config.get('data', {})
        monitoring_config = config.get('monitoring', {})
        hardware_config = config.get('hardware', {})
        system_config = config.get('system', {})
        presets_config = config.get('presets', {})
        hpo_config = config.get('hyperparameter_optimization', {})
        validation_config = config.get('validation', {})
        experimental_config = config.get('experimental', {})
        
        # Multiple fallback approaches for determining preset name
        preset_name = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Unknown")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Unknown", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Unknown")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Unknown", None, ""]:
            preset_name = config.get("_preset_name", "Unknown")
        
        # Method 4: Check runtime information
        if preset_name in ["Unknown", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Unknown")
        
        # Determine configuration source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Clean up preset name display
        if preset_name in ["Unknown", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Get additional configuration info
        model_type = "Unknown"
        total_sections = len(config) if isinstance(config, dict) else 0
        
        try:
            model_section = config.get("model", {})
            if isinstance(model_section, dict):
                model_type = model_section.get("model_type", "Unknown")
        except Exception:
            model_type = "Unknown"
        
        # Get configuration health if available
        if "runtime" in config and isinstance(config["runtime"], dict):
            runtime = config["runtime"]
            if "configuration_health" in runtime:
                health = runtime["configuration_health"]
                health_status = health.get("status", "unknown")
                
                # Color code health status
                if health_status in ["healthy", "passed"]:
                    health_color = Fore.GREEN
                elif health_status in ["needs_attention", "warning"]:
                    health_color = Fore.YELLOW
                else:
                    health_color = Fore.RED
        
        if return_config:
            return config
        
    except Exception as e:
        logger.warning(f"Error getting configuration status: {e}")
        print(Fore.RED + Style.BRIGHT + f"Error getting configuration status: {e}\n")
        if return_config:
            return {}

def print_main_menu(config: Optional[Dict[str, Any]] = None):
    """Print the main menu options with context display."""
    
    # Get configuration context if available
    preset_name = "Custom/Default"
    model_type = "Unknown"
    config_source = "Unknown"
    
    if config:
        # Extract preset name
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
    
    # menu header with context
    #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
    print(Fore.YELLOW + Style.BRIGHT + "MAIN APPLICATION MENU")
    print(Fore.CYAN + Style.BRIGHT + "-"*40)
    print(Fore.YELLOW + Style.BRIGHT + f"Active Context:")
    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
    print(Fore.GREEN + Style.BRIGHT + f"  └─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
    
    # Print menu options with descriptions
    print(Fore.YELLOW + Style.BRIGHT + "\nCore Functions:")
    print(Fore.WHITE + Style.BRIGHT + "1. Model Training " + Fore.GREEN + Style.BRIGHT + "(Train & Evaluate Models)")
    print(Fore.WHITE + Style.BRIGHT + "2. Hyperparameter Optimization " + Fore.GREEN + Style.BRIGHT + "(Auto-tune Parameters)")
    print(Fore.WHITE + Style.BRIGHT + "3. Model Architecture Comparison " + Fore.GREEN + Style.BRIGHT + "(Compare Performance)")
    print(Fore.WHITE + Style.BRIGHT + "4. Configuration Management " + Fore.GREEN + Style.BRIGHT + "(Manage Settings)")
    print(Fore.WHITE + Style.BRIGHT + "5. System Information " + Fore.GREEN + Style.BRIGHT + "(Hardware & Resources)")
    print(Fore.WHITE + Style.BRIGHT + "6. Performance Benchmark " + Fore.GREEN + Style.BRIGHT + "(Speed & Memory Tests)")
    print(Fore.WHITE + Style.BRIGHT + "7. Model Analysis & Visualization " + Fore.GREEN + Style.BRIGHT + "(Metrics & Charts)")
    print(Fore.WHITE + Style.BRIGHT + "8. Advanced Tools " + Fore.GREEN + Style.BRIGHT + "(Expert Features)")
    print(Fore.WHITE + Style.BRIGHT + "9. View Initialization Reports " + Fore.GREEN + Style.BRIGHT + "(System Status)")
    print(Fore.RED + Style.BRIGHT + "0. Exit Application")

def interactive_main():
    """Main interactive interface with banner integration."""
    # Clear any residual input buffer from system initialization
    if hasattr(sys.stdin, 'flush'):
        try:
            sys.stdin.flush()
        except:
            pass
    
    # Small delay to ensure all output is complete
    time.sleep(1)
    
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Print main menu with context
        print_main_menu(config)
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-9): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nExiting...")
                print(Fore.YELLOW + Style.BRIGHT + "Goodbye!")
                return
        
        try:
            if choice == "1":
                try:
                    model_training_menu(config)
                except Exception as e:
                    message = (
                        f"Error encountered while showing model training menu: {str(e)}\n"
                        f"Context:\n"
                        f"- Current Preset: {config.get('presets', {}).get('current_preset', 'Custom/Default')}\n"
                        f"- Model Type: {config.get('model', {}).get('model_type', 'Unknown')}\n\n"
                        f"This could be due to:\n"
                        f"- Configuration file corruption\n"
                        f"- Missing model dependencies\n"
                        f"- System resource constraints\n"
                        f"- Data availability issues"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="TRAINING MENU ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
            elif choice == "2":
                try:
                    console.clear()
                    
                    current_preset = config.get('presets', {}).get('current_preset', 'Custom/Default')
                    model_type = config.get('model', {}).get('model_type', 'Unknown')
                    
                    message = (
                        f"Hyperparameter Optimization (HPO) is a powerful tool\n"
                        f"that allows automatic tuning of the model's parameters\n"
                        f"to achieve better performance.\n\n"
                        f"Current Context:\n"
                        f"- Active Preset: {current_preset}\n"
                        f"- Model Type: {model_type}\n\n"
                        f"The HPO menu provides various optimization strategies:\n"
                        f"- Express setup for quick optimization\n"
                        f"- Custom configuration for advanced users\n"
                        f"- Preset configurations for different scenarios\n"
                        f"- Model comparison and analysis tools\n\n"
                        f"HPO can significantly improve model accuracy and efficiency\n"
                        f"by finding optimal settings for your specific dataset and task."
                    )
                    console.print(
                        Panel.fit(
                            f"[bold white]{message}[/bold white]",
                            title="HYPERPARAMETER OPTIMIZATION",
                            style="bold yellow",
                            border_style="yellow",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    # Small delay to ensure panel is rendered before proceeding
                    time.sleep(3)
                    
                    console.clear()
                    
                    hpo_training_menu(config)
                except Exception as e:
                    message = (
                        f"Error encountered while showing HPO training menu: {str(e)}\n"
                        f"Context:\n"
                        f"- Current Configuration: {config.get('presets', {}).get('current_preset', 'Custom/Default')}\n\n"
                        f"Please check:\n"
                        f"- HPO dependencies are installed\n"
                        f"- Configuration files are valid\n"
                        f"- System has sufficient resources"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="HPO MENU ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "3":
                try:
                    display_model_comparison()
                except Exception as e:
                    message = (
                        f"Error encountered while displaying model comparison: {str(e)}\n"
                        "Please check configuration and try again.\n\n"
                        "Ensure:\n"
                        "- Multiple models are configured\n"
                        "- Comparison data is available\n"
                        "- Visualization dependencies are installed"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="MODEL COMPARISON ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "4":
                try:
                    configuration_menu()
                except Exception as e:
                    message = (
                        f"Error encountered while showing configuration menu: {str(e)}\n"
                        "Please check configuration files and permissions."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="CONFIGURATION ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "5":
                try:
                    show_system_info()
                except Exception as e:
                    message = (
                        f"Error encountered while showing system information: {str(e)}\n"
                        "Please check system dependencies and permissions."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="SYSTEM INFO ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "6":
                try:
                    run_performance_benchmark_interactive()
                except Exception as e:
                    message = (
                        f"Error encountered while running performance benchmark: {str(e)}\n"
                        "Please check configuration and system resources."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="BENCHMARK ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "7":
                try:
                    model_analysis_menu()
                except Exception as e:
                    message = (
                        f"Error encountered while showing model analysis menu: {str(e)}\n"
                        "Please check configuration and visualization dependencies."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="ANALYSIS ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "8":
                try:
                    advanced_tools_menu()
                except Exception as e:
                    message = (
                        f"Error encountered while showing advanced tools menu: {str(e)}\n"
                        "Please check configuration and expert feature dependencies."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="ADVANCED TOOLS ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "9":
                try:
                    show_init_report()
                except Exception as e:
                    message = (
                        f"Error encountered while showing initialization reports: {str(e)}\n"
                        "Please check if reports exist and try again."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="REPORTS ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "0":
                console.clear()
                # Exit message
                console.print(
                    Panel.fit(
                        "Thank you for using GreyChamp IDS Deep Learning Suite!",
                        title="EXIT APPLICATION",
                        style="bold yellow",
                        border_style="yellow",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                print(Fore.RED + Style.BRIGHT + "\nExiting...")
                print(Fore.YELLOW + Style.BRIGHT + "Goodbye!")
                break
            else:
                print(Fore.RED + Style.BRIGHT + f"\nInvalid selection '{choice}'. Please enter a number from 0-9.")
            
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nOperation interrupted by user")
        except Exception as e:
            logger.error(f"Main menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in main menu: {str(e)}\n"
                f"Context:\n"
                f"- Selected Option: {choice}\n"
                f"- Current Preset: {config.get('presets', {}).get('current_preset', 'Custom/Default')}\n"
                f"- Model Type: {config.get('model', {}).get('model_type', 'Unknown')}\n\n"
                f"This could indicate:\n"
                f"- System resource issues\n"
                f"- Configuration problems\n"
                f"- Dependency conflicts\n"
                f"- Data corruption\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="MAIN MENU ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nExiting...")
                print(Fore.YELLOW + Style.BRIGHT + "Goodbye!")
                break

def model_training_menu(config: Optional[Dict[str, Any]] = None):
    """Menu for model training options with context display and error handling."""
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        # if banner_config is not None:
        #     config = banner_config
        # elif config is None:
        #     config = get_current_config()
        
        if config is None and banner_config is not None:
            config = banner_config
        else:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        model_config = config.get('model', {})
        system_config = config.get('system', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        preset_count = len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        epoch_count = training_config.get('epochs', 'Default')
        batch_count = training_config.get('batch_size', 'Default')
        normal_samples_count = data_config.get('normal_samples', 10000)
        data_path_config = data_config.get('data_path', 'Default')
        
        # Menu display with context
        #print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "MODEL TRAINING MENU")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Active Training Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{epoch_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.YELLOW + Style.BRIGHT + f"{batch_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Data Path: " + Fore.YELLOW + Style.BRIGHT + f"{data_path_config}")
        
        # Menu options with context-aware descriptions
        print(Fore.YELLOW + Style.BRIGHT + "\nCore Training Operations:")
        print(Fore.WHITE + Style.BRIGHT + "1. Train with Current Configuration " + Fore.GREEN + Style.BRIGHT + f"(Epochs: {epoch_count}, Batch Size: {batch_count})")
        print(Fore.WHITE + Style.BRIGHT + "2. Train with Synthetic Data " + Fore.GREEN + Style.BRIGHT + f"(Samples: {normal_samples_count})")
        print(Fore.WHITE + Style.BRIGHT + "3. Train with Real Data " + Fore.GREEN + Style.BRIGHT + f"(Source: {data_path_config})")
        print(Fore.WHITE + Style.BRIGHT + "4. Quick Training (Fast Test) " + Fore.GREEN + Style.BRIGHT + f"(Model: {model_type})")
        print(Fore.WHITE + Style.BRIGHT + "5. Custom Training Parameters " + Fore.GREEN + Style.BRIGHT + "(Interactive Setup)")
        print(Fore.WHITE + Style.BRIGHT + "6. Select Preset Configuration " + Fore.GREEN + Style.BRIGHT + f"(Available: {preset_count})")
        print(Fore.WHITE + Style.BRIGHT + "7. Stability Test " + Fore.GREEN + Style.BRIGHT + "(10 Epochs Quick Test)")
        print(Fore.RED + Style.BRIGHT + "0. Back to Main Menu")
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-7): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                return
        
        try:
            if choice == "1":
                try:
                    train_model_interactive(use_current_config=True, config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during training with current configuration: {str(e)}\n"
                        f"Context:\n"
                        f"- Preset: {preset_name}\n"
                        f"- Model: {model_type}\n"
                        f"- Epochs: {epoch_count}\n\n"
                        f"This could be due to:\n"
                        f"- Invalid configuration parameters\n"
                        f"- Missing or corrupted data\n"
                        f"- Insufficient system resources\n"
                        f"- Model architecture issues"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="TRAINING ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "2":
                try:
                    train_model_interactive(use_real_data=False, config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during synthetic data training: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Synthetic Samples: {normal_samples_count}\n\n"
                        f"Please check:\n"
                        f"- Data generation configuration\n"
                        f"- Model compatibility with synthetic data\n"
                        f"- System memory availability"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="SYNTHETIC TRAINING ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "3":
                try:
                    train_model_interactive(use_real_data=True, config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during real data training: {str(e)}\n"
                        f"Context:\n"
                        f"- Data Path: {data_path_config}\n"
                        f"- Model: {model_type}\n\n"
                        f"Please verify:\n"
                        f"- Data file exists and is accessible\n"
                        f"- Data format is compatible\n"
                        f"- Sufficient disk space\n"
                        f"- Data preprocessing requirements"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="REAL DATA TRAINING ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "4":
                try:
                    train_model_quick(config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during quick training: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Mode: Fast test\n\n"
                        f"This could indicate:\n"
                        f"- Model initialization issues\n"
                        f"- Basic configuration problems\n"
                        f"- Critical dependency missing"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="QUICK TRAINING ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "5":
                try:
                    train_model_custom(config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during custom training setup: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Preset: {preset_name}\n\n"
                        f"Please check:\n"
                        f"- Parameter validation rules\n"
                        f"- Configuration file permissions\n"
                        f"- Interactive input handling"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CUSTOM TRAINING ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "6":
                try:
                    #select_preset_config()
                    _interactive_preset_setup(base_config=config, use_real_data=False)
                except Exception as e:
                    message = (
                        f"Error encountered during preset selection: {str(e)}\n"
                        f"Context:\n"
                        f"- Available Presets: {preset_count}\n"
                        f"- Current Preset: {preset_name}\n\n"
                        f"Please verify:\n"
                        f"- Preset configuration files\n"
                        f"- Preset validation logic\n"
                        f"- Configuration loading mechanisms"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="PRESET SELECTION ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "7":
                try:
                    run_stability_test(config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during stability test: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Test Type: 10-epoch quick test\n\n"
                        f"This may indicate:\n"
                        f"- Fundamental model issues\n"
                        f"- Training loop problems\n"
                        f"- Data loading failures"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="STABILITY TEST ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "0":
                return
            else:
                print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-7.")
        
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nTraining operation interrupted by user")
        except Exception as e:
            logger.error(f"Training menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in training menu: {str(e)}\n"
                f"Context:\n"
                f"- Selected Option: {choice}\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could indicate:\n"
                f"- System resource exhaustion\n"
                f"- Configuration corruption\n"
                f"- Dependency conflicts\n"
                f"- Data access issues\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="TRAINING MENU ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                break



def configuration_menu():
    """Menu for configuration management options with context display and error handling."""
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        metadata_config = config.get('metadata', {})
        training_config = config.get('training', {})
        model_config = config.get('model', {})
        data_config = config.get('data', {})
        system_config = config.get('system', {})
        presets_config = config.get('presets', {})
        hpo_config = config.get('hyperparameter_optimization', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        config_status = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
            config_status = config["runtime"].get("config_status", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
            config_status = config["metadata"].get("config_status", "Unknown")
        
        # Get configuration health if available
        health_status = "unknown"
        health_color = Fore.WHITE
        if "runtime" in config and isinstance(config["runtime"], dict):
            runtime = config["runtime"]
            if "configuration_health" in runtime:
                health = runtime["configuration_health"]
                health_status = health.get("status", "unknown")
                if health_status in ["healthy", "passed"]:
                    health_color = Fore.GREEN
                elif health_status in ["needs_attention", "warning"]:
                    health_color = Fore.YELLOW
                else:
                    health_color = Fore.RED
        
        preset_count = len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        
        # Menu display with context
        print(Fore.YELLOW + Style.BRIGHT + "CONFIGURATION MANAGEMENT MENU")
        print(Fore.CYAN + Style.BRIGHT + "-" * 40)
        print(Fore.YELLOW + Style.BRIGHT + "Active Configuration Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Status: " + health_color + Style.BRIGHT + f"{config_status}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Health: " + health_color + Style.BRIGHT + f"{health_status}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Sections: " + Fore.YELLOW + Style.BRIGHT + f"{len(config) if isinstance(config, dict) else 'Unknown'}")
        
        # Context-aware Menu options
        print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration Operations:")
        print(Fore.WHITE + Style.BRIGHT + "1. Show Current Configuration " + Fore.GREEN + Style.BRIGHT + f"(Preset: {preset_name})")
        print(Fore.WHITE + Style.BRIGHT + "2. Save Current Configuration " + Fore.GREEN + Style.BRIGHT + f"(Status: {config_status})")
        print(Fore.WHITE + Style.BRIGHT + "3. Select Preset Configuration " + Fore.GREEN + Style.BRIGHT + f"(Available: {preset_count})")
        print(Fore.WHITE + Style.BRIGHT + "4. Load Saved Configuration " + Fore.GREEN + Style.BRIGHT + "(From File)")
        print(Fore.WHITE + Style.BRIGHT + "5. Reset to Default Configuration " + Fore.GREEN + Style.BRIGHT + "(Factory Reset)")
        print(Fore.WHITE + Style.BRIGHT + "6. Validate Current Configuration " + Fore.GREEN + Style.BRIGHT + f"(Health: {health_status})")
        print(Fore.WHITE + Style.BRIGHT + "7. Edit Configuration Interactively " + Fore.GREEN + Style.BRIGHT + "(Live Editor)")
        print(Fore.WHITE + Style.BRIGHT + "8. Compare Configurations " + Fore.GREEN + Style.BRIGHT + "(Side-by-Side)")
        print(Fore.WHITE + Style.BRIGHT + "9. Configuration Health Report " + Fore.GREEN + Style.BRIGHT + "(Detailed Analysis)")
        print(Fore.RED + Style.BRIGHT + "0. Back to Main Menu")
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-9): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                return
        
        try:
            if choice == "1":
                try:
                    show_current_config()
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "CONFIGURATION DISPLAY ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error encountered while displaying current configuration: {str(e)}")
                    print(Fore.CYAN + Style.BRIGHT + "Context:")
                    print(Fore.WHITE + Style.BRIGHT + f"- Preset: {preset_name}")
                    print(Fore.WHITE + Style.BRIGHT + f"- Model: {model_type}")
                    print(Fore.WHITE + Style.BRIGHT + f"- Config Source: {config_source}")
                    print(Fore.YELLOW + Style.BRIGHT + "\nThis could be due to:")
                    print(Fore.WHITE + Style.BRIGHT + "- Configuration file corruption")
                    print(Fore.WHITE + Style.BRIGHT + "- Invalid configuration structure")
                    print(Fore.WHITE + Style.BRIGHT + "- Display formatting issues")
                    print(Fore.WHITE + Style.BRIGHT + "- System resource constraints")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    
            elif choice == "2":
                try:
                    save_config_interactive()
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "CONFIGURATION SAVE ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error encountered while saving configuration: {str(e)}")
                    print(Fore.CYAN + Style.BRIGHT + "Context:")
                    print(Fore.WHITE + Style.BRIGHT + f"- Preset: {preset_name}")
                    print(Fore.WHITE + Style.BRIGHT + f"- Config Status: {config_status}")
                    print(Fore.YELLOW + Style.BRIGHT + "\nPlease check:")
                    print(Fore.WHITE + Style.BRIGHT + "- File system permissions")
                    print(Fore.WHITE + Style.BRIGHT + "- Available disk space")
                    print(Fore.WHITE + Style.BRIGHT + "- Configuration validity")
                    print(Fore.WHITE + Style.BRIGHT + "- File path accessibility")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    
            elif choice == "3":
                try:
                    select_preset_config()
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "PRESET SELECTION ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error encountered during preset selection: {str(e)}")
                    print(Fore.CYAN + Style.BRIGHT + "Context:")
                    print(Fore.WHITE + Style.BRIGHT + f"- Available Presets: {preset_count}")
                    print(Fore.WHITE + Style.BRIGHT + f"- Current Preset: {preset_name}")
                    print(Fore.YELLOW + Style.BRIGHT + "\nPlease verify:")
                    print(Fore.WHITE + Style.BRIGHT + "- Preset configuration files exist")
                    print(Fore.WHITE + Style.BRIGHT + "- Preset validation logic")
                    print(Fore.WHITE + Style.BRIGHT + "- Configuration loading mechanisms")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    
            elif choice == "4":
                try:
                    load_saved_config_interactive()
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "CONFIGURATION LOAD ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error encountered while loading configuration: {str(e)}")
                    print(Fore.CYAN + Style.BRIGHT + "Context:")
                    print(Fore.WHITE + Style.BRIGHT + f"- Current Preset: {preset_name}")
                    print(Fore.WHITE + Style.BRIGHT + f"- Config Status: {config_status}")
                    print(Fore.YELLOW + Style.BRIGHT + "\nPlease check:")
                    print(Fore.WHITE + Style.BRIGHT + "- Configuration file exists and is accessible")
                    print(Fore.WHITE + Style.BRIGHT + "- File format is supported")
                    print(Fore.WHITE + Style.BRIGHT + "- Configuration is compatible with current system")
                    print(Fore.WHITE + Style.BRIGHT + "- File is not corrupted")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    
            elif choice == "5":
                try:
                    reset_config_interactive()
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "CONFIGURATION RESET ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error encountered while resetting configuration: {str(e)}")
                    print(Fore.CYAN + Style.BRIGHT + "Context:")
                    print(Fore.WHITE + Style.BRIGHT + f"- Current Preset: {preset_name}")
                    print(Fore.YELLOW + Style.BRIGHT + "\nThis could indicate:")
                    print(Fore.WHITE + Style.BRIGHT + "- Default configuration files missing")
                    print(Fore.WHITE + Style.BRIGHT + "- System file permissions issues")
                    print(Fore.WHITE + Style.BRIGHT + "- Configuration restoration problems")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    
            elif choice == "6":
                try:
                    validate_config_interactive(silent=False)
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "CONFIGURATION VALIDATION ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error encountered during configuration validation: {str(e)}")
                    print(Fore.CYAN + Style.BRIGHT + "Context:")
                    print(Fore.WHITE + Style.BRIGHT + f"- Current Health: {health_status}")
                    print(Fore.WHITE + Style.BRIGHT + f"- Preset: {preset_name}")
                    print(Fore.YELLOW + Style.BRIGHT + "\nPlease check:")
                    print(Fore.WHITE + Style.BRIGHT + "- Configuration file integrity")
                    print(Fore.WHITE + Style.BRIGHT + "- Validation rule definitions")
                    print(Fore.WHITE + Style.BRIGHT + "- System state and dependencies")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    
            elif choice == "7":
                try:
                    edit_config_interactive()
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "CONFIGURATION EDITOR ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error encountered in interactive configuration editor: {str(e)}")
                    print(Fore.CYAN + Style.BRIGHT + "Context:")
                    print(Fore.WHITE + Style.BRIGHT + f"- Preset: {preset_name}")
                    print(Fore.WHITE + Style.BRIGHT + f"- Model: {model_type}")
                    print(Fore.YELLOW + Style.BRIGHT + "\nPlease check:")
                    print(Fore.WHITE + Style.BRIGHT + "- Editor dependencies are installed")
                    print(Fore.WHITE + Style.BRIGHT + "- Configuration file permissions")
                    print(Fore.WHITE + Style.BRIGHT + "- Interactive input handling")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    
            elif choice == "8":
                try:
                    compare_configs_interactive()
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "CONFIGURATION COMPARISON ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error encountered during configuration comparison: {str(e)}")
                    print(Fore.CYAN + Style.BRIGHT + "Context:")
                    print(Fore.WHITE + Style.BRIGHT + f"- Active Preset: {preset_name}")
                    print(Fore.WHITE + Style.BRIGHT + f"- Config Source: {config_source}")
                    print(Fore.YELLOW + Style.BRIGHT + "\nPlease verify:")
                    print(Fore.WHITE + Style.BRIGHT + "- Comparison configurations exist")
                    print(Fore.WHITE + Style.BRIGHT + "- Configuration formats are compatible")
                    print(Fore.WHITE + Style.BRIGHT + "- Sufficient system resources available")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    
            elif choice == "9":
                try:
                    # Placeholder for configuration health report
                    print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration Health Report feature coming soon!")
                    print(Fore.CYAN + Style.BRIGHT + "This will provide detailed analysis of:")
                    print(Fore.WHITE + Style.BRIGHT + "- Configuration performance metrics")
                    print(Fore.WHITE + Style.BRIGHT + "- Resource utilization patterns")
                    print(Fore.WHITE + Style.BRIGHT + "- Optimization recommendations")
                    print(Fore.WHITE + Style.BRIGHT + "- Health scoring and trends")
                except Exception as e:
                    print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + "HEALTH REPORT ERROR")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    print(Fore.RED + Style.BRIGHT + f"Error generating health report: {str(e)}")
                    print(Fore.RED + Style.BRIGHT + "-" * 40)
                    
            elif choice == "0":
                return
            else:
                print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-9.")
        
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nConfiguration operation interrupted by user")
        except Exception as e:
            logger.error(f"Configuration menu error: {e}", exc_info=True)
            print(Fore.RED + Style.BRIGHT + "\n" + "-" * 40)
            print(Fore.RED + Style.BRIGHT + "CONFIGURATION MENU ERROR")
            print(Fore.RED + Style.BRIGHT + "-" * 40)
            print(Fore.RED + Style.BRIGHT + f"Unexpected error in configuration menu: {str(e)}")
            print(Fore.CYAN + Style.BRIGHT + "Context:")
            print(Fore.WHITE + Style.BRIGHT + f"- Selected Option: {choice}")
            print(Fore.WHITE + Style.BRIGHT + f"- Current Preset: {preset_name}")
            print(Fore.WHITE + Style.BRIGHT + f"- Model Type: {model_type}")
            print(Fore.WHITE + Style.BRIGHT + f"- Config Source: {config_source}")
            print(Fore.YELLOW + Style.BRIGHT + "\nThis could indicate:")
            print(Fore.WHITE + Style.BRIGHT + "- System resource issues")
            print(Fore.WHITE + Style.BRIGHT + "- Configuration file corruption")
            print(Fore.WHITE + Style.BRIGHT + "- Dependency conflicts")
            print(Fore.WHITE + Style.BRIGHT + "- File system problems")
            print(Fore.CYAN + Style.BRIGHT + "\nPlease check the logs for detailed information.")
            print(Fore.RED + Style.BRIGHT + "-" * 40)
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                break

def model_analysis_menu():
    """Menu for model analysis and visualization with context display and error handling."""
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        validation_config = config.get('validation', {})
        monitoring_config = config.get('monitoring', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        preset_count = len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        epoch_count = training_config.get('epochs', 'Default')
        batch_count = training_config.get('batch_size', 'Default')
        normal_samples_count = data_config.get('normal_samples', 10000)
        data_path_config = data_config.get('data_path', 'Default')
        
        # Menu display with context
        #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + "MODEL ANALYSIS & VISUALIZATION MENU")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Active Analysis Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{epoch_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Data: " + Fore.YELLOW + Style.BRIGHT + f"{data_path_config}")
        
        # Menu options with context-aware display
        print(Fore.YELLOW + Style.BRIGHT + "\nAnalysis & Visualization Options:")
        print(Fore.WHITE + Style.BRIGHT + "1. Training Performance Analysis " + Fore.GREEN + Style.BRIGHT + f"(Model: {model_type})")
        print(Fore.WHITE + Style.BRIGHT + "2. Model Architecture Visualization " + Fore.GREEN + Style.BRIGHT + "(Layer Details)")
        print(Fore.WHITE + Style.BRIGHT + "3. Anomaly Detection Results " + Fore.GREEN + Style.BRIGHT + "(Detection Metrics)")
        print(Fore.WHITE + Style.BRIGHT + "4. Feature Importance Analysis " + Fore.GREEN + Style.BRIGHT + "(Input Analysis)")
        print(Fore.WHITE + Style.BRIGHT + "5. Confusion Matrix & Metrics " + Fore.GREEN + Style.BRIGHT + "(Performance Visualization)")
        print(Fore.WHITE + Style.BRIGHT + "6. ROC Curve Analysis " + Fore.GREEN + Style.BRIGHT + "(Classification Performance)")
        print(Fore.WHITE + Style.BRIGHT + "7. Training History Visualization " + Fore.GREEN + Style.BRIGHT + "(Learning Curves)")
        print(Fore.WHITE + Style.BRIGHT + "8. Model Comparison Dashboard " + Fore.GREEN + Style.BRIGHT + "(Multi-Model Analysis)")
        print(Fore.WHITE + Style.BRIGHT + "9. Export Analysis Reports " + Fore.GREEN + Style.BRIGHT + "(PDF/HTML Reports)")
        print(Fore.RED + Style.BRIGHT + "0. Back to Main Menu")
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-9): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                return
        
        try:
            # Show "Coming Soon" message for all analysis options (1-9)
            if choice in ["1", "2", "3", "4", "5", "6", "7", "8", "9"]:
                analysis_type = {
                    "1": "Training Performance Analysis",
                    "2": "Model Architecture Visualization",
                    "3": "Anomaly Detection Results",
                    "4": "Feature Importance Analysis",
                    "5": "Confusion Matrix & Metrics",
                    "6": "ROC Curve Analysis",
                    "7": "Training History Visualization", 
                    "8": "Model Comparison Dashboard",
                    "9": "Export Analysis Reports"
                }.get(choice, "Advanced Analysis")
                
                # Context-specific details for each analysis type
                analysis_context = {
                    "1": f"- Model: {model_type}\n- Epochs: {epoch_count}",
                    "2": f"- Model: {model_type}\n- Source: {config_source}",
                    "3": f"- Model: {model_type}\n- Data: {data_path_config}",
                    "4": f"- Model: {model_type}\n- Data Features: Various input dimensions",
                    "5": f"- Model: {model_type}\n- Validation: {validation_config.get('validation_split', 'Default')}",
                    "6": f"- Model: {model_type}\n- Validation: {validation_config.get('validation_split', 'Default')}",
                    "7": f"- Model: {model_type}\n- Epochs: {training_config.get('epochs', 'Default')}",
                    "8": f"- Model: {model_type}\n- Available Models: Multiple comparison",
                    "9": f"- Model: {model_type}\n- Export Formats: PDF/HTML/CSV"
                }.get(choice, f"- Model: {model_type}\n- Preset: {preset_name}")
                
                message = (
                    f"{analysis_type}\n\n"
                    f"Current Context:\n"
                    f"{analysis_context}\n"
                    f"- Preset: {preset_name}\n"
                    f"- Status: Under Development\n\n"
                    f"This advanced analysis feature is currently in development\n"
                    f"and will be available in the next release.\n\n"
                    f"Planned features include:\n"
                    f"- Interactive visualization dashboards\n"
                    f"- Advanced statistical analysis\n"
                    f"- Export capabilities for reports\n"
                    f"- Comparative analysis tools\n"
                    f"- Real-time performance monitoring\n"
                    f"- Automated insights generation\n\n"
                    f"Check back soon for updates!\n"
                )
                
                # Different colors for different analysis types
                panel_styles = {
                    "1": ("bold cyan", "cyan"),
                    "2": ("bold green", "green"),
                    "3": ("bold magenta", "magenta"),
                    "4": ("bold yellow", "yellow"),
                    "5": ("bold blue", "blue"),
                    "6": ("bold cyan", "cyan"),
                    "7": ("bold green", "green"),
                    "8": ("bold magenta", "magenta"),
                    "9": ("bold yellow", "yellow")
                }
                
                style, border_style = panel_styles.get(choice, ("bold yellow", "yellow"))
                
                console.print(
                    Panel.fit(
                        f"[bold white]{message}[/bold white]",
                        title=f"{analysis_type} - COMING SOON",
                        style=style,
                        border_style=border_style,
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                time.sleep(3)
                
            elif choice == "0":
                return
            else:
                print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-9.")
        
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nAnalysis operation interrupted by user")
        except Exception as e:
            logger.error(f"Model analysis menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in model analysis menu: {str(e)}\n\n"
                f"Context:\n"
                f"- Selected Option: {choice}\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could indicate:\n"
                f"- Analysis dependency issues\n"
                f"- System resource constraints\n"
                f"- Data accessibility problems\n"
                f"- Visualization library conflicts\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"[bold red]{message}[/bold red]",
                    title="MODEL ANALYSIS MENU ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                break

def advanced_tools_menu():
    """Menu for advanced tools and utilities with context display and error handling."""
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        model_config = config.get('model', {})
        system_config = config.get('system', {})
        experimental_config = config.get('experimental', {})
        hardware_config = config.get('hardware', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu display with context
        #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + "ADVANCED TOOLS MENU")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Advanced Tools Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Hardware: " + Fore.YELLOW + Style.BRIGHT + f"{hardware_config.get('device', 'Default')}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Experimental: " + Fore.YELLOW + Style.BRIGHT + f"{'Enabled' if experimental_config.get('enabled', False) else 'Disabled'}")
        
        # Menu options with context-aware display
        print(Fore.YELLOW + Style.BRIGHT + "\nAdvanced Tools & Utilities:")
        print(Fore.WHITE + Style.BRIGHT + "1. Model Export Utilities " + Fore.GREEN + Style.BRIGHT + f"(Model: {model_type})")
        print(Fore.WHITE + Style.BRIGHT + "2. Custom Data Preprocessing " + Fore.GREEN + Style.BRIGHT + "(Advanced Pipelines)")
        print(Fore.WHITE + Style.BRIGHT + "3. Batch Processing Tools " + Fore.GREEN + Style.BRIGHT + "(Large-scale Operations)")
        print(Fore.WHITE + Style.BRIGHT + "4. Performance Profiling " + Fore.GREEN + Style.BRIGHT + "(System Optimization)")
        print(Fore.WHITE + Style.BRIGHT + "5. Model Conversion Tools " + Fore.GREEN + Style.BRIGHT + "(Format Conversion)")
        print(Fore.WHITE + Style.BRIGHT + "6. Custom Training Loops " + Fore.GREEN + Style.BRIGHT + "(Expert Configuration)")
        print(Fore.WHITE + Style.BRIGHT + "7. Distributed Training Setup " + Fore.GREEN + Style.BRIGHT + "(Multi-GPU/Node)")
        print(Fore.WHITE + Style.BRIGHT + "8. Model Serving Deployment " + Fore.GREEN + Style.BRIGHT + "(Production Ready)")
        print(Fore.WHITE + Style.BRIGHT + "9. Experimental Features " + Fore.GREEN + Style.BRIGHT + "(Cutting-edge Tools)")
        print(Fore.RED + Style.BRIGHT + "0. Back to Main Menu")
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-9): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                return
        
        try:
            if choice in ["1", "2", "3", "4", "5", "6", "7", "8", "9"]:
                # Coming soon message with context for all tools
                tool_type = {
                    "1": "Model Export Utilities",
                    "2": "Custom Data Preprocessing",
                    "3": "Batch Processing Tools",
                    "4": "Performance Profiling",
                    "5": "Model Conversion Tools", 
                    "6": "Custom Training Loops",
                    "7": "Distributed Training Setup",
                    "8": "Model Serving Deployment",
                    "9": "Experimental Features"
                }.get(choice, "Advanced Tool")
                
                message = (
                    f"{tool_type}\n\n"
                    f"Current Context:\n"
                    f"- Model: {model_type}\n"
                    f"- Preset: {preset_name}\n"
                    f"- Hardware: {hardware_config.get('device', 'Default')}\n"
                    f"- Status: Under Development\n\n"
                    f"This advanced tool is currently in development\n"
                    f"and will be available in the next release.\n\n"
                    f"Planned features include:\n"
                    f"- Professional-grade utilities and tools\n"
                    f"- Enterprise-level functionality\n"
                    f"- Production deployment capabilities\n"
                    f"- Advanced optimization features\n\n"
                    f"These tools are designed for:\n"
                    f"- Expert users and developers\n"
                    f"- Production system deployment\n"
                    f"- Research and development\n"
                    f"- Large-scale implementations\n\n"
                    f"Check back soon for updates!\n"
                )
                console.print(
                    Panel.fit(
                        f"[bold white]{message}[/bold white]",
                        title=f"{tool_type} - COMING SOON",
                        style="bold magenta",
                        border_style="magenta",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                time.sleep(3)
                
            elif choice == "0":
                return
            else:
                print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-9.")
        
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nAdvanced tools operation interrupted by user")
        except Exception as e:
            logger.error(f"Advanced tools menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in advanced tools menu: {str(e)}\n\n"
                f"Context:\n"
                f"- Selected Option: {choice}\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could indicate:\n"
                f"- System compatibility issues\n"
                f"- Advanced feature dependencies\n"
                f"- Experimental tool conflicts\n"
                f"- Hardware configuration problems\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"[bold red]{message}[/bold red]",
                    title="ADVANCED TOOLS MENU ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                break

def show_init_report():
    """
    Display available initialization reports and provide options to view them.
    Supports multiple report formats: HTML dashboard, JSON, TXT summary, and diagnostics.
    """
    from datetime import datetime
    
    try:
        # Clear screen and show banner with configuration
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        system_config = config.get('system', {})
        hardware_config = config.get('hardware', {})
        monitoring_config = config.get('monitoring', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        model_config = config.get('model', {})
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Determine report directory
        report_dir = Path(__file__).resolve().parent / "reports"
        
        if not report_dir.exists():
            message = (
                f"No reports directory found!\n"
                f"System Context:\n"
                f"- Preset: {preset_name}\n"
                f"- Model: {model_type}\n"
                f"- Source: {config_source}\n"
                f"Initialize the system first to generate reports.\n"
                f"Reports will be saved to: {report_dir}\n"
                f"Run system initialization from the main menu to create reports."
            )
            
            console.print(
                Panel.fit(
                    f"{message}",
                    title="NO REPORTS DIRECTORY",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
            # Input handling
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
            return
        
        # Scan for available reports
        report_files = {
            'html_dashboards': list(report_dir.glob("deep_system_dashboard_*.html")),
            'json_reports': list(report_dir.glob("deep_init_report_*.json")),
            'txt_summaries': list(report_dir.glob("deep_init_summary_*.txt")),
            'status_files': list(report_dir.glob("deep_init_status_*.json")),
            'diagnostics': list(report_dir.glob("deep_init_diagnostics_*.json")),
            'dashboard_data': list(report_dir.glob("deep_system_dashboard_data_*.json")),
            'latest_html': list(report_dir.glob("deep_latest_system_dashboard.html")),
            'latest_summary': list(report_dir.glob("deep_latest_init_summary.txt")),
            'index_file': list(report_dir.glob("deep_initialization_reports.txt"))
        }
        
        # Count total reports
        total_files = sum(len(files) for files in report_files.values())
        
        if total_files == 0:
            message = (
                f"No initialization reports found!\n"
                f"System Context:\n"
                f"- Preset: {preset_name}\n"
                f"- Model: {model_type}\n"
                f"- Source: {config_source}\n"
                f"Run system initialization to generate reports.\n"
                f"Reports will include:\n"
                f"- System configuration analysis\n"
                f"- Hardware capability assessment\n"
                f"- Dependency verification\n"
                f"- Performance benchmarks\n"
                f"- Health status evaluation\n"
                f"Check the reports directory: {report_dir}"
            )
            
            console.print(
                Panel.fit(
                    f"{message}",
                    title="NO REPORTS FOUND",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
            return
        
        # Menu display with context
        #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + "INITIALIZATION REPORTS MENU")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"System Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Reports: " + Fore.YELLOW + Style.BRIGHT + f"{total_files} files")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Location: " + Fore.YELLOW + Style.BRIGHT + f"{report_dir}")
        
        # Prepare menu categories
        categories = []
        if report_files['html_dashboards'] or report_files['latest_html']:
            categories.append("1. HTML Dashboards " + Fore.GREEN + Style.BRIGHT + "(Interactive Visualization)")
        if report_files['txt_summaries'] or report_files['latest_summary']:
            categories.append("2. Text Summaries " + Fore.GREEN + Style.BRIGHT + "(Human-readable)")
        if report_files['json_reports']:
            categories.append("3. Full JSON Reports " + Fore.GREEN + Style.BRIGHT + "(Machine-readable)")
        if report_files['status_files']:
            categories.append("4. Status Reports " + Fore.GREEN + Style.BRIGHT + "(Compact Overview)")
        if report_files['diagnostics']:
            categories.append("5. Diagnostic Reports " + Fore.GREEN + Style.BRIGHT + "(Technical Details)")
        if report_files['dashboard_data']:
            categories.append("6. Dashboard Data " + Fore.GREEN + Style.BRIGHT + "(Raw Data Files)")
        if report_files['index_file']:
            categories.append("7. Report Index " + Fore.GREEN + Style.BRIGHT + "(Quick Overview)")
        
        categories.append("8. Show All Available Files " + Fore.GREEN + Style.BRIGHT + "(Complete Listing)")
        categories.append(Fore.RED + Style.BRIGHT + "0. Return to Main Menu")
        
        # def display_menu():
        #     """Display the report menu categories with enhanced styling."""
        #     print(Fore.YELLOW + Style.BRIGHT + "\nAvailable Report Categories:")
        #     for category in categories:
        #         if category.startswith("0."):
        #             print(f"  {category}")
        #         else:
        #             print(f"  {Fore.WHITE + Style.BRIGHT}{category.split(' (')[0]}{Style.RESET_ALL}{' (' + category.split(' (')[1] if '(' in category else ''}")
        
        def display_menu():
            """Display the report menu categories with enhanced styling."""
            print(Fore.YELLOW + Style.BRIGHT + "\nAvailable Report Categories:")
            for category in categories:
                if category.startswith("0."):
                    print(f"  {category}")
                else:
                    # Use partition which always returns 3 parts (before, separator, after)
                    main_part, separator, desc_part = category.partition(' (')
                    if separator:  # If ' (' was found in the string
                        print(f"  {Fore.WHITE + Style.BRIGHT}{main_part}{Style.RESET_ALL}{separator}{desc_part}")
                    else:
                        # Handle categories without parentheses
                        print(f"  {Fore.WHITE + Style.BRIGHT}{category}{Style.RESET_ALL}")
        
        # Display menu initially
        display_menu()
        
        while True:
            # Input handling with retry logic
            choice = None
            while not choice:
                try:
                    choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect report type (0-8): ").strip()
                    
                    # If empty input, retry
                    if not choice:
                        continue
                        
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                    return
            
            try:
                # HTML Dashboards
                if choice == "1":
                    try:
                        html_files = report_files['html_dashboards'] + report_files['latest_html']
                        if not html_files:
                            message = (
                                f"No HTML dashboard files found.\n"
                                f"HTML dashboards provide interactive visualizations of:\n"
                                f"- System performance metrics\n"
                                f"- Hardware utilization charts\n"
                                f"- Configuration health status\n"
                                f"- Training progress analytics\n"
                                f"Run system initialization to generate HTML dashboards."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO HTML DASHBOARDS",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nHTML Dashboard Reports:")
                        for i, file_path in enumerate(html_files, 1):
                            file_size = file_path.stat().st_size / 1024  # KB
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        # File selection with error handling
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(html_files):
                                    selected_file = html_files[file_choice - 1]
                                    message = (
                                        f"Opening HTML dashboard in browser...\n"
                                        f"File: {selected_file.name}\n"
                                        f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                                        f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                                        f"The dashboard will open in your default web browser.\n"
                                        f"This may take a few moments..."
                                    )
                                    console.print(
                                        Panel.fit(
                                            f"{message}",
                                            title="OPENING DASHBOARD",
                                            style="bold green",
                                            border_style="green",
                                            padding=(1, 2),
                                            box=box.ROUNDED
                                        )
                                    )
                                    webbrowser.open(f"file://{selected_file.absolute()}")
                                    print(Fore.GREEN + Style.BRIGHT + f"Dashboard opened: {selected_file.name}")
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection. Please choose a valid file number.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing HTML dashboards: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Model: {model_type}\n"
                            f"This could be due to:\n"
                            f"- File permission issues\n"
                            f"- Corrupted report files\n"
                            f"- Browser compatibility problems"
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="HTML DASHBOARD ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Text Summaries
                elif choice == "2":
                    try:
                        txt_files = report_files['txt_summaries'] + report_files['latest_summary']
                        if not txt_files:
                            print(Fore.RED + Style.BRIGHT + "No text summary files found.")
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nText Summary Reports:")
                        for i, file_path in enumerate(txt_files, 1):
                            file_size = file_path.stat().st_size / 1024
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(txt_files):
                                    selected_file = txt_files[file_choice - 1]
                                    _display_text_report(selected_file)
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing text summaries: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Total Files: {len(txt_files) if 'txt_files' in locals() else 0}\n"
                            f"Please check file permissions and integrity."
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="TEXT SUMMARY ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Full JSON Reports
                elif choice == "3":
                    try:
                        json_files = report_files['json_reports']
                        if not json_files:
                            message = (
                                f"No JSON report files found.\n"
                                f"JSON reports provide comprehensive machine-readable data including:\n"
                                f"- Complete system configuration details\n"
                                f"- Hardware specifications and capabilities\n"
                                f"- Dependency verification results\n"
                                f"- Performance benchmark data\n"
                                f"- Health assessment metrics\n"
                                f"Run system initialization to generate JSON reports."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO JSON REPORTS",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nFull JSON Reports:")
                        for i, file_path in enumerate(json_files, 1):
                            file_size = file_path.stat().st_size / 1024  # KB
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            # Try to get entry count from JSON
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    data = json.load(f)
                                    entry_count = len(data.get('reports', []))
                                    print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {entry_count} entries, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                            except:
                                print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(json_files):
                                    selected_file = json_files[file_choice - 1]
                                    message = (
                                        f"Displaying JSON Report\n"
                                        f"File: {selected_file.name}\n"
                                        f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                                        f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                                        f"JSON reports contain structured data suitable for:\n"
                                        f"- Programmatic analysis\n"
                                        f"- Data processing pipelines\n"
                                        f"- Integration with other tools\n"
                                        f"- Automated reporting systems"
                                    )
                                    console.print(
                                        Panel.fit(
                                            f"{message}",
                                            title="JSON REPORT VIEWER",
                                            style="bold green",
                                            border_style="green",
                                            padding=(1, 2),
                                            box=box.ROUNDED
                                        )
                                    )
                                    _display_json_report(selected_file)
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection. Please choose a valid file number.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing JSON reports: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Total JSON Files: {len(json_files) if 'json_files' in locals() else 0}\n"
                            f"This could be due to:\n"
                            f"- JSON file corruption\n"
                            f"- Encoding issues\n"
                            f"- File permission problems\n"
                            f"- Memory constraints for large files"
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="JSON REPORT ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Status Reports
                elif choice == "4":
                    try:
                        status_files = report_files['status_files']
                        if not status_files:
                            message = (
                                f"No status report files found.\n"
                                f"Status reports provide compact overviews including:\n"
                                f"- System health status summary\n"
                                f"- Key performance indicators\n"
                                f"- Critical configuration settings\n"
                                f"- Quick reference metrics\n"
                                f"- Alert and warning summaries\n"
                                f"Run system initialization to generate status reports."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO STATUS REPORTS",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nStatus Reports:")
                        for i, file_path in enumerate(status_files, 1):
                            file_size = file_path.stat().st_size / 1024  # KB
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            # Try to get entry count from JSON
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    data = json.load(f)
                                    entry_count = len(data.get('entries', []))
                                    print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {entry_count} entries, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                            except:
                                print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(status_files):
                                    selected_file = status_files[file_choice - 1]
                                    message = (
                                        f"Displaying Status Report\n"
                                        f"File: {selected_file.name}\n"
                                        f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                                        f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                                        f"Status reports provide quick overviews of:\n"
                                        f"- System operational status\n"
                                        f"- Critical metrics at a glance\n"
                                        f"- Health indicators summary\n"
                                        f"- Performance snapshots"
                                    )
                                    console.print(
                                        Panel.fit(
                                            f"{message}",
                                            title="STATUS REPORT VIEWER",
                                            style="bold green",
                                            border_style="green",
                                            padding=(1, 2),
                                            box=box.ROUNDED
                                        )
                                    )
                                    _display_status_report(selected_file)
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection. Please choose a valid file number.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing status reports: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Total Status Files: {len(status_files) if 'status_files' in locals() else 0}\n"
                            f"Please check file integrity and permissions."
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="STATUS REPORT ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Diagnostic Reports
                elif choice == "5":
                    try:
                        diag_files = report_files['diagnostics']
                        if not diag_files:
                            message = (
                                f"No diagnostic report files found.\n"
                                f"Diagnostic reports provide technical details including:\n"
                                f"- Detailed system diagnostics\n"
                                f"- Performance profiling data\n"
                                f"- Resource utilization analysis\n"
                                f"- Error and warning logs\n"
                                f"- Troubleshooting information\n"
                                f"Run system initialization to generate diagnostic reports."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO DIAGNOSTIC REPORTS",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nDiagnostic Reports:")
                        for i, file_path in enumerate(diag_files, 1):
                            file_size = file_path.stat().st_size / 1024  # KB
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(diag_files):
                                    selected_file = diag_files[file_choice - 1]
                                    message = (
                                        f"Displaying Diagnostic Report\n"
                                        f"File: {selected_file.name}\n"
                                        f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                                        f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                                        f"Diagnostic reports contain technical information for:\n"
                                        f"- System troubleshooting\n"
                                        f"- Performance optimization\n"
                                        f"- Resource analysis\n"
                                        f"- Technical support scenarios"
                                    )
                                    console.print(
                                        Panel.fit(
                                            f"{message}",
                                            title="DIAGNOSTIC REPORT VIEWER",
                                            style="bold green",
                                            border_style="green",
                                            padding=(1, 2),
                                            box=box.ROUNDED
                                        )
                                    )
                                    _display_diagnostic_report(selected_file)
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection. Please choose a valid file number.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing diagnostic reports: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Total Diagnostic Files: {len(diag_files) if 'diag_files' in locals() else 0}\n"
                            f"This could indicate:\n"
                            f"- Diagnostic data corruption\n"
                            f"- Technical analysis issues\n"
                            f"- File format problems"
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="DIAGNOSTIC REPORT ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Dashboard Data
                elif choice == "6":
                    try:
                        data_files = report_files['dashboard_data']
                        if not data_files:
                            message = (
                                f"No dashboard data files found.\n"
                                f"Dashboard data files contain raw data for:\n"
                                f"- Interactive visualization generation\n"
                                f"- Custom chart creation\n"
                                f"- Data analysis and processing\n"
                                f"- External tool integration\n"
                                f"- Historical data comparison\n"
                                f"Run system initialization to generate dashboard data files."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO DASHBOARD DATA",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nDashboard Data Files:")
                        for i, file_path in enumerate(data_files, 1):
                            file_size = file_path.stat().st_size / 1024  # KB
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(data_files):
                                    selected_file = data_files[file_choice - 1]
                                    message = (
                                        f"Displaying Dashboard Data\n"
                                        f"File: {selected_file.name}\n"
                                        f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                                        f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                                        f"Dashboard data files contain structured data for:\n"
                                        f"- Visualization backend processing\n"
                                        f"- Custom analytics development\n"
                                        f"- Data export and integration\n"
                                        f"- Performance trend analysis"
                                    )
                                    console.print(
                                        Panel.fit(
                                            f"{message}",
                                            title="DASHBOARD DATA VIEWER",
                                            style="bold green",
                                            border_style="green",
                                            padding=(1, 2),
                                            box=box.ROUNDED
                                        )
                                    )
                                    _display_dashboard_data(selected_file)
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection. Please choose a valid file number.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing dashboard data: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Total Data Files: {len(data_files) if 'data_files' in locals() else 0}\n"
                            f"Please check data file integrity and format."
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="DASHBOARD DATA ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Report Index
                elif choice == "7":
                    try:
                        index_files = report_files['index_file']
                        if not index_files:
                            message = (
                                f"No index file found.\n"
                                f"The report index provides:\n"
                                f"- Quick overview of all available reports\n"
                                f"- File locations and timestamps\n"
                                f"- Report categories and types\n"
                                f"- Summary statistics\n"
                                f"- Navigation guidance\n"
                                f"Run system initialization to generate the report index."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO REPORT INDEX",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        selected_file = index_files[0]
                        message = (
                            f"Displaying Report Index\n"
                            f"File: {selected_file.name}\n"
                            f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                            f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                            f"The report index provides a comprehensive overview of:\n"
                            f"- All available report files and formats\n"
                            f"- File locations and access information\n"
                            f"- Summary statistics and metadata\n"
                            f"- Quick navigation references"
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="REPORT INDEX VIEWER",
                                style="bold green",
                                border_style="green",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        _display_text_report(selected_file, title="Report Index")
                        display_menu()
                        
                    except Exception as e:
                        message = (
                            f"Error accessing report index: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Index File: {index_files[0].name if index_files else 'None'}\n"
                            f"Please check index file accessibility and format."
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="REPORT INDEX ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Show All Report Files
                elif choice == "8":
                    try:
                        _show_all_report_files(report_dir, report_files)
                        display_menu()
                    except Exception as e:
                        message = (
                            f"Error displaying all files: {str(e)}\n"
                            f"Context:\n"
                            f"- Report Directory: {report_dir}\n"
                            f"- Total Files: {total_files}\n"
                            f"Please check directory accessibility."
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="FILE LISTING ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Exit to Main Menu
                elif choice == "0":
                    return
                
                else:
                    print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-8.")
                    display_menu()
                    
            except KeyboardInterrupt:
                print(Fore.RED + Style.BRIGHT + "\nReport operation interrupted by user")
                display_menu()
            except Exception as e:
                logger.error(f"Init report menu error: {e}", exc_info=True)
                message = (
                    f"Unexpected error in reports menu: {str(e)}\n"
                    f"Context:\n"
                    f"- Selected Option: {choice}\n"
                    f"- Current Preset: {preset_name}\n"
                    f"- Model Type: {model_type}\n"
                    f"- Config Source: {config_source}\n\n"
                    f"This could indicate:\n"
                    f"- File system issues\n"
                    f"- Report corruption\n"
                    f"- Permission problems\n"
                    f"- Resource constraints\n"
                    f"Please check the logs for detailed information."
                )
                console.print(
                    Panel.fit(
                        f"{message}",
                        title="REPORTS MENU ERROR",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                display_menu()
    
    except Exception as e:
        logger.error(f"Show init report error: {e}", exc_info=True)
        message = (
            f"Error accessing initialization reports: {str(e)}\n"
            f"This could be due to:\n"
            f"- Missing reports directory\n"
            f"- File system permissions\n"
            f"- Configuration issues\n"
            f"- System resource problems\n"
            f"Please check system initialization and try again."
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="INIT REPORTS ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        # Input handling on fatal error
        try:
            input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
        except (EOFError, KeyboardInterrupt):
            print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")

def run_performance_benchmark_interactive():
    """Interactive performance benchmark with context display and error handling."""
    try:
        # Clear screen and show banner with configuration
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        system_config = config.get('system', {})
        hardware_config = config.get('hardware', {})
        training_config = config.get('training', {})
        model_config = config.get('model', {})
        monitoring_config = config.get('monitoring', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Context-aware Menu options
        #print(Fore.YELLOW + Style.BRIGHT + "\n" + "="*40)
        print(Fore.YELLOW + Style.BRIGHT + "PERFORMANCE BENCHMARK")
        print(Fore.CYAN + Style.BRIGHT + "="*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Benchmark Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Device: " + Fore.YELLOW + Style.BRIGHT + f"{hardware_config.get('device', 'Default')}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{training_config.get('epochs', 'Default')}")
        
        # Display benchmark overview panel
        message = (
            f"Performance Benchmark Analysis\n\n"
            f"This benchmark will test multiple model configurations to evaluate:\n"
            f"- Training speed and efficiency\n"
            f"- Memory usage and optimization\n"
            f"- Model scalability across sizes\n"
            f"- System resource utilization\n\n"
            f"Benchmark configurations include:\n"
            f"- Small model (8 encoding dimensions)\n"
            f"- Medium model (16 encoding dimensions)  \n"
            f"- Large model (32 encoding dimensions)\n\n"
            f"Each configuration will be trained for 10 epochs with performance metrics.\n"
            f"Results will be saved for comparison and analysis."
        )
        console.print(
            Panel.fit(
                f"[bold white]{message}[/bold white]",
                title="PERFORMANCE BENCHMARK OVERVIEW",
                style="bold cyan",
                border_style="cyan",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nRun performance benchmark? This will train multiple models. (Y/n): ").strip().lower()
                
                # If empty input, default to yes
                if not choice:
                    choice = 'y'
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nBenchmark cancelled by user")
                return
        
        if choice in ('', 'y', 'yes'):
            try:
                # Create benchmark arguments with configuration
                args = argparse.Namespace()
                args.model_dir = DEFAULT_MODEL_DIR / "benchmarks"
                args.epochs = 10
                args.non_interactive = True
                args.export_onnx = False
                
                # Show starting message
                message = (
                    f"Starting Performance Benchmark\n\n"
                    f"Configuration:\n"
                    f"- Preset: {preset_name}\n"
                    f"- Model Type: {model_type}\n"
                    f"- Benchmark Directory: {args.model_dir}\n"
                    f"- Epochs per Configuration: {args.epochs}\n"
                    f"- Test Configurations: Small, Medium, Large\n\n"
                    f"This may take several minutes depending on your hardware.\n"
                    f"Progress will be displayed below..."
                )
                console.print(
                    Panel.fit(
                        f"[bold green]{message}[/bold green]",
                        title="STARTING BENCHMARK",
                        style="bold green",
                        border_style="green",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                
                # Small delay to ensure user sees the message
                time.sleep(3)
                
                # Run the benchmark
                run_performance_benchmark(args)
                
            except Exception as e:
                message = (
                    f"Error encountered during benchmark execution: {str(e)}\n\n"
                    f"Context:\n"
                    f"- Preset: {preset_name}\n"
                    f"- Model: {model_type}\n"
                    f"- Benchmark Directory: {args.model_dir if 'args' in locals() else 'Unknown'}\n\n"
                    f"This could be due to:\n"
                    f"- Insufficient system resources\n"
                    f"- Model configuration issues\n"
                    f"- File system permissions\n"
                    f"- Dependency conflicts\n"
                )
                console.print(
                    Panel.fit(
                        f"[bold red]{message}[/bold red]",
                        title="BENCHMARK EXECUTION ERROR",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
        else:
            print(Fore.RED + Style.BRIGHT + "\nBenchmark cancelled")
            
    except Exception as e:
        logger.error(f"Performance benchmark interactive error: {e}", exc_info=True)
        message = (
            f"Unexpected error in performance benchmark setup: {str(e)}\n\n"
            f"This could indicate:\n"
            f"- System configuration issues\n"
            f"- Resource allocation problems\n"
            f"- Benchmark dependency conflicts\n"
            f"- File system access restrictions\n\n"
            f"Please check the logs for detailed information."
        )
        console.print(
            Panel.fit(
                f"[bold red]{message}[/bold red]",
                title="BENCHMARK SETUP ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
    
    # Input handling on fatal error
    try:
        input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
    except (EOFError, KeyboardInterrupt):
        print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")

def run_performance_benchmark(args: argparse.Namespace) -> None:
    """Run performance benchmark across different configurations with error handling and reporting."""
    try:
        logger.info("Running performance benchmark...")
        
        # Benchmark configurations with descriptions
        benchmark_configs = [
            ('small', {
                'description': 'Small model for quick testing',
                'config': {'model': {'encoding_dim': 8}, 'training': {'batch_size': 32, 'epochs': 10}}
            }),
            ('medium', {
                'description': 'Medium model for balanced performance', 
                'config': {'model': {'encoding_dim': 16}, 'training': {'batch_size': 64, 'epochs': 10}}
            }),
            ('large', {
                'description': 'Large model for maximum accuracy',
                'config': {'model': {'encoding_dim': 32}, 'training': {'batch_size': 128, 'epochs': 10}}
            })
        ]
        
        results = {}
        total_configs = len(benchmark_configs)
        
        # Display benchmark progress header
        console.print(
            Panel.fit(
                f"[bold cyan]Running {total_configs} benchmark configurations...[/bold cyan]",
                title="BENCHMARK PROGRESS",
                style="bold cyan",
                border_style="cyan",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        for i, (name, benchmark_info) in enumerate(benchmark_configs, 1):
            config_override = benchmark_info['config']
            description = benchmark_info['description']
            
            # Show current benchmark progress
            progress_message = (
                f"Configuration {i}/{total_configs}: {name.upper()}\n"
                f"Description: {description}\n"
                f"Encoding Dimensions: {config_override['model']['encoding_dim']}\n"
                f"Batch Size: {config_override['training']['batch_size']}\n"
                f"Epochs: {config_override['training']['epochs']}\n\n"
                f"Starting benchmark..."
            )
            console.print(
                Panel.fit(
                    f"[bold white]{progress_message}[/bold white]",
                    title=f"BENCHMARKING {name.upper()}",
                    style="bold yellow",
                    border_style="yellow",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
            logger.info(f"Benchmarking {name} configuration: {description}")
            
            # Create benchmark args
            benchmark_args = argparse.Namespace(**vars(args))
            benchmark_args.model_dir = args.model_dir / f"benchmark_{name}"
            benchmark_args.epochs = 10
            benchmark_args.non_interactive = True
            benchmark_args.export_onnx = False
            
            # Apply config override
            current_config = get_current_config()
            benchmark_config = deep_update(current_config, config_override)
            update_global_config(benchmark_config)
            
            start_time = time.time()
            try:
                training_results = train_model(benchmark_args)
                duration = time.time() - start_time
                
                # Results collection
                results[name] = {
                    'duration': duration,
                    'final_loss': training_results.get('evaluation', {}).get('test_loss', float('inf')),
                    'parameters': training_results.get('model', {}).get('total_parameters', 0),
                    'training_accuracy': training_results.get('evaluation', {}).get('training_accuracy', 0),
                    'validation_accuracy': training_results.get('evaluation', {}).get('validation_accuracy', 0),
                    'memory_usage': training_results.get('system', {}).get('peak_memory_mb', 0),
                    'description': description,
                    'success': True
                }
                
                # Show success message
                success_message = (
                    f"✓ {name.upper()} benchmark completed successfully!\n\n"
                    f"Results:\n"
                    f"- Duration: {duration:.1f}s\n"
                    f"- Final Loss: {results[name]['final_loss']:.4f}\n"
                    f"- Parameters: {results[name]['parameters']:,}\n"
                    f"- Training Accuracy: {results[name]['training_accuracy']:.2%}\n"
                    f"- Peak Memory: {results[name]['memory_usage']:.1f} MB"
                )
                console.print(
                    Panel.fit(
                        f"[bold green]{success_message}[/bold green]",
                        title=f"BENCHMARK COMPLETE - {name.upper()}",
                        style="bold green",
                        border_style="green",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                
            except Exception as e:
                duration = time.time() - start_time
                results[name] = {
                    'duration': duration,
                    'error': str(e),
                    'description': description,
                    'success': False
                }
                
                # Show error message
                error_message = (
                    f"✗ {name.upper()} benchmark failed!\n\n"
                    f"Error: {str(e)}\n"
                    f"Duration: {duration:.1f}s\n\n"
                    f"The benchmark will continue with remaining configurations."
                )
                console.print(
                    Panel.fit(
                        f"[bold red]{error_message}[/bold red]",
                        title=f"BENCHMARK FAILED - {name.upper()}",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                logger.error(f"Benchmark {name} failed: {e}")
        
        # Save benchmark results with metadata
        benchmark_path = args.model_dir / "benchmark_results.json"
        benchmark_metadata = {
            'timestamp': datetime.now().isoformat(),
            'total_configurations': total_configs,
            'successful_runs': sum(1 for r in results.values() if r['success']),
            'failed_runs': sum(1 for r in results.values() if not r['success']),
            'system_info': {
                'preset': get_current_config().get('presets', {}).get('current_preset', 'Unknown'),
                'model_type': get_current_config().get('model', {}).get('model_type', 'Unknown'),
                'device': get_current_config().get('hardware', {}).get('device', 'Unknown')
            },
            'results': results
        }
        
        with open(benchmark_path, 'w', encoding='utf-8') as f:
            json.dump(benchmark_metadata, f, indent=2, default=str)
        
        # Results display
        console.print(
            Panel.fit(
                f"[bold cyan]Benchmark Complete![/bold cyan]\n\n"
                f"Results saved to: {benchmark_path}\n"
                f"Total configurations: {total_configs}\n"
                f"Successful: {benchmark_metadata['successful_runs']}\n"
                f"Failed: {benchmark_metadata['failed_runs']}",
                title="BENCHMARK SUMMARY",
                style="bold cyan",
                border_style="cyan",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        # Display results table
        logger.info("Benchmark Results:")
        logger.info("=" * 70)
        for name, result in results.items():
            if result['success']:
                logger.info(f"{name:>10}: {result['duration']:6.1f}s | Loss: {result['final_loss']:.4f} | "
                           f"Params: {result['parameters']:,} | Acc: {result['training_accuracy']:.2%} | "
                           f"Memory: {result['memory_usage']:.1f}MB")
            else:
                logger.info(f"{name:>10}: {result['duration']:6.1f}s | FAILED: {result['error']}")
        
    except Exception as e:
        logger.error(f"Performance benchmark execution error: {e}", exc_info=True)
        message = (
            f"Critical error during benchmark execution: {str(e)}\n\n"
            f"The benchmark could not complete successfully.\n"
            f"Please check system resources and configuration.\n\n"
            f"Detailed error information has been logged."
        )
        console.print(
            Panel.fit(
                f"[bold red]{message}[/bold red]",
                title="BENCHMARK CRITICAL ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        raise



def show_system_info():
    """Enhanced system information display leveraging comprehensive get_system_info() analysis.
    
    This function provides a rich, detailed system overview using Rich tables
    for improved readability and organization. Now fully integrated with the
    enhanced get_system_info() capabilities including performance metrics,
    memory optimization, and detailed analysis.
    """
    try:
        # Clear screen and show banner with configuration
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration context using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu display with context
        #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + "SYSTEM INFORMATION & ANALYSIS")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Active System Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        
        # Get system information
        try:
            system_info = get_system_info(
                include_versions=True,
                include_hardware=True,
                include_memory_usage=True,
                include_detailed_analysis=True,
                include_performance_baseline=False,
                include_memory_optimization=False
            )
            analysis_available = True
            collection_successful = system_info.get('collection_metadata', {}).get('success', True)
            
        except Exception as e:
            logger.error(f"Failed to generate comprehensive system analysis: {e}")
            
            # Display errors with context
            message = (
                f"Failed to generate comprehensive system analysis: {str(e)}\n"
                f"Context:\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could be due to:\n"
                f"- System resource constraints\n"
                f"- Hardware detection issues\n"
                f"- Missing system dependencies\n"
                f"- Permission or access problems\n\n"
                f"Attempting fallback analysis..."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="ANALYSIS WARNING",
                    style="bold yellow",
                    border_style="yellow",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
            # Fallback to basic hardware check
            try:
                hw = check_hardware(include_memory_usage=True)
                system_info = {
                    'hardware': hw,
                    'collection_metadata': {
                        'success': False,
                        'data_quality': 'fallback',
                        'errors': [str(e)]
                    }
                }
                analysis_available = False
                collection_successful = False
            except Exception as fallback_error:
                logger.error(f"Fallback hardware check also failed: {fallback_error}")
                message = (
                    f"Critical: Comprehensive system analysis failed\n"
                    f"Primary Error: {str(e)}\n"
                    f"Fallback Error: {str(fallback_error)}\n\n"
                    f"Context:\n"
                    f"- Current Preset: {preset_name}\n"
                    f"- Model Type: {model_type}\n\n"
                    f"This indicates serious system issues that may affect\n"
                    f"application functionality. Please check:\n"
                    f"1. System resource availability\n"
                    f"2. Hardware detection permissions\n"
                    f"3. Required system dependencies\n"
                    f"4. System logs for detailed errors"
                )
                console.print(
                    Panel.fit(
                        f"{message}",
                        title="CRITICAL ANALYSIS FAILURE",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                system_info = {
                    'collection_metadata': {
                        'success': False,
                        'data_quality': 'failed',
                        'errors': [str(e), str(fallback_error)]
                    }
                }
                analysis_available = False
                collection_successful = False
        
        # Create main system info table
        main_table = Table(
            title="\n[bold yellow]COMPREHENSIVE SYSTEM INFORMATION & ANALYSIS[/bold yellow]",
            box=box.ROUNDED,
            header_style="bold yellow",
            border_style="magenta",
            title_style="bold yellow",
            title_justify="left",
            show_lines=True,
            expand=True,
            width=min(120, console.width - 4)
        )
        
        # Configure columns
        main_table.add_column("Category", style="bold cyan", width=28, no_wrap=True)
        main_table.add_column("Status", width=12, justify="center")
        main_table.add_column("Details", style="bold", min_width=50, max_width=75)
        
        # COLLECTION METADATA & PERFORMANCE
        if analysis_available and 'collection_metadata' in system_info:
            metadata = system_info['collection_metadata']
            collection_quality = metadata.get('data_quality', 'unknown')
            collection_time = metadata.get('collection_duration_ms', 0)
            data_sources = len(metadata.get('data_sources', []))
            errors = len(metadata.get('errors', []))
            warnings = len(metadata.get('warnings', []))
            
            main_table.add_row(
                Text("COLLECTION METADATA", style="bold white on blue"),
                "",
                ""
            )
            
            # Data quality
            quality_colors = {
                'excellent': "bold green",
                'good': "bold green", 
                'acceptable': "bold yellow",
                'degraded': "bold yellow",
                'poor': "bold red",
                'failed': "bold red"
            }
            quality_style = quality_colors.get(collection_quality, "bold yellow")
            
            main_table.add_row(
                "Data Quality",
                Text(collection_quality.upper(), style=quality_style),
                f"Collection: {collection_time:.1f}ms | Sources: {data_sources} | Errors: {errors} | Warnings: {warnings}"
            )
            
            # Performance metrics summary if available
            perf_metrics = metadata.get('performance_metrics', {})
            if perf_metrics:
                ops_completed = len(perf_metrics)
                perf_summary = metadata.get('performance_summary', {})
                efficiency_score = perf_summary.get('efficiency_score', 0)
                efficiency_style = "bold green" if efficiency_score > 80 else "bold yellow" if efficiency_score > 50 else "bold red"
                main_table.add_row(
                    "Performance Metrics",
                    Text(f"{efficiency_score:.0f}%", style=efficiency_style),
                    f"Operations: {ops_completed} | Efficiency Score: {efficiency_score:.1f}%"
                )
            
            # Memory optimization results if available
            mem_opt_results = metadata.get('memory_optimization_results', {})
            if mem_opt_results:
                pre_opt = mem_opt_results.get('pre_collection', {})
                post_opt = mem_opt_results.get('post_collection', {})
                
                if pre_opt.get('success') or post_opt.get('success'):
                    actions_count = len(pre_opt.get('actions_taken', [])) + len(post_opt.get('actions_taken', []))
                    main_table.add_row(
                        "Memory Optimization",
                        Text("ACTIVE", style="bold green"),
                        f"Optimization actions: {actions_count} | Memory management active"
                    )
            
            main_table.add_row(
                "Timestamp",
                Text("INFO", style="bold blue"),
                system_info.get('timestamp', 'unknown')
            )
        
        # PLATFORM & SYSTEM INFORMATION
        main_table.add_row(
            Text("PLATFORM & SYSTEM", style="bold white on blue"),
            "",
            ""
        )
        
        if 'platform' in system_info:
            platform_info = system_info['platform']
            
            # System with boot time if available
            system_detail = f"{platform_info.get('system', 'Unknown')} {platform_info.get('release', '')}"
            if platform_info.get('boot_time'):
                boot_time = platform_info['boot_time'][:19].replace('T', ' ')
                system_detail += f" | Boot: {boot_time}"
                
            main_table.add_row(
                "Operating System",
                Text("INFO", style="bold blue"),
                system_detail
            )
            
            # Platform architecture info
            arch_info = platform_info.get('architecture', ['Unknown', ''])
            platform_detail = platform_info.get('platform', 'Unknown')
            if len(arch_info) > 1 and arch_info[1]:
                platform_detail += f" ({arch_info[0]} - {arch_info[1]})"
            else:
                platform_detail += f" ({arch_info[0] if arch_info else 'Unknown'})"
                
            main_table.add_row(
                "Platform Architecture",
                Text("INFO", style="bold blue"),
                platform_detail
            )
            
            # Processor details
            processor = platform_info.get('processor', 'Unknown')
            processor_detail = processor[:60] + "..." if len(processor) > 60 else processor
            machine = platform_info.get('machine', '')
            if machine and machine not in processor_detail:
                processor_detail += f" ({machine})"
                
            main_table.add_row(
                "Processor",
                Text("INFO", style="bold blue"),
                processor_detail
            )
            
            main_table.add_row(
                "Hostname",
                Text("INFO", style="bold blue"),
                platform_info.get('node', 'Unknown')
            )
        else:
            # Fallback to basic platform info
            main_table.add_row(
                "Operating System",
                Text("INFO", style="bold blue"),
                f"{platform.system()} {platform.release()}"
            )
            
            main_table.add_row(
                "Platform Architecture", 
                Text("INFO", style="bold blue"),
                f"{platform.platform()} ({platform.machine()})"
            )
            
            main_table.add_row(
                "Hostname",
                Text("INFO", style="bold blue"),
                platform.node()
            )
        
        # PYTHON ENVIRONMENT
        main_table.add_row(
            Text("PYTHON ENVIRONMENT", style="bold white on blue"),
            "",
            ""
        )
        
        if 'python' in system_info:
            python_info = system_info['python']
            version_info = python_info.get('version_info', {})
            
            # Version string with implementation
            version_str = f"Python {version_info.get('major', '?')}.{version_info.get('minor', '?')}.{version_info.get('micro', '?')}"
            implementation = python_info.get('implementation', 'Unknown')
            if implementation != 'CPython':
                version_str += f" ({implementation})"
                
            main_table.add_row(
                "Python Version",
                Text("INFO", style="bold blue"),
                version_str
            )
            
            # Build info with compiler
            build_info = python_info.get('build', ['Unknown', ''])
            compiler = python_info.get('compiler', 'Unknown')
            build_detail = f"{build_info[0] if build_info else 'Unknown'}"
            if compiler != 'Unknown' and compiler not in build_detail:
                build_detail += f" | {compiler}"
                
            main_table.add_row(
                "Build & Compiler",
                Text("INFO", style="bold blue"),
                build_detail
            )
            
            # Executable path (shortened for display)
            executable = python_info.get('executable', 'Unknown')
            if len(executable) > 50:
                executable = "..." + executable[-47:]
            main_table.add_row(
                "Executable Path",
                Text("INFO", style="bold blue"),
                executable
            )
            
            # Encoding information
            encoding = python_info.get('encoding', {})
            encoding_detail = f"Default: {encoding.get('default', 'Unknown')}"
            fs_encoding = encoding.get('filesystem', '')
            if fs_encoding and fs_encoding != encoding.get('default'):
                encoding_detail += f" | Filesystem: {fs_encoding}"
                
            main_table.add_row(
                "Character Encoding",
                Text("INFO", style="bold blue"),
                encoding_detail
            )
            
            # Module and path information
            modules_count = python_info.get('modules_count', 0)
            path_count = len(python_info.get('path', []))
            main_table.add_row(
                "Environment Info",
                Text("INFO", style="bold blue"),
                f"Loaded modules: {modules_count} | Path entries: {path_count}"
            )
        else:
            # Fallback Python info
            version_parts = sys.version.split()
            main_table.add_row(
                "Python Version",
                Text("INFO", style="bold blue"),
                version_parts[0] if version_parts else "Unknown"
            )
            
            executable = sys.executable
            if len(executable) > 50:
                executable = "..." + executable[-47:]
            main_table.add_row(
                "Executable Path",
                Text("INFO", style="bold blue"),
                executable
            )
        
        # PACKAGE ENVIRONMENT
        if 'package_versions' in system_info and 'package_analysis' in system_info:
            pkg_analysis = system_info['package_analysis']
            env_health = pkg_analysis.get('environment_health', {})
            
            main_table.add_row(
                Text("PACKAGE ENVIRONMENT", style="bold white on blue"),
                "",
                ""
            )
            
            # Health status
            health_status = env_health.get('overall_status', 'unknown')
            health_colors = {
                'healthy': "bold green",
                'degraded': "bold yellow", 
                'critical': "bold red"
            }
            health_style = health_colors.get(health_status, "bold yellow")
            compat_score = env_health.get('compatibility_score', 0)
            complete_score = env_health.get('completeness_score', 0)
            health_detail = f"Compatibility: {compat_score:.1f}% | Completeness: {complete_score:.1f}%"
            critical_issues = env_health.get('critical_issues', 0)
            warnings_count = env_health.get('warnings', 0)
            if critical_issues > 0 or warnings_count > 0:
                health_detail += f" | Issues: {critical_issues} critical, {warnings_count} warnings"
            
            main_table.add_row(
                "Environment Health",
                Text(health_status.upper(), style=health_style),
                health_detail
            )
            
            # Package status summary
            status_summary = pkg_analysis.get('status_summary', {})
            total_packages = pkg_analysis.get('total_packages', 0)
            available_packages = pkg_analysis.get('available_packages', 0)
            status_details = []
            for status, count in status_summary.items():
                if count > 0:
                    status_color = "green" if status == "OK" else "yellow" if status == "WARNING" else "red"
                    status_details.append(f"[{status_color}]{status}: {count}[/{status_color}]")
                    
            main_table.add_row(
                "Package Status",
                Text("INFO", style="bold blue"),
                f"Total: {total_packages} | Available: {available_packages} | " + " | ".join(status_details)
            )
            
            # Key packages with version details
            packages = system_info['package_versions']
            key_packages = ['PyTorch', 'NumPy', 'Pandas', 'Scikit-learn', 'Rich', 'Optuna']
            key_pkg_details = []
            
            for pkg_name in key_packages:
                # Try different possible keys for the package
                pkg_info = None
                for possible_key in [pkg_name, pkg_name.lower(), pkg_name.replace('-', '_')]:
                    if possible_key in packages:
                        pkg_info = packages[possible_key]
                        break
                if pkg_info:
                    status = pkg_info.get('status', 'UNKNOWN')
                    version = pkg_info.get('version', 'Unknown')
                    
                    # Truncate long version strings
                    if len(version) > 15:
                        version = version[:12] + "..."
                    status_color = "bold green" if status == "OK" else "bold yellow" if status == "WARNING" else "bold red"
                    key_pkg_details.append(f"[{status_color}]{pkg_name}: {version}[/{status_color}]")
                else:
                    key_pkg_details.append(f"[bold red]{pkg_name}: Missing[/bold red]")
            
            main_table.add_row(
                "Key Packages",
                Text("INFO", style="bold blue"),
                " | ".join(key_pkg_details[:3]) + (" | ..." if len(key_pkg_details) > 3 else "")
            )
            
            # Missing required packages
            missing_required = pkg_analysis.get('missing_required', [])
            if missing_required:
                missing_display = ", ".join(missing_required[:3])
                if len(missing_required) > 3:
                    missing_display += f" (and {len(missing_required) - 3} more)"
                main_table.add_row(
                    "Missing Critical",
                    Text("ERROR", style="bold red"),
                    missing_display
                )
            
        else:
            # Fallback package environment check
            main_table.add_row(
                Text("PACKAGE ENVIRONMENT", style="bold white on blue"),
                "",
                ""
            )
            
            fallback_packages = []
            package_checks = [
                ('PyTorch', 'torch', torch if 'torch' in globals() else None),
                ('NumPy', 'numpy', np if 'np' in globals() else None),
                ('Rich', 'rich', None),
                ('Pandas', 'pandas', None)
            ]
            
            for display_name, module_name, module_obj in package_checks:
                try:
                    if module_obj and hasattr(module_obj, '__version__'):
                        version = module_obj.__version__
                        fallback_packages.append(f"[bold green]{display_name}: {version}[/bold green]")
                    else:
                        # Try to import and get version
                        try:
                            imported_module = __import__(module_name)
                            version = getattr(imported_module, '__version__', 'Available')
                            fallback_packages.append(f"[bold green]{display_name}: {version}[/bold green]")
                        except ImportError:
                            fallback_packages.append(f"[bold red]{display_name}: Missing[/bold red]")
                except Exception:
                    fallback_packages.append(f"[bold yellow]{display_name}: Unknown[/bold yellow]")
            
            if fallback_packages:
                main_table.add_row(
                    "Package Status",
                    Text("BASIC", style="bold yellow"),
                    " | ".join(fallback_packages)
                )
        
        # HARDWARE ANALYSIS
        if analysis_available and 'hardware_analysis' in system_info:
            hw_analysis = system_info['hardware_analysis']
            capabilities = hw_analysis.get('capabilities', {})
            
            main_table.add_row(
                Text("HARDWARE ANALYSIS", style="bold white on blue"),
                "",
                ""
            )
            
            # System performance overview
            system_class = hw_analysis.get('system_class', 'unknown')
            performance_score = hw_analysis.get('performance_score', 0)
            overall_health = hw_analysis.get('overall_health', 'unknown')
            
            # Performance class styling
            class_colors = {
                'high_performance': "bold green",
                'standard': "bold blue",
                'limited': "bold yellow",
                'critical': "bold red"
            }
            
            class_style = class_colors.get(system_class, "bold yellow")
            health_colors = {
                'healthy': "bold green",
                'degraded': "bold yellow",
                'critical': "bold red"
            }
            
            health_style = health_colors.get(overall_health, "bold yellow")
            
            # Performance details
            perf_detail = f"Performance Score: {performance_score}/100"
            
            # Add component summary
            components_info = []
            
            if 'components_healthy' in hw_analysis and 'components_detected' in hw_analysis:
                healthy = hw_analysis['components_healthy']
                total = hw_analysis['components_detected']
                warning = hw_analysis.get('components_warning', 0)
                failed = hw_analysis.get('components_failed', 0)
                if total > 0:
                    components_info.append(f"Components: {healthy}/{total} healthy")
                    if warning > 0:
                        components_info.append(f"{warning} warnings")
                    if failed > 0:
                        components_info.append(f"{failed} failed")
            
            if components_info:
                perf_detail += f" | {' | '.join(components_info)}"
            
            main_table.add_row(
                "System Classification",
                Text(system_class.replace('_', ' ').title(), style=class_style),
                perf_detail
            )
            
            main_table.add_row(
                "Overall Health",
                Text(overall_health.upper(), style=health_style),
                ""
            )
            
            # CPU Information with performance metrics
            cpu_info = capabilities.get('cpu', {})
            
            if cpu_info:
                logical_cores = cpu_info.get('logical_cores', 'Unknown')
                physical_cores = cpu_info.get('physical_cores', 'Unknown')
                perf_class = cpu_info.get('performance_class', 'unknown')
                cpu_detail = f"Logical: {logical_cores}, Physical: {physical_cores}"
                
                # Add frequency information
                freq_ghz = cpu_info.get('frequency_ghz')
                max_freq_ghz = cpu_info.get('max_frequency_ghz')
                
                if freq_ghz:
                    cpu_detail += f" | Base: {freq_ghz:.2f}GHz"
                
                if max_freq_ghz and max_freq_ghz != freq_ghz:
                    cpu_detail += f" | Max: {max_freq_ghz:.2f}GHz"
                
                main_table.add_row(
                    "CPU Cores",
                    Text("INFO", style="bold blue"),
                    cpu_detail
                )
                
                # Hyperthreading and performance
                hyperthreading = cpu_info.get('hyperthreading', False)
                ht_style = "bold green" if hyperthreading else "bold yellow"
                perf_detail = f"Performance Class: {perf_class.replace('_', ' ').title()}"
                
                # Add current load if available
                current_load = cpu_info.get('current_load')
                
                if current_load is not None:
                    load_color = "bold green" if current_load < 50 else "bold yellow" if current_load < 80 else "bold red"
                    perf_detail += f" | Load: [{load_color}]{current_load:.1f}%[/{load_color}]"
                
                main_table.add_row(
                    "CPU Performance",
                    Text("HT-ON" if hyperthreading else "HT-OFF", style=ht_style),
                    perf_detail
                )
            
            # Memory Information with usage metrics
            memory_info = capabilities.get('memory', {})
            
            if memory_info:
                total_ram = memory_info.get('total_gb', 'Unknown')
                perf_class = memory_info.get('performance_class', 'unknown')
                mem_detail = f"{total_ram}GB Total | Performance: {perf_class.replace('_', ' ').title()}"
                
                # Add swap information
                swap_gb = memory_info.get('swap_gb', 0)
                has_swap = memory_info.get('has_swap', False)
                
                if has_swap and swap_gb > 0:
                    mem_detail += f" | Swap: {swap_gb:.1f}GB"
                
                main_table.add_row(
                    "System Memory",
                    Text("INFO", style="bold blue"),
                    mem_detail
                )
                
                # Memory usage if available
                usage_percent = memory_info.get('usage_percent')
                
                if usage_percent is not None:
                    used_gb = memory_info.get('used_gb', 0)
                    available_gb = memory_info.get('available_gb', 0)
                    usage_color = "green" if usage_percent < 70 else "yellow" if usage_percent < 85 else "red"
                    usage_detail = f"Used: {used_gb:.1f}GB | Available: {available_gb:.1f}GB"
                    
                    # Add swap usage if available
                    swap_usage = memory_info.get('swap_usage_percent', 0)
                    
                    if swap_usage > 0:
                        swap_color = "yellow" if swap_usage < 50 else "red"
                        usage_detail += f" | Swap: [{swap_color}]{swap_usage:.1f}%[/{swap_color}]"
                    
                    main_table.add_row(
                        "Memory Usage",
                        Text(f"{usage_percent:.1f}%", style=f"bold {usage_color}"),
                        usage_detail
                    )
            
            # GPU Information with metrics
            gpu_info = capabilities.get('gpu', {})
            
            if gpu_info:
                gpu_available = gpu_info.get('available', False)
                gpu_count = gpu_info.get('count', 0)
                total_memory = gpu_info.get('total_memory_gb', 0)
                perf_class = gpu_info.get('performance_class', 'none')
                gpu_style = "bold green" if gpu_available else "bold red"
                gpu_detail = ""
                
                if gpu_available:
                    gpu_detail = f"Count: {gpu_count} | Memory: {total_memory:.1f}GB | Class: {perf_class.replace('_', ' ').title()}"
                    # Add version info
                    cuda_ver = gpu_info.get('cuda_version')
                    cudnn_ver = gpu_info.get('cudnn_version')
                    
                    if cuda_ver:
                        gpu_detail += f" | CUDA: {cuda_ver}"
                    
                    if cudnn_ver:
                        gpu_detail += f" | cuDNN: {cudnn_ver}"
                
                else:
                    gpu_detail = "No CUDA-capable devices detected"
                
                main_table.add_row(
                    "CUDA/GPU Support",
                    Text("AVAILABLE" if gpu_available else "UNAVAILABLE", style=gpu_style),
                    gpu_detail
                )
                
                # Individual GPU details if available
                devices = gpu_info.get('devices', [])
                
                if devices and len(devices) <= 3:  # Show details for up to 3 GPUs
                    for i, device in enumerate(devices):
                        gpu_name = device.get('name', 'Unknown')
                        gpu_memory = device.get('memory_gb', 0)
                        compute_cap = device.get('compute_capability', 'Unknown')
                        device_detail = f"{gpu_name} | {gpu_memory:.1f}GB | Compute: {compute_cap}"
                        
                        # Add utilization if available
                        utilization = device.get('utilization_percent', None)
                        
                        if utilization is not None:
                            util_color = "bold green" if utilization < 50 else "bold yellow" if utilization < 80 else "bold red"
                            device_detail += f" | Usage: [{util_color}]{utilization:.1f}%[/{util_color}]"
                        
                        main_table.add_row(
                            f"GPU {device.get('index', i)}",
                            Text("INFO", style="bold blue"),
                            device_detail
                        )
            
            # Storage Information with performance metrics
            storage_info = capabilities.get('storage', {})
            
            if storage_info:
                total_gb = storage_info.get('total_gb', 0)
                free_gb = storage_info.get('free_gb', 0)
                used_gb = storage_info.get('used_gb', 0)
                usage_percent = storage_info.get('usage_percent', 0)
                perf_class = storage_info.get('performance_class', 'unknown')
                storage_detail = f"Total: {total_gb:.1f}GB | Free: {free_gb:.1f}GB"
                storage_detail += f" | Performance: {perf_class.replace('_', ' ').title()}"
                usage_color = "green" if usage_percent < 70 else "yellow" if usage_percent < 85 else "red"
                
                main_table.add_row(
                    "Storage Capacity",
                    Text("INFO", style="bold blue"),
                    storage_detail
                )
                
                main_table.add_row(
                    "Storage Usage",
                    Text(f"{usage_percent:.1f}%", style=f"bold {usage_color}"),
                    f"Used: {used_gb:.1f}GB | Available: {free_gb:.1f}GB"
                )
        else:
            # Fallback hardware information
            main_table.add_row(
                Text("HARDWARE ANALYSIS", style="bold white on blue"),
                "",
                ""
            )
            
            if 'hardware' in system_info:
                hw = system_info['hardware']
                
                # Device information
                device = hw.get('device', 'Unknown')
                
                main_table.add_row(
                    "Primary Device",
                    Text("INFO", style="bold blue"),
                    device.upper()
                )
                
                # CPU information
                cpu_info = hw.get('cpu_cores', {})
                
                if cpu_info:
                    logical_cores = cpu_info.get('logical_cores', os.cpu_count() or 'Unknown')
                    physical_cores = cpu_info.get('physical_cores', 'Unknown')
                    cpu_detail = f"Logical: {logical_cores}"
                    if physical_cores != 'Unknown' and physical_cores != logical_cores:
                        cpu_detail += f" | Physical: {physical_cores}"
                    
                    main_table.add_row(
                        "CPU Cores",
                        Text("INFO", style="bold blue"),
                        cpu_detail
                    )
                
                # CUDA information
                if 'cuda' in hw:
                    cuda_info = hw['cuda']
                    cuda_available = cuda_info.get('available', False)
                    cuda_style = "bold green" if cuda_available else "bold red"
                    cuda_detail = ""
                    
                    if cuda_available:
                        gpu_count = cuda_info.get('gpu_count', 0)
                        cuda_detail = f"GPU Count: {gpu_count}"
                        # Add basic GPU info if available
                        gpus = cuda_info.get('gpus', [])
                        
                        if gpus and len(gpus) > 0:
                            total_memory = sum(gpu.get('memory_gb', 0) for gpu in gpus)
                            cuda_detail += f" | Total Memory: {total_memory:.1f}GB"
                    
                    else:
                        cuda_detail = "No CUDA support detected"
                    
                    main_table.add_row(
                        "CUDA Support",
                        Text("YES" if cuda_available else "NO", style=cuda_style),
                        cuda_detail
                    )
                
                # Memory information
                ram_info = hw.get('system_ram', {})
                
                if ram_info and ram_info.get('available'):
                    total_ram = ram_info.get('ram_total_gb', 0)
                    main_table.add_row(
                        "System Memory",
                        Text("INFO", style="bold blue"),
                        f"{total_ram:.1f}GB Total"
                    )
            
            else:
                # Basic fallback using system calls
                cpu_cores = os.cpu_count() or 'Unknown'
                
                main_table.add_row(
                    "CPU Cores",
                    Text("INFO", style="bold blue"),
                    str(cpu_cores)
                )
                
                # Basic CUDA check
                cuda_available = 'torch' in globals() and torch.cuda.is_available()
                cuda_style = "bold green" if cuda_available else "bold red"
                cuda_detail = ""
                
                if cuda_available:
                    gpu_count = torch.cuda.device_count()
                    cuda_detail = f"GPU Count: {gpu_count}"
                
                main_table.add_row(
                    "CUDA Support",
                    Text("YES" if cuda_available else "NO", style=cuda_style),
                    cuda_detail
                )
        
        # CURRENT CONFIGURATION
        if config:
            main_table.add_row(
                Text("CURRENT CONFIGURATION", style="bold white on blue"),
                "",
                ""
            )
            
            # Configuration metadata
            metadata = config.get('metadata', {})
            
            if metadata:
                config_version = metadata.get('config_version', 'Unknown')
                config_type = metadata.get('config_type', 'Unknown')
                
                main_table.add_row(
                    "Config Version",
                    Text("INFO", style="bold blue"),
                    f"v{config_version} ({config_type})"
                )
                
                # Preset information
                if preset_name:
                    preset_style = "bold green" if preset_name in ['Default', 'Performance', 'Stability'] else "bold yellow"
                    
                    # Add preset compatibility info
                    compatibility = metadata.get('compatibility', [])
                    preset_detail = preset_name
                    
                    if compatibility:
                        preset_detail += f" | Compatible: {', '.join(compatibility[:2])}"
                        if len(compatibility) > 2:
                            preset_detail += "..."
                    
                    main_table.add_row(
                        "Active Preset",
                        Text(preset_name.upper(), style=preset_style),
                        preset_detail
                    )
                
                # Creation/modification time
                created = metadata.get('created')
                last_modified = metadata.get('last_modified') or metadata.get('modified')
                
                if created:
                    created_display = created[:19].replace('T', ' ')
                    time_detail = f"Created: {created_display}"
                    
                    if last_modified and last_modified != created:
                        modified_display = last_modified[:19].replace('T', ' ')
                        time_detail += f" | Modified: {modified_display}"
                    
                    main_table.add_row(
                        "Timestamps",
                        Text("INFO", style="bold blue"),
                        time_detail
                    )
            
            # Training configuration
            training_config = config.get('training', {})
            
            if training_config:
                # Core training parameters
                epochs = training_config.get('epochs', 'Unknown')
                batch_size = training_config.get('batch_size', 'Unknown')
                learning_rate = training_config.get('learning_rate', 'Unknown')
                optimizer = training_config.get('optimizer', 'Unknown')
                train_detail = f"Epochs: {epochs} | Batch: {batch_size} | LR: {learning_rate} | Optimizer: {optimizer}"
                
                main_table.add_row(
                    "Training Config",
                    Text("INFO", style="bold blue"),
                    train_detail
                )
                
                # Advanced training features
                features = []
                
                if training_config.get('mixed_precision', False):
                    features.append("[green]Mixed Precision[/green]")
                
                if training_config.get('gradient_accumulation_steps', 1) > 1:
                    steps = training_config.get('gradient_accumulation_steps')
                    features.append(f"[blue]Grad Accum: {steps}[/blue]")
                
                if training_config.get('scheduler'):
                    scheduler = training_config.get('scheduler')
                    features.append(f"[yellow]Scheduler: {scheduler}[/yellow]")
                
                if features:
                    main_table.add_row(
                        "Training Features",
                        Text("INFO", style="bold blue"),
                        " | ".join(features)
                    )
            
            # Model configuration
            model_config = config.get('model', {})
            
            if model_config:
                model_type = model_config.get('model_type', 'Unknown')
                encoding_dim = model_config.get('encoding_dim', 'Unknown')
                
                # Model type styling
                model_colors = {
                    'SimpleAutoencoder': "bold blue",
                    'EnhancedAutoencoder': "bold green", 
                    'AutoencoderEnsemble': "bold magenta"
                }
                
                model_style = model_colors.get(model_type, "bold blue")
                model_detail = f"Encoding Dim: {encoding_dim}"
                
                # Add architecture info
                hidden_dims = model_config.get('hidden_dims', [])
                
                if isinstance(hidden_dims, list) and hidden_dims:
                    arch_summary = f"Architecture: {len(hidden_dims)} layers"
                    
                    if len(hidden_dims) <= 4:
                        arch_summary += f" [{', '.join(map(str, hidden_dims))}]"
                    
                    else:
                        arch_summary += f" [{', '.join(map(str, hidden_dims[:2]))}...{hidden_dims[-1]}]"
                    
                    model_detail += f" | {arch_summary}"
                
                main_table.add_row(
                    "Model Architecture",
                    Text(model_type, style=model_style),
                    model_detail
                )
                
                # Model features
                features = []
                
                if model_config.get('use_batch_norm', False):
                    features.append("[green]Batch Norm[/green]")
                
                if model_config.get('use_layer_norm', False):
                    features.append("[green]Layer Norm[/green]")
                
                if model_config.get('skip_connection', False):
                    features.append("[blue]Skip Connections[/blue]")
                
                if model_config.get('residual_blocks', False):
                    features.append("[blue]Residual Blocks[/blue]")
                
                if model_config.get('use_attention', False):
                    features.append("[magenta]Attention[/magenta]")
                
                # Ensemble-specific info
                if model_type == 'AutoencoderEnsemble':
                    num_models = model_config.get('num_models', 'Unknown')
                    diversity_factor = model_config.get('diversity_factor', 'Unknown')
                    features.insert(0, f"[magenta]Ensemble: {num_models} models (diversity: {diversity_factor})[/magenta]")
                
                if features:
                    main_table.add_row(
                        "Model Features",
                        Text("INFO", style="bold blue"),
                        " | ".join(features)
                    )
            
            # Security and data configuration summary
            security_config = config.get('security', {})
            data_config = config.get('data', {})
            
            if security_config or data_config:
                summary_parts = []
                
                if security_config:
                    percentile = security_config.get('percentile', 'Unknown')
                    threshold = security_config.get('attack_threshold', 'Unknown')
                    summary_parts.append(f"Security: {percentile}th percentile, threshold {threshold}")
                
                if data_config:
                    normal_samples = data_config.get('normal_samples', 'Unknown')
                    attack_samples = data_config.get('attack_samples', 'Unknown')
                    features = data_config.get('features', 'Unknown')
                    summary_parts.append(f"Data: {normal_samples}+{attack_samples} samples, {features} features")
                
                if summary_parts:
                    main_table.add_row(
                        "Security & Data",
                        Text("INFO", style="bold blue"),
                        " | ".join(summary_parts)
                    )
        
        # Print the main table
        console.print(main_table)
        console.print()
        
        # SYSTEM STATUS & HEALTH
        status_table = Table(
            title="[bold yellow]SYSTEM STATUS & HEALTH[/bold yellow]",
            box=box.ROUNDED,
            header_style="bold yellow",
            border_style="white",
            title_style="bold yellow",
            title_justify="left",
            show_lines=True,
            expand=True,
            width=min(120, console.width - 4)
        )
        
        status_table.add_column("Component", style="bold cyan", width=25)
        status_table.add_column("Status", width=12, justify="center")
        status_table.add_column("Details", style="bold", min_width=50, max_width=80)
        
        # Directory check
        try:
            required_dirs = [DEFAULT_MODEL_DIR, LOG_DIR, TB_DIR, CONFIG_DIR]
            dirs_status = [(d, d.exists(), os.access(d, os.W_OK) if d.exists() else False) for d in required_dirs]
            dirs_ok = all(exists for _, exists, _ in dirs_status)
            dirs_writable = all(writable for _, exists, writable in dirs_status if exists)
            
            if dirs_ok and dirs_writable:
                status_style = "bold green"
                status_text = "OK"
                details = "All directories present and writable"
            
            elif dirs_ok:
                status_style = "bold yellow"
                status_text = "WARN"
                details = "Directories exist but some may not be writable"
            
            else:
                status_style = "bold red"
                status_text = "FAIL"
                missing_dirs = [str(d.name) for d, exists, _ in dirs_status if not exists]
                details = f"Missing: {', '.join(missing_dirs[:3])}"
                
                if len(missing_dirs) > 3:
                    details += f" (+{len(missing_dirs)-3} more)"
            
            status_table.add_row(
                "[bold cyan]Directory Structure[/bold cyan]",
                Text(status_text, style=status_style),
                details, style=status_style
            )
        
        except Exception as e:
            status_style = "bold red"
            status_text = "ERROR"
            details = f"Check failed: {str(e)[:50]}..."
            details_style = "bold red"
            
            status_table.add_row(
                "[bold cyan]Directory Structure[/bold cyan]",
                Text(status_text, style=status_style),
                details, style=details_style
            )
        
        # Model variants check
        try:
            if 'MODEL_VARIANTS' in globals() and MODEL_VARIANTS:
                variants_count = len(MODEL_VARIANTS)
                variants_names = list(MODEL_VARIANTS.keys())
                variants_style = "bold green" if variants_count > 0 else "bold yellow"
                status_text = "OK" if variants_count > 0 else "WARN"
                details_style = "bold green" if status_text == "OK" else "bold yellow"
                details = f"{variants_count} variants: {', '.join(variants_names[:3])}"
                
                if len(variants_names) > 3:
                    details += "..."
                
                status_table.add_row(
                    "[bold cyan]Model Variants[/bold cyan]",
                    Text(status_text, style=variants_style),
                    details, style=details_style
                )
            
            else:
                status_table.add_row(
                    "[bold cyan]Model Variants[/bold cyan]", 
                    Text("INIT", style="bold yellow"),
                    "Not initialized or empty", style="bold yellow"
                )
        
        except Exception as e:
            status_style = "bold red"
            status_text = "ERROR"
            details = f"Check failed: {str(e)[:50]}..."
            details_style = "bold red"
            
            status_table.add_row(
                "Model Variants",
                Text(status_text, style=status_style),
                details, style=details_style
            )
        
        # Configuration validation using the config from show_banner
        if config:
            try:
                # Use the validate_config function
                is_valid, errors, warnings = validate_config(config, strict=False)
                
                if is_valid:
                    status_style = "bold green"
                    status_text = "VALID"
                    details = f"Configuration validated successfully"
                    
                    if warnings:
                        details += f" ({len(warnings)} warnings)"
                
                elif errors:
                    status_style = "bold red" 
                    status_text = "INVALID"
                    details = f"{len(errors)} errors found"
                    
                    if warnings:
                        details += f", {len(warnings)} warnings"
                
                else:
                    status_style = "bold yellow"
                    status_text = "WARN"
                    details = f"{len(warnings)} warnings found"
                
                status_table.add_row(
                    "[bold cyan]Configuration[/bold cyan]",
                    Text(status_text, style=status_style),
                    details, style=status_style
                )
            
            except Exception as e:
                status_style = "bold red"
                status_text = "ERROR"
                details = f"Validation failed: {str(e)[:50]}..."
                details_style = "bold red"
                
                status_table.add_row(
                    "[bold cyan]Configuration[/bold cyan]",
                    Text(status_text, style=status_style),
                    details, style=details_style
                )
        
        else:
            status_table.add_row(
                "[bold cyan]Configuration[/bold cyan]",
                Text("MISSING", style="bold yellow"),
                "No configuration loaded", style="bold yellow"
            )
        
        # Memory status if available
        if analysis_available and 'collection_metadata' in system_info:
            metadata = system_info['collection_metadata']
            memory_impact = metadata.get('memory_impact', {})
            
            if memory_impact:
                rss_delta = memory_impact.get('rss_delta_mb', 0)
                system_delta = memory_impact.get('system_delta_gb', 0)
                
                if abs(rss_delta) < 10 and abs(system_delta) < 0.1:
                    mem_style = "bold green"
                    mem_status = "STABLE"
                    mem_details = f"Memory stable (±{abs(rss_delta):.1f}MB process, ±{abs(system_delta*1024):.0f}MB system)"
                
                elif rss_delta > 50 or system_delta > 0.5:
                    mem_style = "bold yellow"
                    mem_status = "HIGH"
                    mem_details = f"Memory usage increased ({rss_delta:+.1f}MB process, {system_delta*1024:+.0f}MB system)"
                
                else:
                    mem_style = "bold blue"
                    mem_status = "NORMAL"
                    mem_details = f"Memory usage: {rss_delta:+.1f}MB process, {system_delta*1024:+.0f}MB system"
                
                status_table.add_row(
                    "[bold cyan] Impact[/bold cyan]",
                    Text(mem_status, style=mem_style),
                    mem_details, style=mem_style
                )
        
        # Optional dependencies status
        if 'optional_dependencies' in system_info:
            opt_deps = system_info['optional_dependencies']
            enabled_count = opt_deps.get('enabled_features', 0)
            total_count = opt_deps.get('feature_count', 0)
            availability_score = opt_deps.get('feature_availability_score', 0)
            
            if availability_score > 80:
                opt_style = "bold green"
                opt_status = "FULL"
            
            elif availability_score > 60:
                opt_style = "bold blue"
                opt_status = "GOOD"  
            
            elif availability_score > 40:
                opt_style = "bold yellow"
                opt_status = "LIMITED"
            
            else:
                opt_style = "bold red"
                opt_status = "MINIMAL"
            
            opt_details = f"{enabled_count}/{total_count} features available ({availability_score:.0f}%)"
            
            status_table.add_row(
                "[bold cyan]Optional Features[/bold cyan]",
                Text(opt_status, style=opt_style),
                opt_details, style=opt_style
            )
        
        # Overall system status
        overall_status = "HEALTHY"
        overall_style = "bold green"
        overall_details = ""
        
        # Collect status information for overall assessment
        status_factors = []
        
        if analysis_available and 'hardware_analysis' in system_info:
            hw_health = system_info['hardware_analysis'].get('overall_health', 'unknown')
            perf_score = system_info['hardware_analysis'].get('performance_score', 0)
            status_factors.append(('hardware', hw_health, perf_score))
            
            if hw_health == 'degraded':
                if overall_status == "HEALTHY":
                    overall_status = "DEGRADED"
                    overall_style = "bold yellow"
            
            elif hw_health == 'critical':
                overall_status = "CRITICAL"
                overall_style = "bold red"
        
        if analysis_available and 'package_analysis' in system_info:
            env_health = system_info['package_analysis'].get('environment_health', {}).get('overall_status', 'unknown')
            compat_score = system_info['package_analysis'].get('environment_health', {}).get('compatibility_score', 0)
            status_factors.append(('packages', env_health, compat_score))
            
            if env_health == 'degraded' and overall_status == "HEALTHY":
                overall_status = "NEEDS ATTENTION"
                overall_style = "bold yellow"
        
        if not analysis_available or not config:
            
            if overall_status in ["HEALTHY", "DEGRADED"]:
                overall_status = "ISSUES DETECTED"
                overall_style = "bold red"
        
        if not collection_successful:
            overall_status = "DATA COLLECTION ISSUES"
            overall_style = "bold red"
        
        # Build overall details
        if status_factors:
            factor_details = []
            for factor_type, health, score in status_factors:
                
                if score > 0:
                    factor_details.append(f"{factor_type}: {health} ({score:.0f}%)")
                
                else:
                    factor_details.append(f"{factor_type}: {health}")
            
            overall_details = " | ".join(factor_details)
        
        status_table.add_row(
            Text("OVERALL STATUS", style="bold white on blue"),
            Text(overall_status, style=overall_style),
            overall_details, style=overall_style
        )
        
        # Print the status table
        console.print(status_table)
        console.print()
        
        # WARNINGS & RECOMMENDATIONS
        if analysis_available and 'detailed_analysis' in system_info:
            analysis = system_info['detailed_analysis']
            
            # Collect all types of recommendations
            all_warnings = []
            
            all_warnings.extend((item, 'System Recommendation', 'HIGH') for item in analysis.get('system_recommendations', []))
            all_warnings.extend((item, 'Compatibility Issue', 'HIGH') for item in analysis.get('compatibility_issues', []))
            all_warnings.extend((item, 'Resource Warning', 'MEDIUM') for item in analysis.get('resource_warnings', []))
            all_warnings.extend((item, 'Performance Optimization', 'MEDIUM') for item in analysis.get('performance_optimizations', []))
            all_warnings.extend((item, 'Configuration Suggestion', 'LOW') for item in analysis.get('configuration_suggestions', []))
            
            if all_warnings:
                recommendations_table = Table(
                    title="[bold yellow]SYSTEM ANALYSIS & RECOMMENDATIONS[/bold yellow]",
                    #box=box.ROUNDED,
                    box=box.DOUBLE_EDGE,
                    header_style="bold cyan",
                    border_style="cyan", 
                    title_style="bold yellow",
                    title_justify="left",
                    show_lines=True,
                    expand=True,
                    width=min(120, console.width - 4)
                )
                
                recommendations_table.add_column("Type", style="bold cyan", width=18)
                recommendations_table.add_column("Priority", width=10, justify="center")
                recommendations_table.add_column("Recommendation", style="bold", min_width=65, max_width=85)
                
                # Sort by priority (HIGH -> MEDIUM -> LOW)
                priority_order = {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}
                all_warnings.sort(key=lambda x: priority_order.get(x[2], 3))
                
                # Add recommendations with styling
                for message, rec_type, priority in all_warnings[:10]:  # Limit to top 10
                    priority_colors = {
                        'HIGH': "bold red",
                        'MEDIUM': "bold yellow", 
                        'LOW': "bold blue"
                    }
                    
                    priority_style = priority_colors.get(priority, "bold blue")
                    message_style = "bold red" if priority == 'HIGH' else "bold yellow" if priority == 'MEDIUM' else "bold blue"
                    
                    # Truncate very long messages
                    display_message = message
                    
                    if len(display_message) > 85:
                        display_message = display_message[:82] + "..."
                    
                    recommendations_table.add_row(
                        rec_type,
                        Text(priority, style=priority_style),
                        display_message, style=message_style
                    )
                
                # Show total count if there are more recommendations
                if len(all_warnings) > 10:
                    recommendations_table.add_row(
                        Text("...", style="bold"),
                        Text("...", style="bold"),
                        Text(f"... and {len(all_warnings) - 10} more recommendations", style="bold italic")
                    )
                
                console.print(recommendations_table)
        
        # Collection errors and warnings if any
        if analysis_available and 'collection_metadata' in system_info:
            metadata = system_info['collection_metadata']
            errors = metadata.get('errors', [])
            warnings = metadata.get('warnings', [])
            
            if errors or warnings:
                issues_table = Table(
                    title="[bold red]COLLECTION ISSUES[/bold red]" if errors else "[bold yellow]COLLECTION WARNINGS[/bold yellow]",
                    box=box.ROUNDED,
                    header_style="bold white",
                    border_style="red" if errors else "yellow",
                    title_style="bold red" if errors else "bold yellow",
                    title_justify="left", 
                    show_lines=True,
                    expand=True,
                    width=min(120, console.width - 4)
                )
                
                issues_table.add_column("Type", style="bold cyan", width=12)
                issues_table.add_column("Issue", style="bold", min_width=80, max_width=100)
                
                for error in errors[:5]:  # Show up to 5 errors
                    display_error = error
                    
                    if len(display_error) > 100:
                        display_error = display_error[:97] + "..."
                    
                    issues_table.add_row(
                        Text("ERROR", style="bold red"),
                        display_error, style="bold red"
                    )
                
                for warning in warnings[:5]:  # Show up to 5 warnings  
                    display_warning = warning
                    
                    if len(display_warning) > 100:
                        display_warning = display_warning[:97] + "..."
                    
                    issues_table.add_row(
                        Text("WARNING", style="bold yellow"),
                        display_warning, style="bold yellow"
                    )
                
                if len(errors) > 5 or len(warnings) > 5:
                    total_remaining = (len(errors) - 5 if len(errors) > 5 else 0) + (len(warnings) - 5 if len(warnings) > 5 else 0)
                    
                    issues_table.add_row(
                        Text("...", style="bold"),
                        Text(f"... and {total_remaining} more issues", style="bold italic")
                    )
                
                console.print(issues_table)
    
    except Exception as e:
        error_msg = f"CRITICAL ERROR in show_system_info(): {e}"
        console.print(f"[bold red]{error_msg}[/bold red]")
        console.print("This indicates a serious system issue that should be investigated.")
        logger.error(error_msg, exc_info=True)
        
        # Last resort basic info with error handling
        try:
            basic_table = Table(
                title="[bold red]BASIC SYSTEM INFO (EMERGENCY FALLBACK)[/bold red]",
                box=box.SIMPLE,
                show_header=False,
                width=min(80, console.width - 4),
                title_style="bold red"
            )
            
            basic_table.add_column("Property", style="bold yellow")
            basic_table.add_column("Value", style="white")
            
            # Basic system information
            try:
                basic_table.add_row("Operating System", f"{platform.system()} {platform.release()}")
            except:
                basic_table.add_row("Operating System", "Unknown")
            
            try:
                basic_table.add_row("Python Version", platform.python_version())
            except:
                basic_table.add_row("Python Version", "Unknown")
            
            try:
                basic_table.add_row("CPU Cores", str(os.cpu_count() or 'Unknown'))
            except:
                basic_table.add_row("CPU Cores", "Unknown")
            
            try:
                basic_table.add_row("Architecture", platform.machine())
            except:
                basic_table.add_row("Architecture", "Unknown")
            
            # PyTorch information if available
            if 'torch' in globals():
                try:
                    pytorch_version = torch.__version__ if hasattr(torch, '__version__') else 'Unknown'
                    basic_table.add_row("PyTorch Version", pytorch_version)
                except:
                    basic_table.add_row("PyTorch Version", "Unknown")
                
                try:
                    cuda_status = "Available" if torch.cuda.is_available() else "Not Available"
                    if torch.cuda.is_available():
                        cuda_status += f" ({torch.cuda.device_count()} devices)"
                    basic_table.add_row("CUDA Support", cuda_status)
                except:
                    basic_table.add_row("CUDA Support", "Unknown")
                
            basic_table.add_row("Error Context", str(e)[:50] + "..." if len(str(e)) > 50 else str(e))
            
            console.print(basic_table)
            
        except Exception as basic_error:
            console.print(f"[bold red]Even basic system info failed: {basic_error}[/bold red]")
            console.print(f"[bold red]Original error: {e}[/bold red]")
            console.print("[bold red]System is in critical failure state[/bold red]")



# Helper functions for configuration management
def select_preset_config():
    """Preset configuration selection with error handling and context integration."""
    try:
        # Clear screen and show banner with config retrieval
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract current context for display
        current_preset = "Custom/Default"
        current_model = "Unknown"
        
        # Extract current preset with multiple fallbacks
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            current_preset = presets_section.get("current_preset", "Custom/Default")
        
        if current_preset in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                current_preset = metadata.get("preset_used", "Custom/Default")
        
        if current_preset in ["Custom/Default", None, ""]:
            current_preset = config.get("_preset_name", "Custom/Default")
        
        # Extract current model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            current_model = model_section.get("model_type", "Unknown")
        
        # Clean up preset name display
        if current_preset in ["Custom/Default", None, "", "none"]:
            current_preset = "Custom/Default"
        elif isinstance(current_preset, str):
            current_preset = current_preset.title()
        
        # Menu header with current context
        #print(Fore.CYAN + Style.BRIGHT + "\n" + "="*40)
        print(Fore.YELLOW + Style.BRIGHT + "PRESET CONFIGURATION SELECTION")
        print(Fore.CYAN + Style.BRIGHT + "="*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Current Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Active Preset: " + Fore.YELLOW + Style.BRIGHT + f"{current_preset}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Current Model: " + Fore.YELLOW + Style.BRIGHT + f"{current_model}")
        
        # Check if presets are available
        if not PRESET_CONFIGS:
            message = (
                f"No preset configurations available.\n"
                f"This could be due to:\n"
                f"- Missing preset configuration files\n"
                f"- Corrupted preset definitions\n"
                f"- Installation issues\n\n"
                f"Please check the configuration directory and logs."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="NO PRESETS AVAILABLE",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            return
        
        # Display available presets table
        print(Fore.YELLOW + Style.BRIGHT + "\nAvailable Preset Configurations:")
        
        # Create main table for preset selection
        preset_table = Table(
            box=box.ROUNDED,
            header_style="bold white",
            border_style="white",
            show_header=True,
            show_lines=True,
            width=min(140, console.width - 4)
        )
        
        # Define columns
        preset_table.add_column("#", style="bold cyan", width=5, justify="center")
        preset_table.add_column("Preset Name", style="bold green", width=20)
        preset_table.add_column("Description", style="bold", width=40)
        preset_table.add_column("Model", style="bold blue", width=25)
        preset_table.add_column("Training", style="bold yellow", width=30)
        
        presets = list(PRESET_CONFIGS.items())
        for i, (name, preset) in enumerate(presets, 1):
            training_config = preset.get('training', {})
            model_config = preset.get('model', {})
            metadata_config = preset.get('metadata', {})
            
            # Highlight current preset
            name_style = "bold green" if name == current_preset else "bold white"
            
            preset_table.add_row(
                str(i),
                Text(name.title(), style=name_style),
                metadata_config.get('description', 'No description')[:50] + "..." if len(metadata_config.get('description', '')) > 50 else metadata_config.get('description', 'No description'),
                model_config.get('model_type', 'N/A'),
                f"Epochs:{training_config.get('epochs', 'N/A')} Batch:{training_config.get('batch_size', 'N/A')}"
            )
        
        console.print(preset_table)
        
        # Add detailed information panels for each preset
        print(Fore.YELLOW + Style.BRIGHT + "\nPreset Details:")
        for i, (name, preset) in enumerate(presets, 1):
            training_config = preset.get('training', {})
            model_config = preset.get('model', {})
            security_config = preset.get('security', {})
            metadata_config = preset.get('metadata', {})
            data_config = preset.get('data', {})
            
            description = metadata_config.get('description', 'No description')
            if len(description) > 50:
                description = description[:50] + "..."
            
            # Highlight current preset in details
            border_style = "green" if name == current_preset else "cyan"
            title_style = "bold green" if name == current_preset else "bold white"
            
            detail_panel = Panel.fit(
                f"[bold]Description:[/bold] {description}\n"
                f"[bold]Training:[/bold] {training_config.get('epochs', 'N/A')} epochs, "
                f"Batch: {training_config.get('batch_size', 'N/A')}, "
                f"LR: {training_config.get('learning_rate', 'N/A')}\n"
                f"[bold]Model:[/bold] {model_config.get('model_type', 'N/A')}, "
                f"Encoding: {model_config.get('encoding_dim', 'N/A')}, "
                f"Hidden: {len(model_config.get('hidden_dims', []))} layers\n"
                f"[bold]Data:[/bold] Samples: {data_config.get('normal_samples', 'N/A')}, "
                f"Path: {data_config.get('data_path', 'N/A')}\n"
                f"[bold]Security:[/bold] Threshold: {security_config.get('attack_threshold', 'N/A')}, "
                f"Percentile: {security_config.get('percentile', 'N/A')}%",
                title=f"[{i}] {name.title()}",
                style=title_style,
                border_style=border_style,
                padding=(1, 2)
            )
            console.print(detail_panel)
        
        # Selection options
        max_choice = len(PRESET_CONFIGS)
        print(Fore.YELLOW + Style.BRIGHT + f"\nSelection Options:")
        print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Choose preset (1-{max_choice})")
        print(Fore.YELLOW + Style.BRIGHT + f"  └─ " + Fore.RED + Style.BRIGHT + "0 to cancel")
        
        # Get user input with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect preset (1-{max_choice}) or 0 to cancel: ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSelection cancelled...")
                return
        
        if choice == "0":
            print(Fore.RED + Style.BRIGHT + "Preset selection cancelled.")
            return
        
        if choice.isdigit() and 1 <= int(choice) <= max_choice:
            preset_name = presets[int(choice)-1][0]
            
            try:
                # Use deepcopy to avoid modifying original
                preset_config = deepcopy(PRESET_CONFIGS[preset_name])
                preset_training_config = preset_config.get('training', {})
                preset_model_config = preset_config.get('model', {})
                preset_data_config = preset_config.get('data', {})
                preset_metadata_config = preset_config.get('metadata', {})
                
                description = preset_metadata_config.get('description', 'No description')
                if len(description) > 50:
                    description = description[:50] + "..."
                
                # Show confirmation pane
                confirm_panel = Panel.fit(
                    f"[bold]Selected Preset:[/bold] [green]{preset_name.title()}[/green]\n"
                    f"[bold]Description:[/bold] {description}\n"
                    f"[bold]Model Type:[/bold] {preset_model_config.get('model_type', 'N/A')}\n"
                    f"[bold]Training:[/bold] {preset_training_config.get('epochs', 'N/A')} epochs, "
                    f"Batch: {preset_training_config.get('batch_size', 'N/A')}, "
                    f"LR: {preset_training_config.get('learning_rate', 'N/A')}\n"
                    f"[bold]Data:[/bold] Samples: {preset_data_config.get('normal_samples', 'N/A')}, "
                    f"Path: {preset_data_config.get('data_path', 'N/A')}",
                    title="[bold]PRESET CONFIRMATION[/bold]",
                    style="bold",
                    border_style="green",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
                console.print(confirm_panel)
                
                # Confirmation prompt
                confirm = None
                while not confirm:
                    try:
                        confirm_input = input(Fore.YELLOW + Style.BRIGHT + "\nApply this configuration? (Y/n): ").lower().strip()
                        
                        if not confirm_input:
                            confirm = 'y'  # Default to yes
                        elif confirm_input in ('y', 'yes', 'n', 'no'):
                            confirm = confirm_input
                        else:
                            print(Fore.RED + Style.BRIGHT + "Please enter Y or N")
                            confirm = None
                            
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nConfirmation cancelled...")
                        return
                
                if confirm in ('y', 'yes'):
                    try:
                        # Get current configuration
                        current_config = get_current_config()
                        
                        # Create a deep copy to avoid modifying the original
                        merged_config = deepcopy(current_config)
                        
                        # Update the preset information BEFORE merging (matching your pattern)
                        if 'presets' not in merged_config:
                            merged_config['presets'] = {}
                        
                        # Set the current preset name
                        merged_config['presets']['current_preset'] = preset_name
                        merged_config['presets']['last_applied'] = datetime.now().isoformat()
                        
                        # Update metadata to reflect preset usage
                        if 'metadata' not in merged_config:
                            merged_config['metadata'] = {}
                        
                        merged_config['metadata']['preset_used'] = preset_name
                        merged_config['metadata']['modified'] = datetime.now().isoformat()
                        merged_config['metadata']['last_preset_change'] = datetime.now().isoformat()
                        
                        # Apply the preset configuration using deep_update
                        merged_config = deep_update(merged_config, preset_config)
                        
                        # Ensure preset information is preserved after merge
                        merged_config['presets']['current_preset'] = preset_name
                        merged_config['metadata']['preset_used'] = preset_name
                        
                        # Update global configuration variables
                        update_global_config(merged_config)
                        
                        # Save the updated configuration
                        try:
                            save_config(merged_config)
                            logger.info(f"Successfully applied and saved preset: {preset_name}")
                        except Exception as save_error:
                            logger.warning(f"Preset applied but save failed: {save_error}")
                            # Continue even if save fails - config is still updated in memory
                        
                        # Invalidate cache to ensure fresh config loading
                        invalidate_config_cache()
                        
                        # Verify the preset was actually applied
                        verification_config = get_current_config()
                        verification_preset = verification_config.get('presets', {}).get('current_preset')
                        
                        if verification_preset == preset_name:
                            success_panel = Panel.fit(
                                f"[bold green]✓ Successfully applied preset: [bold yellow]{preset_name.title()}[/bold yellow][/bold green]\n"
                                f"[bold green]Configuration updated with preset settings[/bold green]\n"
                                f"[bold green]Active preset: {verification_preset}[/bold green]",
                                title="[bold]SUCCESS[/bold]",
                                border_style="green",
                                style="bold green",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                            console.print(success_panel)
                            
                            # Show key changes for verification
                            preset_training = preset_config.get('training', {})
                            preset_model = preset_config.get('model', {})
                            preset_data = preset_config.get('data', {})
                            
                            if preset_training or preset_model:
                                changes_panel = Panel.fit(
                                    f"[bold]Key Configuration Changes:[/bold]\n"
                                    f"Model Type: {preset_model.get('model_type', 'N/A')}\n"
                                    f"Epochs: {preset_training.get('epochs', 'N/A')}\n"
                                    f"Batch Size: {preset_training.get('batch_size', 'N/A')}\n"
                                    f"Learning Rate: {preset_training.get('learning_rate', 'N/A')}\n"
                                    f"Encoding Dim: {preset_model.get('encoding_dim', 'N/A')}\n"
                                    f"Data Samples: {preset_data.get('normal_samples', 'N/A')}",
                                    title="[bold]APPLIED SETTINGS[/bold]",
                                    border_style="blue",
                                    style="bold blue",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                                console.print(changes_panel)
                        else:
                            error_panel = Panel.fit(
                                f"[bold red]Warning: Preset application may not have completed successfully[/bold red]\n"
                                f"Expected: {preset_name}, Current: {verification_preset}\n\n"
                                f"Please try again or check configuration files.",
                                title="[bold]WARNING[/bold]",
                                border_style="yellow",
                                style="bold yellow",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                            console.print(error_panel)
                            
                    except Exception as apply_error:
                        message = (
                            f"Failed to apply preset: {str(apply_error)}\n"
                            f"Context:\n"
                            f"- Selected Preset: {preset_name}\n"
                            f"- Current Model: {current_model}\n\n"
                            f"This could be due to:\n"
                            f"- Configuration file corruption\n"
                            f"- Permission issues\n"
                            f"- Invalid preset structure\n"
                            f"- System resource constraints"
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="PRESET APPLICATION ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        logger.error(f"Preset application failed: {apply_error}")
                else:
                    print(Fore.RED + Style.BRIGHT + "Configuration not applied.")
                    
            except Exception as selection_error:
                message = (
                    f"Error during preset selection: {str(selection_error)}\n"
                    f"Context:\n"
                    f"- Selected Option: {choice}\n"
                    f"- Preset Name: {preset_name if 'preset_name' in locals() else 'Unknown'}\n\n"
                    f"Please try selecting a different preset or check preset configuration."
                )
                console.print(
                    Panel.fit(
                        f"{message}",
                        title="SELECTION PROCESSING ERROR",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                logger.error(f"Preset selection processing failed: {selection_error}")
        else:
            message = (
                f"Invalid selection: '{choice}'\n"
                f"Please enter a number between 1 and {max_choice}\n"
                f"or 0 to cancel the selection."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="INVALID SELECTION",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
    except Exception as e:
        message = (
            f"Unexpected error in preset selection: {str(e)}\n"
            f"This could indicate:\n"
            f"- Preset configuration corruption\n"
            f"- System resource issues\n"
            f"- Dependency problems\n"
            f"- Configuration loading failures\n\n"
            f"Please check the logs for detailed information."
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="PRESET SELECTION ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        logger.error(f"Preset selection failed: {e}", exc_info=True)
    
    # Only continue if not exiting
    try:
        input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
    except (EOFError, KeyboardInterrupt):
        print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")

def show_current_config():
    """Display current configuration in formatted rich tables for each section."""
    config = None
    try:
        # clear screen and show banner
        console.clear()
        
        # Get configuration from show_banner function
        config = show_banner(return_config=True)
        
        console.print("\n[bold yellow]CURRENT CONFIGURATION[/bold yellow]\n")
        
        metadata = config.get('metadata', {})
        if metadata:
            meta_table = Table(
                title="[bold yellow]Metadata[/bold yellow]",
                title_justify="left",
                box=box.ROUNDED,
                header_style="bold cyan",
                border_style="blue",
                show_header=False,
                show_lines=True,
                width=min(85, console.width - 4)
            )
            
            meta_table.add_column("Property", style="bold green", width=25)
            meta_table.add_column("Value", style="bold", width=60)
            
            for key, value in metadata.items():
                if key != 'system':
                    if isinstance(value, dict):
                        meta_table.add_row(key, f"{len(value)} nested parameters")
                    elif isinstance(value, list):
                        meta_table.add_row(key, f"{len(value)} items")
                    else:
                        meta_table.add_row(key, str(value))
            
            console.print(meta_table)
            console.print()

        system_info = config.get('metadata', {}).get('system', {})
        if system_info:
            sys_table = Table(
                title="[bold yellow]System Information[/bold yellow]",
                title_justify="left",
                box=box.ROUNDED,
                header_style="bold cyan",
                border_style="green",
                show_header=False,
                show_lines=True,
                width=min(85, console.width - 4)
            )
            
            sys_table.add_column("Component", style="bold green", width=25)
            sys_table.add_column("Value", style="bold", width=60)
            
            for key, value in system_info.items():
                if not isinstance(value, (dict, list)):
                    sys_table.add_row(key.replace('_', ' ').title(), str(value))
            
            console.print(sys_table)
            console.print()

        sections = [
            ('training', 'Training Configuration', 'yellow'),
            ('model', 'Model Architecture', 'magenta'),
            ('security', 'Security Settings', 'red'),
            ('data', 'Data Configuration', 'blue'),
            ('monitoring', 'Monitoring Settings', 'cyan'),
            ('hardware', 'Hardware Requirements', 'green'),
            ('presets', 'Preset Information', 'bright_blue'),
            ('hyperparameter_optimization', 'HPO Settings', 'bright_magenta'),
            ('validation', 'Validation Settings', 'bright_yellow'),
            ('experimental', 'Experimental Features', 'bright_red')
        ]
        
        for section_key, section_title, color in sections:
            if section_key in config:
                section_data = config[section_key]
                
                section_table = Table(
                    title=f"[bold yellow]{section_title}[/bold yellow]",
                    title_justify="left",
                    box=box.ROUNDED,
                    header_style=f"bold {color}",
                    border_style=color,
                    show_header=True,
                    show_lines=True,
                    width=min(85, console.width - 4)
                )
                
                section_table.add_column("Parameter", style="bold green", width=25)
                section_table.add_column("Value", style="bold", width=60)
                
                if isinstance(section_data, dict):
                    for key, value in section_data.items():
                        formatted_value = str(value)
                        
                        if isinstance(value, dict):
                            if key == 'synthetic_generation':
                                nested_values = []
                                for k, v in value.items():
                                    nested_values.append(f"{k}={v}")
                                formatted_value = ", ".join(nested_values) if nested_values else "empty"
                            elif key == 'preprocessing':
                                preproc_values = []
                                for k, v in value.items():
                                    preproc_values.append(f"{k}={v}")
                                formatted_value = ", ".join(preproc_values) if preproc_values else "none"
                            elif key == 'performance_optimization':
                                perf_values = []
                                for k, v in value.items():
                                    perf_values.append(f"{k}={v}")
                                formatted_value = ", ".join(perf_values) if perf_values else "none"
                            elif key == 'minimum_system_requirements' or key == 'optimal_system_requirements':
                                req_values = []
                                for k, v in value.items():
                                    req_values.append(f"{k}={v}")
                                formatted_value = ", ".join(req_values) if req_values else "none"
                            elif key == 'tensorboard':
                                tb_values = []
                                for k, v in value.items():
                                    tb_values.append(f"{k}={v}")
                                formatted_value = ", ".join(tb_values) if tb_values else "disabled"
                            elif key == 'scheduler_params':
                                if value:
                                    sched_values = []
                                    for k, v in value.items():
                                        sched_values.append(f"{k}={v}")
                                    formatted_value = ", ".join(sched_values)
                                else:
                                    formatted_value = "none"
                            else:
                                formatted_value = f"{len(value)} parameters"
                                
                        elif isinstance(value, list):
                            if key == 'hidden_dims':
                                formatted_value = f"[{', '.join(map(str, value))}] ({len(value)} layers)"
                            elif key == 'dropout_rates':
                                formatted_value = f"[{', '.join(map(str, value))}]"
                            elif key == 'available_presets':
                                formatted_value = f"{len(value)} presets: " + ", ".join(value[:3]) + ("..." if len(value) > 3 else "")
                            elif key == 'custom_presets_available':
                                formatted_value = f"{len(value)} custom presets" + (f": {', '.join(value[:3])}" if value else "")
                            elif key == 'compatibility':
                                formatted_value = ", ".join(value) if value else "none"
                            elif key == 'metrics_to_track':
                                formatted_value = ", ".join(value) if value else "none"
                            elif key == 'detection_methods':
                                formatted_value = ", ".join(value) if value else "none"
                            elif key == 'alert_levels':
                                formatted_value = ", ".join(value) if value else "none"
                            else:
                                formatted_value = f"[{len(value)} items]"
                        
                        elif isinstance(value, bool):
                            formatted_value = "enabled" if value else "disabled"
                        
                        elif isinstance(value, float):
                            if key == 'learning_rate' or key == 'weight_decay' or key == 'adam_eps':
                                formatted_value = f"{value:.6f}"
                            else:
                                formatted_value = f"{value:.3f}"
                        
                        elif value is None:
                            formatted_value = "none"
                        
                        section_table.add_row(key.replace('_', ' ').title(), formatted_value)
                else:
                    section_table.add_row("Configuration", str(section_data))
                
                console.print(section_table)
                console.print()

        hardware = config.get('hardware', {})
        system_info = config.get('metadata', {}).get('system', {})
        
        if hardware and system_info:
            compat_table = Table(
                title="[bold yellow]Hardware Compatibility Check[/bold yellow]",
                title_justify="left",
                box=box.ROUNDED,
                header_style="bold yellow",
                border_style="bright_white",
                show_header=True,
                width=min(80, console.width - 4)
            )
            
            compat_table.add_column("Component", style="bold cyan", width=15)
            compat_table.add_column("Available", style="bold green", width=15)
            compat_table.add_column("Recommended", style="bold blue", width=20)
            compat_table.add_column("Status", style="bold", width=10, justify="center")
            
            cuda_available = system_info.get('cuda_available', False)
            cuda_devices = system_info.get('cuda_devices', 0)
            recommended_gpu = hardware.get('recommended_gpu_memory', 0)
            
            gpu_status = "OK" if cuda_available and cuda_devices > 0 else "FAIL"
            gpu_style = "bold green" if gpu_status == "OK" else "bold red"
            compat_table.add_row(
                "GPU",
                f"{cuda_devices} devices" if cuda_available else "None",
                f"{recommended_gpu}GB",
                Text(gpu_status, style=gpu_style)
            )
            
            cpu_count = system_info.get('cpu_count', 1)
            min_cpu = hardware.get('minimum_system_requirements', {}).get('cpu_cores', 2)
            recommended_cpu = hardware.get('optimal_system_requirements', {}).get('cpu_cores', 4)
            
            cpu_status = "OK" if cpu_count >= min_cpu else "WARN"
            cpu_style = "bold green" if cpu_status == "OK" else "bold yellow"
            compat_table.add_row(
                "CPU Cores",
                str(cpu_count),
                f"{min_cpu} (min) / {recommended_cpu} (opt)",
                Text(cpu_status, style=cpu_style)
            )
            
            min_ram = hardware.get('minimum_system_requirements', {}).get('ram_gb', 4)
            recommended_ram = hardware.get('optimal_system_requirements', {}).get('ram_gb', 8)
            ram_status = "OK"
            compat_table.add_row(
                "RAM",
                "Unknown",
                f"{min_ram}GB (min) / {recommended_ram}GB (opt)",
                Text(ram_status, style="bold green")
            )
            
            console.print(compat_table)
            console.print()

        presets = config.get('presets', {})
        runtime = config.get('runtime', {})
        current_preset = presets.get('current_preset')
        
        # Gather preset information
        preset_info_content = []
        if current_preset:
            preset_configs = presets.get('preset_configs', {})
            
            if isinstance(preset_configs, dict) and current_preset in preset_configs:
                preset_info = preset_configs[current_preset]
                if isinstance(preset_info, str):
                    description = preset_info
                elif isinstance(preset_info, dict):
                    description = preset_info.get('description', 'No description available')
                else:
                    description = 'No description available'
            else:
                description = 'No description available'
            
            available_count = len(presets.get('available_presets', []))
            custom_count = len(presets.get('custom_presets_available', []))
            last_applied = presets.get('last_applied', 'Unknown')
            # Handle various timestamp formats
            if last_applied != 'Unknown':
                try:
                    if last_applied.endswith('Z'):
                        applied_dt = datetime.fromisoformat(last_applied.replace('Z', '+00:00'))
                    elif 'T' in last_applied:
                        applied_dt = datetime.fromisoformat(last_applied)
                    else:
                        applied_dt = datetime.fromisoformat(last_applied)
                    last_applied = applied_dt.strftime('%Y-%m-%d %H:%M:%S')
                except (ValueError, TypeError):
                    # truncate if invalid
                    last_applied = str(last_applied)[:19]
            else:
                last_applied = 'Unknown'
            
            preset_info_content.extend([
                f"Active Preset: [bold green]{current_preset}[/bold green]",
                f"Description: [dim]{description}[/dim]",
                f"Available Presets: [bold]{available_count}[/bold]",
                f"Custom Presets: [bold]{custom_count}[/bold]",
                f"Last Applied: [dim]{last_applied}[/dim]"
            ])
        else:
            preset_info_content.append("Active Preset: [bold yellow]None (custom configuration)[/bold yellow]")
        
        # Gather runtime information
        config_source = (
            runtime.get('config_source') or 
            config.get('metadata', {}).get('config_source') or
            (presets.get('current_preset', 'unknown') + '_preset' if presets.get('current_preset') else 'Unknown') or
            'Unknown'
        )
        
        config_loaded_at = (
            runtime.get('config_loaded_at') or
            runtime.get('config_generated_at') or
            config.get('metadata', {}).get('last_accessed') or
            config.get('metadata', {}).get('created') or
            'Unknown'
        )
        
        if config_loaded_at != 'Unknown':
            try:
                # Handle various timestamp formats
                if config_loaded_at.endswith('Z'):
                    loaded_dt = datetime.fromisoformat(config_loaded_at.replace('Z', '+00:00'))
                elif 'T' in config_loaded_at:
                    loaded_dt = datetime.fromisoformat(config_loaded_at)
                else:
                    loaded_dt = datetime.fromisoformat(config_loaded_at)
                loaded_str = loaded_dt.strftime('%Y-%m-%d %H:%M:%S')
            except (ValueError, TypeError):
                loaded_str = str(config_loaded_at)[:19]  # Truncate if invalid
        else:
            loaded_str = 'Unknown'
        
        health_info = runtime.get('configuration_health', {})
        
        # Determine health status with intelligent fallback logic
        health_status = health_info.get('status')
        if not health_status:
            # Analyze configuration to determine health
            try:
                is_valid, errors, warnings = validate_config(config, strict=False)
                if is_valid and not warnings:
                    health_status = 'healthy'
                elif is_valid and len(warnings) <= 3:
                    health_status = 'needs_attention' 
                elif is_valid:
                    health_status = 'degraded'
                else:
                    health_status = 'critical'
            except Exception:
                # Final fallback based on config completeness
                required_sections = ['training', 'model', 'security', 'data']
                missing_sections = [s for s in required_sections if s not in config]
                health_status = 'healthy' if not missing_sections else 'needs_attention'
        
        # Get warning and recommendation counts with fallbacks
        warning_count = (
            health_info.get('warning_count') or
            len(runtime.get('system_warnings', [])) or
            len(config.get('metadata', {}).get('validation_warnings', [])) or
            0
        )
        
        recommendation_count = (
            health_info.get('recommendation_count') or
            len(runtime.get('recommendations', [])) or
            0
        )
        
        health_color = "green" if health_status == "healthy" else "yellow" if health_status == "needs_attention" else "red"
        
        # Add runtime information to the content
        preset_info_content.extend([
            "",
            f"Config Source: [bold blue]{config_source}[/bold blue]",
            f"Loaded At: [dim]{loaded_str}[/dim]",
            f"Health Status: [bold {health_color}]{health_status.upper()}[/bold {health_color}]",
            f"Warnings: [bold yellow]{warning_count}[/bold yellow]",
            f"Recommendations: [bold cyan]{recommendation_count}[/bold cyan]"
        ])
        
        # Create combined panel
        combined_panel = Panel.fit(
            "\n".join(preset_info_content),
            title="[bold]Configuration Status & Runtime Information[/bold]",
            border_style="cyan",
            style="bold",
            padding=(1, 2)
        )
        console.print(combined_panel)

    except Exception as e:
        console.print(f"[bold red]Failed to display configuration: {e}[/bold red]")
        try:
            if config is not None and isinstance(config, dict):
                console.print("\n[bold yellow]Fallback: Configuration as JSON:[/bold yellow]")
                console.print(json.dumps(config, indent=2, default=str))
            else:
                try:
                    fallback_config = get_current_config()
                    console.print("\n[bold yellow]Fallback: Fresh configuration as JSON:[/bold yellow]")
                    console.print(json.dumps(fallback_config, indent=2, default=str))
                except Exception as fallback_error:
                    console.print(f"[bold red]Could not retrieve configuration for fallback: {fallback_error}[/bold red]")
                    console.print("[bold yellow]Minimal configuration information:[/bold yellow]")
                    console.print("[bold yellow]Training: batch_size=32, epochs=100[/bold yellow]")
                    console.print("[bold yellow]Model: SimpleAutoencoder with encoding_dim=8[/bold yellow]")
                    console.print("[bold yellow]Status: Emergency fallback active[/bold yellow]")
        except Exception as fallback_error:
            console.print(f"[bold red]Fallback display also failed: {fallback_error}[/bold red]")
            console.print("[bold yellow]Configuration system requires attention[/bold yellow]")

def load_config_from_file():
    """Load configuration from file."""
    file_path = input(Fore.YELLOW + Style.BRIGHT + "\nConfiguration file path: ")
    if file_path:
        try:
            with open(file_path, 'r') as f:
                config = json.load(f)
            update_global_config(config)
            print(Fore.GREEN + Style.BRIGHT + f"Configuration loaded from '{file_path}'")
        except Exception as e:
            print(Fore.RED + Style.BRIGHT + f"Failed to load configuration: {e}")



def edit_config_interactive():
    """Interactive configuration editor."""
    # clear screen and show banner
    print("\033c", end="")
    show_banner()
    
    print(Fore.YELLOW + Style.BRIGHT + "\nInteractive configuration editing not yet implemented")
    print(Fore.YELLOW + Style.BRIGHT + "Please use the preset configurations or manual file editing for now.")

def compare_configs_interactive():
    """Interactive configuration comparison."""
    # clear screen and show banner
    print("\033c", end="")
    show_banner()
    
    print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration comparison not yet implemented")
    print(Fore.YELLOW + Style.BRIGHT + "This feature will allow side-by-side comparison of different configurations.")



def main(logger: logging.Logger):
    """Main entry point with comprehensive argument parsing and system orchestration."""
    # Initialize basic logger first (fallback)
    #logger = logging.getLogger(__name__)
    
    # Initialize system to set up logging and configuration
    try:
        system_status, config, logger = initialize_system()
        #validate_config(config)
    except Exception as e:
        # Use fallback logger since initialize_system failed
        logger.warning(f"Configuration initialization failed, using defaults: {e}")
        config = get_current_config()
    
    # If no arguments provided, launch interactive mode
    if len(sys.argv) == 1:
        interactive_main()
        return

    # Initialize configuration system early
    
    # Extract configuration sections for defaults
    training_config = config.get('training', {})
    model_config = config.get('model', {})
    data_config = config.get('data', {})
    security_config = config.get('security', {})
    system_config = config.get('system', {})
    hpo_config = config.get('hyperparameter_optimization', {})
    monitoring_config = config.get('monitoring', {})
    
    # Create argument parser with comprehensive configuration integration
    parser = argparse.ArgumentParser(
        description="Enhanced Anomaly Detection Model Training with Configuration Management",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        epilog="""
Examples:
  %(prog)s --preset development               # Use development preset
  %(prog)s --epochs 100 --batch-size 64     # Custom training parameters
  %(prog)s --hpo-trials 50                   # Hyperparameter optimization
  %(prog)s --show-config                     # Display current configuration
  %(prog)s --compare-models                  # Compare model architectures
        """
    )
    
    # Training configuration group
    training_group = parser.add_argument_group('Training Parameters')
    training_group.add_argument(
        "--epochs",
        type=int,
        default=training_config.get('epochs'),
        help=f"Maximum number of training epochs (config: {training_config.get('epochs', DEFAULT_EPOCHS)})"
    )
    training_group.add_argument(
        "--batch-size",
        type=int,
        default=training_config.get('batch_size'),
        help=f"Training batch size (config: {training_config.get('batch_size', DEFAULT_BATCH_SIZE)})"
    )
    training_group.add_argument(
        "--lr",
        type=float,
        default=training_config.get('learning_rate'),
        help=f"Learning rate (config: {training_config.get('learning_rate', LEARNING_RATE)})"
    )
    training_group.add_argument(
        "--patience",
        type=int,
        default=training_config.get('patience'),
        help=f"Early stopping patience in epochs (config: {training_config.get('patience', EARLY_STOPPING_PATIENCE)})"
    )
    training_group.add_argument(
        "--weight-decay",
        type=float,
        default=training_config.get('weight_decay'),
        help=f"Weight decay for optimizer (config: {training_config.get('weight_decay', WEIGHT_DECAY)})"
    )
    training_group.add_argument(
        "--grad-clip",
        type=float,
        default=training_config.get('gradient_clip'),
        help=f"Gradient clipping value (config: {training_config.get('gradient_clip', GRADIENT_CLIP)})"
    )
    training_group.add_argument(
        "--mixed-precision",
        action="store_true",
        default=training_config.get('mixed_precision', MIXED_PRECISION),
        help="Enable mixed precision training"
    )
    
    # Model configuration group
    model_group = parser.add_argument_group('Model Parameters')
    model_group.add_argument(
        "--model-type",
        choices=['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble'],
        default=model_config.get('model_type', 'EnhancedAutoencoder'),
        help="Type of model architecture to use"
    )
    model_group.add_argument(
        "--features",
        type=int,
        default=data_config.get('features'),
        help=f"Number of input features (config: {data_config.get('features', FEATURES)})"
    )
    model_group.add_argument(
        "--encoding-dim",
        type=int,
        default=model_config.get('encoding_dim'),
        help=f"Encoder hidden dimension (config: {model_config.get('encoding_dim', DEFAULT_ENCODING_DIM)})"
    )
    model_group.add_argument(
        "--num-models",
        type=int,
        default=model_config.get('num_models', NUM_MODELS),
        help=f"Number of models in ensemble (config: {model_config.get('num_models', NUM_MODELS)})"
    )
    model_group.add_argument(
        "--hidden-dims",
        type=int,
        nargs='+',
        default=model_config.get('hidden_dims'),
        help=f"Hidden layer dimensions (config: {model_config.get('hidden_dims', HIDDEN_LAYER_SIZES)})"
    )
    model_group.add_argument(
        "--dropout-rates",
        type=float,
        nargs='+',
        default=model_config.get('dropout_rates'),
        help=f"Dropout rates for each layer (config: {model_config.get('dropout_rates', DROPOUT_RATES)})"
    )
    model_group.add_argument(
        "--activation",
        choices=['relu', 'leaky_relu', 'gelu', 'elu', 'swish'],
        default=model_config.get('activation', ACTIVATION),
        help=f"Activation function (config: {model_config.get('activation', ACTIVATION)})"
    )
    model_group.add_argument(
        "--normalization",
        choices=[None, 'batch', 'layer', 'instance'],
        default=model_config.get('normalization'),
        help=f"Normalization type (config: {model_config.get('normalization', NORMALIZATION)})"
    )
    
    # Data configuration group
    data_group = parser.add_argument_group('Data Parameters')
    data_group.add_argument(
        "--use-real-data",
        action="store_true",
        default=data_config.get('use_real_data', False),
        help="Use preprocessed data instead of synthetic"
    )
    data_group.add_argument(
        "--normal-samples",
        type=int,
        default=data_config.get('normal_samples'),
        help=f"Normal training samples for synthetic data (config: {data_config.get('normal_samples', NORMAL_SAMPLES)})"
    )
    data_group.add_argument(
        "--attack-samples",
        type=int,
        default=data_config.get('attack_samples'),
        help=f"Anomalous test samples for synthetic data (config: {data_config.get('attack_samples', ATTACK_SAMPLES)})"
    )
    data_group.add_argument(
        "--validation-split",
        type=float,
        default=data_config.get('validation_split', 0.2),
        help="Fraction of data to use for validation"
    )
    data_group.add_argument(
        "--data-path",
        type=Path,
        default=data_config.get('data_path', DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        help="Path to preprocessed data file"
    )
    data_group.add_argument(
        "--artifacts-path",
        type=Path,
        default=data_config.get('artifacts_path', DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        help="Path to preprocessing artifacts file"
    )
    
    # Security configuration group
    security_group = parser.add_argument_group('Security Parameters')
    security_group.add_argument(
        "--percentile",
        type=int,
        default=security_config.get('percentile'),
        help=f"Percentile for anomaly threshold (config: {security_config.get('percentile', DEFAULT_PERCENTILE)})"
    )
    security_group.add_argument(
        "--anomaly-threshold-strategy",
        choices=['percentile', 'iqr', 'zscore', 'isolation_forest'],
        default=security_config.get('anomaly_threshold_strategy', 'percentile'),
        help="Strategy for calculating anomaly threshold"
    )
    
    # System configuration group
    system_group = parser.add_argument_group('System Parameters')
    system_group.add_argument(
        "--model-dir",
        type=Path,
        default=Path(system_config.get('model_dir', DEFAULT_MODEL_DIR)),
        help="Directory to save model artifacts"
    )
    system_group.add_argument(
        "--tb-dir",
        type=Path,
        default=Path(monitoring_config.get('tensorboard_dir', TB_DIR)),
        help="TensorBoard logging directory"
    )
    system_group.add_argument(
        "--log-dir",
        type=Path,
        default=Path(system_config.get('log_dir', LOG_DIR)),
        help="Logging directory"
    )
    system_group.add_argument(
        "--config-dir",
        type=Path,
        default=Path(system_config.get('config_dir', CONFIG_DIR)),
        help="Configuration directory"
    )
    system_group.add_argument(
        "--num-workers",
        type=int,
        default=training_config.get('num_workers', min(4, os.cpu_count() or 1)),
        help="Number of workers for data loading"
    )
    system_group.add_argument(
        "--export-onnx",
        action="store_true",
        default=system_config.get('export_onnx', False),
        help="Export model to ONNX format"
    )
    system_group.add_argument(
        "--non-interactive",
        action="store_true",
        default=system_config.get('non_interactive', False),
        help="Disable all interactive prompts"
    )
    system_group.add_argument(
        "--debug",
        action="store_true",
        default=system_config.get('debug', False),
        help="Enable debug logging"
    )
    
    # Hyperparameter optimization group
    hpo_group = parser.add_argument_group('Hyperparameter Optimization')
    hpo_group.add_argument(
        "--hpo",
        action="store_true",
        default=hpo_config.get('enabled', False),
        help="Enable hyperparameter optimization"
    )
    hpo_group.add_argument(
        "--hpo-trials",
        type=int,
        default=hpo_config.get('n_trials', 50),
        help=f"Number of hyperparameter optimization trials (config: {hpo_config.get('n_trials', 50)})"
    )
    hpo_group.add_argument(
        "--hpo-timeout",
        type=int,
        default=hpo_config.get('timeout_seconds', 3600),
        help="Timeout for hyperparameter optimization in seconds (0 for no timeout)"
    )
    hpo_group.add_argument(
        "--hpo-sampler",
        choices=['TPE', 'Random', 'CmaEs'],
        default=hpo_config.get('sampler', {}).get('type', 'TPE'),
        help="Sampler type for hyperparameter optimization"
    )
    hpo_group.add_argument(
        "--hpo-pruner",
        choices=['Median', 'Hyperband', 'None'],
        default=hpo_config.get('pruner', {}).get('type', 'Median'),
        help="Pruner type for hyperparameter optimization"
    )
    
    # Configuration management group
    config_group = parser.add_argument_group('Configuration Management')
    config_group.add_argument(
        "--preset",
        choices=list(PRESET_CONFIGS.keys()),
        help=f"Use a preset configuration. Available: {list(PRESET_CONFIGS.keys())}"
    )
    config_group.add_argument(
        "--show-config",
        action="store_true",
        help="Display current configuration and exit"
    )
    config_group.add_argument(
        "--validate-config",
        action="store_true",
        help="Validate current configuration and exit"
    )
    config_group.add_argument(
        "--save-config",
        type=str,
        metavar="NAME",
        help="Save current configuration with given name"
    )
    config_group.add_argument(
        "--load-config",
        type=str,
        metavar="NAME",
        help="Load saved configuration by name"
    )
    config_group.add_argument(
        "--list-configs",
        action="store_true",
        help="List all available configurations"
    )
    config_group.add_argument(
        "--reset-config",
        action="store_true",
        help="Reset configuration to defaults"
    )
    config_group.add_argument(
        "--compare-models",
        action="store_true",
        help="Compare available model architectures"
    )
    config_group.add_argument(
        "--benchmark",
        action="store_true",
        help="Run performance benchmark"
    )
    
    # Monitoring configuration group
    monitoring_group = parser.add_argument_group('Monitoring Parameters')
    monitoring_group.add_argument(
        "--disable-tensorboard",
        action="store_true",
        help="Disable TensorBoard logging"
    )
    monitoring_group.add_argument(
        "--log-frequency",
        type=int,
        default=monitoring_config.get('log_frequency', 1),
        help="Frequency of progress logging (epochs)"
    )
    monitoring_group.add_argument(
        "--checkpoint-frequency",
        type=int,
        default=monitoring_config.get('checkpoint_frequency', 10),
        help="Frequency of model checkpointing (epochs)"
    )
    monitoring_group.add_argument(
        "--metrics-frequency",
        type=int,
        default=monitoring_config.get('metrics_frequency', 10),
        help="Frequency of detailed metrics logging (epochs)"
    )
    
    # Parse arguments
    args = parser.parse_args()
    
    # Handle configuration commands first (these exit after completion)
    if args.show_config:
        current_config = get_current_config()
        #current_config = config  # Use the config we have
        print("Current Configuration:")
        print("=" * 60)
        print(json.dumps(current_config, indent=2, default=str))
        return
    
    if args.validate_config:
        try:
            validate_config(config)
            logger.info("[INFO] Configuration is valid")
        except ValueError as e:
            logger.error(f"[ERROR] Configuration validation failed: {e}")
            sys.exit(1)
        return
    
    if args.list_configs:
        print("Available Configurations:")
        print("=" * 40)
        print("\nPreset Configurations:")
        for name, preset in PRESET_CONFIGS.items():
            print(f"  [+] {name}: {preset.get('description', 'No description')}")
        
        # List saved configurations if any
        saved_configs = list_saved_configs()
        if saved_configs:
            print("\nSaved Configurations:")
            for name in saved_configs:
                print(f"  [+] {name}")
        return
    
    if args.reset_config:
        if args.non_interactive or prompt_user("Reset configuration to defaults?", default=False):
            reset_config()
            logger.info("[INFO] Configuration reset to defaults")
        return
    
    if args.compare_models:
        display_model_comparison()
        return
    
    if args.benchmark:
        run_performance_benchmark(args)
        return
    
    # Handle configuration loading
    if args.load_config:
        try:
            #config = load_saved_config(args.load_config)
            config = load_config(args.load_config)
            update_global_config(config)
            logger.info(f"[INFO] Loaded configuration: {args.load_config}")
        except FileNotFoundError:
            logger.error(f"[ERROR] Configuration '{args.load_config}' not found")
            sys.exit(1)
    
    # Apply preset configuration if specified
    if args.preset:
        if args.preset not in PRESET_CONFIGS:
            logger.error(f"[ERROR] Invalid preset: {args.preset}")
            logger.info(f"Available presets: {list(PRESET_CONFIGS.keys())}")
            sys.exit(1)
        
        preset_config = PRESET_CONFIGS[args.preset].copy()
        current_config = get_current_config()
        merged_config = deep_update(current_config, preset_config)
        update_global_config(merged_config)
        logger.info(f"[INFO] Applied preset configuration: {args.preset}")
    
    # Configure logging level early
    if args.debug:
        logger.setLevel(logging.DEBUG)
        logging.getLogger("torch").setLevel(logging.DEBUG)
        logging.getLogger("optuna").setLevel(logging.DEBUG)
        logger.debug("Debug logging enabled")
    else:
        logging.getLogger("optuna").setLevel(logging.WARNING)
    
    # Update configuration with command line arguments
    current_config = get_current_config()
    
    # Map command line args to configuration structure
    arg_config_mapping = {
        # Training parameters
        'epochs': ('training', 'epochs'),
        'batch_size': ('training', 'batch_size'),
        'lr': ('training', 'learning_rate'),
        'patience': ('training', 'patience'),
        'weight_decay': ('training', 'weight_decay'),
        'grad_clip': ('training', 'gradient_clip'),
        'mixed_precision': ('training', 'mixed_precision'),
        'num_workers': ('training', 'num_workers'),
        
        # Model parameters
        'model_type': ('model', 'model_type'),
        'encoding_dim': ('model', 'encoding_dim'),
        'num_models': ('model', 'num_models'),
        'hidden_dims': ('model', 'hidden_dims'),
        'dropout_rates': ('model', 'dropout_rates'),
        'activation': ('model', 'activation'),
        'normalization': ('model', 'normalization'),
        
        # Data parameters
        'features': ('data', 'features'),
        'normal_samples': ('data', 'normal_samples'),
        'attack_samples': ('data', 'attack_samples'),
        'use_real_data': ('data', 'use_real_data'),
        'validation_split': ('data', 'validation_split'),
        'data_path': ('data', 'data_path'),
        'artifacts_path': ('data', 'artifacts_path'),
        
        # Security parameters
        'percentile': ('security', 'percentile'),
        'anomaly_threshold_strategy': ('security', 'anomaly_threshold_strategy'),
        
        # System parameters
        'model_dir': ('system', 'model_dir'),
        'tb_dir': ('system', 'tensorboard_dir'),
        'log_dir': ('system', 'log_dir'),
        'config_dir': ('system', 'config_dir'),
        'export_onnx': ('system', 'export_onnx'),
        'non_interactive': ('system', 'non_interactive'),
        'debug': ('system', 'debug'),
        
        # HPO parameters
        'hpo_trials': ('hyperparameter_optimization', 'n_trials'),
        'hpo_timeout': ('hyperparameter_optimization', 'timeout_seconds'),
        'hpo_sampler': ('hyperparameter_optimization', 'sampler', 'type'),
        'hpo_pruner': ('hyperparameter_optimization', 'pruner', 'type'),
        
        # Monitoring parameters
        'log_frequency': ('monitoring', 'log_frequency'),
        'checkpoint_frequency': ('monitoring', 'checkpoint_frequency'),
        'metrics_frequency': ('monitoring', 'metrics_frequency'),
    }
    
    # Update configuration with non-None command line arguments
    for arg_name, config_path in arg_config_mapping.items():
        arg_value = getattr(args, arg_name, None)
        if arg_value is not None:
            # Navigate to the correct nested dictionary
            target = current_config
            for key in config_path[:-1]:
                target = target.setdefault(key, {})
            target[config_path[-1]] = arg_value
    
    # Handle special flags
    if args.disable_tensorboard:
        current_config.setdefault('monitoring', {})['tensorboard_logging'] = False
    
    # Update global configuration
    update_global_config(current_config)
    
    # Save configuration if requested
    if args.save_config:
        try:
            #save_named_config(args.save_config, current_config)
            save_config(args.save_config, current_config)
            logger.info(f"[INFO] Configuration saved as: {args.save_config}")
        except Exception as e:
            logger.error(f"[ERROR] Failed to save configuration: {e}")
    
    # Validate final configuration
    try:
        validate_config(current_config)
    except ValueError as e:
        logger.error(f"[ERROR] Configuration validation failed: {e}")
        if not args.non_interactive and prompt_user("Continue with invalid configuration?", default=False):
            logger.warning("Proceeding with potentially invalid configuration")
        else:
            sys.exit(1)
    
    # Log system information
    logger.info("=" * 80)
    logger.info("SYSTEM INITIALIZATION")
    logger.info("=" * 80)
    logger.info(f"Python version: {sys.version}")
    logger.info(f"PyTorch version: {torch.__version__}")
    logger.info(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logger.info(f"CUDA version: {torch.version.cuda}")
        logger.info(f"GPU count: {torch.cuda.device_count()}")
    logger.info(f"Working directory: {os.getcwd()}")
    logger.info(f"Configuration preset: {args.preset or 'custom'}")
    
    # Run hyperparameter optimization if requested
    if args.hpo or args.hpo_trials > 0:
        logger.info("=" * 80)
        logger.info("HYPERPARAMETER OPTIMIZATION")
        logger.info("=" * 80)
        
        try:
            hpo_results = setup_hyperparameter_optimization(args, current_config)
            
            # Update args with best parameters for final training
            if hpo_results and 'best_config' in hpo_results:
                best_config = hpo_results['best_config']
                merged_config = deep_update(current_config, best_config)
                update_global_config(merged_config)
                
                logger.info("[INFO] Hyperparameter optimization completed")
                logger.info(f"Best objective value: {hpo_results['best_value']:.5f}")
                logger.info("Best parameters:")
                for key, value in hpo_results['best_params'].items():
                    logger.info(f"  {key}: {value}")
                
                # Ask if user wants to train final model
                if not args.non_interactive:
                    if not prompt_user("Train final model with best parameters?", default=True):
                        logger.info("Hyperparameter optimization completed. Exiting.")
                        return
                
                logger.info("Training final model with optimized parameters...")
            else:
                logger.warning("Hyperparameter optimization failed to produce results")
                return
                
        except Exception as e:
            logger.error(f"Hyperparameter optimization failed: {e}")
            if args.debug:
                logger.exception("HPO error details:")
            return
    
    # Ensure all required directories exist
    try:
        args.model_dir.mkdir(parents=True, exist_ok=True)
        args.log_dir.mkdir(parents=True, exist_ok=True)
        args.tb_dir.mkdir(parents=True, exist_ok=True)
        args.config_dir.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.error(f"Failed to create directories: {e}")
        sys.exit(1)
    
    # Log final training configuration
    logger.info("=" * 80)
    logger.info("FINAL CONFIGURATION")
    logger.info("=" * 80)
    
    final_config = get_current_config()
    config_summary = {
        'training': {
            'epochs': final_config.get('training', {}).get('epochs', DEFAULT_EPOCHS),
            'batch_size': final_config.get('training', {}).get('batch_size', DEFAULT_BATCH_SIZE),
            'learning_rate': final_config.get('training', {}).get('learning_rate', LEARNING_RATE),
            'patience': final_config.get('training', {}).get('patience', EARLY_STOPPING_PATIENCE),
            'mixed_precision': final_config.get('training', {}).get('mixed_precision', MIXED_PRECISION)
        },
        'model': {
            'type': final_config.get('model', {}).get('model_type', 'EnhancedAutoencoder'),
            'encoding_dim': final_config.get('model', {}).get('encoding_dim', DEFAULT_ENCODING_DIM),
            'features': final_config.get('data', {}).get('features', FEATURES)
        },
        'data': {
            'use_real_data': final_config.get('data', {}).get('use_real_data', False),
            'normal_samples': final_config.get('data', {}).get('normal_samples', NORMAL_SAMPLES),
            'attack_samples': final_config.get('data', {}).get('attack_samples', ATTACK_SAMPLES)
        },
        'system': {
            'model_dir': str(args.model_dir),
            'export_onnx': final_config.get('system', {}).get('export_onnx', False),
            'debug': final_config.get('system', {}).get('debug', False)
        }
    }
    
    for section, params in config_summary.items():
        logger.info(f"{section.upper()}:")
        for key, value in params.items():
            logger.info(f"  {key}: {value}")
    
    # Save final configuration to model directory
    final_config_path = args.model_dir / "run_configuration.json"
    try:
        with open(final_config_path, 'w') as f:
            json.dump(final_config, f, indent=2, default=str)
        logger.info(f"[INFO] Configuration saved to: {final_config_path}")
    except Exception as e:
        logger.warning(f"Could not save run configuration: {e}")
    
    # Run training
    logger.info("=" * 80)
    logger.info("STARTING TRAINING")
    logger.info("=" * 80)
    
    try:
        training_results = train_model(args)
        
        # Log training completion
        logger.info("=" * 80)
        logger.info("TRAINING COMPLETED SUCCESSFULLY")
        logger.info("=" * 80)
        
        # Display key results
        if training_results:
            metrics = training_results.get('evaluation', {})
            logger.info("Key Results:")
            logger.info(f"  Best validation loss: {training_results.get('training', {}).get('best_val_loss', 'N/A'):.4f}")
            logger.info(f"  Test loss: {metrics.get('test_loss', 'N/A'):.4f}")
            logger.info(f"  Anomaly detection rate: {metrics.get('anomaly_detection_rate', 'N/A'):.2%}")
            logger.info(f"  Model parameters: {training_results.get('model', {}).get('total_parameters', 'N/A'):,}")
            
            # Save training summary
            summary_path = args.model_dir / "training_complete.json"
            try:
                with open(summary_path, 'w') as f:
                    json.dump({
                        'status': 'completed',
                        'timestamp': datetime.now().isoformat(),
                        'configuration': final_config,
                        'results': training_results
                    }, f, indent=2, default=str)
            except Exception as e:
                logger.warning(f"Could not save training summary: {e}")
        
        logger.info("=" * 80)
        
    except KeyboardInterrupt:
        logger.info("Training interrupted by user")
        # Save interruption info
        interruption_path = args.model_dir / "training_interrupted.json"
        try:
            with open(interruption_path, 'w') as f:
                json.dump({
                    'status': 'interrupted',
                    'timestamp': datetime.now().isoformat(),
                    'configuration': final_config
                }, f, indent=2, default=str)
        except:
            pass
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"Training failed: {str(e)}")
        if args.debug:
            logger.exception("Training error details:")
        
        # Save error info
        error_path = args.model_dir / "training_failed.json"
        try:
            with open(error_path, 'w') as f:
                json.dump({
                    'status': 'failed',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat(),
                    'configuration': final_config,
                    'traceback': traceback.format_exc()
                }, f, indent=2, default=str)
        except:
            pass
        
        sys.exit(1)
        
    finally:
        # Cleanup
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass

if __name__ == "__main__":
    # Configure warnings and logging before anything else
    warnings.filterwarnings("ignore", category=UserWarning)
    warnings.filterwarnings("ignore", category=FutureWarning)
    
    # Ensure required directories exist
    try:
        LOG_DIR.mkdir(parents=True, exist_ok=True)
        DEFAULT_MODEL_DIR.mkdir(parents=True, exist_ok=True)
        CONFIG_DIR.mkdir(parents=True, exist_ok=True)
        TB_DIR.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        print(f"Failed to create required directories: {e}")
        sys.exit(1)
    
    try:
        main(logger)
    except KeyboardInterrupt:
        logger.info("Application interrupted by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}", exc_info=True)
        sys.exit(1)