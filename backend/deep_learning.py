# Standard library imports
import os
import json
import logging
import argparse
import platform
import shutil
import traceback
import time
import select
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Tuple, Union, Any, Callable
from enum import Enum, auto
from copy import deepcopy
from collections import defaultdict, OrderedDict, deque
from functools import wraps, partial
from contextlib import nullcontext
import threading
import subprocess
import hashlib
import pickle
import gc
import re
import uuid
import tempfile
import webbrowser
import tracemalloc
from tqdm import tqdm
from alive_progress import alive_bar
import msvcrt
import math
import importlib.util

# Scientific computing and data manipulation
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, KFold, StratifiedKFold
from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_classification, make_blobs
from sklearn.covariance import EllipticEnvelope
from sklearn.mixture import GaussianMixture

# PyTorch ecosystem
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, Dataset, WeightedRandomSampler, DistributedSampler
from torch.utils.data.dataloader import default_collate
from torch.utils.tensorboard import SummaryWriter
from torch.cuda.amp import GradScaler, autocast
import torchvision
import torchvision.transforms as transforms
import torch.distributed as dist
from torch.cuda.amp import autocast as cuda_autocast, GradScaler
from torch.cpu.amp import autocast as cpu_autocast

# Hyperparameter optimization
import optuna
import optuna.visualization as vis
from optuna import visualization as vis
from optuna.samplers import TPESampler, RandomSampler, CmaEsSampler
from optuna.pruners import MedianPruner, HyperbandPruner, NopPruner
from optuna.storages import RDBStorage
import joblib

# Visualization and plotting
import plotly
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
from plotly.subplots import make_subplots
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns

# Rich UI components for enhanced terminal interface
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, BarColumn, TimeElapsedColumn, SpinnerColumn, track, ProgressColumn, TextColumn
from rich import box
from rich.text import Text
from rich.columns import Columns
from rich.prompt import Prompt, Confirm, IntPrompt, FloatPrompt
from rich.syntax import Syntax
from rich.tree import Tree
from rich.layout import Layout
from rich.live import Live
from rich.spinner import Spinner
from rich.rule import Rule
from rich.align import Align
from rich.padding import Padding
from rich.markup import escape
from rich.status import Status, Spinner

# Terminal styling and colors
from colorama import Fore, Back, Style, init

# Initialize colorama
init(autoreset=True)

# Initialize rich console
console = Console()

# Configuration and serialization
import yaml
try:
    from yaml import CLoader as Loader, CDumper as Dumper
except ImportError:
    from yaml import Loader, Dumper

import toml
import configparser
from dataclasses import dataclass, field, asdict
from enum import Enum, auto

# Networking and API (for future remote capabilities)
import socket
import urllib.request
import urllib.parse
import urllib.error

# Parallel processing and concurrency
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from multiprocessing import Pool, cpu_count
#import asyncio

# File handling and compression
import zipfile
import tarfile
import gzip
import lzma

# Additional utility libraries
import itertools
import random
import string
from contextlib import contextmanager, suppress, nullcontext
import weakref
from types import SimpleNamespace
import pathlib
from pynput.keyboard import Key, Listener
#import pkg_resources
import packaging.version
from packaging import version as pkg_version
from sklearn.exceptions import ConvergenceWarning
from matplotlib import MatplotlibDeprecationWarning

# Development and debugging tools
import inspect
import cProfile
import pstats

# Version checking utilities (safe, future-proof)
import sys
import warnings

# Use importlib.metadata (stdlib) or backport
try:
    from importlib.metadata import version as _get_version, PackageNotFoundError
    IMPORTLIB_METADATA_AVAILABLE = True
except ImportError:  # Python <3.8
    try:
        from importlib_metadata import version as _get_version, PackageNotFoundError  # type: ignore
        IMPORTLIB_METADATA_AVAILABLE = True
    except ImportError:
        IMPORTLIB_METADATA_AVAILABLE = False
        PackageNotFoundError = Exception  # type: ignore

# Packaging for version parsing/comparison
try:
    from packaging import version as pkg_version
    PACKAGING_AVAILABLE = True
except ImportError:
    PACKAGING_AVAILABLE = False
    # Create a dummy version class for fallback
    class DummyVersion:
        def __init__(self, version_str):
            self.version_str = str(version_str)
        
        def __str__(self):
            return self.version_str
        
        def __lt__(self, other):
            return self.version_str < str(other)
        
        def __le__(self, other):
            return self.version_str <= str(other)
        
        def __eq__(self, other):
            return self.version_str == str(other)
        
        def __ge__(self, other):
            return self.version_str >= str(other)
        
        def __gt__(self, other):
            return self.version_str > str(other)
    
    def dummy_parse(version_str):
        return DummyVersion(version_str)
    
    # Fallback Dummy version module to mimic packaging.version
    pkg_version = type('DummyVersionModule', (), {'parse': dummy_parse})()

def safe_version(package_name: str) -> str:
    """
    Safely get the version of a package.
    Uses importlib.metadata, falls back to getattr(__version__),
    and returns 'N/A' if not found.
    """
    try:
        # Special cases where direct import is safer or required
        if package_name == "sklearn":
            import sklearn
            return sklearn.__version__
        if package_name == "torch":
            import torch
            return torch.__version__
        if package_name == "numpy":
            import numpy as np
            return np.__version__
        if package_name == "pandas":
            import pandas as pd
            return pd.__version__
        if package_name == "optuna":
            import optuna
            return optuna.__version__
        if package_name == "plotly":
            import plotly
            return plotly.__version__

        # Try importlib.metadata (preferred)
        if IMPORTLIB_METADATA_AVAILABLE:
            return _get_version(package_name)
        
        # Fallback: import the module and check __version__
        module = __import__(package_name)
        return getattr(module, "__version__", "unknown")
        
    except (PackageNotFoundError, ImportError):
        return "N/A"
    except Exception:
        return "unknown"

# Alias for backward compatibility with your existing code
get_package_version = safe_version

# Model export and optimization
import onnx

# Global flag and dummy module
ONNXRUNTIME_AVAILABLE = False
ort: Any = None

def initialize_onnx_runtime() -> bool:
    """
    Safely initialize ONNX Runtime with proper error handling.
    Returns True if ONNX Runtime is available and functional.
    """
    global ONNXRUNTIME_AVAILABLE, ort
    
    try:
        import onnxruntime as _ort
        # Test basic functionality
        try:
            # Simple test to verify DLL loading works
            _ort.get_device()
            ort = _ort
            ONNXRUNTIME_AVAILABLE = True
            return True
        except Exception as dll_error:
            warnings.warn(
                f"ONNX Runtime DLL load failed: {str(dll_error)}. "
                "ONNX validation will be disabled.",
                RuntimeWarning
            )
            create_dummy_ort()
            return False
    except ImportError:
        create_dummy_ort()
        return False

def create_dummy_ort() -> None:
    """Create a dummy ONNX Runtime module for compatibility."""
    global ort
    
    class DummyORT:
        __version__ = "N/A"
        
        class InferenceSession:
            def __init__(self, *args, **kwargs):
                raise RuntimeError(
                    "ONNX Runtime not available. "
                    "Original error: DLL load failed"
                )
        
        @staticmethod
        def get_available_providers() -> list:
            return []
        
        @staticmethod
        def get_device() -> str:
            return "CPU (ONNX Runtime not available)"
    
    ort = DummyORT()

# Initialize at module level
initialize_onnx_runtime()

try:
    from torch.jit import script, trace
    TORCH_JIT_AVAILABLE = True
except ImportError:
    TORCH_JIT_AVAILABLE = False

# System monitoring and profiling
import psutil
try:
    import memory_profiler
    MEMORY_PROFILER_AVAILABLE = True
except ImportError:
    MEMORY_PROFILER_AVAILABLE = False

try:
    import nvidia_ml_py3 as nvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False

# Cryptography and security (for model protection)
try:
    from cryptography.fernet import Fernet
    from cryptography.hazmat.primitives import hashes
    from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
    import base64
    CRYPTO_AVAILABLE = True
except ImportError:
    CRYPTO_AVAILABLE = False

# Database connectivity (for advanced storage)
try:
    import sqlite3
    import sqlalchemy
    DATABASE_AVAILABLE = True
except ImportError:
    DATABASE_AVAILABLE = False

# Additional ML libraries
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.svm import OneClassSVM
    from sklearn.neighbors import LocalOutlierFactor
    SKLEARN_ANOMALY_AVAILABLE = True
except ImportError:
    SKLEARN_ANOMALY_AVAILABLE = False

# Time series analysis (for temporal anomaly detection)
try:
    import statsmodels.api as sm
    from statsmodels.tsa.arima.model import ARIMA
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False

# Advanced numerical computation
try:
    from numba import jit, cuda
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False

# Profiling tools
try:
    from line_profiler import LineProfiler
    LINE_PROFILER_AVAILABLE = True
except ImportError:
    LINE_PROFILER_AVAILABLE = False

# Availability flags for optional dependencies
OPTIONAL_DEPENDENCIES = {
    'torch_jit': TORCH_JIT_AVAILABLE,
    'onnxruntime': ONNXRUNTIME_AVAILABLE,
    'nvml': NVML_AVAILABLE,
    'crypto': CRYPTO_AVAILABLE,
    'database': DATABASE_AVAILABLE,
    'sklearn_anomaly': SKLEARN_ANOMALY_AVAILABLE,
    'statsmodels': STATSMODELS_AVAILABLE,
    'numba': NUMBA_AVAILABLE,
    'memory_profiler': MEMORY_PROFILER_AVAILABLE,
    'line_profiler': LINE_PROFILER_AVAILABLE,
    'packaging': PACKAGING_AVAILABLE,
    'importlib_metadata': IMPORTLIB_METADATA_AVAILABLE
}

# Version information - Updated to properly detect Rich
VERSION_INFO = {
    'python': sys.version.split()[0],
    'torch': safe_version('torch'),
    'numpy': safe_version('numpy'),
    'pandas': safe_version('pandas'),
    'optuna': safe_version('optuna'),
    'rich': safe_version('rich'),
    'plotly': safe_version('plotly'),
    'sklearn': safe_version('sklearn'),
    'onnx': safe_version('onnx'),
    'psutil': safe_version('psutil')
}

# Setup logging
LOG_DIR = Path("logs")
LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE_NAME = "deep_learning.log"
LOG_FILE = LOG_DIR / LOG_FILE_NAME

# Configure directories
DEFAULT_MODEL_DIR = Path("models")
DEFAULT_MODEL_DIR.mkdir(exist_ok=True)

CONFIG_DIR = Path("config")
CONFIG_DIR.mkdir(exist_ok=True)
CONFIG_FILE_NAME = "deep_learning_config.json"
CONFIG_FILE = CONFIG_DIR / CONFIG_FILE_NAME

REPORTS_DIR = Path("reports")
REPORTS_DIR.mkdir(exist_ok=True)

TB_DIR = Path("tensorboard")
TB_DIR.mkdir(exist_ok=True)

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

CACHE_DIR = Path("cache")
CACHE_DIR.mkdir(exist_ok=True)

CHECKPOINTS_DIR = Path("checkpoints") / f"checkpoints_v{VERSION_INFO['torch']}"
CHECKPOINTS_DIR.mkdir(exist_ok=True)

RESULTS_DIR = Path("results")
RESULTS_DIR.mkdir(exist_ok=True)
RESULTS_FILE_NAME = "training_results.json"
RESULTS_FILE = RESULTS_DIR / RESULTS_FILE_NAME

def configure_device(device_type='auto'):
    """
    Configure device settings based on preference and availability.
    Defaults to CPU if CUDA is requested but not available.
    """
    # Handle CUDA preference with fallback to CPU
    if device_type == 'cuda' and not torch.cuda.is_available():
        print("Warning: CUDA requested but not available. Defaulting to CPU.")
        device_type = 'cpu'
    
    # Resolve device
    if device_type == 'auto':
        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    elif device_type in ['cpu', 'cuda']:
        DEVICE = device_type
    else:
        raise ValueError("device_type must be 'auto', 'cpu', or 'cuda'")
    
    MIXED_PRECISION = False if DEVICE == 'cpu' else True
    
    return DEVICE, MIXED_PRECISION

def configure_normalization(normalization_type='batch'):
    """
    Configure normalization settings based on the specified type.
    
    Args:
        normalization_type (str): 'batch', 'layer', 'instance', 'group', 'none', or None
    
    Returns:
        tuple: (NORMALIZATION, USE_BATCH_NORM, USE_LAYER_NORM)
    """
    
    normalization_options = ['batch', 'layer', 'instance', 'group', 'none', None]
    
    if normalization_type not in normalization_options:
        raise ValueError(f"Invalid normalization_type: {normalization_type}, it must be one of {normalization_options}")
    
    # Set the normalization type
    NORMALIZATION = normalization_type
    
    # Configure the boolean flags based on normalization type
    if normalization_type == 'batch':
        USE_BATCH_NORM = True
        USE_LAYER_NORM = False
    elif normalization_type == 'layer':
        USE_BATCH_NORM = False
        USE_LAYER_NORM = True
    elif normalization_type in ['none', None]:
        USE_BATCH_NORM = False
        USE_LAYER_NORM = False
    else:  # 'instance' or 'group'
        USE_BATCH_NORM = False
        USE_LAYER_NORM = False
    
    return NORMALIZATION, USE_BATCH_NORM, USE_LAYER_NORM

# Configuration Constants
DEFAULT_BATCH_SIZE = 64
DEFAULT_EPOCHS = 10
EARLY_STOPPING_PATIENCE = 100
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-4
GRADIENT_CLIP = 1.0
GRADIENT_ACCUMULATION_STEPS = 4
#DEVICE = 'cpu' if not torch.cuda.is_available() else 'cuda'
#MIXED_PRECISION = False if not torch.cuda.is_available() else True
DEVICE, MIXED_PRECISION = configure_device('auto')

# Model Architecture Constants
DEFAULT_ENCODING_DIM = 10
HIDDEN_LAYER_SIZES = [128, 64]
DROPOUT_RATES = [0.2, 0.15]
ACTIVATION = 'leaky_relu'
ACTIVATION_PARAM = 0.2
NORMALIZATION_OPTIONS = ['batch', 'layer', 'instance', 'group', 'none', None]
#NORMALIZATION = 'batch'
#USE_BATCH_NORM = True
#USE_LAYER_NORM = False
NORMALIZATION, USE_BATCH_NORM, USE_LAYER_NORM = configure_normalization('batch')

DIVERSITY_FACTOR = 0.1
MIN_FEATURES = 5
NUM_MODELS = 3
FEATURES = 20
INPUT_DIM = 20
NORMAL_SAMPLES = 8000
ATTACK_SAMPLES = 2000
ANOMALY_FACTOR = 1.5
RANDOM_STATE = 42

# Security Constants
DEFAULT_PERCENTILE = 95
DEFAULT_ATTACK_THRESHOLD = 0.3
FALSE_NEGATIVE_COST = 2.0
SECURITY_METRICS = True

# System Constants
NUM_WORKERS = min(4, os.cpu_count() or 1)
MAX_MEMORY_PERCENT = 80
# Cache timeout for model artifacts in seconds
CACHE_TIMEOUT = 3600

# Forward declarations for classes that will be defined later
class SimpleAutoencoder:
    """Forward declaration for SimpleAutoencoder class."""
    pass

class EnhancedAutoencoder:
    """Forward declaration for EnhancedAutoencoder class."""
    pass

class AutoencoderEnsemble:
    """Forward declaration for AutoencoderEnsemble class."""
    pass

class EnhancedCollateFn:
    """Forward declaration for EnhancedCollateFn class."""
    pass

class WorkerInitializer:
    """Forward declaration for WorkerInitializer class."""
    pass

class UnicodeStreamHandler:
    # Forward declaration
    pass

def setup_safe_globals():
    """
    Register a curated list of safe, version-stable classes for torch.load
    to prevent unpickling errors across different PyTorch, NumPy, and Pandas
    versions, including fallbacks for older/newer library structures.
    """
    # Dictionary of safe classes with their full import paths
    safe_classes = {
        # PyTorch core classes
        'torch.Tensor': torch.Tensor,
        'torch.nn.Module': torch.nn.Module,
        'torch.nn.parameter.Parameter': torch.nn.parameter.Parameter,
        'torch.FloatTensor': torch.FloatTensor,
        'torch.LongTensor': torch.LongTensor,
        'torch.IntTensor': torch.IntTensor,
        'torch.DoubleTensor': torch.DoubleTensor,
        
        # PyTorch optimizers and schedulers
        'torch.optim.Optimizer': torch.optim.Optimizer,
        'torch.optim.AdamW': torch.optim.AdamW,
        'torch.optim.SGD': torch.optim.SGD,
        'torch.optim.lr_scheduler.ReduceLROnPlateau': torch.optim.lr_scheduler.ReduceLROnPlateau,
        
        # PyTorch data loading
        'torch.utils.data.Dataset': torch.utils.data.Dataset,
        'torch.utils.data.DataLoader': torch.utils.data.DataLoader,
        'torch.utils.data.TensorDataset': torch.utils.data.TensorDataset,
        
        # Path handling classes (critical for the error you're seeing)
        'pathlib.Path': pathlib.Path,
        'pathlib.WindowsPath': pathlib.WindowsPath,
        'pathlib.PosixPath': pathlib.PosixPath,
        
        # Version handling
        'torch.torch_version.TorchVersion': torch.torch_version.TorchVersion,
        
        # NumPy core classes
        'numpy.ndarray': np.ndarray,
        'numpy.float32': np.float32,
        'numpy.float64': np.float64,
        'numpy.int32': np.int32,
        'numpy.int64': np.int64,
        'numpy.dtype': np.dtype,
        'numpy.number': np.number,
        
        # Python built-ins
        'builtins.dict': dict,
        'builtins.list': list,
        'builtins.tuple': tuple,
        'builtins.set': set,
        
        'EnhancedCollateFn': EnhancedCollateFn,
        'WorkerInitializer': WorkerInitializer,
        'UnicodeStreamHandler': UnicodeStreamHandler
    }

    # Add custom model classes if they exist
    try:
        from models import SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble
        safe_classes.update({
            'models.SimpleAutoencoder': SimpleAutoencoder,
            'models.EnhancedAutoencoder': EnhancedAutoencoder,
            'models.AutoencoderEnsemble': AutoencoderEnsemble
        })
    except ImportError:
        pass

    # Special handling for PyTorch storage types
    storage_types = [
        'torch.FloatStorage',
        'torch.LongStorage',
        'torch.IntStorage',
        'torch.DoubleStorage',
    ]
    
    for stype in storage_types:
        try:
            if hasattr(torch, stype):
                safe_classes[f'torch.{stype}'] = getattr(torch, stype)
        except Exception:
            pass

    # Register all safe classes using torch.serialization.add_safe_globals
    try:
        # Convert values to list for add_safe_globals
        safe_objects = list(safe_classes.values())
        torch.serialization.add_safe_globals(safe_objects)
        
        # Additional numpy-specific reconstruction functions
        numpy_reconstructors = []
        try:
            # For numpy >= 1.20 (new structure)
            if hasattr(np, '_core') and hasattr(np._core, 'multiarray'):
                numpy_reconstructors.extend([
                    np._core.multiarray._reconstruct,
                    np._core.multiarray.scalar,
                    np._core.multiarray.array,
                ])
            # For numpy < 1.20 (old structure)
            elif hasattr(np, 'core') and hasattr(np.core, 'multiarray'):
                numpy_reconstructors.extend([
                    np.core.multiarray._reconstruct,
                    np.core.multiarray.scalar,
                    np.core.multiarray.array,
                ])
        except AttributeError:
            pass
        
        if numpy_reconstructors:
            torch.serialization.add_safe_globals(numpy_reconstructors)

        # Add PyTorch internal reconstruction functions
        torch_reconstructors = []
        try:
            torch_reconstructors.extend([
                torch._utils._rebuild_tensor_v2,
                torch._utils._rebuild_parameter,
                torch._utils._rebuild_tensor,
            ])
        except AttributeError:
            pass
        
        if torch_reconstructors:
            torch.serialization.add_safe_globals(torch_reconstructors)

    except Exception as e:
        logging.warning(f"Failed to register some safe globals: {str(e)}")

# Setup safe globals at module level
setup_safe_globals()

# Disable PyTorch's duplicate logging
torch._logging.set_logs(all=logging.ERROR)

# Initialize logger at module level
logger = logging.getLogger(__name__)

# Loading Screen and System Check Framework
class CheckLevel(Enum):
    """Enumeration representing the severity of a system check."""
    # Check must pass for the program to continue running
    CRITICAL = auto()
    
    # Check should pass for full functionality but not fatal
    IMPORTANT = auto()
    
    # Non-essential check providing useful system information
    INFORMATIONAL = auto()

class CheckResult:
    """Encapsulates the outcome of a system check with enhanced functionality."""
    
    def __init__(self, 
                 passed: bool, 
                 message: str, 
                 level: CheckLevel = CheckLevel.IMPORTANT,
                 details: Optional[Union[str, Dict[str, Any]]] = None,
                 metadata: Optional[Dict[str, Any]] = None,
                 exception: Optional[Exception] = None):
        self.passed = passed
        self.message = message
        self.level = level
        self.details = details
        self.metadata = metadata if metadata is not None else {}
        self.exception = exception

    def with_details(self, details: Union[str, Dict[str, Any]]) -> 'CheckResult':
        """Return CheckResult with additional details."""
        self.details = details
        return self
    
    def with_exception(self, exception: Exception) -> 'CheckResult':
        """Return CheckResult with an exception."""
        self.exception = exception
        return self
    
    def with_metadata(self, metadata: Dict[str, Any]) -> 'CheckResult':
        """Return CheckResult with additional metadata."""
        if self.metadata is None:
            self.metadata = {}
        self.metadata.update(metadata)
        return self
    
    def with_passed(self, passed: bool) -> 'CheckResult':
        """Update the passed status and return self."""
        self.passed = passed
        return self
    
    def with_message(self, message: str) -> 'CheckResult':
        """Update the message and return self."""
        self.message = message
        return self
    
    def with_level(self, level: CheckLevel) -> 'CheckResult':
        """Update the check level and return self."""
        self.level = level
        return self

def loading_screen(
    logger: logging.Logger,
    extended: bool = False,
    include_performance: bool = False,
    hardware_data: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Display loading screen with system checks and interactive prompts.
    
    Args:
        logger: Logger for recording system check results
        extended: Whether to run extended initialization-specific checks
        include_performance: Whether to include performance-related checks
        hardware_data: Pre-fetched hardware data (optional, for optimization)
        
    Returns:
        bool: True if all critical checks pass and user chooses to continue,
              False if critical checks fail or user chooses to quit
    """
    # Thread safety lock
    _loading_lock = threading.RLock()
    
    with _loading_lock:
        try:
            # Console safety checks
            if not hasattr(console, 'width'):
                # Safe default
                console_width = 80
            else:
                # Minimum width
                console_width = max(60, getattr(console, 'width', 80))
            
            # Terminal capability detection
            is_tty = sys.stdout.isatty()
            supports_color = is_tty and hasattr(sys.stdout, 'isatty')
            
            # Safe console clear
            try:
                if is_tty:
                    console.clear()
                else:
                    # Fallback for non-TTY
                    console.print("\n" * 3)
            except Exception:
                # Safe fallback
                console.print("\n" * 3)
            
            # Initialize timing with thread-safe approach
            start_time = time.perf_counter()
            status_messages = [
                "Running System Diagnostics...",
                "Initializing system checks...",
                "Validating environment...",
                "Executing system checks..."
            ]
            
            # Non-blocking loading animation with proper status management
            current_status = None
            try:
                # Sequential status updates to avoid context conflicts
                for i, message in enumerate(status_messages):
                    if current_status:
                        current_status.stop()
                    
                    if is_tty:
                        current_status = console.status(
                            f"[bold blue]{message}[/bold blue]" if supports_color else message,
                            spinner="dots" if is_tty else None
                        )
                        current_status.start()
                        # Progressive timing
                        time.sleep(0.3 + i * 0.1)
                    else:
                        console.print(f"- {message}")
                        # Minimal delay for non-TTY
                        time.sleep(0.1)
            finally:
                if current_status:
                    current_status.stop()
                    current_status = None
            
            # ASCII art banner with width adaptation
            banner_width = min(console_width - 8, 100)
            ascii_art = """
        ⠀⠀⠀⢠⣾⣷⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⠀⠀⣰⣿⣿⣿⣿⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⠀⢰⣿⣿⣿⣿⣿⣿⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣤⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣄⣀⣀⣤⣤⣶⣾⣿⣿⣿⡷
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠁
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠁⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠏⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠏⠀⠀⠀⠀
        ⣿⣿⣿⡇⠀⡾⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠁⠀⠀⠀⠀⠀
        ⣿⣿⣿⣧⡀⠁⣀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠉⢹⠉⠙⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣀⠀⣀⣼⣿⣿⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠋⠀⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠛⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠛⠀⠤⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⣿⣿⠿⣿⣿⣿⣿⣿⣿⣿⠿⠋⢃⠈⠢⡁⠒⠄⡀⠈⠁⠀⠀⠀⠀⠀⠀⠀
        ⣿⣿⠟⠁⠀⠀⠈⠉⠉⠁⠀⠀⠀⠀⠈⠆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
        ⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠀⠀⠀⠀⠀⠀⠀⠀⠀
            """
            
            # Safe banner display with width adaptation
            try:
                if banner_width > 80 and supports_color:
                    console.print("\n", Panel.fit(
                        ascii_art,
                        style="bold cyan" if supports_color else "",
                        title="[bold yellow]GreyChamp | IDS[/]" if supports_color else "GreyChamp | IDS",
                        subtitle="[magenta]SYSTEM INITIALIZATION[/]" if supports_color else "SYSTEM INITIALIZATION",
                        border_style="bold blue" if supports_color else "ascii",
                        box=box.DOUBLE if supports_color else box.ASCII,
                        padding=(1, 1),
                        width=min(banner_width, console_width - 4)
                    ))
                else:
                    # Simple fallback for narrow terminals
                    console.print("\n" + "=" * min(60, banner_width))
                    console.print("    GreyChamp | IDS - SYSTEM INITIALIZATION")
                    console.print("=" * min(60, banner_width) + "\n")
            except Exception as banner_error:
                # Ultra-safe fallback
                console.print("\nGreyChamp | IDS - SYSTEM INITIALIZATION\n")
                if logger:
                    logger.debug(f"Banner display failed: {banner_error}")
            
            # Check type information display
            check_type_info = "BASIC CHECKS"
            if extended and include_performance:
                check_type_info = "EXTENDED CHECKS (with Performance)"
            elif extended:
                check_type_info = "EXTENDED CHECKS"
            
            try:
                if supports_color and banner_width > 60:
                    console.print(Panel.fit(
                        f"[bold cyan]Running {check_type_info}[/bold cyan]\n"
                        "[bold cyan]Please wait while we validate your system...[/bold cyan]",
                        border_style="cyan",
                        style="bold cyan",
                        padding=(0, 2),
                        width=min(banner_width, console_width - 4)
                    ))
                else:
                    console.print(f"\nRunning {check_type_info}")
                    console.print("Please wait while we validate your system...\n")
            except Exception:
                console.print(f"\nRunning {check_type_info}\n")
            
            # Thread-safe system checks execution
            console.print("Executing system checks..." if not supports_color else "[bold cyan]Executing system checks...[/bold cyan]")
            
            # Use thread-safe timing measurement
            checks_start = time.perf_counter()
            # Pass the hardware_data to run_system_checks if provided
            results = run_system_checks(logger, extended=extended, include_performance=include_performance, hardware_data=hardware_data)
            elapsed_time = time.perf_counter() - checks_start
            
            # Thread-safe results processing
            if "system_summary" in results and results["system_summary"].details:
                if isinstance(results["system_summary"].details, dict):
                    results["system_summary"].details["execution_time"] = f"{elapsed_time:.2f}s"
            
            # Display results with error handling
            console.print()
            try:
                display_check_results(results, logger, extended=extended, include_performance=include_performance)
            except Exception as display_error:
                console.print(f"[red]Error displaying results: {display_error}[/red]" if supports_color else f"Error displaying results: {display_error}")
                if logger:
                    logger.error(f"Failed to display check results: {display_error}")
            console.print()
            
            # Safe results analysis
            summary = results.get("system_summary")
            system_error = results.get("system_error")
            
            # Determine system status with null safety
            system_status = "UNKNOWN"
            if summary and summary.details and isinstance(summary.details, dict):
                system_status = summary.details.get('system_status', 'UNKNOWN')
            
            # Count failures by level with safety checks
            critical_failed = sum(1 for result in results.values() 
                                if result and hasattr(result, 'level') and hasattr(result, 'passed') and
                                result.level == CheckLevel.CRITICAL and not result.passed 
                                and result != summary)
            
            important_failed = sum(1 for result in results.values() 
                                 if result and hasattr(result, 'level') and hasattr(result, 'passed') and
                                 result.level == CheckLevel.IMPORTANT and not result.passed)
            
            informational_failed = sum(1 for result in results.values() 
                                     if result and hasattr(result, 'level') and hasattr(result, 'passed') and
                                     result.level == CheckLevel.INFORMATIONAL and not result.passed)
            
            # Handle different scenarios with proper cleanup
            return_value = False
            
            try:
                if system_error or system_status == "CRITICAL_FAILURE" or critical_failed > 0:
                    # Critical failure - system cannot continue
                    failed_critical_checks = [
                        name.replace("_", " ").title() 
                        for name, result in results.items() 
                        if (result and hasattr(result, 'level') and hasattr(result, 'passed') and
                            result.level == CheckLevel.CRITICAL and not result.passed and 
                            name != "system_summary")
                    ]
                    
                    error_message = (
                        f"CRITICAL SYSTEM CHECKS FAILED\n\n"
                        f"The system cannot continue due to critical failures.\n"
                        f"Failed checks: {', '.join(failed_critical_checks) if failed_critical_checks else 'System error occurred'}\n\n"
                        f"Please check the logs and resolve these issues before continuing."
                    )
                    
                    try:
                        if supports_color and banner_width > 60:
                            console.print(Panel.fit(
                                f"[bold red]{error_message}[/bold red]",
                                border_style="red",
                                title="Critical Failure",
                                padding=(1, 3),
                                width=min(banner_width, console_width - 4)
                            ))
                        else:
                            console.print(f"\nCRITICAL FAILURE:\n{error_message}")
                    except Exception:
                        console.print(f"\nCRITICAL FAILURE:\n{error_message}")
                    
                    if logger:
                        logger.critical(f"Critical system checks failed - cannot continue. Failed checks: {failed_critical_checks}")
                    
                    return_value = False
                    
                elif system_status in ["DEGRADED", "LIMITED"] or important_failed > 0 or informational_failed > 0:
                    # Non-critical failures - user decision with proper input handling
                    user_choice = _handle_user_decision_safe(
                        results, system_status, important_failed, informational_failed, 
                        elapsed_time, supports_color, banner_width, console_width, logger
                    )
                    
                    if user_choice is False:
                        return_value = False
                        # Cleanup handled in _handle_user_decision_safe
                    else:
                        return_value = True
                        
                else:
                    # All checks passed - success scenario
                    return_value = _handle_success_scenario(
                        summary, elapsed_time, supports_color, banner_width, console_width, logger
                    )
            
            except Exception as scenario_error:
                console.print(f"Error handling system check results: {scenario_error}")
                if logger:
                    logger.error(f"Error in scenario handling: {scenario_error}")
                return_value = False
            
            # Safe console clear before return
            try:
                if return_value and is_tty:
                    console.clear()
            except Exception:
                # Ignore clear failures
                pass
            
            #return return_value
            return return_value, results if return_value else None
            
        except KeyboardInterrupt:
            # Thread-safe interrupt handling
            try:
                console.print(Panel.fit(
                    "INITIALIZATION INTERRUPTED\n\n"
                    "System initialization was cancelled by user.",
                    border_style="red" if supports_color else "ascii",
                    title="Interrupted",
                    padding=(1, 3)
                ) if supports_color else "\nINITIALIZATION INTERRUPTED\n\nSystem initialization was cancelled by user.\n")
            except Exception:
                console.print("\nINITIALIZATION INTERRUPTED\n\nSystem initialization was cancelled by user.\n")
            
            if logger:
                logger.warning("System initialization interrupted by user (Ctrl+C)")
            
            sys.exit(0)
            
        except Exception as e:
            # Thread-safe error handling
            error_msg = f"UNEXPECTED ERROR DURING INITIALIZATION\n\nAn unexpected error occurred: {str(e)}\nError type: {type(e).__name__}"
            
            try:
                if supports_color:
                    console.print(Panel.fit(
                        f"[bold red]{error_msg}[/bold red]",
                        border_style="red",
                        title="System Error",
                        padding=(1, 3)
                    ))
                else:
                    console.print(f"\nSYSTEM ERROR:\n{error_msg}\n")
            except Exception:
                console.print(f"\nSYSTEM ERROR:\n{error_msg}\n")
            
            if logger:
                logger.critical(f"Loading screen failed with unexpected error: {str(e)}", exc_info=True)
                logger.error(f"Error occurred during {'extended' if extended else 'basic'} system checks")
            
            return False

def _handle_user_decision_safe(results, system_status, important_failed, informational_failed, 
                              elapsed_time, supports_color, banner_width, console_width, logger):
    """Thread-safe user decision handling with proper resource cleanup."""
    listener = None
    
    try:
        # Collect failed non-critical checks safely
        failed_checks = []
        for name, result in results.items():
            if (result and hasattr(result, 'passed') and hasattr(result, 'level') and 
                hasattr(result, 'message') and not result.passed and 
                result.level in [CheckLevel.IMPORTANT, CheckLevel.INFORMATIONAL] and 
                name not in ["system_summary", "system_error"]):
                failed_checks.append({
                    'name': name.replace("_", " ").title(),
                    'level': result.level.name,
                    'message': result.message
                })
        
        # Display failed checks summary safely
        if failed_checks:
            try:
                if supports_color and banner_width > 80:
                    fail_table = Table(
                        title="[bold yellow]Failed Non-Critical Checks[/bold yellow]",
                        box=box.SIMPLE,
                        header_style="bold magenta",
                        title_justify="left",
                        show_header=True,
                        show_lines=True,
                        width=min(100, console_width - 4)
                    )
                    fail_table.add_column("Check", style="bold cyan", width=28)
                    fail_table.add_column("Issue", style="bold white", no_wrap=False)
                    fail_table.add_column("Level", justify="center", width=14)
                    
                    for check in failed_checks:
                        level_style = {
                            "IMPORTANT": "bold yellow",
                            "INFORMATIONAL": "bold blue"
                        }.get(check['level'], "white")
                        
                        fail_table.add_row(
                            check['name'],
                            check['message'],
                            Text(check['level'], style=level_style)
                        )
                    
                    console.print(fail_table)
                else:
                    # Simple fallback display
                    #console.print("Failed Non-Critical Checks:")
                    print(Fore.YELLOW + Style.BRIGHT + "Failed Non-Critical Checks:")
                    for check in failed_checks:
                        console.print(f"  - {check['name']}: {check['message']} ({check['level']})")
                
                console.print()
                
            except Exception as table_error:
                # Ultra-safe fallback
                #console.print("Some non-critical checks failed:")
                print(Fore.YELLOW + Style.BRIGHT + "Some non-critical checks failed:")
                for check in failed_checks:
                    console.print(f"  {check['name']}: {check['message']}")
                console.print()
                if logger:
                    logger.debug(f"Failed checks table display error: {table_error}")
        
        # Status display with safe formatting
        status_color = "yellow" if system_status == "DEGRADED" else "blue" if system_status == "LIMITED" else "yellow"
        status_message = {
            "DEGRADED": "SYSTEM DEGRADED",
            "LIMITED": "LIMITED FUNCTIONALITY", 
        }.get(system_status, "SOME CHECKS FAILED")
        
        prompt_text = (
            f"{status_message}\n\n"
            f"System Status Details:\n"
            f"- Important failures: {important_failed}\n"
            f"- Informational failures: {informational_failed}\n"
            f"- Total execution time: {elapsed_time:.2f}s\n\n"
            f"The system can continue with reduced functionality.\n"
            #f"Continue anyway? (Y/n)"
        )
        
        try:
            if supports_color and banner_width > 60:
                console.print(Panel.fit(
                    f"[bold {status_color}]{prompt_text}[/bold {status_color}]",
                    border_style=status_color,
                    title="User Decision Required",
                    padding=(1, 3),
                    width=min(banner_width, console_width - 4)
                ))
            else:
                console.print(f"\n{status_message}\n")
                console.print(prompt_text)
        except Exception:
            console.print(f"\n{status_message}\n")
            console.print(prompt_text)
        
        # Use standard input instead of pynput to avoid buffer issues
        user_choice = None
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                #response = input("\nYour choice: ").strip().lower()
                #response = input(Fore.YELLOW + Style.BRIGHT + "\nYour choice: ").strip().lower()
                response = input(Fore.YELLOW + Style.BRIGHT + "\nContinue anyway? (Y/n/q): ").strip().lower()
                
                # Default to yes
                if response in ['y', 'yes', '']:
                    user_choice = True
                    break
                elif response in ['n', 'no', 'q', 'quit']:
                    user_choice = False
                    break
                else:
                    if attempt < max_attempts - 1:
                        #console.print("Please enter 'y' for yes or 'n' for no.")
                        print(Fore.YELLOW + Style.BRIGHT + "Please enter 'y' for yes or 'n' for no or 'q' for quit.")
                    
            except (EOFError, KeyboardInterrupt):
                user_choice = False
                break
            except Exception as input_error:
                if logger:
                    logger.debug(f"Input error on attempt {attempt + 1}: {input_error}")
                if attempt < max_attempts - 1:
                    #console.print("Input error, please try again.")
                    print(Fore.RED + Style.BRIGHT + "Input error, please try again.")
        
        # Default to continue if no valid choice after max attempts
        if user_choice is None:
            user_choice = True
            #console.print("Using default choice: continue")
            print(Fore.CYAN + Style.BRIGHT + "Using default choice: continue")
        
        # Handle user choice with safe output
        if user_choice is False:
            try:
                cancel_message = (
                    "USER CANCELLED INITIALIZATION\n\n"
                    "You chose to quit and resolve the issues.\n"
                    "Please check the logs and fix the failed checks."
                )
                
                if supports_color and banner_width > 60:
                    console.print(Panel.fit(
                        #f"[bold red]{cancel_message}[/bold red]",
                        f"{cancel_message}",
                        border_style="red",
                        title="CANCELLED",
                        style="bold red",
                        padding=(1, 3),
                        width=min(banner_width, console_width - 4)
                    ))
                else:
                    #console.print(f"\nCANCELLED:\n{cancel_message}")
                    print(Fore.RED + Style.BRIGHT + f"\nCANCELLED:\n{cancel_message}")
            except Exception:
                #console.print(f"\nCANCELLED:\n{cancel_message}")
                print(Fore.RED + Style.BRIGHT + f"\nCANCELLED:\n{cancel_message}")
            
            if logger:
                logger.warning("User chose to quit after seeing failed checks")
                logger.info(f"Failed checks summary: {len(failed_checks)} non-critical failures")
            
            #console.print("Exiting system initialization...")
            print(Fore.RED + Style.BRIGHT + "Exiting system initialization...")
            
            # give user time to read the message
            time.sleep(2)
            sys.exit(0)
            
            return False
        
        # User chose to continue
        try:
            continue_message = (
                "CONTINUING WITH WARNINGS\n\n"
                "You chose to continue despite the warnings.\n"
                "Some functionality may be limited."
            )
            
            if supports_color and banner_width > 60:
                console.print(Panel.fit(
                    #f"[bold green]{continue_message}[/bold green]",
                    f"{continue_message}",
                    border_style="green",
                    title="CONTINUING",
                    style="bold green",
                    padding=(1, 2),
                    width=min(banner_width, console_width - 4)
                ))
            else:
                #console.print(f"\nCONTINUING:\n{continue_message}")
                print(Fore.GREEN + Style.BRIGHT + f"\nCONTINUING:\n{continue_message}")
        except Exception:
            #console.print(f"\nCONTINUING:\n{continue_message}")
            print(Fore.GREEN + Style.BRIGHT + f"\nCONTINUING:\n{continue_message}")
        
        if logger:
            logger.info("User chose to continue despite failed checks")
            logger.info(f"System status: {system_status} with {len(failed_checks)} failed checks")
        
        # give user time to read the message
        time.sleep(2)
        
        return True
        
    except Exception as decision_error:
        if logger:
            logger.error(f"Error in user decision handling: {decision_error}")
        
        #console.print(f"Error in user input - continuing with warnings: {decision_error}")
        print(Fore.RED + Style.BRIGHT + f"Error in user input - continuing with warnings: {decision_error}")
        
        # Default to continue on error
        return True
    
    finally:
        # Clean up input buffer
        try:
            # Small delay for any pending I/O
            time.sleep(0.05)
            
            # Flush streams
            sys.stdout.flush()
            sys.stderr.flush()
            
            # Clear input buffer
            if hasattr(select, 'select') and sys.stdin.isatty():
                while select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], []):
                    line = sys.stdin.readline()
                    if not line:
                        break
            
            # Windows approach
            try:
                import msvcrt
                while msvcrt.kbhit():
                    msvcrt.getch()
            except ImportError:
                pass
            
            # Final flush
            sys.stdin.flush()
            
        except Exception as cleanup_error:
            if logger:
                logger.debug(f"Input buffer cleanup failed: {cleanup_error}")

def _handle_success_scenario(summary, elapsed_time, supports_color, banner_width, console_width, logger):
    """Handle successful system checks scenario with safe input."""
    try:
        success_details = ""
        elapsed_time_details = f"{elapsed_time:.2f}"
        if summary and summary.details and isinstance(summary.details, dict):
            total_checks = summary.details.get('total_checks', 0)
            if supports_color:
                success_details = f"[bold yellow]{total_checks}[/bold yellow]"
                elapsed_time_details = f"[bold yellow]{elapsed_time:.2f}[/bold yellow]"
            else:
                success_details = str(total_checks)
        
        success_message = (
            f"ALL SYSTEM CHECKS PASSED\n"
            f"System is fully operational and ready!\n"
            f"Completed {success_details} checks successfully.\n"
            f"Completed in {elapsed_time_details} seconds.\n\n"
            f"Continue to system? (Y/n)"
        )
        
        try:
            if supports_color and banner_width > 60:
                console.print(Panel.fit(
                    #f"[bold green]{success_message}[/]",
                    f"{success_message}",
                    border_style="green",
                    style="bold green",
                    #title="[bold green]SUCCESS[/]",
                    title="SUCCESS",
                    padding=(1, 3),
                    width=min(banner_width, console_width - 4)
                ))
            else:
                #console.print(f"\nSUCCESS:\n{success_message}")
                print(Fore.GREEN + Style.BRIGHT + f"\nSUCCESS:\n{success_message}")
        except Exception:
            #console.print(f"\nSUCCESS:\n{success_message}")
            print(Fore.GREEN + Style.BRIGHT + f"\nSUCCESS:\n{success_message}")
        
        # Handle user choice with safe input
        user_choice = None
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                #response = input("\nYour choice: ").strip().lower()
                response = input(Fore.YELLOW + Style.BRIGHT + "\nYour choice: ").strip().lower()
                
                # Default to yes (continue)
                if response in ['y', 'yes', '']:
                    user_choice = True
                    break
                elif response in ['n', 'no', 'q', 'quit']:
                    user_choice = False
                    break
                else:
                    if attempt < max_attempts - 1:
                        #console.print("Please enter 'y' for yes or 'n' for no.")
                        print(Fore.YELLOW + Style.BRIGHT + "Please enter 'y' for yes or 'n' for no.")
                    
            except (EOFError, KeyboardInterrupt):
                user_choice = False
                break
            except Exception as input_error:
                if logger:
                    logger.debug(f"Input error on attempt {attempt + 1}: {input_error}")
                if attempt < max_attempts - 1:
                    #console.print("Input error, please try again.")
                    print(Fore.RED + Style.BRIGHT + "Input error, please try again.")
        
        # Default to continue if no valid choice after max attempts
        if user_choice is None:
            user_choice = True
            #console.print("Using default choice: continue")
            print(Fore.CYAN + Style.BRIGHT + "Using default choice: continue")
        
        # Handle user choice
        if user_choice is False:
            try:
                quit_message = (
                    "USER CHOSE TO QUIT\n\n"
                    "You chose to quit despite all checks passing.\n"
                    "System initialization cancelled."
                )
                
                if supports_color and banner_width > 60:
                    console.print(Panel.fit(
                        #f"[bold yellow]{quit_message}[/bold yellow]",
                        f"{quit_message}",
                        border_style="red",
                        style="bold red",
                        title="QUIT",
                        padding=(1, 3),
                        width=min(banner_width, console_width - 4)
                    ))
                else:
                    #console.print(f"\nQUIT:\n{quit_message}")
                    print(Fore.RED + Style.BRIGHT + f"\nQUIT:\n{quit_message}")
            except Exception:
                #console.print(f"\nQUIT:\n{quit_message}")
                print(Fore.RED + Style.BRIGHT + f"\nQUIT:\n{quit_message}")
            
            if logger:
                logger.debug("User chose to quit after successful system checks")
            
            #console.print("Exiting system initialization...")
            print(Fore.RED + Style.BRIGHT + "Exiting system initialization...")
            
            time.sleep(2)
            sys.exit(0)
        
        # User chose to continue
        try:
            continue_message = (
                "CONTINUING TO SYSTEM\n\n"
                "All checks passed - proceeding to main system."
            )
            
            if supports_color and banner_width > 60:
                console.print(Panel.fit(
                    #f"[bold green]{continue_message}[/bold green]",
                    f"{continue_message}",
                    border_style="green",
                    title="PROCEEDING",
                    style="bold green",
                    padding=(1, 2),
                    width=min(banner_width, console_width - 4)
                ))
            else:
                #console.print(f"\nPROCEEDING:\n{continue_message}")
                print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{continue_message}")
        except Exception:
            #console.print(f"\nPROCEEDING:\n{continue_message}")
            print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{continue_message}")
        
        if logger:
            logger.debug(f"All system checks passed successfully in {elapsed_time:.2f}s - user chose to continue")
            if summary and summary.details:
                system_status = summary.details.get('system_status', 'OPTIMAL')
                logger.info(f"System status: {system_status}")
        
        # give user time to read the message
        time.sleep(2)
        
        return True
        
    except Exception as success_error:
        if logger:
            logger.error(f"Error in success scenario: {success_error}")
        
        #console.print("All checks passed - continuing...")
        print(Fore.GREEN + Style.BRIGHT + f"All checks passed - continuing...")
        
        return True
    
    finally:
        # Clean up input buffer
        try:
            # Small delay for any pending I/O
            time.sleep(0.05)
            
            # Flush streams
            sys.stdout.flush()
            sys.stderr.flush()
            
            # Clear input buffer
            if hasattr(select, 'select') and sys.stdin.isatty():
                while select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], []):
                    line = sys.stdin.readline()
                    if not line:
                        break
            
            # Windows approach
            try:
                while msvcrt.kbhit():
                    msvcrt.getch()
            except ImportError:
                pass
            
            # Final flush
            sys.stdin.flush()
            
        except Exception as cleanup_error:
            if logger:
                logger.debug(f"Input buffer cleanup failed: {cleanup_error}")

def run_system_checks(
    logger: logging.Logger, 
    extended: bool = False,
    include_performance: bool = False,
    hardware_data: Optional[Dict[str, Any]] = None
) -> Dict[str, CheckResult]:
    """
    Run comprehensive system checks with optional extended validations.
    
    Args:
        logger: Configured logger for recording check results
        extended: Whether to include initialization-specific checks
        include_performance: Whether to include performance-related checks
        hardware_data: Pre-fetched hardware data (optional, for optimization)
        
    Returns:
        Dictionary mapping check names to their CheckResult objects
    """
    checks: Dict[str, CheckResult] = {}
    
    try:
        # Core system checks (always run)
        raw_checks = {
            # Critical checks (essential for operation)
            'python_version': check_python_version(),
            'torch_available': check_torch(),
            
            # Important checks (affects functionality but not critical)
            'package_versions': check_package_versions_wrapper(),
            'directory_access': check_directory_access_wrapper(),
            
            # Hardware resource checks - use provided hardware_data if available
            'hardware': check_hardware_wrapper() if hardware_data is None else CheckResult(
                passed=True,
                message="Hardware check completed (using cached data)",
                level=CheckLevel.INFORMATIONAL,
                details=hardware_data
            ),
            
            # Informational checks (diagnostic purposes)
            'logging_setup': check_logging_setup(),
            'seed_config': check_seed_config(hardware_data)
        }

        # Extended system checks (when requested)
        if extended:
            raw_checks.update({
                'exception_handler': check_global_exception_handler(),
                'configuration_system': check_configuration_system_wrapper(),
                'model_variants': check_model_variants_wrapper()
            })
            
            # Performance-related checks (only when explicitly requested)
            if include_performance:
                raw_checks.update({
                    'performance_monitoring': check_performance_monitoring(),
                    'memory_management': check_memory_management(),
                    'performance_baseline': check_performance_baseline()
                })
        
        # Convert all results to CheckResult objects if they aren't already
        for name, result in raw_checks.items():
            if isinstance(result, CheckResult):
                checks[name] = result
            elif isinstance(result, dict):
                # Convert dictionary result to CheckResult
                checks[name] = CheckResult(
                    passed=result.get('passed', False),
                    message=result.get('message', f"Check {name} completed"),
                    # Default level, should be overridden by specific checks
                    level=CheckLevel.INFORMATIONAL,
                    details=result.get('details', result)
                )
            else:
                # Handle unexpected result types
                checks[name] = CheckResult(
                    passed=False,
                    message=f"Invalid result type for {name}: {type(result)}",
                    level=CheckLevel.CRITICAL,
                    details={'raw_result': str(result), 'result_type': str(type(result))}
                )
        
        # Calculate overall system status
        critical_checks = [
            result for result in checks.values() 
            if result.level in {CheckLevel.CRITICAL, CheckLevel.IMPORTANT}
        ]
        overall_passed = all(result.passed for result in critical_checks)
        
        # Determine system status based on failures
        critical_failures = sum(1 for r in checks.values() if not r.passed and r.level == CheckLevel.CRITICAL)
        important_failures = sum(1 for r in checks.values() if not r.passed and r.level == CheckLevel.IMPORTANT)
        
        if critical_failures > 0:
            system_status = "CRITICAL_FAILURE"
        elif important_failures > 0:
            system_status = "DEGRADED"
        elif any(not r.passed for r in checks.values()):
            system_status = "LIMITED"
        else:
            system_status = "OPTIMAL"
        
        # Create comprehensive summary
        summary_details = {
            'total_checks': len(checks),
            'passed_checks': sum(1 for r in checks.values() if r.passed),
            'failed_checks': sum(1 for r in checks.values() if not r.passed),
            'critical_failures': critical_failures,
            'important_failures': important_failures,
            'system_status': system_status,
            'check_results': {
                name: {
                    'passed': result.passed,
                    'message': result.message,
                    'level': result.level.name,
                    'details': result.details if isinstance(result.details, (str, dict)) else str(result.details)
                }
                for name, result in checks.items()
            }
        }
        
        summary_message = (
            f"{system_status}: Extended system check summary" if extended 
            else f"{system_status}: Basic system check summary"
        )
        
        checks['system_summary'] = CheckResult(
            passed=overall_passed,
            message=summary_message,
            level=CheckLevel.CRITICAL if not overall_passed else CheckLevel.INFORMATIONAL,
            details=summary_details
        )
        
        # Log critical failures immediately
        if logger:
            for name, result in checks.items():
                if not result.passed and result.level == CheckLevel.CRITICAL:
                    logger.error(
                        f"Critical check failed: {name} - {result.message}",
                        extra={'check_details': result.details}
                    )
            
            if not overall_passed:
                logger.warning(
                    f"System checks completed with failures - Status: {system_status}",
                    extra={'summary': summary_details}
                )
            else:
                logger.debug(
                    f"All system checks passed - Status: {system_status}",
                    extra={'summary': {
                        'total_checks': summary_details['total_checks'],
                        'passed_checks': summary_details['passed_checks']
                    }}
                )
        
        return checks
    
    except Exception as e:
        error_result = CheckResult(
            passed=False,
            message="System checks failed to complete",
            level=CheckLevel.CRITICAL,
            details={
                'error': str(e),
                'completed_checks': list(checks.keys()),
                'traceback': traceback.format_exc()
            }
        ).with_exception(e)
        
        checks['system_error'] = error_result
        
        if logger:
            logger.critical(
                "Fatal error during system checks",
                exc_info=True,
                extra={
                    'completed_checks': list(checks.keys()),
                    'error': str(e)
                }
            )
        
        return checks

def display_check_results(
    results: Dict[str, CheckResult], 
    logger: logging.Logger,
    extended: bool = False,
    include_performance: bool = False
) -> None:
    """
    Display check results in a styled table with improved formatting that matches
    the structure of run_system_checks output.
    
    Args:
        results: Dictionary of check results from run_system_checks()
        logger: Configured logger for recording the output
        extended: Whether extended checks were included (affects display)
        include_performance: Whether performance checks were included (affects display)
    """
    try:
        # Create the report table with dynamic title
        report_type = "Extended" if extended else "Basic"
        if include_performance:
            report_type += " (Performance)"
            
        table = Table(
            title=f"\n[bold]SYSTEM DIAGNOSTICS REPORT - {report_type}[/bold]",
            box=box.ROUNDED,
            header_style="bold white",
            border_style="white",
            title_style="bold yellow",
            title_justify="left",
            show_lines=True,
            expand=True,
            width=min(120, console.width - 4)
        )
        
        # Configure columns
        table.add_column("Check", style="bold cyan", width=25)
        table.add_column("Status", width=12, justify="center")
        table.add_column("Level", width=14, justify="center")
        table.add_column("Details", style="bold", min_width=50, max_width=80)
        
        # Group by check level in priority order
        for level in [CheckLevel.CRITICAL, CheckLevel.IMPORTANT, CheckLevel.INFORMATIONAL]:
            # Filter checks for this level, excluding summary/error entries
            level_rows = [
                (name, result) for name, result in results.items() 
                if result.level == level 
                and name not in ["system_summary", "system_error"]
            ]
            
            if not level_rows:
                continue
                
            # Add section header with colored background
            level_style = {
                CheckLevel.CRITICAL: "bold white on red",
                CheckLevel.IMPORTANT: "bold white on yellow",
                CheckLevel.INFORMATIONAL: "bold white on blue"
            }[level]
            
            table.add_row(
                Text(level.name, style=level_style),
                "",
                "",
                "",
                style=level_style
            )
            
            # Add checks for this level
            for name, result in level_rows:
                # Determine status styling
                if result.passed:
                    status_style = "bold green"
                    status_text = "PASS"
                else:
                    status_style = "bold red" if level == CheckLevel.CRITICAL else "bold yellow"
                    status_text = "FAIL" if level == CheckLevel.CRITICAL else "WARN"
                
                # Format details with special handling for specific checks
                details_lines = []
                
                # Main message
                details_lines.append(f"[white]{result.message}[/white]")
                
                # Enhanced detail formatting
                if isinstance(result.details, dict):
                    # Special handling for configuration system - show basic info only
                    if name == 'configuration_system':
                        if 'sections_loaded' in result.details:
                            details_lines.append(f"Sections: [bold green]{result.details['sections_loaded']}[/bold green], Parameters: [bold green]{result.details['total_parameters']}[/bold green]")
                        if 'active_preset' in result.details:
                            details_lines.append(f"Active preset: [bold green]{result.details['active_preset'] or 'default'}[/bold green]")
                        if 'model_type' in result.details:
                            details_lines.append(f"Model type: [bold green]{result.details['model_type']}[/bold green]")
                        # Note that detailed tables were already displayed by the wrapper
                    
                    # Special handling for model variants
                    elif name == 'model_variants':
                        if 'variant_names' in result.details:
                            details_lines.append(f"Available: [bold green]{', '.join(result.details['variant_names'])}[/bold green]")
                        if 'initialization_summary' in result.details:
                            summary = result.details['initialization_summary']
                            details_lines.append(f"Success rate: [bold green]{summary['successful']}/{summary['attempted']}[/bold green]")
                    
                    # Special handling for version info
                    elif 'version_info' in result.details:
                        versions = result.details['version_info']
                        details_lines.append("Dependencies:")
                        for pkg, info in versions.items():
                            status = "[bold green]OK" if info['compatible'] else "[bold red]FAIL"
                            details_lines.append(
                                f"  {status} [bold green]{pkg}: {info['version']}[/bold green]"
                                f"(requires {info['required_version'] or 'any'})"
                            )
                    
                    # General dict handling for other checks
                    else:
                        for key, value in result.details.items():
                            if key not in ['error', 'exception', 'traceback', 'variant_status', 'section_details', 'full_details']:
                                if isinstance(value, (int, float, str, bool)):
                                    details_lines.append(f"{key}: {value}")
                
                elif result.details and isinstance(result.details, str):
                    details_lines.append(f"{result.details}")
                
                # Add exception if present
                if result.exception:
                    details_lines.append(f"[bold red]Error: {str(result.exception)}[/bold red]")
                
                details_text = "\n".join(details_lines)
                
                # Add row to table
                table.add_row(
                    Text(name.replace("_", " ").title()), 
                    Text(status_text, style=status_style),
                    Text(level.name, style="bold yellow" if level == CheckLevel.IMPORTANT else "bold blue" if level == CheckLevel.INFORMATIONAL else "bold red"),
                    details_text
                )
        
        # Add summary/error rows if present
        if "system_summary" in results:
            summary = results["system_summary"]
            summary_style = "bold white on green" if summary.passed else "bold white on red"
            checks_run = summary.details.get('total_checks', 0)
            passed_checks = summary.details.get('passed_checks', 0)
            checks_critical = summary.details.get('critical_failures', 0)
            summary_status = summary.details.get('system_status', 'UNKNOWN')
            
            table.add_row(
                Text("SUMMARY", style="bold white on yellow"),
                Text("PASS" if summary.passed else "FAIL", style=summary_style),
                "",
                Text(
                    f"{passed_checks}/{checks_run} checks passed | "
                    f"{checks_critical} critical failures | "
                    f"Status: {summary_status}",
                    #style="white"
                    style=summary_style
                )
            )
        
        if "system_error" in results:
            error = results["system_error"]
            error_details = error.details.get('error', 'Unknown error')
            checks_completed = error.details.get('completed_checks', [])
            error_message = (f"{error.message}\n"
                             f"[bold yellow]{error_details}[/bold yellow]")
            table.add_row(
                Text("FATAL ERROR", style="bold white on red"),
                Text("ERROR", style="bold white on red"),
                "",
                Text(
                    #f"{error.message}\n"
                    #f"[bold yellow]{error.details.get('error', 'Unknown error')}[/bold yellow]\n"
                    #f"Completed checks: {', '.join(error.details.get('completed_checks', []))}",
                    #style="white"
                    f"{error_message}\n"
                    f"Completed checks: {', '.join(checks_completed)}",
                    style="bold white on red"
                )
            )
        
        # Print the main table
        console.print(table)
        
        # Enhanced logging - suppress redundant configuration/model messages
        if logger:
            # Log only the summary
            summary = results.get("system_summary")
            if summary:
                logger.debug(
                    f"System diagnostics completed: {summary.details['passed_checks']}/"
                    f"{summary.details['total_checks']} checks passed, "
                    f"{summary.details['critical_failures']} critical failures, "
                    f"Status: {summary.details.get('system_status', 'UNKNOWN')}"
                )
            
            # Log configuration and model status briefly
            if 'configuration_system' in results:
                config_result = results['configuration_system']
                if config_result.passed and isinstance(config_result.details, dict):
                    logger.debug(f"Configuration system: {config_result.details.get('sections_loaded', 0)} sections loaded")
            
            if 'model_variants' in results:
                model_result = results['model_variants']
                if model_result.passed and isinstance(model_result.details, dict):
                    variants = model_result.details.get('variant_names', [])
                    logger.debug(f"Model variants: {len(variants)} available ({', '.join(variants)})")
            
            # Log only critical failures
            critical_failures = [
                (name, result) for name, result in results.items()
                if not result.passed and result.level == CheckLevel.CRITICAL
                and name not in ["system_summary", "system_error"]
            ]
            
            if critical_failures:
                for name, result in critical_failures:
                    logger.critical(f"Critical check failed: {name} - {result.message}")
            
            # Log any system error
            if "system_error" in results:
                error = results["system_error"]
                logger.critical(f"System checks failed: {error.details.get('error', 'Unknown error')}")
    
    except Exception as e:
        error_msg = f"Failed to display check results: {str(e)}"
        
        #console.print(f"[bold red]{error_msg}[/bold red]")
        print(Fore.RED + Style.BRIGHT + f"{error_msg}")
        
        if logger:
            logger.critical(error_msg, exc_info=True)

# Individual check implementations
def check_python_version(min_version: Tuple[int, int] = (3, 8)) -> CheckResult:
    """Verify that the current Python version meets the minimum requirement."""
    try:
        # Leverage the version info from check_versions
        version_info = check_versions(include_optional=False)
        python_info = version_info.get('Python', {})
        
        if not python_info:
            return CheckResult(
                passed=False,
                message="Python version information not available",
                level=CheckLevel.CRITICAL
            )
        
        # Get version from the comprehensive check
        current_version = tuple(map(int, python_info['version'].split('.')[:2]))
        passed = current_version >= min_version
        
        message = (
            f"Python version {'meets' if passed else 'fails'} minimum requirement "
            f"({'.'.join(map(str, min_version))})"
        )
        
        base_result = CheckResult(
            passed=passed,
            message=message,
            level=CheckLevel.CRITICAL if not passed else CheckLevel.INFORMATIONAL,
            details=python_info.get('details')
        )
        
        return base_result.with_details(
            f"Current version: {python_info['version']}\n"
            f"Minimum required: {'.'.join(map(str, min_version))}\n"
            f"Status: {'Compatible' if passed else 'Incompatible'}"
        )
        
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Python version check failed",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Could not determine Python version: {str(e)}")
            .with_exception(e)
        )

def safe_version_compare(current_version: str, requirement: Optional[str]) -> bool:
    """Safely compare version against requirement."""
    if current_version in ['N/A', 'unknown', 'Available']:
        return current_version == 'Available'
    if not requirement:
        return True
    
    try:
        # Extract version number from requirement (remove >=, ==, etc.)
        req_version = requirement.replace('>=', '').replace('<=', '').replace('==', '').replace('>', '').replace('<', '').strip()
        
        if not PACKAGING_AVAILABLE:
            # Fallback comparison without packaging
            return str(current_version) >= req_version
        
        return pkg_version.parse(current_version) >= pkg_version.parse(req_version)
    except Exception:
        return False

def check_versions(include_optional: bool = True) -> Dict[str, Dict[str, Any]]:
    """
    Verify package versions with comprehensive dependency checking.
    Returns a dictionary containing version status for all dependencies.
    
    Args:
        include_optional: Whether to include optional dependencies in the check
        
    Returns:
        Dictionary with package names as keys and version info as values
    """
    version_info = {}
    
    # Core dependencies - get actual versions from VERSION_INFO
    core_deps = {
        'Python': (VERSION_INFO['python'], '>=3.7', True),
        'PyTorch': (VERSION_INFO['torch'], '>=1.8', True),
        'NumPy': (VERSION_INFO['numpy'], '>=1.19', True),
        'Pandas': (VERSION_INFO['pandas'], '>=1.2', True),
        'Scikit-learn': (VERSION_INFO['sklearn'], '>=0.24', True),
        'Optuna': (VERSION_INFO['optuna'], '>=2.8', True),
        'Rich': (VERSION_INFO['rich'], '>=10.0', True),
        'Plotly': (VERSION_INFO['plotly'], '>=5.0', False)
    }
    
    # Optional dependencies - all using safe_version now
    optional_deps = {
        'ONNX Runtime': (safe_version('onnxruntime') if OPTIONAL_DEPENDENCIES.get('onnxruntime', False) else 'N/A', '>=1.8', False),
        'ONNX': (VERSION_INFO['onnx'], '>=1.8', False),
        'NVIDIA ML': (safe_version('nvidia-ml-py') if OPTIONAL_DEPENDENCIES.get('nvml', False) else 'N/A', '>=11.0', False),
        'Torch JIT': ('Available' if OPTIONAL_DEPENDENCIES.get('torch_jit', False) else 'N/A', None, False),
        'Cryptography': (safe_version('cryptography') if OPTIONAL_DEPENDENCIES.get('crypto', False) else 'N/A', None, False),
        'Database': ('Available' if OPTIONAL_DEPENDENCIES.get('database', False) else 'N/A', None, False),
        'Sklearn Anomaly': ('Available' if OPTIONAL_DEPENDENCIES.get('sklearn_anomaly', False) else 'N/A', None, False),
        'Statsmodels': (safe_version('statsmodels') if OPTIONAL_DEPENDENCIES.get('statsmodels', False) else 'N/A', None, False),
        'Numba': (safe_version('numba') if OPTIONAL_DEPENDENCIES.get('numba', False) else 'N/A', None, False),
        'Memory Profiler': (safe_version('memory-profiler') if OPTIONAL_DEPENDENCIES.get('memory_profiler', False) else 'N/A', None, False),
        'Line Profiler': (safe_version('line-profiler') if OPTIONAL_DEPENDENCIES.get('line_profiler', False) else 'N/A', None, False),
        'Packaging': ('Available' if OPTIONAL_DEPENDENCIES.get('packaging', False) else 'N/A', None, False),
        'PSUtil': (VERSION_INFO['psutil'], '>=5.8', False)
    }
    
    # Check all dependencies
    all_deps = {**core_deps}
    if include_optional:
        all_deps.update(optional_deps)
    
    for name, (version, requirement, required) in all_deps.items():
        if not include_optional and not required:
            continue
            
        try:
            if version == 'N/A':
                status = 'MISSING'
                meets_req = False
            elif version == 'Available' or requirement is None:
                status = 'OK'
                meets_req = True
            elif version == 'unknown':
                status = 'UNKNOWN'
                meets_req = False
            else:
                meets_req = safe_version_compare(version, requirement)
                status = 'OK' if meets_req else 'WARNING'
                
        except Exception as e:
            meets_req = False
            status = 'ERROR'
            version = f"Error: {str(e)}"
        
        version_info[name] = {
            'version': version,
            'required_version': requirement,
            'status': status,
            'required': required,
            'description': get_dependency_description(name),
            'compatible': meets_req,
            'available': version not in ['N/A', 'unknown'] and not str(version).startswith('Error:')
        }
    
    return version_info

def check_torch() -> CheckResult:
    """Confirm that PyTorch is installed and operational."""
    try:
        # Use the comprehensive version check
        version_info = check_versions(include_optional=False)
        torch_info = version_info.get('PyTorch', {})
        
        if not torch_info:
            return CheckResult(
                passed=False,
                message="PyTorch version information not available",
                level=CheckLevel.CRITICAL
            )
        
        passed = torch_info.get('compatible', False)
        base_result = CheckResult(
            passed=passed,
            message=f"PyTorch is {'available and compatible' if passed else 'not properly installed or incompatible'}",
            level=CheckLevel.CRITICAL,
            details=torch_info.get('details')
        )
        
        if passed:
            return base_result.with_details(
                f"Version: {torch_info['version']}\n"
                f"Required: {torch_info['required_version'] or 'Not specified'}\n"
                f"Description: {torch_info['description']}"
            )
        return base_result.with_details(
            f"PyTorch check failed\n"
            f"Installed version: {torch_info.get('version', 'unknown')}\n"
            f"Required version: {torch_info.get('required_version', 'unknown')}"
        )
        
    except ImportError as e:
        return (
            CheckResult(
                passed=False,
                message="PyTorch is not installed",
                level=CheckLevel.CRITICAL
            )
            .with_details("PyTorch package not found in Python environment")
            .with_exception(e)
        )
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="PyTorch check failed unexpectedly",
                level=CheckLevel.CRITICAL
            )
            .with_details(str(e))
            .with_exception(e)
        )

def check_package_versions_wrapper(include_optional: bool = True) -> CheckResult:
    """
    Run comprehensive package version validation with rich output.
    
    Args:
        include_optional: Whether to include optional dependencies
        
    Returns:
        CheckResult object with detailed version information
    """
    try:
        version_info = check_versions(include_optional)
        details = []
        passed = True
        
        # Prepare detailed information
        for name, info in version_info.items():
            status_icon = "[PASS]" if info['status'] == 'OK' else "[WARN]" if info['status'] == 'WARNING' else "[FAIL]"
            req_text = f"(requires {info['required_version']})" if info['required_version'] else ""
            details.append(
                f"{status_icon} {name}: {info['version']} {req_text} - "
                f"{'Required' if info['required'] else 'Optional'}"
            )
            if info['required'] and not info['compatible']:
                passed = False
        
        # Create rich table for display
        table = Table(title="DEPENDENCY CHECK", title_justify="left", title_style="bold yellow", box=box.ROUNDED, show_lines=True)
        table.add_column("Package", style="bold cyan", no_wrap=True)
        table.add_column("Version", style="bold magenta")
        table.add_column("Type", style="bold green")
        table.add_column("Status", justify="center")
        table.add_column("Description", style="bold blue")
        
        for name, info in version_info.items():
            status_style = "bold green" if info['status'] == 'OK' else "bold yellow" if info['status'] == 'WARNING' else "bold red"
            required_style = "bold green" if info['required'] else "bold yellow"
            required_text = f"[{required_style}]{'Required' if info ['required'] else 'Optional'}[/{required_style}]"
            table.add_row(
                name,
                info['version'],
                required_text,
                f"[{status_style}]{info['status']}[/{status_style}]",
                info['description']
            )
        
        console.print(table)
        
        base_result = CheckResult(
            passed=passed,
            message="Package version check completed",
            level=CheckLevel.CRITICAL if not passed else CheckLevel.IMPORTANT
        )
        
        return (
            base_result
            .with_details("\n".join(details))
            .with_metadata({
                'version_info': version_info,
                'table': table,
                'summary': {
                    'total': len(version_info),
                    'passed': sum(1 for info in version_info.values() if info['status'] == 'OK'),
                    'warnings': sum(1 for info in version_info.values() if info['status'] == 'WARNING'),
                    'missing': sum(1 for info in version_info.values() if info['status'] == 'MISSING')
                }
            })
        )
        
    except ImportError as e:
        return (
            CheckResult(
                passed=False,
                message="Package version check failed - missing dependencies",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Required package not found: {str(e)}")
            .with_exception(e)
        )
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Package version check failed unexpectedly",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Error during version check: {str(e)}")
            .with_exception(e)
        )

def get_dependency_description(dep_name: str) -> str:
    """Get detailed description for a dependency."""
    descriptions = {
        # Core dependencies
        'Python': 'Python programming language runtime',
        'PyTorch': 'Deep learning framework (core dependency)',
        'NumPy': 'Fundamental package for numerical computing',
        'Pandas': 'Data manipulation and analysis toolkit',
        'Scikit-learn': 'Machine learning algorithms and utilities',
        'Optuna': 'Hyperparameter optimization framework',
        'Rich': 'Rich text and beautiful formatting in terminal',
        'Plotly': 'Interactive visualization library',
        'PSUtil': 'Process and system monitoring utilities',
        
        # Optional dependencies
        'ONNX Runtime': 'Cross-platform inference engine for ONNX models',
        'ONNX': 'Open Neural Network Exchange format',
        'NVIDIA ML': 'NVIDIA Management Library for GPU monitoring',
        'Torch JIT': 'PyTorch Just-In-Time compilation for model optimization',
        'Cryptography': 'Cryptographic primitives for model security',
        'Database': 'Database connectivity for model storage and retrieval',
        'Sklearn Anomaly': 'Scikit-learn anomaly detection algorithms',
        'Statsmodels': 'Statistical modeling and econometrics',
        'Numba': 'JIT compiler for numerical functions',
        'Memory Profiler': 'Memory usage tracking and analysis',
        'Line Profiler': 'Line-by-line performance profiling',
        'Packaging': 'Core utilities for Python packaging'
    }
    return descriptions.get(dep_name, 'Additional functionality')

def check_directory_access_wrapper() -> CheckResult:
    """Verify directory access using the setup_directories function."""
    try:
        dirs = setup_directories(logger)
        access_issues = []
        
        # Check each directory
        for name, path in dirs.items():
            if not path.exists():
                access_issues.append(f"Missing: {name} ({path})")
            elif not os.access(path, os.W_OK):
                access_issues.append(f"No write access: {name} ({path})")
        
        passed = len(access_issues) == 0
        base_result = CheckResult(
            passed=passed,
            message="Directory access verification",
            level=CheckLevel.CRITICAL
        )
        
        if passed:
            details = "\n".join(f"{name}: {path}" for name, path in dirs.items())
            return base_result.with_details(details)
        else:
            details = "\n".join([
                "Directory access issues found:",
                *access_issues,
                "",
                "All directories:",
                *[f"- {name}: {path}" for name, path in dirs.items()]
            ])
            return base_result.with_details(details)
            
    except PermissionError as e:
        return (
            CheckResult(
                passed=False,
                message="Permission denied for directory access",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Permission error: {str(e)}")
            .with_exception(e)
        )
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Directory access check failed unexpectedly",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Error: {str(e)}")
            .with_exception(e)
        )

def check_logging_setup() -> CheckResult:
    """
    Verify that logging is configured according to setup_logging().

    Checks:
    - At least one file handler with UTF-8 encoding
    - Log file exists
    - At least one console handler using UnicodeStreamHandler
    - Produces a compliance score (0-100%)
    - Returns human-readable feedback
    - Colorized terminal output (auto-disabled if not TTY)
    """
    try:
        # Initialize base result
        base_result = CheckResult(
            passed=False,
            message="Logging configuration check",
            level=CheckLevel.IMPORTANT
        )

        # Detect if output is a TTY (interactive terminal)
        use_color = sys.stdout.isatty()

        # Get logger and handlers
        logger = logging.getLogger(__name__)
        handlers = logger.handlers

        # Initialize check data
        check_data: Dict[str, any] = {
            'handlers': [],
            'checks': {
                'file_handler_found': False,
                'file_handler_utf8': False,
                'file_handler_exists': False,
                'console_handler_found': False,
                'console_handler_unicode': False
            },
            'feedback': [],
            'compliance_score': 0
        }

        # If no handlers, return immediately
        if not handlers:
            return (
                base_result
                .with_details({
                    'error': 'No handlers configured',
                    'compliance_score': 0,
                    'feedback': ['No logging handlers configured — run setup_logging()']
                })
                .with_message("No logging handlers configured")
            )

        # Analyze each handler
        for handler in handlers:
            handler_info = {
                'type': handler.__class__.__name__,
                'level': logging.getLevelName(handler.level)
            }

            # File handler checks
            if isinstance(handler, logging.FileHandler):
                check_data['checks']['file_handler_found'] = True
                encoding = getattr(handler, 'encoding', '').lower()
                if encoding == 'utf-8':
                    check_data['checks']['file_handler_utf8'] = True

                log_path = Path(getattr(handler, 'baseFilename', ''))
                if log_path.exists():
                    check_data['checks']['file_handler_exists'] = True

                handler_info.update({
                    'encoding': encoding,
                    'file_path': str(log_path),
                    'exists': log_path.exists()
                })

            # Console handler checks (must be UnicodeStreamHandler)
            elif isinstance(handler, UnicodeStreamHandler):
                check_data['checks']['console_handler_found'] = True
                check_data['checks']['console_handler_unicode'] = True
                handler_info['stream'] = getattr(handler.stream, 'name', str(handler.stream))

            check_data['handlers'].append(handler_info)

        # Calculate compliance score
        total_checks = len(check_data['checks'])
        passed_checks = sum(check_data['checks'].values())
        compliance_score = int((passed_checks / total_checks) * 100)
        passed = compliance_score == 100

        # Generate feedback messages
        feedback = []
        if not check_data['checks']['file_handler_found']:
            feedback.append("Missing file handler")
        if not check_data['checks']['file_handler_utf8']:
            feedback.append("File handler not using UTF-8 encoding")
        if not check_data['checks']['file_handler_exists']:
            feedback.append("Log file does not exist")
        if not check_data['checks']['console_handler_found']:
            feedback.append("Missing console handler")
        if not check_data['checks']['console_handler_unicode']:
            feedback.append("Console handler is not Unicode-safe (must use UnicodeStreamHandler)")

        # Prepare final details
        details = {
            **check_data,
            'compliance_score': compliance_score,
            'feedback': feedback,
            'passed_checks': passed_checks,
            'total_checks': total_checks
        }

        return (
            base_result
            .with_passed(passed)
            .with_details(details)
            .with_message(f"Logging configuration {'passed' if passed else 'failed'} ({compliance_score}%)")
        )

    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Logging check failed",
                level=CheckLevel.IMPORTANT
            )
            .with_details({
                'error': str(e),
                'compliance_score': 0,
                'feedback': ['Exception occurred during logging setup check']
            })
            .with_exception(e)
        )

def check_hardware(min_disk_gb: float = 1.0, include_memory_usage: bool = False) -> Dict[str, Dict[str, Any]]:
    """
    Low-level engine for hardware/system validation.
    Collects and validates raw hardware/system information.
    
    Args:
        min_disk_gb: Minimum required disk space in GB
        include_memory_usage: Whether to include current memory usage statistics
    
    Returns:
        Dictionary with detailed metadata for each hardware/system dependency
    """
    hardware_data = {}
    
    # Disk space check
    disk_info = {}
    try:
        usage = shutil.disk_usage('.')
        free_gb = usage.free / (1024**3)
        disk_info.update({
            'available': free_gb >= min_disk_gb,
            'free_gb': free_gb,
            'min_required_gb': min_disk_gb,
            'total_gb': usage.total / (1024**3),
            'used_gb': usage.used / (1024**3),
            'status': 'PASS' if free_gb >= min_disk_gb else 'FAIL',
            'required': True,
            'type': 'storage'
        })
    except Exception as e:
        disk_info.update({
            'available': False,
            'status': 'FAIL',
            'error': str(e),
            'required': True,
            'type': 'storage'
        })
    hardware_data['disk_space'] = disk_info
    
    # Enhanced CPU cores check with usage data
    cpu_info = {}
    try:
        logical_cores = psutil.cpu_count(logical=True)
        physical_cores = psutil.cpu_count(logical=False)
        
        # Get CPU frequency with proper error handling
        cpu_frequency = None
        cpu_frequency_ghz = 0.0
        cpu_frequency_mhz = 0.0
        try:
            cpu_freq = psutil.cpu_freq()
            if cpu_freq:
                cpu_frequency = cpu_freq._asdict()
                cpu_frequency_ghz = cpu_frequency.get('current', 0.0) / 1000 if cpu_frequency.get('current') else 0.0
                cpu_frequency_mhz = cpu_frequency.get('current', 0.0)
        except (AttributeError, NotImplementedError):
            # Some systems may not support CPU frequency monitoring
            pass
        
        cpu_info.update({
            'available': True,
            'logical_cores': logical_cores,
            'physical_cores': physical_cores,
            'hyperthreading': logical_cores != physical_cores if logical_cores and physical_cores else False,
            'status': 'PASS',
            'required': False,
            'type': 'processor',
            'capacity': {
                # Static capacity information
                'logical_cores': logical_cores,
                'physical_cores': physical_cores,
                'frequency_ghz': round(cpu_frequency_ghz, 2) if cpu_frequency_ghz else None,
                'frequency_mhz': round(cpu_frequency_mhz, 2) if cpu_frequency_mhz else None,
                'max_frequency_ghz': round(cpu_frequency.get('max', 0.0) / 1000, 2) if cpu_frequency and cpu_frequency.get('max') else None,
                'min_frequency_ghz': round(cpu_frequency.get('min', 0.0) / 1000, 2) if cpu_frequency and cpu_frequency.get('min') else None
            }
        })
        
        # Add current CPU usage if requested
        if include_memory_usage:
            try:
                # Get CPU usage percentages
                # Shorter interval for responsiveness
                cpu_percent = psutil.cpu_percent(interval=0.1)
                
                # Get CPU times
                cpu_times = None
                try:
                    cpu_times_percent = psutil.cpu_times_percent(interval=0.1)
                    if cpu_times_percent:
                        cpu_times = cpu_times_percent._asdict()
                except (AttributeError, NotImplementedError):
                    pass
                
                # Get CPU stats
                cpu_stats_data = None
                try:
                    cpu_stats = psutil.cpu_stats()
                    if cpu_stats:
                        cpu_stats_data = cpu_stats._asdict()
                except (AttributeError, NotImplementedError):
                    pass
                
                # Get per-core usage
                per_cpu_percent = None
                try:
                    per_cpu_percent = psutil.cpu_percent(interval=0.1, percpu=True)
                except (AttributeError, NotImplementedError):
                    pass
                
                cpu_info['current_usage'] = {
                    'timestamp': datetime.now().isoformat(),
                    'cpu_percent_total': cpu_percent,
                    'cpu_percent_per_core': per_cpu_percent,
                    'cpu_times': cpu_times,
                    'cpu_stats': cpu_stats_data,
                    'frequency_current_ghz': round(cpu_frequency_ghz, 2) if cpu_frequency_ghz else None,
                    'frequency_current_mhz': round(cpu_frequency_mhz, 2) if cpu_frequency_mhz else None
                }
                
            except Exception as usage_error:
                cpu_info['current_usage_error'] = str(usage_error)
        
    except Exception as e:
        cpu_info.update({
            'available': False,
            'status': 'FAIL',
            'error': str(e),
            'required': False,
            'type': 'processor'
        })
    hardware_data['cpu_cores'] = cpu_info
    
    # Enhanced System RAM check with usage data
    ram_info = {}
    try:
        ram = psutil.virtual_memory()
        swap = psutil.swap_memory()
        
        ram_info.update({
            'available': True,
            'ram_total_gb': ram.total / (1024**3),
            'ram_available_gb': ram.available / (1024**3),
            'ram_used_gb': ram.used / (1024**3),
            'ram_percent': ram.percent,
            'swap_total_gb': swap.total / (1024**3),
            'swap_used_gb': swap.used / (1024**3),
            'swap_percent': swap.percent,
            'status': 'PASS',
            'required': False,
            'type': 'memory',
            'capacity': {
                # Static capacity information
                'total_bytes': ram.total,
                'total_gb': ram.total / (1024**3),
                'swap_total_bytes': swap.total,
                'swap_total_gb': swap.total / (1024**3)
            }
        })
        
        # Add current usage data if requested
        if include_memory_usage:
            ram_info['current_usage'] = {
                'timestamp': datetime.now().isoformat(),
                'available_bytes': ram.available,
                'used_bytes': ram.used,
                'percent_used': ram.percent,
                'available_gb': ram.available / (1024**3),
                'used_gb': ram.used / (1024**3),
                'swap_used_bytes': swap.used,
                'swap_used_gb': swap.used / (1024**3),
                'swap_percent': swap.percent
            }
            
    except Exception as e:
        ram_info.update({
            'available': False,
            'status': 'FAIL',
            'error': str(e),
            'required': False,
            'type': 'memory'
        })
    hardware_data['system_ram'] = ram_info
    
    # System architecture check
    arch_info = {}
    try:
        arch_info.update({
            'available': True,
            'architecture': platform.architecture()[0],
            'machine': platform.machine(),
            'system': platform.system(),
            'release': platform.release(),
            'processor': platform.processor(),
            'python_build': ' '.join(platform.python_build()),
            'word_size': platform.architecture()[0],
            'status': 'PASS',
            'required': False,
            'type': 'platform'
        })
    except Exception as e:
        arch_info.update({
            'available': False,
            'status': 'FAIL',
            'error': str(e),
            'required': False,
            'type': 'platform'
        })
    hardware_data['system_architecture'] = arch_info
    
    # Enhanced CUDA check with memory capacity
    cuda_info = {}
    try:
        cuda_available = torch.cuda.is_available()
        cuda_info.update({
            'available': cuda_available,
            'status': 'PASS' if cuda_available else 'WARN',
            'required': False,
            'type': 'gpu'
        })
        
        if cuda_available:
            cuda_info.update({
                'cuda_version': torch.version.cuda,
                'cudnn_version': torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'N/A',
                'gpu_count': torch.cuda.device_count(),
                'gpus': []
            })
            
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                gpu_data = {
                    'name': props.name,
                    'compute_capability': f"{props.major}.{props.minor}",
                    'memory_gb': props.total_memory / (1024**3),
                    'total_memory_bytes': props.total_memory,
                    'multiprocessors': props.multi_processor_count,
                    'capacity': {
                        # Static memory capacity
                        'total_bytes': props.total_memory,
                        'total_gb': props.total_memory / (1024**3)
                    }
                }
                
                # Add current GPU memory usage if requested
                if include_memory_usage:
                    try:
                        torch.cuda.set_device(i)
                        gpu_data['current_usage'] = {
                            'timestamp': datetime.now().isoformat(),
                            'allocated_bytes': torch.cuda.memory_allocated(i),
                            'reserved_bytes': torch.cuda.memory_reserved(i),
                            'max_allocated_bytes': torch.cuda.max_memory_allocated(i),
                            'allocated_mb': torch.cuda.memory_allocated(i) / (1024**2),
                            'reserved_mb': torch.cuda.memory_reserved(i) / (1024**2),
                            'max_allocated_mb': torch.cuda.max_memory_allocated(i) / (1024**2),
                            'percent_allocated': (torch.cuda.memory_allocated(i) / props.total_memory * 100) if props.total_memory > 0 else 0
                        }
                    except Exception as e:
                        gpu_data['current_usage_error'] = str(e)
                
                cuda_info['gpus'].append(gpu_data)
        else:
            cuda_info['details'] = "CUDA not available - Using CPU"
            
    except ImportError:
        cuda_info.update({
            'available': False,
            'status': 'WARN',
            'details': "PyTorch not installed",
            'required': False,
            'type': 'gpu'
        })
    except Exception as e:
        cuda_info.update({
            'available': False,
            'status': 'FAIL',
            'error': str(e),
            'required': False,
            'type': 'gpu'
        })
    hardware_data['cuda'] = cuda_info
    
    # Add process memory usage if requested
    if include_memory_usage:
        try:
            proc = psutil.Process()
            process_mem = proc.memory_info()
            hardware_data['process_memory'] = {
                'timestamp': datetime.now().isoformat(),
                'rss_bytes': process_mem.rss,
                'vms_bytes': process_mem.vms,
                'rss_mb': process_mem.rss / (1024**2),
                'vms_mb': process_mem.vms / (1024**2),
                'percent': proc.memory_percent(),
                # Informational, not a pass/fail check
                'status': 'INFO',
                'type': 'process'
            }
        except Exception as e:
            hardware_data['process_memory'] = {
                'error': str(e),
                'status': 'FAIL',
                'type': 'process'
            }
    
    return hardware_data

def check_hardware_wrapper(min_disk_gb: float = 1.0, include_memory_usage: bool = False) -> CheckResult:
    """
    High-level wrapper and presentation layer for hardware checks.
    Formats the raw data and produces a consistent CheckResult.
    
    Args:
        min_disk_gb: Minimum required disk space in GB
        include_memory_usage: Whether to include current memory usage statistics
    """
    try:
        # Get raw hardware data
        hardware_data = check_hardware(min_disk_gb, include_memory_usage)
        
        # Analyze results and build summary
        summary = {
            'total': len(hardware_data),
            'passed': 0,
            'warnings': 0,
            'failed': 0,
            'required_failed': 0,
            'informational': 0
        }
        
        details_lines = []
        overall_passed = True
        
        # Create rich table for formatted output
        table_title = "Hardware/System Check Results" + (" with Memory Usage" if include_memory_usage else "")
        table = Table(title=table_title, show_header=True, header_style="bold magenta")
        table.add_column("Component", style="cyan", no_wrap=True)
        table.add_column("Status", justify="center")
        table.add_column("Details", style="green")
        
        for component, data in hardware_data.items():
            status = data.get('status', 'UNKNOWN')
            
            # Update summary counts
            if status == 'PASS':
                summary['passed'] += 1
            elif status == 'WARN':
                summary['warnings'] += 1
            elif status == 'FAIL':
                summary['failed'] += 1
                if data.get('required', False):
                    summary['required_failed'] += 1
                    overall_passed = False
            elif status == 'INFO':
                summary['informational'] += 1
            
            # Build details string
            comp_details = format_component_details(component, data, include_memory_usage)
            details_lines.append(comp_details)
            
            # Add to rich table
            status_icon = get_status_icon(status)
            table.add_row(component.replace('_', ' ').title(), status_icon, comp_details)
        
        # Build final details string
        details_summary = f"Hardware Check Summary:\n"
        details_summary += f"Total Checks: {summary['total']}, "
        details_summary += f"Passed: {summary['passed']}, "
        details_summary += f"Warnings: {summary['warnings']}, "
        details_summary += f"Failed: {summary['failed']}"
        if include_memory_usage:
            details_summary += f", Informational: {summary['informational']}"
        details_summary += f"\n\n"
        details_summary += "\n".join(details_lines)
        
        # Create CheckResult
        result = CheckResult(
            passed=overall_passed,
            message="Hardware/System check completed" + (" with memory usage" if include_memory_usage else ""),
            level=CheckLevel.IMPORTANT if summary['required_failed'] > 0 else CheckLevel.INFORMATIONAL,
            details=details_summary
        )
        
        # Add metadata
        result.with_metadata({
            'hardware_data': hardware_data,
            'summary': summary,
            'rich_table': table,
            'include_memory_usage': include_memory_usage
        })
        
        return result
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Hardware check failed unexpectedly",
            level=CheckLevel.CRITICAL,
            details=str(e)
        ).with_exception(e)

def format_component_details(component: str, data: Dict[str, Any], include_memory_usage: bool = False) -> str:
    """Format component details for human-readable output."""
    status = data.get('status', 'UNKNOWN')
    
    if component == 'disk_space':
        if status == 'PASS':
            return f"[OK] Free: {data['free_gb']:.1f}GB (Required: {data['min_required_gb']}GB)"
        elif status == 'FAIL':
            if 'error' in data:
                return f"[X] Error: {data['error']}"
            else:
                return f"[X] Insufficient: {data['free_gb']:.1f}GB (Required: {data['min_required_gb']}GB)"
    
    elif component == 'cpu_cores':
        if status == 'PASS':
            ht_status = "Enabled" if data['hyperthreading'] else "Disabled"
            return f"[OK] {data['logical_cores']} logical, {data['physical_cores']} physical cores ({ht_status})"
        else:
            return f"[X] Error: {data.get('error', 'Unknown error')}"
    
    elif component == 'system_ram':
        if status == 'PASS':
            base_info = f"[OK] RAM: {data['ram_total_gb']:.1f}GB total"
            if include_memory_usage and 'current_usage' in data:
                usage = data['current_usage']
                base_info += f", {usage['used_gb']:.1f}GB used ({usage['percent_used']:.1f}%)"
            else:
                base_info += f", {data['ram_available_gb']:.1f}GB available"
            return base_info
        else:
            return f"[X] Error: {data.get('error', 'Unknown error')}"
    
    elif component == 'system_architecture':
        if status == 'PASS':
            return f"[OK] {data['system']} {data['release']} ({data['architecture']})"
        else:
            return f"[X] Error: {data.get('error', 'Unknown error')}"
    
    elif component == 'cuda':
        if status == 'PASS':
            gpu_count = data.get('gpu_count', 0)
            if gpu_count == 0:
                return "[OK] CUDA available (no GPUs detected)"
            
            gpu_info = f"{gpu_count} GPU(s): "
            gpu_details = []
            for i, gpu in enumerate(data.get('gpus', [])):
                gpu_text = gpu['name']
                if include_memory_usage and 'current_usage' in gpu:
                    usage = gpu['current_usage']
                    gpu_text += f" ({usage['allocated_mb']:.0f}MB/{gpu['memory_gb']:.1f}GB)"
                gpu_details.append(gpu_text)
            
            gpu_info += ", ".join(gpu_details)
            return f"[OK] CUDA available - {gpu_info}"
        elif status == 'WARN':
            return f"[i] {data.get('details', 'CUDA not available')}"
        else:
            return f"[X] Error: {data.get('error', 'Unknown error')}"
    
    elif component == 'process_memory':
        if status == 'INFO' and include_memory_usage:
            return f"Process: {data['rss_mb']:.1f}MB RSS, {data['vms_mb']:.1f}MB VMS ({data['percent']:.1f}%)"
        elif status == 'FAIL':
            return f"[X] Process memory error: {data.get('error', 'Unknown error')}"
        else:
            return "Process memory info not available"
    
    return f"[?] Unknown component: {component}"

def get_status_icon(status: str) -> str:
    """Get status icon for rich table."""
    icons = {
        'PASS': '[green][OK][/green]',
        'WARN': '[yellow][!][/yellow]',
        'FAIL': '[red][X][/red]',
        'INFO': '[blue][i][/blue]',
        'UNKNOWN': '[grey][?][/grey]'
    }
    return icons.get(status, '[grey]?[/grey]')

def check_seed_config(hardware_data: Optional[Dict[str, Any]] = None) -> CheckResult:
    """
    Verify that reproducibility seeds and related configurations are set.

    Checks:
    - PYTHONHASHSEED environment variable set and numeric
    - CUBLAS_WORKSPACE_CONFIG set to ':4096:8'
    - NumPy RNG available and seeded
    - PyTorch RNG available, CUDA deterministic mode on, benchmark off
    - TensorFlow RNG available (if installed) and seeded
    - Hardware-aware CUDA configurations
    - Per-GPU seed validation for multi-GPU setups
    - Produces compliance score (0-100%)
    - Human-readable feedback with colorized PASS/WARN/FAIL
    - Auto-disables colors if not in a TTY
    - Structured details for programmatic use
    - Graceful handling and exception safety
    
    Args:
        hardware_data: Hardware information from check_hardware() (optional)
        
    Returns:
        CheckResult with detailed seed configuration status
    """
    try:
        # Get hardware info if not provided
        if hardware_data is None:
            try:
                hardware_data = check_hardware(include_memory_usage=False)
            except Exception:
                hardware_data = {}
        
        # Extract hardware context
        cuda_info = hardware_data.get('cuda', {})
        cuda_available = cuda_info.get('available', False)
        gpu_count = cuda_info.get('gpu_count', 0)
        gpus = cuda_info.get('gpus', [])
        
        # Initialize base result
        base_result = CheckResult(
            passed=False,
            message="Reproducibility configuration check",
            level=CheckLevel.IMPORTANT
        )
        
        # Initialize check data with hardware awareness
        check_data: Dict[str, any] = {
            'hardware_context': {
                'cuda_available': cuda_available,
                'gpu_count': gpu_count,
                'optimization_level': 'high' if cuda_available else 'standard'
            },
            'checks': {
                'PYTHONHASHSEED': {'passed': False, 'value': None, 'expected': 'numeric'},
                'CUBLAS_WORKSPACE_CONFIG': {'passed': False, 'value': None, 'expected': ':4096:8'},
                'numpy_rng': {'passed': False, 'value': None, 'seeded': False},
                'torch_rng': {'passed': False, 'value': None, 'seeded': False},
                'torch_cuda': {
                    'deterministic': False,
                    'benchmark': None,
                    'tf32_disabled': None,
                    'passed': False,
                    'per_gpu_seeds': [],
                    'global_seed_set': False
                },
                'tensorflow_rng': {'passed': False, 'value': None, 'seeded': False}
            },
            'weights': {
                'PYTHONHASHSEED': 20,
                'CUBLAS_WORKSPACE_CONFIG': 25 if cuda_available else 10,
                'numpy_rng': 15,
                'torch_rng': 15,
                'torch_cuda': 20 if cuda_available else 5,
                # Optional but beneficial
                'tensorflow_rng': 5
            },
            'feedback': [],
            'compliance_score': 0,
            'recommendations': []
        }
        
        # Check PYTHONHASHSEED
        hash_seed = os.environ.get('PYTHONHASHSEED')
        check_data['checks']['PYTHONHASHSEED']['value'] = hash_seed or "Not set"
        if hash_seed is not None and hash_seed.isdigit():
            check_data['checks']['PYTHONHASHSEED']['passed'] = True
        else:
            check_data['feedback'].append("PYTHONHASHSEED not set or invalid - required for hash reproducibility")
            check_data['recommendations'].append("Set PYTHONHASHSEED environment variable to a numeric value (e.g., '42')")
        
        # Check CUBLAS_WORKSPACE_CONFIG (critical for CUDA reproducibility)
        cublas_cfg = os.environ.get('CUBLAS_WORKSPACE_CONFIG')
        check_data['checks']['CUBLAS_WORKSPACE_CONFIG']['value'] = cublas_cfg or "Not set"
        if cublas_cfg == ':4096:8':
            check_data['checks']['CUBLAS_WORKSPACE_CONFIG']['passed'] = True
        else:
            if cuda_available:
                check_data['feedback'].append("CUBLAS_WORKSPACE_CONFIG not set to ':4096:8' - required for CUDA reproducibility")
                check_data['recommendations'].append("Set CUBLAS_WORKSPACE_CONFIG=':4096:8' for deterministic CUDA operations")
            else:
                # Less critical for CPU-only systems
                check_data['checks']['CUBLAS_WORKSPACE_CONFIG']['passed'] = True
        
        # Check NumPy RNG with seed validation
        try:
            # Test NumPy RNG availability
            test_array = np.random.rand(2)
            check_data['checks']['numpy_rng']['passed'] = True
            check_data['checks']['numpy_rng']['value'] = "Available"
            
            # Try to detect if seeded (basic check)
            # Temporary seed
            np.random.seed(12345)
            test1 = np.random.rand(5)
            # Same seed
            np.random.seed(12345)
            test2 = np.random.rand(5)
            if np.array_equal(test1, test2):
                check_data['checks']['numpy_rng']['seeded'] = True
            else:
                check_data['feedback'].append("NumPy RNG may not be properly seeded")
                check_data['recommendations'].append("Call np.random.seed(seed_value) to ensure reproducibility")
                
        except Exception as e:
            check_data['checks']['numpy_rng']['value'] = f"Error: {str(e)}"
            check_data['feedback'].append("NumPy RNG not available or malfunctioning")
        
        # Check PyTorch RNG with comprehensive validation
        try:
            # Test PyTorch RNG availability
            test_tensor = torch.rand(2)
            check_data['checks']['torch_rng']['passed'] = True
            check_data['checks']['torch_rng']['value'] = "Available"
            
            # Test seeding
            torch.manual_seed(12345)
            test1 = torch.rand(5)
            torch.manual_seed(12345)
            test2 = torch.rand(5)
            if torch.equal(test1, test2):
                check_data['checks']['torch_rng']['seeded'] = True
            else:
                check_data['feedback'].append("PyTorch RNG may not be properly seeded")
                check_data['recommendations'].append("Call torch.manual_seed(seed_value) for CPU reproducibility")
            
        except Exception as e:
            check_data['checks']['torch_rng']['value'] = f"Error: {str(e)}"
            check_data['feedback'].append("PyTorch RNG not available or malfunctioning")
        
        # Comprehensive CUDA reproducibility checks
        if cuda_available and gpu_count > 0:
            try:
                # Check deterministic mode
                check_data['checks']['torch_cuda']['deterministic'] = torch.backends.cudnn.deterministic
                if not torch.backends.cudnn.deterministic:
                    check_data['feedback'].append("CUDA deterministic mode disabled - may cause non-reproducible results")
                    check_data['recommendations'].append("Enable with torch.backends.cudnn.deterministic = True")
                
                # Check benchmark mode (should be disabled for reproducibility)
                check_data['checks']['torch_cuda']['benchmark'] = not torch.backends.cudnn.benchmark
                if torch.backends.cudnn.benchmark:
                    check_data['feedback'].append("CUDA benchmark mode enabled - may cause non-reproducible results")
                    check_data['recommendations'].append("Disable with torch.backends.cudnn.benchmark = False")
                
                # Check TF32 settings for modern GPUs
                tf32_properly_configured = True
                for gpu in gpus:
                    compute_cap = float(gpu.get('compute_capability', '0.0'))
                    # Tensor cores available
                    if compute_cap >= 7.0:
                        try:
                            tf32_disabled = not torch.backends.cuda.matmul.allow_tf32
                            check_data['checks']['torch_cuda']['tf32_disabled'] = tf32_disabled
                            if not tf32_disabled:
                                tf32_properly_configured = False
                                check_data['feedback'].append("TF32 enabled on capable GPU - may reduce reproducibility precision")
                                check_data['recommendations'].append("Disable with torch.backends.cuda.matmul.allow_tf32 = False for maximum reproducibility")
                        except AttributeError:
                            # TF32 control not available in this PyTorch version
                            pass
                        break
                
                # Test CUDA seeding
                if torch.cuda.is_available():
                    try:
                        torch.cuda.manual_seed(12345)
                        test1 = torch.cuda.FloatTensor(5).uniform_()
                        torch.cuda.manual_seed(12345)
                        test2 = torch.cuda.FloatTensor(5).uniform_()
                        if torch.equal(test1, test2):
                            check_data['checks']['torch_cuda']['global_seed_set'] = True
                        else:
                            check_data['feedback'].append("CUDA global seed may not be properly set")
                            check_data['recommendations'].append("Call torch.cuda.manual_seed_all(seed) for multi-GPU reproducibility")
                    except Exception as cuda_seed_error:
                        check_data['feedback'].append(f"CUDA seed test failed: {cuda_seed_error}")
                
                # Validate per-GPU seeding for multi-GPU setups
                if gpu_count > 1:
                    per_gpu_seeds_valid = []
                    for gpu_id in range(gpu_count):
                        try:
                            torch.cuda.set_device(gpu_id)
                            torch.cuda.manual_seed(12345 + gpu_id)
                            test_tensor = torch.cuda.FloatTensor(3).uniform_()
                            per_gpu_seeds_valid.append(True)
                        except Exception:
                            per_gpu_seeds_valid.append(False)
                            check_data['feedback'].append(f"Per-GPU seeding failed for GPU {gpu_id}")
                    
                    check_data['checks']['torch_cuda']['per_gpu_seeds'] = per_gpu_seeds_valid
                    if not all(per_gpu_seeds_valid):
                        check_data['recommendations'].append("Ensure per-GPU seeding with individual torch.cuda.manual_seed() calls")
                
                # Calculate CUDA check pass status
                cuda_checks_passed = (
                    check_data['checks']['torch_cuda']['deterministic'] and
                    check_data['checks']['torch_cuda']['benchmark'] and
                    check_data['checks']['torch_cuda']['global_seed_set']
                )
                
                # For multi-GPU, also require per-GPU seeds
                if gpu_count > 1:
                    cuda_checks_passed = cuda_checks_passed and all(check_data['checks']['torch_cuda']['per_gpu_seeds'])
                
                check_data['checks']['torch_cuda']['passed'] = cuda_checks_passed
                
            except Exception as cuda_error:
                check_data['feedback'].append(f"CUDA configuration check failed: {cuda_error}")
                check_data['checks']['torch_cuda']['passed'] = False
        else:
            # CPU-only system - CUDA checks not applicable
            check_data['checks']['torch_cuda']['passed'] = True
            # Not applicable for CPU
            check_data['checks']['torch_cuda']['deterministic'] = True
        
        # Check TensorFlow RNG (optional but comprehensive)
        try:
            version_info = check_versions(include_optional=False)
            tf_available = any('tensorflow' in str(v).lower() for v in version_info.values())
            
            if tf_available:
                import tensorflow as tf
                # Test TensorFlow RNG
                test_tensor = tf.random.uniform((2,))
                check_data['checks']['tensorflow_rng']['passed'] = True
                check_data['checks']['tensorflow_rng']['value'] = "Available"
                
                # Test seeding
                tf.random.set_seed(12345)
                test1 = tf.random.uniform((5,))
                tf.random.set_seed(12345)
                test2 = tf.random.uniform((5,))
                if tf.reduce_all(tf.equal(test1, test2)):
                    check_data['checks']['tensorflow_rng']['seeded'] = True
                else:
                    check_data['feedback'].append("TensorFlow RNG may not be properly seeded")
                    check_data['recommendations'].append("Call tf.random.set_seed(seed_value) for TensorFlow reproducibility")
            else:
                # TensorFlow not installed - not required
                check_data['checks']['tensorflow_rng']['passed'] = True
                check_data['checks']['tensorflow_rng']['value'] = "Not installed (optional)"
                
        except ImportError:
            check_data['checks']['tensorflow_rng']['value'] = "Not installed (optional)"
            # Not required
            check_data['checks']['tensorflow_rng']['passed'] = True
        except Exception as tf_error:
            check_data['checks']['tensorflow_rng']['value'] = f"Error: {str(tf_error)}"
            check_data['feedback'].append("TensorFlow RNG check failed")
        
        # Calculate weighted compliance score
        total_points = sum(check_data['weights'].values())
        earned_points = 0
        
        for check_name, weight in check_data['weights'].items():
            check_status = check_data['checks'][check_name]
            if check_status['passed']:
                earned_points += weight
            # Partial credit for some configurations
            elif check_name == 'torch_cuda' and cuda_available:
                # Partial credit based on individual CUDA settings
                partial_score = 0
                if check_status['deterministic']:
                    partial_score += 0.4
                if check_status['benchmark']:
                    partial_score += 0.3
                if check_status['global_seed_set']:
                    partial_score += 0.3
                earned_points += weight * partial_score
        
        compliance_score = round((earned_points / total_points) * 100, 2)
        
        # Determine pass threshold based on system type
        pass_threshold = 85 if cuda_available else 90
        passed = compliance_score >= pass_threshold
        
        # Add summary feedback
        if not passed:
            check_data['feedback'].append(f"Reproducibility score {compliance_score}% below {pass_threshold}% threshold")
            if cuda_available:
                check_data['recommendations'].append("CUDA systems require comprehensive seed configuration for full reproducibility")
            else:
                check_data['recommendations'].append("CPU systems should achieve >90% reproducibility compliance")
        
        # Prepare final details with hardware context
        details = {
            **check_data,
            'compliance_score': compliance_score,
            'pass_threshold': pass_threshold,
            'passed': passed,
            'summary': {
                'total_checks': len([c for c in check_data['checks'].values() if c.get('value') != "Not installed (optional)"]),
                'passed_checks': len([c for c in check_data['checks'].values() if c['passed'] and c.get('value') != "Not installed (optional)"]),
                'critical_issues': len([f for f in check_data['feedback'] if 'required' in f.lower() or 'critical' in f.lower()]),
                'recommendations_count': len(check_data['recommendations'])
            }
        }
        
        # Create status message with hardware context
        status_msg = f"Reproducibility configuration {'passed' if passed else 'failed'} ({compliance_score}%)"
        if cuda_available:
            status_msg += f" - CUDA system with {gpu_count} GPU(s)"
        else:
            status_msg += " - CPU-only system"
        
        return (
            base_result
            .with_passed(passed)
            .with_details(details)
            .with_message(status_msg)
            .with_metadata({
                'hardware_context': check_data['hardware_context'],
                'compliance_score': compliance_score,
                'recommendations': check_data['recommendations']
            })
        )
        
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Seed configuration check failed",
                level=CheckLevel.IMPORTANT
            )
            .with_details({
                'error': str(e),
                'compliance_score': 0,
                'feedback': ['Exception occurred during seed config check'],
                'hardware_context': hardware_data.get('cuda', {}) if hardware_data else {}
            })
            .with_exception(e)
        )

def check_core_dependencies() -> CheckResult:
    """
    Check core dependencies using the existing check_versions() function.
    Simplified to avoid redundancy.
    """
    try:
        # Use the comprehensive version check, but filter to core only
        version_info = check_versions(include_optional=False)
        
        # Filter only core dependencies (required=True)
        core_deps = {k: v for k, v in version_info.items() if v.get('required', False)}
        
        # Determine overall status
        passed = all(info['compatible'] for info in core_deps.values())
        
        # Prepare summary details
        compatible_count = sum(1 for info in core_deps.values() if info['compatible'])
        total_count = len(core_deps)
        
        details = f"Core Dependencies: {compatible_count}/{total_count} compatible"
        
        base_result = CheckResult(
            passed=passed,
            message=f"Core dependencies check {'passed' if passed else 'failed'}",
            level=CheckLevel.CRITICAL if not passed else CheckLevel.INFORMATIONAL
        )
        
        return (
            base_result
            .with_details(details)
            .with_metadata({
                'core_dependencies': core_deps,
                'summary': {
                    'total': total_count,
                    'compatible': compatible_count,
                    'incompatible': total_count - compatible_count
                }
            })
        )
        
    except Exception as e:
        return (
            CheckResult(
                passed=False,
                message="Core dependencies check failed",
                level=CheckLevel.CRITICAL
            )
            .with_details(f"Error checking core dependencies: {str(e)}")
            .with_exception(e)
        )

# Logging and Directory Setup
class UnicodeStreamHandler(logging.StreamHandler):
    """
    A logging StreamHandler that preserves Unicode output on Windows consoles,
    falling back to ASCII-safe output if encoding errors occur.
    """
    def emit(self, record):
        try:
            msg = self.format(record)
            if sys.platform == 'win32':
                try:
                    self.stream.write(msg + self.terminator)
                except UnicodeEncodeError:
                    # Fallback to ASCII-only output
                    msg = msg.encode('ascii', errors='replace').decode('ascii')
                    self.stream.write(msg + self.terminator)
            else:
                self.stream.write(msg + self.terminator)
            self.flush()
        except Exception:
            self.handleError(record)

# Setup logging configuration
def setup_logging(log_dir: Path = None) -> logging.Logger:
    """
    Configure the logger with:
    - UTF-8 file handler
    - Unicode-safe console handler (via UnicodeStreamHandler)

    Features:
    1. If log_dir is None, defaults to 'logs' folder next to this script.
    2. Adds handlers only if they don't already exist.
    3. Falls back to basic logging config if setup fails.
    """
    try:
        # Determine log_dir (default: script's directory / logs)
        if log_dir is None:
            log_dir = Path(__file__).resolve().parent / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)

        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)

        # File Handler
        log_file = log_dir / "deep_learning.log"
        file_handler_exists = any(
            isinstance(h, logging.FileHandler) and getattr(h, 'baseFilename', None) == str(log_file)
            for h in logger.handlers
        )
        if not file_handler_exists:
            file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
            #file_handler.setLevel(logging.DEBUG)
            file_formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)

        # Unicode-Safe Console Handler
        console_handler_exists = any(isinstance(h, UnicodeStreamHandler) for h in logger.handlers)
        if not console_handler_exists:
            console_handler = UnicodeStreamHandler(sys.stdout)
            console_handler.setLevel(logging.INFO)
            console_formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%H:%M:%S'
            )
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)

        return logger

    except Exception as e:
        # Fallback basic configuration if setup fails
        logging.basicConfig(
            level=logging.INFO,
            format='%(levelname)s - %(message)s'
        )
        logger = logging.getLogger(__name__)
        logger.warning(f"Failed to setup proper logging: {e}")
        return logger

# Setup directories and assign global directory variables
def setup_directories(logger: logging.Logger) -> Dict[str, Path]:
    """Create and return essential directories with versioned subdirectories."""
    base_dir = Path(__file__).resolve().parent
    
    dirs = {
        'logs': base_dir / "logs",
        'models': base_dir / "models",
        'data': base_dir / "data",
        'config': base_dir / "config",
        'reports': base_dir / "reports",
        'tensorboard': base_dir / "tensorboard",
        'cache': base_dir / "cache",
        'exports': base_dir / "exports",
        'results': base_dir / "results",
        'checkpoints': base_dir / "checkpoints" / f"checkpoints_v{VERSION_INFO['torch']}"
    }
    
    # Create directories
    for name, path in dirs.items():
        try:
            if not path.exists():
                path.mkdir(parents=True, exist_ok=True)
                if logger:
                    logger.debug(f"Created directory: {path}")
        except PermissionError as e:
            if logger:
                logger.error(f"Permission denied creating directory {path}: {e}")
            raise
        except Exception as e:
            if logger:
                logger.error(f"Failed to create directory {path}: {e}")
            raise
    
    return dirs

def configure_directories(logger: logging.Logger) -> Dict[str, Path]:
    """
    Initialize and assign global directory path variables using
    setup_directories(), ensuring they are accessible across modules.
    """
    try:
        dirs = setup_directories(logger)
        
        # Assign to global variables if needed
        global LOG_DIR, DEFAULT_MODEL_DIR, DATA_DIR, CONFIG_DIR, REPORTS_DIR, TB_DIR, CACHE_DIR, EXPORTS_DIR, CHECKPOINTS_DIR, RESULTS_DIR
        LOG_DIR = dirs['logs']
        DEFAULT_MODEL_DIR = dirs['models']
        DATA_DIR = dirs['data']
        CONFIG_DIR = dirs['config']
        REPORTS_DIR = dirs['reports']
        TB_DIR = dirs['tensorboard']
        CACHE_DIR = dirs['cache']
        EXPORTS_DIR = dirs['exports']
        CHECKPOINTS_DIR = dirs['checkpoints']
        RESULTS_DIR = dirs['results']
        
        if logger:
            logger.info("Directory configuration completed successfully")
        
        return dirs
        
    except Exception as e:
        if logger:
            logger.critical(f"Directory configuration failed: {e}")
        raise

# Additional system initialization checks
def check_global_exception_handler() -> CheckResult:
    """
    Check if global exception handler is properly configured.
    Enhanced with comprehensive validation and configuration details.
    """
    try:
        # Test if our custom handler is set
        current_handler = sys.excepthook
        is_custom = current_handler.__name__ == 'enhanced_global_exception_handler'
        
        handler_info = {
            'handler_name': current_handler.__name__,
            'handler_module': getattr(current_handler, '__module__', 'unknown'),
            'is_enhanced': is_custom,
            'is_default': current_handler is sys.__excepthook__,
            'handler_doc': getattr(current_handler, '__doc__', '').strip()[:100] if hasattr(current_handler, '__doc__') else None
        }
        
        # Check if logging system is available
        logging_available = logger is not None and logger.handlers
        
        # Check if required directories exist
        log_dir_exists = LOG_DIR.exists() and LOG_DIR.is_dir()
        log_dir_writable = os.access(LOG_DIR, os.W_OK) if log_dir_exists else False
        
        # Comprehensive status check
        all_components_ready = is_custom and logging_available and log_dir_exists and log_dir_writable
        
        details = {
            'handler_info': handler_info,
            'logging_system': {
                'available': logging_available,
                'handlers_count': len(logger.handlers) if logger else 0
            },
            'log_directory': {
                'exists': log_dir_exists,
                'writable': log_dir_writable,
                'path': str(LOG_DIR)
            },
            'dependencies': {
                'psutil_available': 'psutil' in sys.modules,
                'torch_available': 'torch' in sys.modules,
                'rich_available': 'rich' in sys.modules,
                'json_available': 'json' in sys.modules
            },
            'all_components_ready': all_components_ready
        }
        
        # Determine result status
        if all_components_ready:
            message = "Enhanced global exception handler fully configured"
            level = CheckLevel.INFORMATIONAL
            passed = True
        elif is_custom:
            message = "Enhanced exception handler set but some components missing"
            level = CheckLevel.IMPORTANT
            passed = False
        else:
            message = "Using default exception handler"
            level = CheckLevel.IMPORTANT
            passed = False
        
        return CheckResult(
            passed=passed,
            message=message,
            level=level,
            details=details
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Failed to check exception handler",
            level=CheckLevel.IMPORTANT
        ).with_exception(e)

def check_performance_monitoring() -> CheckResult:
    """
    Check if performance monitoring is available and properly configured.
    Enhanced with comprehensive validation of monitoring capabilities.
    """
    try:
        monitoring_status = {
            'decorator_available': False,
            'wrapper_available': False,
            'dependencies': {},
            'hardware_integration': False,
            'logging_integration': False,
            'capabilities': {}
        }
        
        # Check if monitoring functions are available
        monitoring_status['decorator_available'] = 'enhanced_monitor_performance' in globals()
        monitoring_status['wrapper_available'] = 'performance_monitor_wrapper' in globals()
        
        # Check dependencies
        try:
            import time
            monitoring_status['dependencies']['time'] = True
        except ImportError:
            monitoring_status['dependencies']['time'] = False
        
        try:
            import psutil
            monitoring_status['dependencies']['psutil'] = True
        except ImportError:
            monitoring_status['dependencies']['psutil'] = False
        
        try:
            import torch
            monitoring_status['dependencies']['torch'] = True
        except ImportError:
            monitoring_status['dependencies']['torch'] = False
        
        try:
            from functools import wraps
            monitoring_status['dependencies']['functools'] = True
        except ImportError:
            monitoring_status['dependencies']['functools'] = False
        
        # Check hardware integration
        try:
            hw_data = check_hardware(include_memory_usage=False)
            monitoring_status['hardware_integration'] = True
            monitoring_status['hardware_context'] = {
                'cuda_available': hw_data.get('cuda', {}).get('available', False),
                'gpu_count': hw_data.get('cuda', {}).get('gpu_count', 0),
                'memory_monitoring': hw_data.get('system_ram', {}).get('available', False)
            }
        except Exception as e:
            monitoring_status['hardware_integration'] = False
            monitoring_status['hardware_error'] = str(e)
        
        # Check logging integration
        monitoring_status['logging_integration'] = logger is not None and logger.handlers
        
        # Determine monitoring capabilities
        all_deps_available = all(monitoring_status['dependencies'].values())
        basic_monitoring = (
            monitoring_status['decorator_available'] and 
            monitoring_status['wrapper_available'] and 
            all_deps_available
        )
        
        advanced_monitoring = (
            basic_monitoring and 
            monitoring_status['hardware_integration'] and 
            monitoring_status['logging_integration']
        )
        
        monitoring_status['capabilities'] = {
            'basic_timing': all_deps_available and monitoring_status['dependencies']['time'],
            'memory_monitoring': monitoring_status['dependencies']['psutil'],
            'gpu_monitoring': monitoring_status['dependencies']['torch'] and monitoring_status.get('hardware_context', {}).get('cuda_available', False),
            'hardware_aware': monitoring_status['hardware_integration'],
            'logging_integrated': monitoring_status['logging_integration'],
            'comprehensive': advanced_monitoring
        }
        
        # Test monitoring functionality
        if basic_monitoring:
            try:
                @enhanced_monitor_performance(include_memory=False, log_level=logging.DEBUG)
                def test_function():
                    time.sleep(0.001)  # 1ms test
                    return "test_result"
                
                result = test_function()
                test_passed = result == "test_result" and hasattr(test_function, '_performance_metrics')
                monitoring_status['test_results'] = {
                    'basic_test_passed': test_passed,
                    'metrics_stored': hasattr(test_function, '_performance_metrics'),
                    'metrics_count': len(getattr(test_function, '_performance_metrics', []))
                }
            except Exception as e:
                monitoring_status['test_results'] = {
                    'basic_test_passed': False,
                    'test_error': str(e)
                }
        
        # Determine overall status
        if advanced_monitoring:
            passed = True
            message = "Performance monitoring fully configured with hardware awareness"
            level = CheckLevel.INFORMATIONAL
        elif basic_monitoring:
            passed = True
            message = "Basic performance monitoring available"
            level = CheckLevel.INFORMATIONAL
        elif monitoring_status['decorator_available']:
            passed = False
            message = "Performance monitoring available but dependencies missing"
            level = CheckLevel.IMPORTANT
        else:
            passed = False
            message = "Performance monitoring not available"
            level = CheckLevel.IMPORTANT
        
        return CheckResult(
            passed=passed,
            message=message,
            level=level,
            details=monitoring_status
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Failed to check performance monitoring",
            level=CheckLevel.IMPORTANT
        ).with_exception(e)

def check_performance_baseline() -> CheckResult:
    """
    Check if performance baseline can be established and validate system performance.
    """
    try:
        logger.debug("Attempting to establish performance baseline")
        
        # Attempt to establish baseline
        baseline_results = establish_performance_baseline()
        
        # Check if baseline was successful
        if 'error' in baseline_results:
            return CheckResult(
                passed=False,
                message="Failed to establish performance baseline",
                level=CheckLevel.INFORMATIONAL,
                details={
                    'error': baseline_results['error'],
                    'timestamp': baseline_results.get('timestamp')
                }
            )
        
        # Analyze baseline results
        baselines = baseline_results.get('baselines', {})
        summary = baseline_results.get('summary', {})
        hardware_context = baseline_results.get('hardware_context', {})
        
        # Validate individual components
        component_status = {
            'cpu': 'cpu' in baselines and 'cpu_error' not in baselines,
            'memory': 'memory' in baselines and 'memory_error' not in baselines,
            'gpu': hardware_context.get('cuda_available', False) and 'gpu' in baselines and 'gpu_error' not in baselines,
            'io': 'io' in baselines and 'io_error' not in baselines
        }
        
        successful_components = sum(1 for status in component_status.values() if status)
        # CPU, Memory, I/O + GPU if available
        total_testable_components = 3 + (1 if hardware_context.get('cuda_available', False) else 0)
        
        # Performance analysis
        performance_analysis = {
            'cpu_performance': 'unknown',
            'memory_performance': 'unknown',
            'gpu_performance': 'unknown',
            'io_performance': 'unknown',
            'overall_score': 0
        }
        
        score = 0
        max_score = 0
        
        # CPU analysis
        if component_status['cpu']:
            cpu_data = baselines['cpu']
            gflops = cpu_data.get('gflops', 0)
            
            if gflops > 10:
                performance_analysis['cpu_performance'] = 'excellent'
                score += 4
            elif gflops > 5:
                performance_analysis['cpu_performance'] = 'good'
                score += 3
            elif gflops > 2:
                performance_analysis['cpu_performance'] = 'fair'
                score += 2
            else:
                performance_analysis['cpu_performance'] = 'limited'
                score += 1
            
            max_score += 4
        
        # Memory analysis
        if component_status['memory']:
            memory_data = baselines['memory']
            avg_speed = np.mean([
                baseline.get('allocation_speed_mbs', 0) 
                for baseline in memory_data.values() 
                if isinstance(baseline, dict) and 'allocation_speed_mbs' in baseline
            ])
            
            if avg_speed > 500:
                performance_analysis['memory_performance'] = 'excellent'
                score += 4
            elif avg_speed > 200:
                performance_analysis['memory_performance'] = 'good'
                score += 3
            elif avg_speed > 100:
                performance_analysis['memory_performance'] = 'fair'
                score += 2
            else:
                performance_analysis['memory_performance'] = 'limited'
                score += 1
            
            max_score += 4
        
        # GPU analysis
        if component_status['gpu']:
            gpu_data = baselines['gpu']
            max_gpu_gflops = max([
                gpu.get('gflops', 0) 
                for gpu in gpu_data.values() 
                if isinstance(gpu, dict) and 'gflops' in gpu
            ], default=0)
            
            if max_gpu_gflops > 1000:
                performance_analysis['gpu_performance'] = 'excellent'
                score += 6
            elif max_gpu_gflops > 500:
                performance_analysis['gpu_performance'] = 'good'
                score += 4
            elif max_gpu_gflops > 100:
                performance_analysis['gpu_performance'] = 'fair'
                score += 2
            else:
                performance_analysis['gpu_performance'] = 'limited'
                score += 1
            
            max_score += 6
        
        # I/O analysis
        if component_status['io']:
            io_data = baselines['io']
            write_speed = io_data.get('write_speed_mbs', 0)
            read_speed = io_data.get('read_speed_mbs', 0)
            avg_io_speed = (write_speed + read_speed) / 2
            
            if avg_io_speed > 200:
                performance_analysis['io_performance'] = 'excellent'
                score += 3
            elif avg_io_speed > 100:
                performance_analysis['io_performance'] = 'good'
                score += 2
            elif avg_io_speed > 50:
                performance_analysis['io_performance'] = 'fair'
                score += 1
            else:
                performance_analysis['io_performance'] = 'limited'
                score += 1
            
            max_score += 3
        
        # Calculate overall score
        if max_score > 0:
            performance_analysis['overall_score'] = round((score / max_score) * 100, 1)
        
        # Determine system classification
        if performance_analysis['overall_score'] > 80:
            system_class = 'high-performance'
            level = CheckLevel.INFORMATIONAL
        elif performance_analysis['overall_score'] > 60:
            system_class = 'good-performance'
            level = CheckLevel.INFORMATIONAL
        elif performance_analysis['overall_score'] > 40:
            system_class = 'fair-performance'
            level = CheckLevel.INFORMATIONAL
        else:
            system_class = 'limited-performance'
            level = CheckLevel.IMPORTANT
        
        # Create comprehensive details
        details = {
            'baseline_results': baseline_results,
            'component_status': component_status,
            'performance_analysis': performance_analysis,
            'system_classification': system_class,
            'successful_components': successful_components,
            'total_testable_components': total_testable_components,
            'completion_rate': f"{successful_components}/{total_testable_components}"
        }
        
        # Determine pass/fail
        # Allow one component to fail
        passed = successful_components >= (total_testable_components - 1)
        
        message = f"Performance baseline established - {system_class} system ({performance_analysis['overall_score']}% score)"
        
        return CheckResult(
            passed=passed,
            message=message,
            level=level,
            details=details
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Failed to check performance baseline",
            level=CheckLevel.INFORMATIONAL
        ).with_exception(e)

def check_memory_management() -> CheckResult:
    """
    Check if memory management functions are available and properly integrated.
    Enhanced with comprehensive validation of memory management capabilities.
    """
    try:
        memory_status = {
            'functions_available': {},
            'hardware_integration': False,
            'dependencies': {},
            'capabilities': {},
            'test_results': {}
        }
        
        # Check if memory management functions are available
        memory_status['functions_available']['enhanced_clear_memory'] = 'enhanced_clear_memory' in globals()
        memory_status['functions_available']['check_hardware'] = 'check_hardware' in globals()
        
        # Check dependencies
        try:
            import gc
            memory_status['dependencies']['gc'] = True
        except ImportError:
            memory_status['dependencies']['gc'] = False
        
        try:
            import psutil
            memory_status['dependencies']['psutil'] = True
        except ImportError:
            memory_status['dependencies']['psutil'] = False
        
        try:
            import torch
            memory_status['dependencies']['torch'] = True
        except ImportError:
            memory_status['dependencies']['torch'] = False
        
        # Check hardware integration
        if memory_status['functions_available']['check_hardware']:
            try:
                hw_data = check_hardware(include_memory_usage=True)
                memory_status['hardware_integration'] = True
                memory_status['hardware_context'] = {
                    'system_ram_gb': hw_data.get('system_ram', {}).get('ram_total_gb', 0),
                    'cuda_available': hw_data.get('cuda', {}).get('available', False),
                    'gpu_count': hw_data.get('cuda', {}).get('gpu_count', 0),
                    'memory_monitoring': 'current_usage' in hw_data.get('system_ram', {})
                }
            except Exception as e:
                memory_status['hardware_integration'] = False
                memory_status['hardware_error'] = str(e)
        
        # Determine capabilities
        all_deps_available = all(memory_status['dependencies'].values())
        basic_memory_mgmt = (
            memory_status['functions_available']['enhanced_clear_memory'] and 
            memory_status['dependencies']['gc']
        )
        
        advanced_memory_mgmt = (
            basic_memory_mgmt and 
            memory_status['dependencies']['psutil'] and 
            memory_status['hardware_integration']
        )
        
        cuda_memory_mgmt = (
            advanced_memory_mgmt and 
            memory_status['dependencies']['torch'] and 
            memory_status.get('hardware_context', {}).get('cuda_available', False)
        )
        
        memory_status['capabilities'] = {
            'basic_gc': memory_status['dependencies']['gc'],
            'process_memory_monitoring': memory_status['dependencies']['psutil'],
            'cuda_memory_management': cuda_memory_mgmt,
            'hardware_aware_clearing': advanced_memory_mgmt,
            'memory_usage_tracking': memory_status.get('hardware_context', {}).get('memory_monitoring', False),
            'comprehensive_management': cuda_memory_mgmt
        }
        
        # Test memory management functionality
        if basic_memory_mgmt:
            try:
                # Test basic memory clearing
                initial_objects = len(gc.get_objects())
                
                # Create some objects to clean up
                test_objects = [list(range(1000)) for _ in range(10)]
                objects_created = len(gc.get_objects()) - initial_objects
                
                # Clear memory
                del test_objects
                if memory_status['functions_available']['enhanced_clear_memory']:
                    clear_results = enhanced_clear_memory(aggressive=False)
                    test_success = clear_results.get('success', False)
                else:
                    gc.collect()
                    test_success = True
                
                final_objects = len(gc.get_objects())
                objects_cleaned = initial_objects + objects_created - final_objects
                
                memory_status['test_results'] = {
                    'basic_test_passed': test_success,
                    'objects_created': objects_created,
                    'objects_cleaned': objects_cleaned,
                    'cleanup_effectiveness': (objects_cleaned / objects_created * 100) if objects_created > 0 else 0
                }
                
                # Test advanced memory management if available
                if advanced_memory_mgmt:
                    try:
                        proc = psutil.Process()
                        initial_memory = proc.memory_info().rss / 1024 / 1024
                        
                        # Test with hardware awareness
                        clear_results = enhanced_clear_memory(aggressive=True)
                        
                        final_memory = proc.memory_info().rss / 1024 / 1024
                        memory_freed_mb = initial_memory - final_memory
                        
                        memory_status['test_results'].update({
                            'advanced_test_passed': clear_results.get('success', False),
                            'memory_freed_mb': memory_freed_mb,
                            'actions_taken': clear_results.get('actions_taken', []),
                            'hardware_context_used': 'hardware_context' in clear_results
                        })
                        
                    except Exception as e:
                        memory_status['test_results']['advanced_test_error'] = str(e)
                
            except Exception as e:
                memory_status['test_results']['basic_test_error'] = str(e)
        
        # Determine overall status
        if memory_status['capabilities']['comprehensive_management']:
            passed = True
            message = "Comprehensive memory management available with CUDA support"
            level = CheckLevel.INFORMATIONAL
        elif memory_status['capabilities']['hardware_aware_clearing']:
            passed = True
            message = "Hardware-aware memory management available"
            level = CheckLevel.INFORMATIONAL
        elif memory_status['capabilities']['basic_gc']:
            passed = True
            message = "Basic memory management available"
            level = CheckLevel.IMPORTANT
        else:
            passed = False
            message = "Memory management functions not available"
            level = CheckLevel.IMPORTANT
        
        # Add recommendations if needed
        recommendations = []
        if not memory_status['dependencies']['psutil']:
            recommendations.append("Install psutil for process memory monitoring")
        if not memory_status['dependencies']['torch'] and memory_status.get('hardware_context', {}).get('cuda_available', False):
            recommendations.append("Install PyTorch for CUDA memory management")
        if not memory_status['hardware_integration']:
            recommendations.append("Enable hardware integration for optimal memory management")
        
        memory_status['recommendations'] = recommendations
        
        return CheckResult(
            passed=passed,
            message=message,
            level=level,
            details=memory_status
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Failed to check memory management",
            level=CheckLevel.IMPORTANT
        ).with_exception(e)

def check_configuration_system() -> CheckResult:
    """Check if configuration system is properly initialized with enhanced detail collection."""
    try:
        # Suppress individual log messages during system checks
        config_logger = setup_logging(log_dir=LOG_DIR)
        original_level = config_logger.getEffectiveLevel()
        # Only show warnings/errors
        config_logger.setLevel(logging.WARNING)
        
        try:
            # Initialize configuration silently
            config = initialize_config()
            validate_config(config)
            
            # Collect detailed configuration information
            config_sections = {}
            total_params = 0
            
            for section_name, section_data in config.items():
                if isinstance(section_data, dict):
                    param_count = len(section_data)
                    total_params += param_count
                    
                    # Collect key parameters for each section
                    key_params = {}
                    if section_name == 'model':
                        key_params = {
                            'model_type': section_data.get('model_type', 'unknown'),
                            #'input_size': section_data.get('input_size', 'unknown'),
                            'input_dim': section_data.get('input_dim', 'unknown'),
                            #'num_classes': section_data.get('num_classes', 'unknown'),
                            'num_models': section_data.get('num_models', 'unknown'),
                            'dropout_rates': section_data.get('dropout_rates', 'unknown')
                        }
                    elif section_name == 'training':
                        key_params = {
                            'batch_size': section_data.get('batch_size', 'unknown'),
                            'learning_rate': section_data.get('learning_rate', 'unknown'),
                            'epochs': section_data.get('epochs', 'unknown'),
                            'optimizer': section_data.get('optimizer', 'unknown')
                        }
                    elif section_name == 'data':
                        key_params = {
                            'data_path': section_data.get('data_path', 'not_set'),
                            'artifacts_path': section_data.get('artifacts_path', 'not_set'),
                            'validation_split': section_data.get('validation_split', 'unknown'),
                            'preprocessing': section_data.get('preprocessing', 'none')
                        }
                    elif section_name == 'presets':
                        key_params = {
                            'current_preset': section_data.get('current_preset', 'default'),
                            'available_presets': list(section_data.get('available', {}).keys()) if 'available' in section_data else []
                        }
                    else:
                        # For other sections, show first few parameters
                        key_params = dict(list(section_data.items())[:3])
                    
                    config_sections[section_name] = {
                        'parameter_count': param_count,
                        'status': 'loaded',
                        'key_parameters': key_params
                    }
                else:
                    config_sections[section_name] = {
                        'parameter_count': 1,
                        'status': 'loaded',
                        'value': str(section_data)
                    }
            
            config_details = {
                'config_file_exists': CONFIG_FILE.exists(),
                'config_file_path': str(CONFIG_FILE),
                'active_preset': config.get('presets', {}).get('current_preset', 'default'),
                'model_type': config.get('model', {}).get('model_type', 'unknown'),
                'sections_loaded': len(config),
                'total_parameters': total_params,
                'section_details': config_sections,
                'validation_passed': True
            }
            
            return CheckResult(
                passed=True,
                message="Configuration system operational",
                level=CheckLevel.CRITICAL,
                details=config_details
            )
        finally:
            # Restore original logging level
            config_logger.setLevel(original_level)
            
    except ValueError as e:
        return CheckResult(
            passed=False,
            message=f"Configuration validation failed: {str(e)}",
            level=CheckLevel.CRITICAL,
            details={
                'config_file_exists': CONFIG_FILE.exists() if 'CONFIG_FILE' in globals() else False,
                'validation_error': str(e),
                'validation_passed': False
            }
        ).with_exception(e)
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Configuration system initialization failed",
            level=CheckLevel.CRITICAL,
            details={
                'config_file_exists': False,
                'initialization_error': str(e),
                'validation_passed': False
            }
        ).with_exception(e)

def check_configuration_system_wrapper() -> CheckResult:
    """
    Run comprehensive configuration system check with rich table output.
    
    Returns:
        CheckResult object with detailed configuration information and rich display
    """
    try:
        # Get the configuration check results
        config_result = check_configuration_system()
        
        # If the check failed, return it as-is
        if not config_result.passed:
            return config_result
        
        # Display the detailed configuration tables
        display_configuration_details(config_result)
        
        # Prepare summary details for the main table
        if isinstance(config_result.details, dict):
            summary_details = {
                'sections_loaded': config_result.details.get('sections_loaded', 0),
                'total_parameters': config_result.details.get('total_parameters', 0),
                'active_preset': config_result.details.get('active_preset', 'default'),
                'model_type': config_result.details.get('model_type', 'unknown'),
                'config_file_exists': config_result.details.get('config_file_exists', False),
                'validation_passed': config_result.details.get('validation_passed', False),
                # Include full details for potential future use
                'full_details': config_result.details
            }
        else:
            summary_details = config_result.details
        
        return CheckResult(
            passed=config_result.passed,
            message="Configuration system check with detailed breakdown",
            level=CheckLevel.CRITICAL,
            details=summary_details,
            metadata=config_result.metadata,
            exception=config_result.exception
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Configuration system wrapper check failed",
            level=CheckLevel.CRITICAL,
            details={
                'wrapper_error': str(e),
                'validation_passed': False
            }
        ).with_exception(e)

def display_configuration_details(config_result: CheckResult) -> None:
    """
    Display detailed configuration information in a rich table format.
    
    Args:
        config_result: CheckResult from check_configuration_system()
    """
    if not isinstance(config_result.details, dict):
        console.print("\n[bold yellow]No detailed configuration information available[/bold yellow]")
        return
    
    try:
        details = config_result.details
        
        # Main configuration overview table
        overview_table = Table(
            title="\n[bold yellow]Configuration System Overview[/bold yellow]",
            title_justify="left",
            box=box.ROUNDED,
            header_style="bold magenta",
            border_style="cyan",
            show_lines=True,
            width=min(100, console.width - 4)
        )
        
        overview_table.add_column("Property", style="bold yellow", width=20)
        overview_table.add_column("Value", style="bold magenta", width=30)
        overview_table.add_column("Status", style="bold green", width=15, justify="center")
        
        # Add overview rows
        overview_table.add_row(
            "Config File",
            str(details.get('config_file_path', 'Unknown')),
            "EXISTS" if details.get('config_file_exists', False) else "MISSING"
        )
        
        overview_table.add_row(
            "Active Preset",
            details.get('active_preset', 'default'),
            "ACTIVE"
        )
        
        overview_table.add_row(
            "Model Type",
            details.get('model_type', 'unknown'),
            "CONFIGURED"
        )
        
        overview_table.add_row(
            "Sections Loaded",
            str(details.get('sections_loaded', 0)),
            "LOADED"
        )
        
        overview_table.add_row(
            "Total Parameters",
            str(details.get('total_parameters', 0)),
            "COUNTED"
        )
        
        overview_table.add_row(
            "Validation",
            "Passed" if details.get('validation_passed', False) else "Failed",
            "PASS" if details.get('validation_passed', False) else "FAIL"
        )
        
        console.print(overview_table)
        console.print()
        
        # Section details table
        if 'section_details' in details and details['section_details']:
            sections_table = Table(
                title="[bold yellow]Configuration Sections Detail[/bold yellow]",
                title_justify="left",
                box=box.ROUNDED,
                header_style="bold magenta",
                border_style="cyan",
                show_lines=True,
                expand=True,
                width=min(120, console.width - 4)
            )
            
            sections_table.add_column("Section", style="bold yellow", width=15)
            sections_table.add_column("Parameters", style="bold magenta", width=12, justify="center")
            sections_table.add_column("Status", style="bold green", width=10, justify="center")
            sections_table.add_column("Key Configuration", style="bold", min_width=50)
            
            for section_name, section_info in details['section_details'].items():
                # Format key parameters
                key_config_lines = []
                
                if 'key_parameters' in section_info:
                    for param, value in section_info['key_parameters'].items():
                        if isinstance(value, list):
                            value_str = ', '.join(str(v) for v in value) if value else 'none'
                        else:
                            value_str = str(value)
                        
                        # Color coding for specific values
                        if param == 'current_preset':
                            key_config_lines.append(f"[bold white]{param}:[/] [bold green]{value_str}[/]")
                        elif param in ['model_type', 'optimizer']:
                            key_config_lines.append(f"[bold white]{param}:[/] [bold cyan]{value_str}[/]")
                        elif param in ['batch_size', 'learning_rate', 'epochs']:
                            key_config_lines.append(f"[bold white]{param}:[/] [bold yellow]{value_str}[/]")
                        elif 'unknown' in str(value) or 'not_set' in str(value):
                            key_config_lines.append(f"[bold white]{param}:[/] [bold red]{value_str}[/]")
                        else:
                            key_config_lines.append(f"[bold white]{param}:[/] [bold blue]{value_str}[/]")
                
                elif 'value' in section_info:
                    key_config_lines.append(f"[bold]Value: {section_info['value']}[/bold]")
                
                key_config_text = "\n".join(key_config_lines) if key_config_lines else "[bold red]No key parameters[/bold red]"
                
                sections_table.add_row(
                    Text(section_name.upper(), style="bold yellow"),
                    str(section_info.get('parameter_count', 0)),
                    Text(section_info.get('status', 'unknown').upper(), style="bold green"),
                    key_config_text
                )
            
            console.print(sections_table)
        
        # Error information if validation failed
        if not details.get('validation_passed', True):
            error_table = Table(
                title="[bold red]Configuration Errors[/bold red]",
                title_justify="left",
                box=box.ROUNDED,
                header_style="bold bright_white",
                border_style="red",
                width=min(100, console.width - 4)
            )
            
            error_table.add_column("Error Type", style="bold red", width=20)
            error_table.add_column("Details", style="bright_white")
            
            if 'validation_error' in details:
                error_table.add_row("Validation Error", details['validation_error'])
            
            if 'initialization_error' in details:
                error_table.add_row("Initialization Error", details['initialization_error'])
            
            console.print()
            console.print(error_table)
    
    except Exception as e:
        console.print(f"[bold red]Error displaying configuration details: {str(e)}[/bold red]")

def check_model_variants() -> CheckResult:
    """Check if model variants are properly initialized with enhanced detail collection."""
    try:
        # Initialize model variants silently using the enhanced function
        initialize_model_variants(silent=True)
        
        if not MODEL_VARIANTS:
            return CheckResult(
                passed=False,
                message="No model variants available after initialization",
                level=CheckLevel.CRITICAL,
                details={
                    'total_variants': 0,
                    'available_variants': 0,
                    'variant_names': [],
                    'variant_status': {},
                    'initialization_passed': False,
                    'error': 'MODEL_VARIANTS dictionary is empty after initialization',
                    'recommendation': 'Check model dependencies and configuration'
                }
            )
        
        # Validate model variants silently using the enhanced function
        variant_status = validate_model_variants(logger, silent=True)
        
        # Enhanced status categorization based on updated validation function
        available_variants = [
            name for name, status in variant_status.items() 
            if status == 'available'
        ]
        warning_variants = [
            name for name, status in variant_status.items() 
            if status.startswith('warning')
        ]
        failed_variants = [
            name for name, status in variant_status.items() 
            if status.startswith('error') or 'failed' in status or status == 'class_not_found' or status == 'class_not_callable'
        ]
        unknown_variants = [
            name for name, status in variant_status.items() 
            if name not in available_variants + warning_variants + failed_variants
        ]
        
        # Collect comprehensive variant information
        variant_details = {}
        initialization_metrics = {}
        hardware_info = {}
        
        try:
            # Get hardware context for detailed reporting
            hardware_info = check_hardware(include_memory_usage=True)
        except Exception as e:
            hardware_info = {'error': str(e)}
        
        for variant_name, variant_class in MODEL_VARIANTS.items():
            variant_info = {
                'class_name': variant_class.__name__ if variant_class else 'None',
                'status': variant_status.get(variant_name, 'unknown'),
                'available': variant_name in available_variants,
                'has_warnings': variant_name in warning_variants,
                'failed': variant_name in failed_variants
            }
            
            # Enhanced metadata collection
            try:
                # Get class documentation
                if hasattr(variant_class, '__doc__') and variant_class.__doc__:
                    doc_lines = [line.strip() for line in variant_class.__doc__.split('\n') if line.strip()]
                    variant_info['description'] = doc_lines[0] if doc_lines else f"{variant_class.__name__} model variant"
                else:
                    variant_info['description'] = f"{variant_class.__name__} model variant"
                
                # Check for configuration methods
                config_methods = {}
                if hasattr(variant_class, 'get_default_config'):
                    try:
                        default_config = variant_class.get_default_config()
                        if isinstance(default_config, dict):
                            config_methods['default_config'] = True
                            variant_info['config_parameters'] = len(default_config)
                            variant_info['key_config'] = {
                                k: v for k, v in list(default_config.items())[:3]
                            }
                        else:
                            variant_info['config_parameters'] = 'invalid_format'
                            variant_info['key_config'] = {}
                    except Exception as e:
                        variant_info['config_parameters'] = 'error'
                        variant_info['key_config'] = {}
                        variant_info['config_error'] = str(e)
                else:
                    variant_info['config_parameters'] = 'none'
                    variant_info['key_config'] = {}
                
                # Check for other common model methods
                for method_name in ['encode', 'decode', 'forward', 'train', 'eval']:
                    if hasattr(variant_class, method_name):
                        config_methods[method_name] = True
                    else:
                        config_methods[method_name] = False
                
                variant_info['supported_methods'] = config_methods
                
                # Analyze initialization signature
                if hasattr(variant_class, '__init__'):
                    import inspect
                    try:
                        sig = inspect.signature(variant_class.__init__)
                        required_params = [
                            p.name for p in sig.parameters.values() 
                            if p.name != 'self' and p.default == inspect.Parameter.empty
                        ]
                        optional_params = [
                            p.name for p in sig.parameters.values() 
                            if p.name != 'self' and p.default != inspect.Parameter.empty
                        ]
                        
                        variant_info['init_parameters'] = {
                            'required': len(required_params),
                            'optional': len(optional_params),
                            'total': len(required_params) + len(optional_params),
                            'required_names': required_params[:5]  # First 5 required params
                        }
                    except (ValueError, TypeError):
                        variant_info['init_parameters'] = 'signature_unavailable'
                else:
                    variant_info['init_parameters'] = 'no_init_method'
                    
                # Check for model-specific features
                model_features = []
                if variant_name == 'EnhancedAutoencoder':
                    model_features.extend(['advanced_architecture', 'potential_attention'])
                elif variant_name == 'AutoencoderEnsemble':
                    model_features.extend(['multiple_models', 'diversity_mechanisms'])
                elif variant_name == 'SimpleAutoencoder':
                    model_features.extend(['basic_architecture', 'minimal_dependencies'])
                
                variant_info['model_features'] = model_features
                
                # Check device compatibility awareness
                device_aware = any(hasattr(variant_class, attr) for attr in ['device', 'to', 'cuda', 'cpu'])
                variant_info['device_aware'] = device_aware
                
            except Exception as e:
                variant_info['description'] = f"Error analyzing {variant_class.__name__ if variant_class else 'unknown'}"
                variant_info['analysis_error'] = str(e)
                variant_info['config_parameters'] = 'error'
                variant_info['init_parameters'] = 'error'
            
            variant_details[variant_name] = variant_info
        
        # Collect initialization metrics from the validation results
        try:
            # Extract metrics from validation status messages
            validation_metrics = {
                'available_count': len(available_variants),
                'warning_count': len(warning_variants),
                'failed_count': len(failed_variants),
                'unknown_count': len(unknown_variants),
                'total_validated': len(variant_status),
                'success_rate': len(available_variants) / len(MODEL_VARIANTS) * 100 if MODEL_VARIANTS else 0
            }
            
            # Proper success determination logic
            passed = len(available_variants) > 0  # Pass if we have ANY working variants
            fully_operational = len(available_variants) > 0 and len(failed_variants) == 0
            
            # Determine message and level based on results
            if fully_operational:
                message = f"Model variants system fully operational ({len(available_variants)} variants available)"
            elif len(available_variants) > 0:
                message = f"Model variants system operational with warnings ({len(available_variants)} available, {len(warning_variants)} with warnings)"
            elif len(warning_variants) > 0:
                message = f"Model variants system partially operational ({len(warning_variants)} variants with warnings)"
            else:
                message = "Model variants system failed - no operational variants"
            
        except Exception as metrics_error:
            initialization_metrics['metrics_error'] = str(metrics_error)
            passed = False
            message = "Error collecting initialization metrics"
            level = CheckLevel.CRITICAL
        
        # Compile comprehensive details
        model_variants_details = {
            'total_variants': len(MODEL_VARIANTS),
            'available_variants': len(available_variants),
            'warning_variants': len(warning_variants),
            'failed_variants': len(failed_variants),
            'unknown_variants': len(unknown_variants),
            'variant_names': list(MODEL_VARIANTS.keys()),
            'available_names': available_variants,
            'warning_names': warning_variants,
            'failed_names': failed_variants,
            'unknown_names': unknown_variants,
            'variant_status': variant_status,
            'variant_details': variant_details,
            'initialization_passed': len(available_variants) > 0,
            'hardware_context': hardware_info,
            'initialization_summary': {
                'attempted': len(MODEL_VARIANTS),
                'successful': len(available_variants),
                'warnings': len(warning_variants),
                'failed': len(failed_variants),
                'unknown': len(unknown_variants),
                'success_rate': len(available_variants) / len(MODEL_VARIANTS) * 100 if MODEL_VARIANTS else 0,
                'operational_status': 'fully_operational' if fully_operational else 'partial' if passed else 'failed'
            },
            'recommendations': {
                'primary_models': available_variants,
                'fallback_models': warning_variants,
                'models_to_avoid': failed_variants,
                'emergency_fallback': 'SimpleAutoencoder' if 'SimpleAutoencoder' in MODEL_VARIANTS else available_variants[0] if available_variants else None
            }
        }
        
        # Add operational capabilities summary
        capabilities = []
        for variant_name in available_variants + warning_variants:
            if variant_name == 'SimpleAutoencoder':
                capabilities.append('basic_autoencoding')
            elif variant_name == 'EnhancedAutoencoder':
                capabilities.append('advanced_features')
            elif variant_name == 'AutoencoderEnsemble':
                capabilities.append('ensemble_modeling')
            else:
                capabilities.append('custom_implementation')
        
        model_variants_details['capabilities'] = list(set(capabilities))
        
        return CheckResult(
            passed=passed,
            message=message,
            level=CheckLevel.CRITICAL,
            details=model_variants_details
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Model variants system check failed completely",
            level=CheckLevel.CRITICAL,
            details={
                'total_variants': 0,
                'available_variants': 0,
                'variant_names': [],
                'initialization_passed': False,
                'initialization_error': str(e),
                'error_type': type(e).__name__,
                'recommendation': 'Check system dependencies and configuration files'
            }
        ).with_exception(e)

def check_model_variants_wrapper() -> CheckResult:
    """
    Run comprehensive model variants check with rich table output.
    
    This wrapper has been updated to fully leverage the enhanced check_model_variants()
    implementation while maintaining rich display capabilities and comprehensive error handling.
    
    Returns:
        CheckResult object with detailed model variants information and rich display
    """
    try:
        # Get the enhanced model variants check results
        variants_result = check_model_variants()
        
        # If the check failed completely, return it as-is
        if not variants_result.passed and not variants_result.details.get('initialization_passed', False):
            return variants_result
        
        # Display the comprehensive model variants tables with enhanced formatting
        display_model_variants_details(variants_result)
        
        # Prepare enhanced summary details for the main table
        if isinstance(variants_result.details, dict):
            details = variants_result.details
            
            # Enhanced metrics extraction with safe type checking
            total_variants = details.get('total_variants', 0)
            
            # Safe extraction of available variants count
            available_variants_raw = details.get('available_variants', 0)
            if isinstance(available_variants_raw, list):
                available_variants = len(available_variants_raw)
            elif isinstance(available_variants_raw, int):
                available_variants = available_variants_raw
            else:
                available_variants = 0
            
            # Safe extraction of warning variants count
            warning_variants_raw = details.get('warning_variants', [])
            if isinstance(warning_variants_raw, list):
                warning_variants = len(warning_variants_raw)
            else:
                warning_variants = 0
            
            # Safe extraction of failed variants count
            failed_variants_raw = details.get('failed_variants', 0)
            if isinstance(failed_variants_raw, list):
                failed_variants = len(failed_variants_raw)
            elif isinstance(failed_variants_raw, int):
                failed_variants = failed_variants_raw
            else:
                failed_variants = 0
            
            # Enhanced success rate calculation
            initialization_summary = details.get('initialization_summary', {})
            success_rate = initialization_summary.get('success_rate', 0)
            operational_status = initialization_summary.get('operational_status', 'unknown')
            
            # Enhanced capabilities tracking
            capabilities = details.get('capabilities', [])
            if not isinstance(capabilities, list):
                capabilities = []
            
            # Hardware context for system awareness
            hardware_context = details.get('hardware_context', {})
            
            summary_details = {
                'total_variants': total_variants,
                'available_variants': available_variants,
                'warning_variants': warning_variants,
                'failed_variants': failed_variants,
                'variant_names': details.get('variant_names', []),
                'available_names': details.get('available_names', []),
                'warning_names': details.get('warning_names', []),
                'failed_names': details.get('failed_names', []),
                'success_rate': success_rate,
                'operational_status': operational_status,
                'initialization_passed': details.get('initialization_passed', False),
                'capabilities': capabilities,
                'hardware_aware': bool(hardware_context and not hardware_context.get('error')),
                'recommendations': details.get('recommendations', {}),
                'full_details': details
            }
        else:
            summary_details = variants_result.details
        
        # Enhanced message based on operational status
        operational_status = summary_details.get('operational_status', 'unknown')
        if operational_status == 'fully_operational':
            message = f"Model variants system fully operational ({summary_details['available_variants']} variants ready)"
        elif operational_status == 'partial':
            message = f"Model variants system partially operational ({summary_details['available_variants']} available, {summary_details['warning_variants']} with warnings)"
        else:
            message = variants_result.message
        
        return CheckResult(
            passed=variants_result.passed,
            message=message,
            level=variants_result.level,
            details=summary_details,
            metadata=variants_result.metadata,
            exception=variants_result.exception
        )
        
    except Exception as e:
        return CheckResult(
            passed=False,
            message="Model variants wrapper check failed",
            level=CheckLevel.CRITICAL,
            details={
                'wrapper_error': str(e),
                'error_type': type(e).__name__,
                'initialization_passed': False,
                'recommendation': 'Check system integration and display dependencies'
            }
        ).with_exception(e)

def display_model_variants_details(variants_result: CheckResult) -> None:
    """
    Display comprehensive model variants information in enhanced rich table format.
    
    This function has been updated to fully harmonize with the enhanced check_model_variants()
    implementation, providing detailed visual presentation of all available metrics,
    status categories, and system capabilities.
    
    Args:
        variants_result: CheckResult from check_model_variants() with comprehensive details
    """
    if not isinstance(variants_result.details, dict):
        console.print(Panel.fit(
            "[bold yellow]No detailed model variants information available[/bold yellow]\n"
            f"Details type: [bold yellow]{type(variants_result.details)}[/bold yellow]\n"
            #f"Check status: {'PASSED' if variants_result.passed else 'FAILED'}\n"
            f"Check status: [bold green]PASSED[/bold green]" if variants_result.passed else "[bold red]FAILED[/bold red]\n"
            f"Message: [bold yellow]{variants_result.message}[/bold yellow]",
            title="Model Variants Details",
            border_style="red",
            style="bold red"
        ))
        return
    
    try:
        details = variants_result.details
        
        # Enhanced metrics extraction with comprehensive type safety
        total_variants = details.get('total_variants', 0)
        
        # Safe extraction of all variant categories
        available_variants_raw = details.get('available_variants', 0)
        if isinstance(available_variants_raw, list):
            available_variants = len(available_variants_raw)
            available_names = available_variants_raw
        elif isinstance(available_variants_raw, int):
            available_variants = available_variants_raw
            available_names = details.get('available_names', [])
        else:
            available_variants = 0
            available_names = []
        
        warning_variants_raw = details.get('warning_variants', [])
        if isinstance(warning_variants_raw, list):
            warning_variants = len(warning_variants_raw)
            warning_names = warning_variants_raw
        else:
            warning_variants = 0
            warning_names = []
        
        failed_variants_raw = details.get('failed_variants', 0)
        if isinstance(failed_variants_raw, list):
            failed_variants = len(failed_variants_raw)
            failed_names = failed_variants_raw
        elif isinstance(failed_variants_raw, int):
            failed_variants = failed_variants_raw
            failed_names = details.get('failed_names', [])
        else:
            failed_variants = 0
            failed_names = []
        
        unknown_variants = details.get('unknown_variants', 0)
        unknown_names = details.get('unknown_names', [])
        
        # Enhanced success metrics
        initialization_summary = details.get('initialization_summary', {})
        success_rate = initialization_summary.get('success_rate', 0)
        operational_status = initialization_summary.get('operational_status', 'unknown')
        
        # Hardware context
        hardware_context = details.get('hardware_context', {})
        hardware_error = hardware_context.get('error') if isinstance(hardware_context, dict) else None
        
        # Capabilities summary
        capabilities = details.get('capabilities', [])
        if not isinstance(capabilities, list):
            capabilities = []
        
        # Recommendations
        recommendations = details.get('recommendations', {})
        
        # Main enhanced model variants overview table
        overview_table = Table(
            title="\n[bold cyan]Model Variants System Overview[/bold cyan]",
            title_justify="left",
            box=box.DOUBLE_EDGE,
            header_style="bold magenta",
            border_style="cyan",
            show_lines=True,
            width=min(110, console.width - 4)
        )
        
        overview_table.add_column("Category", style="bold yellow", width=18)
        overview_table.add_column("Count", style="bold white", width=12, justify="center")
        overview_table.add_column("Status", style="bold green", width=15, justify="center")
        overview_table.add_column("Details", style="bold", width=45)
        
        # Add comprehensive overview rows
        overview_table.add_row(
            "Total Variants",
            str(total_variants),
            "REGISTERED",
            f"[bold green]{', '.join(map(str, details.get('variant_names', [])))}[/bold green]" if details.get('variant_names') else "[bold red]No variants registered[/bold red]"
        )
        
        overview_table.add_row(
            "Available Variants",
            str(available_variants),
            "[bold green]READY[/bold green]" if available_variants > 0 else "[bold red]NONE[/bold red]",
            f"[bold green]{', '.join(map(str, available_names))}[/bold green]" if available_names else "[bold red]No available variants[/bold red]"
        )
        
        overview_table.add_row(
            "Warning Variants",
            str(warning_variants),
            "[bold yellow]WARNING[/bold yellow]" if warning_variants > 0 else "[bold green]NONE[/bold green]",
            f"[bold yellow]{', '.join(map(str, warning_names))}[/bold yellow]" if warning_names else "[bold green]No warnings[/bold green]"
        )
        
        overview_table.add_row(
            "Failed Variants",
            str(failed_variants),
            "[bold red]FAILED[/bold red]" if failed_variants > 0 else "[bold green]NONE[/bold green]",
            f"[bold red]{', '.join(map(str, failed_names))}[/bold red]" if failed_names else "[bold green]No failures[/bold green]"
        )
        
        overview_table.add_row(
            "Success Rate",
            f"{success_rate:.1f}%",
            "[bold green]EXCELLENT[/bold green]" if success_rate >= 80 else "[bold cyan]GOOD[/bold cyan]" if success_rate >= 60 else "[bold yellow]FAIR[/bold yellow]" if success_rate > 0 else "[bold red]POOR[/bold red]",
            f"[bold white]Operational:[/bold white] {operational_status.replace('_', ' ').title()}"
        )
        
        overview_table.add_row(
            "Capabilities",
            str(len(capabilities)),
            "[bold green]READY[/bold green]" if capabilities else "[bold red]NONE[/bold red]",
            f"[bold cyan]{', '.join(capabilities)}[/bold cyan]" if capabilities else "[bold red]No capabilities detected[/bold red]"
        )
        
        overview_table.add_row(
            "Hardware Context",
            "Available" if not hardware_error else "Error",
            "[bold green]OK[/bold green]" if not hardware_error else "[bold red]ERROR[/bold red]",
            #f"[bold cyan]{'System aware' if not hardware_error else f'Hardware error: {hardware_error}'}[/bold cyan]"
            f"[bold cyan]SYSTEM-AWARE[/bold cyan]" if not hardware_error else f"[bold red]HARDWARE ERROR: {hardware_error}[/bold red]"
        )
        
        console.print(overview_table)
        console.print()
        
        # Enhanced variant details table with comprehensive information
        variant_details = details.get('variant_details', {})
        if isinstance(variant_details, dict) and variant_details:
            variants_table = Table(
                title="[bold cyan]Model Variants Detailed Analysis[/bold cyan]",
                title_justify="left",
                box=box.DOUBLE_EDGE,
                header_style="bold magenta",
                border_style="blue",
                show_lines=True,
                expand=True,
                width=min(125, console.width - 4)
            )
            
            variants_table.add_column("Variant", style="bold yellow", width=18)
            variants_table.add_column("Class", style="bold magenta", width=20)
            variants_table.add_column("Status", width=12, justify="left")
            variants_table.add_column("Configuration", style="bold", width=15)
            variants_table.add_column("Methods", style="bold", width=10)
            variants_table.add_column("Details", style="bold", min_width=40)
            
            for variant_name, variant_info in variant_details.items():
                if not isinstance(variant_info, dict):
                    continue
                    
                # Enhanced detail formatting
                detail_lines = []
                
                # Description
                description = variant_info.get('description', '')
                if description and isinstance(description, str):
                    detail_lines.append(f"[bold white]Desc:[/] [bold yellow]{description[:60]}{'...' if len(description) > 60 else ''}[/bold yellow]")
                
                # Model features
                model_features = variant_info.get('model_features', [])
                if model_features:
                    detail_lines.append(f"[bold white]Features:[/] [bold cyan]{', '.join(model_features[:3])}[/bold cyan]")
                
                # Device awareness
                if variant_info.get('device_aware'):
                    detail_lines.append(f"[bold white]Device:[/] [bold green]Aware[/bold green]")
                
                # Error information
                analysis_error = variant_info.get('analysis_error')
                if analysis_error:
                    detail_lines.append(f"[bold red]Error: {str(analysis_error)[:50]}...[/bold red]")
                
                details_text = "\n".join(detail_lines) if detail_lines else "[bold blue]Standard implementation[/bold blue]"
                
                # Enhanced status styling
                status = variant_info.get('status', 'unknown')
                status_str = str(status) if status is not None else 'unknown'
                
                if status_str == 'available':
                    status_style = "bold green"
                    status_text = "AVAILABLE"
                elif status_str.startswith('warning'):
                    status_style = "bold yellow"
                    status_text = "WARNING"
                elif status_str.startswith('error') or 'failed' in status_str:
                    status_style = "bold red"
                    status_text = "FAILED"
                else:
                    status_style = "bold white"
                    status_text = f"{status_str.upper()}"
                
                # Configuration information
                config_params = variant_info.get('config_parameters', 'unknown')
                if config_params == 'error':
                    config_text = "[bold red]ERROR[/bold red]"
                elif config_params == 'none':
                    config_text = "[bold]NONE[/bold]"
                elif isinstance(config_params, (int, float)):
                    config_text = f"[bold cyan]{config_params}[/bold cyan]"
                else:
                    config_text = f"[bold yellow]{str(config_params)}[/bold yellow]"
                
                # Methods information
                supported_methods = variant_info.get('supported_methods', {})
                if isinstance(supported_methods, dict):
                    available_methods = [method for method, available in supported_methods.items() if available]
                    methods_text = f"[bold green]{len(available_methods)}[/bold green]" if available_methods else "[bold red]0[/bold red]"
                else:
                    methods_text = "[bold yellow]Unknown[/bold yellow]"
                
                # Safe extraction of class name
                class_name = variant_info.get('class_name', 'Unknown')
                class_name_str = str(class_name) if class_name is not None else 'Unknown'
                
                variants_table.add_row(
                    Text(str(variant_name), style="bold yellow"),
                    class_name_str,
                    Text(status_text, style=status_style),
                    config_text,
                    methods_text,
                    details_text
                )
            
            console.print(variants_table)
            console.print()
        
        # Enhanced recommendations panel
        if recommendations and isinstance(recommendations, dict):
            rec_table = Table(
                title="[bold green]Model Variants Recommendations[/bold green]",
                title_justify="left",
                #box=box.SQUARE,
                box=box.DOUBLE_EDGE,
                header_style="bold white",
                border_style="green",
                width=min(100, console.width - 4)
            )
            
            rec_table.add_column("Category", style="bold yellow", width=20)
            rec_table.add_column("Suggested Models", style="bold cyan")
            
            primary_models = recommendations.get('primary_models', [])
            fallback_models = recommendations.get('fallback_models', [])
            models_to_avoid = recommendations.get('models_to_avoid', [])
            emergency_fallback = recommendations.get('emergency_fallback')
            
            if primary_models:
                rec_table.add_row("Primary Models", ", ".join(map(str, primary_models)), style="bold green")
            if fallback_models:
                rec_table.add_row("Fallback Models", ", ".join(map(str, fallback_models)), style="bold yellow")
            if models_to_avoid:
                rec_table.add_row("Avoid Models", ", ".join(map(str, models_to_avoid)), style="bold red")
            if emergency_fallback:
                rec_table.add_row("Emergency Fallback", str(emergency_fallback), style="bold magenta")
            
            if primary_models or fallback_models or models_to_avoid:
                console.print(rec_table)
                console.print()
        
        # Enhanced operational status summary with corrected styling
        if operational_status == 'fully_operational':
            border_color = "green"
        elif operational_status == 'partial':
            border_color = "yellow"
        else:
            border_color = "red"
        
        status_panel = Panel.fit(
            f"Operational Status: {operational_status.replace('_', ' ').title()}\n"
            f"Available Models: {available_variants}\n"
            f"Models with Warnings: {warning_variants}\n"
            f"Failed Models: {failed_variants}\n"
            f"Overall Success Rate: {success_rate:.1f}%",
            title="MODEL SUMMARY",
            style=f"bold {border_color}",
            border_style=border_color
        )
        
        console.print(status_panel)
        
    except Exception as e:
        console.print(
            Panel.fit(
                f"Error displaying enhanced model variants details:\n"
                f"Error: {str(e)}\n"
                f"Error Type: {type(e).__name__}\n"
                #f"Check Status: {'PASSED' if variants_result.passed else 'FAILED'}\n"
                f"Check status: [bold green]PASSED[/bold green]" if variants_result.passed else "[bold red]FAILED[/bold red]\n"
                f"Message: {variants_result.message}",
                title="Enhanced Display Rendering Error",
                border_style="red",
                style="bold red"
            )
        )
        
        # Comprehensive fallback display
        try:
            #console.print("\n[bold yellow]Comprehensive Fallback Display:[/bold yellow]")
            print(Fore.YELLOW + Style.BRIGHT + "\nComprehensive Fallback Display:")
            
            # Basic check information
            #console.print(f"[cyan]Check Result:[/cyan] {'PASSED' if variants_result.passed else 'FAILED'}")
            #console.print(f"[cyan]Message:[/cyan] {variants_result.message}")
            #console.print(f"[cyan]Level:[/cyan] {variants_result.level}")
            
            variants_result_status = 'PASSED' if variants_result.passed else 'FAILED'
            if variants_result_status == 'PASSED':
                status_color = Fore.GREEN + Style.BRIGHT
            else:
                status_color = Fore.RED + Style.BRIGHT
            
            print(Fore.YELLOW + Style.BRIGHT + "Check Result: " + f"{status_color}" + f"{variants_result_status}")
            print(Fore.YELLOW + Style.BRIGHT + "Message: " + f"{status_color}" + f"{variants_result.message}")
            print(Fore.YELLOW + Style.BRIGHT + "Level: " + f"{status_color}" + f"{variants_result.level}")
            
            if isinstance(variants_result.details, dict):
                details = variants_result.details
                
                # Enhanced metrics extraction
                total_variants = details.get('total_variants', 0)
                
                # Safe variant counting
                available_count = 0
                warning_count = 0
                failed_count = 0
                
                available_names = details.get('available_names', [])
                warning_names = details.get('warning_names', [])
                failed_names = details.get('failed_names', [])
                
                if isinstance(available_names, list):
                    available_count = len(available_names)
                    available_count_status = f"{Fore.GREEN + Style.BRIGHT}{available_count}{Style.RESET_ALL}" if available_count > 0 else f"{Fore.RED + Style.BRIGHT}{available_count}{Style.RESET_ALL}"
                if isinstance(warning_names, list):
                    warning_count = len(warning_names)
                    warning_count_status = f"{Fore.YELLOW + Style.BRIGHT}{warning_count}{Style.RESET_ALL}" if warning_count > 0 else f"{Fore.GREEN + Style.BRIGHT}{warning_count}{Style.RESET_ALL}"
                if isinstance(failed_names, list):
                    failed_count = len(failed_names)
                    failed_count_status = f"{Fore.RED + Style.BRIGHT}{failed_count}{Style.RESET_ALL}" if failed_count > 0 else f"{Fore.GREEN + Style.BRIGHT}{failed_count}{Style.RESET_ALL}"
                
                success_rate = details.get('initialization_summary', {}).get('success_rate', 0)
                success_rate_status = f"{Fore.GREEN + Style.BRIGHT}{success_rate:.1f}%{Style.RESET_ALL}" if success_rate >= 80 else f"{Fore.YELLOW + Style.BRIGHT}{success_rate:.1f}%{Style.RESET_ALL}" if success_rate >= 60 else f"{Fore.RED + Style.BRIGHT}{success_rate:.1f}%{Style.RESET_ALL}"
                
                operational_status = details.get('initialization_summary', {}).get('operational_status', 'unknown')
                operational_status_details = operational_status.replace('_', '').title()
                
                if operational_status_details == 'Fully Operational':
                    operational_status = f"{Fore.GREEN + Style.BRIGHT}{operational_status_details}{Style.RESET_ALL}"
                elif operational_status_details == 'Partial':
                    operational_status = f"{Fore.YELLOW + Style.BRIGHT}{operational_status_details}{Style.RESET_ALL}"
                else:
                    operational_status = f"{Fore.RED + Style.BRIGHT}{operational_status_details}{Style.RESET_ALL}"
                
                # console.print(f"[cyan]Variants:[/cyan] {available_count} available, {warning_count} warnings, {failed_count} failed (of {total_variants} total)")
                # console.print(f"[cyan]Success Rate:[/cyan] {success_rate:.1f}%")
                # console.print(f"[cyan]Operational Status:[/cyan] {operational_status.replace('_', ' ').title()}")
                
                print(Fore.YELLOW + Style.BRIGHT + f"Variants: {available_count_status} available, {warning_count_status} warnings, {failed_count_status} failed (of {total_variants} total)")
                print(Fore.YELLOW + Style.BRIGHT + f"Success Rate: {success_rate_status}")
                print(Fore.YELLOW + Style.BRIGHT + f"Operational Status: {operational_status}")
                
                # Display variant names safely
                if available_names:
                    safe_names = [str(name) for name in available_names if name is not None]
                    #console.print(f"[green]Available:[/green] {', '.join(safe_names)}")
                    print(Fore.YELLOW + Style.BRIGHT + f"Available: " + Fore.GREEN + Style.BRIGHT + f"{', '.join(safe_names)}")
                if warning_names:
                    safe_names = [str(name) for name in warning_names if name is not None]
                    #console.print(f"[yellow]Warnings:[/yellow] {', '.join(safe_names)}")
                    print(Fore.YELLOW + Style.BRIGHT + f"Warnings: " + Fore.GREEN + Style.BRIGHT + f"{', '.join(safe_names)}")
                if failed_names:
                    safe_names = [str(name) for name in failed_names if name is not None]
                    #console.print(f"[red]Failed:[/red] {', '.join(safe_names)}")
                    print(Fore.YELLOW + Style.BRIGHT + f"Failed: " + Fore.RED + Style.BRIGHT + f"{', '.join(safe_names)}")
                
                # Capabilities
                capabilities = details.get('capabilities', [])
                if capabilities:
                    #console.print(f"[cyan]Capabilities:[/cyan] {', '.join(capabilities)}")
                    print(Fore.YELLOW + Style.BRIGHT + f"Capabilities: " + Fore.CYAN + Style.BRIGHT + f"{', '.join(capabilities)}")
                
            else:
                #console.print("[yellow]Details format not recognized for fallback display[/yellow]")
                print(Fore.YELLOW + Style.BRIGHT + "Details format not recognized for fallback display")
                
        except Exception as fallback_error:
            #console.print(f"[red]Comprehensive fallback display also failed: {fallback_error}[/red]")
            print(Fore.RED + Style.BRIGHT + f"Comprehensive fallback display also failed: " + Fore.YELLOW + Style.BRIGHT + f"{str(fallback_error)}")
            
            #console.print(f"[red]Original display error: {str(e)}[/red]")
            print(Fore.RED + Style.BRIGHT + f"Original display error: " + Fore.YELLOW + Style.BRIGHT + f"{str(e)}")

# System and environment configuration
def configure_system() -> Dict[str, Any]:
    """
    Configure system-wide settings for optimal performance and logging.
    
    Returns:
        Dictionary containing all applied configurations with validation data
    """
    config = {
        'torch': {},
        'numpy': {},
        'warnings': {},
        'environment': {},
        'validation': {},
        'optimizations': {},
        'detected_capabilities': {}
    }

    # Get comprehensive system information
    try:
        version_info = check_versions(include_optional=True)
        hardware_data = check_hardware(min_disk_gb=1.0, include_memory_usage=True)
        
        config['validation']['versions'] = version_info
        config['validation']['hardware'] = hardware_data
        
        # Extract key capabilities
        cpu_info = hardware_data.get('cpu_cores', {})
        ram_info = hardware_data.get('system_ram', {})
        cuda_info = hardware_data.get('cuda', {})
        
        logical_cores = cpu_info.get('logical_cores', os.cpu_count() or 1)
        total_ram_gb = ram_info.get('ram_total_gb', 4.0)
        cuda_available = cuda_info.get('available', False)
        gpu_count = cuda_info.get('gpu_count', 0)
        
        config['detected_capabilities'] = {
            'cpu_cores': logical_cores,
            'ram_gb': total_ram_gb,
            'cuda_available': cuda_available,
            'gpu_count': gpu_count,
            'hyperthreading': cpu_info.get('hyperthreading', False)
        }
        
    except Exception as e:
        # Fallback to basic detection
        logical_cores = os.cpu_count() or 1
        total_ram_gb = 4.0
        cuda_available = torch.cuda.is_available()
        gpu_count = torch.cuda.device_count() if cuda_available else 0
        config['validation']['error'] = str(e)

    # Intelligent thread configuration based on detected hardware
    # Conservative approach
    optimal_threads = min(4, max(1, logical_cores // 2))
    if total_ram_gb > 16:
        # More threads if more RAM
        optimal_threads = min(8, logical_cores)
    elif total_ram_gb < 4:
        # Fewer threads if limited RAM
        optimal_threads = min(2, logical_cores)
    
    # Environment variable configurations
    env_vars = {
        # TensorFlow logging
        'TF_CPP_MIN_LOG_LEVEL': '3',
        # Intel MKL warnings
        'KMP_WARNINGS': '0',
        # OpenMP threads - now optimized based on hardware
        'OMP_NUM_THREADS': str(optimal_threads),
        # MKL threads - now optimized based on hardware
        'MKL_NUM_THREADS': str(optimal_threads),
        # For CUDA reproducibility
        'CUBLAS_WORKSPACE_CONFIG': ':4096:8',
        # Disable TensorFlow GPU if CUDA not available
        'CUDA_VISIBLE_DEVICES': '' if not cuda_available else None
    }
    
    # Add memory-aware configurations
    if total_ram_gb > 32:
        env_vars['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    elif total_ram_gb < 8:
        env_vars['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    # Apply environment variables
    for key, value in env_vars.items():
        if value is not None:
            os.environ[key] = value
            config['environment'][key] = value

    # PyTorch configuration
    torch_config = {
        'deterministic': True,
        # Enable benchmark if CUDA available
        'benchmark': cuda_available and gpu_count > 0,
        'float32_matmul_precision': 'high' if cuda_available else 'highest',
        'num_threads': optimal_threads,
        'precision': 4,
        'sci_mode': False,
        'cuda_memory_fraction': 0.8 if cuda_available else None
    }
    
    # Apply PyTorch configurations
    torch.set_num_threads(torch_config['num_threads'])
    if cuda_available:
        torch.backends.cudnn.deterministic = torch_config['deterministic']
        torch.backends.cudnn.benchmark = torch_config['benchmark']
        
        # Configure CUDA memory management based on GPU memory
        for gpu in cuda_info.get('gpus', []):
            gpu_memory_gb = gpu.get('memory_gb', 0)
            if gpu_memory_gb > 16:
                torch.cuda.set_per_process_memory_fraction(0.9)
            elif gpu_memory_gb < 6:
                torch.cuda.set_per_process_memory_fraction(0.7)
    
    torch.set_printoptions(
        precision=torch_config['precision'],
        sci_mode=torch_config['sci_mode']
    )
    
    config['torch'].update(torch_config)

    # NumPy configuration
    np_config = {
        'precision': 4,
        'suppress': True,
        # More output if more RAM
        'threshold': 1000 if total_ram_gb > 8 else 100,
        'linewidth': 120,
        'float_division_warning': False
    }
    
    np.set_printoptions(
        precision=np_config['precision'],
        suppress=np_config['suppress'],
        threshold=np_config['threshold'],
        linewidth=np_config['linewidth']
    )
    config['numpy'].update(np_config)

    # Warning configurations
    warning_config = {
        'ignored_categories': {
            UserWarning: ['joblib', 'torch', 'numpy'],
            FutureWarning: None,
            DeprecationWarning: None,
            ConvergenceWarning: ['sklearn'],
            RuntimeWarning: None,
        },
        'simplefilter': 'ignore'
    }
    
    # Add version-specific warning filters
    torch_info = version_info.get('PyTorch', {})
    if torch_info.get('available', False):
        torch_version = torch_info.get('version', '')
        if torch_version.startswith('2.'):
            warning_config['ignored_categories'][DeprecationWarning] = ['torch']
    
    # Apply warning filters
    for category, modules in warning_config['ignored_categories'].items():
        if modules:
            for module in modules:
                warnings.filterwarnings('ignore', category=category, module=module)
        else:
            warnings.filterwarnings('ignore', category=category)
    
    warnings.simplefilter(warning_config['simplefilter'])
    config['warnings'].update(warning_config)
    
    # Record optimizations applied
    config['optimizations'] = {
        'thread_optimization': f"Set to {optimal_threads} threads based on {logical_cores} cores and {total_ram_gb:.1f}GB RAM",
        'cuda_optimization': f"CUDA benchmark {'enabled' if torch_config['benchmark'] else 'disabled'}",
        'memory_optimization': f"Configured for {total_ram_gb:.1f}GB RAM",
        'version_optimizations': f"Applied optimizations for {len([v for v in version_info.values() if v.get('available', False)])} available packages"
    }

    return config

# Reproducibility configuration
def set_seed(seed: int = 42, hardware_info: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Configure all random seeds for full reproducibility.
    
    Args:
        seed: Base seed value
        hardware_info: Hardware information from check_hardware() (optional)
        
    Returns:
        Dictionary containing the seed configuration with hardware context
    """
    # Get hardware info if not provided
    if hardware_info is None:
        try:
            hardware_info = check_hardware(include_memory_usage=False)
        except Exception:
            hardware_info = {}
    
    cuda_info = hardware_info.get('cuda', {})
    cuda_available = cuda_info.get('available', False)
    gpu_count = cuda_info.get('gpu_count', 0)
    
    seed_config = {
        'base_seed': seed,
        'hardware_context': {
            'cuda_available': cuda_available,
            'gpu_count': gpu_count,
            'optimization_level': 'high' if cuda_available else 'standard'
        },
        'python': {
            'random_seed': seed,
            'hash_seed': seed
        },
        'numpy_seed': seed,
        'torch': {
            'cpu_seed': seed,
            'cuda_seeds': None,
            'cuda_deterministic': False,
            'cuda_benchmark': False
        },
        'environment': {
            'PYTHONHASHSEED': str(seed),
            'CUBLAS_WORKSPACE_CONFIG': ':4096:8'
        },
        'tensorflow_seed': None,
        'per_gpu_seeds': []
    }
    
    # Set Python seeds
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    # Set NumPy seed
    np.random.seed(seed)
    
    # Set PyTorch seeds with hardware awareness
    torch.manual_seed(seed)
    
    if cuda_available:
        # Set different seeds for each GPU to ensure variety while maintaining reproducibility
        gpu_seeds = []
        for i in range(gpu_count):
            # Deterministic but different per GPU
            gpu_seed = seed + i
            torch.cuda.manual_seed(gpu_seed)
            gpu_seeds.append(gpu_seed)
        
        # Also set the global seed
        torch.cuda.manual_seed_all(seed)
        
        # Configure deterministic operations based on GPU capabilities
        torch.backends.cudnn.deterministic = True
        # Disable for reproducibility
        torch.backends.cudnn.benchmark = False
        
        # Use hardware-aware CUDA configuration
        gpus = cuda_info.get('gpus', [])
        if gpus:
            # Configure based on GPU compute capability
            for i, gpu in enumerate(gpus):
                compute_cap = gpu.get('compute_capability', '0.0')
                major_version = int(float(compute_cap))
                if major_version >= 7:  # Tensor cores available
                    # For reproducibility
                    torch.backends.cuda.matmul.allow_tf32 = False
        
        seed_config['torch'].update({
            'cuda_seeds': gpu_seeds,
            'cuda_deterministic': True,
            'cuda_benchmark': False,
            'global_cuda_seed': seed
        })
        seed_config['per_gpu_seeds'] = gpu_seeds
    
    # Set CUDA workspace config for deterministic operations
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    
    # Set TensorFlow seed if available
    try:
        version_info = check_versions(include_optional=False)
        tf_available = any('tensorflow' in str(v).lower() for v in version_info.values())
        if tf_available:
            import tensorflow as tf
            tf.random.set_seed(seed)
            seed_config['tensorflow_seed'] = seed
    except (ImportError, Exception):
        pass
    
    return seed_config

# Hardware and Package Configuration
def setup_gpu(logger: logging.Logger, hardware_data: Optional[Dict[str, Any]] = None) -> torch.device:
    """
    Detect and configure the primary compute device with full hardware awareness.
    
    Args:
        logger: Logger instance for recording device information
        hardware_data: Hardware data from check_hardware() (optional)
        
    Returns:
        Configured torch.device with optimal settings applied
    """
    # Get comprehensive hardware data if not provided
    if hardware_data is None:
        try:
            hardware_data = check_hardware(include_memory_usage=True)
        except Exception as e:
            logger.warning(f"Could not get hardware data: {e}")
            hardware_data = {}
    
    # Extract hardware information
    cuda_info = hardware_data.get('cuda', {})
    cpu_info = hardware_data.get('cpu_cores', {})
    ram_info = hardware_data.get('system_ram', {})
    
    cuda_available = cuda_info.get('available', False)
    gpu_count = cuda_info.get('gpu_count', 0)
    gpus = cuda_info.get('gpus', [])
    logical_cores = cpu_info.get('logical_cores', os.cpu_count() or 1)
    total_ram_gb = ram_info.get('ram_total_gb', 4.0)
    
    device = torch.device('cpu')
    device_info = {
        'type': 'CPU',
        'count': logical_cores,
        'details': cpu_info.get('capacity', {}),
        'optimization_applied': []
    }
    
    if cuda_available and gpu_count > 0:
        # Select the best GPU based on memory and compute capability
        best_gpu_idx = 0
        best_gpu_score = 0
        
        for i, gpu in enumerate(gpus):
            memory_gb = gpu.get('memory_gb', 0)
            compute_cap = float(gpu.get('compute_capability', '0.0'))
            
            # Score based on memory and compute capability
            score = memory_gb * 10 + compute_cap * 100
            
            # Consider current memory usage if available
            if 'current_usage' in gpu:
                usage_percent = gpu['current_usage'].get('percent_allocated', 0)
                # Prefer less utilized GPUs
                score *= (1 - usage_percent / 100)
            
            if score > best_gpu_score:
                best_gpu_score = score
                best_gpu_idx = i
        
        device = torch.device(f'cuda:{best_gpu_idx}')
        selected_gpu = gpus[best_gpu_idx]
        
        device_info.update({
            'type': 'CUDA',
            'device_id': best_gpu_idx,
            'count': gpu_count,
            'selected_gpu': selected_gpu,
            'cuda_version': cuda_info.get('cuda_version'),
            'cudnn_version': cuda_info.get('cudnn_version'),
            'all_gpus': gpus,
            'selection_score': best_gpu_score
        })
        
        # Apply GPU-specific optimizations
        gpu_memory_gb = selected_gpu.get('memory_gb', 0)
        compute_cap = float(selected_gpu.get('compute_capability', '0.0'))
        
        optimizations = []
        
        # Memory management based on GPU memory
        if gpu_memory_gb > 16:
            torch.backends.cudnn.benchmark = True
            torch.cuda.set_per_process_memory_fraction(0.9, best_gpu_idx)
            optimizations.append("High memory GPU: enabled cuDNN benchmark, 90% memory fraction")
        elif gpu_memory_gb > 8:
            torch.backends.cudnn.benchmark = True
            torch.cuda.set_per_process_memory_fraction(0.8, best_gpu_idx)
            optimizations.append("Medium memory GPU: enabled cuDNN benchmark, 80% memory fraction")
        else:
            torch.backends.cudnn.benchmark = False
            torch.cuda.set_per_process_memory_fraction(0.7, best_gpu_idx)
            optimizations.append("Low memory GPU: disabled cuDNN benchmark, 70% memory fraction")
        
        # Compute capability optimizations
        # Turing+ architecture
        if compute_cap >= 7.5:
            torch.backends.cuda.matmul.allow_tf32 = True
            optimizations.append("Modern GPU: enabled TF32 for performance")
        # Volta architecture
        elif compute_cap >= 7.0:
            torch.backends.cuda.matmul.allow_tf32 = False
            optimizations.append("Volta GPU: disabled TF32 for compatibility")
        
        # Multi-GPU optimizations
        if gpu_count > 1:
            # Enable peer-to-peer access if supported
            try:
                for i in range(gpu_count):
                    for j in range(gpu_count):
                        if i != j and torch.cuda.can_device_access_peer(i, j):
                            torch.cuda.device_enable_peer_access(i, j)
                optimizations.append(f"Multi-GPU: enabled peer access for {gpu_count} GPUs")
            except Exception as e:
                logger.debug(f"Could not enable peer access: {e}")
        
        # System RAM vs GPU memory optimization
        if total_ram_gb < gpu_memory_gb:
            torch.backends.cudnn.deterministic = True
            optimizations.append("Low system RAM: enabled deterministic mode to reduce memory usage")
        
        device_info['optimization_applied'] = optimizations
        
        # Set the device for current context
        torch.cuda.set_device(best_gpu_idx)
        
    else:
        # CPU-only optimizations based on hardware
        optimizations = []
        
        if logical_cores > 8:
            torch.set_num_threads(min(8, logical_cores))
            optimizations.append(f"High-core CPU: limited threads to {min(8, logical_cores)}")
        elif logical_cores <= 2:
            torch.set_num_threads(logical_cores)
            optimizations.append(f"Low-core CPU: using all {logical_cores} threads")
        
        if total_ram_gb > 16:
            torch.set_num_interop_threads(4)
            optimizations.append("High RAM: increased interop threads")
        elif total_ram_gb < 8:
            torch.set_num_interop_threads(1)
            optimizations.append("Low RAM: reduced interop threads")
        
        device_info['optimization_applied'] = optimizations
    
    # Log comprehensive device information
    logger.info(f"Primary device configured: {device}")
    logger.info(f"Device type: {device_info['type']}")
    logger.info(f"Available resources: {device_info['count']} {'GPU(s)' if cuda_available else 'CPU cores'}")
    
    if cuda_available:
        selected_gpu = device_info['selected_gpu']
        logger.info(f"Selected GPU: {selected_gpu['name']} ({selected_gpu['memory_gb']:.1f}GB)")
        logger.info(f"Compute capability: {selected_gpu['compute_capability']}")
        
        if 'current_usage' in selected_gpu:
            usage = selected_gpu['current_usage']
            logger.info(f"Current GPU memory usage: {usage['allocated_mb']:.0f}MB allocated")
    
    # Log optimizations applied
    for opt in device_info['optimization_applied']:
        logger.info(f"Applied optimization: {opt}")
    
    return device

def log_system_configuration(
    logger: logging.Logger,
    include_versions: bool = True,
    include_hardware: bool = True,
    include_seed_config: bool = True
) -> None:
    """
    Log comprehensive system configuration using existing check functions.
    Replaces individual logging functions with unified configuration logging.
    
    Args:
        logger: Logger instance
        include_versions: Whether to log version information
        include_hardware: Whether to log hardware information  
        include_seed_config: Whether to log seed configuration
    """
    try:
        logger.info("=" * 80)
        logger.info("SYSTEM CONFIGURATION SUMMARY")
        logger.info("=" * 80)
        
        # Get comprehensive system info
        system_info = get_system_info(include_versions, include_hardware)
        
        # Log platform information
        platform_info = system_info.get('platform', {})
        logger.info(f"Platform: {platform_info.get('platform', 'Unknown')}")
        logger.info(f"System: {platform_info.get('system', 'Unknown')} {platform_info.get('release', '')}")
        logger.info(f"Architecture: {platform_info.get('architecture', ['Unknown'])[0]}")
        logger.info(f"Processor: {platform_info.get('processor', 'Unknown')}")
        
        # Log Python information
        python_info = system_info.get('python', {})
        version_info = python_info.get('version_info', {})
        logger.info(f"Python: {version_info.get('major', '?')}.{version_info.get('minor', '?')}.{version_info.get('micro', '?')}")
        
        # Log hardware information if available
        if include_hardware and 'hardware' in system_info:
            hardware_data = system_info['hardware']
            logger.info("\n[Hardware Configuration]")
            
            # CPU information
            cpu_info = hardware_data.get('cpu_cores', {})
            if cpu_info.get('available'):
                logger.info(f"{'CPU Cores':>20}: {cpu_info.get('logical_cores', 'Unknown')} logical, {cpu_info.get('physical_cores', 'Unknown')} physical")
                if cpu_info.get('hyperthreading'):
                    logger.info(f"{'Hyperthreading':>20}: Enabled")
            
            # RAM information
            ram_info = hardware_data.get('system_ram', {})
            if ram_info.get('available'):
                logger.info(f"{'System RAM':>20}: {ram_info.get('ram_total_gb', 0):.1f}GB total, {ram_info.get('ram_available_gb', 0):.1f}GB available")
            
            # GPU information
            cuda_info = hardware_data.get('cuda', {})
            if cuda_info.get('available'):
                gpu_count = cuda_info.get('gpu_count', 0)
                logger.info(f"{'CUDA':>20}: Available with {gpu_count} GPU(s)")
                for i, gpu in enumerate(cuda_info.get('gpus', [])):
                    logger.info(f"{'GPU ' + str(i):>20}: {gpu.get('name', 'Unknown')} ({gpu.get('memory_gb', 0):.1f}GB)")
            else:
                logger.info(f"{'CUDA':>20}: Not available")
            
            # Disk space
            disk_info = hardware_data.get('disk_space', {})
            if disk_info.get('available') is not None:
                logger.info(f"{'Disk Space':>20}: {disk_info.get('free_gb', 0):.1f}GB free of {disk_info.get('total_gb', 0):.1f}GB total")
        
        # Log package versions if available
        if include_versions and 'package_versions' in system_info:
            logger.info("\n[Package Versions]")
            package_versions = system_info['package_versions']
            
            # Core packages first
            core_packages = [name for name, info in package_versions.items() if info.get('required', False)]
            for package in core_packages:
                info = package_versions[package]
                status = "[OK]" if info.get('compatible', False) else "[FAIL]"
                logger.info(f"{status} {package:>18}: {info.get('version', 'Unknown')}")
            
            # Optional packages
            optional_packages = [name for name, info in package_versions.items() if not info.get('required', False)]
            if optional_packages:
                logger.info("\n[Optional Packages]")
                # Limit to avoid spam
                for package in optional_packages[:10]:
                    info = package_versions[package]
                    if info.get('available', False):
                        logger.info(f"- {package:>18}: {info.get('version', 'Available')}")
        
        # Log seed configuration if requested
        if include_seed_config:
            try:
                seed_result = check_seed_config()
                if seed_result.passed:
                    logger.info(f"\n[Reproducibility]: Configured ({seed_result.metadata.get('compliance_score', 0)}% compliance)")
                else:
                    logger.warning(f"\n[Reproducibility]: Issues detected ({seed_result.metadata.get('compliance_score', 0)}% compliance)")
            except Exception as e:
                logger.debug(f"Could not check seed configuration: {e}")
        
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"Failed to log system configuration: {e}")

# Helper functions for system diagnostics and error handling
def enhanced_global_exception_handler(exc_type, exc_value, exc_traceback):
    """
    Enhanced global exception handler with detailed logging and recovery.
    """
    if issubclass(exc_type, KeyboardInterrupt):
        logger.info("System interrupted by user")
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    
    # Log the exception with full context
    logger.critical("CRITICAL: Uncaught exception occurred", exc_info=(exc_type, exc_value, exc_traceback))
    
    try:
        # Get comprehensive system state using existing functions
        system_state = {
            'timestamp': datetime.now().isoformat(),
            'exception_type': exc_type.__name__,
            'exception_module': getattr(exc_type, '__module__', 'unknown'),
            'exception_message': str(exc_value),
            'traceback': traceback.format_exception(exc_type, exc_value, exc_traceback),
            'traceback_summary': traceback.format_exception_only(exc_type, exc_value)
        }
        
        # Add comprehensive system information using existing functions
        try:
            system_state['system_info'] = get_system_info(include_versions=True, include_hardware=True)
        except Exception as e:
            system_state['system_info_error'] = str(e)
        
        # Add hardware state with memory usage
        try:
            system_state['hardware_state'] = check_hardware(include_memory_usage=True)
        except Exception as e:
            system_state['hardware_state_error'] = str(e)
        
        # Add version information
        try:
            system_state['package_versions'] = check_versions(include_optional=True)
        except Exception as e:
            system_state['package_versions_error'] = str(e)
        
        # Add seed configuration state
        try:
            seed_check = check_seed_config()
            system_state['seed_config'] = {
                'passed': seed_check.passed,
                'compliance_score': seed_check.metadata.get('compliance_score', 0),
                'details': seed_check.details
            }
        except Exception as e:
            system_state['seed_config_error'] = str(e)
        
        # Add memory clearing attempt results
        try:
            clear_results = enhanced_clear_memory(aggressive=False)
            system_state['memory_state'] = clear_results
        except Exception as e:
            system_state['memory_state_error'] = str(e)
        
        # Add process information
        try:
            proc = psutil.Process()
            system_state['process_info'] = {
                'pid': proc.pid,
                'name': proc.name(),
                'memory_percent': proc.memory_percent(),
                'cpu_percent': proc.cpu_percent(),
                'num_threads': proc.num_threads(),
                'status': proc.status(),
                'create_time': proc.create_time()
            }
        except Exception as e:
            system_state['process_info_error'] = str(e)
        
        # Save comprehensive error report
        try:
            error_file = LOG_DIR / f"critical_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(error_file, 'w', encoding='utf-8') as f:
                json.dump(system_state, f, indent=2, default=str, ensure_ascii=False)
            logger.info(f"Comprehensive error report saved to: {error_file}")
        except Exception as save_error:
            logger.error(f"Failed to save error report: {save_error}")
        
        # Display user-friendly error in interactive mode
        if hasattr(sys, 'ps1') or sys.stdin.isatty():
            console.print(f"[red]CRITICAL ERROR: {exc_value}[/red]")
            console.print(f"[dim]Comprehensive error report saved to logs directory[/dim]")
            console.print(f"[dim]Exception type: {exc_type.__name__}[/dim]")
            console.print(f"[dim]Module: {getattr(exc_type, '__module__', 'unknown')}[/dim]")
            
            # Show memory state if available
            if 'hardware_state' in system_state:
                ram_info = system_state['hardware_state'].get('system_ram', {})
                if 'current_usage' in ram_info:
                    usage = ram_info['current_usage']
                    console.print(f"[dim]System RAM: {usage.get('used_gb', 0):.1f}GB used ({usage.get('percent_used', 0):.1f}%)[/dim]")
        
        # Attempt emergency cleanup
        try:
            enhanced_clear_memory(aggressive=True)
            logger.info("Emergency memory cleanup completed")
        except Exception as cleanup_error:
            logger.error(f"Emergency cleanup failed: {cleanup_error}")
        
    except Exception as handler_error:
        # Fallback if our enhanced handler fails
        logger.error(f"Enhanced exception handler failed: {handler_error}")
        sys.__excepthook__(exc_type, exc_value, exc_traceback)

def performance_monitor_wrapper(func, include_memory: bool, log_level: int, hardware_data: Optional[Dict[str, Any]] = None):
    """
    Enhanced performance monitoring wrapper with hardware awareness.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        # Get hardware context if not provided
        if hardware_data is None:
            try:
                hw_data = check_hardware(include_memory_usage=include_memory)
            except Exception:
                hw_data = {}
        else:
            hw_data = hardware_data
        
        # Extract hardware capabilities
        cuda_info = hw_data.get('cuda', {})
        ram_info = hw_data.get('system_ram', {})
        cpu_info = hw_data.get('cpu_cores', {})
        
        cuda_available = cuda_info.get('available', False)
        gpu_count = cuda_info.get('gpu_count', 0)
        total_ram_gb = ram_info.get('ram_total_gb', 4.0)
        logical_cores = cpu_info.get('logical_cores', 1)
        
        # Pre-execution metrics with hardware awareness
        start_time = time.time()
        start_metrics = {
            'timestamp': datetime.now().isoformat(),
            'function': func.__name__,
            'hardware_context': {
                'cuda_available': cuda_available,
                'gpu_count': gpu_count,
                'total_ram_gb': total_ram_gb,
                'logical_cores': logical_cores
            }
        }
        
        # Memory monitoring based on system capabilities
        if include_memory:
            try:
                proc = psutil.Process()
                start_metrics['system_memory'] = {
                    'process_rss_mb': proc.memory_info().rss / 1024 / 1024,
                    'process_vms_mb': proc.memory_info().vms / 1024 / 1024,
                    'process_percent': proc.memory_percent()
                }
                
                # Add system RAM if available
                if 'current_usage' in ram_info:
                    start_metrics['system_memory']['system_used_gb'] = ram_info['current_usage']['used_gb']
                    start_metrics['system_memory']['system_percent'] = ram_info['current_usage']['percent_used']
            except Exception as e:
                start_metrics['memory_error'] = str(e)
        
        # GPU monitoring if available
        if cuda_available:
            try:
                start_metrics['gpu_memory'] = {}
                for i in range(gpu_count):
                    start_metrics['gpu_memory'][f'gpu_{i}'] = {
                        'allocated_mb': torch.cuda.memory_allocated(i) / 1024 / 1024,
                        'reserved_mb': torch.cuda.memory_reserved(i) / 1024 / 1024,
                        'max_allocated_mb': torch.cuda.max_memory_allocated(i) / 1024 / 1024
                    }
            except Exception as e:
                start_metrics['gpu_memory_error'] = str(e)
        
        # Execute function with monitoring
        try:
            result = func(*args, **kwargs)
            
            # Post-execution metrics
            end_time = time.time()
            duration = end_time - start_time
            
            end_metrics = {
                'duration': duration,
                'status': 'completed',
                'timestamp': datetime.now().isoformat()
            }
            
            # Memory analysis
            if include_memory:
                try:
                    proc = psutil.Process()
                    end_rss = proc.memory_info().rss / 1024 / 1024
                    end_vms = proc.memory_info().vms / 1024 / 1024
                    
                    end_metrics['memory_usage'] = {
                        'process_rss_mb': end_rss,
                        'process_vms_mb': end_vms,
                        'process_percent': proc.memory_percent(),
                        'rss_delta_mb': end_rss - start_metrics.get('system_memory', {}).get('process_rss_mb', 0),
                        'vms_delta_mb': end_vms - start_metrics.get('system_memory', {}).get('process_vms_mb', 0)
                    }
                    
                    # System memory change if available
                    try:
                        current_hw = check_hardware(include_memory_usage=True)
                        current_ram = current_hw.get('system_ram', {}).get('current_usage', {})
                        if current_ram:
                            end_metrics['memory_usage']['system_used_gb'] = current_ram['used_gb']
                            end_metrics['memory_usage']['system_delta_gb'] = (
                                current_ram['used_gb'] - 
                                start_metrics.get('system_memory', {}).get('system_used_gb', 0)
                            )
                    except Exception:
                        pass
                        
                except Exception as e:
                    end_metrics['memory_error'] = str(e)
            
            # GPU memory analysis
            if cuda_available:
                try:
                    end_metrics['gpu_memory'] = {}
                    total_gpu_delta = 0
                    
                    for i in range(gpu_count):
                        current_allocated = torch.cuda.memory_allocated(i) / 1024 / 1024
                        current_reserved = torch.cuda.memory_reserved(i) / 1024 / 1024
                        
                        start_gpu = start_metrics.get('gpu_memory', {}).get(f'gpu_{i}', {})
                        allocated_delta = current_allocated - start_gpu.get('allocated_mb', 0)
                        reserved_delta = current_reserved - start_gpu.get('reserved_mb', 0)
                        
                        end_metrics['gpu_memory'][f'gpu_{i}'] = {
                            'allocated_mb': current_allocated,
                            'reserved_mb': current_reserved,
                            'allocated_delta_mb': allocated_delta,
                            'reserved_delta_mb': reserved_delta
                        }
                        
                        total_gpu_delta += allocated_delta
                    
                    end_metrics['gpu_memory']['total_delta_mb'] = total_gpu_delta
                    
                except Exception as e:
                    end_metrics['gpu_memory_error'] = str(e)
            
            # Performance analysis and logging
            log_message = f"{func.__name__} completed in {duration:.3f}s"
            
            # Add memory information to log
            if include_memory and 'memory_usage' in end_metrics:
                mem_delta = end_metrics['memory_usage'].get('rss_delta_mb', 0)
                log_message += f", memory: {mem_delta:+.1f}MB"
                
                # Add system memory if significant change
                sys_delta = end_metrics['memory_usage'].get('system_delta_gb', 0)
                # > 100MB change
                if abs(sys_delta) > 0.1:
                    log_message += f" (system: {sys_delta:+.1f}GB)"
            
            # Add GPU information to log
            if cuda_available and 'gpu_memory' in end_metrics:
                total_gpu_delta = end_metrics['gpu_memory'].get('total_delta_mb', 0)
                # > 1MB change
                if abs(total_gpu_delta) > 1:
                    log_message += f", GPU: {total_gpu_delta:+.1f}MB"
            
            # Performance warnings based on hardware
            warnings = []
            if duration > 10:  # Long execution
                warnings.append("long_execution")
            if include_memory and 'memory_usage' in end_metrics:
                mem_delta = end_metrics['memory_usage'].get('rss_delta_mb', 0)
                # > 10% of total RAM
                if mem_delta > (total_ram_gb * 100):
                    warnings.append("high_memory_usage")
            
            if warnings:
                log_message += f" [WARNING: {', '.join(warnings)}]"
                logger.warning(log_message)
            else:
                logger.log(log_level, log_message)
            
            # Store comprehensive metrics for analysis
            combined_metrics = {
                **start_metrics,
                **end_metrics,
                'warnings': warnings
            }
            
            if not hasattr(wrapper, '_performance_metrics'):
                wrapper._performance_metrics = []
            wrapper._performance_metrics.append(combined_metrics)
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            error_message = f"{func.__name__} failed after {duration:.3f}s: {str(e)}"
            
            # Add memory info to error log if available
            if include_memory:
                try:
                    proc = psutil.Process()
                    current_rss = proc.memory_info().rss / 1024 / 1024
                    start_rss = start_metrics.get('system_memory', {}).get('process_rss_mb', 0)
                    mem_at_error = current_rss - start_rss
                    error_message += f", memory at error: {mem_at_error:+.1f}MB"
                except Exception:
                    pass
            
            logger.error(error_message)
            
            # Store error metrics
            error_metrics = {
                **start_metrics,
                'duration': duration,
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            
            if not hasattr(wrapper, '_performance_metrics'):
                wrapper._performance_metrics = []
            wrapper._performance_metrics.append(error_metrics)
            
            raise
    
    return wrapper

def enhanced_monitor_performance(
    include_memory: bool = True,
    log_level: int = logging.DEBUG,
    hardware_data: Optional[Dict[str, Any]] = None
):
    """
    Enhanced performance monitoring decorator with hardware awareness.
    
    Args:
        include_memory: Whether to monitor memory usage
        log_level: Logging level for performance messages
        hardware_data: Pre-fetched hardware data (optional)
        
    Returns:
        Decorator function with hardware-aware monitoring
    """
    def decorator(func):
        return performance_monitor_wrapper(func, include_memory, log_level, hardware_data)
    return decorator

def establish_performance_baseline(hardware_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Run performance tests to establish system baselines with hardware awareness.
    """
    try:
        # Get comprehensive hardware data
        if hardware_data is None:
            hardware_data = check_hardware(include_memory_usage=True)
        
        # Extract hardware capabilities
        cuda_info = hardware_data.get('cuda', {})
        ram_info = hardware_data.get('system_ram', {})
        cpu_info = hardware_data.get('cpu_cores', {})
        
        cuda_available = cuda_info.get('available', False)
        gpu_count = cuda_info.get('gpu_count', 0)
        total_ram_gb = ram_info.get('ram_total_gb', 4.0)
        logical_cores = cpu_info.get('logical_cores', 1)
        
        performance_metrics = {
            'timestamp': datetime.now().isoformat(),
            'hardware_context': {
                'cuda_available': cuda_available,
                'gpu_count': gpu_count,
                'total_ram_gb': total_ram_gb,
                'logical_cores': logical_cores
            },
            'baselines': {},
            'system_state': {}
        }
        
        # Record initial system state
        try:
            proc = psutil.Process()
            performance_metrics['system_state']['initial'] = {
                'memory_rss_mb': proc.memory_info().rss / 1024 / 1024,
                'memory_percent': proc.memory_percent(),
                'cpu_percent': proc.cpu_percent()
            }
        except Exception as e:
            performance_metrics['system_state']['initial_error'] = str(e)
        
        # CPU performance test - scale based on available RAM
        logger.debug("Running CPU performance baseline test")
        try:
            # Adaptive matrix size based on available RAM
            if total_ram_gb > 16:
                # High-end systems
                matrix_size = 2000
            elif total_ram_gb > 8:
                # Mid-range systems
                matrix_size = 1500
            else:
                # Low-end systems
                matrix_size = 1000
            
            start_time = time.time()
            test_array = np.random.rand(matrix_size, matrix_size).astype(np.float32)
            result = np.dot(test_array, test_array.T)
            cpu_time = time.time() - start_time
            
            performance_metrics['baselines']['cpu'] = {
                'matrix_size': matrix_size,
                'computation_time': cpu_time,
                'operations_per_second': (matrix_size ** 3) / cpu_time,
                # Input + output
                'memory_used_mb': test_array.nbytes * 2 / 1024 / 1024,
                'gflops': (2 * matrix_size ** 3) / (cpu_time * 1e9)
            }
            
            del test_array, result
            
        except Exception as e:
            performance_metrics['baselines']['cpu_error'] = str(e)
            logger.warning(f"CPU baseline test failed: {e}")
        
        # Memory allocation test - adaptive based on system RAM
        logger.debug("Running memory allocation baseline test")
        try:
            # Test different allocation sizes in MB
            allocation_sizes = []
            if total_ram_gb > 16:
                allocation_sizes = [100, 500, 1000]
            elif total_ram_gb > 8:
                allocation_sizes = [50, 200, 500]
            else:
                allocation_sizes = [20, 100, 200]
            
            memory_baselines = {}
            
            for size_mb in allocation_sizes:
                try:
                    start_memory = psutil.Process().memory_info().rss
                    start_time = time.time()
                    
                    # Create tensor of specified size
                    # 4 bytes per float32
                    elements = int((size_mb * 1024 * 1024) // 4)
                    test_tensor = torch.randn(elements, dtype=torch.float32)
                    
                    allocation_time = time.time() - start_time
                    end_memory = psutil.Process().memory_info().rss
                    actual_memory_mb = (end_memory - start_memory) / 1024 / 1024
                    
                    memory_baselines[f'{size_mb}mb'] = {
                        'target_size_mb': size_mb,
                        'actual_memory_mb': actual_memory_mb,
                        'allocation_time': allocation_time,
                        'allocation_speed_mbs': size_mb / allocation_time if allocation_time > 0 else 0,
                        'overhead_percent': ((actual_memory_mb - size_mb) / size_mb * 100) if size_mb > 0 else 0
                    }
                    
                    del test_tensor
                    
                except Exception as e:
                    memory_baselines[f'{size_mb}mb_error'] = str(e)
            
            performance_metrics['baselines']['memory'] = memory_baselines
            
        except Exception as e:
            performance_metrics['baselines']['memory_error'] = str(e)
            logger.warning(f"Memory baseline test failed: {e}")
        
        # GPU performance tests if available
        if cuda_available:
            logger.debug(f"Running GPU performance baseline test on {gpu_count} GPU(s)")
            try:
                gpu_baselines = {}
                
                for gpu_id in range(gpu_count):
                    try:
                        torch.cuda.set_device(gpu_id)
                        gpu_props = torch.cuda.get_device_properties(gpu_id)
                        gpu_memory_gb = gpu_props.total_memory / (1024**3)
                        
                        # Adaptive GPU test based on GPU memory
                        if gpu_memory_gb > 8:
                            matrix_size = 2000
                        elif gpu_memory_gb > 4:
                            matrix_size = 1500
                        else:
                            matrix_size = 1000
                        
                        # Warm up GPU
                        warm_tensor = torch.randn(100, 100, device=f'cuda:{gpu_id}')
                        torch.mm(warm_tensor, warm_tensor.t())
                        del warm_tensor
                        
                        torch.cuda.synchronize(gpu_id)
                        start_time = time.time()
                        start_memory = torch.cuda.memory_allocated(gpu_id)
                        
                        # GPU computation test
                        gpu_tensor = torch.randn(matrix_size, matrix_size, device=f'cuda:{gpu_id}', dtype=torch.float32)
                        result = torch.mm(gpu_tensor, gpu_tensor.t())
                        
                        torch.cuda.synchronize(gpu_id)
                        end_time = time.time()
                        end_memory = torch.cuda.memory_allocated(gpu_id)
                        
                        gpu_time = end_time - start_time
                        memory_used_mb = (end_memory - start_memory) / 1024 / 1024
                        
                        gpu_baselines[f'gpu_{gpu_id}'] = {
                            'name': gpu_props.name,
                            'compute_capability': f"{gpu_props.major}.{gpu_props.minor}",
                            'total_memory_gb': gpu_memory_gb,
                            'matrix_size': matrix_size,
                            'computation_time': gpu_time,
                            'memory_used_mb': memory_used_mb,
                            'gflops': (2 * matrix_size ** 3) / (gpu_time * 1e9),
                            'memory_bandwidth_gbs': (memory_used_mb / gpu_time / 1024) if gpu_time > 0 else 0,
                            'performance_ratio_vs_cpu': performance_metrics['baselines'].get('cpu', {}).get('computation_time', 1) / gpu_time if gpu_time > 0 else 0
                        }
                        
                        del gpu_tensor, result
                        
                    except Exception as e:
                        gpu_baselines[f'gpu_{gpu_id}_error'] = str(e)
                        logger.warning(f"GPU {gpu_id} baseline test failed: {e}")
                
                performance_metrics['baselines']['gpu'] = gpu_baselines
                
            except Exception as e:
                performance_metrics['baselines']['gpu_error'] = str(e)
                logger.warning(f"GPU baseline tests failed: {e}")
        
        # I/O performance test
        logger.debug("Running I/O performance baseline test")
        try:
            # 1MB of data
            test_data = b"0" * (1024 * 1024)
            test_file = LOG_DIR / "io_test.tmp"
            
            # Write test
            start_time = time.time()
            with open(test_file, 'wb') as f:
                # Write 10MB total
                for _ in range(10):
                    f.write(test_data)
            write_time = time.time() - start_time
            
            # Read test
            start_time = time.time()
            with open(test_file, 'rb') as f:
                while f.read(1024 * 1024):
                    pass
            read_time = time.time() - start_time
            
            # Cleanup
            test_file.unlink()
            
            performance_metrics['baselines']['io'] = {
                'write_time': write_time,
                'read_time': read_time,
                'write_speed_mbs': 10 / write_time if write_time > 0 else 0,
                'read_speed_mbs': 10 / read_time if read_time > 0 else 0,
                'data_size_mb': 10
            }
            
        except Exception as e:
            performance_metrics['baselines']['io_error'] = str(e)
            logger.warning(f"I/O baseline test failed: {e}")
        
        # Record final system state
        try:
            proc = psutil.Process()
            performance_metrics['system_state']['final'] = {
                'memory_rss_mb': proc.memory_info().rss / 1024 / 1024,
                'memory_percent': proc.memory_percent(),
                'cpu_percent': proc.cpu_percent()
            }
            
            # Calculate system impact
            initial_state = performance_metrics['system_state'].get('initial', {})
            final_state = performance_metrics['system_state']['final']
            
            performance_metrics['system_state']['impact'] = {
                'memory_delta_mb': final_state['memory_rss_mb'] - initial_state.get('memory_rss_mb', 0),
                'cpu_delta_percent': final_state['cpu_percent'] - initial_state.get('cpu_percent', 0)
            }
            
        except Exception as e:
            performance_metrics['system_state']['final_error'] = str(e)
        
        # Cleanup and final memory clear
        try:
            enhanced_clear_memory(aggressive=True)
        except Exception as e:
            performance_metrics['cleanup_error'] = str(e)
        
        # Generate performance summary
        summary = {
            'cpu_performance': 'good' if performance_metrics['baselines'].get('cpu', {}).get('gflops', 0) > 1 else 'limited',
            'memory_performance': 'good' if any(
                baseline.get('allocation_speed_mbs', 0) > 100 
                for baseline in performance_metrics['baselines'].get('memory', {}).values()
                if isinstance(baseline, dict)
            ) else 'limited',
            'gpu_available': cuda_available,
            'io_performance': 'good' if performance_metrics['baselines'].get('io', {}).get('write_speed_mbs', 0) > 50 else 'limited',
            'overall_capability': 'high-end' if (
                cuda_available and 
                total_ram_gb > 16 and 
                logical_cores > 8
            ) else 'standard'
        }
        
        performance_metrics['summary'] = summary
        
        logger.debug(f"Performance baseline established: {summary['overall_capability']} system capability")
        return performance_metrics
        
    except Exception as e:
        logger.error(f"Failed to establish performance baseline: {e}")
        return {
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
            'hardware_context': hardware_data.get('cuda', {}) if hardware_data else {}
        }

def enhanced_clear_memory(aggressive: bool = False, hardware_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Enhanced memory clearing with hardware awareness and detailed reporting.
    
    Args:
        aggressive: Whether to perform aggressive memory clearing
        hardware_data: Hardware data from check_hardware() (optional)
        
    Returns:
        Dictionary with clearing results and memory statistics
    """
    try:
        # Suppress individual log messages during system checks
        # logger = logging.getLogger(__name__)
        # original_level = logger.level
        # logger.setLevel(logging.WARNING)  # Only show warnings/errors
        # LOG ALL DETAILED INFORMATION TO FILE ONLY (no console output)
        # Store the original logger level to suppress console handlers temporarily
        handlers_to_suppress = []
        for handler in logger.handlers:
            if isinstance(handler, logging.StreamHandler) and handler.stream.name in ['<stdout>', '<stderr>']:
                handlers_to_suppress.append(handler)
                handler.setLevel(logging.CRITICAL)  # Temporarily suppress console output
        
        try:
            # Get hardware info if not provided
            if hardware_data is None:
                try:
                    hardware_data = check_hardware(include_memory_usage=True)
                except Exception:
                    hardware_data = {}
            
            cuda_info = hardware_data.get('cuda', {})
            ram_info = hardware_data.get('system_ram', {})
            
            cuda_available = cuda_info.get('available', False)
            gpu_count = cuda_info.get('gpu_count', 0)
            total_ram_gb = ram_info.get('ram_total_gb', 4.0)
            
            clearing_results = {
                'hardware_context': {
                    'cuda_available': cuda_available,
                    'gpu_count': gpu_count,
                    'total_ram_gb': total_ram_gb
                },
                'actions_taken': [],
                'memory_before': {},
                'memory_after': {},
                'success': True
            }
            
            # Capture memory state before clearing
            if 'current_usage' in ram_info:
                clearing_results['memory_before']['system_ram'] = ram_info['current_usage']
            
            if cuda_available and 'gpus' in cuda_info:
                clearing_results['memory_before']['gpu_memory'] = {}
                for i, gpu in enumerate(cuda_info['gpus']):
                    if 'current_usage' in gpu:
                        clearing_results['memory_before']['gpu_memory'][f'gpu_{i}'] = gpu['current_usage']
            
            # Clear Python garbage collection
            gc.collect()
            clearing_results['actions_taken'].append("Python garbage collection")
            
            # Clear PyTorch cache with hardware awareness
            if cuda_available:
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                clearing_results['actions_taken'].append("CUDA cache cleared and synchronized")
                
                if aggressive:
                    # Reset peak memory stats for each GPU
                    for i in range(gpu_count):
                        torch.cuda.reset_peak_memory_stats(i)
                        torch.cuda.reset_accumulated_memory_stats(i)
                    clearing_results['actions_taken'].append(f"Reset memory stats for {gpu_count} GPU(s)")
            
            # Aggressive mode with hardware considerations
            if aggressive:
                # More aggressive GC for systems with limited RAM
                gc_rounds = 5 if total_ram_gb < 8 else 3
                for _ in range(gc_rounds):
                    gc.collect()
                clearing_results['actions_taken'].append(f"Aggressive GC ({gc_rounds} rounds)")
                
                # Additional PyTorch optimizations for low-memory systems
                if total_ram_gb < 8:
                    # Reduce memory overhead
                    torch.backends.cudnn.benchmark = False
                    clearing_results['actions_taken'].append("Disabled cuDNN benchmark for memory conservation")
            
            # Capture memory state after clearing (if hardware data included usage)
            try:
                post_clear_hardware = check_hardware(include_memory_usage=True)
                post_ram_info = post_clear_hardware.get('system_ram', {})
                post_cuda_info = post_clear_hardware.get('cuda', {})
                
                if 'current_usage' in post_ram_info:
                    clearing_results['memory_after']['system_ram'] = post_ram_info['current_usage']
                
                if cuda_available and 'gpus' in post_cuda_info:
                    clearing_results['memory_after']['gpu_memory'] = {}
                    for i, gpu in enumerate(post_cuda_info['gpus']):
                        if 'current_usage' in gpu:
                            clearing_results['memory_after']['gpu_memory'][f'gpu_{i}'] = gpu['current_usage']
            except Exception as e:
                clearing_results['memory_after_error'] = str(e)
            
            logger.debug(f"Memory cleared successfully: {', '.join(clearing_results['actions_taken'])}")
            return clearing_results
        
        finally:
            # Restore original logging level
            #logger.setLevel(original_level)
            # Restore original logger levels
            for handler in handlers_to_suppress:
                #handler.setLevel(logging.NOTSET)
                handler.setLevel(logging.ERROR)
            pass
        
    except Exception as e:
        logger.warning(f"Memory clearing failed: {e}")
        return {
            'success': False,
            'error': str(e),
            'actions_taken': [],
            'hardware_context': hardware_data.get('cuda', {}) if hardware_data else {}
        }

# Utility function for external use
def get_version_info(package_name: str) -> Dict[str, str]:
    """
    Get comprehensive version information for a package.
    
    Args:
        package_name: Name of the package to check
        
    Returns:
        Dictionary with version information and availability status
    """
    version_str = safe_version(package_name)
    
    return {
        'package': package_name,
        'version': version_str,
        'available': version_str not in ['N/A', 'unknown'],
        'importlib_metadata_available': IMPORTLIB_METADATA_AVAILABLE,
        'packaging_available': PACKAGING_AVAILABLE
    }

# List custom presets from config directory
def list_custom_presets() -> List[str]:
    """List all available custom presets."""
    custom_dir = CONFIG_DIR / "custom_presets"
    if not custom_dir.exists():
        return []
    
    return [f.stem.replace("preset_", "") for f in custom_dir.glob("preset_*.json")]

# Model Architecture Options
MODEL_VARIANTS = {
    'SimpleAutoencoder': SimpleAutoencoder,
    'EnhancedAutoencoder': EnhancedAutoencoder,
    'AutoencoderEnsemble': AutoencoderEnsemble
}

# Consolidated Preset Configurations
# Initialize empty PRESET_CONFIGS to avoid forward reference errors
PRESET_CONFIGS = {}

def get_available_presets():
    """Dynamically get available preset names."""
    return list(PRESET_CONFIGS.keys()) if PRESET_CONFIGS else ['default', 'stability', 'performance', 'baseline', 'debug', 'lightweight', 'advanced']

def get_preset_descriptions():
    """Dynamically get preset descriptions."""
    return {k: v.get("metadata", {}).get("description", f"{k.title()} preset") 
            for k, v in PRESET_CONFIGS.items() 
            if isinstance(v, dict)} if PRESET_CONFIGS else {
        'default': 'Default balanced configuration for general use',
        'stability': 'High stability configuration for reliable training',
        'performance': 'High-performance configuration for production deployment',
        'baseline': 'Standardized configuration for benchmarking',
        'debug': 'Lightweight configuration for debugging',
        'lightweight': 'Lightweight configuration for edge devices',
        'advanced': 'Advanced configuration for research experiments'
    }

# Preset Configurations for Testing Different Architectures
STABILITY_PRESET = {
    'metadata': {
        'description': 'High stability configuration for reliable training and robust performance',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'stability', 'recommended_hardware': {'gpu_memory_gb': 4, 'cpu_cores': 2, 'ram_gb': 4},
        'compatibility': ['SimpleAutoencoder', 'EnhancedAutoencoder'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets', 'hyperparameter_optimization']
        }
    },
    'training': {
        'batch_size': 32, 'epochs': 100, 'learning_rate': 0.0005, 'patience': 15, 'weight_decay': 1e-3,
        'gradient_clip': 0.5, 'gradient_accumulation_steps': 1, 'mixed_precision': False,
        'num_workers': 2, 'optimizer': 'Adam', 'scheduler': None, 'scheduler_params': {},
        'early_stopping': True, 'validation_split': 0.25, 'shuffle': True,
        'pin_memory': False, 'persistent_workers': False,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 5, 'lr_factor': 0.7, 'min_lr': 1e-6
    },
    'model': {
        'model_type': 'SimpleAutoencoder', 'input_dim': 15, 'encoding_dim': 8, 'hidden_dims': [64],
        'dropout_rates': [0.3], 'activation': 'relu', 'activation_param': 0.0,
        'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False,
        'diversity_factor': 0.0, 'min_features': 5, 'num_models': 1, 'skip_connection': False,
        'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
        'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu', 'tanh'],
        'available_normalizations': ['batch', 'layer', 'instance', None],
        'available_initializers': ['xavier_uniform', 'xavier_normal', 'kaiming_uniform'],
        'legacy_mode': False, 'use_attention': False
    },
    'security': {
        'percentile': 99, 'attack_threshold': 0.2, 'false_negative_cost': 3.0,
        'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
        'early_warning_threshold': 0.15, 'adaptive_threshold': False, 'confidence_interval': 0.99,
        'detection_methods': ['reconstruction_error', 'statistical_analysis'],
        'alert_levels': ['low', 'medium', 'high', 'critical'], 'threshold_validation': True,
        'robust_detection': True, 'false_positive_tolerance': 0.01
    },
    'data': {
        'normal_samples': 5000, 'attack_samples': 1000, 'features': 15, 'use_real_data': False,
        'data_normalization': 'minmax', 'anomaly_factor': 2.0, 'random_state': 42,
        'validation_split': 0.25, 'test_split': 0.25, 'stratified_split': True,
        'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.05, 'anomaly_sparsity': 0.2, 'noise_factor': 0.02,
                               'correlation_strength': 0.2, 'outlier_ratio': 0.01},
        'preprocessing': {'remove_outliers': True, 'outlier_threshold': 2.5, 'impute_missing': True,
                         'imputation_strategy': 'median', 'feature_scaling': 'robust', 'data_cleaning': True},
        'shuffle': True, 'pin_memory': False
    },
    'monitoring': {
        'metrics_frequency': 5, 'checkpoint_frequency': 10, 'tensorboard_logging': False,
        'console_logging_level': 'DEBUG', 'save_best_model': True, 'save_model_history': True,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss', 'learning_rate', 'epoch_time',
                           'gradient_norm', 'training_stability'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
        'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 5, 'save_checkpoints': True,
        'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False,
                       'max_scalars': 500, 'max_histograms': 50, 'max_images': 10, 'save_summary': False},
        'stability_metrics': True
    },
    'hardware': {
        'device': 'cpu', 'recommended_gpu_memory': 4,
        'minimum_system_requirements': {'cpu_cores': 2, 'ram_gb': 4, 'disk_space': 5},
        'optimal_system_requirements': {'cpu_cores': 4, 'ram_gb': 8, 'disk_space': 10, 'gpu_memory': 4},
        'memory_management': {'max_memory_fraction': 0.7, 'allow_memory_growth': False, 'memory_limit': 2048},
        'performance_optimization': {'use_cuda': False, 'use_amp': False, 'benchmark_mode': False, 'deterministic': True}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "stability"), 'log_dir': str(LOG_DIR / "stability"),
        'config_dir': str(CONFIG_DIR / "stability"), 'data_dir': str(DATA_DIR / "stability"),
        'checkpoint_dir': str(Path(CHECKPOINTS_DIR / "stability")), 'tensorboard_dir': str(TB_DIR / "stability"), 'results_dir': str(RESULTS_DIR / "stability"),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True,
        'parallel_processing': False, 'max_workers': 2, 'export_onnx': True, 'non_interactive': False,
        'cuda_optimizations': False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True,
                       'optimize_for_mobile': False, 'runtime_validation': True,
                       'validation_tolerance': 1e-5, 'verbose': False}
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'stability', 'current_override': None,
        'override_rules': {'security': False, 'monitoring': False, 'hardware': False},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(),
        'auto_apply': False, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': False, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_stability',
        'direction': 'minimize', 'n_trials': 50, 'timeout': 1800,
        'sampler': 'RandomSampler', 'pruner': 'NopPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-4, 'high': 1e-2, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [16, 32, 64]},
            'dropout_rate': {'type': 'float', 'low': 0.1, 'high': 0.4}
        },
        'early_stopping': {'enabled': True, 'patience': 8, 'min_improvement': 1e-4},
        'timeout_seconds': 1800, 'trial_epochs': 50, 'trial_patience': 6,
        'cleanup_trials': True, 'generate_plots': False,
        'search_space': {
            'encoding_dim_min': 6, 'encoding_dim_max': 12, 'hidden_layers_min': 1, 'hidden_layers_max': 2,
            'lr_min': 1e-4, 'lr_max': 1e-2, 'batch_sizes': [16, 32, 64],
            'weight_decay_min': 1e-4, 'weight_decay_max': 1e-2, 'dropout_min': 0.1, 'dropout_max': 0.4,
            'activations': ["relu", "leaky_relu"], 'normalizations': [None, "batch"],
            'percentile_min': 95, 'percentile_max': 99
        },
        'hpo_sampler': {'type': 'Random', 'seed': 42, 'consider_prior': False, 'prior_weight': 1.0,
                   'consider_magic_clip': False, 'consider_endpoints': False, 'n_startup_trials': 10,
                   'n_ei_candidates': 15, 'multivariate': False},
        'hpo_pruner': {'type': 'Nop', 'n_startup_trials': 0, 'n_warmup_steps': 0, 'interval_steps': 1},
        'scoring': {'use_composite_score': False, 'validation_weight': 0.8, 'test_weight': 0.2,
                   'complexity_weight': 0.1, 'max_params_penalty': 10000},
        'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/stability_study.db",
                   'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': True, 'folds': 4, 'stratified': True, 'random_state': 42},
        'metrics': ['mse', 'mae', 'r2_score', 'explained_variance', 'precision', 'recall', 'f1_score',
                   'auc_roc', 'stability_score'],
        'validation_frequency': 1, 'save_validation_results': True, 'detailed_metrics': True,
        'robustness_testing': True
    },
    'experimental': {
        'features': {'advanced_logging': False, 'model_interpretability': False, 'federated_learning': False,
                    'active_learning': False, 'robust_training': True},
        'settings': {'experimental_mode': False, 'beta_features': False, 'research_mode': False}
    }
}

PERFORMANCE_PRESET = {
    'metadata': {
        'description': 'High-performance configuration for production deployment and optimal throughput',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'performance', 'recommended_hardware': {'gpu_memory_gb': 8, 'cpu_cores': 8, 'ram_gb': 16},
        'compatibility': ['EnhancedAutoencoder', 'AutoencoderEnsemble'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets', 'hyperparameter_optimization']
        }
    },
    'training': {
        'batch_size': 128, 'epochs': 200, 'learning_rate': 0.0001, 'patience': 20, 'weight_decay': 1e-5,
        'gradient_clip': 0.1, 'gradient_accumulation_steps': 8, 'mixed_precision': True if torch.cuda.is_available() else False,
        'num_workers': max(4, os.cpu_count() or 4), 'optimizer': 'AdamW', 'scheduler': 'CosineAnnealingLR',
        'scheduler_params': {'T_max': 200, 'eta_min': 1e-7, 'last_epoch': -1},
        'early_stopping': True, 'validation_split': 0.15, 'shuffle': True,
        'pin_memory': True if torch.cuda.is_available() else False, 'persistent_workers': True,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 8, 'lr_factor': 0.5, 'min_lr': 1e-7
    },
    'model': {
        'model_type': 'AutoencoderEnsemble', 'input_dim': 30, 'encoding_dim': 16, 'hidden_dims': [256, 128, 64],
        'dropout_rates': [0.1, 0.1, 0.05], 'activation': 'gelu', 'activation_param': 0.0,
        'normalization': 'batch', 'use_batch_norm': True, 'use_layer_norm': False,
        'diversity_factor': 0.2, 'min_features': 10, 'num_models': 5, 'skip_connection': True,
        'residual_blocks': True, 'bias': True, 'weight_init': 'kaiming_normal',
        'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu', 'swish'],
        'available_normalizations': ['batch', 'layer', 'instance', 'group'],
        'available_initializers': ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal'],
        'legacy_mode': False, 'use_attention': True
    },
    'security': {
        'percentile': 90, 'attack_threshold': 0.4, 'false_negative_cost': 1.5,
        'enable_security_metrics': True, 'anomaly_threshold_strategy': 'dynamic_percentile',
        'early_warning_threshold': 0.35, 'adaptive_threshold': True, 'confidence_interval': 0.95,
        'detection_methods': ['reconstruction_error', 'statistical_analysis', 'ensemble_voting'],
        'alert_levels': ['low', 'medium', 'high', 'critical'], 'threshold_validation': True,
        'performance_optimized_detection': True, 'real_time_monitoring': True
    },
    'data': {
        'normal_samples': 10000, 'attack_samples': 3000, 'features': 30, 'use_real_data': True,
        'data_normalization': 'standard', 'anomaly_factor': 1.2, 'random_state': 42,
        'validation_split': 0.15, 'test_split': 0.15, 'stratified_split': True,
        'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.15, 'anomaly_sparsity': 0.4, 'noise_factor': 0.03,
                               'correlation_strength': 0.35, 'high_performance_mode': True},
        'preprocessing': {'remove_outliers': True, 'outlier_threshold': 2.8, 'impute_missing': True,
                         'imputation_strategy': 'mean', 'feature_scaling': 'standard', 'streaming_processing': True},
        'shuffle': True, 'pin_memory': True if torch.cuda.is_available() else False
    },
    'monitoring': {
        'metrics_frequency': 20, 'checkpoint_frequency': 25, 'tensorboard_logging': True,
        'console_logging_level': 'INFO', 'save_best_model': True, 'save_model_history': True,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss', 'learning_rate', 'epoch_time',
                           'memory_usage', 'throughput', 'latency', 'gpu_utilization'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
        'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 10, 'save_checkpoints': True,
        'tensorboard': {'export_formats': ["json", "csv"], 'include_histograms': True, 'include_images': True,
                       'max_scalars': 2000, 'max_histograms': 100, 'max_images': 15, 'save_summary': True},
        'performance_metrics': True
    },
    'hardware': {
        'device': 'cuda' if torch.cuda.is_available() else 'cpu', 'recommended_gpu_memory': 8,
        'minimum_system_requirements': {'cpu_cores': 4, 'ram_gb': 8, 'disk_space': 10},
        'optimal_system_requirements': {'cpu_cores': 8, 'ram_gb': 16, 'disk_space': 20, 'gpu_memory': 8},
        'memory_management': {'max_memory_fraction': 0.85, 'allow_memory_growth': True, 'memory_limit': None},
        'performance_optimization': {'use_cuda': True if torch.cuda.is_available() else False, 'use_amp': True if torch.cuda.is_available() else False, 'benchmark_mode': True, 
                                   'deterministic': False, 'cudnn_benchmark': True if torch.cuda.is_available() else False}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "performance"), 'log_dir': str(LOG_DIR / "performance"),
        'config_dir': str(CONFIG_DIR / "performance"), 'data_dir': str(DATA_DIR / "performance"),
        'checkpoint_dir': str(CHECKPOINTS_DIR / "performance"), 'tensorboard_dir': str(TB_DIR / "performance"), 'results_dir': str(RESULTS_DIR / "performance"),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True,
        'parallel_processing': True, 'max_workers': max(4, os.cpu_count() or 4),
        'export_onnx': True, 'non_interactive': False, 'cuda_optimizations': True if torch.cuda.is_available() else False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True,
                       'optimize_for_mobile': False, 'runtime_validation': True,
                       'validation_tolerance': 1e-5, 'verbose': True},
        'distributed_training': False
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'performance', 'current_override': None,
        'override_rules': {'security': True, 'monitoring': True, 'hardware': True},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(),
        'auto_apply': True, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': True, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_performance',
        'direction': 'minimize', 'n_trials': 200, 'timeout': 7200,
        'sampler': 'TPESampler', 'pruner': 'HyperbandPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 1e-2, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [64, 128, 256]},
            'encoding_dim': {'type': 'int', 'low': 12, 'high': 32},
            'hidden_dims': {'type': 'suggest', 'options': [[128, 64], [256, 128, 64], [512, 256, 128]]},
            'dropout_rate': {'type': 'float', 'low': 0.0, 'high': 0.2},
            'diversity_factor': {'type': 'float', 'low': 0.1, 'high': 0.3}
        },
        'early_stopping': {'enabled': True, 'patience': 12, 'min_improvement': 1e-5},
        'timeout_seconds': 7200, 'trial_epochs': 80, 'trial_patience': 8,
        'cleanup_trials': True, 'generate_plots': True,
        'search_space': {
            'encoding_dim_min': 12, 'encoding_dim_max': 32, 'hidden_layers_min': 2, 'hidden_layers_max': 3,
            'lr_min': 1e-5, 'lr_max': 1e-2, 'batch_sizes': [64, 128, 256],
            'weight_decay_min': 1e-6, 'weight_decay_max': 1e-4, 'dropout_min': 0.0, 'dropout_max': 0.2,
            'activations': ["relu", "leaky_relu", "gelu", "swish"], 'normalizations': ["batch", "layer"],
            'percentile_min': 85, 'percentile_max': 95
        },
        'hpo_sampler': {'type': 'TPE', 'seed': 42, 'consider_prior': True, 'prior_weight': 1.0,
                   'consider_magic_clip': True, 'consider_endpoints': False, 'n_startup_trials': 20,
                   'n_ei_candidates': 24, 'multivariate': True},
        'hpo_pruner': {'type': 'Hyperband', 'n_startup_trials': 10, 'n_warmup_steps': 5, 'interval_steps': 1,
                   'min_resource': 1, 'max_resource': 'auto', 'reduction_factor': 3},
        'scoring': {'use_composite_score': True, 'validation_weight': 0.6, 'test_weight': 0.2,
                   'complexity_weight': 0.1, 'performance_weight': 0.1, 'max_params_penalty': 50000},
        'storage': {'enabled': True, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/performance_study.db",
                   'load_if_exists': True, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': True, 'folds': 3, 'stratified': True, 'random_state': 42},
        'metrics': ['mse', 'mae', 'r2_score', 'explained_variance', 'precision', 'recall', 'f1_score',
                   'auc_roc', 'inference_time', 'throughput'],
        'validation_frequency': 1, 'save_validation_results': True, 'detailed_metrics': True,
        'performance_benchmarking': True
    },
    'experimental': {
        'features': {'advanced_logging': True, 'model_interpretability': False, 'federated_learning': False,
                    'active_learning': False, 'performance_tuning': True},
        'settings': {'experimental_mode': False, 'beta_features': True, 'research_mode': False}
    }
}

BASELINE_PRESET = {
    'metadata': {
        'description': 'Standardized configuration for benchmarking and performance comparison',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'baseline', 'recommended_hardware': {'gpu_memory_gb': 6, 'cpu_cores': 4, 'ram_gb': 8},
        'compatibility': ['EnhancedAutoencoder', 'SimpleAutoencoder'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets', 'hyperparameter_optimization']
        }
    },
    'training': {
        'batch_size': 64, 'epochs': 75, 'learning_rate': 0.001, 'patience': 10, 'weight_decay': 1e-4,
        'gradient_clip': 1.0, 'gradient_accumulation_steps': 4, 'mixed_precision': False,
        'num_workers': min(4, os.cpu_count() or 1), 'optimizer': 'Adam', 'scheduler': 'ReduceLROnPlateau',
        'scheduler_params': {'mode': 'min', 'factor': 0.5, 'patience': 5, 'min_lr': 1e-6},
        'early_stopping': True, 'validation_split': 0.2, 'shuffle': True,
        'pin_memory': True if torch.cuda.is_available() else False, 'persistent_workers': False,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 5, 'lr_factor': 0.5, 'min_lr': 1e-6
    },
    'model': {
        'model_type': 'EnhancedAutoencoder', 'input_dim': 20, 'encoding_dim': 12, 'hidden_dims': [128, 64],
        'dropout_rates': [0.25, 0.2], 'activation': 'leaky_relu', 'activation_param': 0.1,
        'normalization': 'batch', 'use_batch_norm': True, 'use_layer_norm': False,
        'diversity_factor': 0.05, 'min_features': 5, 'num_models': 1, 'skip_connection': True,
        'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
        'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu', 'tanh'],
        'available_normalizations': ['batch', 'layer', 'instance', None],
        'available_initializers': ['xavier_uniform', 'xavier_normal', 'kaiming_uniform'],
        'legacy_mode': False, 'use_attention': False
    },
    'security': {
        'percentile': 95, 'attack_threshold': 0.3, 'false_negative_cost': 2.0,
        'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
        'early_warning_threshold': 0.25, 'adaptive_threshold': False, 'confidence_interval': 0.95,
        'detection_methods': ['reconstruction_error', 'statistical_analysis'],
        'alert_levels': ['low', 'medium', 'high', 'critical'], 'threshold_validation': True
    },
    'data': {
        'normal_samples': 8000, 'attack_samples': 2000, 'features': 20, 'use_real_data': False,
        'data_normalization': 'standard', 'anomaly_factor': 1.5, 'random_state': 42,
        'validation_split': 0.2, 'test_split': 0.2, 'stratified_split': True,
        'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.1, 'anomaly_sparsity': 0.3, 'noise_factor': 0.05, 'correlation_strength': 0.3},
        'preprocessing': {'remove_outliers': True, 'outlier_threshold': 3.0, 'impute_missing': True, 'imputation_strategy': 'mean'},
        'shuffle': True, 'pin_memory': True if torch.cuda.is_available() else False
    },
    'monitoring': {
        'metrics_frequency': 10, 'checkpoint_frequency': 5, 'tensorboard_logging': True,
        'console_logging_level': 'INFO', 'save_best_model': True, 'save_model_history': True,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss', 'learning_rate', 'epoch_time', 'memory_usage'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
        'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 10, 'save_checkpoints': True,
        'tensorboard': {'export_formats': ["json", "csv"], 'include_histograms': True, 'include_images': False,
                       'max_scalars': 1000, 'max_histograms': 50, 'max_images': 10, 'save_summary': True}
    },
    'hardware': {
        'device': 'auto', 'recommended_gpu_memory': 6,
        'minimum_system_requirements': {'cpu_cores': 2, 'ram_gb': 4, 'disk_space': 5},
        'optimal_system_requirements': {'cpu_cores': 4, 'ram_gb': 8, 'disk_space': 10, 'gpu_memory': 6},
        'memory_management': {'max_memory_fraction': 0.8, 'allow_memory_growth': True, 'memory_limit': None},
        'performance_optimization': {'use_cuda': True if torch.cuda.is_available() else False, 'use_amp': False, 'benchmark_mode': False, 'deterministic': True}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "baseline"), 'log_dir': str(LOG_DIR / "baseline"),
        'config_dir': str(CONFIG_DIR / "baseline"), 'data_dir': str(DATA_DIR / "baseline"),
        'checkpoint_dir': str(CHECKPOINTS_DIR / "baseline"), 'tensorboard_dir': str(TB_DIR / "baseline"), 'results_dir': str(RESULTS_DIR / "baseline"),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True,
        'parallel_processing': True, 'max_workers': min(4, os.cpu_count() or 1),
        'export_onnx': True, 'non_interactive': False, 'cuda_optimizations': True if torch.cuda.is_available() else False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True,
                       'optimize_for_mobile': False, 'runtime_validation': True,
                       'validation_tolerance': 1e-5, 'verbose': False}
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'baseline', 'current_override': None,
        'override_rules': {'security': False, 'monitoring': False, 'hardware': False},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(),
        'auto_apply': False, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': False, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_baseline',
        'direction': 'minimize', 'n_trials': 100, 'timeout': 3600,
        'sampler': 'TPESampler', 'pruner': 'MedianPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 1e-1, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [32, 64, 128]},
            'encoding_dim': {'type': 'int', 'low': 8, 'high': 24},
            'hidden_dims': {'type': 'suggest', 'options': [[64], [128, 64], [256, 128]]},
            'dropout_rate': {'type': 'float', 'low': 0.1, 'high': 0.4}
        },
        'early_stopping': {'enabled': True, 'patience': 10, 'min_improvement': 1e-4},
        'timeout_seconds': 3600, 'trial_epochs': 50, 'trial_patience': 7,
        'cleanup_trials': True, 'generate_plots': True,
        'search_space': {
            'encoding_dim_min': 8, 'encoding_dim_max': 24, 'hidden_layers_min': 1, 'hidden_layers_max': 2,
            'lr_min': 1e-5, 'lr_max': 1e-1, 'batch_sizes': [32, 64, 128],
            'weight_decay_min': 1e-5, 'weight_decay_max': 1e-3, 'dropout_min': 0.1, 'dropout_max': 0.4,
            'activations': ["relu", "leaky_relu", "gelu"], 'normalizations': ["batch", None],
            'percentile_min': 90, 'percentile_max': 98
        },
        'hpo_sampler': {'type': 'TPE', 'seed': 42, 'consider_prior': True, 'prior_weight': 1.0,
                   'consider_magic_clip': True, 'consider_endpoints': False, 'n_startup_trials': 20,
                   'n_ei_candidates': 24, 'multivariate': True},
        'hpo_pruner': {'type': 'Median', 'n_startup_trials': 5, 'n_warmup_steps': 10, 'interval_steps': 1},
        'scoring': {'use_composite_score': False, 'validation_weight': 0.7, 'test_weight': 0.2,
                   'complexity_weight': 0.1, 'max_params_penalty': 50000},
        'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/baseline_study.db",
                   'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': False, 'folds': 3, 'stratified': True, 'random_state': 42},
        'metrics': ['mse', 'mae', 'r2_score', 'explained_variance', 'precision', 'recall', 'f1_score', 'auc_roc'],
        'validation_frequency': 1, 'save_validation_results': True, 'detailed_metrics': False
    },
    'experimental': {
        'features': {'advanced_logging': False, 'model_interpretability': False, 'federated_learning': False, 'active_learning': False},
        'settings': {'experimental_mode': False, 'beta_features': False, 'research_mode': False}
    }
}

DEBUG_PRESET = {
    'metadata': {
        'description': 'Lightweight configuration for debugging and rapid development',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'debug', 'recommended_hardware': {'gpu_memory_gb': 2, 'cpu_cores': 1, 'ram_gb': 2},
        'compatibility': ['SimpleAutoencoder'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets']
        }
    },
    'training': {
        'batch_size': 16, 'epochs': 5, 'learning_rate': 0.01, 'patience': 3, 'weight_decay': 0.0,
        'gradient_clip': 5.0, 'gradient_accumulation_steps': 1, 'mixed_precision': False,
        'num_workers': 1, 'optimizer': 'SGD', 'scheduler': None, 'scheduler_params': {},
        'early_stopping': False, 'validation_split': 0.3, 'shuffle': True,
        'pin_memory': False, 'persistent_workers': False,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 2, 'lr_factor': 0.5, 'min_lr': 1e-7
    },
    'model': {
        'model_type': 'SimpleAutoencoder', 'input_dim': 10, 'encoding_dim': 4, 'hidden_dims': [32],
        'dropout_rates': [0.1], 'activation': 'relu', 'activation_param': 0.0,
        'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False,
        'diversity_factor': 0.0, 'min_features': 3, 'num_models': 1, 'skip_connection': False,
        'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
        'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu'],
        'available_normalizations': ['batch', 'layer', None],
        'available_initializers': ['xavier_uniform', 'xavier_normal'],
        'legacy_mode': False, 'use_attention': False
    },
    'security': {
        'percentile': 85, 'attack_threshold': 0.5, 'false_negative_cost': 1.0,
        'enable_security_metrics': False, 'anomaly_threshold_strategy': 'fixed_percentile',
        'early_warning_threshold': 0.45, 'adaptive_threshold': False, 'confidence_interval': 0.8,
        'detection_methods': ['reconstruction_error'], 'alert_levels': ['low', 'medium'],
        'threshold_validation': False
    },
    'data': {
        'normal_samples': 100, 'attack_samples': 50, 'features': 10, 'use_real_data': False,
        'data_normalization': 'minmax', 'anomaly_factor': 2.0, 'random_state': 42,
        'validation_split': 0.3, 'test_split': 0.3, 'stratified_split': False,
        'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.2, 'anomaly_sparsity': 0.5, 'noise_factor': 0.15, 'correlation_strength': 0.1},
        'preprocessing': {'remove_outliers': False, 'outlier_threshold': 3.0, 'impute_missing': False, 'imputation_strategy': 'mean'},
        'shuffle': True, 'pin_memory': False
    },
    'monitoring': {
        'metrics_frequency': 1, 'checkpoint_frequency': 1, 'tensorboard_logging': False,
        'console_logging_level': 'DEBUG', 'save_best_model': False, 'save_model_history': False,
        'metrics_to_track': ['loss', 'reconstruction_error'], 'early_stopping_metric': 'loss',
        'checkpoint_format': 'pytorch', 'log_model_summary': True, 'tensorboard_dir': str(TB_DIR),
        'log_frequency': 1, 'save_checkpoints': False,
        'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False,
                       'max_scalars': 100, 'max_histograms': 10, 'max_images': 5, 'save_summary': False}
    },
    'hardware': {
        'device': 'cpu', 'recommended_gpu_memory': 2,
        'minimum_system_requirements': {'cpu_cores': 1, 'ram_gb': 2, 'disk_space': 2},
        'optimal_system_requirements': {'cpu_cores': 2, 'ram_gb': 4, 'disk_space': 5},
        'memory_management': {'max_memory_fraction': 0.5, 'allow_memory_growth': False, 'memory_limit': 512},
        'performance_optimization': {'use_cuda': False, 'use_amp': False, 'benchmark_mode': False, 'deterministic': True}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "debug"), 'log_dir': str(LOG_DIR / "debug"),
        'config_dir': str(CONFIG_DIR / "debug"), 'data_dir': str(DATA_DIR / "debug"),
        'checkpoint_dir': str(CHECKPOINTS_DIR / "debug"), 'tensorboard_dir': str(TB_DIR / "debug"), 'results_dir': str(RESULTS_DIR / "debug"),
        'debug': True, 'verbose': True, 'random_seed': 42, 'reproducible': False,
        'parallel_processing': False, 'max_workers': 1, 'export_onnx': False, 'non_interactive': False,
        'cuda_optimizations': False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': False, 'constant_folding': False,
                       'optimize_for_mobile': False, 'runtime_validation': False,
                       'validation_tolerance': 1e-5, 'verbose': False}
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'debug', 'current_override': None,
        'override_rules': {'security': False, 'monitoring': False, 'hardware': False},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(),
        'auto_apply': False, 'validate_compatibility': False
    },
    'hyperparameter_optimization': {
        'enabled': False, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_debug',
        'direction': 'minimize', 'n_trials': 10, 'timeout': 600,
        'sampler': 'RandomSampler', 'pruner': 'NopPruner', 'objective_metric': 'loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-3, 'high': 1e-1, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [8, 16, 32]},
            'encoding_dim': {'type': 'int', 'low': 2, 'high': 8}
        },
        'early_stopping': {'enabled': False, 'patience': 3, 'min_improvement': 1e-2},
        'timeout_seconds': 600, 'trial_epochs': 5, 'trial_patience': 2,
        'cleanup_trials': False, 'generate_plots': False,
        'search_space': {
            'encoding_dim_min': 2, 'encoding_dim_max': 8, 'hidden_layers_min': 1, 'hidden_layers_max': 1,
            'lr_min': 1e-3, 'lr_max': 1e-1, 'batch_sizes': [8, 16, 32],
            'weight_decay_min': 0.0, 'weight_decay_max': 0.0, 'dropout_min': 0.0, 'dropout_max': 0.1,
            'activations': ["relu"], 'normalizations': [None], 'percentile_min': 80, 'percentile_max': 90
        },
        'hpo_sampler': {'type': 'Random', 'seed': 42, 'consider_prior': False, 'prior_weight': 1.0,
                   'consider_magic_clip': False, 'consider_endpoints': False, 'n_startup_trials': 5,
                   'n_ei_candidates': 10, 'multivariate': False},
        'hpo_pruner': {'type': 'Nop', 'n_startup_trials': 0, 'n_warmup_steps': 0, 'interval_steps': 1},
        'scoring': {'use_composite_score': False, 'validation_weight': 1.0, 'test_weight': 0.0,
                   'complexity_weight': 0.0, 'max_params_penalty': 1000},
        'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/debug_study.db",
                   'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': False, 'folds': 2, 'stratified': False, 'random_state': 42},
        'metrics': ['mse', 'mae'], 'validation_frequency': 1,
        'save_validation_results': False, 'detailed_metrics': False
    },
    'experimental': {
        'features': {'advanced_logging': True, 'model_interpretability': False,
                    'federated_learning': False, 'active_learning': False},
        'settings': {'experimental_mode': True, 'beta_features': False, 'research_mode': False}
    }
}

LIGHTWEIGHT_PRESET = {
    'metadata': {
        'description': 'Lightweight configuration for edge devices and resource-constrained environments',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'lightweight', 'recommended_hardware': {'gpu_memory_gb': 1, 'cpu_cores': 1, 'ram_gb': 2},
        'compatibility': ['SimpleAutoencoder'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets']
        }
    },
    'training': {
        'batch_size': 8, 'epochs': 30, 'learning_rate': 0.005, 'patience': 5, 'weight_decay': 0.0,
        'gradient_clip': 2.0, 'gradient_accumulation_steps': 1, 'mixed_precision': False,
        'num_workers': 1, 'optimizer': 'Adam', 'scheduler': None, 'scheduler_params': {},
        'early_stopping': True, 'validation_split': 0.25, 'shuffle': True,
        'pin_memory': False, 'persistent_workers': False,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 3, 'lr_factor': 0.7, 'min_lr': 1e-6
    },
    'model': {
        'model_type': 'SimpleAutoencoder', 'input_dim': 12, 'encoding_dim': 6, 'hidden_dims': [48],
        'dropout_rates': [0.15], 'activation': 'relu', 'activation_param': 0.0,
        'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False,
        'diversity_factor': 0.0, 'min_features': 4, 'num_models': 1, 'skip_connection': False,
        'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
        'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu'],
        'available_normalizations': ['batch', 'layer', None],
        'available_initializers': ['xavier_uniform', 'xavier_normal'],
        'legacy_mode': False, 'use_attention': False
    },
    'security': {
        'percentile': 92, 'attack_threshold': 0.35, 'false_negative_cost': 1.2,
        'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
        'early_warning_threshold': 0.3, 'adaptive_threshold': False, 'confidence_interval': 0.9,
        'detection_methods': ['reconstruction_error'], 'alert_levels': ['low', 'medium', 'high'],
        'threshold_validation': False
    },
    'data': {
        'normal_samples': 2000, 'attack_samples': 500, 'features': 12, 'use_real_data': False,
        'data_normalization': 'minmax', 'anomaly_factor': 1.8, 'random_state': 42,
        'validation_split': 0.25, 'test_split': 0.25, 'stratified_split': False,
        'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.08, 'anomaly_sparsity': 0.25, 'noise_factor': 0.1, 'correlation_strength': 0.2},
        'preprocessing': {'remove_outliers': False, 'outlier_threshold': 3.0, 'impute_missing': True, 'imputation_strategy': 'mean'},
        'shuffle': True, 'pin_memory': False
    },
    'monitoring': {
        'metrics_frequency': 5, 'checkpoint_frequency': 5, 'tensorboard_logging': False,
        'console_logging_level': 'INFO', 'save_best_model': True, 'save_model_history': False,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
        'log_model_summary': False, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 5, 'save_checkpoints': True,
        'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False,
                       'max_scalars': 100, 'max_histograms': 10, 'max_images': 5, 'save_summary': False}
    },
    'hardware': {
        'device': 'cpu', 'recommended_gpu_memory': 1,
        'minimum_system_requirements': {'cpu_cores': 1, 'ram_gb': 2, 'disk_space': 2},
        'optimal_system_requirements': {'cpu_cores': 2, 'ram_gb': 4, 'disk_space': 5},
        'memory_management': {'max_memory_fraction': 0.6, 'allow_memory_growth': False, 'memory_limit': 1024},
        'performance_optimization': {'use_cuda': False, 'use_amp': False, 'benchmark_mode': False, 'deterministic': True}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "lightweight"), 'log_dir': str(LOG_DIR / "lightweight"),
        'config_dir': str(CONFIG_DIR / "lightweight"), 'data_dir': str(DATA_DIR / "lightweight"),
        'checkpoint_dir': str(CHECKPOINTS_DIR / "lightweight"), 'tensorboard_dir': str(TB_DIR / "lightweight"), 'results_dir': str(RESULTS_DIR / "lightweight"),
        'debug': False, 'verbose': False, 'random_seed': 42, 'reproducible': True,
        'parallel_processing': False, 'max_workers': 1, 'export_onnx': True, 'non_interactive': False,
        'cuda_optimizations': False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True,
                       'optimize_for_mobile': True, 'runtime_validation': True,
                       'validation_tolerance': 1e-5, 'verbose': False}
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'lightweight', 'current_override': None,
        'override_rules': {'security': False, 'monitoring': False, 'hardware': False},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(),
        'auto_apply': False, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': False, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_lightweight',
        'direction': 'minimize', 'n_trials': 20, 'timeout': 1800,
        'sampler': 'RandomSampler', 'pruner': 'NopPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-4, 'high': 1e-2, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [8, 16, 32]},
            'encoding_dim': {'type': 'int', 'low': 4, 'high': 12},
            'dropout_rate': {'type': 'float', 'low': 0.0, 'high': 0.3}
        },
        'early_stopping': {'enabled': False, 'patience': 5, 'min_improvement': 1e-3},
        'timeout_seconds': 1800, 'trial_epochs': 20, 'trial_patience': 4,
        'cleanup_trials': True, 'generate_plots': False,
        'search_space': {
            'encoding_dim_min': 4, 'encoding_dim_max': 12, 'hidden_layers_min': 1, 'hidden_layers_max': 1,
            'lr_min': 1e-4, 'lr_max': 1e-2, 'batch_sizes': [8, 16, 32],
            'weight_decay_min': 0.0, 'weight_decay_max': 1e-4, 'dropout_min': 0.0, 'dropout_max': 0.3,
            'activations': ["relu", "leaky_relu"], 'normalizations': [None], 'percentile_min': 90, 'percentile_max': 95
        },
        'hpo_sampler': {'type': 'Random', 'seed': 42, 'consider_prior': False, 'prior_weight': 1.0,
                   'consider_magic_clip': False, 'consider_endpoints': False, 'n_startup_trials': 10,
                   'n_ei_candidates': 15, 'multivariate': False},
        'hpo_pruner': {'type': 'Nop', 'n_startup_trials': 0, 'n_warmup_steps': 0, 'interval_steps': 1},
        'scoring': {'use_composite_score': False, 'validation_weight': 0.8, 'test_weight': 0.2,
                   'complexity_weight': 0.1, 'max_params_penalty': 5000},
        'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/lightweight_study.db",
                   'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': False, 'folds': 3, 'stratified': False, 'random_state': 42},
        'metrics': ['mse', 'mae', 'precision', 'recall', 'f1_score'],
        'validation_frequency': 2, 'save_validation_results': False, 'detailed_metrics': False
    },
    'experimental': {
        'features': {'advanced_logging': False, 'model_interpretability': False,
                    'federated_learning': False, 'active_learning': False},
        'settings': {'experimental_mode': False, 'beta_features': False, 'research_mode': False}
    }
}

ADVANCED_PRESET = {
    'metadata': {
        'description': 'Advanced configuration for research experiments and high-performance systems',
        'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
        'created': datetime.now().isoformat(), 'last_modified': datetime.now().isoformat(),
        'preset_used': 'advanced', 'recommended_hardware': {'gpu_memory_gb': 16, 'cpu_cores': 16, 'ram_gb': 32},
        'compatibility': ['EnhancedAutoencoder', 'AutoencoderEnsemble'],
        'system': {
            'python_version': platform.python_version(), 'platform': platform.platform(),
            'architecture': platform.machine(), 'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__, 'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(), 'os': platform.system(),
            'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets', 'hyperparameter_optimization']
        }
    },
    'training': {
        'batch_size': 256, 'epochs': 300, 'learning_rate': 0.00005, 'patience': 30, 'weight_decay': 1e-6,
        'gradient_clip': 0.05, 'gradient_accumulation_steps': 16, 'mixed_precision': True if torch.cuda.is_available() else False,
        'num_workers': max(8, os.cpu_count() or 8), 'optimizer': 'AdamW', 'scheduler': 'CosineAnnealingWarmRestarts',
        'scheduler_params': {'T_0': 50, 'T_mult': 2, 'eta_min': 1e-7, 'last_epoch': -1},
        'early_stopping': True, 'validation_split': 0.1, 'shuffle': True,
        'pin_memory': True if torch.cuda.is_available() else False, 'persistent_workers': True,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 10, 'lr_factor': 0.5, 'min_lr': 1e-8
    },
    'model': {
        'model_type': 'AutoencoderEnsemble', 'input_dim': 50, 'encoding_dim': 24, 'hidden_dims': [512, 256, 128, 64],
        'dropout_rates': [0.05, 0.05, 0.03, 0.02], 'activation': 'gelu', 'activation_param': 0.0,
        'normalization': 'layer', 'use_batch_norm': False, 'use_layer_norm': True,
        'diversity_factor': 0.3, 'min_features': 15, 'num_models': 7, 'skip_connection': True,
        'residual_blocks': True, 'bias': True, 'weight_init': 'kaiming_normal',
        'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish'],
        'available_normalizations': ['batch', 'layer', 'instance', 'group', None],
        'available_initializers': ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'orthogonal'],
        'legacy_mode': False, 'use_attention': True
    },
    'security': {
        'percentile': 88, 'attack_threshold': 0.45, 'false_negative_cost': 1.0,
        'enable_security_metrics': True, 'anomaly_threshold_strategy': 'dynamic_percentile',
        'early_warning_threshold': 0.4, 'adaptive_threshold': True, 'confidence_interval': 0.99,
        'detection_methods': ['reconstruction_error', 'statistical_analysis', 'mahalanobis_distance', 'isolation_forest'],
        'alert_levels': ['low', 'medium', 'high', 'critical'], 'threshold_validation': True,
        'ensemble_voting': 'weighted', 'uncertainty_threshold': 0.2
    },
    'data': {
        'normal_samples': 20000, 'attack_samples': 5000, 'features': 50, 'use_real_data': True,
        'data_normalization': 'standard', 'anomaly_factor': 1.1, 'random_state': 42,
        'validation_split': 0.1, 'test_split': 0.1, 'stratified_split': True,
        'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.2, 'anomaly_sparsity': 0.5, 'noise_factor': 0.02, 
                               'correlation_strength': 0.4, 'feature_interactions': True},
        'preprocessing': {'remove_outliers': True, 'outlier_threshold': 2.5, 'impute_missing': True, 
                         'imputation_strategy': 'median', 'feature_scaling': 'robust', 'dimensionality_reduction': 'auto'},
        'shuffle': True, 'pin_memory': True if torch.cuda.is_available() else False
    },
    'monitoring': {
        'metrics_frequency': 50, 'checkpoint_frequency': 50, 'tensorboard_logging': True,
        'console_logging_level': 'DEBUG', 'save_best_model': True, 'save_model_history': True,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss', 'training_accuracy',
                           'validation_accuracy', 'learning_rate', 'epoch_time', 'memory_usage',
                           'gradient_norm', 'parameter_updates'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
        'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 10, 'save_checkpoints': True,
        'tensorboard': {'export_formats': ["json", "csv"], 'include_histograms': True, 'include_images': True,
                       'max_scalars': 5000, 'max_histograms': 200, 'max_images': 20, 'save_summary': True},
        'profiling_enabled': True, 'performance_metrics': True
    },
    'hardware': {
        'device': 'cuda' if torch.cuda.is_available() else 'cpu', 'recommended_gpu_memory': 16,
        'minimum_system_requirements': {'cpu_cores': 8, 'ram_gb': 16, 'disk_space': 20},
        'optimal_system_requirements': {'cpu_cores': 16, 'ram_gb': 32, 'disk_space': 50, 'gpu_memory': 16},
        'memory_management': {'max_memory_fraction': 0.9, 'allow_memory_growth': True, 'memory_limit': None},
        'performance_optimization': {'use_cuda': True if torch.cuda.is_available() else False, 'use_amp': True if torch.cuda.is_available() else False, 'benchmark_mode': True, 
                                   'deterministic': False, 'cudnn_benchmark': True if torch.cuda.is_available() else False}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR / "advanced"), 'log_dir': str(LOG_DIR / "advanced"),
        'config_dir': str(CONFIG_DIR / "advanced"), 'data_dir': str(DATA_DIR / "advanced"),
        'checkpoint_dir': str(CHECKPOINTS_DIR / "advanced"), 'tensorboard_dir': str(TB_DIR / "advanced"), 'results_dir': str(RESULTS_DIR / "advanced"),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True,
        'parallel_processing': True, 'max_workers': max(8, os.cpu_count() or 8),
        'export_onnx': True, 'non_interactive': False, 'cuda_optimizations': True if torch.cuda.is_available() else False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True,
                       'optimize_for_mobile': False, 'runtime_validation': True,
                       'validation_tolerance': 1e-5, 'verbose': True},
        'distributed_training': False
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'advanced', 'current_override': None,
        'override_rules': {'security': True, 'monitoring': True, 'hardware': True},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(),
        'auto_apply': True, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': True, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo_advanced',
        'direction': 'minimize', 'n_trials': 500, 'timeout': 14400,
        'sampler': 'TPESampler', 'pruner': 'HyperbandPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-6, 'high': 1e-3, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [64, 128, 256, 512]},
            'encoding_dim': {'type': 'int', 'low': 16, 'high': 64},
            'hidden_dims': {'type': 'suggest', 'options': [[256, 128], [512, 256, 128], [1024, 512, 256, 128]]},
            'dropout_rate': {'type': 'float', 'low': 0.0, 'high': 0.3},
            'diversity_factor': {'type': 'float', 'low': 0.1, 'high': 0.5},
            'weight_decay': {'type': 'float', 'low': 1e-7, 'high': 1e-4, 'log': True}
        },
        'early_stopping': {'enabled': True, 'patience': 15, 'min_improvement': 1e-5},
        'timeout_seconds': 14400, 'trial_epochs': 100, 'trial_patience': 10,
        'cleanup_trials': True, 'generate_plots': True,
        'search_space': {
            'encoding_dim_min': 16, 'encoding_dim_max': 64, 'hidden_layers_min': 2, 'hidden_layers_max': 4,
            'lr_min': 1e-6, 'lr_max': 1e-3, 'batch_sizes': [64, 128, 256, 512],
            'weight_decay_min': 1e-7, 'weight_decay_max': 1e-4, 'dropout_min': 0.0, 'dropout_max': 0.3,
            'activations': ["relu", "leaky_relu", "gelu", "swish"], 'normalizations': ["batch", "layer", "instance"],
            'percentile_min': 85, 'percentile_max': 95
        },
        'hpo_sampler': {'type': 'TPE', 'seed': 42, 'consider_prior': True, 'prior_weight': 1.0,
                   'consider_magic_clip': True, 'consider_endpoints': False, 'n_startup_trials': 20,
                   'n_ei_candidates': 24, 'multivariate': True},
        'hpo_pruner': {'type': 'Hyperband', 'n_startup_trials': 10, 'n_warmup_steps': 5, 'interval_steps': 1,
                   'min_resource': 1, 'max_resource': 'auto', 'reduction_factor': 3},
        'scoring': {'use_composite_score': True, 'validation_weight': 0.7, 'test_weight': 0.2,
                   'complexity_weight': 0.1, 'max_params_penalty': 100000},
        'storage': {'enabled': True, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/advanced_study.db",
                   'load_if_exists': True, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': True, 'folds': 5, 'stratified': True, 'random_state': 42},
        'metrics': ['mse', 'mae', 'r2_score', 'explained_variance', 'precision', 'recall', 'f1_score', 
                   'auc_roc', 'average_precision', 'brier_score'],
        'validation_frequency': 1, 'save_validation_results': True, 'detailed_metrics': True,
        'confidence_intervals': True
    },
    'experimental': {
        'features': {'advanced_logging': True, 'model_interpretability': True, 'federated_learning': False,
                    'active_learning': True, 'bayesian_optimization': True},
        'settings': {'experimental_mode': True, 'beta_features': True, 'research_mode': True, 'enable_debugging': False}
    }
}

DEFAULT_PRESET = {
    'metadata': {
        'description': 'Default balanced configuration for general use',
        'version': '2.1',
        'config_version': '2.1',
        'config_type': 'autoencoder',
        'created': datetime.now().isoformat(),
        'last_modified': datetime.now().isoformat(),
        'preset_used': 'default',
        'recommended_hardware': {'gpu_memory_gb': 8, 'cpu_cores': 4, 'ram_gb': 8},
        'compatibility': ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble'],
        'system': {
            'python_version': platform.python_version(),
            'platform': platform.platform(),
            'architecture': platform.machine(),
            'processor': platform.processor() or 'unknown',
            'pytorch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
            'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'hostname': platform.node(),
            'os': platform.system(),
            'os_release': platform.release(),
            'cpu_count': os.cpu_count() or 1
        },
        'validation': {
            'schema_version': '2.1',
            'required_sections': ['training', 'model', 'security', 'data'],
            'optional_sections': ['monitoring', 'hardware', 'presets', 'hyperparameter_optimization']
        }
    },
    'training': {
        'batch_size': 64, 'epochs': 100, 'learning_rate': 0.001, 'patience': 10, 'weight_decay': 1e-4,
        'gradient_clip': 1.0, 'gradient_accumulation_steps': 4, 'mixed_precision': True if torch.cuda.is_available() else False,
        'num_workers': min(4, os.cpu_count() or 1), 'optimizer': 'AdamW', 'scheduler': 'ReduceLROnPlateau',
        'scheduler_params': {'mode': 'min', 'factor': 0.5, 'patience': 5, 'min_lr': 1e-6},
        'early_stopping': True, 'validation_split': 0.2, 'shuffle': True,
        'pin_memory': True if torch.cuda.is_available() else False, 'persistent_workers': False,
        'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 2, 'lr_factor': 0.5, 'min_lr': 1e-7
    },
    'model': {
        'model_type': 'EnhancedAutoencoder', 'input_dim': 20, 'encoding_dim': 12, 'hidden_dims': [128, 64],
        'dropout_rates': [0.2, 0.15], 'activation': 'leaky_relu', 'activation_param': 0.2,
        'normalization': 'batch', 'use_batch_norm': True, 'use_layer_norm': False,
        'diversity_factor': 0.1, 'min_features': 5, 'num_models': 1, 'skip_connection': True,
        'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
        'model_types': list(MODEL_VARIANTS.keys()),
        'available_activations': ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid'],
        'available_normalizations': ['batch', 'layer', 'instance', None],
        'available_initializers': ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal'],
        'legacy_mode': False, 'use_attention': False
    },
    'security': {
        'percentile': 95, 'attack_threshold': 0.3, 'false_negative_cost': 2.0,
        'enable_security_metrics': True, 'anomaly_threshold_strategy': 'percentile',
        'early_warning_threshold': 0.25, 'adaptive_threshold': True, 'confidence_interval': 0.95,
        'detection_methods': ['reconstruction_error', 'statistical_analysis'],
        'alert_levels': ['low', 'medium', 'high', 'critical'], 'threshold_validation': True
    },
    'data': {
        'normal_samples': 8000, 'attack_samples': 2000, 'features': 20, 'use_real_data': False,
        'data_normalization': 'standard', 'anomaly_factor': 1.5, 'random_state': 42,
        'validation_split': 0.2, 'test_split': 0.2, 'stratified_split': True,
        'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        'synthetic_generation': {'cluster_variance': 0.1, 'anomaly_sparsity': 0.3, 'noise_factor': 0.05, 'correlation_strength': 0.3},
        'preprocessing': {'remove_outliers': True, 'outlier_threshold': 3.0, 'impute_missing': True, 'imputation_strategy': 'mean'},
        'shuffle': True, 'pin_memory': True if torch.cuda.is_available() else False
    },
    'monitoring': {
        'metrics_frequency': 10, 'checkpoint_frequency': 5, 'tensorboard_logging': True,
        'console_logging_level': 'INFO', 'save_best_model': True, 'save_model_history': True,
        'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss', 'learning_rate', 'epoch_time', 'memory_usage'],
        'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
        'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 1, 'save_checkpoints': True,
        'tensorboard': {'export_formats': ["json", "csv"], 'include_histograms': False, 'include_images': False,
                       'max_scalars': 1000, 'max_histograms': 100, 'max_images': 10, 'save_summary': True}
    },
    'hardware': {
        'device': 'auto', 'recommended_gpu_memory': 8,
        'minimum_system_requirements': {'cpu_cores': 2, 'ram_gb': 4, 'disk_space': 5},
        'optimal_system_requirements': {'cpu_cores': 4, 'ram_gb': 8, 'disk_space': 10, 'gpu_memory': 8},
        'memory_management': {'max_memory_fraction': 0.8, 'allow_memory_growth': True, 'memory_limit': None},
        'performance_optimization': {'use_cuda': True if torch.cuda.is_available() else False, 'use_amp': True if torch.cuda.is_available() else False,
                                   'benchmark_mode': True, 'deterministic': False}
    },
    'system': {
        'model_dir': str(DEFAULT_MODEL_DIR), 'log_dir': str(LOG_DIR), 'config_dir': str(CONFIG_DIR),
        'data_dir': str(DATA_DIR), 'checkpoint_dir': str(CHECKPOINTS_DIR), 'tensorboard_dir': str(TB_DIR), 'results_dir': str(RESULTS_DIR),
        'debug': False, 'verbose': True, 'random_seed': 42, 'reproducible': True,
        'parallel_processing': True, 'max_workers': min(4, os.cpu_count() or 1),
        'export_onnx': False, 'non_interactive': False, 'cuda_optimizations': True if torch.cuda.is_available() else False,
        'onnx_export': {'opset_version': 14, 'dynamic_axes': True, 'constant_folding': True,
                       'optimize_for_mobile': False, 'runtime_validation': True,
                       'validation_tolerance': 1e-5, 'verbose': False}
    },
    'presets': {
        'available_presets': get_available_presets(), 'current_preset': 'default', 'current_override': None,
        'override_rules': {'security': False, 'monitoring': True, 'hardware': False},
        'preset_configs': get_preset_descriptions(), 'custom_presets_available': list_custom_presets(),
        'auto_apply': False, 'validate_compatibility': True
    },
    'hyperparameter_optimization': {
        'enabled': False, 'strategy': 'optuna', 'study_name': 'autoencoder_hpo',
        'direction': 'minimize', 'n_trials': 50, 'timeout': 3600,
        'sampler': 'TPESampler', 'pruner': 'MedianPruner', 'objective_metric': 'validation_loss',
        'optimization_space': {
            'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 1e-1, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [16, 32, 64, 128]},
            'encoding_dim': {'type': 'int', 'low': 4, 'high': 32},
            'hidden_dims': {'type': 'suggest', 'options': [[64], [128, 64], [256, 128, 64]]},
            'dropout_rate': {'type': 'float', 'low': 0.0, 'high': 0.5}
        },
        'early_stopping': {'enabled': True, 'patience': 10, 'min_improvement': 1e-4},
        'timeout_seconds': 3600, 'trial_epochs': 30, 'trial_patience': 5,
        'cleanup_trials': True, 'generate_plots': True,
        'search_space': {
            'encoding_dim_min': 4, 'encoding_dim_max': 64, 'hidden_layers_min': 1, 'hidden_layers_max': 3,
            'lr_min': 1e-5, 'lr_max': 1e-2, 'batch_sizes': [32, 64, 128, 256],
            'weight_decay_min': 1e-6, 'weight_decay_max': 1e-2, 'dropout_min': 0.1, 'dropout_max': 0.5,
            'activations': ["relu", "leaky_relu", "gelu"], 'normalizations': [None, "batch", "layer"],
            'percentile_min': 90, 'percentile_max': 99
        },
        'hpo_sampler': {'type': 'TPE', 'seed': 42, 'consider_prior': True, 'prior_weight': 1.0,
                   'consider_magic_clip': True, 'consider_endpoints': False, 'n_startup_trials': 10,
                   'n_ei_candidates': 24, 'multivariate': False},
        'hpo_pruner': {'type': 'Median', 'n_startup_trials': 5, 'n_warmup_steps': 10, 'interval_steps': 1},
        'scoring': {'use_composite_score': False, 'validation_weight': 0.7, 'test_weight': 0.2,
                   'complexity_weight': 0.1, 'max_params_penalty': 100000},
        'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/study.db",
                   'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
    },
    'validation': {
        'cross_validation': {'enabled': False, 'folds': 5, 'stratified': True, 'random_state': 42},
        'metrics': ['mse', 'mae', 'r2_score', 'explained_variance', 'precision', 'recall', 'f1_score', 'auc_roc'],
        'validation_frequency': 1, 'save_validation_results': True, 'detailed_metrics': False
    },
    'experimental': {
        'features': {'advanced_logging': False, 'model_interpretability': False,
                    'federated_learning': False, 'active_learning': False},
        'settings': {'experimental_mode': False, 'beta_features': False, 'research_mode': False}
    }
}

# Populate PRESET_CONFIGS after all presets are defined
PRESET_CONFIGS.update({
    'default': DEFAULT_PRESET,
    'stability': STABILITY_PRESET,
    'performance': PERFORMANCE_PRESET,
    'baseline': BASELINE_PRESET,
    'debug': DEBUG_PRESET,
    'lightweight': LIGHTWEIGHT_PRESET,
    'advanced': ADVANCED_PRESET
})

# Add specialized configs that don't have preset equivalents
PRESET_CONFIGS.update({
    'stability_test': {
        **STABILITY_PRESET,
        'metadata': {
            **STABILITY_PRESET['metadata'],
            'description': 'Stability-focused configuration for architecture testing',
            'config_type': 'architecture_test',
            'base_preset': 'stability'
        },
        'testing': {
            'num_architecture_variants': 5,
            'stability_threshold': 0.95,
            'convergence_tolerance': 1e-4,
            'max_variance': 0.1,
            'test_cycles': 3,
            'stability_metrics': ['loss_variance', 'gradient_norm', 'parameter_updates']
        }
    },
    'performance_test': {
        **PERFORMANCE_PRESET,
        'metadata': {
            **PERFORMANCE_PRESET['metadata'],
            'description': 'Performance-optimized configuration for architecture testing',
            'config_type': 'performance_test',
            'base_preset': 'performance'
        },
        'performance_metrics': {
            'target_throughput': 1000,
            'max_latency': 50,
            'memory_threshold': 0.8,
            'warmup_cycles': 3,
            'measurement_cycles': 5,
            'stability_requirement': 0.9
        },
        'architecture_tests': {
            'variants_to_test': [
                {'name': 'baseline', 'config': 'performance'},
                {'name': 'reduced_ensemble', 'num_models': 3},
                {'name': 'increased_dropout', 'dropout_rates': [0.2, 0.15]},
                {'name': 'batch_norm_only', 'use_batch_norm': True, 'use_layer_norm': False}
            ],
            'comparison_metrics': [
                'throughput', 'latency', 'memory_usage', 
                'reconstruction_error', 'training_stability'
            ]
        }
    }
})

_cached_config = None
_config_cache_time = None

def invalidate_config_cache():
    """Invalidate the configuration cache to force reload with enhanced logging."""
    global _cached_config, _config_cache_time
    
    # Check if cache was active before invalidation
    was_cached = _cached_config is not None and _config_cache_time is not None
    
    if was_cached:
        cache_age = time.time() - _config_cache_time
        logger.debug(f"Invalidating configuration cache (age: {cache_age:.1f}s)")
    
    _cached_config = None
    _config_cache_time = None
    
    # Log cache invalidation for debugging
    if was_cached:
        logger.info("Configuration cache invalidated - next access will reload from source")
    else:
        logger.debug("Cache invalidation requested but no cache was active")

def get_safe_custom_presets() -> List[str]:
    """Safely get custom presets list with error handling."""
    try:
        return list_custom_presets() if 'list_custom_presets' in globals() and callable(list_custom_presets) else []
    except Exception as e:
        logger.debug(f"Could not get custom presets: {e}")
        return []

def _update_preset_information(config: Dict[str, Any]) -> None:
    """Centralized function to update preset information across all functions."""
    try:
        if 'presets' not in config:
            config['presets'] = {}
        
        config['presets'].update({
            'available_presets': get_available_presets(),
            'preset_configs': get_preset_descriptions(),
            'custom_presets_available': get_safe_custom_presets()
        })
    except Exception as e:
        logger.debug(f"Failed to update preset information: {e}")

def ensure_preset_consistency(config: Dict[str, Any]) -> Dict[str, Any]:
    """Ensure preset configuration is consistent and up-to-date."""
    try:
        _update_preset_information(config)
        
        # Validate current preset
        current_preset = config.get('presets', {}).get('current_preset')
        if current_preset and current_preset not in get_available_presets():
            logger.warning(f"Current preset '{current_preset}' is not available, clearing")
            config['presets']['current_preset'] = None
        
        return config
        
    except Exception as e:
        logger.warning(f"Preset consistency check failed: {e}")
        return config

def get_default_config() -> Dict[str, Any]:
    """Get comprehensive default system configuration leveraging PRESET_CONFIGS and system analysis.
    
    This function loads the default preset from PRESET_CONFIGS and enhances it with
    comprehensive system information and runtime-specific updates. It provides intelligent
    system-aware configuration with performance optimizations and compatibility checks.
    
    Returns:
        Dictionary containing the complete default configuration with system analysis
    """
    global _cached_config, _config_cache_time
    
    try:
        current_time = datetime.now().isoformat()
        
        # Check cache validity (30 seconds)
        if (_cached_config is not None and _config_cache_time is not None and 
            time.time() - _config_cache_time < 30):
            logger.debug("Returning cached default configuration")
            return _cached_config
        
        # INITIAL MEMORY OPTIMIZATION - Get hardware context early for memory-aware processing
        hardware_data = None
        total_ram_gb = 8.0  # Conservative default
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        except Exception as e:
            logger.debug(f"Hardware detection failed: {e}")
            hardware_data = {}
        
        # PRE-PROCESSING MEMORY OPTIMIZATION - Clear memory before intensive operations
        if total_ram_gb < 8:
            try:
                pre_clear = enhanced_clear_memory(
                    aggressive=total_ram_gb < 4,
                    hardware_data=hardware_data
                )
                if pre_clear.get('success'):
                    logger.debug("Pre-processing memory optimization completed")
            except Exception as e:
                logger.debug(f"Pre-processing memory optimization failed: {e}")
        
        # Load base configuration from PRESET_CONFIGS
        if 'default' in PRESET_CONFIGS and PRESET_CONFIGS['default']:
            base_config = deepcopy(PRESET_CONFIGS['default'])
            logger.debug("Loaded default configuration from PRESET_CONFIGS")
            config_source = 'PRESET_CONFIGS[default]'
        else:
            logger.warning("Default preset not found in PRESET_CONFIGS, creating minimal fallback")
            base_config = _create_minimal_fallback_config('minimal')
            config_source = 'minimal_fallback'
        
        # Validate configuration structure
        try:
            validate_config(base_config)
            logger.debug("Configuration validation passed")
        except Exception as e:
            logger.error(f"Configuration validation failed: {e}")
            # Use standard fallback instead of emergency
            base_config = _create_minimal_fallback_config('standard')
            config_source = 'validation_failure_fallback'
            # Re-validate fallback
            try:
                validate_config(base_config)
            except Exception as validation_error:
                logger.critical(f"Even fallback configuration failed validation: {validation_error}")
                # Use emergency fallback
                base_config = _create_minimal_fallback_config('emergency')
                config_source = 'emergency_fallback'
        
        # MEMORY OPTIMIZATION - Clear memory after validation for low-memory systems
        if total_ram_gb < 8:
            try:
                post_validation_clear = enhanced_clear_memory(
                    aggressive=total_ram_gb < 4,
                    hardware_data=hardware_data
                )
                if post_validation_clear.get('success'):
                    logger.debug("Post-validation memory optimization completed")
            except Exception as e:
                logger.debug(f"Post-validation memory optimization failed: {e}")
        
        # Gather comprehensive system information with memory-aware approach
        try:
            # For systems with limited RAM, use basic system info to avoid memory pressure
            if total_ram_gb < 4:
                system_analysis = _get_basic_system_info()
                logger.debug("Using basic system info due to memory constraints")
            else:
                system_analysis = get_system_info(
                    include_versions=True,
                    include_hardware=True, 
                    include_memory_usage=True,
                    include_detailed_analysis=total_ram_gb >= 8  # Only detailed analysis for systems with adequate RAM
                )
                analysis_duration = system_analysis.get('collection_metadata', {}).get('collection_duration_ms', 0)
                logger.debug(f"System analysis completed in {analysis_duration:.1f}ms")
        except Exception as e:
            logger.warning(f"System analysis failed, using basic system info: {e}")
            system_analysis = _get_basic_system_info()
        
        # MEMORY OPTIMIZATION - Clear memory after intensive system analysis
        system_analysis_size = len(str(system_analysis)) / (1024 * 1024)  # Size in MB
        if system_analysis_size > 1.0 or total_ram_gb < 8:
            try:
                post_analysis_clear = enhanced_clear_memory(
                    aggressive=system_analysis_size > 5.0 or total_ram_gb < 4,
                    hardware_data=hardware_data
                )
                if post_analysis_clear.get('success'):
                    logger.debug(f"Post-analysis memory optimization completed for {system_analysis_size:.1f}MB data")
            except Exception as e:
                logger.debug(f"Post-analysis memory optimization failed: {e}")
        
        # Update metadata with comprehensive system information
        if 'metadata' in base_config:
            base_config['metadata'].update({
                'last_accessed': current_time,
                'config_loaded_at': current_time,
                'config_source': config_source,
                'system_analysis_timestamp': system_analysis.get('timestamp', current_time),
                'system_analysis_quality': system_analysis.get('collection_metadata', {}).get('data_quality', 'unknown'),
                'config_generation_method': 'system_aware_default'
            })
            
            # Enhanced system information from analysis
            if 'system' in base_config['metadata']:
                base_config['metadata']['system'].update({
                    # Core system info from analysis
                    'python_version': system_analysis.get('python', {}).get('version_info', {}).get('version_tuple', platform.python_version()),
                    'platform': system_analysis.get('platform', {}).get('platform', platform.platform()),
                    'architecture': system_analysis.get('platform', {}).get('architecture', [platform.machine(), ''])[0] if isinstance(system_analysis.get('platform', {}).get('architecture', platform.machine()), list) else platform.machine(),
                    'processor': system_analysis.get('platform', {}).get('processor', platform.processor() or 'unknown'),
                    'hostname': system_analysis.get('platform', {}).get('node', platform.node()),
                    'os': system_analysis.get('platform', {}).get('system', platform.system()),
                    'os_release': system_analysis.get('platform', {}).get('release', platform.release()),
                    
                    # Hardware capabilities from analysis
                    'cpu_count': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('cpu', {}).get('logical_cores', os.cpu_count() or 1),
                    'cpu_performance_class': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('cpu', {}).get('performance_class', 'unknown'),
                    'memory_gb': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('memory', {}).get('total_gb', 0),
                    'memory_performance_class': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('memory', {}).get('performance_class', 'unknown'),
                    'system_performance_class': system_analysis.get('hardware_analysis', {}).get('system_class', 'unknown'),
                    'hardware_performance_score': system_analysis.get('hardware_analysis', {}).get('performance_score', 0),
                    
                    # Package environment health
                    'environment_health': system_analysis.get('package_analysis', {}).get('environment_health', {}),
                    'package_compatibility_score': system_analysis.get('package_analysis', {}).get('environment_health', {}).get('compatibility_score', 0),
                    
                    # Dynamic CUDA information
                    'cuda_available': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('available', False),
                    'cuda_devices': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('count', 0),
                    'cuda_total_memory_gb': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('total_memory_gb', 0),
                    'gpu_performance_class': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('performance_class', 'none'),
                })
                
                # Add PyTorch version information from package analysis
                if 'package_versions' in system_analysis:
                    torch_info = system_analysis['package_versions'].get('torch', {})
                    base_config['metadata']['system'].update({
                        'pytorch_version': torch_info.get('version', 'unknown'),
                        'pytorch_status': torch_info.get('status', 'unknown'),
                        'pytorch_compatible': torch_info.get('compatible', False),
                        'cuda_version': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('cuda_version', 'unknown')
                    })
        
        # MEMORY OPTIMIZATION - Clear memory after metadata processing for large configs
        if len(str(base_config.get('metadata', {}))) > 50000 and total_ram_gb < 16:
            try:
                metadata_clear = enhanced_clear_memory(
                    aggressive=False,
                    hardware_data=hardware_data
                )
                if metadata_clear.get('success'):
                    logger.debug("Metadata processing memory optimization completed")
            except Exception as e:
                logger.debug(f"Metadata processing memory optimization failed: {e}")
        
        # Apply system-aware optimizations to training configuration
        if 'training' in base_config and 'hardware_analysis' in system_analysis:
            hardware_caps = system_analysis['hardware_analysis'].get('capabilities', {})
            system_class = system_analysis['hardware_analysis'].get('system_class', 'unknown')
            
            # CPU-based optimizations
            cpu_info = hardware_caps.get('cpu', {})
            logical_cores = cpu_info.get('logical_cores', 1)
            
            # Optimize num_workers based on CPU cores and system class
            if system_class == 'high_performance':
                optimal_workers = min(8, max(4, logical_cores // 2))
            elif system_class == 'standard':
                optimal_workers = min(4, max(2, logical_cores // 2))
            else:  # limited
                optimal_workers = min(2, max(1, logical_cores // 4))
            
            # Only update if different from preset value
            current_workers = base_config['training'].get('num_workers', 1)
            if abs(optimal_workers - current_workers) > 1:
                base_config['training']['num_workers'] = optimal_workers
                logger.debug(f"Optimized num_workers: {current_workers} -> {optimal_workers}")
            
            # Memory-based optimizations
            memory_info = hardware_caps.get('memory', {})
            total_memory_gb = memory_info.get('total_gb', 0)
            current_batch_size = base_config['training'].get('batch_size', 64)
            
            # Adjust batch size based on available memory
            if total_memory_gb >= 32:
                # High memory system - can handle larger batches
                if current_batch_size < 128:
                    new_batch_size = min(128, current_batch_size * 2)
                    base_config['training']['batch_size'] = new_batch_size
                    logger.debug(f"Increased batch_size for high memory: {current_batch_size} -> {new_batch_size}")
            elif total_memory_gb > 0 and total_memory_gb < 8:
                # Low memory system - reduce batch size
                new_batch_size = min(32, current_batch_size)
                base_config['training']['batch_size'] = new_batch_size
                base_config['training']['gradient_accumulation_steps'] = max(2, base_config['training'].get('gradient_accumulation_steps', 1))
                logger.debug(f"Reduced batch_size for low memory: {current_batch_size} -> {new_batch_size}")
            
            # GPU-based optimizations  
            gpu_info = hardware_caps.get('gpu', {})
            cuda_available = gpu_info.get('available', False)
            gpu_memory_gb = gpu_info.get('total_memory_gb', 0)
            
            # Update CUDA-dependent settings
            base_config['training']['pin_memory'] = cuda_available
            base_config['training']['mixed_precision'] = cuda_available and gpu_memory_gb >= 4
            
            # Adjust batch size based on GPU memory
            if cuda_available and gpu_memory_gb > 0:
                current_batch_size = base_config['training'].get('batch_size', 64)
                
                if gpu_memory_gb >= 16:
                    # High-end GPU - can handle larger models and batches
                    new_batch_size = min(256, current_batch_size * 2)
                    if new_batch_size != current_batch_size:
                        base_config['training']['batch_size'] = new_batch_size
                        logger.debug(f"Increased batch_size for high-end GPU: {current_batch_size} -> {new_batch_size}")
                elif gpu_memory_gb < 4:
                    # Limited GPU memory - reduce batch size
                    new_batch_size = min(16, current_batch_size // 2)
                    base_config['training']['batch_size'] = max(1, new_batch_size)
                    base_config['training']['gradient_accumulation_steps'] = max(4, base_config['training'].get('gradient_accumulation_steps', 1))
                    logger.debug(f"Reduced batch_size for limited GPU: {current_batch_size} -> {new_batch_size}")
        
        # Apply system-aware model configuration optimizations
        if 'model' in base_config and 'hardware_analysis' in system_analysis:
            hardware_caps = system_analysis['hardware_analysis'].get('capabilities', {})
            system_class = system_analysis['hardware_analysis'].get('system_class', 'unknown')
            
            # Adjust model complexity based on system capabilities
            if system_class == 'limited':
                # Reduce model complexity for limited systems
                current_encoding_dim = base_config['model'].get('encoding_dim', 12)
                new_encoding_dim = min(8, current_encoding_dim)
                if new_encoding_dim != current_encoding_dim:
                    base_config['model']['encoding_dim'] = new_encoding_dim
                    logger.debug(f"Reduced encoding_dim for limited system: {current_encoding_dim} -> {new_encoding_dim}")
                
                # Reduce hidden layer sizes
                current_dims = base_config['model'].get('hidden_dims', [128, 64])
                new_dims = [min(64, dim) for dim in current_dims]
                if new_dims != current_dims:
                    base_config['model']['hidden_dims'] = new_dims
                    # Update dropout rates to match
                    current_dropout = base_config['model'].get('dropout_rates', [0.2, 0.15])
                    if len(current_dropout) > len(new_dims):
                        base_config['model']['dropout_rates'] = current_dropout[:len(new_dims)]
                    elif len(current_dropout) < len(new_dims):
                        base_config['model']['dropout_rates'] = current_dropout + [0.2] * (len(new_dims) - len(current_dropout))
                    logger.debug(f"Reduced hidden_dims for limited system: {current_dims} -> {new_dims}")
                
                # Force single model for ensembles
                if base_config['model'].get('model_type') == 'AutoencoderEnsemble':
                    base_config['model']['num_models'] = 1
                    logger.debug("Reduced ensemble size to 1 for limited system")
                
                # Reduce memory usage by disabling normalization
                base_config['model']['use_batch_norm'] = True
                base_config['model']['use_layer_norm'] = False
                logger.debug("Disabled normalization for limited system")
                
            elif system_class == 'high_performance':
                # Increase model complexity for high-performance systems
                gpu_info = hardware_caps.get('gpu', {})
                if gpu_info.get('available') and gpu_info.get('total_memory_gb', 0) >= 8:
                    # Can handle more complex models
                    current_encoding_dim = base_config['model'].get('encoding_dim', 12)
                    new_encoding_dim = max(16, current_encoding_dim)
                    if new_encoding_dim != current_encoding_dim:
                        base_config['model']['encoding_dim'] = new_encoding_dim
                        logger.debug(f"Increased encoding_dim for high-performance system: {current_encoding_dim} -> {new_encoding_dim}")
                    
                    current_dims = base_config['model'].get('hidden_dims', [128, 64])
                    if len(current_dims) < 3:
                        new_dims = [256, 128, 64]
                        base_config['model']['hidden_dims'] = new_dims
                        # Update dropout rates to match
                        base_config['model']['dropout_rates'] = [0.2, 0.15, 0.1]
                        logger.debug(f"Enhanced hidden_dims for high-performance system: {current_dims} -> {new_dims}")
        
        # MEMORY OPTIMIZATION - Clear memory after configuration optimizations
        config_complexity = len(str(base_config.get('model', {}))) + len(str(base_config.get('training', {})))
        if config_complexity > 10000 and total_ram_gb < 16:
            try:
                config_optimization_clear = enhanced_clear_memory(
                    aggressive=config_complexity > 50000,
                    hardware_data=hardware_data
                )
                if config_optimization_clear.get('success'):
                    logger.debug("Configuration optimization memory management completed")
            except Exception as e:
                logger.debug(f"Configuration optimization memory management failed: {e}")
        
        # Update hardware configuration with system-specific recommendations
        if 'hardware' in base_config and 'hardware_analysis' in system_analysis:
            hardware_caps = system_analysis['hardware_analysis'].get('capabilities', {})
            
            # Update hardware requirements based on actual system
            memory_gb = hardware_caps.get('memory', {}).get('total_gb', 0)
            gpu_memory_gb = hardware_caps.get('gpu', {}).get('total_memory_gb', 0)
            
            base_config['hardware'].update({
                'detected_gpu_memory': gpu_memory_gb,
                'detected_system_memory': memory_gb,
                'recommended_gpu_memory': max(4, min(gpu_memory_gb, base_config['hardware'].get('recommended_gpu_memory', 8))) if gpu_memory_gb > 0 else base_config['hardware'].get('recommended_gpu_memory', 8),
                'system_performance_class': system_analysis['hardware_analysis'].get('system_class', 'unknown'),
                'optimization_recommendations': system_analysis.get('detailed_analysis', {}).get('configuration_suggestions', [])
            })
            
            # Update performance optimization flags
            cuda_available = hardware_caps.get('gpu', {}).get('available', False)
            base_config['hardware']['performance_optimization'].update({
                'use_cuda': cuda_available,
                'use_amp': cuda_available and gpu_memory_gb >= 4,
                # Enable benchmarking if CUDA available
                'benchmark_mode': cuda_available,
                # Deterministic mode for CPU-only systems
                'deterministic': not cuda_available
            })
        
        # Update system paths and ensure directories exist
        if 'system' in base_config:
            # Ensure all required directories exist
            required_dirs = ['model_dir', 'log_dir', 'config_dir', 'data_dir', 'checkpoint_dir']
            for dir_key in required_dirs:
                if dir_key in base_config['system']:
                    dir_path = Path(base_config['system'][dir_key])
                    try:
                        dir_path.mkdir(parents=True, exist_ok=True)
                        logger.debug(f"Ensured directory exists: {dir_path}")
                    except Exception as e:
                        logger.warning(f"Could not create directory {dir_path}: {e}")
            
            # Update system configuration with current environment
            base_config['system'].update({
                'python_executable': sys.executable,
                'working_directory': str(Path.cwd()),
                'environment_health': system_analysis.get('package_analysis', {}).get('environment_health', {}).get('overall_status', 'unknown')
            })
        
        # Update preset information with dynamic data
        if 'presets' in base_config:
            base_config['presets'].update({
                'available_presets': get_available_presets(),
                'current_preset': 'default',
                'preset_configs': get_preset_descriptions(),
                'custom_presets_available': get_safe_custom_presets(),
                'system_recommended_preset': _recommend_preset_for_system(system_analysis),
                'preset_compatibility': _check_preset_system_compatibility(base_config, system_analysis)
            })
        
        # Add comprehensive runtime configuration
        base_config['runtime'] = {
            'config_loaded_at': current_time,
            'config_source': config_source,
            'runtime_id': hashlib.md5(current_time.encode()).hexdigest()[:8] if 'hashlib' in globals() else 'unknown',
            'process_id': os.getpid(),
            'working_directory': str(Path.cwd()),
            'python_executable': sys.executable,
            
            # System analysis integration
            'system_analysis_completed': 'hardware_analysis' in system_analysis,
            'system_performance_score': system_analysis.get('hardware_analysis', {}).get('performance_score', 0),
            'system_class': system_analysis.get('hardware_analysis', {}).get('system_class', 'unknown'),
            'environment_health': system_analysis.get('package_analysis', {}).get('environment_health', {}).get('overall_status', 'unknown'),
            
            # Configuration optimizations applied
            'optimizations_applied': {
                'training_optimized': 'training' in base_config and 'hardware_analysis' in system_analysis,
                'model_optimized': 'model' in base_config and 'hardware_analysis' in system_analysis,
                'hardware_optimized': 'hardware' in base_config and 'hardware_analysis' in system_analysis,
                'system_aware': True,
                'preset_based': config_source.startswith('PRESET_CONFIGS')
            },
            
            # Resource availability
            'resource_status': {
                'cuda_available': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('available', False),
                'memory_adequate': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('memory', {}).get('total_gb', 0) >= 4,
                'cpu_adequate': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('cpu', {}).get('logical_cores', 0) >= 2,
                'disk_adequate': system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('storage', {}).get('free_gb', 0) >= 5
            }
        }
        
        # Add warnings and recommendations from system analysis
        if 'detailed_analysis' in system_analysis:
            analysis = system_analysis['detailed_analysis']
            
            base_config['runtime']['system_warnings'] = []
            base_config['runtime']['recommendations'] = []
            
            # Collect all warnings and recommendations
            for category in ['system_recommendations', 'performance_optimizations', 'compatibility_issues', 'resource_warnings']:
                if category in analysis and analysis[category]:
                    base_config['runtime']['system_warnings'].extend(analysis[category])
            
            if 'configuration_suggestions' in analysis:
                base_config['runtime']['recommendations'].extend(analysis['configuration_suggestions'])
            
            # Add configuration health status
            warning_count = len(base_config['runtime']['system_warnings'])
            critical_issues = len([w for w in base_config['runtime']['system_warnings'] 
                                 if any(keyword in w.lower() for keyword in ['critical', 'missing required', 'failed', 'error'])])
            
            base_config['runtime']['configuration_health'] = {
                'status': 'healthy' if warning_count == 0 else 'needs_attention' if critical_issues == 0 else 'critical',
                'warning_count': warning_count,
                'recommendation_count': len(base_config['runtime']['recommendations']),
                'critical_issues': critical_issues,
                'overall_score': max(0, 100 - (warning_count * 10) - (critical_issues * 25))
            }
        
        # Ensure model variants are initialized and compatible
        if not MODEL_VARIANTS:
            try:
                logger.debug("MODEL_VARIANTS not initialized, attempting initialization")
                #initialize_model_variants(silent=True)
                initialize_model_variants(silent=False)
            except Exception as e:
                logger.warning(f"Failed to initialize model variants: {e}")
        
        # Validate model type compatibility
        if MODEL_VARIANTS:
            model_type = base_config.get('model', {}).get('model_type', 'SimpleAutoencoder')
            if model_type not in MODEL_VARIANTS:
                logger.warning(f"Model type '{model_type}' not available, falling back to SimpleAutoencoder")
                base_config['model']['model_type'] = 'SimpleAutoencoder'
                # Simplify configuration for SimpleAutoencoder
                base_config['model']['hidden_dims'] = [base_config['model']['hidden_dims'][0]] if base_config['model'].get('hidden_dims') else [64]
                base_config['model']['dropout_rates'] = [base_config['model']['dropout_rates'][0]] if base_config['model'].get('dropout_rates') else [0.2]
        
        # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
        # Aggressive cleanup after all processing is complete
        try:
            final_clear_results = enhanced_clear_memory(
                aggressive=True,  # Aggressive final cleanup
                hardware_data=hardware_data
            )
            
            if final_clear_results.get('success'):
                actions_taken = final_clear_results.get('actions_taken', [])
                logger.debug(f"Final memory optimization completed: {', '.join(actions_taken)}")
                
                # Add memory optimization summary to runtime
                if 'memory_optimization_summary' not in base_config['runtime']:
                    base_config['runtime']['memory_optimization_summary'] = {
                        'optimizations_performed': len(actions_taken),
                        'final_cleanup': True,
                        'hardware_aware': True,
                        'aggressive_mode': True
                    }
                
        except Exception as e:
            logger.debug(f"Final memory optimization failed: {e}")
        
        # Cache the configuration
        _cached_config = base_config
        _config_cache_time = time.time()
        
        # Log configuration summary
        model_info = base_config.get('model', {})
        training_info = base_config.get('training', {})
        system_class = system_analysis.get('hardware_analysis', {}).get('system_class', 'unknown')
        
        logger.info("Successfully generated system-aware default configuration:")
        logger.info(f"  - System Class: {system_class}")
        logger.info(f"  - Model: {model_info.get('model_type', 'unknown')} (encoding_dim={model_info.get('encoding_dim', 'unknown')})")
        logger.info(f"  - Training: batch_size={training_info.get('batch_size', 'unknown')}, epochs={training_info.get('epochs', 'unknown')}")
        logger.info(f"  - Hardware: CUDA={'available' if base_config.get('hardware', {}).get('performance_optimization', {}).get('use_cuda', False) else 'disabled'}")
        logger.info(f"  - Config Source: {config_source}")
        
        return base_config
        
    except Exception as e:
        logger.error(f"Failed to generate default configuration: {e}", exc_info=True)
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        # Return minimal fallback with error information
        try:
            fallback_config = _create_minimal_fallback_config('minimal')
        except Exception as fallback_error:
            logger.critical(f"Even minimal fallback failed: {fallback_error}")
            # Last resort configuration
            fallback_config = {
                'metadata': {
                    'version': '2.1', 'config_version': '2.1',
                    'created': datetime.now().isoformat() if 'datetime' in globals() else 'unknown',
                    'description': 'Emergency last resort configuration',
                    'preset_used': 'emergency_last_resort',
                    'compatibility': ['SimpleAutoencoder'],
                    'system': {
                        'python_version': platform.python_version() if 'platform' in globals() else 'unknown',
                        'os': platform.system() if 'platform' in globals() else 'unknown'
                    }
                },
                'training': {
                    'batch_size': 8, 'epochs': 5, 'learning_rate': 0.01, 'num_workers': 1,
                    'optimizer': 'SGD', 'mixed_precision': False, 'patience': 3
                },
                'model': {
                    'model_type': 'SimpleAutoencoder', 'encoding_dim': 4, 'hidden_dims': [32],
                    'dropout_rates': [0.1], 'activation': 'relu', 'num_models': 1
                },
                'data': {
                    'normal_samples': 100, 'attack_samples': 20, 'features': 8, 'validation_split': 0.3
                },
                'security': {
                    'percentile': 90, 'attack_threshold': 0.5, 'enable_security_metrics': False
                },
                'hardware': {
                    'device': 'cpu', 'recommended_gpu_memory': 1,
                    'performance_optimization': {'use_cuda': False, 'use_amp': False, 'deterministic': True}
                },
                'system': {
                    'model_dir': './models/emergency', 'debug': True, 'random_seed': 42
                },
                'presets': {
                    'current_preset': 'emergency_last_resort', 'available_presets': []
                }
            }
        
        fallback_config['runtime'] = {
            'config_loaded_at': datetime.now().isoformat() if 'datetime' in globals() else 'unknown',
            'config_source': 'emergency_fallback',
            'error': str(e),
            'system_analysis_failed': True,
            'configuration_health': {
                'status': 'critical',
                'error_message': str(e),
                'fallback_used': True
            }
        }
        
        # Cache the fallback
        _cached_config = fallback_config
        _config_cache_time = time.time()
        
        return fallback_config

def _get_basic_system_info() -> Dict[str, Any]:
    """Get basic system information when full analysis fails."""
    try:
        return {
            'timestamp': datetime.now().isoformat(),
            'platform': {
                'system': platform.system(),
                'platform': platform.platform(),
                'machine': platform.machine(),
                'node': platform.node()
            },
            'python': {
                'version_info': {
                    'version_tuple': tuple(sys.version_info[:3])
                }
            },
            'hardware_analysis': {
                'capabilities': {
                    'cpu': {
                        'logical_cores': os.cpu_count() or 1,
                        'performance_class': 'unknown'
                    },
                    'memory': {
                        'total_gb': 0,
                        'performance_class': 'unknown'
                    },
                    'gpu': {
                        'available': torch.cuda.is_available() if 'torch' in globals() else False,
                        'count': torch.cuda.device_count() if 'torch' in globals() and torch.cuda.is_available() else 0,
                        'performance_class': 'unknown'
                    }
                },
                'system_class': 'unknown',
                'performance_score': 50
            },
            'package_analysis': {
                'environment_health': {
                    'overall_status': 'unknown',
                    'compatibility_score': 0
                }
            },
            'collection_metadata': {
                'data_quality': 'basic',
                'errors': ['Full system analysis failed, using basic info']
            }
        }
    except Exception:
        return {
            'timestamp': datetime.now().isoformat(),
            'hardware_analysis': {'system_class': 'unknown', 'performance_score': 0},
            'package_analysis': {'environment_health': {'overall_status': 'unknown'}},
            'collection_metadata': {'data_quality': 'minimal', 'errors': ['Basic system info collection failed']}
        }

def reset_config() -> None:
    """Reset configuration to default values with comprehensive cleanup.
    
    This function now leverages get_default_config() to obtain a fresh, system-aware
    default configuration instead of creating one from scratch, reducing code
    redundancy and ensuring consistency with the default configuration generation logic.
    """
    try:
        logger.info("Resetting configuration to system-aware defaults...")
        
        # Clear any cached configurations to ensure fresh generation
        global _cached_config, _config_cache_time
        _cached_config = None
        _config_cache_time = None
        
        # Get fresh default configuration using get_default_config()
        # This automatically includes system analysis, hardware optimization, and preset integration
        try:
            default_config = get_default_config()
            logger.debug("Successfully obtained fresh default configuration from get_default_config()")
        except Exception as e:
            logger.error(f"Failed to get default configuration: {e}")
            logger.warning("Falling back to minimal emergency configuration")
            
            # Emergency fallback - create absolute minimal configuration
            default_config = _create_minimal_fallback_config('emergency')
            logger.warning("Using emergency fallback configuration due to get_default_config() failure")
        
        # Validate the default configuration before applying
        try:
            validate_config(default_config)
            logger.debug("Default configuration validation passed")
        except Exception as validation_error:
            logger.error(f"Default configuration validation failed: {validation_error}")
            logger.warning("Attempting to fix validation issues automatically")
            
            # Try to fix common validation issues
            try:
                # Ensure required sections exist
                required_sections = ['training', 'model', 'security', 'data']
                for section in required_sections:
                    if section not in default_config:
                        logger.warning(f"Adding missing section: {section}")
                        if section == 'training':
                            default_config[section] = {
                                'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001,
                                'num_workers': 1, 'optimizer': 'Adam'
                            }
                        elif section == 'model':
                            default_config[section] = {
                                'model_type': 'SimpleAutoencoder', 'encoding_dim': 8,
                                'hidden_dims': [64], 'dropout_rates': [0.2]
                            }
                        elif section == 'security':
                            default_config[section] = {
                                'percentile': 95, 'attack_threshold': 0.3,
                                'enable_security_metrics': True
                            }
                        elif section == 'data':
                            default_config[section] = {
                                'normal_samples': 1000, 'attack_samples': 200,
                                'features': 10, 'validation_split': 0.2
                            }
                
                # Re-validate after fixes
                validate_config(default_config)
                logger.info("Successfully fixed validation issues in default configuration")
                
            except Exception as fix_error:
                logger.critical(f"Could not fix default configuration: {fix_error}")
                # Last resort - use emergency fallback
                default_config = _create_minimal_fallback_config('emergency')
                logger.critical("Using emergency fallback configuration due to validation failures")
        
        # Ensure metadata reflects the reset operation
        if 'metadata' not in default_config:
            default_config['metadata'] = {}
        
        default_config['metadata'].update({
            'modified': datetime.now().isoformat(),
            'reset_timestamp': datetime.now().isoformat(),
            'reset_reason': 'manual_reset_to_defaults',
            'config_source': 'reset_to_system_aware_defaults'
        })
        
        # Add reset operation to runtime information
        if 'runtime' not in default_config:
            default_config['runtime'] = {}
        
        default_config['runtime'].update({
            'last_reset': datetime.now().isoformat(),
            'reset_method': 'get_default_config',
            'config_generation_method': 'system_aware_reset'
        })
        
        # Update preset information to reflect reset
        if 'presets' in default_config:
            default_config['presets']['current_preset'] = 'default'
            default_config['presets']['last_reset'] = datetime.now().isoformat()
            default_config['presets']['reset_to_preset'] = 'default'
        
        # Update global configuration with the new default
        try:
            update_global_config(default_config)
            logger.debug("Successfully updated global configuration")
        except Exception as e:
            logger.error(f"Failed to update global configuration: {e}")
            # Continue anyway as this isn't critical for reset operation
        
        # Save the reset configuration to file
        try:
            save_config(default_config)
            logger.debug("Successfully saved reset configuration to file")
        except Exception as e:
            logger.error(f"Failed to save reset configuration: {e}")
            # Log but don't fail - the configuration is still reset in memory
        
        # Clear any model variants cache to ensure fresh initialization
        global MODEL_VARIANTS
        if MODEL_VARIANTS:
            logger.debug("Clearing model variants cache for fresh initialization")
            MODEL_VARIANTS.clear()
        
        # Re-initialize model variants with the new configuration
        try:
            initialize_model_variants(silent=False)
            logger.debug("Successfully re-initialized model variants")
        except Exception as e:
            logger.warning(f"Failed to re-initialize model variants: {e}")
            # Non-critical for reset operation
        
        # Force memory cleanup after reset
        try:
            enhanced_clear_memory()
            logger.debug("Completed memory cleanup after reset")
        except Exception as e:
            logger.debug(f"Memory cleanup had issues: {e}")
        
        # Log successful reset with configuration summary
        model_info = default_config.get('model', {})
        training_info = default_config.get('training', {})
        system_class = default_config.get('runtime', {}).get('system_class', 'unknown')
        
        logger.info("Configuration reset completed successfully:")
        logger.info(f"  - System Class: {system_class}")
        logger.info(f"  - Model: {model_info.get('model_type', 'unknown')} (encoding_dim={model_info.get('encoding_dim', 'unknown')})")
        logger.info(f"  - Training: batch_size={training_info.get('batch_size', 'unknown')}, epochs={training_info.get('epochs', 'unknown')}")
        logger.info(f"  - Config Source: {default_config.get('runtime', {}).get('config_source', 'unknown')}")
        
        # Verify reset by checking key parameters
        verification_checks = [
            ('training.batch_size', training_info.get('batch_size')),
            ('model.model_type', model_info.get('model_type')),
            ('model.encoding_dim', model_info.get('encoding_dim')),
            ('security.percentile', default_config.get('security', {}).get('percentile')),
            ('data.normal_samples', default_config.get('data', {}).get('normal_samples'))
        ]
        
        logger.debug("Reset verification checks:")
        for param_name, param_value in verification_checks:
            logger.debug(f"  - {param_name}: {param_value}")
        
        logger.info("[SUCCESS] Configuration has been reset to system-aware defaults")
        
    except Exception as e:
        logger.error(f"Critical failure during configuration reset: {e}", exc_info=True)
        
        # Emergency recovery attempt
        try:
            logger.critical("Attempting emergency configuration recovery...")
            
            # Create absolute minimal configuration
            emergency_config = {
                'metadata': {
                    'version': '2.1',
                    'config_version': '2.1',
                    'created': datetime.now().isoformat(),
                    'description': 'Emergency recovery configuration',
                    'preset_used': 'emergency_recovery',
                    'reset_failed': True,
                    'original_error': str(e)
                },
                'training': {
                    'batch_size': 16, 'epochs': 5, 'learning_rate': 0.001,
                    'num_workers': 1, 'optimizer': 'SGD', 'patience': 3
                },
                'model': {
                    'model_type': 'SimpleAutoencoder', 'encoding_dim': 4,
                    'hidden_dims': [32], 'dropout_rates': [0.1], 'activation': 'relu'
                },
                'data': {
                    'normal_samples': 100, 'attack_samples': 20, 'features': 8,
                    'validation_split': 0.3, 'normalization': 'minmax'
                },
                'security': {
                    'percentile': 90, 'attack_threshold': 0.5,
                    'enable_security_metrics': False
                },
                'system': {
                    'model_dir': './models/emergency', 'debug': True,
                    'random_seed': 42, 'export_onnx': False
                },
                'hardware': {
                    'device': 'cpu', 'recommended_gpu_memory': 1,
                    'performance_optimization': {'use_cuda': False, 'use_amp': False}
                },
                'presets': {
                    'current_preset': 'emergency_recovery',
                    'available_presets': []
                },
                'runtime': {
                    'config_source': 'emergency_recovery',
                    'reset_failed': True,
                    'emergency_recovery': True,
                    'recovery_timestamp': datetime.now().isoformat()
                }
            }
            
            # Try to apply emergency configuration
            update_global_config(emergency_config)
            
            # Try to save emergency configuration
            try:
                save_config(emergency_config)
            except:
                logger.critical("Could not save emergency configuration to file")
            
            logger.critical("[EMERGENCY] Applied minimal emergency configuration")
            logger.critical("System is in degraded state - manual intervention recommended")
            
        except Exception as recovery_error:
            logger.critical(f"Emergency recovery also failed: {recovery_error}")
            logger.critical("System configuration is in critical failure state")
            raise RuntimeError(f"Configuration reset failed completely: {str(e)}. Recovery failed: {str(recovery_error)}")
        
        # Re-raise the original exception with context
        raise RuntimeError(f"Configuration reset failed: {str(e)}. Emergency recovery applied.")

def reset_config_interactive():
    """Interactive configuration reset with enhanced confirmation and feedback."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        show_banner()
        
        # Get current configuration info for display
        try:
            current_config = get_current_config()
            current_preset = current_config.get('presets', {}).get('current_preset', 'unknown')
            current_model = current_config.get('model', {}).get('model_type', 'unknown')
            system_class = current_config.get('runtime', {}).get('system_class', 'unknown')
        except Exception as e:
            logger.debug(f"Could not get current config for display: {e}")
            current_preset = 'unknown'
            current_model = 'unknown'
            system_class = 'unknown'
        
        # Display current configuration summary
        print(Fore.CYAN + Style.BRIGHT + "\n" + "="*40)
        print(Fore.YELLOW + Style.BRIGHT + "CONFIGURATION RESET")
        print(Fore.CYAN + Style.BRIGHT + "="*40)
        print(Fore.GREEN + Style.BRIGHT + f"\nCurrent Preset: {current_preset}")
        print(Fore.GREEN + Style.BRIGHT + f"Current Model:  {current_model}")
        print(Fore.GREEN + Style.BRIGHT + f"System Class:   {system_class}")
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "="*60)
        print(Fore.RED + Style.BRIGHT + "This will reset your configuration to system-aware defaults.")
        print(Fore.RED + Style.BRIGHT + "All current settings will be lost and cannot be recovered.")
        print(Fore.RED + Style.BRIGHT + "The new configuration will be optimized for your system.")
        print(Fore.YELLOW + Style.BRIGHT + "="*60)
        
        # Get user confirmation with multiple prompts for safety
        response1 = input(Fore.YELLOW + Style.BRIGHT + "\nDo you want to reset configuration to defaults? (y/N): ").lower().strip()
        
        if response1 != 'y':
            print(Fore.RED + Style.BRIGHT + "Configuration reset cancelled.")
            return
        
        print(Fore.YELLOW + Style.BRIGHT + "\nWARNING! This action cannot be undone!")
        reset_message = (Fore.RED + Style.BRIGHT + "RESET")
        response2 = input(Fore.YELLOW + Style.BRIGHT + f"Are you absolutely sure? Type '{reset_message}' to confirm: ").lower().strip()
        
        if response2 != 'reset':
            print(Fore.RED + Style.BRIGHT + "Configuration reset cancelled.")
            return
        
        # Perform the reset
        print(Fore.GREEN + Style.BRIGHT + "\nResetting configuration...")
        try:
            reset_config()
            
            # Get new configuration summary
            try:
                new_config = get_current_config()
                new_preset = new_config.get('presets', {}).get('current_preset', 'unknown')
                new_model = new_config.get('model', {}).get('model_type', 'unknown')
                new_system_class = new_config.get('runtime', {}).get('system_class', 'unknown')
                new_batch_size = new_config.get('training', {}).get('batch_size', 'unknown')
                new_epochs = new_config.get('training', {}).get('epochs', 'unknown')
            except Exception as e:
                logger.debug(f"Could not get new config for display: {e}")
                new_preset = 'unknown'
                new_model = 'unknown'
                new_system_class = 'unknown'
                new_batch_size = 'unknown'
                new_epochs = 'unknown'
            
            print(Fore.GREEN + Style.BRIGHT + "\n" + "="*60)
            print(Fore.GREEN + Style.BRIGHT + "RESET COMPLETED SUCCESSFULLY")
            print(Fore.GREEN + Style.BRIGHT + "="*60)
            print(Fore.GREEN + Style.BRIGHT + f"\nNew Preset:       {new_preset}")
            print(Fore.GREEN + Style.BRIGHT + f"New Model:        {new_model}")
            print(Fore.GREEN + Style.BRIGHT + f"System Class:     {new_system_class}")
            print(Fore.GREEN + Style.BRIGHT + f"Batch Size:       {new_batch_size}")
            print(Fore.GREEN + Style.BRIGHT + f"Epochs:           {new_epochs}")
            print(Fore.GREEN + Style.BRIGHT + "\n" + "="*60)
            print(Fore.GREEN + Style.BRIGHT + "\nConfiguration has been reset to optimized defaults for your system.")
            print(Fore.GREEN + Style.BRIGHT + "You can now customize the settings as needed.")
            
        except Exception as e:
            print(Fore.RED + Style.BRIGHT + f"\nERROR: Configuration reset failed: {e}")
            print(Fore.RED + Style.BRIGHT + "System may be in an inconsistent state.")
            print(Fore.RED + Style.BRIGHT + "Please check logs for details and consider manual recovery.")
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\n\nConfiguration reset cancelled by user.")
    except Exception as e:
        print(Fore.RED + Style.BRIGHT + f"\nUnexpected error during interactive reset: {e}")
        logger.error(f"Interactive reset failed: {e}", exc_info=True)

def _create_minimal_fallback_config(fallback_level: str = 'minimal') -> Dict[str, Any]:
    """Create fallback configuration with different levels of completeness.
    
    Args:
        fallback_level: Level of fallback configuration
            - 'minimal': Ultra-minimal for emergency use
            - 'standard': More complete fallback with enhanced features
            - 'emergency': Absolute minimal when everything fails
    
    Returns:
        Dictionary containing fallback configuration
    """
    try:
        current_time = datetime.now().isoformat()
        
        # Common base configuration matching updated preset structure
        base_config = {
            'metadata': {
                'description': f'{fallback_level.title()} fallback configuration for system recovery',
                'version': '2.1', 'config_version': '2.1', 'config_type': 'autoencoder',
                'created': current_time, 'last_modified': current_time,
                'preset_used': f'{fallback_level}_fallback',
                'recommended_hardware': {'gpu_memory_gb': 1, 'cpu_cores': 1, 'ram_gb': 2},
                'compatibility': ['SimpleAutoencoder'],
                'system': {
                    'python_version': platform.python_version(),
                    'platform': platform.platform(),
                    'architecture': platform.machine(),
                    'processor': platform.processor() or 'unknown',
                    'pytorch_version': torch.__version__ if 'torch' in globals() else 'unknown',
                    'cuda_available': torch.cuda.is_available() if 'torch' in globals() and hasattr(torch, 'cuda') else False,
                    'cuda_version': torch.version.cuda if hasattr(torch.version, 'cuda') else 'unknown',
                    'cuda_devices': torch.cuda.device_count() if torch.cuda.is_available() else 0,
                    'hostname': platform.node(), 'os': platform.system(),
                    'os_release': platform.release(), 'cpu_count': os.cpu_count() or 1
                },
                'validation': {
                    'schema_version': '2.1',
                    'required_sections': ['training', 'model', 'security', 'data'],
                    'optional_sections': ['monitoring', 'hardware', 'presets']
                },
                'fallback_info': {
                    'is_fallback': True, 'level': fallback_level,
                    'creation_time': current_time,
                    'reason': 'Configuration system failure - using safe defaults',
                    'warnings': [
                        f'This is a {fallback_level} fallback configuration',
                        'Limited functionality available',
                        'Recommend fixing configuration system'
                    ]
                }
            }
        }
        
        # Configuration based on fallback level
        if fallback_level == 'emergency':
            # Absolute minimal configuration
            base_config.update({
                'training': {
                    'batch_size': 8, 'epochs': 2, 'learning_rate': 0.01, 'patience': 2, 'weight_decay': 0.0,
                    'gradient_clip': 5.0, 'gradient_accumulation_steps': 1, 'mixed_precision': False,
                    'num_workers': 1, 'optimizer': 'SGD', 'scheduler': None, 'scheduler_params': {},
                    'early_stopping': False, 'validation_split': 0.4, 'shuffle': True,
                    'pin_memory': False, 'persistent_workers': False,
                    'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 2, 'lr_factor': 0.5, 'min_lr': 1e-7
                },
                'model': {
                    'model_type': 'SimpleAutoencoder', 'input_dim': 8, 'encoding_dim': 2, 'hidden_dims': [16],
                    'dropout_rates': [0.0], 'activation': 'relu', 'activation_param': 0.0,
                    'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False,
                    'diversity_factor': 0.0, 'min_features': 2, 'num_models': 1, 'skip_connection': False,
                    'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
                    'model_types': ['SimpleAutoencoder'],
                    'available_activations': ['relu'],
                    'available_normalizations': [None],
                    'available_initializers': ['xavier_uniform'],
                    'legacy_mode': False, 'use_attention': False
                },
                'security': {
                    'percentile': 85, 'attack_threshold': 0.5, 'false_negative_cost': 1.0,
                    'enable_security_metrics': False, 'anomaly_threshold_strategy': 'fixed_percentile',
                    'early_warning_threshold': 0.45, 'adaptive_threshold': False, 'confidence_interval': 0.8,
                    'detection_methods': ['reconstruction_error'], 'alert_levels': ['low', 'medium'],
                    'threshold_validation': False
                },
                'data': {
                    'normal_samples': 50, 'attack_samples': 10, 'features': 4, 'use_real_data': False,
                    'data_normalization': 'minmax', 'anomaly_factor': 2.0, 'random_state': 42,
                    'validation_split': 0.4, 'test_split': 0.4, 'stratified_split': False,
                    'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
                    'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
                    'synthetic_generation': {'cluster_variance': 0.2, 'anomaly_sparsity': 0.5},
                    'preprocessing': {'remove_outliers': False, 'impute_missing': False},
                    'shuffle': True, 'pin_memory': False
                },
                'monitoring': {
                    'metrics_frequency': 1, 'checkpoint_frequency': 1, 'tensorboard_logging': False,
                    'console_logging_level': 'ERROR', 'save_best_model': False, 'save_model_history': False,
                    'metrics_to_track': ['loss'], 'early_stopping_metric': 'loss',
                    'checkpoint_format': 'pytorch', 'log_model_summary': False,
                    'tensorboard_dir': str(TB_DIR), 'log_frequency': 1, 'save_checkpoints': False,
                    'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False,
                                   'max_scalars': 50, 'max_histograms': 0, 'max_images': 0, 'save_summary': False}
                }
            })
            
        elif fallback_level == 'standard':
            # Enhanced fallback with more features
            safe_batch_size = 16
            safe_epochs = 5
            safe_workers = min(2, os.cpu_count() or 1)
            
            base_config.update({
                'training': {
                    'batch_size': safe_batch_size, 'epochs': safe_epochs, 'learning_rate': 0.001, 
                    'patience': 3, 'weight_decay': 0.0, 'gradient_clip': 1.0, 'gradient_accumulation_steps': 1, 
                    'mixed_precision': False, 'num_workers': safe_workers, 'optimizer': 'Adam', 'scheduler': None, 
                    'scheduler_params': {}, 'early_stopping': True, 'validation_split': 0.3, 'shuffle': True,
                    'pin_memory': False, 'persistent_workers': False,
                    'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 3, 'lr_factor': 0.7, 'min_lr': 1e-6
                },
                'model': {
                    'model_type': 'SimpleAutoencoder', 'input_dim': 16, 'encoding_dim': 4, 'hidden_dims': [32],
                    'dropout_rates': [0.1], 'activation': 'relu', 'activation_param': 0.0,
                    'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False,
                    'diversity_factor': 0.0, 'min_features': 3, 'num_models': 1, 'skip_connection': False,
                    'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
                    'model_types': list(MODEL_VARIANTS.keys()) if 'MODEL_VARIANTS' in globals() else ['SimpleAutoencoder'],
                    'available_activations': ['relu', 'leaky_relu'],
                    'available_normalizations': [None, 'batch'],
                    'available_initializers': ['xavier_uniform', 'xavier_normal'],
                    'legacy_mode': False, 'use_attention': False
                },
                'security': {
                    'percentile': 90, 'attack_threshold': 0.4, 'false_negative_cost': 1.5,
                    'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
                    'early_warning_threshold': 0.35, 'adaptive_threshold': False, 'confidence_interval': 0.9,
                    'detection_methods': ['reconstruction_error'], 'alert_levels': ['low', 'medium', 'high'],
                    'threshold_validation': True
                },
                'data': {
                    'normal_samples': 200, 'attack_samples': 50, 'features': 8, 'use_real_data': False,
                    'data_normalization': 'minmax', 'anomaly_factor': 2.0, 'random_state': 42,
                    'validation_split': 0.3, 'test_split': 0.3, 'stratified_split': False,
                    'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
                    'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
                    'synthetic_generation': {'cluster_variance': 0.1, 'anomaly_sparsity': 0.3, 'noise_factor': 0.05},
                    'preprocessing': {'remove_outliers': False, 'impute_missing': True, 'imputation_strategy': 'mean'},
                    'shuffle': True, 'pin_memory': False
                },
                'monitoring': {
                    'metrics_frequency': 2, 'checkpoint_frequency': 5, 'tensorboard_logging': False,
                    'console_logging_level': 'WARNING', 'save_best_model': True, 'save_model_history': False,
                    'metrics_to_track': ['loss', 'reconstruction_error'], 'early_stopping_metric': 'loss',
                    'checkpoint_format': 'pytorch', 'log_model_summary': False,
                    'tensorboard_dir': str(TB_DIR), 'log_frequency': 2, 'save_checkpoints': True,
                    'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False,
                                   'max_scalars': 100, 'max_histograms': 10, 'max_images': 0, 'save_summary': False}
                },
                'fallback_info': {
                    'is_fallback': True, 'level': 'standard', 'creation_time': current_time,
                    'reason': 'Configuration system failure - using enhanced safe defaults',
                    'recommendations': [
                        'Check configuration file format', 'Verify preset definitions',
                        'Check file permissions', 'Review system requirements'
                    ],
                    'limitations': [
                        'Limited model architectures', 'Reduced functionality',
                        'Basic monitoring only', 'CPU-only execution'
                    ]
                }
            })
            
        else:
            # 'minimal' - default case with balanced functionality
            base_config.update({
                'training': {
                    'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'patience': 5, 'weight_decay': 1e-4,
                    'gradient_clip': 1.0, 'gradient_accumulation_steps': 2, 'mixed_precision': False,
                    'num_workers': min(2, os.cpu_count() or 1), 'optimizer': 'Adam', 'scheduler': None,
                    'scheduler_params': {}, 'early_stopping': True, 'validation_split': 0.2, 'shuffle': True,
                    'pin_memory': False, 'persistent_workers': False,
                    'adam_betas': (0.9, 0.999), 'adam_eps': 1e-8, 'lr_patience': 3, 'lr_factor': 0.7, 'min_lr': 1e-6
                },
                'model': {
                    'model_type': 'SimpleAutoencoder', 'input_dim': 20, 'encoding_dim': 8, 'hidden_dims': [64],
                    'dropout_rates': [0.2], 'activation': 'relu', 'activation_param': 0.0,
                    'normalization': None, 'use_batch_norm': False, 'use_layer_norm': False,
                    'diversity_factor': 0.0, 'min_features': 5, 'num_models': 1, 'skip_connection': False,
                    'residual_blocks': False, 'bias': True, 'weight_init': 'xavier_uniform',
                    'model_types': list(MODEL_VARIANTS.keys()) if 'MODEL_VARIANTS' in globals() else ['SimpleAutoencoder'],
                    'available_activations': ['relu', 'leaky_relu', 'gelu'],
                    'available_normalizations': [None, 'batch'],
                    'available_initializers': ['xavier_uniform', 'xavier_normal'],
                    'legacy_mode': False, 'use_attention': False
                },
                'security': {
                    'percentile': 95, 'attack_threshold': 0.3, 'false_negative_cost': 2.0,
                    'enable_security_metrics': True, 'anomaly_threshold_strategy': 'fixed_percentile',
                    'early_warning_threshold': 0.25, 'adaptive_threshold': False, 'confidence_interval': 0.95,
                    'detection_methods': ['reconstruction_error'], 'alert_levels': ['low', 'medium', 'high'],
                    'threshold_validation': True
                },
                'data': {
                    'normal_samples': 1000, 'attack_samples': 200, 'features': 10, 'use_real_data': False,
                    'data_normalization': 'minmax', 'anomaly_factor': 2.0, 'random_state': 42,
                    'validation_split': 0.2, 'test_split': 0.2, 'stratified_split': True,
                    'data_path': str(DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
                    'artifacts_path': str(DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
                    'synthetic_generation': {'cluster_variance': 0.1, 'anomaly_sparsity': 0.3, 'noise_factor': 0.05},
                    'preprocessing': {'remove_outliers': True, 'impute_missing': True, 'imputation_strategy': 'mean'},
                    'shuffle': True, 'pin_memory': False
                },
                'monitoring': {
                    'metrics_frequency': 5, 'checkpoint_frequency': 10, 'tensorboard_logging': False,
                    'console_logging_level': 'INFO', 'save_best_model': True, 'save_model_history': False,
                    'metrics_to_track': ['loss', 'reconstruction_error', 'validation_loss'],
                    'early_stopping_metric': 'validation_loss', 'checkpoint_format': 'pytorch',
                    'log_model_summary': True, 'tensorboard_dir': str(TB_DIR), 'log_frequency': 5, 'save_checkpoints': True,
                    'tensorboard': {'export_formats': [], 'include_histograms': False, 'include_images': False,
                                   'max_scalars': 200, 'max_histograms': 20, 'max_images': 5, 'save_summary': False}
                }
            })

        # Common sections for all fallback levels
        base_config.update({
            'hardware': {
                'device': 'cpu', 'recommended_gpu_memory': 1,
                'minimum_system_requirements': {'cpu_cores': 1, 'ram_gb': 1, 'disk_space': 2},
                'optimal_system_requirements': {'cpu_cores': 2, 'ram_gb': 2, 'disk_space': 5},
                'memory_management': {'max_memory_fraction': 0.5, 'allow_memory_growth': False, 'memory_limit': 1024},
                'performance_optimization': {'use_cuda': False, 'use_amp': False, 'benchmark_mode': False, 'deterministic': True}
            },
            'system': {
                'model_dir': str(Path('./models/fallback')), 'log_dir': str(Path('./logs/fallback')),
                'config_dir': str(Path('./config/fallback')), 'data_dir': str(Path('./data/fallback')),
                'checkpoint_dir': str(Path('./checkpoints/fallback')),
                'debug': True if fallback_level == 'emergency' else False, 'verbose': True,
                'random_seed': 42, 'reproducible': True, 'parallel_processing': False, 'max_workers': 1,
                'export_onnx': False, 'non_interactive': False, 'cuda_optimizations': False,
                'onnx_export': {'opset_version': 14, 'dynamic_axes': False, 'constant_folding': False,
                               'optimize_for_mobile': False, 'runtime_validation': False,
                               'validation_tolerance': 1e-5, 'verbose': False}
            },
            'presets': {
                'available_presets': [], 'current_preset': f'{fallback_level}_fallback', 'current_override': None,
                'override_rules': {'security': False, 'monitoring': False, 'hardware': False},
                'preset_configs': {}, 'custom_presets_available': [],
                'auto_apply': False, 'validate_compatibility': False
            },
            'hyperparameter_optimization': {
                'enabled': False, 'strategy': 'optuna', 'study_name': f'autoencoder_hpo_{fallback_level}',
                'direction': 'minimize', 'n_trials': 5, 'timeout': 300,
                'sampler': 'RandomSampler', 'pruner': 'NopPruner', 'objective_metric': 'loss',
                'optimization_space': {
                    'learning_rate': {'type': 'float', 'low': 1e-3, 'high': 1e-1, 'log': True},
                    'batch_size': {'type': 'categorical', 'choices': [8, 16, 32]}
                },
                'early_stopping': {'enabled': False, 'patience': 2, 'min_improvement': 1e-2},
                'timeout_seconds': 300, 'trial_epochs': 3, 'trial_patience': 2,
                'cleanup_trials': False, 'generate_plots': False,
                'search_space': {
                    'encoding_dim_min': 2, 'encoding_dim_max': 8, 'hidden_layers_min': 1, 'hidden_layers_max': 1,
                    'lr_min': 1e-3, 'lr_max': 1e-1, 'batch_sizes': [8, 16, 32],
                    'weight_decay_min': 0.0, 'weight_decay_max': 0.0, 'dropout_min': 0.0, 'dropout_max': 0.1,
                    'activations': ["relu"], 'normalizations': [None], 'percentile_min': 80, 'percentile_max': 90
                },
                'hpo_sampler': {'type': 'Random', 'seed': 42, 'consider_prior': False, 'prior_weight': 1.0,
                           'consider_magic_clip': False, 'consider_endpoints': False, 'n_startup_trials': 2,
                           'n_ei_candidates': 5, 'multivariate': False},
                'hpo_pruner': {'type': 'Nop', 'n_startup_trials': 0, 'n_warmup_steps': 0, 'interval_steps': 1},
                'scoring': {'use_composite_score': False, 'validation_weight': 1.0, 'test_weight': 0.0,
                           'complexity_weight': 0.0, 'max_params_penalty': 1000},
                'storage': {'enabled': False, 'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/{fallback_level}_study.db",
                           'load_if_exists': False, 'heartbeat_interval': 60, 'grace_period': 120}
            },
            'validation': {
                'cross_validation': {'enabled': False, 'folds': 2, 'stratified': False, 'random_state': 42},
                'metrics': ['mse', 'mae'], 'validation_frequency': 1,
                'save_validation_results': False, 'detailed_metrics': False
            },
            'experimental': {
                'features': {'advanced_logging': True if fallback_level == 'emergency' else False, 
                            'model_interpretability': False, 'federated_learning': False, 'active_learning': False},
                'settings': {'experimental_mode': True if fallback_level == 'emergency' else False, 
                            'beta_features': False, 'research_mode': False}
            }
        })
        
        # Add runtime information
        base_config['runtime'] = {
            'config_loaded_at': current_time,
            'config_source': f'{fallback_level}_fallback',
            'process_id': os.getpid(),
            'configuration_health': {
                'status': f'{fallback_level}_fallback',
                'last_validation': current_time,
                'validation_errors': []
            }
        }
        
        # Log the fallback creation
        if fallback_level == 'emergency':
            logger.critical("Created emergency fallback configuration - system in critical state")
        elif fallback_level == 'standard':
            logger.warning("Created standard fallback configuration due to system failure")
            logger.info(f"Fallback config: {base_config['data']['features']} features, "
                       f"{base_config['training']['batch_size']} batch size, "
                       f"{base_config['model']['model_type']} model")
        else:
            logger.info(f"Created {fallback_level} fallback configuration")
        
        return base_config
        
    except Exception as e:
        logger.critical(f"Failed to create {fallback_level} fallback configuration: {e}")
        
        # Last resort emergency config - absolute minimal
        return {
            'metadata': {
                'version': '2.1', 'config_version': '2.1',
                'created': datetime.now().isoformat() if 'datetime' in globals() else 'unknown',
                'description': 'Emergency minimal configuration - last resort',
                'preset_used': 'emergency_last_resort',
                'config_source': 'last_resort_fallback',
                'compatibility': ['SimpleAutoencoder']
            },
            'training': {
                'batch_size': 8, 'epochs': 2, 'learning_rate': 0.01, 'num_workers': 1, 'optimizer': 'SGD'
            },
            'model': {
                'model_type': 'SimpleAutoencoder', 'encoding_dim': 2, 'hidden_dims': [16],
                'dropout_rates': [0.0], 'activation': 'relu'
            },
            'data': {
                'normal_samples': 50, 'attack_samples': 10, 'features': 4, 'validation_split': 0.4
            },
            'security': {
                'percentile': 90, 'attack_threshold': 0.5, 'enable_security_metrics': False
            },
            'system': {
                'model_dir': './models/emergency', 'debug': True, 'random_seed': 42
            },
            'presets': {
                'current_preset': 'emergency_last_resort', 'available_presets': []
            },
            'fallback_info': {
                'is_fallback': True, 'level': 'emergency_last_resort', 'critical_error': str(e)
            },
            'runtime': {
                'config_source': 'last_resort_fallback',
                'configuration_health': {'status': 'critical'}
            }
        }

def _recommend_preset_for_system(system_analysis: Dict[str, Any]) -> str:
    """Recommend optimal preset based on system analysis."""
    try:
        system_class = system_analysis.get('hardware_analysis', {}).get('system_class', 'unknown')
        performance_score = system_analysis.get('hardware_analysis', {}).get('performance_score', 0)
        gpu_available = system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('available', False)
        memory_gb = system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('memory', {}).get('total_gb', 0)
        
        if system_class == 'high_performance' and gpu_available and memory_gb >= 16:
            return 'advanced'
        elif system_class == 'high_performance' and gpu_available:
            return 'performance'
        elif system_class == 'standard':
            return 'default'
        elif memory_gb < 4 or performance_score < 40:
            return 'lightweight'
        else:
            return 'default'
            
    except Exception:
        return 'default'

def _check_preset_system_compatibility(config: Dict[str, Any], system_analysis: Dict[str, Any]) -> Dict[str, Any]:
    """Check compatibility between preset configuration and system capabilities."""
    try:
        compatibility = {
            'overall_compatible': True,
            'warnings': [],
            'recommendations': []
        }
        
        # Check memory requirements
        training_config = config.get('training', {})
        model_config = config.get('model', {})
        system_memory = system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('memory', {}).get('total_gb', 0)
        
        # Estimate memory requirements
        batch_size = training_config.get('batch_size', 64)
        model_complexity = len(model_config.get('hidden_dims', [])) + 1
        # Rough estimate in GB
        estimated_memory_need = (batch_size * model_complexity) / 1000
        
        if system_memory > 0 and estimated_memory_need > system_memory * 0.8:
            compatibility['overall_compatible'] = False
            compatibility['warnings'].append(f"High memory usage expected ({estimated_memory_need:.1f}GB) for available memory ({system_memory:.1f}GB)")
            compatibility['recommendations'].append("Consider reducing batch size or model complexity")
        
        # Check GPU requirements
        cuda_available = system_analysis.get('hardware_analysis', {}).get('capabilities', {}).get('gpu', {}).get('available', False)
        mixed_precision = training_config.get('mixed_precision', False)
        
        if mixed_precision and not cuda_available:
            compatibility['warnings'].append("Mixed precision training enabled but CUDA not available")
            compatibility['recommendations'].append("Disable mixed precision for CPU-only systems")
        
        # Check model type compatibility
        model_type = model_config.get('model_type', '')
        num_models = model_config.get('num_models', 1)
        
        if 'Ensemble' in model_type and num_models > 1 and system_analysis.get('hardware_analysis', {}).get('system_class') == 'limited':
            compatibility['warnings'].append("Ensemble model may be too complex for limited system resources")
            compatibility['recommendations'].append("Consider using SimpleAutoencoder for better compatibility")
        
        return compatibility
        
    except Exception as e:
        return {
            'overall_compatible': True,
            'warnings': [f"Compatibility check failed: {str(e)}"],
            'recommendations': []
        }

@enhanced_monitor_performance(include_memory=True, log_level=logging.DEBUG)
def get_system_info(include_versions: bool = True, include_hardware: bool = True, 
                   include_memory_usage: bool = True, include_detailed_analysis: bool = False,
                   include_performance_baseline: bool = False, include_memory_optimization: bool = False) -> Dict[str, Any]:
    """
    Gather comprehensive system information by fully leveraging existing check functions.
    Now enhanced with performance monitoring, baseline establishment, and memory management.
    All helper functions are integrated directly for optimal performance.
    
    Args:
        include_versions: Whether to include version information
        include_hardware: Whether to include hardware information  
        include_memory_usage: Whether to include current memory usage statistics
        include_detailed_analysis: Whether to include performance analysis and recommendations
        include_performance_baseline: Whether to establish performance baseline during collection
        include_memory_optimization: Whether to perform memory optimization during collection
        
    Returns:
        Dictionary with comprehensive system information, analysis, and recommendations
    """
    try:
        # Initialize comprehensive system info structure with performance tracking
        system_info = {
            'timestamp': datetime.now().isoformat(),
            'collection_metadata': {
                'include_versions': include_versions,
                'include_hardware': include_hardware,
                'include_memory_usage': include_memory_usage,
                'include_detailed_analysis': include_detailed_analysis,
                'include_performance_baseline': include_performance_baseline,
                'include_memory_optimization': include_memory_optimization,
                'collection_duration_ms': 0,
                'data_sources': [],
                'warnings': [],
                'errors': [],
                'performance_metrics': {},
                'memory_optimization_results': {}
            }
        }
        
        start_time = time.time()
        
        # Pre-collection memory state capture
        if include_memory_usage or include_memory_optimization:
            try:
                # Capture detailed memory state for monitoring system info collection impact
                initial_memory_state = {
                    'timestamp': datetime.now().isoformat()
                }
                
                # Process memory info
                if OPTIONAL_DEPENDENCIES.get('psutil', False):
                    proc = psutil.Process()
                    mem_info = proc.memory_info()
                    initial_memory_state.update({
                        'process_rss_mb': mem_info.rss / (1024**2),
                        'process_vms_mb': mem_info.vms / (1024**2),
                        'process_percent': proc.memory_percent()
                    })
                    
                    # System memory info
                    sys_mem = psutil.virtual_memory()
                    initial_memory_state.update({
                        'system_total_gb': sys_mem.total / (1024**3),
                        'system_available_gb': sys_mem.available / (1024**3),
                        'system_used_gb': sys_mem.used / (1024**3),
                        'system_percent': sys_mem.percent
                    })
                
                # GPU memory if available
                if torch.cuda.is_available():
                    try:
                        gpu_memory = {}
                        for i in range(torch.cuda.device_count()):
                            gpu_memory[f'gpu_{i}'] = {
                                'allocated_mb': torch.cuda.memory_allocated(i) / (1024**2),
                                'reserved_mb': torch.cuda.memory_reserved(i) / (1024**2)
                            }
                        initial_memory_state['gpu_memory'] = gpu_memory
                    except:
                        pass
                
                system_info['collection_metadata']['initial_memory_state'] = initial_memory_state
                logger.debug(f"Initial memory state: {initial_memory_state.get('process_rss_mb', 0):.1f}MB RSS")
            except Exception as e:
                system_info['collection_metadata']['warnings'].append(f"Failed to capture initial memory state: {e}")
        
        # Optional memory optimization before intensive operations
        if include_memory_optimization:
            try:
                logger.debug("Performing pre-collection memory optimization")
                optimization_results = enhanced_clear_memory(aggressive=True)
                system_info['collection_metadata']['memory_optimization_results']['pre_collection'] = optimization_results
                
                if optimization_results.get('success', False):
                    logger.debug(f"Pre-collection optimization: {', '.join(optimization_results.get('actions_taken', []))}")
                else:
                    logger.warning("Pre-collection memory optimization failed")
            except Exception as e:
                system_info['collection_metadata']['errors'].append(f"Pre-collection memory optimization failed: {e}")
        
        # Core platform information (always included) - monitored
        try:
            with _monitored_operation("platform_info_collection") as op_monitor:
                system_info['platform'] = {
                    'system': platform.system(),
                    'release': platform.release(), 
                    'version': platform.version(),
                    'platform': platform.platform(),
                    'architecture': platform.architecture(),
                    'machine': platform.machine(),
                    'processor': platform.processor(),
                    'node': platform.node(),
                    'boot_time': datetime.fromtimestamp(psutil.boot_time()).isoformat() if OPTIONAL_DEPENDENCIES.get('psutil', False) else None
                }
                system_info['collection_metadata']['data_sources'].append('platform')
                system_info['collection_metadata']['performance_metrics']['platform_collection'] = op_monitor.get_metrics()
        except Exception as e:
            system_info['platform_error'] = str(e)
            system_info['collection_metadata']['errors'].append(f"Platform info collection failed: {e}")
        
        # Python runtime information (always included) - monitored
        try:
            with _monitored_operation("python_info_collection") as op_monitor:
                system_info['python'] = {
                    'version': sys.version,
                    'version_info': {
                        'major': sys.version_info.major,
                        'minor': sys.version_info.minor,
                        'micro': sys.version_info.micro,
                        'releaselevel': sys.version_info.releaselevel,
                        'serial': sys.version_info.serial,
                        'version_tuple': tuple(sys.version_info[:3])
                    },
                    'executable': sys.executable,
                    'build': platform.python_build(),
                    'compiler': platform.python_compiler(),
                    'implementation': platform.python_implementation(),
                    'prefix': sys.prefix,
                    'path': sys.path[:5],  # First 5 paths to avoid clutter
                    'modules_count': len(sys.modules),
                    'encoding': {
                        'default': sys.getdefaultencoding(),
                        'filesystem': sys.getfilesystemencoding(),
                        'stdout': getattr(sys.stdout, 'encoding', 'unknown'),
                        'stderr': getattr(sys.stderr, 'encoding', 'unknown')
                    }
                }
                system_info['collection_metadata']['data_sources'].append('python')
                system_info['collection_metadata']['performance_metrics']['python_collection'] = op_monitor.get_metrics()
        except Exception as e:
            system_info['python_error'] = str(e)
            system_info['collection_metadata']['errors'].append(f"Python info collection failed: {e}")
        
        # Leverage check_versions() for comprehensive package analysis - monitored
        if include_versions:
            try:
                with _monitored_operation("version_analysis") as op_monitor:
                    version_data = check_versions(include_optional=True)
                    system_info['package_versions'] = version_data
                    system_info['collection_metadata']['data_sources'].append('check_versions')
                    
                    # Enhanced version analysis
                    version_analysis = {
                        'total_packages': len(version_data),
                        'available_packages': sum(1 for pkg in version_data.values() if pkg.get('available', False)),
                        'compatible_packages': sum(1 for pkg in version_data.values() if pkg.get('compatible', False)),
                        'required_packages': sum(1 for pkg in version_data.values() if pkg.get('required', False)),
                        'missing_required': [name for name, pkg in version_data.items() if pkg.get('required', False) and not pkg.get('available', False)],
                        'incompatible_packages': [name for name, pkg in version_data.items() if pkg.get('available', False) and not pkg.get('compatible', False)],
                        'optional_available': [name for name, pkg in version_data.items() if not pkg.get('required', False) and pkg.get('available', False)],
                        'status_summary': {
                            'OK': sum(1 for pkg in version_data.values() if pkg.get('status') == 'OK'),
                            'WARNING': sum(1 for pkg in version_data.values() if pkg.get('status') == 'WARNING'),
                            'MISSING': sum(1 for pkg in version_data.values() if pkg.get('status') == 'MISSING'),
                            'ERROR': sum(1 for pkg in version_data.values() if pkg.get('status') == 'ERROR')
                        }
                    }
                    
                    # Add environment health assessment
                    version_analysis['environment_health'] = {
                        'overall_status': 'healthy' if len(version_analysis['missing_required']) == 0 else 'degraded',
                        'compatibility_score': (version_analysis['compatible_packages'] / max(version_analysis['available_packages'], 1)) * 100,
                        'completeness_score': (version_analysis['available_packages'] / version_analysis['total_packages']) * 100,
                        'critical_issues': len(version_analysis['missing_required']),
                        'warnings': len(version_analysis['incompatible_packages'])
                    }
                    
                    system_info['package_analysis'] = version_analysis
                    
                    # Store performance metrics
                    version_metrics = op_monitor.get_metrics()
                    version_metrics['packages_analyzed'] = len(version_data)
                    version_metrics['analysis_efficiency'] = len(version_data) / max(version_metrics.get('duration', 0.001), 0.001)
                    system_info['collection_metadata']['performance_metrics']['version_analysis'] = version_metrics
                    
                    # Add warnings for missing critical packages
                    if version_analysis['missing_required']:
                        system_info['collection_metadata']['warnings'].append(
                            f"Missing required packages: {', '.join(version_analysis['missing_required'])}"
                        )
                        
            except Exception as e:
                system_info['package_versions_error'] = str(e)
                system_info['collection_metadata']['errors'].append(f"Package version collection failed: {e}")
        
        # Leverage check_hardware() for comprehensive hardware analysis - monitored
        if include_hardware:
            try:
                with _monitored_operation("hardware_analysis") as op_monitor:
                    hardware_data = check_hardware(min_disk_gb=1.0, include_memory_usage=include_memory_usage)
                    system_info['hardware'] = hardware_data
                    system_info['collection_metadata']['data_sources'].append('check_hardware')
                    
                    # Enhanced hardware analysis
                    hardware_analysis = {
                        'components_detected': len(hardware_data),
                        'components_healthy': sum(1 for comp in hardware_data.values() if comp.get('status') == 'PASS'),
                        'components_warning': sum(1 for comp in hardware_data.values() if comp.get('status') == 'WARN'),
                        'components_failed': sum(1 for comp in hardware_data.values() if comp.get('status') == 'FAIL'),
                        'required_failures': sum(1 for comp in hardware_data.values() if comp.get('status') == 'FAIL' and comp.get('required', False))
                    }
                    
                    # Extract key hardware capabilities with integrated classification
                    capabilities = {}
                    
                    # CPU capabilities
                    if 'cpu_cores' in hardware_data and hardware_data['cpu_cores'].get('available'):
                        cpu_data = hardware_data['cpu_cores']
                        cores = cpu_data.get('logical_cores', 0)
                        frequency_ghz = cpu_data.get('capacity', {}).get('frequency_ghz', 0.0)
                        
                        # Integrated CPU performance classification
                        if cores >= 16 and frequency_ghz >= 3.0:
                            cpu_perf_class = 'high'
                        elif cores >= 8 and frequency_ghz >= 2.5:
                            cpu_perf_class = 'medium-high'  
                        elif cores >= 4 and frequency_ghz >= 2.0:
                            cpu_perf_class = 'medium'
                        elif cores >= 2:
                            cpu_perf_class = 'low-medium'
                        else:
                            cpu_perf_class = 'low'
                        
                        capabilities['cpu'] = {
                            'logical_cores': cores,
                            'physical_cores': cpu_data.get('physical_cores', 0),
                            'hyperthreading': cpu_data.get('hyperthreading', False),
                            'performance_class': cpu_perf_class,
                            'frequency_ghz': frequency_ghz,
                            'max_frequency_ghz': cpu_data.get('capacity', {}).get('max_frequency_ghz')
                        }
                        
                        if 'current_usage' in cpu_data:
                            capabilities['cpu']['current_load'] = cpu_data['current_usage'].get('cpu_percent_total', 0)
                            capabilities['cpu']['per_core_load'] = cpu_data['current_usage'].get('cpu_percent_per_core', [])
                    
                    # Memory capabilities
                    if 'system_ram' in hardware_data and hardware_data['system_ram'].get('available'):
                        ram_data = hardware_data['system_ram']
                        total_gb = ram_data.get('ram_total_gb', 0)
                        
                        # Integrated memory performance classification
                        if total_gb >= 32:
                            memory_perf_class = 'high'
                        elif total_gb >= 16:
                            memory_perf_class = 'medium-high'
                        elif total_gb >= 8:
                            memory_perf_class = 'medium'
                        elif total_gb >= 4:
                            memory_perf_class = 'low-medium'
                        else:
                            memory_perf_class = 'low'
                        
                        capabilities['memory'] = {
                            'total_gb': total_gb,
                            'performance_class': memory_perf_class,
                            'swap_gb': ram_data.get('swap_total_gb', 0),
                            'has_swap': ram_data.get('swap_total_gb', 0) > 0
                        }
                        
                        if 'current_usage' in ram_data:
                            capabilities['memory'].update({
                                'available_gb': ram_data['current_usage'].get('available_gb', 0),
                                'used_gb': ram_data['current_usage'].get('used_gb', 0),
                                'usage_percent': ram_data['current_usage'].get('percent_used', 0),
                                'swap_used_gb': ram_data['current_usage'].get('swap_used_gb', 0),
                                'swap_usage_percent': ram_data['current_usage'].get('swap_percent', 0)
                            })
                    
                    # GPU capabilities
                    if 'cuda' in hardware_data:
                        cuda_data = hardware_data['cuda']
                        cuda_available = cuda_data.get('available', False)
                        gpus = cuda_data.get('gpus', [])
                        
                        # Integrated GPU performance classification
                        if not cuda_available or not gpus:
                            gpu_perf_class = 'none'
                        else:
                            # Use the best GPU for classification
                            best_memory = max(gpu.get('memory_gb', 0) for gpu in gpus)
                            gpu_count = len(gpus)
                            
                            if best_memory >= 16 and gpu_count >= 2:
                                gpu_perf_class = 'high'
                            elif best_memory >= 12 or (best_memory >= 8 and gpu_count >= 2):
                                gpu_perf_class = 'medium-high'
                            elif best_memory >= 6:
                                gpu_perf_class = 'medium'
                            elif best_memory >= 4:
                                gpu_perf_class = 'low-medium'
                            else:
                                gpu_perf_class = 'low'
                        
                        capabilities['gpu'] = {
                            'available': cuda_available,
                            'count': cuda_data.get('gpu_count', 0),
                            'cuda_version': cuda_data.get('cuda_version'),
                            'cudnn_version': cuda_data.get('cudnn_version'),
                            'performance_class': gpu_perf_class,
                            'total_memory_gb': sum(gpu.get('memory_gb', 0) for gpu in gpus)
                        }
                        
                        if cuda_available and gpus:
                            capabilities['gpu']['devices'] = []
                            for i, gpu in enumerate(gpus):
                                gpu_info = {
                                    'index': i,
                                    'name': gpu.get('name', 'Unknown'),
                                    'memory_gb': gpu.get('memory_gb', 0),
                                    'compute_capability': gpu.get('compute_capability'),
                                    'multiprocessors': gpu.get('multiprocessors', 0)
                                }
                                
                                if 'current_usage' in gpu:
                                    gpu_usage = gpu['current_usage']
                                    gpu_info.update({
                                        'allocated_mb': gpu_usage.get('allocated_mb', 0),
                                        'reserved_mb': gpu_usage.get('reserved_mb', 0),
                                        'utilization_percent': gpu_usage.get('percent_allocated', 0)
                                    })
                                
                                capabilities['gpu']['devices'].append(gpu_info)
                    
                    # Storage capabilities
                    if 'disk_space' in hardware_data and hardware_data['disk_space'].get('available'):
                        disk_data = hardware_data['disk_space']
                        free_gb = disk_data.get('free_gb', 0)
                        total_gb = disk_data.get('total_gb', 0)
                        usage_percent = (disk_data.get('used_gb', 0) / max(total_gb, 1)) * 100
                        
                        # Integrated storage performance classification
                        if free_gb >= 100 and usage_percent < 70:
                            storage_perf_class = 'high'
                        elif free_gb >= 50 and usage_percent < 80:
                            storage_perf_class = 'medium'
                        elif free_gb >= 20 and usage_percent < 90:
                            storage_perf_class = 'low-medium'
                        else:
                            storage_perf_class = 'low'
                        
                        capabilities['storage'] = {
                            'free_gb': free_gb,
                            'total_gb': total_gb,
                            'used_gb': disk_data.get('used_gb', 0),
                            'usage_percent': usage_percent,
                            'performance_class': storage_perf_class
                        }
                    
                    # hardware performance score calculation
                    scores = []
                    
                    # CPU score
                    cpu_class = capabilities.get('cpu', {}).get('performance_class', 'low')
                    cpu_scores = {'high': 100, 'medium-high': 80, 'medium': 60, 'low-medium': 40, 'low': 20}
                    scores.append(cpu_scores.get(cpu_class, 20))
                    
                    # Memory score  
                    memory_class = capabilities.get('memory', {}).get('performance_class', 'low')
                    memory_scores = {'high': 100, 'medium-high': 80, 'medium': 60, 'low-medium': 40, 'low': 20}
                    scores.append(memory_scores.get(memory_class, 20))
                    
                    # GPU score
                    gpu_class = capabilities.get('gpu', {}).get('performance_class', 'none')
                    gpu_scores = {'high': 100, 'medium-high': 80, 'medium': 60, 'low-medium': 40, 'low': 20, 'none': 0}
                    scores.append(gpu_scores.get(gpu_class, 0))
                    
                    # Storage score
                    storage_class = capabilities.get('storage', {}).get('performance_class', 'low')
                    storage_scores = {'high': 100, 'medium': 80, 'low-medium': 60, 'low': 40}
                    scores.append(storage_scores.get(storage_class, 40))
                    
                    performance_score = int(sum(scores) / len(scores))
                    
                    # system performance classification
                    if performance_score >= 80:
                        system_class = 'high_performance'
                    elif performance_score >= 60:
                        system_class = 'standard'
                    else:
                        system_class = 'limited'
                    
                    hardware_analysis.update({
                        'capabilities': capabilities,
                        'overall_health': 'healthy' if hardware_analysis['required_failures'] == 0 else 'degraded',
                        'performance_score': performance_score,
                        'system_class': system_class
                    })
                    
                    system_info['hardware_analysis'] = hardware_analysis
                    
                    # Store performance metrics
                    hardware_metrics = op_monitor.get_metrics()
                    hardware_metrics['components_analyzed'] = len(hardware_data)
                    hardware_metrics['analysis_efficiency'] = len(hardware_data) / max(hardware_metrics.get('duration', 0.001), 0.001)
                    system_info['collection_metadata']['performance_metrics']['hardware_analysis'] = hardware_metrics
                    
                    # Add warnings for hardware issues
                    if hardware_analysis['required_failures'] > 0:
                        failed_components = [name for name, comp in hardware_data.items() 
                                           if comp.get('status') == 'FAIL' and comp.get('required', False)]
                        system_info['collection_metadata']['warnings'].append(
                            f"Critical hardware failures: {', '.join(failed_components)}"
                        )
                        
            except Exception as e:
                system_info['hardware_error'] = str(e)
                system_info['collection_metadata']['errors'].append(f"Hardware collection failed: {e}")
        
        # Enhanced performance baseline establishment
        if include_performance_baseline:
            try:
                logger.debug("Establishing comprehensive performance baseline")
                with _monitored_operation("performance_baseline_establishment") as op_monitor:
                    # Use hardware data from previous step if available
                    baseline_hardware_data = system_info.get('hardware') if include_hardware else None
                    
                    baseline_results = establish_performance_baseline(hardware_data=baseline_hardware_data)
                    system_info['performance_baseline'] = baseline_results
                    system_info['collection_metadata']['data_sources'].append('performance_baseline')
                    
                    # Store baseline establishment metrics
                    baseline_metrics = op_monitor.get_metrics()
                    baseline_metrics['baseline_tests_completed'] = len(baseline_results.get('baselines', {}))
                    baseline_metrics['baseline_success_rate'] = len([b for b in baseline_results.get('baselines', {}).values() if not isinstance(b, str) or not b.endswith('_error')]) / max(len(baseline_results.get('baselines', {})), 1)
                    system_info['collection_metadata']['performance_metrics']['baseline_establishment'] = baseline_metrics
                    
                    # Integrate baseline results with hardware analysis
                    if 'hardware_analysis' in system_info and 'summary' in baseline_results:
                        system_info['hardware_analysis']['performance_baseline'] = baseline_results['summary']
                        
                    logger.debug(f"Performance baseline established: {baseline_results.get('summary', {}).get('overall_capability', 'unknown')} system")
                    
            except Exception as e:
                system_info['performance_baseline_error'] = str(e)
                system_info['collection_metadata']['errors'].append(f"Performance baseline establishment failed: {e}")
        
        # Detailed analysis with performance context
        if include_detailed_analysis:
            try:
                with _monitored_operation("detailed_analysis") as op_monitor:
                    analysis = {
                        'system_recommendations': [],
                        'performance_optimizations': [],
                        'compatibility_issues': [],
                        'resource_warnings': [],
                        'configuration_suggestions': []
                    }
                    
                    # Analyze system for recommendations
                    if 'hardware_analysis' in system_info:
                        hw_analysis = system_info['hardware_analysis']
                        capabilities = hw_analysis.get('capabilities', {})
                        
                        # CPU recommendations
                        if 'cpu' in capabilities:
                            cpu = capabilities['cpu']
                            if cpu.get('logical_cores', 0) < 4:
                                analysis['system_recommendations'].append(
                                    "Consider upgrading to a system with at least 4 CPU cores for optimal performance"
                                )
                            if cpu.get('performance_class') == 'low':
                                analysis['performance_optimizations'].append(
                                    "CPU performance is limited - consider enabling CPU-optimized algorithms"
                                )
                        
                        # Memory recommendations  
                        if 'memory' in capabilities:
                            memory = capabilities['memory']
                            if memory.get('total_gb', 0) < 8:
                                analysis['system_recommendations'].append(
                                    "Consider upgrading to at least 8GB RAM for better performance"
                                )
                            if memory.get('usage_percent', 0) > 80:
                                analysis['resource_warnings'].append(
                                    f"High memory usage detected ({memory.get('usage_percent', 0):.1f}%) - consider closing unnecessary applications"
                                )
                        
                        # GPU recommendations
                        if 'gpu' in capabilities:
                            gpu = capabilities['gpu']
                            if not gpu.get('available'):
                                analysis['performance_optimizations'].append(
                                    "CUDA not available - training will use CPU (slower performance expected)"
                                )
                            elif gpu.get('total_memory_gb', 0) < 6:
                                analysis['configuration_suggestions'].append(
                                    "Limited GPU memory detected - consider using smaller batch sizes"
                                )
                    
                    # Performance baseline integration
                    if include_performance_baseline and 'performance_baseline' in system_info:
                        baseline = system_info['performance_baseline']
                        
                        # CPU performance analysis
                        if 'cpu' in baseline.get('baselines', {}):
                            cpu_baseline = baseline['baselines']['cpu']
                            gflops = cpu_baseline.get('gflops', 0)
                            
                            if gflops < 1:
                                analysis['performance_optimizations'].append(
                                    f"CPU performance is very low ({gflops:.2f} GFLOPS) - consider optimized algorithms"
                                )
                            elif gflops > 10:
                                analysis['configuration_suggestions'].append(
                                    f"Excellent CPU performance ({gflops:.1f} GFLOPS) - can handle complex computations"
                                )
                        
                        # Memory performance analysis
                        if 'memory' in baseline.get('baselines', {}):
                            memory_baseline = baseline['baselines']['memory']
                            avg_speed = np.mean([
                                baseline_item.get('allocation_speed_mbs', 0) 
                                for baseline_item in memory_baseline.values() 
                                if isinstance(baseline_item, dict) and 'allocation_speed_mbs' in baseline_item
                            ])
                            
                            if avg_speed < 50:
                                analysis['performance_optimizations'].append(
                                    f"Memory allocation is slow ({avg_speed:.0f} MB/s) - consider memory optimization"
                                )
                            elif avg_speed > 500:
                                analysis['configuration_suggestions'].append(
                                    f"Fast memory allocation ({avg_speed:.0f} MB/s) - can handle large datasets"
                                )
                        
                        # GPU performance analysis
                        if 'gpu' in baseline.get('baselines', {}):
                            gpu_baseline = baseline['baselines']['gpu']
                            max_gpu_gflops = max([
                                gpu.get('gflops', 0) 
                                for gpu in gpu_baseline.values() 
                                if isinstance(gpu, dict) and 'gflops' in gpu
                            ], default=0)
                            
                            if max_gpu_gflops > 100:
                                analysis['configuration_suggestions'].append(
                                    f"High-performance GPU available ({max_gpu_gflops:.0f} GFLOPS) - enable mixed precision and large batch sizes"
                                )
                            elif max_gpu_gflops > 0 and max_gpu_gflops < 50:
                                analysis['performance_optimizations'].append(
                                    f"Limited GPU performance ({max_gpu_gflops:.0f} GFLOPS) - use smaller models and batch sizes"
                                )
                    
                    # Package compatibility analysis
                    if 'package_analysis' in system_info:
                        pkg_analysis = system_info['package_analysis']
                        
                        if pkg_analysis.get('missing_required'):
                            analysis['compatibility_issues'].extend([
                                f"Missing required package: {pkg}" for pkg in pkg_analysis['missing_required']
                            ])
                        
                        if pkg_analysis.get('incompatible_packages'):
                            analysis['compatibility_issues'].extend([
                                f"Incompatible package version: {pkg}" for pkg in pkg_analysis['incompatible_packages']
                            ])
                        
                        if pkg_analysis['environment_health']['compatibility_score'] < 90:
                            analysis['system_recommendations'].append(
                                "Package environment needs attention - some dependencies may be outdated"
                            )
                    
                    # System class based recommendations
                    if 'hardware_analysis' in system_info:
                        system_class = system_info['hardware_analysis'].get('system_class', 'unknown')
                        
                        if system_class == 'high_performance':
                            analysis['configuration_suggestions'].extend([
                                "System is high-performance - consider enabling advanced features",
                                "Large batch sizes and complex models recommended",
                                "Enable mixed precision training for optimal GPU utilization"
                            ])
                        elif system_class == 'standard':
                            analysis['configuration_suggestions'].extend([
                                "Standard system configuration detected",
                                "Use moderate batch sizes and model complexity",
                                "Consider gradient accumulation if memory limited"
                            ])
                        elif system_class == 'limited':
                            analysis['configuration_suggestions'].extend([
                                "Limited system resources detected",
                                "Use small batch sizes and simple models",
                                "Enable aggressive memory management",
                                "Consider using checkpointing to save memory"
                            ])
                    
                    system_info['detailed_analysis'] = analysis
                    system_info['collection_metadata']['data_sources'].append('detailed_analysis')
                    
                    # Store analysis performance metrics
                    analysis_metrics = op_monitor.get_metrics()
                    analysis_metrics['recommendations_generated'] = sum(len(recommendations) for recommendations in analysis.values() if isinstance(recommendations, list))
                    analysis_metrics['analysis_efficiency'] = analysis_metrics['recommendations_generated'] / max(analysis_metrics.get('duration', 0.001), 0.001)
                    system_info['collection_metadata']['performance_metrics']['detailed_analysis'] = analysis_metrics
                    
            except Exception as e:
                system_info['detailed_analysis_error'] = str(e)
                system_info['collection_metadata']['errors'].append(f"Detailed analysis failed: {e}")
        
        # Runtime process information - monitored
        try:
            with _monitored_operation("runtime_info_collection") as op_monitor:
                if OPTIONAL_DEPENDENCIES.get('psutil', False):
                    process = psutil.Process()
                    system_info['runtime'] = {
                        'process_id': os.getpid(),
                        'parent_process_id': os.getppid(),
                        'process_name': process.name(),
                        'process_status': process.status(),
                        'process_create_time': datetime.fromtimestamp(process.create_time()).isoformat(),
                        'working_directory': os.getcwd(),
                        'command_line': ' '.join(sys.argv),
                        'environment_variables': {
                            'PATH': os.environ.get('PATH', ''),
                            'PYTHONPATH': os.environ.get('PYTHONPATH', ''),
                            'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', ''),
                            'OMP_NUM_THREADS': os.environ.get('OMP_NUM_THREADS', ''),
                            'NUMBA_NUM_THREADS': os.environ.get('NUMBA_NUM_THREADS', '')
                        }
                    }
                    
                    if include_memory_usage:
                        memory_info = process.memory_info()
                        system_info['runtime']['memory'] = {
                            'rss_mb': memory_info.rss / (1024**2),
                            'vms_mb': memory_info.vms / (1024**2),
                            'percent': process.memory_percent(),
                            'open_files': len(process.open_files()),
                            'num_threads': process.num_threads()
                        }
                    
                    system_info['collection_metadata']['data_sources'].append('runtime')
                    system_info['collection_metadata']['performance_metrics']['runtime_collection'] = op_monitor.get_metrics()
        except Exception as e:
            system_info['runtime_error'] = str(e)
            system_info['collection_metadata']['errors'].append(f"Runtime info collection failed: {e}")
        
        # Post-collection memory state and optimization
        if include_memory_usage or include_memory_optimization:
            try:
                # Capture detailed memory state for monitoring system info collection impact
                final_memory_state = {
                    'timestamp': datetime.now().isoformat()
                }
                
                # Process memory info
                if OPTIONAL_DEPENDENCIES.get('psutil', False):
                    proc = psutil.Process()
                    mem_info = proc.memory_info()
                    final_memory_state.update({
                        'process_rss_mb': mem_info.rss / (1024**2),
                        'process_vms_mb': mem_info.vms / (1024**2),
                        'process_percent': proc.memory_percent()
                    })
                    
                    # System memory info
                    sys_mem = psutil.virtual_memory()
                    final_memory_state.update({
                        'system_total_gb': sys_mem.total / (1024**3),
                        'system_available_gb': sys_mem.available / (1024**3),
                        'system_used_gb': sys_mem.used / (1024**3),
                        'system_percent': sys_mem.percent
                    })
                
                # GPU memory if available
                if torch.cuda.is_available():
                    try:
                        gpu_memory = {}
                        for i in range(torch.cuda.device_count()):
                            gpu_memory[f'gpu_{i}'] = {
                                'allocated_mb': torch.cuda.memory_allocated(i) / (1024**2),
                                'reserved_mb': torch.cuda.memory_reserved(i) / (1024**2)
                            }
                        final_memory_state['gpu_memory'] = gpu_memory
                    except:
                        pass
                
                system_info['collection_metadata']['final_memory_state'] = final_memory_state
                
                # Calculate memory impact of system info collection
                if 'initial_memory_state' in system_info['collection_metadata']:
                    initial = system_info['collection_metadata']['initial_memory_state']
                    memory_impact = {
                        'rss_delta_mb': final_memory_state.get('process_rss_mb', 0) - initial.get('process_rss_mb', 0),
                        'vms_delta_mb': final_memory_state.get('process_vms_mb', 0) - initial.get('process_vms_mb', 0),
                        'system_delta_gb': final_memory_state.get('system_used_gb', 0) - initial.get('system_used_gb', 0)
                    }
                    system_info['collection_metadata']['memory_impact'] = memory_impact
                    
                    logger.debug(f"System info collection memory impact: {memory_impact['rss_delta_mb']:+.1f}MB RSS")
                
            except Exception as e:
                system_info['collection_metadata']['warnings'].append(f"Failed to capture final memory state: {e}")
        
        # Optional post-collection memory optimization
        if include_memory_optimization:
            try:
                logger.debug("Performing post-collection memory optimization")
                post_optimization_results = enhanced_clear_memory(
                    aggressive=False,  # Less aggressive after collection
                    hardware_data=system_info.get('hardware')
                )
                system_info['collection_metadata']['memory_optimization_results']['post_collection'] = post_optimization_results
                
                if post_optimization_results.get('success', False):
                    logger.debug(f"Post-collection optimization: {', '.join(post_optimization_results.get('actions_taken', []))}")
                    
                    # Calculate optimization effectiveness/impact of memory optimization
                    optimization_impact = {
                        'timestamp': datetime.now().isoformat(),
                        'optimization_effective': False,
                        'improvements': {}
                    }
                    
                    try:
                        # System RAM impact
                        if 'system_ram' in post_optimization_results.get('memory_before', {}) and 'system_ram' in post_optimization_results.get('memory_after', {}):
                            before_ram = post_optimization_results['memory_before']['system_ram']
                            after_ram = post_optimization_results['memory_after']['system_ram']
                            
                            ram_freed = before_ram.get('used_gb', 0) - after_ram.get('used_gb', 0)
                            if ram_freed > 0.1:  # More than 100MB freed
                                optimization_impact['optimization_effective'] = True
                                optimization_impact['improvements']['system_ram_freed_gb'] = ram_freed
                        
                        # GPU memory impact
                        if 'gpu_memory' in post_optimization_results.get('memory_before', {}) and 'gpu_memory' in post_optimization_results.get('memory_after', {}):
                            gpu_improvements = {}
                            
                            for gpu_id in post_optimization_results['memory_before']['gpu_memory']:
                                if gpu_id in post_optimization_results['memory_after']['gpu_memory']:
                                    before_gpu = post_optimization_results['memory_before']['gpu_memory'][gpu_id]
                                    after_gpu = post_optimization_results['memory_after']['gpu_memory'][gpu_id]
                                    
                                    allocated_freed = before_gpu.get('allocated_mb', 0) - after_gpu.get('allocated_mb', 0)
                                    if allocated_freed > 10:  # More than 10MB freed
                                        gpu_improvements[gpu_id] = {
                                            'allocated_freed_mb': allocated_freed,
                                            'reserved_freed_mb': before_gpu.get('reserved_mb', 0) - after_gpu.get('reserved_mb', 0)
                                        }
                                        optimization_impact['optimization_effective'] = True
                            
                            if gpu_improvements:
                                optimization_impact['improvements']['gpu_memory'] = gpu_improvements
                        
                    except Exception as e:
                        optimization_impact['error'] = str(e)
                    
                    system_info['collection_metadata']['optimization_impact'] = optimization_impact
                        
            except Exception as e:
                system_info['collection_metadata']['errors'].append(f"Post-collection memory optimization failed: {e}")
        
        # Add optional dependencies status
        system_info['optional_dependencies'] = {
            'available_features': OPTIONAL_DEPENDENCIES,
            'feature_count': len(OPTIONAL_DEPENDENCIES),
            'enabled_features': sum(1 for enabled in OPTIONAL_DEPENDENCIES.values() if enabled),
            'disabled_features': sum(1 for enabled in OPTIONAL_DEPENDENCIES.values() if not enabled),
            'feature_availability_score': (sum(1 for enabled in OPTIONAL_DEPENDENCIES.values() if enabled) / max(len(OPTIONAL_DEPENDENCIES), 1)) * 100
        }
        
        # Finalize collection metadata with comprehensive performance analysis
        end_time = time.time()
        total_duration = end_time - start_time
        system_info['collection_metadata']['collection_duration_ms'] = round(total_duration * 1000, 2)
        system_info['collection_metadata']['success'] = len(system_info['collection_metadata']['errors']) == 0
        
        # Assess the quality of collected system information
        errors = len(system_info['collection_metadata']['errors'])
        warnings = len(system_info['collection_metadata']['warnings'])
        data_sources = len(system_info['collection_metadata']['data_sources'])
        
        if errors == 0 and warnings == 0:
            data_quality = 'excellent'
        elif errors == 0 and warnings <= 2:
            data_quality = 'good' 
        elif errors == 0:
            data_quality = 'acceptable'
        elif errors <= 2 and data_sources >= 3:
            data_quality = 'degraded'
        else:
            data_quality = 'poor'
        
        system_info['collection_metadata']['data_quality'] = data_quality
        
        # Add performance summary
        performance_metrics = system_info['collection_metadata'].get('performance_metrics', {})
        summary = {
            'total_duration': total_duration,
            'operations_completed': len(performance_metrics),
            'average_operation_duration': 0,
            'slowest_operation': None,
            'fastest_operation': None,
            'total_memory_impact_mb': 0,
            'efficiency_score': 0
        }
        
        try:
            if performance_metrics:
                durations = []
                memory_deltas = []
                
                for op_name, metrics in performance_metrics.items():
                    duration = metrics.get('duration', 0)
                    durations.append(duration)
                    
                    if 'memory_delta' in metrics:
                        memory_deltas.append(metrics['memory_delta'].get('rss_mb', 0))
                    
                    # Track slowest and fastest operations
                    if summary['slowest_operation'] is None or duration > performance_metrics[summary['slowest_operation']].get('duration', 0):
                        summary['slowest_operation'] = op_name
                    
                    if summary['fastest_operation'] is None or duration < performance_metrics[summary['fastest_operation']].get('duration', 0):
                        summary['fastest_operation'] = op_name
                
                if durations:
                    summary['average_operation_duration'] = sum(durations) / len(durations)
                
                if memory_deltas:
                    summary['total_memory_impact_mb'] = sum(memory_deltas)
                
                # Calculate efficiency score (operations per second, weighted by memory efficiency)
                if total_duration > 0:
                    ops_per_second = len(performance_metrics) / total_duration
                    memory_efficiency = max(0, 1 - abs(summary['total_memory_impact_mb']) / 100)  # Penalty for high memory usage
                    base_efficiency = min(ops_per_second * 10, 100)
                    weighted_efficiency = base_efficiency * memory_efficiency
                    summary['efficiency_score'] = round(max(0, min(weighted_efficiency, 100)), 2)
            
        except Exception as e:
            summary['error'] = str(e)
        
        system_info['collection_metadata']['performance_summary'] = summary
        
        return system_info
        
    except Exception as e:
        # Fallback error response with performance context
        error_response = {
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
            'collection_metadata': {
                'success': False,
                'data_quality': 'failed',
                'errors': [f"Critical system info collection failure: {e}"]
            },
            'platform': None,
            'python': None,
            'partial_data': True
        }
        
        # Try to preserve any performance metrics that were collected
        try:
            if 'performance_metrics' in locals():
                error_response['collection_metadata']['performance_metrics'] = locals()['performance_metrics']
        except:
            pass
            
        return error_response

# Helper functions for enhanced system info collection
@contextmanager
def _monitored_operation(operation_name: str):
    """Context manager for monitoring individual operations within get_system_info()."""
    class OperationMonitor:
        def __init__(self, name):
            self.name = name
            self.start_time = time.time()
            self.start_memory = None
            self.metrics = {}
            
            # Capture initial memory state if psutil available
            try:
                proc = psutil.Process()
                self.start_memory = {
                    'rss_mb': proc.memory_info().rss / (1024**2),
                    'vms_mb': proc.memory_info().vms / (1024**2)
                }
            except:
                pass
        
        def get_metrics(self):
            end_time = time.time()
            duration = end_time - self.start_time
            
            self.metrics = {
                'operation': self.name,
                'duration': duration,
                'timestamp': datetime.now().isoformat()
            }
            
            # Add memory metrics if available
            if self.start_memory:
                try:
                    proc = psutil.Process()
                    end_memory = {
                        'rss_mb': proc.memory_info().rss / (1024**2),
                        'vms_mb': proc.memory_info().vms / (1024**2)
                    }
                    self.metrics['memory_delta'] = {
                        'rss_mb': end_memory['rss_mb'] - self.start_memory['rss_mb'],
                        'vms_mb': end_memory['vms_mb'] - self.start_memory['vms_mb']
                    }
                except:
                    pass
            
            return self.metrics
    
    monitor = OperationMonitor(operation_name)
    try:
        yield monitor
    finally:
        # Ensure metrics are available even if operation fails
        monitor.get_metrics()



def validate_config(config: Dict[str, Any], strict: bool = False) -> Tuple[bool, List[str], List[str]]:
    """
    Comprehensive configuration validation with enhanced preset compatibility,
    hardware requirement validation, automatic error correction capabilities,
    and intelligent memory management for optimal performance.
    
    This function provides deep validation of all configuration sections including
    the new preset features, enhanced model configurations, and system-aware validation.
    
    Args:
        config: Configuration dictionary to validate
        strict: If True, apply stricter validation rules and fail on warnings
        
    Returns:
        Tuple containing:
        - bool: True if configuration is valid (or auto-corrected)
        - List[str]: List of validation errors found
        - List[str]: List of validation warnings and recommendations
        
    Raises:
        ValueError: Only if configuration is fundamentally invalid and cannot be auto-corrected
    """
    errors = []
    warnings = []
    auto_fixes = []
    
    try:
        # Input validation
        if not isinstance(config, dict):
            raise ValueError("Configuration must be a dictionary")
        
        if not config:
            raise ValueError("Configuration cannot be empty")
        
        # INITIAL MEMORY OPTIMIZATION - Get hardware context early
        hardware_data = None
        total_ram_gb = 8.0  # Conservative default
        cuda_available = False
        
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
            cuda_available = hardware_data.get('cuda', {}).get('available', False)
        except Exception as e:
            logger.debug(f"Hardware detection failed during validation: {e}")
        
        # Track validation context
        validation_context = {
            'timestamp': datetime.now().isoformat(),
            'strict_mode': strict,
            'config_size': len(str(config)),
            'sections_present': list(config.keys()),
            'auto_fixes_applied': 0,
            'hardware_context': {
                'total_ram_gb': total_ram_gb,
                'cuda_available': cuda_available
            }
        }
        
        # MEMORY OPTIMIZATION - Clear memory before intensive validation for large configs
        config_size_mb = len(str(config)) / (1024 * 1024)
        if config_size_mb > 1.0 or total_ram_gb < 8:
            try:
                pre_validation_clear = enhanced_clear_memory(
                    aggressive=config_size_mb > 5.0 or total_ram_gb < 4,
                    hardware_data=hardware_data
                )
                if pre_validation_clear.get('success'):
                    logger.debug(f"Memory optimized before validation of {config_size_mb:.1f}MB config")
            except Exception as e:
                logger.debug(f"Pre-validation memory optimization failed: {e}")
        
        # 1. STRUCTURAL VALIDATION
        logger.debug("Starting structural validation")
        
        # Check for required top-level sections
        required_sections = ['training', 'model', 'security', 'data']
        missing_sections = [section for section in required_sections if section not in config]
        
        if missing_sections:
            errors.append(f"Missing required configuration sections: {missing_sections}")
            if strict:
                return False, errors, warnings
        
        # Check for optional but recommended sections
        recommended_sections = ['metadata', 'hardware', 'monitoring', 'system', 'presets']
        missing_recommended = [section for section in recommended_sections if section not in config]
        
        if missing_recommended:
            warnings.append(f"Missing recommended sections: {missing_recommended}")
        
        # 2. METADATA VALIDATION WITH ENHANCED CHECKS
        logger.debug("Validating metadata section")
        
        if 'metadata' in config:
            metadata = config['metadata']
            if not isinstance(metadata, dict):
                errors.append("Metadata section must be a dictionary")
            else:
                # Version compatibility validation
                config_version = metadata.get('config_version', '1.0')
                if config_version not in ['2.1', '2.0', '1.9', '1.8']:
                    warnings.append(f"Unknown config version '{config_version}', expecting '2.1'")
                
                # Schema validation
                if 'validation' in metadata:
                    validation_info = metadata['validation']
                    schema_version = validation_info.get('schema_version', '1.0')
                    if schema_version != '2.1':
                        warnings.append(f"Schema version mismatch: {schema_version} != 2.1")
                    
                    # Check required sections against metadata
                    metadata_required = validation_info.get('required_sections', [])
                    if metadata_required != required_sections:
                        warnings.append("Metadata required_sections don't match validation requirements")
                
                # System information validation
                if 'system' in metadata:
                    system_info = metadata['system']
                    
                    # Critical system fields
                    critical_fields = ['python_version', 'pytorch_version', 'os']
                    missing_system = [field for field in critical_fields if field not in system_info]
                    if missing_system:
                        warnings.append(f"Missing system information: {missing_system}")
                    
                    # CUDA consistency check
                    cuda_available_meta = system_info.get('cuda_available', False)
                    cuda_devices = system_info.get('cuda_devices', 0)
                    if cuda_available_meta and cuda_devices == 0:
                        warnings.append("CUDA reported as available but no devices found")
                    elif not cuda_available_meta and cuda_devices > 0:
                        warnings.append("CUDA devices reported but CUDA not available")
                
                # Hardware requirements validation
                if 'recommended_hardware' in metadata:
                    hw_req = metadata['recommended_hardware']
                    
                    # Validate reasonable hardware requirements
                    gpu_memory = hw_req.get('gpu_memory_gb', 0)
                    if gpu_memory > 80:
                        warnings.append(f"Very high GPU memory requirement: {gpu_memory}GB")
                    elif gpu_memory > 0 and gpu_memory < 1:
                        warnings.append(f"Very low GPU memory requirement: {gpu_memory}GB")
                    
                    cpu_cores = hw_req.get('cpu_cores', 1)
                    if cpu_cores > 128:
                        warnings.append(f"Very high CPU core requirement: {cpu_cores}")
                    elif cpu_cores < 1:
                        errors.append(f"Invalid CPU core requirement: {cpu_cores}")
                
                # Preset compatibility validation
                if 'compatibility' in metadata:
                    compatibility = metadata['compatibility']
                    if isinstance(compatibility, list):
                        # Check against available model types
                        if MODEL_VARIANTS:
                            invalid_models = [model for model in compatibility if model not in MODEL_VARIANTS]
                            if invalid_models:
                                warnings.append(f"Compatibility includes unavailable models: {invalid_models}")
                    else:
                        warnings.append("Compatibility field should be a list of model types")
        
        # MEMORY OPTIMIZATION - Clear memory after metadata validation for large configs
        if len(str(metadata)) > 50000 and total_ram_gb < 16:
            try:
                metadata_clear = enhanced_clear_memory(
                    aggressive=False,
                    hardware_data=hardware_data
                )
                if metadata_clear.get('success'):
                    logger.debug("Memory optimized after metadata validation")
            except Exception as e:
                logger.debug(f"Post-metadata memory optimization failed: {e}")
        
        # 3. TRAINING CONFIGURATION VALIDATION WITH PRESET AWARENESS
        logger.debug("Validating training section")
        
        if 'training' in config:
            training = config['training']
            if not isinstance(training, dict):
                errors.append("Training section must be a dictionary")
            else:
                # Core parameter validation with auto-correction
                batch_size = training.get('batch_size', 32)
                if not isinstance(batch_size, int) or batch_size < 1:
                    errors.append(f"Invalid batch_size: {batch_size}, must be positive integer")
                elif batch_size > 2048:
                    warnings.append(f"Very large batch_size: {batch_size}, may cause memory issues")
                elif batch_size < 2:
                    warnings.append(f"Very small batch_size: {batch_size}, may cause training instability")
                
                epochs = training.get('epochs', 100)
                if not isinstance(epochs, int) or epochs < 1:
                    errors.append(f"Invalid epochs: {epochs}, must be positive integer")
                elif epochs > 5000:
                    warnings.append(f"Very high epoch count: {epochs}, consider early stopping")
                elif epochs < 5:
                    warnings.append(f"Very low epoch count: {epochs}, may not converge properly")
                
                learning_rate = training.get('learning_rate', 0.001)
                if not isinstance(learning_rate, (int, float)) or learning_rate <= 0:
                    errors.append(f"Invalid learning_rate: {learning_rate}, must be positive number")
                elif learning_rate > 1.0:
                    warnings.append(f"Very high learning_rate: {learning_rate}, may cause instability")
                elif learning_rate < 1e-8:
                    warnings.append(f"Very low learning_rate: {learning_rate}, may not converge")
                
                patience = training.get('patience', 10)
                if not isinstance(patience, int) or patience < 0:
                    errors.append(f"Invalid patience: {patience}, must be non-negative integer")
                elif patience > 1000:
                    warnings.append(f"Very high patience: {patience}, training may run too long")
                
                # Enhanced optimizer validation
                optimizer = training.get('optimizer', 'Adam')
                valid_optimizers = ['SGD', 'Adam', 'AdamW', 'RMSprop', 'Adagrad', 'LBFGS']
                if optimizer not in valid_optimizers:
                    errors.append(f"Invalid optimizer: {optimizer}, must be one of {valid_optimizers}")
                
                # Optimizer-specific parameter validation
                if optimizer == 'Adam' or optimizer == 'AdamW':
                    adam_betas = training.get('adam_betas', (0.9, 0.999))
                    if not (isinstance(adam_betas, (list, tuple)) and len(adam_betas) == 2):
                        warnings.append("adam_betas should be a tuple/list of 2 values")
                    elif not all(0 < beta < 1 for beta in adam_betas):
                        errors.append("adam_betas values must be between 0 and 1")
                    
                    adam_eps = training.get('adam_eps', 1e-8)
                    if not isinstance(adam_eps, (int, float)) or adam_eps <= 0:
                        errors.append("adam_eps must be positive number")
                
                # Scheduler validation with parameter checking
                scheduler = training.get('scheduler')
                if scheduler is not None:
                    valid_schedulers = [
                        'StepLR', 'MultiStepLR', 'ExponentialLR', 'CosineAnnealingLR',
                        'ReduceLROnPlateau', 'CosineAnnealingWarmRestarts', 'OneCycleLR', 'CyclicLR'
                    ]
                    if scheduler not in valid_schedulers:
                        errors.append(f"Invalid scheduler: {scheduler}, must be one of {valid_schedulers}")
                    
                    # Validate scheduler parameters
                    scheduler_params = training.get('scheduler_params', {})
                    if scheduler == 'ReduceLROnPlateau':
                        mode = scheduler_params.get('mode', 'min')
                        if mode not in ['min', 'max']:
                            errors.append("ReduceLROnPlateau mode must be 'min' or 'max'")
                        
                        factor = scheduler_params.get('factor', 0.5)
                        if not isinstance(factor, (int, float)) or not (0 < factor < 1):
                            errors.append("ReduceLROnPlateau factor must be between 0 and 1")
                    
                    elif scheduler == 'CosineAnnealingLR':
                        T_max = scheduler_params.get('T_max')
                        if T_max and (not isinstance(T_max, int) or T_max <= 0):
                            errors.append("CosineAnnealingLR T_max must be positive integer")
                
                # Advanced training parameter validation
                gradient_clip = training.get('gradient_clip', 1.0)
                if not isinstance(gradient_clip, (int, float)) or gradient_clip < 0:
                    errors.append("gradient_clip must be non-negative number")
                elif gradient_clip > 100:
                    warnings.append(f"Very high gradient clipping: {gradient_clip}")
                
                gradient_accumulation = training.get('gradient_accumulation_steps', 1)
                if not isinstance(gradient_accumulation, int) or gradient_accumulation < 1:
                    errors.append("gradient_accumulation_steps must be positive integer")
                elif gradient_accumulation > 64:
                    warnings.append(f"High gradient accumulation: {gradient_accumulation}, may slow training")
                
                # Hardware-aware validation
                mixed_precision = training.get('mixed_precision', False)
                if mixed_precision and not cuda_available:
                    warnings.append("mixed_precision enabled but CUDA not available")
                
                num_workers = training.get('num_workers', 1)
                max_workers = (os.cpu_count() or 1) * 2
                if not isinstance(num_workers, int) or num_workers < 0:
                    errors.append("num_workers must be non-negative integer")
                elif num_workers > max_workers:
                    warnings.append(f"num_workers ({num_workers}) exceeds recommended max ({max_workers})")
                
                pin_memory = training.get('pin_memory', False)
                if pin_memory and not cuda_available:
                    warnings.append("pin_memory enabled but CUDA not available")
                
                # Cross-parameter validation
                effective_batch_size = batch_size * gradient_accumulation
                if effective_batch_size > 1024:
                    warnings.append(f"Large effective batch size ({effective_batch_size}) may hurt generalization")
                elif effective_batch_size < 4:
                    warnings.append(f"Small effective batch size ({effective_batch_size}) may cause noisy gradients")
        
        # 4. MODEL CONFIGURATION VALIDATION WITH ENHANCED PRESET SUPPORT
        logger.debug("Validating model section")
        
        if 'model' in config:
            model = config['model']
            if not isinstance(model, dict):
                errors.append("Model section must be a dictionary")
            else:
                # MEMORY OPTIMIZATION - Clear memory before intensive model validation for complex models
                model_complexity = len(str(model))
                if model_complexity > 10000 and total_ram_gb < 16:
                    try:
                        model_validation_clear = enhanced_clear_memory(
                            aggressive=model_complexity > 50000,
                            hardware_data=hardware_data
                        )
                        if model_validation_clear.get('success'):
                            logger.debug("Memory optimized before model validation")
                    except Exception as e:
                        logger.debug(f"Model validation memory optimization failed: {e}")
                
                # Model type validation with MODEL_VARIANTS checking
                model_type = model.get('model_type', 'SimpleAutoencoder')
                
                if MODEL_VARIANTS:
                    if model_type not in MODEL_VARIANTS:
                        errors.append(f"Invalid model_type: {model_type}, available: {list(MODEL_VARIANTS.keys())}")
                else:
                    # Fallback validation
                    valid_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                    if model_type not in valid_types:
                        errors.append(f"Invalid model_type: {model_type}, expected one of: {valid_types}")
                
                # Encoding dimension validation
                encoding_dim = model.get('encoding_dim', 8)
                if not isinstance(encoding_dim, int) or encoding_dim <= 0:
                    errors.append(f"Invalid encoding_dim: {encoding_dim}, must be positive integer")
                elif encoding_dim > 1000:
                    warnings.append(f"Very large encoding_dim: {encoding_dim}, may cause memory issues")
                elif encoding_dim < 2:
                    warnings.append(f"Very small encoding_dim: {encoding_dim}, may limit expressiveness")
                
                # Architecture validation with enhanced checks
                hidden_dims = model.get('hidden_dims', [])
                if not isinstance(hidden_dims, list):
                    errors.append("hidden_dims must be a list")
                elif not hidden_dims:
                    errors.append("hidden_dims cannot be empty")
                else:
                    # Validate all dimensions
                    invalid_dims = [dim for dim in hidden_dims if not isinstance(dim, int) or dim <= 0]
                    if invalid_dims:
                        errors.append(f"Invalid hidden dimensions: {invalid_dims}")
                    
                    # Architecture complexity checks
                    if len(hidden_dims) > 10:
                        warnings.append(f"Very deep architecture ({len(hidden_dims)} layers) may be hard to train")
                    
                    large_dims = [dim for dim in hidden_dims if dim > 2048]
                    if large_dims:
                        warnings.append(f"Very large hidden dimensions: {large_dims}")
                    
                    # Check for decreasing pattern
                    if len(hidden_dims) > 1:
                        increasing_count = sum(1 for i in range(1, len(hidden_dims)) if hidden_dims[i] > hidden_dims[i-1])
                        if increasing_count > len(hidden_dims) // 2:
                            warnings.append("Hidden dimensions mostly increase - consider decreasing pattern for autoencoders")
                
                # Dropout validation with length matching
                dropout_rates = model.get('dropout_rates', [])
                if not isinstance(dropout_rates, list):
                    errors.append("dropout_rates must be a list")
                elif not dropout_rates:
                    errors.append("dropout_rates cannot be empty")
                else:
                    # Validate all rates
                    invalid_rates = [rate for rate in dropout_rates if not isinstance(rate, (int, float)) or not (0 <= rate < 1)]
                    if invalid_rates:
                        errors.append(f"Invalid dropout rates: {invalid_rates}, must be between 0 and 1")
                    
                    # Length compatibility
                    if len(hidden_dims) != len(dropout_rates):
                        errors.append(f"Length mismatch: {len(hidden_dims)} hidden layers, {len(dropout_rates)} dropout rates")
                    
                    # Dropout strategy validation
                    if all(rate == 0 for rate in dropout_rates):
                        warnings.append("No dropout applied (all rates are 0)")
                    elif any(rate > 0.8 for rate in dropout_rates):
                        warnings.append("Very high dropout rates may hurt performance")
                
                # Activation function validation
                activation = model.get('activation', 'relu')
                available_activations = model.get('available_activations', ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish'])
                if activation not in available_activations:
                    errors.append(f"Invalid activation: {activation}, available: {available_activations}")
                
                activation_param = model.get('activation_param', 0.0)
                if not isinstance(activation_param, (int, float)):
                    errors.append("activation_param must be a number")
                elif activation == 'leaky_relu' and activation_param <= 0:
                    warnings.append("leaky_relu typically uses positive activation_param")
                
                # Normalization validation with consistency checks
                normalization = model.get('normalization')
                available_normalizations = model.get('available_normalizations', ['batch', 'layer', 'instance', 'group', None])
                if normalization not in available_normalizations:
                    errors.append(f"Invalid normalization: {normalization}, available: {available_normalizations}")

                use_batch_norm = model.get('use_batch_norm', False)
                use_layer_norm = model.get('use_layer_norm', False)

                # Fix inconsistencies: ensure normalization setting matches use_*_norm flags
                if normalization is None:
                    # If no normalization specified, both flags should be False
                    if use_batch_norm or use_layer_norm:
                        warnings.append("normalization=None but batch_norm or layer_norm enabled - auto-correcting to disable both")
                        model['use_batch_norm'] = False
                        model['use_layer_norm'] = False
                elif normalization == 'batch':
                    # If batch normalization specified, use_batch_norm should be True, use_layer_norm should be False
                    if not use_batch_norm:
                        warnings.append("normalization='batch' but use_batch_norm=False - auto-correcting to use_batch_norm=True")
                        model['use_batch_norm'] = True
                    if use_layer_norm:
                        warnings.append("normalization='batch' but use_layer_norm=True - auto-correcting to use_layer_norm=False")
                        model['use_layer_norm'] = False
                elif normalization == 'layer':
                    # If layer normalization specified, use_layer_norm should be True, use_batch_norm should be False
                    if not use_layer_norm:
                        warnings.append("normalization='layer' but use_layer_norm=False - auto-correcting to use_layer_norm=True")
                        model['use_layer_norm'] = True
                    if use_batch_norm:
                        warnings.append("normalization='layer' but use_batch_norm=True - auto-correcting to use_batch_norm=False")
                        model['use_batch_norm'] = False
                elif normalization in ['instance', 'group']:
                    # For instance/group normalization, both batch_norm and layer_norm should be False
                    if use_batch_norm or use_layer_norm:
                        warnings.append(f"normalization='{normalization}' but batch_norm or layer_norm enabled - auto-correcting to disable both")
                        model['use_batch_norm'] = False
                        model['use_layer_norm'] = False

                # Final validation check - should not have both enabled after auto-corrections
                use_batch_norm = model.get('use_batch_norm', False)
                use_layer_norm = model.get('use_layer_norm', False)
                if use_batch_norm and use_layer_norm:
                    warnings.append("Both batch_norm and layer_norm still enabled after auto-correction - may cause conflicts")

                # Batch norm compatibility with batch size
                if use_batch_norm and 'training' in config:
                    batch_size = config['training'].get('batch_size', 32)
                    if batch_size < 2:
                        errors.append("Batch normalization requires training batch_size >= 2")
                
                # Weight initialization validation
                weight_init = model.get('weight_init', 'xavier_uniform')
                available_initializers = model.get('available_initializers', ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'orthogonal'])
                if weight_init not in available_initializers:
                    errors.append(f"Invalid weight_init: {weight_init}, available: {available_initializers}")
                
                # Advanced features validation
                skip_connection = model.get('skip_connection', False)
                residual_blocks = model.get('residual_blocks', False)
                use_attention = model.get('use_attention', False)
                
                if skip_connection and len(hidden_dims) < 2:
                    warnings.append("Skip connections are most effective with deeper architectures")
                
                if residual_blocks and len(hidden_dims) < 3:
                    warnings.append("Residual blocks are most effective with deeper architectures")
                
                if use_attention and model_type == 'SimpleAutoencoder':
                    warnings.append("Attention mechanism may not be supported in SimpleAutoencoder")
                
                # Ensemble-specific validation
                if model_type == 'AutoencoderEnsemble':
                    num_models = model.get('num_models', 1)
                    if not isinstance(num_models, int) or num_models < 1:
                        errors.append(f"Invalid num_models: {num_models}, must be positive integer")
                    elif num_models < 2:
                        warnings.append("Ensemble should have at least 2 models for effectiveness")
                    elif num_models > 20:
                        warnings.append(f"Large ensemble ({num_models} models) may require significant memory")
                    
                    diversity_factor = model.get('diversity_factor', 0.1)
                    if not isinstance(diversity_factor, (int, float)) or not (0 <= diversity_factor <= 1):
                        errors.append(f"Invalid diversity_factor: {diversity_factor}, must be between 0 and 1")
                    elif diversity_factor == 0:
                        warnings.append("diversity_factor=0 may reduce ensemble effectiveness")
                
                # Feature requirements validation
                min_features = model.get('min_features', 5)
                if not isinstance(min_features, int) or min_features <= 0:
                    errors.append(f"Invalid min_features: {min_features}, must be positive integer")
                
                # Cross-validation with data configuration
                if 'data' in config:
                    data_features = config['data'].get('features', 20)
                    if data_features < min_features:
                        errors.append(f"Data features ({data_features}) < model min_features ({min_features})")
        
        # 5. SECURITY CONFIGURATION VALIDATION
        logger.debug("Validating security section")
        
        if 'security' in config:
            security = config['security']
            if not isinstance(security, dict):
                errors.append("Security section must be a dictionary")
            else:
                # Threshold validation
                percentile = security.get('percentile', 95)
                if not isinstance(percentile, (int, float)) or not (0 < percentile <= 100):
                    errors.append(f"Invalid percentile: {percentile}, must be between 0 and 100")
                elif percentile < 50:
                    warnings.append(f"Low percentile threshold ({percentile}) may cause many false positives")
                elif percentile > 99.9:
                    warnings.append(f"Very high percentile ({percentile}) may miss attacks")
                
                attack_threshold = security.get('attack_threshold', 0.3)
                if not isinstance(attack_threshold, (int, float)) or attack_threshold < 0:
                    errors.append(f"Invalid attack_threshold: {attack_threshold}, must be non-negative")
                elif attack_threshold > 10:
                    warnings.append(f"Very high attack threshold ({attack_threshold})")
                
                false_negative_cost = security.get('false_negative_cost', 2.0)
                if not isinstance(false_negative_cost, (int, float)) or false_negative_cost < 0:
                    errors.append(f"Invalid false_negative_cost: {false_negative_cost}, must be non-negative")
                
                # Advanced security features validation
                threshold_strategy = security.get('anomaly_threshold_strategy', 'fixed_percentile')
                valid_strategies = ['fixed_percentile', 'dynamic_percentile', 'adaptive', 'statistical']
                if threshold_strategy not in valid_strategies:
                    errors.append(f"Invalid threshold strategy: {threshold_strategy}, available: {valid_strategies}")
                
                detection_methods = security.get('detection_methods', ['reconstruction_error'])
                if not isinstance(detection_methods, list):
                    errors.append("detection_methods must be a list")
                else:
                    valid_methods = [
                        'reconstruction_error', 'statistical_analysis', 'mahalanobis_distance',
                        'isolation_forest', 'ensemble_voting', 'neural_network'
                    ]
                    invalid_methods = [method for method in detection_methods if method not in valid_methods]
                    if invalid_methods:
                        errors.append(f"Invalid detection methods: {invalid_methods}")
                
                # Alert levels validation
                alert_levels = security.get('alert_levels', ['low', 'medium', 'high'])
                if not isinstance(alert_levels, list) or len(alert_levels) < 2:
                    errors.append("alert_levels must be a list with at least 2 levels")
                
                # Confidence interval validation
                confidence_interval = security.get('confidence_interval', 0.95)
                if not isinstance(confidence_interval, (int, float)) or not (0 < confidence_interval < 1):
                    errors.append(f"Invalid confidence_interval: {confidence_interval}, must be between 0 and 1")
        
        # 6. DATA CONFIGURATION VALIDATION
        logger.debug("Validating data section")
        
        if 'data' in config:
            data = config['data']
            if not isinstance(data, dict):
                errors.append("Data section must be a dictionary")
            else:
                # Sample size validation
                normal_samples = data.get('normal_samples', 1000)
                if not isinstance(normal_samples, int) or normal_samples < 1:
                    errors.append(f"Invalid normal_samples: {normal_samples}, must be positive integer")
                elif normal_samples < 100:
                    warnings.append(f"Small normal sample size ({normal_samples}) may not be representative")
                elif normal_samples > 1000000:
                    warnings.append(f"Large normal sample size ({normal_samples}) may require significant memory")
                
                attack_samples = data.get('attack_samples', 200)
                if not isinstance(attack_samples, int) or attack_samples < 1:
                    errors.append(f"Invalid attack_samples: {attack_samples}, must be positive integer")
                elif attack_samples < 50:
                    warnings.append(f"Small attack sample size ({attack_samples}) may not be representative")
                
                # Feature validation
                features = data.get('features', 20)
                if not isinstance(features, int) or features < 1:
                    errors.append(f"Invalid features: {features}, must be positive integer")
                elif features > 10000:
                    warnings.append(f"Very high feature count ({features}) may cause curse of dimensionality")
                
                # Split validation
                validation_split = data.get('validation_split', 0.2)
                if not isinstance(validation_split, (int, float)) or not (0 < validation_split < 1):
                    errors.append(f"Invalid validation_split: {validation_split}, must be between 0 and 1")
                
                test_split = data.get('test_split', 0.2)
                if not isinstance(test_split, (int, float)) or not (0 < test_split < 1):
                    errors.append(f"Invalid test_split: {test_split}, must be between 0 and 1")
                
                # Total split validation
                total_split = validation_split + test_split
                if total_split >= 1.0:
                    errors.append(f"Combined splits ({total_split:.2f}) must be < 1.0")
                elif total_split > 0.8:
                    warnings.append(f"High combined split ratio ({total_split:.2f}) leaves little training data")
                
                # Normalization validation
                #normalization = data.get('normalization', 'standard')
                normalization = data.get('data_normalization', 'standard')
                valid_normalizations = ['standard', 'minmax', 'robust', 'quantile', 'none']
                if normalization not in valid_normalizations:
                    errors.append(f"Invalid normalization: {normalization}, available: {valid_normalizations}")
                
                # Anomaly factor validation
                anomaly_factor = data.get('anomaly_factor', 1.5)
                if not isinstance(anomaly_factor, (int, float)) or anomaly_factor <= 0:
                    errors.append(f"Invalid anomaly_factor: {anomaly_factor}, must be positive")
                elif anomaly_factor > 10:
                    warnings.append(f"Very high anomaly factor ({anomaly_factor})")
                
                # Synthetic data generation validation
                if 'synthetic_generation' in data:
                    synthetic = data['synthetic_generation']
                    if isinstance(synthetic, dict):
                        cluster_variance = synthetic.get('cluster_variance', 0.1)
                        if not isinstance(cluster_variance, (int, float)) or cluster_variance < 0:
                            errors.append("cluster_variance must be non-negative")
                        
                        anomaly_sparsity = synthetic.get('anomaly_sparsity', 0.3)
                        if not isinstance(anomaly_sparsity, (int, float)) or not (0 <= anomaly_sparsity <= 1):
                            errors.append("anomaly_sparsity must be between 0 and 1")
                
                # Preprocessing validation
                if 'preprocessing' in data:
                    preprocessing = data['preprocessing']
                    if isinstance(preprocessing, dict):
                        outlier_threshold = preprocessing.get('outlier_threshold', 3.0)
                        if not isinstance(outlier_threshold, (int, float)) or outlier_threshold <= 0:
                            errors.append("outlier_threshold must be positive")
                        elif outlier_threshold < 1:
                            warnings.append("Very low outlier threshold may remove too much data")
                        elif outlier_threshold > 5:
                            warnings.append("Very high outlier threshold may not remove outliers effectively")
        
        # MEMORY OPTIMIZATION - Clear memory before hardware validation for low-memory systems
        if total_ram_gb < 8:
            try:
                mid_validation_clear = enhanced_clear_memory(
                    aggressive=True,
                    hardware_data=hardware_data
                )
                if mid_validation_clear.get('success'):
                    logger.debug("Memory optimized mid-validation for low-memory system")
            except Exception as e:
                logger.debug(f"Mid-validation memory optimization failed: {e}")
        
        # 7. HARDWARE CONFIGURATION VALIDATION
        logger.debug("Validating hardware section")
        
        if 'hardware' in config:
            hardware = config['hardware']
            if not isinstance(hardware, dict):
                errors.append("Hardware section must be a dictionary")
            else:
                # Device validation
                device = hardware.get('device', 'auto')
                valid_devices = ['auto', 'cpu', 'cuda', 'mps']
                if device not in valid_devices:
                    errors.append(f"Invalid device: {device}, available: {valid_devices}")
                
                # Device availability check
                if device == 'cuda' and not cuda_available:
                    warnings.append("CUDA device specified but CUDA not available")
                elif device == 'mps' and not (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()):
                    warnings.append("MPS device specified but MPS not available")
                
                # Memory requirements validation
                recommended_gpu_memory = hardware.get('recommended_gpu_memory', 4)
                if not isinstance(recommended_gpu_memory, (int, float)) or recommended_gpu_memory < 0:
                    errors.append("recommended_gpu_memory must be non-negative")
                elif recommended_gpu_memory > 80:
                    warnings.append(f"Very high GPU memory requirement: {recommended_gpu_memory}GB")
                
                # System requirements validation
                if 'minimum_system_requirements' in hardware:
                    min_req = hardware['minimum_system_requirements']
                    if isinstance(min_req, dict):
                        cpu_cores = min_req.get('cpu_cores', 1)
                        if not isinstance(cpu_cores, int) or cpu_cores < 1:
                            errors.append("minimum cpu_cores must be positive integer")
                        
                        ram_gb = min_req.get('ram_gb', 2)
                        if not isinstance(ram_gb, (int, float)) or ram_gb < 0.5:
                            errors.append("minimum ram_gb must be >= 0.5")
                
                # Performance optimization validation
                if 'performance_optimization' in hardware:
                    perf_opt = hardware['performance_optimization']
                    if isinstance(perf_opt, dict):
                        use_cuda = perf_opt.get('use_cuda', False)
                        if use_cuda and not cuda_available:
                            warnings.append("use_cuda enabled but CUDA not available")
                        
                        use_amp = perf_opt.get('use_amp', False)
                        if use_amp and not cuda_available:
                            warnings.append("Automatic Mixed Precision requires CUDA")
                        
                        benchmark_mode = perf_opt.get('benchmark_mode', False)
                        deterministic = perf_opt.get('deterministic', False)
                        if benchmark_mode and deterministic:
                            warnings.append("benchmark_mode and deterministic may conflict")
        
        # 8. PRESET CONFIGURATION VALIDATION
        logger.debug("Validating presets section")
        
        if 'presets' in config:
            presets = config['presets']
            if not isinstance(presets, dict):
                errors.append("Presets section must be a dictionary")
            else:
                # Current preset validation
                current_preset = presets.get('current_preset')
                if current_preset is not None:
                    available_presets = get_available_presets()
                    if current_preset not in available_presets:
                        errors.append(f"Invalid current_preset: {current_preset}, available: {available_presets}")
                    else:
                        # Preset compatibility validation
                        try:
                            model_type = config.get('model', {}).get('model_type', 'SimpleAutoencoder')
                            if not validate_model_preset_compatibility(model_type, config):
                                warnings.append(f"Model type '{model_type}' may not be compatible with preset '{current_preset}'")
                        except Exception as e:
                            warnings.append(f"Preset compatibility check failed: {e}")
                
                # Override rules validation
                if 'override_rules' in presets:
                    override_rules = presets['override_rules']
                    if not isinstance(override_rules, dict):
                        errors.append("override_rules must be a dictionary")
                    else:
                        valid_sections = ['security', 'monitoring', 'hardware', 'training', 'model', 'data']
                        for section, enabled in override_rules.items():
                            if section not in valid_sections:
                                warnings.append(f"Unknown override rule section: {section}")
                            elif not isinstance(enabled, bool):
                                errors.append(f"Override rule for {section} must be boolean")
                
                # Auto-apply validation
                auto_apply = presets.get('auto_apply', False)
                if not isinstance(auto_apply, bool):
                    errors.append("auto_apply must be boolean")
        
        # 9. HYPERPARAMETER OPTIMIZATION VALIDATION
        logger.debug("Validating hyperparameter optimization section")
        
        if 'hyperparameter_optimization' in config:
            hpo = config['hyperparameter_optimization']
            if not isinstance(hpo, dict):
                errors.append("hyperparameter_optimization section must be a dictionary")
            else:
                enabled = hpo.get('enabled', False)
                if enabled:
                    # Strategy validation
                    strategy = hpo.get('strategy', 'optuna')
                    valid_strategies = ['optuna', 'hyperopt', 'skopt', 'random', 'grid']
                    if strategy not in valid_strategies:
                        errors.append(f"Invalid HPO strategy: {strategy}, available: {valid_strategies}")
                    
                    # Trials validation
                    n_trials = hpo.get('n_trials', 50)
                    if not isinstance(n_trials, int) or n_trials < 1:
                        errors.append("n_trials must be positive integer")
                    elif n_trials > 10000:
                        warnings.append(f"Very high trial count ({n_trials}) may take very long")
                    
                    # Timeout validation
                    timeout = hpo.get('timeout', 3600)
                    if not isinstance(timeout, (int, float)) or timeout < 60:
                        warnings.append("Very short HPO timeout may not find good solutions")
                    elif timeout > 86400:
                        warnings.append("Very long HPO timeout (>24h) may be excessive")
                    
                    # Optimization space validation
                    if 'optimization_space' in hpo:
                        opt_space = hpo['optimization_space']
                        if not isinstance(opt_space, dict):
                            errors.append("optimization_space must be a dictionary")
                        elif not opt_space:
                            warnings.append("Empty optimization space - HPO will have no effect")
        
        # 10. CROSS-SECTION VALIDATION
        logger.debug("Performing cross-section validation")
        
        # Model-Training compatibility
        if 'model' in config and 'training' in config:
            model_config = config['model']
            training_config = config['training']
            
            # Batch normalization vs batch size
            if model_config.get('use_batch_norm') and training_config.get('batch_size', 32) < 2:
                errors.append("Batch normalization requires training batch_size >= 2")
            
            # Mixed precision compatibility
            if training_config.get('mixed_precision') and model_config.get('model_type') == 'SimpleAutoencoder':
                warnings.append("Mixed precision may not be fully supported with SimpleAutoencoder")
            
            # Memory estimation
            try:
                batch_size = training_config.get('batch_size', 32)
                hidden_dims = model_config.get('hidden_dims', [])
                if hidden_dims:
                    model_complexity = sum(hidden_dims) + model_config.get('encoding_dim', 8)
                    # Rough MB estimate
                    memory_estimate = batch_size * model_complexity * 4 / (1024**2)
                    
                    # > 1GB
                    if memory_estimate > 1024:
                        warnings.append(f"High memory usage estimated: ~{memory_estimate:.0f}MB")
            except Exception:
                # Skip memory estimation if it fails
                pass
        
        # Hardware-Training compatibility
        if 'hardware' in config and 'training' in config:
            hardware_config = config['hardware']
            training_config = config['training']
            
            # CUDA settings consistency
            device = hardware_config.get('device', 'auto')
            use_cuda = hardware_config.get('performance_optimization', {}).get('use_cuda', False)
            
            if device == 'cpu' and use_cuda:
                warnings.append("Device set to CPU but use_cuda=True in performance optimization")
            elif device == 'cuda' and not use_cuda:
                warnings.append("Device set to CUDA but use_cuda=False in performance optimization")
            
            # Mixed precision consistency
            use_amp = hardware_config.get('performance_optimization', {}).get('use_amp', False)
            mixed_precision = training_config.get('mixed_precision', False)
            
            if use_amp and not mixed_precision:
                warnings.append("AMP enabled in hardware but mixed_precision=False in training")
            elif mixed_precision and not use_amp:
                warnings.append("mixed_precision=True in training but AMP not enabled in hardware")
        
        # Data-Model compatibility
        if 'data' in config and 'model' in config:
            data_config = config['data']
            model_config = config['model']
            
            # Feature count compatibility
            data_features = data_config.get('features', 20)
            min_features = model_config.get('min_features', 5)
            
            if data_features < min_features:
                errors.append(f"Data features ({data_features}) < model min_features ({min_features})")
            
            # Sample size vs model complexity
            normal_samples = data_config.get('normal_samples', 1000)
            hidden_dims = model_config.get('hidden_dims', [])
            # Rough parameter estimate
            total_params = sum(hidden_dims) if hidden_dims else 100
            
            if normal_samples < total_params:
                warnings.append(f"Small dataset ({normal_samples}) for model complexity (~{total_params} params)")
        
        # MEMORY OPTIMIZATION - Clear memory before strict mode validation for low-memory systems
        if strict and total_ram_gb < 8:
            try:
                strict_mode_clear = enhanced_clear_memory(
                    aggressive=True,
                    hardware_data=hardware_data
                )
                if strict_mode_clear.get('success'):
                    logger.debug("Memory optimized before strict mode validation")
            except Exception as e:
                logger.debug(f"Strict mode memory optimization failed: {e}")
        
        # 11. STRICT MODE ADDITIONAL VALIDATIONS
        if strict:
            logger.debug("Applying strict mode validations")
            
            # Require all recommended sections
            if missing_recommended:
                errors.extend([f"Strict mode: missing section '{section}'" for section in missing_recommended])
            
            # Stricter parameter bounds
            if 'training' in config:
                training = config['training']
                
                lr = training.get('learning_rate', 0.001)
                if lr > 0.1:
                    errors.append("Strict mode: learning_rate must be <= 0.1")
                elif lr < 1e-6:
                    errors.append("Strict mode: learning_rate must be >= 1e-6")
                
                batch_size = training.get('batch_size', 32)
                if batch_size > 512:
                    errors.append("Strict mode: batch_size must be <= 512")
                elif batch_size < 8:
                    errors.append("Strict mode: batch_size must be >= 8")
            
            # Require specific metadata fields
            if 'metadata' in config:
                metadata = config['metadata']
                required_strict_metadata = ['description', 'version', 'config_version']
                missing_metadata = [field for field in required_strict_metadata if field not in metadata]
                if missing_metadata:
                    errors.extend([f"Strict mode: missing metadata field '{field}'" for field in missing_metadata])
            
            # Hardware requirements in strict mode
            if 'hardware' in config:
                hardware = config['hardware']
                if 'minimum_system_requirements' not in hardware:
                    errors.append("Strict mode: hardware section must specify minimum_system_requirements")
            
            # Convert strict warnings to errors
            if any("Very high" in warning or "Very low" in warning for warning in warnings):
                extreme_warnings = [w for w in warnings if "Very high" in w or "Very low" in w]
                errors.extend([f"Strict mode: {warning}" for warning in extreme_warnings])
        
        # 12. FINAL VALIDATION SUMMARY
        logger.debug("Generating validation summary")
        
        # Calculate validation statistics
        total_checks = len(errors) + len(warnings)
        error_count = len(errors)
        warning_count = len(warnings)
        
        # Add validation context to config if requested
        validation_context.update({
            'total_checks': total_checks,
            'error_count': error_count,
            'warning_count': warning_count,
            'validation_passed': error_count == 0,
            'validation_quality': (
                'excellent' if error_count == 0 and warning_count == 0 else
                'good' if error_count == 0 and warning_count <= 5 else
                'needs_attention' if error_count == 0 else
                'failed'
            )
        })
        
        # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
        # Aggressive cleanup after validation completion
        try:
            final_clear_results = enhanced_clear_memory(
                aggressive=True,  # Aggressive final cleanup
                hardware_data=hardware_data
            )
            
            if final_clear_results.get('success'):
                logger.debug(f"Final validation memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
                
        except Exception as e:
            logger.debug(f"Final validation memory optimization failed: {e}")
        
        # Log validation summary
        if error_count == 0:
            if warning_count == 0:
                logger.debug("Configuration validation passed with no issues")
            else:
                logger.warning(f"Configuration validation passed with {warning_count} warnings")
        else:
            logger.error(f"Configuration validation failed with {error_count} errors and {warning_count} warnings")
        
        # Return results
        is_valid = error_count == 0
        return is_valid, errors, warnings
        
    except Exception as e:
        error_msg = f"Configuration validation failed with exception: {str(e)}"
        logger.error(error_msg, exc_info=True)
        errors.append(error_msg)
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after validation error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        return False, errors, warnings

def load_config(config_path: Path = CONFIG_FILE) -> Dict[str, Any]:
    """Load config file with enhanced validation, error recovery, migration support,
    integrated named configuration functionality, and intelligent memory management 
    for optimal performance during large file processing.
    
    Args:
        config_path: Path to the configuration file or name of saved/named configuration
        
    Returns:
        Dictionary containing the loaded configuration
        
    Raises:
        ValueError: If configuration format is invalid
        FileNotFoundError: If configuration file not found
    """
    try:
        # INITIAL MEMORY OPTIMIZATION - Get hardware context early for memory-aware processing
        hardware_data = None
        total_ram_gb = 8.0  # Conservative default
        
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        except Exception as e:
            logger.debug(f"Hardware detection failed during load_config: {e}")
        
        # Handle named configuration loading
        if isinstance(config_path, str) or (isinstance(config_path, Path) and not config_path.exists()):
            config_name = str(config_path)
            
            # First check if it's a named configuration
            registry_path = CONFIG_DIR / "named_configs_registry.json"
            if registry_path.exists():
                try:
                    with open(registry_path, 'r', encoding='utf-8') as f:
                        registry = json.load(f)
                    named_configs = registry.get("configs", {})
                    
                    if config_name in named_configs:
                        actual_config_path = Path(named_configs[config_name]["path"])
                        
                        if not actual_config_path.exists():
                            raise FileNotFoundError(f"Named configuration file not found: {actual_config_path}")
                        
                        # MEMORY OPTIMIZATION - Clear memory before recursive call
                        if total_ram_gb < 8:
                            try:
                                pre_recursive_clear = enhanced_clear_memory(
                                    aggressive=True,
                                    hardware_data=hardware_data
                                )
                                if pre_recursive_clear.get('success'):
                                    logger.debug("Memory optimized before named config recursive load")
                            except Exception as e:
                                logger.debug(f"Pre-recursive memory optimization failed: {e}")
                        
                        # Update last accessed time in registry
                        try:
                            update_named_config_registry(config_name, actual_config_path, {
                                "created": named_configs[config_name].get("created"),
                                "modified": named_configs[config_name].get("modified"),
                                "config": {
                                    "preset_used": named_configs[config_name].get("preset_used"),
                                    "model_type": named_configs[config_name].get("model_type"),
                                    "checksum": named_configs[config_name].get("checksum")
                                }
                            })
                        except Exception as e:
                            logger.debug(f"Failed to update access time for {config_name}: {e}")
                        
                        # Load the actual config file
                        return load_config(actual_config_path)
                        
                except Exception as e:
                    logger.warning(f"Failed to check named config registry: {e}")
            
            # If not a named config, try regular saved config
            regular_config_path = CONFIG_DIR / f"{config_name}.json"
            if regular_config_path.exists():
                return load_config(regular_config_path)
            
            raise FileNotFoundError(f"Configuration '{config_name}' not found")
        
        # Standard file loading logic with memory optimization
        try:
            if not config_path.exists():
                logger.info(f"No configuration file found at {config_path}")
                return {}
            
            # Check file size and basic validity with memory considerations
            file_size = config_path.stat().st_size
            if file_size == 0:
                logger.warning(f"Configuration file {config_path} is empty")
                return {}
            
            # 10MB limit
            if file_size > 10 * 1024 * 1024:
                logger.warning(f"Configuration file {config_path} is unusually large ({file_size} bytes)")
            
            # MEMORY OPTIMIZATION - Clear memory before loading large files
            large_file_threshold = 1024 * 1024  # 1MB
            if file_size > large_file_threshold or total_ram_gb < 8:
                try:
                    pre_load_clear = enhanced_clear_memory(
                        aggressive=file_size > large_file_threshold * 5 or total_ram_gb < 4,
                        hardware_data=hardware_data
                    )
                    if pre_load_clear.get('success'):
                        logger.debug(f"Memory optimized before loading {file_size} byte config file")
                except Exception as e:
                    logger.debug(f"Pre-load memory optimization failed: {e}")
            
            # Load with enhanced error handling
            logger.debug(f"Loading configuration from {config_path} ({file_size} bytes)")
            
            with open(config_path, 'r', encoding='utf-8') as f:
                try:
                    config_data = json.load(f)
                except json.JSONDecodeError as e:
                    # Try to recover from common JSON errors
                    logger.error(f"JSON decode error in {config_path}: {e}")
                    
                    # MEMORY OPTIMIZATION - Clear memory before intensive recovery operations
                    try:
                        recovery_clear = enhanced_clear_memory(
                            aggressive=total_ram_gb < 8,
                            hardware_data=hardware_data
                        )
                        if recovery_clear.get('success'):
                            logger.debug("Memory optimized before JSON recovery")
                    except Exception as e:
                        logger.debug(f"Pre-recovery memory optimization failed: {e}")
                    
                    # Attempt basic recovery
                    f.seek(0)
                    content = f.read()
                    
                    # Try to fix common issues
                    recovered_config = attempt_json_recovery(content, config_path)
                    if recovered_config:
                        config_data = recovered_config
                        logger.warning("Configuration recovered from JSON errors")
                    else:
                        raise ValueError(f"Cannot parse configuration file: {e}")
            
            # MEMORY OPTIMIZATION - Clear memory after file loading for large configs
            if file_size > large_file_threshold and total_ram_gb < 16:
                try:
                    post_load_clear = enhanced_clear_memory(
                        aggressive=file_size > large_file_threshold * 10,
                        hardware_data=hardware_data
                    )
                    if post_load_clear.get('success'):
                        logger.debug("Memory optimized after config file loading")
                except Exception as e:
                    logger.debug(f"Post-load memory optimization failed: {e}")
            
            # Validate basic structure
            if not isinstance(config_data, dict):
                raise ValueError("Configuration file must contain a JSON object")
            
            # Handle different configuration formats with memory optimization for large configs
            if 'config' in config_data and 'metadata' in config_data:
                # New format with metadata
                loaded_config = config_data['config']
                metadata = config_data['metadata']
                
                # MEMORY OPTIMIZATION - Clear memory before intensive metadata processing
                if len(str(metadata)) > 10000 and total_ram_gb < 16:
                    try:
                        metadata_clear = enhanced_clear_memory(
                            aggressive=False,
                            hardware_data=hardware_data
                        )
                        if metadata_clear.get('success'):
                            logger.debug("Memory optimized before metadata processing")
                    except Exception as e:
                        logger.debug(f"Metadata memory optimization failed: {e}")
                
                logger.debug(f"Loaded configuration with metadata: version={metadata.get('version', 'unknown')}")
                
                # Check version compatibility
                file_version = metadata.get('version', '1.0')
                if file_version != '2.1':
                    logger.info(f"Configuration version {file_version} detected, may need migration")
                    # The migration will be handled by the caller if needed
                
                # Verify checksum if present
                expected_checksum = metadata.get('config', {}).get('checksum')
                if expected_checksum:
                    actual_checksum = generate_config_checksum(loaded_config)
                    if actual_checksum != expected_checksum:
                        logger.warning("Configuration checksum mismatch - file may have been modified externally")
                
                # MEMORY OPTIMIZATION - Clear memory before preset information update
                if len(str(loaded_config)) > 50000 and total_ram_gb < 16:
                    try:
                        preset_clear = enhanced_clear_memory(
                            aggressive=False,
                            hardware_data=hardware_data
                        )
                        if preset_clear.get('success'):
                            logger.debug("Memory optimized before preset information update")
                    except Exception as e:
                        logger.debug(f"Preset memory optimization failed: {e}")
                
                # Ensure preset information is current
                if 'presets' in loaded_config:
                    try:
                        loaded_config['presets']['available_presets'] = get_available_presets()
                        loaded_config['presets']['preset_configs'] = get_preset_descriptions()
                        loaded_config['presets']['custom_presets_available'] = get_safe_custom_presets()
                    except Exception as e:
                        logger.debug(f"Failed to update preset information: {e}")
                
            else:
                # Legacy format - assume it's the configuration directly
                loaded_config = config_data
                logger.info("Loaded legacy configuration format")
                
                # MEMORY OPTIMIZATION - Clear memory before legacy format processing
                if len(str(loaded_config)) > 25000 and total_ram_gb < 16:
                    try:
                        legacy_clear = enhanced_clear_memory(
                            aggressive=len(str(loaded_config)) > 50000,
                            hardware_data=hardware_data
                        )
                        if legacy_clear.get('success'):
                            logger.debug("Memory optimized before legacy format processing")
                    except Exception as e:
                        logger.debug(f"Legacy format memory optimization failed: {e}")
                
                # Attempt to add current preset information to legacy configs
                try:
                    if 'presets' not in loaded_config:
                        loaded_config['presets'] = {}
                    loaded_config['presets']['available_presets'] = get_available_presets()
                    loaded_config['presets']['preset_configs'] = get_preset_descriptions()
                    loaded_config['presets']['custom_presets_available'] = get_safe_custom_presets()
                except Exception as e:
                    logger.debug(f"Failed to add preset information to legacy config: {e}")
            
            # Validate loaded configuration structure
            if not isinstance(loaded_config, dict):
                raise ValueError("Invalid configuration structure")
            
            # Basic sanity checks
            if not loaded_config:
                logger.warning("Configuration is empty")
                return {}
            
            # Enhanced validation for critical sections
            required_sections = ['training', 'model', 'security', 'data']
            missing_sections = [section for section in required_sections if section not in loaded_config]
            if missing_sections:
                logger.warning(f"Missing required sections: {missing_sections}")
                # Don't fail loading, but warn - these will be filled by defaults
            
            # Validate model type compatibility with current MODEL_VARIANTS
            model_config = loaded_config.get('model', {})
            model_type = model_config.get('model_type')
            if model_type and MODEL_VARIANTS and model_type not in MODEL_VARIANTS:
                logger.warning(f"Model type '{model_type}' not available in current MODEL_VARIANTS")
                # Don't modify the loaded config, let the caller handle this
            
            # MEMORY OPTIMIZATION - Clear memory before final processing steps
            if len(str(loaded_config)) > 75000 and total_ram_gb < 16:
                try:
                    final_processing_clear = enhanced_clear_memory(
                        aggressive=len(str(loaded_config)) > 150000,
                        hardware_data=hardware_data
                    )
                    if final_processing_clear.get('success'):
                        logger.debug("Memory optimized before final config processing")
                except Exception as e:
                    logger.debug(f"Final processing memory optimization failed: {e}")
            
            # Log loading statistics
            section_count = len([k for k, v in loaded_config.items() if isinstance(v, dict)])
            total_params = sum(len(v) if isinstance(v, dict) else 1 for v in loaded_config.values())
            
            logger.debug(f"Successfully loaded configuration: {section_count} sections, {total_params} parameters")
            logger.debug(f"Configuration sections: {list(loaded_config.keys())}")
            
            # Add load metadata
            if 'metadata' not in loaded_config:
                loaded_config['metadata'] = {}
            loaded_config['metadata']['last_loaded'] = datetime.now().isoformat()
            loaded_config['metadata']['loaded_from'] = str(config_path)
            
            # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
            # Aggressive cleanup after configuration loading completion
            try:
                final_clear_results = enhanced_clear_memory(
                    aggressive=True,  # Aggressive final cleanup
                    hardware_data=hardware_data
                )
                
                if final_clear_results.get('success'):
                    logger.debug(f"Final load memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
                    
            except Exception as e:
                logger.debug(f"Final load memory optimization failed: {e}")
            
            return loaded_config
            
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in config file {config_path}: {str(e)}")
            raise ValueError(f"Configuration file contains invalid JSON: {e}")
        except FileNotFoundError:
            logger.info(f"Configuration file not found: {config_path}")
            return {}
        except PermissionError as e:
            logger.error(f"Permission denied reading config file {config_path}: {e}")
            raise ValueError(f"Cannot read configuration file: permission denied")
        except Exception as e:
            logger.error(f"Error loading config from {config_path}: {str(e)}", exc_info=True)
            
            # MEMORY OPTIMIZATION - Clear memory before backup loading attempt
            try:
                backup_clear = enhanced_clear_memory(
                    aggressive=total_ram_gb < 8,
                    hardware_data=hardware_data
                )
                if backup_clear.get('success'):
                    logger.debug("Memory optimized before backup loading attempt")
            except Exception as cleanup_error:
                logger.debug(f"Pre-backup memory optimization failed: {cleanup_error}")
            
            # Try to load from backup if available
            backup_config = try_load_from_backup(config_path)
            if backup_config:
                logger.warning("Loaded configuration from backup due to primary file error")
                return backup_config
            
            raise ValueError(f"Failed to load configuration: {str(e)}")
            
    except Exception as e:
        logger.error(f"Critical error in load_config: {e}", exc_info=True)
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after load_config error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        # Re-raise the original exception
        raise

def update_named_config_registry(name: str, config_path: Path, metadata: Dict[str, Any]) -> None:
    """Update the named configuration registry for easier management with enhanced error handling."""
    try:
        registry_path = CONFIG_DIR / "named_configs_registry.json"
        
        # Ensure CONFIG_DIR exists
        CONFIG_DIR.mkdir(parents=True, exist_ok=True)
        
        # Load existing registry or create new
        registry = {
            "version": "2.1",
            "created": datetime.now().isoformat(),
            "configs": {}
        }
        
        if registry_path.exists():
            try:
                with open(registry_path, 'r', encoding='utf-8') as f:
                    existing_registry = json.load(f)
                    if isinstance(existing_registry, dict) and 'configs' in existing_registry:
                        registry = existing_registry
                        logger.debug(f"Loaded existing named config registry with {len(registry['configs'])} entries")
                    else:
                        logger.warning("Invalid registry format, creating new registry")
            except json.JSONDecodeError as e:
                logger.warning(f"Registry file corrupted, creating new: {e}")
            except Exception as e:
                logger.warning(f"Failed to load existing registry: {e}")
        
        # Extract configuration details safely
        config_dict = metadata.get("config", {})
        
        # Update registry entry with comprehensive information
        registry_entry = {
            "path": str(config_path),
            "created": metadata.get("created", datetime.now().isoformat()),
            "modified": metadata.get("modified", datetime.now().isoformat()),
            "last_accessed": datetime.now().isoformat(),
            "preset_used": config_dict.get("preset_used", "unknown"),
            "model_type": config_dict.get("model_type", "unknown"),
            "checksum": config_dict.get("checksum", "unknown"),
            "file_size": config_path.stat().st_size if config_path.exists() else 0,
            "config_version": metadata.get("version", "2.1"),
            "registry_version": "2.1"
        }
        
        # Add additional metadata if available
        if "system" in metadata:
            registry_entry["system_info"] = {
                "hostname": metadata["system"].get("hostname", "unknown"),
                "os": metadata["system"].get("os", "unknown"),
                "created_by": "deep_learning_config_system"
            }
        
        registry["configs"][name] = registry_entry
        registry["modified"] = datetime.now().isoformat()
        registry["total_configs"] = len(registry["configs"])
        
        # Add registry metadata
        if "metadata" not in registry:
            registry["metadata"] = {}
        registry["metadata"].update({
            "last_updated": datetime.now().isoformat(),
            "update_count": registry.get("metadata", {}).get("update_count", 0) + 1,
            "schema_version": "2.1"
        })
        
        # Atomic write operation with backup
        temp_path = registry_path.with_suffix(f".tmp_{int(time.time())}")
        try:
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(registry, f, indent=2, ensure_ascii=False, sort_keys=True)
            
            # Verify the written file
            with open(temp_path, 'r', encoding='utf-8') as f:
                verification_data = json.load(f)
                if not verification_data.get('configs'):
                    raise ValueError("Registry verification failed")
            
            # Create backup of existing registry if it exists
            if registry_path.exists():
                backup_path = registry_path.with_suffix(f".backup_{int(time.time())}")
                shutil.copy2(registry_path, backup_path)
                # Keep only last 5 backups
                cleanup_registry_backups(registry_path.parent, 5)
            
            # Atomic replacement
            if os.name == 'nt' and registry_path.exists():
                registry_path.unlink()
            temp_path.replace(registry_path)
            
        except Exception as write_error:
            if temp_path.exists():
                try:
                    temp_path.unlink()
                except:
                    pass
            raise RuntimeError(f"Failed to write registry: {write_error}") from write_error
        
        logger.debug(f"Updated named configuration registry: {name} -> {config_path}")
        
    except Exception as e:
        logger.error(f"Failed to update named configuration registry: {e}", exc_info=True)
        # Don't raise exception to avoid breaking config save operations

def cleanup_registry_backups(registry_dir: Path, keep_count: int):
    """Clean up old registry backup files."""
    try:
        backup_pattern = "named_configs_registry.backup_*"
        backup_files = list(registry_dir.glob(backup_pattern))
        
        if len(backup_files) > keep_count:
            # Sort by modification time (newest first)
            backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # Remove old backups
            for old_backup in backup_files[keep_count:]:
                old_backup.unlink()
                logger.debug(f"Removed old registry backup: {old_backup}")
                
    except Exception as e:
        logger.debug(f"Failed to cleanup registry backups: {e}")

def generate_config_checksum(config: Dict[str, Any]) -> str:
    """Generate a checksum for configuration integrity verification with enhanced stability.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        String checksum (SHA-256 hash of serialized config)
    """
    try:
        # Create a deep copy to avoid modifying original config
        config_copy = deepcopy(config)
        
        # Remove volatile/timestamp fields that shouldn't affect checksum
        volatile_fields = [
            'last_loaded', 'last_accessed', 'last_modified', 'modified',
            'created', 'timestamp', 'save_time', 'load_time',
            'runtime_id', 'process_id', 'collection_duration_ms'
        ]
        
        def remove_volatile_fields(obj, path=""):
            """Recursively remove volatile fields from nested dictionaries."""
            if isinstance(obj, dict):
                # Create new dict without volatile fields
                cleaned = {}
                for key, value in obj.items():
                    if key not in volatile_fields:
                        cleaned[key] = remove_volatile_fields(value, f"{path}.{key}" if path else key)
                return cleaned
            elif isinstance(obj, list):
                return [remove_volatile_fields(item, f"{path}[{i}]") for i, item in enumerate(obj)]
            else:
                return obj
        
        cleaned_config = remove_volatile_fields(config_copy)
        
        # Create a stable, normalized string representation
        # Sort keys at all levels for consistency
        config_str = json.dumps(
            cleaned_config,
            sort_keys=True,
            ensure_ascii=True,
            separators=(',', ':'),
            # Handle any non-serializable objects
            default=str
        )
        
        # Generate SHA-256 hash
        checksum = hashlib.sha256(config_str.encode('utf-8')).hexdigest()
        
        # Return first 16 characters for brevity while maintaining good collision resistance
        short_checksum = checksum[:16]
        
        logger.debug(f"Generated config checksum: {short_checksum} (from {len(config_str)} chars)")
        
        return short_checksum
        
    except Exception as e:
        logger.warning(f"Failed to generate config checksum: {e}")
        # Fallback checksum based on timestamp and basic config info
        try:
            fallback_data = {
                'timestamp': datetime.now().isoformat(),
                'config_keys': sorted(config.keys()) if isinstance(config, dict) else [],
                'config_size': len(str(config))
            }
            fallback_str = json.dumps(fallback_data, sort_keys=True)
            fallback_hash = hashlib.sha256(fallback_str.encode('utf-8')).hexdigest()
            # 'fb' prefix indicates fallback
            return f"fb_{fallback_hash[:14]}"
        except:
            # Minimal fallback
            return f"err_{int(time.time() * 1000) % 1000000:06d}"

def attempt_json_recovery(content: str, config_path: Path) -> Optional[Dict]:
    """Attempt to recover from common JSON formatting errors with enhanced recovery strategies."""
    try:
        logger.info(f"Attempting JSON recovery for {config_path}")
        
        # Strategy 1: Remove trailing commas before closing brackets/braces
        fixed_content = content
        
        # Remove trailing commas before closing brackets
        fixed_content = re.sub(r',(\s*[\]}])', r'\1', fixed_content)
        
        # Remove comments (// and /* */ style)
        fixed_content = re.sub(r'//.*?$', '', fixed_content, flags=re.MULTILINE)
        fixed_content = re.sub(r'/\*.*?\*/', '', fixed_content, flags=re.DOTALL)
        
        # Try parsing the fixed content
        try:
            recovered_data = json.loads(fixed_content)
            logger.info(f"Successfully recovered JSON using trailing comma fix")
            return recovered_data
        except json.JSONDecodeError:
            pass
        
        # Strategy 2: Fix common quote issues
        # Replace single quotes with double quotes (be careful about strings)
        fixed_content = content
        
        # Simple single quote to double quote replacement
        # This is naive but works for many simple cases
        fixed_content = re.sub(r"'([^']*)'(\s*:)", r'"\1"\2', fixed_content)
        fixed_content = re.sub(r":\s*'([^']*)'", r': "\1"', fixed_content)
        
        try:
            recovered_data = json.loads(fixed_content)
            logger.info(f"Successfully recovered JSON using quote fix")
            return recovered_data
        except json.JSONDecodeError:
            pass
        
        # Strategy 3: Try to fix missing quotes around keys
        fixed_content = content
        
        # Add quotes around unquoted keys (simple pattern)
        fixed_content = re.sub(r'(\n\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', fixed_content)
        fixed_content = re.sub(r'(\{\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', fixed_content)
        
        try:
            recovered_data = json.loads(fixed_content)
            logger.info(f"Successfully recovered JSON using key quote fix")
            return recovered_data
        except json.JSONDecodeError:
            pass
        
        # Strategy 4: Try to extract valid JSON from corrupted file
        # Look for the largest valid JSON object/array
        json_start_patterns = [r'\{', r'\[']
        json_end_patterns = [r'\}', r'\]']
        
        for start_pattern, end_pattern in zip(json_start_patterns, json_end_patterns):
            matches = list(re.finditer(start_pattern, content))
            for match in matches:
                start_pos = match.start()
                
                # Try to find matching closing bracket/brace
                bracket_count = 0
                end_pos = -1
                in_string = False
                escape_next = False
                
                for i, char in enumerate(content[start_pos:], start_pos):
                    if escape_next:
                        escape_next = False
                        continue
                    
                    if char == '\\':
                        escape_next = True
                        continue
                    
                    if char == '"' and not escape_next:
                        in_string = not in_string
                        continue
                    
                    if not in_string:
                        if re.match(start_pattern, char):
                            bracket_count += 1
                        elif re.match(end_pattern, char):
                            bracket_count -= 1
                            if bracket_count == 0:
                                end_pos = i + 1
                                break
                
                if end_pos > start_pos:
                    try:
                        candidate_json = content[start_pos:end_pos]
                        recovered_data = json.loads(candidate_json)
                        logger.info(f"Successfully recovered JSON by extraction ({len(candidate_json)} chars)")
                        return recovered_data
                    except json.JSONDecodeError:
                        continue
        
        # Strategy 5: Try eval() for Python-like dictionaries (DANGEROUS - only for trusted configs)
        if config_path and config_path.suffix == '.json':
            try:
                # Only attempt this if the content looks like a Python dict
                if content.strip().startswith('{') and 'True' in content or 'False' in content:
                    logger.warning("Attempting dangerous eval() recovery - ensure config file is trusted")
                    
                    # Basic safety checks
                    dangerous_patterns = ['import', 'exec', 'eval', '__', 'open', 'file']
                    if not any(pattern in content.lower() for pattern in dangerous_patterns):
                        
                        # Replace Python booleans with JSON booleans
                        python_content = content
                        python_content = re.sub(r'\bTrue\b', 'true', python_content)
                        python_content = re.sub(r'\bFalse\b', 'false', python_content)
                        python_content = re.sub(r'\bNone\b', 'null', python_content)
                        
                        try:
                            recovered_data = json.loads(python_content)
                            logger.warning("Successfully recovered JSON using Python-to-JSON conversion")
                            return recovered_data
                        except json.JSONDecodeError:
                            pass
            except Exception as eval_error:
                logger.debug(f"Eval recovery failed safely: {eval_error}")
        
        # Strategy 6: Line-by-line recovery for corrupted multi-line JSON
        lines = content.split('\n')
        recovered_lines = []
        
        for line in lines:
            # Skip obviously corrupted lines
            if line.strip() and not line.strip().startswith('#'):
                # Try to fix common issues in individual lines
                fixed_line = line
                
                # Remove trailing commas at end of line
                fixed_line = re.sub(r',\s*$', '', fixed_line)
                
                # Add missing commas between adjacent values
                if '"' in fixed_line and ':' in fixed_line:
                    # This line looks like it contains a key-value pair
                    recovered_lines.append(fixed_line)
                elif line.strip() in ['{', '}', '[', ']']:
                    # Structural characters
                    recovered_lines.append(fixed_line)
        
        if recovered_lines:
            try:
                reconstructed = '\n'.join(recovered_lines)
                recovered_data = json.loads(reconstructed)
                logger.info(f"Successfully recovered JSON using line-by-line reconstruction")
                return recovered_data
            except json.JSONDecodeError:
                pass
        
        logger.warning(f"All JSON recovery strategies failed for {config_path}")
        return None
        
    except Exception as e:
        logger.warning(f"JSON recovery attempt failed with exception: {e}")
        return None

def try_load_from_backup(config_path: Path) -> Optional[Dict]:
    """Try to load configuration from the most recent backup with enhanced backup discovery."""
    try:
        logger.info(f"Attempting to load from backup for {config_path}")
        
        # Strategy 1: Look in standard backup directory
        backup_dir = config_path.parent / "backups"
        backup_configs = []
        
        if backup_dir.exists():
            # Find backup files for this config
            backup_pattern = f"{config_path.stem}_v*{config_path.suffix}"
            backup_files = list(backup_dir.glob(backup_pattern))
            
            for backup_file in backup_files:
                try:
                    stat_info = backup_file.stat()
                    backup_configs.append({
                        'path': backup_file,
                        'mtime': stat_info.st_mtime,
                        'size': stat_info.st_size,
                        'age_hours': (time.time() - stat_info.st_mtime) / 3600
                    })
                except OSError:
                    continue
        
        # Strategy 2: Look for .bak files in the same directory
        bak_file = config_path.with_suffix(config_path.suffix + '.bak')
        if bak_file.exists():
            try:
                stat_info = bak_file.stat()
                backup_configs.append({
                    'path': bak_file,
                    'mtime': stat_info.st_mtime,
                    'size': stat_info.st_size,
                    'age_hours': (time.time() - stat_info.st_mtime) / 3600,
                    'type': 'bak_file'
                })
            except OSError:
                pass
        
        # Strategy 3: Look for auto-saved configurations
        auto_save_patterns = [
            config_path.with_name(f"{config_path.stem}.autosave{config_path.suffix}"),
            config_path.with_name(f"{config_path.stem}_backup{config_path.suffix}"),
            config_path.with_name(f"backup_{config_path.name}")
        ]
        
        for auto_save_path in auto_save_patterns:
            if auto_save_path.exists():
                try:
                    stat_info = auto_save_path.stat()
                    backup_configs.append({
                        'path': auto_save_path,
                        'mtime': stat_info.st_mtime,
                        'size': stat_info.st_size,
                        'age_hours': (time.time() - stat_info.st_mtime) / 3600,
                        'type': 'auto_save'
                    })
                except OSError:
                    continue
        
        if not backup_configs:
            logger.info("No backup configurations found")
            return None
        
        # Sort by modification time (newest first)
        backup_configs.sort(key=lambda x: x['mtime'], reverse=True)
        
        logger.info(f"Found {len(backup_configs)} potential backup configurations")
        
        # Try to load backups in order of preference
        for i, backup_info in enumerate(backup_configs):
            backup_path = backup_info['path']
            age_hours = backup_info['age_hours']
            
            try:
                logger.info(f"Trying backup {i+1}: {backup_path} (age: {age_hours:.1f}h)")
                
                # Basic file validation
                if backup_info['size'] == 0:
                    logger.debug(f"Skipping empty backup: {backup_path}")
                    continue
                
                # Skip very old backups (older than 30 days) unless it's the only option
                if age_hours > 24 * 30 and i < len(backup_configs) - 1:
                    logger.debug(f"Skipping old backup: {backup_path} ({age_hours:.1f}h old)")
                    continue
                
                # Try to load the backup
                backup_config = load_config(backup_path)
                
                if backup_config:
                    # Validate the backup configuration
                    if isinstance(backup_config, dict) and backup_config:
                        # Check if it has required sections
                        required_sections = ['training', 'model', 'data']
                        missing_sections = [s for s in required_sections if s not in backup_config]
                        
                        # Allow one missing section
                        if len(missing_sections) <= 1:
                            logger.info(f"Successfully loaded backup from: {backup_path}")
                            
                            # Add backup metadata to the loaded config
                            if 'metadata' not in backup_config:
                                backup_config['metadata'] = {}
                            
                            backup_config['metadata']['loaded_from_backup'] = {
                                'backup_path': str(backup_path),
                                'backup_age_hours': age_hours,
                                'original_path': str(config_path),
                                'loaded_at': datetime.now().isoformat(),
                                'backup_type': backup_info.get('type', 'standard')
                            }
                            
                            return backup_config
                        else:
                            logger.debug(f"Backup missing too many required sections: {missing_sections}")
                    else:
                        logger.debug(f"Backup config is not a valid dictionary: {type(backup_config)}")
                else:
                    logger.debug(f"Backup config is empty: {backup_path}")
                    
            except Exception as e:
                logger.debug(f"Failed to load backup {backup_path}: {e}")
                continue
        
        logger.warning("All backup loading attempts failed")
        return None
        
    except Exception as e:
        logger.warning(f"Error during backup loading process: {e}")
        return None

# Extracted validation logic from validate_config() into reusable validators
class ConfigSectionValidators:
    """Centralized validation logic extracted from validate_config() for reuse across update functions."""
    
    @staticmethod
    def validate_model_parameters(model: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        """Extract model validation logic from validate_config()."""
        errors = []
        warnings = []
        
        # Model type validation with MODEL_VARIANTS checking
        model_type = model.get('model_type', 'SimpleAutoencoder')
        if MODEL_VARIANTS:
            if model_type not in MODEL_VARIANTS:
                errors.append(f"Invalid model_type: {model_type}, available: {list(MODEL_VARIANTS.keys())}")
        else:
            valid_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
            if model_type not in valid_types:
                errors.append(f"Invalid model_type: {model_type}, expected one of: {valid_types}")
        
        # Encoding dimension validation
        encoding_dim = model.get('encoding_dim', 8)
        if not isinstance(encoding_dim, int) or encoding_dim <= 0:
            errors.append(f"Invalid encoding_dim: {encoding_dim}, must be positive integer")
        elif encoding_dim > 1000:
            warnings.append(f"Very large encoding_dim: {encoding_dim}, may cause memory issues")
        elif encoding_dim < 2:
            warnings.append(f"Very small encoding_dim: {encoding_dim}, may limit expressiveness")
        
        # Architecture validation
        hidden_dims = model.get('hidden_dims', [])
        if not isinstance(hidden_dims, list):
            errors.append("hidden_dims must be a list")
        elif not hidden_dims:
            errors.append("hidden_dims cannot be empty")
        else:
            invalid_dims = [dim for dim in hidden_dims if not isinstance(dim, int) or dim <= 0]
            if invalid_dims:
                errors.append(f"Invalid hidden dimensions: {invalid_dims}")
            
            if len(hidden_dims) > 10:
                warnings.append(f"Very deep architecture ({len(hidden_dims)} layers) may be hard to train")
            
            large_dims = [dim for dim in hidden_dims if dim > 2048]
            if large_dims:
                warnings.append(f"Very large hidden dimensions: {large_dims}")
        
        # Dropout validation
        dropout_rates = model.get('dropout_rates', [])
        if not isinstance(dropout_rates, list):
            errors.append("dropout_rates must be a list")
        elif not dropout_rates:
            errors.append("dropout_rates cannot be empty")
        else:
            invalid_rates = [rate for rate in dropout_rates if not isinstance(rate, (int, float)) or not (0 <= rate < 1)]
            if invalid_rates:
                errors.append(f"Invalid dropout rates: {invalid_rates}, must be between 0 and 1")
            
            if len(hidden_dims) != len(dropout_rates):
                errors.append(f"Length mismatch: {len(hidden_dims)} hidden layers, {len(dropout_rates)} dropout rates")
            
            if all(rate == 0 for rate in dropout_rates):
                warnings.append("No dropout applied (all rates are 0)")
            elif any(rate > 0.8 for rate in dropout_rates):
                warnings.append("Very high dropout rates may hurt performance")
        
        # Activation function validation
        activation = model.get('activation', 'relu')
        available_activations = model.get('available_activations', ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish'])
        if activation not in available_activations:
            errors.append(f"Invalid activation: {activation}, available: {available_activations}")
        
        # Normalization validation
        normalization = model.get('normalization')
        available_normalizations = model.get('available_normalizations', ['batch', 'layer', 'instance', 'group', None])
        if normalization not in available_normalizations:
            errors.append(f"Invalid normalization: {normalization}, available: {available_normalizations}")

        use_batch_norm = model.get('use_batch_norm', False)
        use_layer_norm = model.get('use_layer_norm', False)

        # Fix inconsistencies: ensure normalization setting matches use_*_norm flags
        if normalization is None:
            # If no normalization specified, both flags should be False
            if use_batch_norm or use_layer_norm:
                warnings.append("normalization=None but batch_norm or layer_norm enabled - auto-correcting to disable both")
                model['use_batch_norm'] = False
                model['use_layer_norm'] = False
        elif normalization == 'batch':
            # If batch normalization specified, use_batch_norm should be True, use_layer_norm should be False
            if not use_batch_norm:
                warnings.append("normalization='batch' but use_batch_norm=False - auto-correcting to use_batch_norm=True")
                model['use_batch_norm'] = True
            if use_layer_norm:
                warnings.append("normalization='batch' but use_layer_norm=True - auto-correcting to use_layer_norm=False")
                model['use_layer_norm'] = False
        elif normalization == 'layer':
            # If layer normalization specified, use_layer_norm should be True, use_batch_norm should be False
            if not use_layer_norm:
                warnings.append("normalization='layer' but use_layer_norm=False - auto-correcting to use_layer_norm=True")
                model['use_layer_norm'] = True
            if use_batch_norm:
                warnings.append("normalization='layer' but use_batch_norm=True - auto-correcting to use_batch_norm=False")
                model['use_batch_norm'] = False
        elif normalization in ['instance', 'group']:
            # For instance/group normalization, both batch_norm and layer_norm should be False
            if use_batch_norm or use_layer_norm:
                warnings.append(f"normalization='{normalization}' but batch_norm or layer_norm enabled - auto-correcting to disable both")
                model['use_batch_norm'] = False
                model['use_layer_norm'] = False

        # Final validation check - should not have both enabled after auto-corrections
        final_use_batch_norm = model.get('use_batch_norm', False)
        final_use_layer_norm = model.get('use_layer_norm', False)
        if final_use_batch_norm and final_use_layer_norm:
            warnings.append("Both batch_norm and layer_norm still enabled after auto-correction - may cause conflicts")
        
        # Advanced features validation
        if model.get('skip_connection', False) and len(hidden_dims) < 2:
            warnings.append("Skip connections are most effective with deeper architectures")
        
        if model.get('residual_blocks', False) and len(hidden_dims) < 3:
            warnings.append("Residual blocks are most effective with deeper architectures")
        
        if model.get('use_attention', False) and model_type == 'SimpleAutoencoder':
            warnings.append("Attention mechanism may not be supported in SimpleAutoencoder")
        
        # Ensemble-specific validation
        if model_type == 'AutoencoderEnsemble':
            num_models = model.get('num_models', 1)
            if not isinstance(num_models, int) or num_models < 1:
                errors.append(f"Invalid num_models: {num_models}, must be positive integer")
            elif num_models < 2:
                warnings.append("Ensemble should have at least 2 models for effectiveness")
            elif num_models > 20:
                warnings.append(f"Large ensemble ({num_models} models) may require significant memory")
        
        return errors, warnings
    
    @staticmethod
    def validate_training_parameters(training: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        """Extract training validation logic from validate_config()."""
        errors = []
        warnings = []
        
        # Core parameter validation
        batch_size = training.get('batch_size', 32)
        if not isinstance(batch_size, int) or batch_size < 1:
            errors.append(f"Invalid batch_size: {batch_size}, must be positive integer")
        elif batch_size > 2048:
            warnings.append(f"Very large batch_size: {batch_size}, may cause memory issues")
        elif batch_size < 2:
            warnings.append(f"Very small batch_size: {batch_size}, may cause training instability")
        
        epochs = training.get('epochs', 100)
        if not isinstance(epochs, int) or epochs < 1:
            errors.append(f"Invalid epochs: {epochs}, must be positive integer")
        elif epochs > 5000:
            warnings.append(f"Very high epoch count: {epochs}, consider early stopping")
        elif epochs < 5:
            warnings.append(f"Very low epoch count: {epochs}, may not converge properly")
        
        learning_rate = training.get('learning_rate', 0.001)
        if not isinstance(learning_rate, (int, float)) or learning_rate <= 0:
            errors.append(f"Invalid learning_rate: {learning_rate}, must be positive number")
        elif learning_rate > 1.0:
            warnings.append(f"Very high learning_rate: {learning_rate}, may cause instability")
        elif learning_rate < 1e-8:
            warnings.append(f"Very low learning_rate: {learning_rate}, may not converge")
        
        # Optimizer validation
        optimizer = training.get('optimizer', 'Adam')
        valid_optimizers = ['SGD', 'Adam', 'AdamW', 'RMSprop', 'Adagrad', 'LBFGS']
        if optimizer not in valid_optimizers:
            errors.append(f"Invalid optimizer: {optimizer}, must be one of {valid_optimizers}")
        
        # Scheduler validation
        scheduler = training.get('scheduler')
        if scheduler is not None:
            valid_schedulers = [
                'StepLR', 'MultiStepLR', 'ExponentialLR', 'CosineAnnealingLR',
                'ReduceLROnPlateau', 'CosineAnnealingWarmRestarts', 'OneCycleLR', 'CyclicLR'
            ]
            if scheduler not in valid_schedulers:
                errors.append(f"Invalid scheduler: {scheduler}, must be one of {valid_schedulers}")
        
        # Hardware-aware validation
        if training.get('mixed_precision', False) and not torch.cuda.is_available():
            warnings.append("mixed_precision enabled but CUDA not available")
        
        num_workers = training.get('num_workers', 1)
        max_workers = (os.cpu_count() or 1) * 2
        if not isinstance(num_workers, int) or num_workers < 0:
            errors.append("num_workers must be non-negative integer")
        elif num_workers > max_workers:
            warnings.append(f"num_workers ({num_workers}) exceeds recommended max ({max_workers})")
        
        return errors, warnings
    
    @staticmethod
    def validate_presets_parameters(presets: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        """Extract presets validation logic from validate_config()."""
        errors = []
        warnings = []
        
        current_preset = presets.get('current_preset')
        if current_preset is not None:
            available_presets = get_available_presets()
            if current_preset not in available_presets:
                errors.append(f"Invalid current_preset: {current_preset}, available: {available_presets}")
        
        # Override rules validation
        if 'override_rules' in presets:
            override_rules = presets['override_rules']
            if not isinstance(override_rules, dict):
                errors.append("override_rules must be a dictionary")
            else:
                valid_sections = ['security', 'monitoring', 'hardware', 'training', 'model', 'data']
                for section, enabled in override_rules.items():
                    if section not in valid_sections:
                        warnings.append(f"Unknown override rule section: {section}")
                    elif not isinstance(enabled, bool):
                        errors.append(f"Override rule for {section} must be boolean")
        
        auto_apply = presets.get('auto_apply', False)
        if not isinstance(auto_apply, bool):
            errors.append("auto_apply must be boolean")
        
        return errors, warnings
    
    @staticmethod
    def validate_cross_section_compatibility(config: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        """Extract cross-section validation logic from validate_config()."""
        errors = []
        warnings = []
        
        # Model-Training compatibility
        if 'model' in config and 'training' in config:
            model_config = config['model']
            training_config = config['training']
            
            if model_config.get('use_batch_norm') and training_config.get('batch_size', 32) < 2:
                errors.append("Batch normalization requires training batch_size >= 2")
            
            if training_config.get('mixed_precision') and model_config.get('model_type') == 'SimpleAutoencoder':
                warnings.append("Mixed precision may not be fully supported with SimpleAutoencoder")
        
        # Hardware-Training compatibility
        if 'hardware' in config and 'training' in config:
            hardware_config = config['hardware']
            training_config = config['training']
            
            device = hardware_config.get('device', 'auto')
            use_cuda = hardware_config.get('performance_optimization', {}).get('use_cuda', False)
            
            if device == 'cpu' and use_cuda:
                warnings.append("Device set to CPU but use_cuda=True in performance optimization")
            elif device == 'cuda' and not use_cuda:
                warnings.append("Device set to CUDA but use_cuda=False in performance optimization")
        
        # Data-Model compatibility
        if 'data' in config and 'model' in config:
            data_config = config['data']
            model_config = config['model']
            
            data_features = data_config.get('features', 20)
            min_features = model_config.get('min_features', 5)
            
            if data_features < min_features:
                errors.append(f"Data features ({data_features}) < model min_features ({min_features})")
        
        return errors, warnings

# Simplified parameter validators using the extracted logic
class ParameterValidator:
    """Simplified parameter validation using extracted validation logic."""
    
    @staticmethod
    def validate_and_update_with_feedback(section_name: str, result_config: Dict, 
                                        update_config: Dict, changes_made: List[Dict]) -> bool:
        """
        Validate and update configuration section using extracted validation logic.
        
        Returns:
            bool: True if validation passed (warnings allowed), False if errors found
        """
        # Create temporary config for validation
        temp_config = result_config.copy()
        temp_config.update(update_config)
        
        # Get validation results based on section
        if section_name == 'model':
            errors, warnings = ConfigSectionValidators.validate_model_parameters(temp_config)
        elif section_name == 'training':
            errors, warnings = ConfigSectionValidators.validate_training_parameters(temp_config)
        elif section_name == 'presets':
            errors, warnings = ConfigSectionValidators.validate_presets_parameters(temp_config)
        else:
            # For other sections, allow updates without validation
            errors, warnings = [], []
        
        # Log validation results
        for error in errors:
            logger.error(f"{section_name}: {error}")
        
        for warning in warnings:
            logger.warning(f"{section_name}: {warning}")
        
        # If validation passed (no errors), apply updates
        if not errors:
            for key, value in update_config.items():
                if value is not None:
                    old_value = result_config.get(key)
                    if old_value != value:
                        result_config[key] = value
                        changes_made.append({
                            'key': f'{section_name}.{key}',
                            'old_value': old_value,
                            'new_value': value,
                            'timestamp': datetime.now().isoformat(),
                            'validation_passed': True,
                            'warnings': len(warnings)
                        })
            return True
        else:
            # Log validation failure
            changes_made.append({
                'key': f'{section_name}.validation_failed',
                'old_value': None,
                'new_value': f'Errors: {len(errors)}, Warnings: {len(warnings)}',
                'timestamp': datetime.now().isoformat(),
                'errors': errors,
                'warnings': warnings
            })
            return False

def deep_update(original: Dict[str, Any], update: Dict[str, Any]) -> Dict[str, Any]:
    """Simplified recursive update using centralized validation."""
    if not isinstance(original, dict) or not isinstance(update, dict):
        raise ValueError("Both original and update must be dictionaries")
    
    changes_made = []
    
    # Handle sections with specialized logic
    specialized_sections = {'model', 'training', 'presets', 'metadata'}
    
    for key, value in update.items():
        try:
            if key in original and isinstance(original[key], dict) and isinstance(value, dict):
                if key == 'model':
                    # Integrated deep_update_model_section functionality
                    result_model = original[key].copy()
                    
                    # Use centralized validation for basic parameters
                    basic_params = {k: v for k, v in value.items() 
                                  if k not in ['hidden_dims', 'dropout_rates', 'num_models', 'diversity_factor']}
                    
                    validation_passed = ParameterValidator.validate_and_update_with_feedback(
                        'model', result_model, basic_params, changes_made
                    )
                    
                    if not validation_passed:
                        logger.warning("Model validation failed, skipping basic parameter updates")
                    
                    # Handle architecture updates (hidden_dims and dropout_rates with length matching)
                    if 'hidden_dims' in value or 'dropout_rates' in value:
                        # Validate and normalize architecture parameters
                        hidden_dims = value.get('hidden_dims', result_model.get('hidden_dims', []))
                        dropout_rates = value.get('dropout_rates', result_model.get('dropout_rates', []))
                        
                        # Ensure both are lists
                        if not isinstance(hidden_dims, list):
                            hidden_dims = [hidden_dims] if isinstance(hidden_dims, int) else []
                        if not isinstance(dropout_rates, list):
                            dropout_rates = [dropout_rates] if isinstance(dropout_rates, (int, float)) else []
                        
                        # Match lengths
                        if len(hidden_dims) != len(dropout_rates):
                            target_length = max(len(hidden_dims), len(dropout_rates))
                            
                            # Extend shorter list
                            while len(hidden_dims) < target_length:
                                hidden_dims.append(hidden_dims[-1] if hidden_dims else 64)
                            while len(dropout_rates) < target_length:
                                dropout_rates.append(dropout_rates[-1] if dropout_rates else 0.2)
                        
                        # Update with validated values
                        result_model['hidden_dims'] = hidden_dims
                        result_model['dropout_rates'] = dropout_rates
                        
                        changes_made.append({
                            'key': 'model.architecture_sync',
                            'old_value': {'hidden': len(result_model.get('hidden_dims', [])), 
                                         'dropout': len(result_model.get('dropout_rates', []))},
                            'new_value': {'hidden': len(hidden_dims), 'dropout': len(dropout_rates)},
                            'timestamp': datetime.now().isoformat()
                        })
                    
                    # Handle ensemble-specific parameters with validation
                    if result_model.get('model_type') == 'AutoencoderEnsemble':
                        for param in ['num_models', 'diversity_factor']:
                            if param in value:
                                param_value = value[param]
                                
                                # Simple validation
                                if param == 'num_models' and isinstance(param_value, int) and 1 <= param_value <= 20:
                                    result_model[param] = param_value
                                elif param == 'diversity_factor' and isinstance(param_value, (int, float)) and 0 <= param_value <= 1:
                                    result_model[param] = param_value
                                else:
                                    logger.warning(f"Invalid ensemble parameter {param}: {param_value}")
                    
                    # Ensure model metadata lists are consistent
                    if 'available_activations' not in result_model:
                        result_model['available_activations'] = ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish']
                    
                    if 'available_normalizations' not in result_model:
                        result_model['available_normalizations'] = ['batch', 'layer', 'instance', 'group', None]
                    
                    original[key] = result_model
                    
                elif key == 'training':
                    # Integrated deep_update_training_section functionality
                    result_training = original[key].copy()
                    
                    # Use centralized validation for basic parameters
                    basic_params = {k: v for k, v in value.items() 
                                  if k not in ['scheduler_params', 'adam_betas', 'adam_eps']}
                    
                    validation_passed = ParameterValidator.validate_and_update_with_feedback(
                        'training', result_training, basic_params, changes_made
                    )
                    
                    if not validation_passed:
                        logger.warning("Training validation failed, skipping basic parameter updates")
                    
                    # Handle optimizer-specific parameters
                    optimizer = result_training.get('optimizer', 'Adam')
                    
                    if optimizer in ['Adam', 'AdamW']:
                        if 'adam_betas' in value:
                            betas = value['adam_betas']
                            if isinstance(betas, (list, tuple)) and len(betas) == 2:
                                result_training['adam_betas'] = betas
                        
                        if 'adam_eps' in value:
                            eps = value['adam_eps']
                            if isinstance(eps, (int, float)) and eps > 0:
                                result_training['adam_eps'] = eps
                    
                    # Handle scheduler parameters
                    if 'scheduler_params' in value and result_training.get('scheduler'):
                        result_training['scheduler_params'] = value['scheduler_params']
                    
                    original[key] = result_training
                    
                elif key == 'presets':
                    # Integrated deep_update_presets_section functionality
                    result_presets = original[key].copy()
                    
                    # Use centralized validation
                    validation_passed = ParameterValidator.validate_and_update_with_feedback(
                        'presets', result_presets, value, changes_made
                    )
                    
                    if not validation_passed:
                        logger.warning("Presets validation failed, skipping updates")
                    else:
                        # Update dynamic preset information
                        try:
                            result_presets['available_presets'] = get_available_presets()
                            result_presets['last_dynamic_update'] = datetime.now().isoformat()
                        except Exception as e:
                            logger.debug(f"Failed to update dynamic preset info: {e}")
                        
                        # Generate simple preset recommendations
                        try:
                            if torch.cuda.is_available():
                                result_presets['system_recommendations'] = ['performance', 'advanced']
                            else:
                                result_presets['system_recommendations'] = ['lightweight', 'debug']
                        except Exception as e:
                            logger.debug(f"Failed to generate preset recommendations: {e}")
                        
                        # Update preset usage statistics
                        if 'usage_statistics' not in result_presets:
                            result_presets['usage_statistics'] = {
                                'total_preset_changes': 0,
                                'last_preset_change': datetime.now().isoformat()
                            }
                        
                        if 'current_preset' in value:
                            stats = result_presets['usage_statistics']
                            stats['total_preset_changes'] += 1
                            stats['last_preset_change'] = datetime.now().isoformat()
                    
                    original[key] = result_presets
                    
                elif key == 'metadata':
                    # Integrated deep_update_metadata_section functionality
                    result_metadata = deepcopy(original[key]) if original[key] else {}
                    
                    # Metadata doesn't need the same validation as other sections
                    # Just handle the special metadata logic
                    current_time = datetime.now().isoformat()
                    result_metadata['modified'] = current_time
                    result_metadata['last_updated'] = current_time
                    
                    # Handle version tracking
                    if 'version' in value:
                        old_version = result_metadata.get('version')
                        result_metadata['version'] = value['version']
                        changes_made.append({
                            'key': 'metadata.version',
                            'old_value': old_version,
                            'new_value': value['version'],
                            'timestamp': current_time
                        })
                    
                    # Handle system information
                    if 'system' in value:
                        if 'system' not in result_metadata:
                            result_metadata['system'] = {}
                        result_metadata['system'].update(value['system'])
                    
                    # Handle preset tracking
                    preset_fields = ['preset_used', 'current_preset', 'preset_source']
                    for field in preset_fields:
                        if field in value:
                            result_metadata[field] = value[field]
                    
                    # Handle migration metadata
                    if 'migration' in value:
                        result_metadata['migration'] = value['migration']
                    
                    # Handle validation metadata
                    if 'validation' in value:
                        result_metadata['validation'] = value['validation']
                    
                    # Handle remaining fields
                    for meta_key, meta_value in value.items():
                        if meta_key not in ['version', 'system', 'preset_used', 'migration', 'validation'] and meta_value is not None:
                            old_value = result_metadata.get(meta_key)
                            if old_value != meta_value:
                                result_metadata[meta_key] = meta_value
                                changes_made.append({
                                    'key': f'metadata.{meta_key}',
                                    'old_value': old_value,
                                    'new_value': meta_value,
                                    'timestamp': current_time
                                })
                    
                    original[key] = result_metadata
                    
                else:
                    # Standard recursive update for other sections
                    original[key] = deep_update(original[key], value)
            else:
                # Handle non-dict values
                if value is not None:
                    original[key] = value
        except Exception as e:
            logger.warning(f"Error updating key '{key}': {e}")
            continue
    
    # Apply cross-section validation using extracted logic
    if len([k for k in update.keys() if k in specialized_sections]) > 1:
        errors, warnings = ConfigSectionValidators.validate_cross_section_compatibility(original)
        
        for error in errors:
            logger.error(f"Cross-section validation error: {error}")
        
        for warning in warnings:
            logger.warning(f"Cross-section validation warning: {warning}")
        
        if errors:
            changes_made.append({
                'key': 'cross_section_validation',
                'old_value': 'unknown',
                'new_value': f'Failed with {len(errors)} errors',
                'timestamp': datetime.now().isoformat(),
                'errors': errors
            })
    
    # Log summary
    if changes_made:
        logger.debug(f"deep_update made {len(changes_made)} changes")
    
    return original

def get_current_config() -> Dict[str, Any]:
    """Return comprehensive configuration with preset awareness, caching, and validation.
    
    This function provides the current active configuration with intelligent fallback
    mechanisms, system-aware defaults, and comprehensive error recovery. It integrates
    with the updated preset system, configuration caching, and validation framework.
    
    Returns:
        Dictionary containing all configuration parameters with metadata,
        respecting any active preset configuration and system optimizations.
        
    Raises:
        RuntimeError: If configuration cannot be retrieved or validated
    """
    global _cached_config, _config_cache_time
    
    current_time = time.time()
    
    # Check cache validity (30 seconds) with comprehensive validation
    if (_cached_config is not None and _config_cache_time is not None and 
        current_time - _config_cache_time < 30):
        return _cached_config
    
    # Step 1: Attempt to load existing configuration
    loaded_config = None
    current_preset = None
    
    try:
        loaded_config = load_config()
        if loaded_config and isinstance(loaded_config, dict):
            current_preset = loaded_config.get('presets', {}).get('current_preset')
            logger.debug(f"Loaded existing configuration with preset: {current_preset}")
        else:
            logger.debug("No valid existing configuration found")
    except Exception as e:
        logger.debug(f"Failed to load existing configuration: {e}")
    
    # Step 2: Determine base configuration source with intelligent selection
    base_config = None
    config_source = None
    
    # Priority 1: Active preset configuration
    if current_preset and current_preset in PRESET_CONFIGS:
        try:
            base_config = deepcopy(PRESET_CONFIGS[current_preset])
            config_source = f'preset_{current_preset}'
            logger.debug(f"Using preset configuration: {current_preset}")
        except Exception as e:
            logger.warning(f"Failed to load preset {current_preset}: {e}")
    
    # Priority 2: Default preset if available
    if base_config is None and 'default' in PRESET_CONFIGS:
        try:
            base_config = deepcopy(PRESET_CONFIGS['default'])
            config_source = 'preset_default'
            logger.debug("Using default preset configuration")
        except Exception as e:
            logger.warning(f"Failed to load default preset: {e}")
    
    # Priority 3: System-aware default configuration
    if base_config is None:
        try:
            base_config = get_default_config()
            config_source = 'system_aware_default'
            logger.debug("Using system-aware default configuration")
        except Exception as e:
            logger.warning(f"Failed to generate system-aware defaults: {e}")
    
    # Priority 4: Minimal fallback configuration
    if base_config is None:
        try:
            base_config = _create_minimal_fallback_config('standard')
            config_source = 'minimal_fallback'
            logger.warning("Using minimal fallback configuration")
        except Exception as e:
            logger.error(f"Failed to create minimal fallback: {e}")
    
    # Priority 5: Emergency fallback
    if base_config is None:
        try:
            base_config = _create_minimal_fallback_config('emergency')
            config_source = 'emergency_fallback'
            logger.critical("Using emergency fallback configuration")
        except Exception as e:
            logger.critical(f"Emergency fallback failed: {e}")
            raise RuntimeError("Complete configuration system failure") from e
    
    # Step 3: Merge with loaded configuration if available
    if loaded_config and base_config:
        try:
            # Use deep_update to intelligently merge configurations
            merged_config = deep_update(base_config, loaded_config)
            config_source = f'{config_source}_merged'
            logger.debug("Successfully merged loaded configuration with base configuration")
            base_config = merged_config
        except Exception as e:
            logger.warning(f"Failed to merge configurations: {e}")
            # Continue with base_config only
    
    # Step 6: Add comprehensive runtime metadata
    current_timestamp = datetime.now().isoformat()
    
    # Ensure metadata section exists
    if 'metadata' not in base_config:
        base_config['metadata'] = {}
    
    # Update core metadata
    base_config['metadata'].update({
        'last_accessed': current_timestamp,
        'config_loaded_at': current_timestamp,
        'config_source': config_source,
        'config_version': '2.1',
        'generation_method': 'get_current_config_v2'
    })
    
    # Add runtime information
    if 'runtime' not in base_config:
        base_config['runtime'] = {}
    
    base_config['runtime'].update({
        'config_generated_at': current_timestamp,
        'config_loaded_at': current_timestamp,
        'config_source': config_source,
        'runtime_id': hashlib.md5(current_timestamp.encode()).hexdigest()[:8],
        'process_id': os.getpid(),
        'cache_status': 'refreshed',
        'validation_status': 'passed',
        'system_integration': True
    })
    
    # Step 7: Validate model variants compatibility
    try:
        model_type = base_config.get('model', {}).get('model_type')
        if model_type and MODEL_VARIANTS:
            if model_type not in MODEL_VARIANTS:
                logger.warning(f"Model type '{model_type}' not available in MODEL_VARIANTS")
                # Try to initialize model variants
                try:
                    #initialize_model_variants(silent=True)
                    initialize_model_variants(silent=False)
                    if model_type not in MODEL_VARIANTS:
                        # Fallback to available model type
                        available_types = list(MODEL_VARIANTS.keys())
                        fallback_type = available_types[0] if available_types else 'SimpleAutoencoder'
                        logger.warning(f"Falling back to model type: {fallback_type}")
                        base_config['model']['model_type'] = fallback_type
                except Exception as init_error:
                    logger.warning(f"Model variants initialization failed: {init_error}")
    except Exception as e:
        logger.debug(f"Model variants validation failed: {e}")
    
    # Step 8: Update preset information dynamically in case PRESET_CONFIGS was populated after creation
    try:
        if 'presets' in base_config:
            base_config['presets']['available_presets'] = get_available_presets()
            base_config['presets']['preset_configs'] = get_preset_descriptions()
            base_config['presets']['custom_presets_available'] = get_safe_custom_presets()
    except Exception as e:
        logger.debug(f"Failed to update preset information: {e}")
    
    # Step 10: Cache the final configuration
    _cached_config = base_config
    _config_cache_time = current_time
    
    # Log success with comprehensive statistics
    
    #return deepcopy(base_config)
    return base_config

def list_saved_configs() -> Dict[str, Any]:
    """List all saved configurations with metadata, including named configurations."""
    try:
        # Get regular config files
        config_files = list(CONFIG_DIR.glob("*.json"))
        config_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        # Get named configurations from registry
        named_configs = {}
        registry_path = CONFIG_DIR / "named_configs_registry.json"
        if registry_path.exists():
            try:
                with open(registry_path, 'r', encoding='utf-8') as f:
                    registry = json.load(f)
                named_configs = registry.get("configs", {})
            except Exception as e:
                logger.warning(f"Failed to read named config registry: {e}")
        
        # Build comprehensive result with metadata
        result = {
            "regular_configs": [],
            "named_configs": named_configs,
            "all_configs": {}
        }
        
        # Process regular config files
        for f in config_files:
            if f.stem == "current" or f.stem == "named_configs_registry":
                continue
                
            config_info = {
                "name": f.stem,
                "path": str(f),
                "type": "regular",
                "file_size": f.stat().st_size,
                "modified_time": f.stat().st_mtime,
                "modified": datetime.fromtimestamp(f.stat().st_mtime).isoformat(),
                "metadata": {}
            }
            
            # Try to load metadata from config file
            try:
                config_data = load_config(f)
                if isinstance(config_data, dict):
                    # Extract metadata from the loaded config structure
                    config_info["model_type"] = config_data.get('model', {}).get('model_type', 'N/A')
                    config_info["epochs"] = config_data.get('training', {}).get('epochs', 'N/A')
                    config_info["preset_used"] = config_data.get('presets', {}).get('current_preset', 'none')
                    
                    # Try to get additional metadata if available
                    try:
                        with open(f, 'r') as config_file:
                            full_config_data = json.load(config_file)
                        if isinstance(full_config_data, dict) and 'metadata' in full_config_data:
                            config_info["metadata"] = full_config_data.get('metadata', {})
                            if 'created' in config_info["metadata"]:
                                config_info["created"] = config_info["metadata"]["created"]
                    except Exception:
                        pass
            except Exception as e:
                logger.debug(f"Could not read metadata for {f.stem}: {e}")
                config_info["model_type"] = 'N/A'
                config_info["epochs"] = 'N/A'
                config_info["preset_used"] = 'none'
            
            result["regular_configs"].append(config_info)
            result["all_configs"][f.stem] = config_info
        
        # Add named configs to all_configs with enhanced metadata
        for name, named_info in named_configs.items():
            if name not in result["all_configs"]:
                enhanced_named_info = {
                    "name": name,
                    "path": named_info.get("path", ""),
                    "type": "named",
                    "metadata": named_info.get("config", {}),
                    "created": named_info.get("created"),
                    "modified": named_info.get("modified"),
                    "model_type": named_info.get("model_type", 'N/A'),
                    "preset_used": named_info.get("preset_used", 'none')
                }
                
                # Add file size and modification time if path exists
                try:
                    named_path = Path(named_info.get("path", ""))
                    if named_path.exists():
                        stat_info = named_path.stat()
                        enhanced_named_info["file_size"] = stat_info.st_size
                        enhanced_named_info["modified_time"] = stat_info.st_mtime
                        # Use file modification time if not in registry
                        if not enhanced_named_info.get("modified"):
                            enhanced_named_info["modified"] = datetime.fromtimestamp(stat_info.st_mtime).isoformat()
                except Exception:
                    enhanced_named_info["file_size"] = 0
                    enhanced_named_info["modified_time"] = 0
                
                result["all_configs"][name] = enhanced_named_info
        
        return result
        
    except Exception as e:
        logger.error(f"Error listing saved configs: {e}")
        return {"regular_configs": [], "named_configs": {}, "all_configs": {}}

def load_saved_config_interactive():
    """Interactive loading of saved configurations with support for named configs."""
    try:
        # clear screen and show banner
        console.clear()
        show_banner()
        
        saved_configs_info = list_saved_configs()
        regular_configs = saved_configs_info["regular_configs"]
        named_configs = saved_configs_info["named_configs"]
        all_configs = saved_configs_info["all_configs"]
        
        if not regular_configs and not named_configs:
            no_config_panel = Panel.fit(
                "[bold red]No saved configurations found[/bold red]\n"
                "Use 'save_config()' to save your current configuration first.",
                title="[bold yellow]WARNING[/bold yellow]",
                border_style="yellow",
                padding=(1, 2)
            )
            console.print(no_config_panel)
            return
        
        console.print("\n[bold yellow]Available Saved Configurations[/bold yellow]\n")
        
        # Create main table for saved configurations
        config_table = Table(
            box=box.ROUNDED,
            header_style="bold bright_magenta",
            border_style="bright_magenta",
            show_header=True,
            show_lines=True,
            width=min(100, console.width - 4)
        )
        
        # Define columns
        config_table.add_column("#", style="bold cyan", width=3, justify="center")
        config_table.add_column("Config Name", style="bold green", width=25)
        config_table.add_column("Type", style="bold yellow", width=10)
        config_table.add_column("File Info", style="bold blue", width=20)
        config_table.add_column("Config Details", style="bold", width=40)
        
        config_names = list(all_configs.keys())
        
        for i, config_name in enumerate(config_names, 1):
            config_info = all_configs[config_name]
            file_info = ""
            config_details = ""
            config_type = config_info.get("type", "regular")
            
            try:
                if config_type == "regular":
                    config_path = Path(config_info["path"])
                    if config_path.exists():
                        file_size = config_path.stat().st_size
                        file_time = datetime.fromtimestamp(config_path.stat().st_mtime)
                        file_info = f"{file_size/1024:.1f}KB\n{file_time.strftime('%Y-%m-%d')}"
                    
                    # Extract key details from metadata
                    metadata = config_info.get("metadata", {})
                    model_type = config_info.get("model_type", 'N/A')
                    epochs = config_info.get("epochs", 'N/A')
                    preset = metadata.get('preset_used', 'None')
                    
                    config_details = f"Model: {model_type}\nEpochs: {epochs}, Preset: {preset}"
                
                else:  # named config
                    named_info = named_configs[config_name]
                    config_path = Path(named_info.get("path", ""))
                    
                    if config_path.exists():
                        file_size = config_path.stat().st_size
                        file_time = datetime.fromtimestamp(config_path.stat().st_mtime)
                        file_info = f"{file_size/1024:.1f}KB\n{file_time.strftime('%Y-%m-%d')}"
                    
                    config_data = named_info.get("config", {})
                    model_type = config_data.get('model_type', 'N/A')
                    preset = config_data.get('preset_used', 'None')
                    
                    config_details = f"Model: {model_type}\nPreset: {preset}\n[Named Config]"
                
            except Exception as e:
                config_details = f"[bold red]Error reading: {str(e)[:30]}...[/bold red]"
            
            config_table.add_row(
                str(i),
                config_name,
                "[bold green]Named[/bold green]" if config_type == "named" else "Regular",
                file_info,
                config_details
            )
        
        console.print(config_table)
        
        # Selection options
        max_choice = len(config_names)
        cancel_message = f"Select [bold red]0[/bold red] to cancel selection"
        selection_message = f"Select Configuration between (1-{max_choice})"
        
        # Get user input
        choice = console.input(f"\n[bold yellow]{selection_message} or {cancel_message}: [/bold yellow]").strip()
        
        if choice == "0":
            console.print("[bold red]Loading cancelled[/bold red]")
            return
        
        if choice.isdigit() and 1 <= int(choice) <= max_choice:
            selected_name = config_names[int(choice)-1]
            config_info = all_configs[selected_name]
            
            # Show confirmation panel with file details
            try:
                if config_info["type"] == "regular":
                    config_path = Path(config_info["path"])
                    file_size = config_path.stat().st_size
                    file_time = datetime.fromtimestamp(config_path.stat().st_mtime)
                    file_info = f"Size: {file_size/1024:.1f}KB, Modified: {file_time.strftime('%Y-%m-%d %H:%M')}"
                    
                    # Load config for preview using the unified load_config function
                    config_data = load_config(config_path)
                    
                    metadata = config_info.get("metadata", {})
                    model_type = config_data.get('model', {}).get('model_type', 'N/A')
                    epochs = config_data.get('training', {}).get('epochs', 'N/A')
                    batch_size = config_data.get('training', {}).get('batch_size', 'N/A')
                    preset = metadata.get('preset_used', 'None')
                    
                else:  # named config
                    named_info = named_configs[selected_name]
                    config_path = Path(named_info.get("path", ""))
                    
                    if config_path.exists():
                        file_size = config_path.stat().st_size
                        file_time = datetime.fromtimestamp(config_path.stat().st_mtime)
                        file_info = f"Size: {file_size/1024:.1f}KB, Modified: {file_time.strftime('%Y-%m-%d %H:%M')}"
                    
                    config_data = named_info.get("config", {})
                    model_type = config_data.get('model_type', 'N/A')
                    preset = config_data.get('preset_used', 'None')
                    epochs = 'N/A'
                    batch_size = 'N/A'
                    metadata = {"created": named_info.get("created", "Unknown")}
                
                confirm_panel = Panel.fit(
                    f"[bold]Configuration:[/bold] [green]{selected_name}[/green]\n"
                    f"[bold]Type:[/bold] {'Named' if config_info['type'] == 'named' else 'Regular'}\n"
                    f"[bold]File:[/bold] {file_info}\n"
                    f"[bold]Model:[/bold] {model_type}\n"
                    f"[bold]Training:[/bold] {epochs} epochs, Batch: {batch_size}\n"
                    f"[bold]Preset:[/bold] {preset}\n"
                    f"[bold]Created:[/bold] {metadata.get('created', 'Unknown')}",
                    title="[bold]CONFIGURATION PREVIEW[/bold]",
                    border_style="green",
                    padding=(1, 2)
                )
                console.print(confirm_panel)
                
            except Exception as e:
                error_panel = Panel.fit(
                    f"[bold red]Error reading configuration: {e}[/bold red]",
                    title="[bold]ERROR[/bold]",
                    border_style="red",
                    padding=(1, 2)
                )
                console.print(error_panel)
                return
            
            # Confirmation prompt
            confirm = console.input("\n[bold yellow]Load this configuration? (Y/n): [/bold yellow]").lower().strip()
            
            if confirm in ('', 'y', 'yes'):
                try:
                    # Use the unified load_config function
                    config = load_config(selected_name)
                    update_global_config(config)
                    
                    success_panel = Panel.fit(
                        f"[bold green]Successfully loaded configuration: {selected_name}[/bold green]\n"
                        f"All settings have been updated from the saved configuration.",
                        title="[bold]SUCCESS[/bold]",
                        border_style="green",
                        padding=(1, 2)
                    )
                    console.print(success_panel)
                    
                except Exception as e:
                    error_panel = Panel.fit(
                        f"[bold red]Failed to load configuration: {e}[/bold red]",
                        title="[bold]ERROR[/bold]",
                        border_style="red",
                        padding=(1, 2)
                    )
                    console.print(error_panel)
            else:
                console.print("[bold red]Loading cancelled[/bold red]")
        else:
            error_panel = Panel.fit(
                f"[bold yellow]Invalid selection: {choice}[/bold yellow]\n"
                f"{selection_message} or {cancel_message}:",
                title="[bold]WARNING[/bold]",
                border_style="yellow",
                padding=(1, 2)
            )
            console.print(error_panel)
            
    except Exception as e:
        error_panel = Panel.fit(
            f"[bold red]Error in configuration loading: {e}[/bold red]",
            title="[bold]FAILED[/bold]",
            border_style="red",
            padding=(1, 2)
        )
        console.print(error_panel)
        logger.error(f"Configuration loading failed: {e}")

# Helper functions for preset and configuration management
def migrate_config(legacy_config: Dict, new_template: Dict = None) -> Dict:
    """Migrate an older configuration to the current version using enhanced preset matching.
    
    Args:
        legacy_config: The old configuration dictionary to migrate from
        new_template: Optional template to use as base (defaults to best matching preset)
        
    Returns:
        New configuration dictionary with migrated values
        
    Raises:
        ValueError: If legacy_config is invalid
    """
    if not isinstance(legacy_config, dict):
        raise ValueError("legacy_config must be a dictionary")
    
    logger.info("Migrating configuration to current format")
    
    # Determine the best template to use
    if new_template is None:
        # Try to find the best matching preset
        try:
            if PRESET_CONFIGS:
                # Use convert_legacy_config's scoring system to find best match
                def score_preset_simple(preset_cfg: Dict[str, Any]) -> float:
                    """Simplified scoring for migration."""
                    score = 0.0
                    total_checks = 0
                    
                    # Check training parameters
                    for param in ['batch_size', 'learning_rate', 'epochs']:
                        if param in legacy_config and param in preset_cfg.get('training', {}):
                            legacy_val = legacy_config[param]
                            preset_val = preset_cfg['training'][param]
                            if isinstance(legacy_val, (int, float)) and isinstance(preset_val, (int, float)):
                                similarity = 1 - min(abs(legacy_val - preset_val) / max(abs(legacy_val), abs(preset_val), 1), 1)
                                score += similarity
                            elif legacy_val == preset_val:
                                score += 1
                            total_checks += 1
                    
                    # Check model parameters
                    for param in ['model_type', 'encoding_dim', 'activation']:
                        if param in legacy_config and param in preset_cfg.get('model', {}):
                            if legacy_config[param] == preset_cfg['model'][param]:
                                score += 1
                            total_checks += 1
                    
                    return score / max(total_checks, 1)
                
                best_preset = None
                best_score = 0
                
                for preset_name, preset_cfg in PRESET_CONFIGS.items():
                    score = score_preset_simple(preset_cfg)
                    if score > best_score:
                        best_score = score
                        best_preset = preset_name
                
                if best_preset and best_score > 0.3:
                    new_template = PRESET_CONFIGS[best_preset]
                    logger.info(f"Using {best_preset} preset as migration template (score: {best_score:.2f})")
                else:
                    new_template = DEFAULT_PRESET
                    logger.info("Using DEFAULT_PRESET as migration template")
            else:
                new_template = DEFAULT_PRESET
                logger.info("PRESET_CONFIGS not available, using DEFAULT_PRESET")
        except Exception as e:
            logger.warning(f"Error selecting migration template: {e}")
            new_template = DEFAULT_PRESET
    
    # Create base configuration from template
    migrated_config = deepcopy(new_template)
    
    # Enhanced key mapping with better fallback handling
    key_mapping = {
        # Direct mappings for training parameters
        'batch_size': ('training', 'batch_size'),
        'epochs': ('training', 'epochs'),
        'learning_rate': ('training', 'learning_rate'),
        'patience': ('training', 'patience'),
        'weight_decay': ('training', 'weight_decay'),
        'gradient_clip': ('training', 'gradient_clip'),
        'gradient_accumulation_steps': ('training', 'gradient_accumulation_steps'),
        'mixed_precision': ('training', 'mixed_precision'),
        'num_workers': ('training', 'num_workers'),
        'optimizer': ('training', 'optimizer'),
        'scheduler': ('training', 'scheduler'),
        
        # Direct mappings for model parameters
        'model_type': ('model', 'model_type'),
        'encoding_dim': ('model', 'encoding_dim'),
        'hidden_dims': ('model', 'hidden_dims'),
        'dropout_rates': ('model', 'dropout_rates'),
        'activation': ('model', 'activation'),
        'activation_param': ('model', 'activation_param'),
        'normalization': ('model', 'normalization'),
        'use_batch_norm': ('model', 'use_batch_norm'),
        'use_layer_norm': ('model', 'use_layer_norm'),
        'diversity_factor': ('model', 'diversity_factor'),
        'min_features': ('model', 'min_features'),
        'skip_connection': ('model', 'skip_connection'),
        'residual_blocks': ('model', 'residual_blocks'),
        'num_models': ('model', 'num_models'),
        
        # Direct mappings for security parameters
        'percentile': ('security', 'percentile'),
        'attack_threshold': ('security', 'attack_threshold'),
        'false_negative_cost': ('security', 'false_negative_cost'),
        'enable_security_metrics': ('security', 'enable_security_metrics'),
        'anomaly_threshold_strategy': ('security', 'anomaly_threshold_strategy'),
        'early_warning_threshold': ('security', 'early_warning_threshold'),
        
        # Direct mappings for data parameters
        'normal_samples': ('data', 'normal_samples'),
        'attack_samples': ('data', 'attack_samples'),
        'features': ('data', 'features'),
        # Avoid conflict with model normalization
        'data_normalization': ('data', 'normalization'),
        'anomaly_factor': ('data', 'anomaly_factor'),
        'random_state': ('data', 'random_state'),
        'validation_split': ('data', 'validation_split'),
        'test_split': ('data', 'test_split'),
        
        # Nested parameters
        'synthetic_generation.cluster_variance': ('data', 'synthetic_generation', 'cluster_variance'),
        'synthetic_generation.anomaly_sparsity': ('data', 'synthetic_generation', 'anomaly_sparsity'),
        
        # Direct mappings for monitoring
        'metrics_frequency': ('monitoring', 'metrics_frequency'),
        'checkpoint_frequency': ('monitoring', 'checkpoint_frequency'),
        'tensorboard_logging': ('monitoring', 'tensorboard_logging'),
        'console_logging_level': ('monitoring', 'console_logging_level'),
        
        # Legacy parameter mappings with transformations
        # Common legacy name
        'hidden_layer_sizes': ('model', 'hidden_dims'),
        # Single rate to list
        'dropout_rate': ('model', 'dropout_rates'),
        # Alternative name
        'n_epochs': ('training', 'epochs'),
        # Short form
        'lr': ('training', 'learning_rate'),
        # Boolean to patience value
        'early_stopping': ('training', 'patience'),
    }
    
    # Track migration statistics
    migration_stats = {
        'mapped_keys': 0,
        'skipped_keys': 0,
        'transformed_keys': 0,
        'invalid_values': 0,
        'auto_fixed': 0
    }
    
    # Apply mapped values with enhanced handling
    def set_nested_value(config_dict: Dict, path: tuple, value: Any) -> bool:
        """Set a nested value in the configuration."""
        try:
            target = config_dict
            for key in path[:-1]:
                if key not in target:
                    target[key] = {}
                target = target[key]
            target[path[-1]] = value
            return True
        except Exception as e:
            logger.warning(f"Failed to set nested value at {path}: {e}")
            return False
    
    def get_nested_value(config_dict: Dict, keys: List[str]) -> Any:
        """Get a nested value from legacy config."""
        try:
            value = config_dict
            for key in keys:
                value = value[key]
            return value
        except (KeyError, TypeError):
            return None
    
    # Process all legacy configuration keys
    for legacy_key, new_path in key_mapping.items():
        # Handle dot notation for nested legacy keys
        legacy_keys = legacy_key.split('.')
        legacy_value = get_nested_value(legacy_config, legacy_keys)
        
        if legacy_value is None:
            migration_stats['skipped_keys'] += 1
            continue
        
        try:
            # Handle special transformations
            if legacy_key == 'dropout_rate' and not isinstance(legacy_value, list):
                # Convert single dropout rate to list
                legacy_value = [float(legacy_value)]
                migration_stats['transformed_keys'] += 1
                logger.info(f"Converted single dropout_rate to list: {legacy_value}")
            
            elif legacy_key == 'early_stopping' and isinstance(legacy_value, bool):
                # Convert boolean early stopping to patience value
                legacy_value = 10 if legacy_value else 0
                migration_stats['transformed_keys'] += 1
                logger.info(f"Converted early_stopping boolean to patience: {legacy_value}")
            
            elif legacy_key == 'hidden_layer_sizes' and not isinstance(legacy_value, list):
                # Convert single size to list
                legacy_value = [int(legacy_value)]
                migration_stats['transformed_keys'] += 1
                logger.info(f"Converted single hidden_layer_size to list: {legacy_value}")
            
            # Validate the value before setting
            if isinstance(new_path, tuple):
                # Check if the value is reasonable for the parameter
                param_name = new_path[-1]
                
                if param_name in ['batch_size', 'epochs'] and (not isinstance(legacy_value, int) or legacy_value < 1):
                    logger.warning(f"Invalid {param_name} value {legacy_value}, using template default")
                    migration_stats['invalid_values'] += 1
                    continue
                
                elif param_name == 'learning_rate' and (not isinstance(legacy_value, (int, float)) or legacy_value <= 0):
                    logger.warning(f"Invalid learning_rate value {legacy_value}, using template default")
                    migration_stats['invalid_values'] += 1
                    continue
                
                elif param_name in ['hidden_dims', 'dropout_rates'] and not isinstance(legacy_value, list):
                    logger.warning(f"Invalid {param_name} value {legacy_value}, using template default")
                    migration_stats['invalid_values'] += 1
                    continue
                
                # Set the value
                if set_nested_value(migrated_config, new_path, legacy_value):
                    migration_stats['mapped_keys'] += 1
                else:
                    migration_stats['invalid_values'] += 1
            
        except Exception as e:
            logger.warning(f"Could not migrate {legacy_key}: {str(e)}")
            migration_stats['invalid_values'] += 1
            continue
    
    # Handle any remaining unmapped keys
    unmapped_keys = set(legacy_config.keys()) - set(k.split('.')[0] for k in key_mapping.keys())
    if unmapped_keys:
        logger.info(f"Unmapped legacy keys found: {list(unmapped_keys)}")
        
        # Try to map some common patterns
        for key in unmapped_keys:
            value = legacy_config[key]
            
            # Try common training parameter patterns
            if 'batch' in key.lower():
                if isinstance(value, int) and value > 0:
                    migrated_config.setdefault('training', {})['batch_size'] = value
                    migration_stats['auto_fixed'] += 1
                    logger.info(f"Auto-mapped {key} -> training.batch_size")
            
            elif 'epoch' in key.lower():
                if isinstance(value, int) and value > 0:
                    migrated_config.setdefault('training', {})['epochs'] = value
                    migration_stats['auto_fixed'] += 1
                    logger.info(f"Auto-mapped {key} -> training.epochs")
            
            elif 'learn' in key.lower() or 'lr' in key.lower():
                if isinstance(value, (int, float)) and value > 0:
                    migrated_config.setdefault('training', {})['learning_rate'] = value
                    migration_stats['auto_fixed'] += 1
                    logger.info(f"Auto-mapped {key} -> training.learning_rate")
    
    # Add comprehensive migration metadata
    migrated_config['metadata']['migration'] = {
        'source_version': legacy_config.get('version', '1.x'),
        'target_version': migrated_config.get('metadata', {}).get('version', '2.1'),
        'timestamp': datetime.now().isoformat(),
        'stats': migration_stats,
        'template_used': new_template.get('metadata', {}).get('description', 'Unknown'),
        'compatibility_checked': True,
        'legacy_keys_count': len(legacy_config),
        'success_rate': migration_stats['mapped_keys'] / max(len(legacy_config), 1)
    }
    
    # Validate and auto-fix the migrated configuration
    try:
        validate_config(migrated_config)
        logger.info("Migrated configuration passed validation")
    except ValueError as e:
        logger.warning(f"Migrated config validation issues: {e}")
        # The validation function should have auto-fixed issues
        migrated_config['metadata']['migration']['validation_fixes_applied'] = True
    
    # Log migration summary
    total_keys = len(legacy_config)
    success_rate = (migration_stats['mapped_keys'] + migration_stats['auto_fixed']) / max(total_keys, 1) * 100
    
    logger.info(f"Migration completed:")
    logger.info(f"  - Total legacy keys: {total_keys}")
    logger.info(f"  - Successfully mapped: {migration_stats['mapped_keys']}")
    logger.info(f"  - Auto-fixed: {migration_stats['auto_fixed']}")
    logger.info(f"  - Transformed: {migration_stats['transformed_keys']}")
    logger.info(f"  - Skipped: {migration_stats['skipped_keys']}")
    logger.info(f"  - Invalid: {migration_stats['invalid_values']}")
    logger.info(f"  - Success rate: {success_rate:.1f}%")
    
    return migrated_config

def convert_legacy_config(
    legacy_config: Dict[str, Any],
    config: Optional[Dict[str, Any]] = None,
    preset_similarity_threshold: Optional[float] = None
) -> Dict[str, Any]:
    """Convert legacy configuration to current format using intelligent preset matching.
    
    Args:
        legacy_config: The old configuration dictionary to convert
        config: Current configuration dictionary (for migration settings)
        preset_similarity_threshold: Override for similarity threshold (0-1)
        
    Returns:
        New configuration dictionary in current format with metadata
        
    Raises:
        ValueError: If legacy_config is invalid or conversion fails
    """
    # Initial Validation
    if not isinstance(legacy_config, dict):
        raise ValueError("legacy_config must be a dictionary")
    
    if not legacy_config:
        raise ValueError("legacy_config cannot be empty")
    
    logger.info("Initiating legacy configuration conversion with preset matching")
    
    # Step 1: Threshold Determination with Enhanced Logic
    def determine_threshold() -> float:
        """Determine the appropriate similarity threshold with enhanced fallbacks."""
        # Argument precedence
        if preset_similarity_threshold is not None:
            if 0 < preset_similarity_threshold <= 1:
                logger.info(f"Using provided threshold: {preset_similarity_threshold:.3f}")
                return preset_similarity_threshold
            logger.warning(f"Invalid threshold {preset_similarity_threshold}, using fallbacks")
        
        # Config file precedence
        if config and config.get("migration", {}).get("preset_similarity_threshold"):
            try:
                threshold = float(config["migration"]["preset_similarity_threshold"])
                if 0 < threshold <= 1:
                    logger.info(f"Using config threshold: {threshold:.3f}")
                    return threshold
                logger.warning(f"Invalid config threshold {threshold}, using fallbacks")
            except (ValueError, TypeError) as e:
                logger.warning(f"Error parsing config threshold: {e}")
        
        # Adaptive threshold based on legacy config complexity
        complexity_score = 0
        # Number of keys
        complexity_score += len(legacy_config) * 0.1
        # Nested dicts
        complexity_score += sum(1 for v in legacy_config.values() if isinstance(v, dict)) * 0.2
        # Lists
        complexity_score += sum(1 for v in legacy_config.values() if isinstance(v, list)) * 0.1
        
        # Adjust threshold based on complexity
        if complexity_score > 10:
            # More lenient for complex configs
            adaptive_threshold = 0.15
        elif complexity_score > 5:
            # Moderate
            adaptive_threshold = 0.10
        else:
            # Strict for simple configs
            adaptive_threshold = 0.05
        
        logger.info(f"Using adaptive threshold based on complexity ({complexity_score:.1f}): {adaptive_threshold:.3f}")
        return adaptive_threshold
    
    similarity_threshold = determine_threshold()
    
    # Step 2: Enhanced Preset Scoring System
    class AdvancedPresetScorer:
        """Enhanced scoring engine with machine learning-inspired features."""
        
        def __init__(self, legacy_config: Dict[str, Any]):
            self.legacy = legacy_config
            self.weights = {
                'training': 0.35,
                'model': 0.40,
                'security': 0.15,
                'data': 0.10
            }
            
            # Feature importance weights based on common usage patterns
            self.feature_weights = {
                # Most important
                'model_type': 0.30,
                'encoding_dim': 0.20,
                'batch_size': 0.15,
                'learning_rate': 0.15,
                'hidden_dims': 0.10,
                'activation': 0.10,
                'percentile': 0.08,
                'features': 0.05,
                'normalization': 0.05,
                'use_batch_norm': 0.05,
                'num_models': 0.05,
                'diversity_factor': 0.03,
                'optimizer': 0.03,
                'scheduler': 0.02
            }
            
            # Value ranges for normalization
            self.value_ranges = {
                'batch_size': (8, 256),
                'learning_rate': (1e-5, 1e-1),
                'encoding_dim': (4, 24),
                'percentile': (85, 99),
                'features': (10, 50),
                'normal_samples': (100, 20000),
                'attack_samples': (50, 5000),
                'epochs': (5, 300),
                'patience': (3, 30),
                'weight_decay': (0, 1e-2),
                'gradient_clip': (0.1, 5.0),
                'diversity_factor': (0, 1),
                'anomaly_factor': (1, 3)
            }
            
            # Pattern matching for string values
            self.string_patterns = {
                'activation': {
                    'relu': ['relu', 'ReLU'],
                    'leaky_relu': ['leaky_relu', 'leakyrelu', 'LeakyReLU'],
                    'gelu': ['gelu', 'GELU']
                },
                'optimizer': {
                    'Adam': ['adam', 'Adam'],
                    'AdamW': ['adamw', 'AdamW'],
                    'SGD': ['sgd', 'SGD']
                },
                'normalization': {
                    'batch': ['batch', 'batch_norm', 'BatchNorm'],
                    'layer': ['layer', 'layer_norm', 'LayerNorm'],
                    None: ['none', 'None', None]
                }
            }
        
        def normalize_value(self, key: str, value: Any) -> float:
            """Enhanced value normalization with outlier handling."""
            if key not in self.value_ranges:
                # Neutral value for unknown parameters
                return 0.5
            
            min_val, max_val = self.value_ranges[key]
            
            if isinstance(value, (int, float)):
                # Handle outliers by capping
                capped_value = max(min_val, min(value, max_val))
                normalized = (capped_value - min_val) / (max_val - min_val)
                
                # Apply log scaling for learning rate and weight decay
                if key in ['learning_rate', 'weight_decay'] and value > 0:
                    log_normalized = (np.log10(value) - np.log10(min_val)) / (np.log10(max_val) - np.log10(min_val))
                    normalized = max(0, min(1, log_normalized))
                
                return normalized
            
            return 0.5
        
        def compare_numeric_enhanced(self, key: str, preset_val: Any) -> float:
            """Enhanced numeric comparison with weighted similarity."""
            if key not in self.legacy:
                # Penalty for missing values
                return 0.3
            
            legacy_val = self.legacy[key]
            if not isinstance(legacy_val, (int, float)) or not isinstance(preset_val, (int, float)):
                return 0
            
            # Normalize both values
            norm_legacy = self.normalize_value(key, legacy_val)
            norm_preset = self.normalize_value(key, preset_val)
            
            # Calculate similarity with sigmoid-like function for smoother scoring
            diff = abs(norm_preset - norm_legacy)
            # Sigmoid-like curve
            similarity = 1 / (1 + diff * 2)
            
            # Apply importance weighting
            weight = self.feature_weights.get(key, 0.1)
            return similarity * weight
        
        def compare_string_enhanced(self, key: str, preset_val: Any) -> float:
            """Enhanced string comparison with pattern matching."""
            if key not in self.legacy:
                return 0.3
            
            legacy_val = self.legacy[key]
            
            # Direct match
            if legacy_val == preset_val:
                return self.feature_weights.get(key, 0.1)
            
            # Pattern matching
            if key in self.string_patterns:
                for standard_val, patterns in self.string_patterns[key].items():
                    if legacy_val in patterns and preset_val == standard_val:
                        # Slight penalty for pattern match
                        return self.feature_weights.get(key, 0.1) * 0.9
                    elif preset_val in patterns and legacy_val == standard_val:
                        return self.feature_weights.get(key, 0.1) * 0.9
            
            return 0
        
        def compare_list_enhanced(self, key: str, preset_val: Any) -> float:
            """Enhanced list comparison with element-wise similarity."""
            if key not in self.legacy:
                return 0.3
            
            legacy_list = self.legacy[key] if isinstance(self.legacy[key], list) else []
            preset_list = preset_val if isinstance(preset_val, list) else []
            
            if not legacy_list and not preset_list:
                return self.feature_weights.get(key, 0.1)
            
            if not legacy_list or not preset_list:
                # Some penalty for missing data
                return 0.1
            
            # Length similarity
            max_len = max(len(legacy_list), len(preset_list))
            min_len = min(len(legacy_list), len(preset_list))
            length_similarity = min_len / max_len
            
            # Element-wise similarity
            element_scores = []
            for i in range(min_len):
                if isinstance(legacy_list[i], (int, float)) and isinstance(preset_list[i], (int, float)):
                    # Numeric similarity
                    diff = abs(legacy_list[i] - preset_list[i]) / max(abs(legacy_list[i]), abs(preset_list[i]), 1e-6)
                    element_scores.append(max(0, 1 - diff))
                else:
                    # Exact match
                    element_scores.append(1.0 if legacy_list[i] == preset_list[i] else 0.0)
            
            avg_element_similarity = sum(element_scores) / len(element_scores) if element_scores else 0
            
            # Combine similarities
            total_similarity = (length_similarity * 0.3 + avg_element_similarity * 0.7)
            return total_similarity * self.feature_weights.get(key, 0.1)
        
        def score_preset_comprehensive(self, preset_name: str, preset_cfg: Dict[str, Any]) -> Dict[str, Any]:
            """Comprehensive preset scoring with detailed breakdown."""
            section_scores = defaultdict(float)
            parameter_scores = {}
            
            # Training parameters
            training_params = [
                ('batch_size', self.compare_numeric_enhanced),
                ('learning_rate', self.compare_numeric_enhanced),
                ('epochs', self.compare_numeric_enhanced),
                ('patience', self.compare_numeric_enhanced),
                ('weight_decay', self.compare_numeric_enhanced),
                ('gradient_clip', self.compare_numeric_enhanced),
                ('mixed_precision', self.compare_string_enhanced),
                ('optimizer', self.compare_string_enhanced),
                ('scheduler', self.compare_string_enhanced),
                ('num_workers', self.compare_numeric_enhanced)
            ]
            
            for param, compare_fn in training_params:
                if param in preset_cfg.get('training', {}):
                    score = compare_fn(param, preset_cfg['training'][param])
                    section_scores['training'] += score
                    parameter_scores[f'training.{param}'] = score
            
            # Model parameters
            model_params = [
                ('model_type', self.compare_string_enhanced),
                ('encoding_dim', self.compare_numeric_enhanced),
                ('hidden_dims', self.compare_list_enhanced),
                ('dropout_rates', self.compare_list_enhanced),
                ('activation', self.compare_string_enhanced),
                ('activation_param', self.compare_numeric_enhanced),
                ('normalization', self.compare_string_enhanced),
                ('use_batch_norm', self.compare_string_enhanced),
                ('use_layer_norm', self.compare_string_enhanced),
                ('skip_connection', self.compare_string_enhanced),
                ('residual_blocks', self.compare_string_enhanced),
                ('num_models', self.compare_numeric_enhanced),
                ('diversity_factor', self.compare_numeric_enhanced)
            ]
            
            for param, compare_fn in model_params:
                if param in preset_cfg.get('model', {}):
                    score = compare_fn(param, preset_cfg['model'][param])
                    section_scores['model'] += score
                    parameter_scores[f'model.{param}'] = score
            
            # Security parameters
            security_params = [
                ('percentile', self.compare_numeric_enhanced),
                ('attack_threshold', self.compare_numeric_enhanced),
                ('false_negative_cost', self.compare_numeric_enhanced),
                ('enable_security_metrics', self.compare_string_enhanced),
                ('anomaly_threshold_strategy', self.compare_string_enhanced)
            ]
            
            for param, compare_fn in security_params:
                if param in preset_cfg.get('security', {}):
                    score = compare_fn(param, preset_cfg['security'][param])
                    section_scores['security'] += score
                    parameter_scores[f'security.{param}'] = score
            
            # Data parameters
            data_params = [
                ('features', self.compare_numeric_enhanced),
                ('normalization', self.compare_string_enhanced),
                ('anomaly_factor', self.compare_numeric_enhanced),
                ('normal_samples', self.compare_numeric_enhanced),
                ('attack_samples', self.compare_numeric_enhanced),
                ('validation_split', self.compare_numeric_enhanced),
                ('test_split', self.compare_numeric_enhanced)
            ]
            
            for param, compare_fn in data_params:
                if param in preset_cfg.get('data', {}):
                    score = compare_fn(param, preset_cfg['data'][param])
                    section_scores['data'] += score
                    parameter_scores[f'data.{param}'] = score
            
            # Apply section weights
            weighted_sections = {}
            for section in section_scores:
                weighted_sections[section] = section_scores[section] * self.weights.get(section, 0.1)
            
            total_score = sum(weighted_sections.values())
            
            return {
                'name': preset_name,
                'total_score': total_score,
                'section_scores': dict(section_scores),
                'weighted_section_scores': weighted_sections,
                'parameter_scores': parameter_scores,
                'config': preset_cfg,
                'compatibility': preset_cfg.get('metadata', {}).get('compatibility', [])
            }

    # Step 3: Score All Available Presets
    try:
        if not PRESET_CONFIGS:
            logger.warning("PRESET_CONFIGS not available, using basic migration")
            return migrate_config(legacy_config)
        
        scorer = AdvancedPresetScorer(legacy_config)
        preset_scores = []
        
        for name, cfg in PRESET_CONFIGS.items():
            try:
                score_result = scorer.score_preset_comprehensive(name, cfg)
                preset_scores.append(score_result)
            except Exception as e:
                logger.warning(f"Error scoring preset {name}: {e}")
                continue
        
        if not preset_scores:
            logger.warning("No presets could be scored, using basic migration")
            return migrate_config(legacy_config)
        
        preset_scores.sort(key=lambda x: x['total_score'], reverse=True)
        
    except Exception as e:
        logger.error(f"Error during preset scoring: {e}")
        return migrate_config(legacy_config)
    
    # Step 4: Enhanced Results Analysis
    def analyze_results_enhanced(scores: List[Dict[str, Any]]) -> Tuple[List[str], Dict[str, Any], Dict[str, Any]]:
        """Enhanced analysis with confidence scoring."""
        if not scores:
            return [], {}, {}
        
        best_score = scores[0]['total_score']
        close_presets = []
        analysis = {
            'best_score': best_score,
            'score_distribution': [s['total_score'] for s in scores[:5]],
            'confidence': 'high' if best_score > 0.7 else 'medium' if best_score > 0.4 else 'low'
        }
        
        # Dynamic threshold adjustment based on score distribution
        effective_threshold = similarity_threshold
        
        if len(scores) > 1:
            second_best = scores[1]['total_score']
            score_gap = best_score - second_best
            
            if score_gap < 0.1 and best_score > 0.3:
                effective_threshold = max(similarity_threshold, 0.15)
                logger.info(f"Close scores detected, adjusting threshold to {effective_threshold:.3f}")
            elif best_score < 0.3:
                effective_threshold = min(similarity_threshold * 2, 0.2)
                logger.info(f"Low best score, relaxing threshold to {effective_threshold:.3f}")
        
        # Find close matches
        for score in scores:
            if (best_score - score['total_score']) <= effective_threshold:
                close_presets.append(score['name'])
            else:
                break
        
        return close_presets, scores[0], analysis
    
    close_presets, best_preset, analysis = analyze_results_enhanced(preset_scores)
    
    # Step 5: Enhanced Reporting
    def generate_enhanced_report(scores: List[Dict[str, Any]], close_presets: List[str], analysis: Dict[str, Any]) -> None:
        """Generate comprehensive conversion report."""
        logger.info("\n" + "="*80)
        logger.info("LEGACY CONFIGURATION CONVERSION REPORT")
        logger.info("="*80)
        
        logger.info(f"Analysis Confidence: {analysis['confidence'].upper()}")
        logger.info(f"Best Score: {analysis['best_score']:.3f}")
        logger.info(f"Threshold Used: {similarity_threshold:.3f}")
        
        logger.info("\nTop 10 Preset Matches:")
        logger.info(f"{'Rank':<5} {'Preset':<20} {'Total':<8} {'Training':<9} {'Model':<8} {'Security':<9} {'Data':<8}")
        logger.info("-" * 80)
        
        for i, score in enumerate(scores[:10], 1):
            logger.info(
                f"{i:<5} "
                f"{score['name']:<20} "
                f"{score['total_score']:.3f}    "
                f"{score['section_scores'].get('training', 0):.3f}     "
                f"{score['section_scores'].get('model', 0):.3f}    "
                f"{score['section_scores'].get('security', 0):.3f}     "
                f"{score['section_scores'].get('data', 0):.3f}"
            )
        
        if close_presets:
            logger.info(f"\nClose Matches (within threshold {similarity_threshold:.3f}):")
            for i, preset in enumerate(close_presets, 1):
                preset_score = next(s for s in scores if s['name'] == preset)
                logger.info(f"  {i}. {preset:<18} (score={preset_score['total_score']:.3f})")
            
            logger.info(f"\nRecommended: {close_presets[0]}")
        else:
            logger.info("\nNo close matches found - will use best available or default")
        
        # Show parameter-level analysis for best match
        if scores and 'parameter_scores' in scores[0]:
            logger.info(f"\nParameter Analysis for '{scores[0]['name']}':")
            param_scores = scores[0]['parameter_scores']
            sorted_params = sorted(param_scores.items(), key=lambda x: x[1], reverse=True)
            
            # Top 15 parameters
            for param, score in sorted_params[:15]:
                # Only show meaningful scores
                if score > 0.1:
                    logger.info(f"  {param:<30} {score:.3f}")

    generate_enhanced_report(preset_scores, close_presets, analysis)
    
    # Step 6: Smart Selection Logic
    def smart_select_preset(close_presets: List[str], scores: List[Dict[str, Any]], analysis: Dict[str, Any]) -> str:
        """Smart preset selection with fallback logic."""
        if not close_presets:
            # No close matches - use best available if reasonable, otherwise default
            if scores and scores[0]['total_score'] > 0.2:
                logger.info(f"Using best available preset: {scores[0]['name']} (score: {scores[0]['total_score']:.3f})")
                return scores[0]['name']
            else:
                logger.info("No reasonable matches found, using default preset")
                return "default"
        
        if len(close_presets) == 1:
            return close_presets[0]
        
        # Multiple close matches - use additional criteria
        best_candidate = close_presets[0]
        
        # Prefer presets with model type compatibility
        legacy_model_type = legacy_config.get('model_type')
        if legacy_model_type:
            for preset_name in close_presets:
                preset_score = next(s for s in scores if s['name'] == preset_name)
                compatibility = preset_score.get('compatibility', [])
                if legacy_model_type in compatibility:
                    logger.info(f"Selected {preset_name} for model type compatibility with {legacy_model_type}")
                    return preset_name
        
        # Interactive selection for terminal environments
        if sys.stdin.isatty():
            return interactive_select_enhanced(close_presets, scores)
        
        return best_candidate
    
    def interactive_select_enhanced(close_presets: List[str], scores: List[Dict[str, Any]]) -> str:
        """Enhanced interactive selection with detailed comparisons."""
        print(f"\nFound {len(close_presets)} similar presets:")
        
        for i, name in enumerate(close_presets, 1):
            score = next(s for s in scores if s['name'] == name)
            print(f"  {i}. {name:<18} (score={score['total_score']:.3f})")
        
        print("  a. Show detailed analysis")
        print("  c. Compare presets side-by-side")
        print("  d. Use default preset")
        
        while True:
            try:
                choice = input(f"\nSelect [1-{len(close_presets)}], or (a/c/d): ").strip().lower()
                
                if choice == 'a':
                    # Show detailed analysis
                    preset_name = input("Enter preset name for analysis: ").strip()
                    preset_data = next((s for s in scores if s['name'] == preset_name), None)
                    if preset_data:
                        print(f"\nDetailed Analysis for '{preset_name}':")
                        print(f"Total Score: {preset_data['total_score']:.3f}")
                        print(f"Section Scores:")
                        for section, score in preset_data['section_scores'].items():
                            print(f"  {section}: {score:.3f}")
                        
                        if 'parameter_scores' in preset_data:
                            print(f"\nTop Parameter Matches:")
                            sorted_params = sorted(preset_data['parameter_scores'].items(), 
                                                 key=lambda x: x[1], reverse=True)
                            for param, score in sorted_params[:10]:
                                if score > 0.1:
                                    print(f"  {param}: {score:.3f}")
                    else:
                        print("Preset not found")
                    continue
                
                elif choice == 'c':
                    # Compare presets
                    if len(close_presets) >= 2:
                        print(f"\nComparison of top {min(3, len(close_presets))} presets:")
                        print(f"{'Parameter':<25}", end="")
                        for name in close_presets[:3]:
                            print(f"{name[:15]:<16}", end="")
                        print()
                        print("-" * (25 + 16 * min(3, len(close_presets))))
                        
                        # Compare key parameters
                        key_params = ['model.model_type', 'model.encoding_dim', 'training.batch_size', 
                                    'training.learning_rate', 'security.percentile']
                        
                        for param in key_params:
                            print(f"{param:<25}", end="")
                            for name in close_presets[:3]:
                                preset_data = next(s for s in scores if s['name'] == name)
                                section, key = param.split('.')
                                value = preset_data['config'].get(section, {}).get(key, 'N/A')
                                print(f"{str(value)[:15]:<16}", end="")
                            print()
                    continue
                
                elif choice == 'd':
                    return "default"
                
                # Numeric selection
                choice_idx = int(choice) - 1
                if 0 <= choice_idx < len(close_presets):
                    return close_presets[choice_idx]
                print(f"Please enter 1-{len(close_presets)}, or a/c/d")
                
            except ValueError:
                print(f"Please enter 1-{len(close_presets)}, or a/c/d")
    
    selected_preset = smart_select_preset(close_presets, preset_scores, analysis)
    
    # Step 7: Create Final Configuration
    try:
        if selected_preset not in PRESET_CONFIGS:
            logger.warning(f"Selected preset '{selected_preset}' not found, using default")
            selected_preset = "default"
        
        new_config = migrate_config(legacy_config, PRESET_CONFIGS[selected_preset])
        
        # Add comprehensive conversion metadata
        new_config['metadata']['conversion'] = {
            'method': 'advanced_preset_matching',
            'selected_preset': selected_preset,
            'similar_presets': [p for p in close_presets if p != selected_preset],
            'similarity_threshold': similarity_threshold,
            'best_score': best_preset['total_score'],
            'confidence': analysis['confidence'],
            'timestamp': datetime.now().isoformat(),
            'legacy_keys_analyzed': len(legacy_config),
            'presets_evaluated': len(preset_scores),
            'selection_method': 'interactive' if sys.stdin.isatty() and len(close_presets) > 1 else 'automatic'
        }
        
        # Validate the final configuration
        try:
            validate_config(new_config)
            logger.info("Converted configuration passed validation")
        except ValueError as e:
            logger.warning(f"Validation issues with converted config: {e}")
            new_config['metadata']['conversion']['validation_warnings'] = str(e)
        
        logger.info(f"Legacy configuration successfully converted using preset: {selected_preset}")
        logger.info(f"Conversion confidence: {analysis['confidence']}")
        
        return new_config
        
    except Exception as e:
        logger.error(f"Error creating final configuration: {e}")
        # Fallback to basic migration
        logger.info("Falling back to basic migration")
        return migrate_config(legacy_config)

def save_custom_preset(name: str, config: Dict) -> Path:
    """Save a custom preset configuration with enhanced validation and metadata.
    
    Args:
        name: Name for the custom preset
        config: Configuration dictionary to save as preset
        
    Returns:
        Path: Path to the saved preset file
        
    Raises:
        ValueError: If name or config is invalid
        RuntimeError: If save operation fails
    """
    try:
        # Input validation
        if not isinstance(name, str) or not name.strip():
            raise ValueError("Preset name must be a non-empty string")
        
        if not isinstance(config, dict) or not config:
            raise ValueError("Configuration must be a non-empty dictionary")
        
        # Sanitize the name
        safe_name = "".join(c for c in name.strip() if c.isalnum() or c in (' ', '_', '-')).strip()
        safe_name = safe_name.replace(' ', '_').lower()
        
        if not safe_name:
            raise ValueError(f"Invalid preset name '{name}' - must contain alphanumeric characters")
        
        if len(safe_name) > 50:
            safe_name = safe_name[:50]
            logger.warning(f"Preset name truncated to: {safe_name}")
        
        # Setup custom presets directory
        custom_dir = CONFIG_DIR / "deep_learning_custom_presets"
        custom_dir.mkdir(parents=True, exist_ok=True)
        
        filename = f"preset_{safe_name}.json"
        filepath = custom_dir / filename
        
        # Check for name conflicts
        if filepath.exists():
            # Create backup of existing preset
            backup_path = filepath.with_suffix(f".backup_{int(time.time())}.json")
            shutil.copy2(filepath, backup_path)
            logger.info(f"Existing preset backed up to: {backup_path}")
        
        # Validate configuration structure
        try:
            validate_config(config)
            logger.info("Custom preset configuration passed validation")
        except ValueError as e:
            logger.warning(f"Custom preset validation issues (will save anyway): {e}")
        
        # Extract metadata from config
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        security_config = config.get('security', {})
        data_config = config.get('data', {})
        
        # Determine model compatibility
        model_type = model_config.get('model_type', 'unknown')
        compatibility = []
        
        if MODEL_VARIANTS:
            if model_type in MODEL_VARIANTS:
                compatibility.append(model_type)
            # Add other compatible types based on configuration
            if model_type == 'SimpleAutoencoder':
                compatibility.extend(['EnhancedAutoencoder', 'AutoencoderEnsemble'])
            elif model_type == 'EnhancedAutoencoder':
                compatibility.extend(['SimpleAutoencoder', 'AutoencoderEnsemble'])
            elif model_type == 'AutoencoderEnsemble':
                compatibility.append('EnhancedAutoencoder')
        else:
            compatibility = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
        
        # Generate comprehensive preset metadata
        preset_metadata = {
            "name": name,
            "safe_name": safe_name,
            "description": f"Custom preset '{name}' - created from current configuration",
            "created": datetime.now().isoformat(),
            "modified": datetime.now().isoformat(),
            "version": "2.1",
            "preset_type": "custom",
            "model_type": model_type,
            "compatibility": list(set(compatibility)),
            "system": {
                "python_version": platform.python_version(),
                "pytorch_version": getattr(torch, '__version__', 'unknown') if 'torch' in globals() else 'unknown',
                "cuda_available": torch.cuda.is_available() if 'torch' in globals() and hasattr(torch, 'cuda') else False,
                "hostname": platform.node(),
                "os": platform.system(),
                "created_by": "save_custom_preset"
            },
            "configuration_summary": {
                "batch_size": training_config.get('batch_size'),
                "learning_rate": training_config.get('learning_rate'),
                "encoding_dim": model_config.get('encoding_dim'),
                "hidden_layers": len(model_config.get('hidden_dims', [])),
                "features": data_config.get('features'),
                "security_percentile": security_config.get('percentile'),
                "total_sections": len(config),
                "estimated_complexity": estimate_config_complexity(config)
            },
            "usage_guidelines": {
                "recommended_for": determine_preset_recommendations(config),
                "memory_requirements": estimate_memory_requirements(config),
                "training_time_estimate": estimate_training_time(config),
                "resource_level": determine_resource_level(config)
            },
            "validation": {
                "config_validated": True,
                "validation_timestamp": datetime.now().isoformat(),
                "warnings": [],
                "auto_fixes_applied": []
            },
            "checksum": generate_config_checksum(config)
        }
        
        # Create complete preset structure
        preset_data = {
            "metadata": preset_metadata,
            "config": deepcopy(config)
        }
        
        # Add preset-specific enhancements
        preset_data["config"]["metadata"] = preset_data["config"].get("metadata", {})
        preset_data["config"]["metadata"]["preset_used"] = safe_name
        preset_data["config"]["metadata"]["is_custom_preset"] = True
        
        # Atomic write operation
        temp_path = filepath.with_suffix(f".tmp_{int(time.time())}")
        try:
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(preset_data, f, indent=4, ensure_ascii=False, sort_keys=False)
            
            # Verify the written file
            with open(temp_path, 'r', encoding='utf-8') as f:
                verification_data = json.load(f)
                if not verification_data.get('config') or not verification_data.get('metadata'):
                    raise ValueError("Verification failed: preset data is incomplete")
            
            # Atomic replacement
            if os.name == 'nt' and filepath.exists():
                filepath.unlink()
            temp_path.replace(filepath)
            
        except Exception as e:
            if temp_path.exists():
                try:
                    temp_path.unlink()
                except:
                    pass
            raise RuntimeError(f"Failed to write preset file: {e}") from e
        
        # Update global preset registry if available
        try:
            if 'PRESET_CONFIGS' in globals() and PRESET_CONFIGS is not None:
                PRESET_CONFIGS[safe_name] = preset_data["config"]
                logger.info(f"Added custom preset '{safe_name}' to global registry")
            
            # Refresh available presets
            invalidate_config_cache()
            
        except Exception as e:
            logger.warning(f"Could not update global preset registry: {e}")
        
        # Log success with statistics
        file_size = filepath.stat().st_size
        logger.info(f"Custom preset '{name}' saved successfully:")
        logger.info(f"  - File: {filepath}")
        logger.info(f"  - Size: {file_size} bytes")
        logger.info(f"  - Model type: {model_type}")
        logger.info(f"  - Compatible with: {', '.join(compatibility)}")
        logger.info(f"  - Resource level: {preset_metadata['usage_guidelines']['resource_level']}")
        
        return filepath
        
    except ValueError:
        # Re-raise validation errors
        raise
    except Exception as e:
        logger.error(f"Failed to save custom preset '{name}': {str(e)}", exc_info=True)
        raise RuntimeError(f"Custom preset save failed: {str(e)}") from e

# Helper functions for save_custom_preset
def estimate_config_complexity(config: Dict[str, Any]) -> str:
    """Estimate configuration complexity level with comprehensive analysis.
    
    Args:
        config: Configuration dictionary to analyze
        
    Returns:
        String indicating complexity level: 'low', 'medium', 'high', or 'very_high'
    """
    try:
        complexity_score = 0.0
        analysis_factors = []
        
        # Model architecture complexity
        model_config = config.get('model', {})
        model_type = model_config.get('model_type', 'SimpleAutoencoder')
        
        # Base complexity by model type
        if model_type == 'SimpleAutoencoder':
            complexity_score += 1.0
            analysis_factors.append('simple_model')
        elif model_type == 'EnhancedAutoencoder':
            complexity_score += 3.0
            analysis_factors.append('enhanced_model')
        elif model_type == 'AutoencoderEnsemble':
            complexity_score += 5.0
            analysis_factors.append('ensemble_model')
            
            # Ensemble-specific complexity
            num_models = model_config.get('num_models', 3)
            if num_models > 5:
                complexity_score += 2.0
                analysis_factors.append('large_ensemble')
            elif num_models > 3:
                complexity_score += 1.0
                analysis_factors.append('medium_ensemble')
        
        # Hidden layer complexity
        hidden_dims = model_config.get('hidden_dims', [])
        if isinstance(hidden_dims, list):
            # Number of layers
            layer_count = len(hidden_dims)
            complexity_score += layer_count * 0.5
            if layer_count > 4:
                analysis_factors.append('deep_architecture')
            
            # Layer sizes
            large_layers = sum(1 for dim in hidden_dims if isinstance(dim, (int, float)) and dim > 256)
            medium_layers = sum(1 for dim in hidden_dims if isinstance(dim, (int, float)) and 128 <= dim <= 256)
            
            complexity_score += large_layers * 1.0
            complexity_score += medium_layers * 0.5
            
            if large_layers > 0:
                analysis_factors.append('large_hidden_layers')
            
            # Non-standard architectures
            if any(dim > 512 for dim in hidden_dims if isinstance(dim, (int, float))):
                complexity_score += 1.5
                analysis_factors.append('very_large_layers')
        
        # Encoding dimension complexity
        encoding_dim = model_config.get('encoding_dim', 12)
        if isinstance(encoding_dim, (int, float)):
            if encoding_dim > 50:
                complexity_score += 1.0
                analysis_factors.append('large_encoding_dim')
            elif encoding_dim > 24:
                complexity_score += 0.5
                analysis_factors.append('medium_encoding_dim')
        
        # Activation and normalization complexity
        activation = model_config.get('activation', 'relu')
        if activation in ['gelu', 'swish', 'mish']:
            complexity_score += 0.5
            analysis_factors.append('advanced_activation')
        elif activation in ['leaky_relu', 'elu']:
            complexity_score += 0.2
            analysis_factors.append('parameterized_activation')
        
        normalization = model_config.get('normalization')
        if normalization == 'batch':
            complexity_score += 0.3
            analysis_factors.append('batch_normalization')
        elif normalization == 'layer':
            complexity_score += 0.4
            analysis_factors.append('layer_normalization')
        elif normalization == 'group':
            complexity_score += 0.6
            analysis_factors.append('group_normalization')
        
        # Advanced features
        if model_config.get('use_batch_norm', False):
            complexity_score += 0.3
            analysis_factors.append('batch_norm_layers')
        
        if model_config.get('use_layer_norm', False):
            complexity_score += 0.4
            analysis_factors.append('layer_norm_layers')
        
        if model_config.get('skip_connection', False):
            complexity_score += 0.5
            analysis_factors.append('skip_connections')
        
        if model_config.get('residual_blocks', False):
            complexity_score += 1.0
            analysis_factors.append('residual_architecture')
        
        # Training complexity
        training_config = config.get('training', {})
        
        # Mixed precision and advanced training features
        if training_config.get('mixed_precision', False):
            complexity_score += 0.5
            analysis_factors.append('mixed_precision')
        
        gradient_accumulation_steps = training_config.get('gradient_accumulation_steps', 1)
        if gradient_accumulation_steps > 1:
            complexity_score += 0.3 * min(gradient_accumulation_steps / 2, 2)
            analysis_factors.append('gradient_accumulation')
        
        # Advanced optimizers and schedulers
        optimizer = training_config.get('optimizer', 'Adam')
        if optimizer in ['AdamW', 'RMSprop', 'Adagrad']:
            complexity_score += 0.2
            analysis_factors.append('advanced_optimizer')
        elif optimizer == 'LBFGS':
            complexity_score += 0.5
            analysis_factors.append('second_order_optimizer')
        
        scheduler = training_config.get('scheduler')
        if scheduler in ['CosineAnnealingLR', 'ReduceLROnPlateau']:
            complexity_score += 0.2
            analysis_factors.append('adaptive_scheduler')
        elif scheduler in ['CyclicLR', 'OneCycleLR']:
            complexity_score += 0.4
            analysis_factors.append('cyclic_scheduler')
        
        # Regularization complexity
        dropout_rates = model_config.get('dropout_rates', [])
        if isinstance(dropout_rates, list) and len(dropout_rates) > 2:
            complexity_score += 0.3
            analysis_factors.append('complex_dropout')
        
        weight_decay = training_config.get('weight_decay', 0)
        if isinstance(weight_decay, (int, float)) and weight_decay > 1e-3:
            complexity_score += 0.1
            analysis_factors.append('strong_regularization')
        
        # Data complexity factors
        data_config = config.get('data', {})
        features = data_config.get('features', 20)
        if isinstance(features, int):
            if features > 100:
                complexity_score += 1.0
                analysis_factors.append('high_dimensional_data')
            elif features > 50:
                complexity_score += 0.5
                analysis_factors.append('medium_dimensional_data')
        
        # Advanced data preprocessing
        if data_config.get('synthetic_generation', {}).get('cluster_variance', 1.0) != 1.0:
            complexity_score += 0.2
            analysis_factors.append('custom_data_generation')
        
        # Security and monitoring complexity
        security_config = config.get('security', {})
        if security_config.get('enable_security_metrics', False):
            complexity_score += 0.3
            analysis_factors.append('security_monitoring')
        
        monitoring_config = config.get('monitoring', {})
        if monitoring_config.get('tensorboard_logging', False):
            complexity_score += 0.2
            analysis_factors.append('tensorboard_logging')
        
        if monitoring_config.get('wandb_logging', False):
            complexity_score += 0.3
            analysis_factors.append('wandb_integration')
        
        # Hardware-specific complexity
        hardware_config = config.get('hardware', {})
        if hardware_config.get('distributed_training', False):
            complexity_score += 2.0
            analysis_factors.append('distributed_training')
        
        if hardware_config.get('multi_gpu', False):
            complexity_score += 1.0
            analysis_factors.append('multi_gpu_training')
        
        # Determine complexity level with more granular categories
        if complexity_score < 2.0:
            level = 'low'
        elif complexity_score < 5.0:
            level = 'medium'
        elif complexity_score < 10.0:
            level = 'high'
        else:
            level = 'very_high'
        
        # Log analysis for debugging
        logger.debug(f"Complexity analysis: score={complexity_score:.2f}, level={level}, factors={analysis_factors}")
        
        return level
        
    except Exception as e:
        logger.warning(f"Error estimating config complexity: {e}")
        return 'unknown'

def determine_preset_recommendations(config: Dict[str, Any]) -> List[str]:
    """Determine comprehensive recommendations for what this preset is suitable for.
    
    Args:
        config: Configuration dictionary to analyze
        
    Returns:
        List of recommendation strings
    """
    recommendations = []
    
    try:
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        security_config = config.get('security', {})
        
        # Model type based recommendations
        model_type = model_config.get('model_type', '')
        if model_type == 'SimpleAutoencoder':
            recommendations.extend([
                'debugging and development',
                'prototyping new features',
                'resource-constrained environments',
                'educational purposes',
                'baseline comparisons',
                'rapid experimentation'
            ])
        elif model_type == 'EnhancedAutoencoder':
            recommendations.extend([
                'production deployment',
                'balanced performance needs',
                'configurable complexity scenarios',
                'standard anomaly detection tasks',
                'research and development',
                'performance optimization studies'
            ])
        elif model_type == 'AutoencoderEnsemble':
            recommendations.extend([
                'high accuracy requirements',
                'critical applications',
                'robust anomaly detection',
                'production systems with high stakes',
                'research requiring state-of-the-art performance',
                'applications where false negatives are costly'
            ])
        
        # Complexity-based recommendations
        complexity = estimate_config_complexity(config)
        if complexity == 'low':
            recommendations.extend([
                'beginner-friendly setups',
                'quick validation experiments',
                'resource-limited testing',
                'CI/CD pipeline integration'
            ])
        elif complexity == 'medium':
            recommendations.extend([
                'balanced complexity needs',
                'typical production workloads',
                'standard research applications'
            ])
        elif complexity == 'high':
            recommendations.extend([
                'advanced users',
                'complex anomaly patterns',
                'high-performance computing environments',
                'research pushing boundaries'
            ])
        elif complexity == 'very_high':
            recommendations.extend([
                'expert users only',
                'cutting-edge research',
                'specialized high-performance applications',
                'dedicated infrastructure requirements'
            ])
        
        # Training configuration recommendations
        batch_size = training_config.get('batch_size', 32)
        if batch_size <= 8:
            recommendations.extend([
                'severely memory-constrained environments',
                'edge computing applications',
                'single-sample inference needs'
            ])
        elif batch_size <= 32:
            recommendations.extend([
                'memory-constrained environments',
                'standard development setups',
                'typical research configurations'
            ])
        elif batch_size <= 128:
            recommendations.extend([
                'high-throughput scenarios',
                'batch processing applications',
                'GPU-optimized training'
            ])
        else:
            recommendations.extend([
                'very high-throughput scenarios',
                'large-scale data processing',
                'distributed computing environments'
            ])
        
        # Learning rate recommendations
        learning_rate = training_config.get('learning_rate', 0.001)
        if isinstance(learning_rate, (int, float)):
            if learning_rate >= 0.01:
                recommendations.append('fast convergence requirements')
            elif learning_rate <= 0.0001:
                recommendations.append('stable, fine-tuned training')
        
        # Mixed precision recommendations
        if training_config.get('mixed_precision', False):
            recommendations.extend([
                'GPU-accelerated training',
                'memory efficiency requirements',
                'modern hardware utilization'
            ])
        
        # Advanced training features
        if training_config.get('gradient_accumulation_steps', 1) > 1:
            recommendations.append('limited memory with large effective batch size needs')
        
        # Hardware-specific recommendations
        encoding_dim = model_config.get('encoding_dim', 12)
        hidden_dims = model_config.get('hidden_dims', [])
        
        total_params_estimate = 0
        if isinstance(hidden_dims, list) and hidden_dims:
            features = data_config.get('features', 20)
            total_params_estimate = features * hidden_dims[0]
            for i in range(len(hidden_dims) - 1):
                total_params_estimate += hidden_dims[i] * hidden_dims[i + 1]
            total_params_estimate += hidden_dims[-1] * encoding_dim
        
        if model_type == 'AutoencoderEnsemble':
            num_models = model_config.get('num_models', 3)
            total_params_estimate *= num_models
        
        if total_params_estimate < 10000:
            recommendations.extend([
                'CPU-only environments',
                'minimal resource scenarios',
                'embedded systems (with modifications)'
            ])
        elif total_params_estimate < 100000:
            recommendations.extend([
                'standard desktop environments',
                'entry-level GPU systems',
                'typical cloud instances'
            ])
        elif total_params_estimate < 1000000:
            recommendations.extend([
                'mid-range GPU systems',
                'professional workstations',
                'dedicated training servers'
            ])
        else:
            recommendations.extend([
                'high-end GPU systems',
                'specialized ML infrastructure',
                'enterprise-grade hardware'
            ])
        
        # Data characteristics recommendations
        features = data_config.get('features', 20)
        if isinstance(features, int):
            if features <= 10:
                recommendations.append('low-dimensional data analysis')
            elif features <= 50:
                recommendations.append('medium-dimensional data analysis')
            else:
                recommendations.append('high-dimensional data analysis')
        
        normal_samples = data_config.get('normal_samples', 8000)
        attack_samples = data_config.get('attack_samples', 2000)
        if isinstance(normal_samples, int) and isinstance(attack_samples, int):
            total_samples = normal_samples + attack_samples
            if total_samples < 1000:
                recommendations.append('small dataset scenarios')
            elif total_samples < 10000:
                recommendations.append('medium dataset scenarios')
            else:
                recommendations.append('large dataset scenarios')
        
        # Security-specific recommendations
        percentile = security_config.get('percentile', 95)
        if isinstance(percentile, (int, float)):
            if percentile >= 99:
                recommendations.append('high-security applications')
            elif percentile >= 95:
                recommendations.append('standard security requirements')
            else:
                recommendations.append('relaxed security thresholds')
        
        if security_config.get('enable_security_metrics', False):
            recommendations.append('security-focused deployments')
        
        # Use case pattern matching
        if 'high accuracy' in ' '.join(recommendations) and 'GPU' in ' '.join(recommendations):
            recommendations.append('mission-critical anomaly detection systems')
        
        if 'memory-constrained' in ' '.join(recommendations) and 'CPU' in ' '.join(recommendations):
            recommendations.append('IoT and edge computing deployments')
        
        if 'research' in ' '.join(recommendations) and complexity in ['high', 'very_high']:
            recommendations.append('academic and industrial research projects')
        
        # Remove duplicates and sort
        recommendations = list(set(recommendations))
        recommendations.sort()
        
        # Ensure we have at least one recommendation
        if not recommendations:
            recommendations = ['general purpose anomaly detection']
        
        return recommendations
        
    except Exception as e:
        logger.warning(f"Error determining preset recommendations: {e}")
        return ['general purpose']

def estimate_memory_requirements(config: Dict[str, Any]) -> str:
    """Estimate comprehensive memory requirements for the configuration.
    
    Args:
        config: Configuration dictionary to analyze
        
    Returns:
        Formatted string describing memory requirements
    """
    try:
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        hardware_config = config.get('hardware', {})
        
        # Extract key parameters
        model_type = model_config.get('model_type', 'SimpleAutoencoder')
        encoding_dim = model_config.get('encoding_dim', 12)
        hidden_dims = model_config.get('hidden_dims', [128, 64])
        features = data_config.get('features', 20)
        batch_size = training_config.get('batch_size', 64)
        mixed_precision = training_config.get('mixed_precision', False)
        
        # Normalize hidden_dims
        if not isinstance(hidden_dims, list):
            hidden_dims = [hidden_dims] if isinstance(hidden_dims, int) else [64]
        
        # Parameter count estimation with more accuracy
        total_params = 0
        
        if model_type == 'SimpleAutoencoder':
            # Encoder: input -> encoding
            # weights + bias
            total_params += features * encoding_dim + encoding_dim
            # Decoder: encoding -> output
            # weights + bias
            total_params += encoding_dim * features + features
            
        elif model_type == 'EnhancedAutoencoder':
            current_dim = features
            
            # Encoder layers
            for hidden_dim in hidden_dims:
                # weights + bias
                total_params += current_dim * hidden_dim + hidden_dim
                current_dim = hidden_dim
            
            # Bottleneck layer
            total_params += current_dim * encoding_dim + encoding_dim
            
            # Decoder layers (reverse)
            current_dim = encoding_dim
            for hidden_dim in reversed(hidden_dims):
                total_params += current_dim * hidden_dim + hidden_dim
                current_dim = hidden_dim
            
            # Output layer
            total_params += current_dim * features + features
            
            # Normalization parameters
            normalization = model_config.get('normalization')
            if normalization in ['batch', 'layer']:
                # Each layer has scale and shift parameters
                norm_params = sum(hidden_dims) * 2 + encoding_dim * 2
                total_params += norm_params
            
        elif model_type == 'AutoencoderEnsemble':
            num_models = model_config.get('num_models', 3)
            
            # Estimate single model parameters (simplified as Enhanced)
            single_model_params = 0
            if hidden_dims:
                current_dim = features
                # Simplified estimation
                for hidden_dim in hidden_dims[:min(2, len(hidden_dims))]:
                    single_model_params += current_dim * hidden_dim + hidden_dim
                    current_dim = hidden_dim
                single_model_params += current_dim * encoding_dim + encoding_dim
                single_model_params += encoding_dim * current_dim + current_dim
                single_model_params += current_dim * features + features
            else:
                single_model_params += features * encoding_dim + encoding_dim
                single_model_params += encoding_dim * features + features
            
            total_params = single_model_params * num_models
        
        # Memory calculations (in bytes)
        # float32 vs float16
        bytes_per_param = 4 if not mixed_precision else 2
        
        # Model parameters memory
        param_memory = total_params * bytes_per_param
        
        # Gradient memory (same size as parameters during training)
        gradient_memory = param_memory
        
        # Optimizer state memory (Adam uses 2x parameters for momentum and variance)
        optimizer = training_config.get('optimizer', 'Adam')
        if optimizer in ['Adam', 'AdamW']:
            optimizer_memory = param_memory * 2
        elif optimizer in ['SGD']:
            momentum = training_config.get('momentum', 0.9)
            optimizer_memory = param_memory if momentum > 0 else 0
        else:
            optimizer_memory = param_memory  # Conservative estimate
        
        # Activation memory (depends on batch size and architecture)
        activation_memory = 0
        
        # Input/output activations
        activation_memory += batch_size * features * bytes_per_param * 2
        
        # Hidden layer activations
        if model_type == 'EnhancedAutoencoder':
            for hidden_dim in hidden_dims:
                activation_memory += batch_size * hidden_dim * bytes_per_param
        
        # Encoding layer activation
        activation_memory += batch_size * encoding_dim * bytes_per_param
        
        # Ensemble multiplier
        if model_type == 'AutoencoderEnsemble':
            num_models = model_config.get('num_models', 3)
            activation_memory *= num_models
        
        # Additional memory for data loading and preprocessing
        data_memory = 0
        
        # Training data in memory
        normal_samples = data_config.get('normal_samples', 8000)
        attack_samples = data_config.get('attack_samples', 2000)
        total_samples = normal_samples + attack_samples
        
        # Assume data is kept in memory during training
        data_memory += total_samples * features * bytes_per_param
        
        # Validation and test sets
        validation_split = data_config.get('validation_split', 0.2)
        test_split = data_config.get('test_split', 0.2)
        data_memory += total_samples * (validation_split + test_split) * features * bytes_per_param
        
        # Buffer for data loading and augmentation
        data_memory *= 1.5
        
        # GPU-specific considerations
        gpu_overhead = 0
        if hardware_config.get('device', 'auto') != 'cpu':
            # GPU memory fragmentation and CUDA overhead
            # At least 100MB overhead
            gpu_overhead = max(param_memory * 0.1, 100 * 1024 * 1024)
        
        # Total memory calculation
        training_memory = param_memory + gradient_memory + optimizer_memory + activation_memory
        total_memory = training_memory + data_memory + gpu_overhead
        
        # Convert to human-readable format
        def format_memory(bytes_val):
            if bytes_val < 1024 ** 2:
                return f"{bytes_val / 1024:.1f} KB"
            elif bytes_val < 1024 ** 3:
                return f"{bytes_val / (1024 ** 2):.1f} MB"
            else:
                return f"{bytes_val / (1024 ** 3):.2f} GB"
        
        # Categorize memory requirements
        total_memory_mb = total_memory / (1024 ** 2)
        
        if total_memory_mb < 50:
            category = "Very Low"
            recommendation = "Suitable for any system"
        elif total_memory_mb < 200:
            category = "Low"
            recommendation = "Standard desktop/laptop"
        elif total_memory_mb < 1000:
            category = "Medium"
            recommendation = "8GB+ RAM, entry GPU"
        elif total_memory_mb < 4000:
            category = "High"
            recommendation = "16GB+ RAM, mid-range GPU"
        elif total_memory_mb < 16000:
            category = "Very High"
            recommendation = "32GB+ RAM, high-end GPU"
        else:
            category = "Extreme"
            recommendation = "Specialized hardware required"
        
        # Create detailed breakdown
        breakdown = {
            'model_parameters': format_memory(param_memory),
            'gradients': format_memory(gradient_memory),
            'optimizer_state': format_memory(optimizer_memory),
            'activations': format_memory(activation_memory),
            'data_storage': format_memory(data_memory),
            'gpu_overhead': format_memory(gpu_overhead) if gpu_overhead > 0 else "N/A",
            'total_training': format_memory(training_memory),
            'total_with_data': format_memory(total_memory)
        }
        
        # Format final result
        result = f"{category} ({format_memory(total_memory)}) - {recommendation}"
        
        # Add breakdown in debug mode
        logger.debug(f"Memory estimation breakdown: {breakdown}")
        
        return result
        
    except Exception as e:
        logger.warning(f"Error estimating memory requirements: {e}")
        return "Unknown - estimation failed"

def estimate_training_time(config: Dict[str, Any]) -> str:
    """Estimate training time based on configuration complexity and data size.
    
    Args:
        config: Configuration dictionary to analyze
        
    Returns:
        Formatted string describing estimated training time
    """
    try:
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        hardware_config = config.get('hardware', {})
        
        # Extract key parameters
        model_type = model_config.get('model_type', 'SimpleAutoencoder')
        encoding_dim = model_config.get('encoding_dim', 12)
        hidden_dims = model_config.get('hidden_dims', [128, 64])
        features = data_config.get('features', 20)
        batch_size = training_config.get('batch_size', 64)
        epochs = training_config.get('epochs', 100)
        mixed_precision = training_config.get('mixed_precision', False)
        
        # Data size
        normal_samples = data_config.get('normal_samples', 8000)
        attack_samples = data_config.get('attack_samples', 2000)
        total_samples = normal_samples + attack_samples
        
        # Calculate basic metrics
        steps_per_epoch = max(1, total_samples // batch_size)
        total_steps = steps_per_epoch * epochs
        
        # Base time per step estimation (in seconds)
        # 1ms baseline for very simple operations
        base_time_per_step = 0.001
        
        # Model complexity multiplier
        complexity_multiplier = 1.0
        
        if model_type == 'SimpleAutoencoder':
            complexity_multiplier = 1.0
        elif model_type == 'EnhancedAutoencoder':
            complexity_multiplier = 2.0
            
            # Hidden layer complexity
            if isinstance(hidden_dims, list):
                layer_complexity = len(hidden_dims) * 0.5
                size_complexity = sum(dim for dim in hidden_dims if isinstance(dim, (int, float))) / 1000
                complexity_multiplier += layer_complexity + size_complexity
            
            # Normalization overhead
            normalization = model_config.get('normalization')
            if normalization == 'batch':
                complexity_multiplier *= 1.2
            elif normalization == 'layer':
                complexity_multiplier *= 1.3
                
        elif model_type == 'AutoencoderEnsemble':
            num_models = model_config.get('num_models', 3)
            # Base ensemble overhead + linear scaling
            complexity_multiplier = 1.5 * num_models
        
        # Feature dimension impact
        if isinstance(features, int):
            # Normalize to 20 features baseline
            feature_multiplier = max(1.0, features / 20)
            complexity_multiplier *= feature_multiplier
        
        # Batch size impact (smaller batches = more overhead)
        if batch_size < 32:
            complexity_multiplier *= 1.5
        elif batch_size > 128:
            # Better GPU utilization
            complexity_multiplier *= 0.8
        
        # Activation function impact
        activation = model_config.get('activation', 'relu')
        if activation in ['gelu', 'swish']:
            complexity_multiplier *= 1.1
        elif activation in ['tanh', 'sigmoid']:
            complexity_multiplier *= 1.05
        
        # Hardware considerations
        device = hardware_config.get('device', 'auto')
        hardware_multiplier = 1.0
        
        # Try to detect actual hardware or use config
        try:
            if torch.cuda.is_available() and device != 'cpu':
                # GPU training - much faster
                hardware_multiplier = 0.1
                
                # GPU-specific optimizations
                if mixed_precision:
                    # Mixed precision speedup
                    hardware_multiplier *= 0.7
                
                # Multi-GPU
                if hardware_config.get('multi_gpu', False):
                    gpu_count = torch.cuda.device_count()
                    # Diminishing returns after 4 GPUs
                    hardware_multiplier /= min(gpu_count, 4)
                    
            else:
                # CPU training
                hardware_multiplier = 1.0
                
                # CPU-specific considerations
                num_workers = training_config.get('num_workers', 1)
                if num_workers > 1:
                    hardware_multiplier *= max(0.5, 1.0 / min(num_workers, 8))
                    
        except Exception:
            # Fallback assumptions
            if device == 'cpu':
                hardware_multiplier = 1.0
            else:
                # Assume some GPU acceleration
                hardware_multiplier = 0.2
        
        # Training-specific factors
        optimizer = training_config.get('optimizer', 'Adam')
        if optimizer in ['LBFGS']:
            # Second-order methods are much slower
            complexity_multiplier *= 3.0
        elif optimizer in ['AdamW', 'RMSprop']:
            complexity_multiplier *= 1.1
        
        # Gradient accumulation
        grad_accum_steps = training_config.get('gradient_accumulation_steps', 1)
        if grad_accum_steps > 1:
            # Some overhead for accumulation
            complexity_multiplier *= 1.2
        
        # Early stopping consideration
        patience = training_config.get('patience', 0)
        early_stop_factor = 1.0
        if patience > 0:
            # Assume early stopping might reduce training by 20-50%
            early_stop_factor = 0.7
        
        # Calculate total time
        time_per_step = base_time_per_step * complexity_multiplier * hardware_multiplier
        total_time_seconds = total_steps * time_per_step * early_stop_factor
        
        # Add overhead for data loading, checkpointing, etc.
        overhead_factor = 1.3
        total_time_seconds *= overhead_factor
        
        # Convert to human-readable format
        def format_time(seconds):
            if seconds < 60:
                return f"{seconds:.0f} seconds"
            elif seconds < 3600:
                return f"{seconds / 60:.1f} minutes"
            elif seconds < 86400:
                return f"{seconds / 3600:.1f} hours"
            else:
                return f"{seconds / 86400:.1f} days"
        
        # Determine category
        if total_time_seconds < 60:
            category = "Very Fast"
        # 10 minutes
        elif total_time_seconds < 600:
            category = "Fast"
        # 1 hour
        elif total_time_seconds < 3600:
            category = "Moderate"
        # 4 hours
        elif total_time_seconds < 14400:
            category = "Slow"
        # 1 day
        elif total_time_seconds < 86400:
            category = "Very Slow"
        else:
            category = "Extremely Slow"
        
        # Create estimate ranges (±50% uncertainty)
        min_time = total_time_seconds * 0.5
        max_time = total_time_seconds * 1.5
        
        # Format result
        if min_time < 60 and max_time > 60:
            result = f"{category} ({format_time(min_time)} - {format_time(max_time)})"
        else:
            result = f"{category} (~{format_time(total_time_seconds)})"
        
        # Add context information
        context_info = []
        if hardware_multiplier <= 0.2:
            context_info.append("GPU-accelerated")
        else:
            context_info.append("CPU-based")
            
        if mixed_precision and hardware_multiplier <= 0.2:
            context_info.append("mixed precision")
            
        if early_stop_factor < 1.0:
            context_info.append("with early stopping")
            
        if context_info:
            result += f" ({', '.join(context_info)})"
        
        # Debug information
        logger.debug(f"Training time estimation: steps={total_steps}, complexity_mult={complexity_multiplier:.2f}, "
                    f"hardware_mult={hardware_multiplier:.2f}, time_per_step={time_per_step:.6f}s")
        
        return result
        
    except Exception as e:
        logger.warning(f"Error estimating training time: {e}")
        return "Unknown - estimation failed"

def determine_resource_level(config: Dict[str, Any]) -> str:
    """Determine overall resource level required for the configuration.
    
    Args:
        config: Configuration dictionary to analyze
        
    Returns:
        String indicating resource level: 'minimal', 'low', 'medium', 'high', or 'extreme'
    """
    try:
        # Get individual assessments
        complexity = estimate_config_complexity(config)
        memory_req = estimate_memory_requirements(config)
        training_time = estimate_training_time(config)
        
        # Extract key indicators from other assessments
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        hardware_config = config.get('hardware', {})
        
        # Initialize scoring system
        resource_score = 0.0
        factors = []
        
        # Complexity contribution (30% weight)
        complexity_scores = {
            'low': 1.0,
            'medium': 2.5,
            'high': 4.0,
            'very_high': 6.0,
            'unknown': 2.0
        }
        resource_score += complexity_scores.get(complexity, 2.0) * 0.3
        factors.append(f"complexity_{complexity}")
        
        # Memory contribution (25% weight)
        if 'Very Low' in memory_req or 'KB' in memory_req:
            memory_score = 0.5
        elif 'Low' in memory_req and 'MB' in memory_req:
            memory_score = 1.0
        elif 'Medium' in memory_req:
            memory_score = 2.0
        elif 'High' in memory_req and 'GB' not in memory_req:
            memory_score = 3.5
        elif 'Very High' in memory_req or 'GB' in memory_req:
            memory_score = 5.0
        elif 'Extreme' in memory_req:
            memory_score = 6.0
        else:
            memory_score = 2.0
        
        resource_score += memory_score * 0.25
        factors.append(f"memory_score_{memory_score}")
        
        # Training time contribution (20% weight)
        if 'Very Fast' in training_time or 'Fast' in training_time:
            time_score = 1.0
        elif 'Moderate' in training_time:
            time_score = 2.0
        elif 'Slow' in training_time:
            time_score = 3.0
        elif 'Very Slow' in training_time:
            time_score = 4.0
        elif 'Extremely Slow' in training_time:
            time_score = 5.0
        else:
            time_score = 2.0
        
        resource_score += time_score * 0.20
        factors.append(f"time_score_{time_score}")
        
        # Model-specific factors (15% weight)
        model_type = model_config.get('model_type', 'SimpleAutoencoder')
        if model_type == 'SimpleAutoencoder':
            model_score = 1.0
        elif model_type == 'EnhancedAutoencoder':
            model_score = 2.0
            
            # Enhanced model complexity factors
            hidden_dims = model_config.get('hidden_dims', [])
            if isinstance(hidden_dims, list):
                if len(hidden_dims) > 3:
                    model_score += 0.5
                if any(dim > 256 for dim in hidden_dims if isinstance(dim, (int, float))):
                    model_score += 0.5
                    
        elif model_type == 'AutoencoderEnsemble':
            num_models = model_config.get('num_models', 3)
            model_score = 2.0 + (num_models - 1) * 0.5
        else:
            model_score = 2.0
        
        resource_score += model_score * 0.15
        factors.append(f"model_score_{model_score}")
        
        # Hardware requirements (10% weight)
        hardware_score = 1.0
        
        # GPU requirements
        if hardware_config.get('multi_gpu', False):
            hardware_score += 2.0
            factors.append("multi_gpu")
        elif hardware_config.get('device', 'auto') != 'cpu':
            hardware_score += 1.0
            factors.append("gpu_required")
        
        # Distributed training
        if hardware_config.get('distributed_training', False):
            hardware_score += 2.0
            factors.append("distributed")
        
        # Mixed precision (actually reduces requirements)
        if training_config.get('mixed_precision', False):
            hardware_score -= 0.2
            factors.append("mixed_precision_benefit")
        
        resource_score += hardware_score * 0.10
        factors.append(f"hardware_score_{hardware_score}")
        
        # Data size and complexity factors (bonus/penalty)
        features = data_config.get('features', 20)
        normal_samples = data_config.get('normal_samples', 8000)
        attack_samples = data_config.get('attack_samples', 2000)
        
        if isinstance(features, int) and features > 100:
            resource_score += 0.5
            factors.append("high_dimensional")
        
        total_samples = (normal_samples if isinstance(normal_samples, int) else 8000) + \
                       (attack_samples if isinstance(attack_samples, int) else 2000)
        
        if total_samples > 50000:
            resource_score += 0.5
            factors.append("large_dataset")
        elif total_samples < 1000:
            resource_score -= 0.2
            factors.append("small_dataset_benefit")
        
        # Batch size considerations
        batch_size = training_config.get('batch_size', 64)
        if isinstance(batch_size, int):
            if batch_size > 256:
                resource_score += 0.3
                factors.append("large_batch")
            elif batch_size < 8:
                resource_score -= 0.1
                factors.append("small_batch_benefit")
        
        # Determine final resource level
        if resource_score < 1.5:
            level = 'minimal'
            description = "Basic CPU, <4GB RAM"
        elif resource_score < 2.5:
            level = 'low'
            description = "Standard desktop, 4-8GB RAM"
        elif resource_score < 4.0:
            level = 'medium'
            description = "Mid-range GPU, 8-16GB RAM"
        elif resource_score < 5.5:
            level = 'high'
            description = "High-end GPU, 16-32GB RAM"
        else:
            level = 'extreme'
            description = "Specialized hardware, >32GB RAM"
        
        # Create detailed result
        result = f"{level} ({description})"
        
        # Debug information
        logger.debug(f"Resource level determination: score={resource_score:.2f}, level={level}, factors={factors}")
        
        return result
        
    except Exception as e:
        logger.warning(f"Error determining resource level: {e}")
        return 'unknown - assessment failed'

def get_config_cache_info() -> Dict[str, Any]:
    """Get information about the current configuration cache state."""
    global _cached_config, _config_cache_time
    
    current_time = time.time()
    cache_info = {
        'cache_exists': _cached_config is not None,
        'cache_time': _config_cache_time,
        'current_time': current_time,
        'cache_age_seconds': (current_time - _config_cache_time) if _config_cache_time else None,
        'cache_fresh': (_cached_config is not None and _config_cache_time is not None and 
                       current_time - _config_cache_time < 30),
        'cache_source': _cached_config.get('runtime', {}).get('config_source') if _cached_config else None,
        'active_preset': _cached_config.get('presets', {}).get('current_preset') if _cached_config else None
    }
    
    return cache_info

def save_config_interactive():
    """Interactive configuration saving with enhanced options, better error handling,
    and intelligent memory management for optimal performance during extensive operations."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        show_banner()
        
        # INITIAL MEMORY OPTIMIZATION - Get hardware context early for memory-aware processing
        hardware_data = None
        total_ram_gb = 8.0  # Conservative default
        
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
            
            # Initial memory cleanup for memory-constrained systems before interactive operations
            initial_clear_results = enhanced_clear_memory(
                aggressive=total_ram_gb < 8,  # More aggressive on low-memory systems
                hardware_data=hardware_data
            )
            
            if initial_clear_results.get('success'):
                logger.debug(f"Initial memory optimization for interactive save: {', '.join(initial_clear_results.get('actions_taken', []))}")
        except Exception as e:
            logger.debug(f"Initial memory optimization failed: {e}")
        
        config = get_current_config()
        
        print(Fore.CYAN + Style.BRIGHT + "\n" + "="*40)
        print(Fore.GREEN + Style.BRIGHT + "INTERACTIVE CONFIGURATION SAVE")
        print(Fore.CYAN + Style.BRIGHT + "="*40)
        
        # Show current configuration summary
        preset_used = config.get('presets', {}).get('current_preset', 'none')
        model_type = config.get('model', {}).get('model_type', 'unknown')
        print(Fore.YELLOW + Style.BRIGHT + f"\nCurrent configuration:")
        print(Fore.GREEN + Style.BRIGHT + f"  - Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_used}")
        print(Fore.GREEN + Style.BRIGHT + f"  - Model type: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  - Sections: " + Fore.CYAN + Style.BRIGHT + f"{len(config)}")
        
        # Get save options
        print(Fore.YELLOW + Style.BRIGHT + f"\nSave options:")
        print(Fore.WHITE + Style.BRIGHT + f"1. Save to default location ({CONFIG_FILE})")
        print(Fore.WHITE + Style.BRIGHT + f"2. Save with a specific name")
        print(Fore.WHITE + Style.BRIGHT + f"3. Save to custom path")
        print(Fore.WHITE + Style.BRIGHT + f"4. Save both to default and with name")
        print(Fore.WHITE + Style.BRIGHT + f"5. Show named configurations")
        print(Fore.RED + Style.BRIGHT + f"0. Cancel")
        
        while True:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect option (0-5): ").strip()
                
                if choice == '1':
                    # Save to default location
                    try:
                        # MEMORY OPTIMIZATION - Clear memory before save operation
                        if total_ram_gb < 16:
                            try:
                                pre_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                                if pre_save_clear.get('success'):
                                    logger.debug("Memory optimized before default save")
                            except Exception as e:
                                logger.debug(f"Pre-save memory optimization failed: {e}")
                        
                        saved_path = save_config(config)
                        print(Fore.GREEN + Style.BRIGHT + f"Configuration saved to: {saved_path}")
                        break
                    except Exception as e:
                        print(Fore.RED + Style.BRIGHT + f"Failed to save configuration: {e}")
                        break
                
                elif choice == '2':
                    # Save with specific name
                    name = input(Fore.YELLOW + Style.BRIGHT + "Enter configuration name: ").strip()
                    if name:
                        try:
                            # MEMORY OPTIMIZATION - Clear memory before save operation
                            if total_ram_gb < 16:
                                try:
                                    pre_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                                    if pre_save_clear.get('success'):
                                        logger.debug("Memory optimized before named save")
                                except Exception as e:
                                    logger.debug(f"Pre-save memory optimization failed: {e}")
                            
                            saved_path = save_config(config, name=name)
                            print(Fore.GREEN + Style.BRIGHT + f"Configuration saved as '{name}': {saved_path}")
                            break
                        except Exception as e:
                            print(Fore.RED + Style.BRIGHT + f"Failed to save configuration '{name}': {e}")
                            break
                    else:
                        print(Fore.YELLOW + Style.BRIGHT + "Name cannot be empty")
                
                elif choice == '3':
                    # Save to custom path
                    path_str = input(Fore.YELLOW + Style.BRIGHT + "Enter custom path: ").strip()
                    if path_str:
                        try:
                            # MEMORY OPTIMIZATION - Clear memory before save operation
                            if total_ram_gb < 16:
                                try:
                                    pre_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                                    if pre_save_clear.get('success'):
                                        logger.debug("Memory optimized before custom path save")
                                except Exception as e:
                                    logger.debug(f"Pre-save memory optimization failed: {e}")
                            
                            custom_path = Path(path_str)
                            saved_path = save_config(config, config_path=custom_path)
                            print(Fore.GREEN + Style.BRIGHT + f"Configuration saved to: {saved_path}")
                            break
                        except Exception as e:
                            print(Fore.RED + Style.BRIGHT + f"Failed to save to '{path_str}': {e}")
                            break
                    else:
                        print(Fore.YELLOW + Style.BRIGHT + "Path cannot be empty")
                
                elif choice == '4':
                    # Save both to default and with name
                    name = input(Fore.YELLOW + Style.BRIGHT + "Enter configuration name: ").strip()
                    if name:
                        try:
                            # MEMORY OPTIMIZATION - Clear memory before dual save operation
                            if total_ram_gb < 16:
                                try:
                                    pre_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                                    if pre_save_clear.get('success'):
                                        logger.debug("Memory optimized before dual save")
                                except Exception as e:
                                    logger.debug(f"Pre-save memory optimization failed: {e}")
                            
                            # This will save to both default location and as named config
                            saved_path = save_config(config, config_path=CONFIG_FILE, name=name)
                            print(Fore.GREEN + Style.BRIGHT + f"Configuration saved to default location and as '{name}'")
                            print(Fore.GREEN + Style.BRIGHT + f"  - Primary: {saved_path}")
                            break
                        except Exception as e:
                            print(Fore.RED + Style.BRIGHT + f"Failed to save configuration: {e}")
                            break
                    else:
                        print(Fore.YELLOW + Style.BRIGHT + "Name cannot be empty")
                
                elif choice == '5':
                    # Show named configurations with improved error handling and memory optimization
                    try:
                        # MEMORY OPTIMIZATION - Clear memory before intensive list operation
                        try:
                            pre_list_clear = enhanced_clear_memory(
                                aggressive=total_ram_gb < 8,  # More aggressive for low-memory systems
                                hardware_data=hardware_data
                            )
                            if pre_list_clear.get('success'):
                                logger.debug(f"Memory optimized before config listing: {', '.join(pre_list_clear.get('actions_taken', []))}")
                        except Exception as e:
                            logger.debug(f"Pre-list memory optimization failed: {e}")
                        
                        saved_configs_info = list_saved_configs()
                        
                        # Ensure we have the expected dictionary structure
                        if not isinstance(saved_configs_info, dict):
                            print(Fore.RED + Style.BRIGHT + "Invalid configuration data format")
                            continue
                        
                        all_configs = saved_configs_info.get("all_configs", {})
                        named_configs = saved_configs_info.get("named_configs", {})
                        
                        if all_configs:
                            print(Fore.YELLOW + Style.BRIGHT + f"\nExisting configurations:")
                            print(Fore.WHITE + Style.BRIGHT + f"{'Name':<20} {'Type':<8} {'Model Type':<15} {'Preset':<15} {'Modified':<20}")
                            print("-" * 85)
                            
                            # MEMORY OPTIMIZATION - Process configs in chunks for large lists
                            config_items = list(all_configs.items())
                            chunk_size = 50 if total_ram_gb >= 8 else 25  # Smaller chunks on low-memory systems
                            
                            for i in range(0, len(config_items), chunk_size):
                                chunk = config_items[i:i + chunk_size]
                                
                                for name, config_info in chunk:
                                    try:
                                        # Safely extract information with defaults
                                        config_type = config_info.get('type', 'unknown')[:7]
                                        model_type = str(config_info.get('model_type', 'N/A'))[:14]
                                        preset = str(config_info.get('preset_used', 'none'))[:14]
                                        
                                        # Handle modified time
                                        modified = config_info.get('modified', '')
                                        if modified:
                                            try:
                                                # Parse ISO format and format for display
                                                if 'T' in modified:
                                                    dt = datetime.fromisoformat(modified.replace('Z', '+00:00'))
                                                    modified_str = dt.strftime('%Y-%m-%d %H:%M')[:19]
                                                else:
                                                    modified_str = modified[:19]
                                            except Exception:
                                                modified_str = 'unknown'[:19]
                                        else:
                                            modified_str = 'unknown'[:19]
                                        
                                        print(Fore.WHITE + Style.BRIGHT + f"{name[:19]:<20} {config_type:<8} {model_type:<15} {preset:<15} {modified_str:<20}")
                                        
                                    except Exception as e:
                                        logger.debug(f"Error displaying config info for {name}: {e}")
                                        print(Fore.RED + Style.BRIGHT + f"{name[:19]:<20} {'error':<8} {'N/A':<15} {'N/A':<15} {'unknown':<20}")
                                
                                # MEMORY OPTIMIZATION - Clear memory between chunks for very large lists
                                if len(config_items) > 100 and total_ram_gb < 16 and i + chunk_size < len(config_items):
                                    try:
                                        chunk_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                                        if chunk_clear.get('success'):
                                            logger.debug(f"Memory optimized between config chunks at position {i + chunk_size}")
                                    except Exception as e:
                                        logger.debug(f"Chunk memory optimization failed: {e}")
                            
                            print(Fore.YELLOW + Style.BRIGHT + f"\nTotal configurations: {len(all_configs)}")
                            if named_configs:
                                print(Fore.YELLOW + Style.BRIGHT + f"Named configurations: {len(named_configs)}")
                        else:
                            print(Fore.RED + Style.BRIGHT + "\nNo configurations found.")
                        
                        # MEMORY OPTIMIZATION - Clear memory after displaying large config lists
                        if len(all_configs) > 50:
                            try:
                                post_list_clear = enhanced_clear_memory(
                                    aggressive=len(all_configs) > 100,  # More aggressive for very large lists
                                    hardware_data=hardware_data
                                )
                                if post_list_clear.get('success'):
                                    logger.debug(f"Post-list memory optimization: {', '.join(post_list_clear.get('actions_taken', []))}")
                            except Exception as e:
                                logger.debug(f"Post-list memory optimization failed: {e}")
                            
                    except Exception as e:
                        print(Fore.RED + Style.BRIGHT + f"Failed to list configurations: {e}")
                        logger.error(Fore.RED + Style.BRIGHT + f"Configuration listing failed: {e}")
                    
                    # Continue the loop to show options again
                
                elif choice == '0':
                    print("Save cancelled.")
                    break
                
                else:
                    print("Please enter 0-5")
                    
            except KeyboardInterrupt:
                print(Fore.RED + Style.BRIGHT + "\nSave cancelled.")
                break
            except Exception as e:
                print(Fore.RED + Style.BRIGHT + f"Error during interactive save: {e}")
                logger.error(f"Interactive save error: {e}")
                break
        
        # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
        # Aggressive cleanup after interactive session completion
        try:
            final_clear_results = enhanced_clear_memory(
                aggressive=True,  # Aggressive final cleanup
                hardware_data=hardware_data
            )
            
            if final_clear_results.get('success'):
                logger.debug(f"Final interactive save memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
                
        except Exception as e:
            logger.debug(f"Final interactive save memory optimization failed: {e}")
                
    except Exception as e:
        print(Fore.RED + Style.BRIGHT + f"Failed to start interactive save: {e}")
        logger.error(f"Interactive save initialization failed: {e}")
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after interactive save error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")

def save_config(config: Dict, config_path: Optional[Union[Path, str]] = None, name: Optional[str] = None) -> Path:
    """Save config with enhanced metadata, backup handling, validation, named configuration support,
    and intelligent memory management for optimal performance.
    
    This function can save configurations in multiple ways:
    1. To the default config file (when no parameters provided)
    2. To a specific path (when config_path provided)  
    3. As a named configuration (when name provided)
    4. Both to default location and as named config (when both provided)
    
    Args:
        config: Configuration dictionary to save
        config_path: Optional path where to save the configuration. If None, uses CONFIG_FILE
        name: Optional name for creating a named configuration file. Creates file as "{name}.json"
        
    Returns:
        Path: The primary path where the configuration was saved
        
    Raises:
        RuntimeError: If configuration save fails
        ValueError: If configuration is invalid or parameters are invalid
        TypeError: If parameters are of wrong type
    """
    try:
        # INITIAL MEMORY OPTIMIZATION - Get hardware context early for memory-aware processing
        hardware_data = None
        total_ram_gb = 8.0  # Conservative default
        
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
            
            # Initial memory cleanup for memory-constrained systems before processing large configs
            if total_ram_gb < 16 and len(str(config)) > 50000:  # Large config (>50KB serialized)
                initial_clear_results = enhanced_clear_memory(
                    aggressive=total_ram_gb < 8,  # More aggressive on low-memory systems
                    hardware_data=hardware_data
                )
                
                if initial_clear_results.get('success'):
                    logger.debug(f"Initial memory optimization for config saving: {', '.join(initial_clear_results.get('actions_taken', []))}")
        except Exception as e:
            logger.debug(f"Initial memory optimization failed: {e}")
        
        # Input validation
        if not isinstance(config, dict):
            raise TypeError("Configuration must be a dictionary")
        
        if config_path is not None and not isinstance(config_path, (str, Path)):
            raise TypeError("config_path must be a string or Path object")
        
        if name is not None and not isinstance(name, str):
            raise TypeError("name must be a string")
        
        if name is not None and not name.strip():
            raise ValueError("name cannot be empty or whitespace-only")
        
        # Validate configuration before saving
        logger.debug("Validating configuration before save")
        validate_config(config)
        
        # MEMORY OPTIMIZATION CHECKPOINT - Clear memory after validation for large configs
        if len(str(config)) > 75000 and total_ram_gb < 16:  # Very large config
            try:
                post_validation_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if post_validation_clear.get('success'):
                    logger.debug(f"Post-validation memory optimization: {', '.join(post_validation_clear.get('actions_taken', []))}")
            except Exception as e:
                logger.debug(f"Post-validation memory optimization failed: {e}")
        
        # Determine primary save path
        primary_path = None
        if config_path is not None:
            primary_path = Path(config_path)
        elif name is not None:
            # When only name is provided, create named config in CONFIG_DIR
            CONFIG_DIR.mkdir(parents=True, exist_ok=True)
            primary_path = CONFIG_DIR / f"{name.strip()}.json"
        else:
            # Default behavior - save to CONFIG_FILE
            primary_path = CONFIG_FILE
        
        # Sanitize name if provided for additional operations
        safe_name = None
        if name is not None:
            safe_name = "".join(c for c in name.strip() if c.isalnum() or c in (' ', '_', '-')).strip()
            safe_name = safe_name.replace(' ', '_').lower()
            
            if not safe_name:
                raise ValueError(f"Invalid configuration name '{name}' - must contain alphanumeric characters")
            
            if len(safe_name) > 50:
                safe_name = safe_name[:50]
                logger.warning(f"Configuration name truncated to: {safe_name}")
        
        # Prepare comprehensive metadata
        save_metadata = {
            "created": config.get('metadata', {}).get('created', datetime.now().isoformat()),
            "modified": datetime.now().isoformat(),
            "version": "2.1",
            "system": {
                "python_version": platform.python_version(),
                "pytorch_version": torch.__version__ if 'torch' in globals() else "unknown",
                "cuda_available": torch.cuda.is_available() if 'torch' in globals() else False,
                "hostname": platform.node(),
                "os": platform.system(),
                "platform_release": platform.release(),
                "architecture": platform.machine()
            },
            "config": {
                "preset_used": config.get('presets', {}).get('current_preset', 'none'),
                "model_type": config.get('model', {}).get('model_type', 'unknown'),
                "sections": list(config.keys()),
                "total_parameters": sum(len(v) if isinstance(v, dict) else 1 for v in config.values()),
                "checksum": generate_config_checksum(config)
            },
            "save_info": {
                "save_reason": "named_save" if name else "manual_save",
                "backup_created": False,
                "atomic_write": True,
                "named_config": safe_name,
                "primary_path": str(primary_path),
                "save_mode": "named_only" if name and config_path is None else "path_specified" if config_path else "default"
            }
        }
        
        # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before metadata processing for large configs
        if len(str(save_metadata)) > 10000 and total_ram_gb < 16:
            try:
                pre_metadata_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if pre_metadata_clear.get('success'):
                    logger.debug(f"Pre-metadata memory optimization: {', '.join(pre_metadata_clear.get('actions_taken', []))}")
            except Exception as e:
                logger.debug(f"Pre-metadata memory optimization failed: {e}")
        
        # Create comprehensive configuration structure
        full_config = {
            "metadata": save_metadata,
            "config": config
        }
        
        # Function to perform atomic save operation
        def atomic_save_to_path(target_path: Path, config_data: Dict, create_backup: bool = True) -> bool:
            """Perform atomic save to a specific path with backup handling."""
            backup_created = False
            
            # Enhanced backup handling with versioning and cleanup
            if create_backup and target_path.exists():
                backup_created = create_config_backup(target_path, save_metadata)
            
            # Ensure directory exists
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            # MEMORY OPTIMIZATION - Clear memory before intensive file operations
            if total_ram_gb < 8:
                try:
                    pre_write_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
                    if pre_write_clear.get('success'):
                        logger.debug("Memory optimized before atomic write operation")
                except Exception as e:
                    logger.debug(f"Pre-write memory optimization failed: {e}")
            
            # Atomic write operation with enhanced error handling
            temp_path = target_path.with_suffix(f".tmp_{int(time.time())}_{os.getpid()}")
            try:
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(config_data, f, indent=4, ensure_ascii=False, sort_keys=False)
                
                # Verify the written file can be read back
                with open(temp_path, 'r', encoding='utf-8') as f:
                    verification_data = json.load(f)
                    if not verification_data.get('config'):
                        raise ValueError("Verification failed: saved config is empty or invalid")
                
                # MEMORY OPTIMIZATION - Clear memory after file verification
                if total_ram_gb < 16:
                    try:
                        post_verify_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                        if post_verify_clear.get('success'):
                            logger.debug("Memory optimized after file verification")
                    except Exception as e:
                        logger.debug(f"Post-verification memory optimization failed: {e}")
                
                # Atomic replacement
                # Windows requires unlinking existing file before replacement
                if os.name == 'nt':
                    if target_path.exists():
                        # Remove existing file on Windows
                        target_path.unlink()
                temp_path.replace(target_path)
                
                logger.info(f"Configuration successfully saved to {target_path}")
                return backup_created
                
            except Exception as e:
                # Clean up temp file if write failed
                if temp_path.exists():
                    try:
                        temp_path.unlink()
                    except:
                        pass
                raise RuntimeError(f"Failed to write configuration file {target_path}: {e}") from e
        
        # Save to primary path
        primary_backup_created = atomic_save_to_path(primary_path, full_config, create_backup=True)
        save_metadata["save_info"]["backup_created"] = primary_backup_created
        
        # Handle additional save scenarios
        additional_saves = []
        
        # If both name and config_path are provided, also save to CONFIG_DIR as named config
        if name is not None and config_path is not None:
            named_path = CONFIG_DIR / f"{safe_name}.json"
            if named_path != primary_path:  # Avoid duplicate saves
                try:
                    named_backup_created = atomic_save_to_path(named_path, full_config, create_backup=True)
                    additional_saves.append({
                        'path': named_path,
                        'type': 'named_config',
                        'backup_created': named_backup_created
                    })
                    logger.info(f"Configuration also saved as named config: {named_path}")
                except Exception as e:
                    logger.warning(f"Failed to save additional named configuration to {named_path}: {e}")
        
        # MEMORY OPTIMIZATION CHECKPOINT - Clear memory between save operations
        if len(additional_saves) > 0 and total_ram_gb < 16:
            try:
                mid_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if mid_save_clear.get('success'):
                    logger.debug(f"Mid-save memory optimization: {', '.join(mid_save_clear.get('actions_taken', []))}")
            except Exception as e:
                logger.debug(f"Mid-save memory optimization failed: {e}")
        
        # If name provided but saving to default location, also create quick-access named file
        elif name is not None and config_path is None and primary_path == CONFIG_FILE:
            # This case is already handled by primary_path logic above
            pass
        
        # If saving to custom path but no name, offer to create a named version
        elif name is None and config_path is not None and config_path != CONFIG_FILE:
            # Extract a reasonable name from the path
            auto_name = Path(config_path).stem
            if auto_name and auto_name != "config":
                try:
                    auto_named_path = CONFIG_DIR / f"{auto_name}.json"
                    if auto_named_path != primary_path:
                        auto_backup_created = atomic_save_to_path(auto_named_path, full_config, create_backup=False)
                        additional_saves.append({
                            'path': auto_named_path,
                            'type': 'auto_named',
                            'backup_created': auto_backup_created
                        })
                        logger.info(f"Configuration auto-saved with name '{auto_name}': {auto_named_path}")
                except Exception as e:
                    logger.debug(f"Failed to create auto-named configuration: {e}")
        
        # Update metadata with additional save information
        if additional_saves:
            save_metadata["save_info"]["additional_saves"] = additional_saves
            save_metadata["save_info"]["total_locations"] = 1 + len(additional_saves)
        
        # Invalidate cache after successful save
        invalidate_config_cache()
        
        # Handle preset-specific save operations
        current_preset = config.get('presets', {}).get('current_preset')
        if current_preset and current_preset not in PRESET_CONFIGS:
            try:
                custom_preset_path = save_custom_preset(current_preset, config)
                logger.info(f"Custom preset '{current_preset}' saved to {custom_preset_path}")
                save_metadata["save_info"]["custom_preset_saved"] = str(custom_preset_path)
            except Exception as e:
                logger.warning(f"Failed to save custom preset '{current_preset}': {e}")
                save_metadata["save_info"]["custom_preset_error"] = str(e)
        
        # Create/update named configuration registry
        if name is not None:
            try:
                update_named_config_registry(safe_name, primary_path, save_metadata)
            except Exception as e:
                logger.warning(f"Failed to update named configuration registry: {e}")
        
        # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
        # Aggressive cleanup after save operations completion
        try:
            final_clear_results = enhanced_clear_memory(
                aggressive=True,  # Aggressive final cleanup
                hardware_data=hardware_data
            )
            
            if final_clear_results.get('success'):
                logger.debug(f"Final save memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
                
        except Exception as e:
            logger.debug(f"Final save memory optimization failed: {e}")
        
        # Log comprehensive save statistics
        config_size = primary_path.stat().st_size if primary_path.exists() else 0
        total_locations = 1 + len(additional_saves)
        
        logger.info(f"Configuration save completed:")
        logger.info(f"  - Primary location: {primary_path}")
        logger.info(f"  - File size: {config_size} bytes")
        logger.info(f"  - Sections: {len(config)}")
        logger.info(f"  - Total save locations: {total_locations}")
        logger.info(f"  - Memory optimizations applied during save process")
        
        if primary_backup_created:
            logger.info("  - Previous configuration backed up")
        
        if additional_saves:
            logger.info(f"  - Additional saves: {len(additional_saves)}")
            for save_info in additional_saves:
                logger.info(f"    * {save_info['type']}: {save_info['path']}")
        
        return primary_path
        
    except ValueError as e:
        logger.error(f"Configuration validation failed during save: {e}")
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after save error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        raise
    except Exception as e:
        logger.error(f"Failed to save configuration: {str(e)}", exc_info=True)
        
        # Emergency memory cleanup on error
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed after save error")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        raise RuntimeError(f"Configuration save failed: {str(e)}") from e

def create_config_backup(config_path: Path, save_metadata: Dict) -> bool:
    """Create a backup of the existing configuration with enhanced versioning."""
    try:
        backup_dir = config_path.parent / "backups"
        backup_dir.mkdir(exist_ok=True)
        
        # Enhanced backup naming with metadata
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        version = save_metadata.get('version', '2.1')
        preset_used = save_metadata.get('config', {}).get('preset_used', 'unknown')
        
        backup_name = f"{config_path.stem}_v{version}_{preset_used}_{timestamp}{config_path.suffix}"
        backup_path = backup_dir / backup_name
        
        # Copy with metadata preservation
        shutil.copy2(config_path, backup_path)
        
        # Cleanup old backups (keep last 10)
        cleanup_old_backups(backup_dir, config_path.stem, keep_count=10)
        
        logger.info(f"Configuration backup created: {backup_path}")
        return True
        
    except Exception as e:
        logger.warning(f"Failed to create backup: {e}")
        return False

def cleanup_old_backups(backup_dir: Path, config_stem: str, keep_count: int = 10):
    """Clean up old backup files keeping only the most recent ones."""
    try:
        backup_pattern = f"{config_stem}_v*"
        backup_files = list(backup_dir.glob(backup_pattern))
        
        if len(backup_files) > keep_count:
            # Sort by modification time (newest first)
            backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # Remove old backups
            for old_backup in backup_files[keep_count:]:
                old_backup.unlink()
                logger.debug(f"Removed old backup: {old_backup}")
                
    except Exception as e:
        logger.debug(f"Failed to cleanup old backups: {e}")

def prompt_user_for_migration_fallback() -> bool:
    """Prompt user for migration failure handling."""
    if sys.stdin.isatty():
        try:
            response = input("Configuration migration failed. Use default configuration? [Y/n]: ").strip().lower()
            return response in ['', 'y', 'yes']
        except:
            # Default to yes if input fails
            return True
    # Non-interactive mode defaults to yes
    return True

def handle_validation_failure(error: ValueError, failed_config: Dict, default_config: Dict) -> bool:
    """Handle configuration validation failure with user interaction."""
    logger.error(f"Configuration validation error: {error}")
    
    if sys.stdin.isatty():
        try:
            print(f"\nConfiguration validation failed: {error}")
            print("Options:")
            print("1. Use default configuration (recommended)")
            print("2. Attempt to fix and retry")
            print("3. Exit with error")
            
            choice = input("Choose option [1-3]: ").strip()
            
            if choice == '1':
                return True
            elif choice == '2':
                # Could implement basic fix attempts here
                logger.info("Automatic fixes not yet implemented")
                return True
            else:
                return False
                
        except:
            # Default to using default config if input fails
            return True
    
    # Non-interactive mode uses default config
    return True

def initialize_config(config_path: Path = CONFIG_FILE) -> Dict[str, Any]:
    """Initialize or load configuration with enhanced preset awareness, validation, 
    robust fallback handling, and intelligent memory management.
    
    Args:
        config_path: Path to the configuration file
        
    Returns:
        Dictionary containing the initialized configuration
        
    Raises:
        ValueError: If configuration cannot be initialized
        RuntimeError: If configuration initialization fails
    """
    initialization_start_time = time.time()
    
    try:
        logger.info(f"Initializing configuration from {config_path}")
        
        # INITIAL MEMORY OPTIMIZATION
        # Get hardware context early and perform initial memory cleanup
        hardware_data = None
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 4.0)
            
            # Initial aggressive memory cleanup for low-memory systems
            initial_clear_results = enhanced_clear_memory(
                aggressive=total_ram_gb < 8,  # More aggressive on low-memory systems
                hardware_data=hardware_data
            )
            
            if initial_clear_results.get('success'):
                logger.debug(f"Initial memory optimization: {', '.join(initial_clear_results.get('actions_taken', []))}")
            
        except Exception as e:
            logger.debug(f"Initial memory optimization failed: {e}")
            total_ram_gb = 4.0
        
        # Track memory optimization throughout initialization
        memory_optimization_log = {
            'initial_cleanup': {
                'success': initial_clear_results.get('success', False) if 'initial_clear_results' in locals() else False,
                'actions': initial_clear_results.get('actions_taken', []) if 'initial_clear_results' in locals() else [],
                'timestamp': datetime.now().isoformat()
            },
            'hardware_context': {
                'total_ram_gb': total_ram_gb,
                'cuda_available': hardware_data.get('cuda', {}).get('available', False) if hardware_data else False,
                'memory_pressure': 'high' if total_ram_gb < 8 else 'medium' if total_ram_gb < 16 else 'low'
            },
            'checkpoints': []
        }
        
        # Step 1: Enhanced Configuration Loading with Memory Management
        loaded_config = None
        load_error = None
        load_method = None
        
        # Clear memory before intensive file operations on low-memory systems
        if total_ram_gb < 8:
            try:
                pre_load_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if pre_load_clear.get('success'):
                    memory_optimization_log['checkpoints'].append({
                        'stage': 'pre_load',
                        'actions': pre_load_clear.get('actions_taken', []),
                        'timestamp': datetime.now().isoformat()
                    })
            except Exception as e:
                logger.debug(f"Pre-load memory optimization failed: {e}")
        
        try:
            loaded_config = load_config(config_path)
            if loaded_config:
                load_method = "primary_path"
                logger.info("Successfully loaded existing configuration from primary path")
                
                # Clear memory after loading large configurations
                if len(str(loaded_config)) > 50000:  # Large config (>50KB serialized)
                    try:
                        post_load_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                        if post_load_clear.get('success'):
                            memory_optimization_log['checkpoints'].append({
                                'stage': 'post_large_config_load',
                                'actions': post_load_clear.get('actions_taken', []),
                                'timestamp': datetime.now().isoformat()
                            })
                    except Exception as e:
                        logger.debug(f"Post-load memory optimization failed: {e}")
            else:
                logger.debug("Primary config file exists but returned empty configuration")
                load_method = "empty_primary"
        except FileNotFoundError:
            logger.info(f"No configuration file found at {config_path} - will create new")
            load_error = "file_not_found"
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"Configuration file corrupted or invalid: {e}")
            load_error = f"corrupted_config: {str(e)}"
        except PermissionError as e:
            logger.error(f"Permission denied accessing {config_path}: {e}")
            load_error = f"permission_denied: {str(e)}"
        except Exception as e:
            logger.error(f"Unexpected error loading configuration: {e}")
            load_error = f"unexpected_error: {str(e)}"
        
        # Step 2: Memory-Optimized Fallback Processing
        if not loaded_config and load_error:
            logger.info(f"Primary configuration load failed ({load_error}), attempting fallback strategies")
            
            # Clear memory before intensive fallback processing
            try:
                fallback_clear = enhanced_clear_memory(
                    aggressive=True,  # More aggressive clearing before fallbacks
                    hardware_data=hardware_data
                )
                
                if fallback_clear.get('success'):
                    memory_optimization_log['checkpoints'].append({
                        'stage': 'pre_fallback',
                        'actions': fallback_clear.get('actions_taken', []),
                        'timestamp': datetime.now().isoformat()
                    })
            except Exception as e:
                logger.debug(f"Pre-fallback memory optimization failed: {e}")
            
            # Fallback 1: Try backup configurations
            try:
                backup_config = try_load_from_backup(config_path)
                if backup_config:
                    loaded_config = backup_config
                    load_method = "backup_recovery"
                    logger.info("Successfully loaded configuration from backup")
                else:
                    logger.debug("No valid backup configurations found")
            except Exception as e:
                logger.debug(f"Backup recovery failed: {e}")
            
            # Fallback 2: Try loading from alternative locations
            if not loaded_config:
                alternative_paths = [
                    CONFIG_DIR / "config.json.bak",
                    CONFIG_DIR / "last_known_good.json",
                    CONFIG_DIR / "autobackup.json",
                    config_path.parent / f"{config_path.stem}_backup{config_path.suffix}",
                    config_path.parent / f"backup_{config_path.name}"
                ]
                
                for alt_path in alternative_paths:
                    if alt_path.exists() and alt_path != config_path:
                        try:
                            logger.debug(f"Trying alternative configuration: {alt_path}")
                            alt_config = load_config(alt_path)
                            if alt_config and isinstance(alt_config, dict):
                                loaded_config = alt_config
                                load_method = f"alternative_path_{alt_path.name}"
                                logger.info(f"Successfully loaded configuration from alternative path: {alt_path}")
                                break
                        except Exception as e:
                            logger.debug(f"Alternative path {alt_path} failed: {e}")
                            continue
            
            # Memory cleanup between fallback strategies for memory-constrained systems
            if total_ram_gb < 16 and not loaded_config:
                try:
                    enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                except Exception as e:
                    logger.debug(f"Intermediate fallback memory cleanup failed: {e}")
            
            # Fallback 3: Try to recover from corrupted file
            if not loaded_config and config_path.exists():
                try:
                    logger.info("Attempting to recover from corrupted configuration file")
                    with open(config_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    recovered_config = attempt_json_recovery(content, config_path)
                    if recovered_config:
                        loaded_config = recovered_config
                        load_method = "json_recovery"
                        logger.info("Successfully recovered configuration from corrupted file")
                        
                        # Save recovered config as backup
                        try:
                            recovery_backup_path = config_path.with_suffix(f".recovered_{int(time.time())}.json")
                            with open(recovery_backup_path, 'w', encoding='utf-8') as f:
                                json.dump(recovered_config, f, indent=4)
                            logger.info(f"Recovered configuration saved as backup: {recovery_backup_path}")
                        except Exception as e:
                            logger.warning(f"Failed to save recovery backup: {e}")
                    else:
                        logger.debug("JSON recovery failed - file too corrupted")
                        
                except Exception as e:
                    logger.debug(f"Configuration recovery attempt failed: {e}")
            
            # Fallback 4: Check for named configurations that could serve as templates
            if not loaded_config:
                try:
                    logger.debug("Checking for suitable named configurations as templates")
                    saved_configs_info = list_saved_configs()
                    
                    # Look for the most recently modified named config
                    named_configs = saved_configs_info.get("named_configs", {})
                    if named_configs:
                        # Sort by modification time
                        sorted_configs = sorted(
                            named_configs.items(),
                            key=lambda x: x[1].get("modified_time", 0),
                            reverse=True
                        )
                        
                        # Try top 3
                        for config_name, config_info in sorted_configs[:3]:
                            try:
                                # This will handle named config loading
                                named_config = load_config(config_name)
                                if named_config and isinstance(named_config, dict):
                                    loaded_config = named_config
                                    load_method = f"named_config_template_{config_name}"
                                    logger.info(f"Using named configuration '{config_name}' as template")
                                    break
                            except Exception as e:
                                logger.debug(f"Named config '{config_name}' failed: {e}")
                                continue
                                
                except Exception as e:
                    logger.debug(f"Named configuration fallback failed: {e}")
        
        # Step 3: Memory-Optimized Template Generation
        default_config = None
        template_source = None
        
        # Clear memory before template generation for memory-constrained systems
        if total_ram_gb < 16:
            try:
                pre_template_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if pre_template_clear.get('success'):
                    memory_optimization_log['checkpoints'].append({
                        'stage': 'pre_template_generation',
                        'actions': pre_template_clear.get('actions_taken', []),
                        'timestamp': datetime.now().isoformat()
                    })
            except Exception as e:
                logger.debug(f"Pre-template memory optimization failed: {e}")
        
        try:
            default_config = get_current_config()
            template_source = "get_current_config"
            logger.debug("Successfully obtained current configuration template")
            
            # Clear memory after large template generation
            if len(str(default_config)) > 100000:  # Very large template
                try:
                    post_template_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                    if post_template_clear.get('success'):
                        memory_optimization_log['checkpoints'].append({
                            'stage': 'post_large_template',
                            'actions': post_template_clear.get('actions_taken', []),
                            'timestamp': datetime.now().isoformat()
                        })
                except Exception as e:
                    logger.debug(f"Post-template memory optimization failed: {e}")
                    
        except Exception as e:
            logger.warning(f"Failed to get current config template: {e}")
            
            # Fallback to preset-based configuration
            try:
                if PRESET_CONFIGS and 'default' in PRESET_CONFIGS:
                    default_config = deepcopy(PRESET_CONFIGS['default'])
                    template_source = "default_preset"
                    logger.info("Using default preset as configuration template")
                elif PRESET_CONFIGS:
                    # Use first available preset
                    first_preset = next(iter(PRESET_CONFIGS.keys()))
                    default_config = deepcopy(PRESET_CONFIGS[first_preset])
                    template_source = f"preset_{first_preset}"
                    logger.info(f"Using preset '{first_preset}' as configuration template")
                else:
                    # Use DEFAULT_PRESET if available
                    if 'DEFAULT_PRESET' in globals() and DEFAULT_PRESET:
                        default_config = deepcopy(DEFAULT_PRESET)
                        template_source = "DEFAULT_PRESET_global"
                        logger.info("Using DEFAULT_PRESET global as fallback template")
                    else:
                        raise RuntimeError("No configuration template available")
                        
            except Exception as template_error:
                logger.error(f"Failed to get any configuration template: {template_error}")
                
                # Last resort - create minimal fallback
                try:
                    default_config = _create_minimal_fallback_config('standard')
                    template_source = "minimal_fallback"
                    logger.warning("Using minimal fallback configuration as template")
                except Exception as minimal_error:
                    logger.critical(f"Even minimal fallback failed: {minimal_error}")
                    raise RuntimeError("No configuration template available") from e
        
        # Step 4: Memory-Optimized Configuration Processing and Merging
        merged_config = None
        
        if loaded_config:
            logger.info(f"Processing loaded configuration (source: {load_method})")
            
            # Clear memory before intensive merge operations for memory-constrained systems
            if total_ram_gb < 8:
                try:
                    pre_merge_clear = enhanced_clear_memory(
                        aggressive=True,  # Aggressive on low-memory systems
                        hardware_data=hardware_data
                    )
                    
                    if pre_merge_clear.get('success'):
                        memory_optimization_log['checkpoints'].append({
                            'stage': 'pre_merge',
                            'actions': pre_merge_clear.get('actions_taken', []),
                            'timestamp': datetime.now().isoformat()
                        })
                except Exception as e:
                    logger.debug(f"Pre-merge memory optimization failed: {e}")
            
            # Check version compatibility and migration needs
            loaded_version = loaded_config.get('metadata', {}).get('version', '1.0')
            default_version = default_config.get('metadata', {}).get('version', '2.1')
            
            if loaded_version != default_version:
                logger.info(f"Configuration migration needed: {loaded_version} -> {default_version}")
                try:
                    # Clear memory before memory-intensive migration
                    if total_ram_gb < 16:
                        try:
                            enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                        except Exception as e:
                            logger.debug(f"Pre-migration memory cleanup failed: {e}")
                    
                    # Use the enhanced migration function
                    migrated_config = migrate_config(loaded_config, default_config)
                    logger.info("Configuration migration completed successfully")
                    loaded_config = migrated_config
                    
                    # Add migration metadata
                    if 'metadata' not in loaded_config:
                        loaded_config['metadata'] = {}
                    loaded_config['metadata']['migration_applied'] = {
                        'from_version': loaded_version,
                        'to_version': default_version,
                        'migration_time': datetime.now().isoformat(),
                        'migration_method': 'initialize_config'
                    }
                    
                    # Clear memory after migration
                    if total_ram_gb < 16:
                        try:
                            post_migration_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                            if post_migration_clear.get('success'):
                                memory_optimization_log['checkpoints'].append({
                                    'stage': 'post_migration',
                                    'actions': post_migration_clear.get('actions_taken', []),
                                    'timestamp': datetime.now().isoformat()
                                })
                        except Exception as e:
                            logger.debug(f"Post-migration memory optimization failed: {e}")
                    
                except Exception as migration_error:
                    logger.error(f"Configuration migration failed: {migration_error}")
                    
                    # Handle migration failure
                    if prompt_user_for_migration_fallback():
                        logger.warning("Migration failed - using default configuration")
                        loaded_config = default_config
                        load_method = "migration_fallback_to_default"
                    else:
                        # Try to use the loaded config anyway with warnings
                        logger.warning("Migration failed - attempting to use original loaded config with risks")
                        loaded_config['metadata'] = loaded_config.get('metadata', {})
                        loaded_config['metadata']['migration_failed'] = {
                            'error': str(migration_error),
                            'time': datetime.now().isoformat(),
                            'risk_level': 'high'
                        }
            
            # Merge configurations with preference for loaded config
            try:
                merged_config = deep_update(deepcopy(default_config), loaded_config)
                logger.info("Configuration merge completed successfully")
                
                # Add merge metadata
                if 'metadata' not in merged_config:
                    merged_config['metadata'] = {}
                merged_config['metadata']['initialization'] = {
                    'load_method': load_method,
                    'template_source': template_source,
                    'merge_successful': True,
                    'initialization_time': datetime.now().isoformat()
                }
                
                # Clear memory after merge for large configs
                if len(str(merged_config)) > 75000:  # Large merged config
                    try:
                        post_merge_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                        if post_merge_clear.get('success'):
                            memory_optimization_log['checkpoints'].append({
                                'stage': 'post_large_merge',
                                'actions': post_merge_clear.get('actions_taken', []),
                                'timestamp': datetime.now().isoformat()
                            })
                    except Exception as e:
                        logger.debug(f"Post-merge memory optimization failed: {e}")
                
            except Exception as merge_error:
                logger.error(f"Configuration merge failed: {merge_error}")
                
                # Decide which config to use
                if load_method in ["primary_path", "backup_recovery", "json_recovery"]:
                    # Trust the loaded config over the template
                    merged_config = loaded_config
                    logger.warning("Using loaded configuration without merge due to merge failure")
                else:
                    # Use the template as it's more reliable
                    merged_config = default_config
                    logger.warning("Using template configuration due to merge failure")
                
                # Add error metadata
                if 'metadata' not in merged_config:
                    merged_config['metadata'] = {}
                merged_config['metadata']['initialization'] = {
                    'load_method': load_method,
                    'template_source': template_source,
                    'merge_failed': str(merge_error),
                    'initialization_time': datetime.now().isoformat()
                }
        else:
            logger.info("No existing configuration found or loaded, using template")
            merged_config = default_config
            
            # Add initialization metadata
            if 'metadata' not in merged_config:
                merged_config['metadata'] = {}
            merged_config['metadata']['initialization'] = {
                'load_method': 'no_existing_config',
                'template_source': template_source,
                'load_error': load_error,
                'initialization_time': datetime.now().isoformat()
            }
        
        # Step 5: Memory-Optimized Validation
        validation_passed = False
        validation_errors = []
        
        # Clear memory before validation
        try:
            pre_validation_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if pre_validation_clear.get('success'):
                memory_optimization_log['checkpoints'].append({
                    'stage': 'pre_validation',
                    'actions': pre_validation_clear.get('actions_taken', []),
                    'timestamp': datetime.now().isoformat()
                })
        except Exception as e:
            logger.debug(f"Pre-validation memory optimization failed: {e}")
        
        try:
            validate_config(merged_config)
            logger.info("Final configuration passed validation")
            validation_passed = True
        except ValueError as validation_error:
            validation_errors.append(str(validation_error))
            logger.error(f"Final configuration validation failed: {validation_error}")
            
            # Handle validation failure with recovery options
            recovery_successful = False
            
            # Recovery attempt 1: Try auto-repair
            try:
                logger.info("Attempting automatic configuration repair")
                
                # Use centralized validation to identify and fix issues
                is_valid, errors, warnings = ConfigSectionValidators.validate_cross_section_compatibility(merged_config)
                
                if not is_valid:
                    # Apply basic fixes for common issues
                    if 'model' in merged_config and 'training' in merged_config:
                        # Fix batch normalization compatibility
                        if (merged_config['model'].get('use_batch_norm', False) and 
                            merged_config['training'].get('batch_size', 32) < 2):
                            merged_config['training']['batch_size'] = 2
                            logger.info("Auto-fixed: Increased batch_size for batch normalization compatibility")
                    
                    # Re-validate after fixes
                    validate_config(merged_config)
                    recovery_successful = True
                    logger.info("Automatic configuration repair successful")
                    
            except Exception as repair_error:
                logger.debug(f"Automatic repair failed: {repair_error}")
            
            # Recovery attempt 2: Use a known good configuration
            if not recovery_successful:
                recovery_options = []
                
                # Try default preset if available and different from current
                if (PRESET_CONFIGS and 'default' in PRESET_CONFIGS and 
                    template_source != "default_preset"):
                    recovery_options.append(('default_preset', PRESET_CONFIGS['default']))
                
                # Try minimal fallback
                recovery_options.append(('minimal_fallback', _create_minimal_fallback_config('standard')))
                
                for recovery_name, recovery_config in recovery_options:
                    try:
                        validate_config(recovery_config)
                        logger.warning(f"Using {recovery_name} due to validation failure")
                        merged_config = recovery_config
                        recovery_successful = True
                        
                        # Update metadata to reflect recovery
                        if 'metadata' not in merged_config:
                            merged_config['metadata'] = {}
                        merged_config['metadata']['initialization']['validation_recovery'] = {
                            'original_errors': validation_errors,
                            'recovery_method': recovery_name,
                            'recovery_time': datetime.now().isoformat()
                        }
                        break
                        
                    except Exception as recovery_error:
                        logger.debug(f"Recovery option {recovery_name} failed: {recovery_error}")
                        continue
            
            # If all recovery failed, decide based on user input or policy
            if not recovery_successful:
                if handle_validation_failure(validation_error, merged_config, default_config):
                    merged_config = default_config
                    logger.warning("Using default configuration due to validation failure")
                else:
                    # User chose to exit or no recovery possible
                    raise ValueError(f"Configuration validation failed and recovery unsuccessful: {validation_error}")
        
        # Step 6: Memory-Optimized Preset Consistency
        try:
            # Clear memory before preset processing
            pre_preset_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if pre_preset_clear.get('success'):
                memory_optimization_log['checkpoints'].append({
                    'stage': 'pre_preset_consistency',
                    'actions': pre_preset_clear.get('actions_taken', []),
                    'timestamp': datetime.now().isoformat()
                })
            
            merged_config = ensure_preset_consistency(merged_config)
            logger.debug("Preset consistency check completed")
        except Exception as preset_error:
            logger.warning(f"Preset consistency check failed: {preset_error}")
        
        # Step 7: Memory-Optimized Configuration Saving
        save_successful = False
        
        # Clear memory before save operations
        try:
            pre_save_clear = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if pre_save_clear.get('success'):
                memory_optimization_log['checkpoints'].append({
                    'stage': 'pre_save',
                    'actions': pre_save_clear.get('actions_taken', []),
                    'timestamp': datetime.now().isoformat()
                })
        except Exception as e:
            logger.debug(f"Pre-save memory optimization failed: {e}")
        
        try:
            # Create backup if original file exists and is valid
            if config_path.exists() and loaded_config and load_method == "primary_path":
                try:
                    backup_path = config_path.with_suffix(f".backup_init_{int(time.time())}.json")
                    shutil.copy2(config_path, backup_path)
                    logger.info(f"Created backup of original config: {backup_path}")
                except Exception as backup_error:
                    logger.warning(f"Failed to create backup: {backup_error}")
            
            save_config(merged_config, config_path)
            save_successful = True
            logger.info("Initialized configuration saved successfully")
            
        except Exception as save_error:
            logger.error(f"Failed to save initialized configuration: {save_error}")
            
            # Try to save to alternative location
            try:
                fallback_save_path = config_path.with_suffix(f".fallback_{int(time.time())}.json")
                save_config(merged_config, fallback_save_path)
                logger.warning(f"Configuration saved to fallback location: {fallback_save_path}")
                save_successful = True
            except Exception as fallback_save_error:
                logger.error(f"Failed to save to fallback location: {fallback_save_error}")
                # Continue with in-memory config even if save fails
        
        # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
        # Aggressive cleanup after initialization completion
        try:
            final_clear_results = enhanced_clear_memory(
                aggressive=True,  # Aggressive final cleanup
                hardware_data=hardware_data
            )
            
            if final_clear_results.get('success'):
                memory_optimization_log['final_cleanup'] = {
                    'actions': final_clear_results.get('actions_taken', []),
                    'timestamp': datetime.now().isoformat(),
                    'memory_impact': {
                        'optimization_effective': True,
                        'final_cleanup_performed': True
                    }
                }
                
                logger.debug(f"Final memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
                
        except Exception as e:
            logger.debug(f"Final memory optimization failed: {e}")
            memory_optimization_log['final_cleanup_error'] = str(e)
        
        # Add comprehensive memory optimization metadata
        initialization_time = time.time() - initialization_start_time
        total_memory_actions = (
            len(memory_optimization_log.get('initial_cleanup', {}).get('actions', [])) +
            sum(len(cp.get('actions', [])) for cp in memory_optimization_log.get('checkpoints', [])) +
            len(memory_optimization_log.get('final_cleanup', {}).get('actions', []))
        )
        
        if 'metadata' not in merged_config:
            merged_config['metadata'] = {}
        
        initialization_summary = {
            'config_path': str(config_path),
            'load_method': load_method,
            'load_error': load_error,
            'template_source': template_source,
            'validation_passed': validation_passed,
            'validation_errors': validation_errors if not validation_passed else [],
            'save_successful': save_successful,
            'initialization_completed': datetime.now().isoformat(),
            'initialization_duration': initialization_time,
            'memory_optimization': memory_optimization_log,
            'total_memory_actions': total_memory_actions,
            'fallback_strategies_used': []
        }
        
        # Track which fallback strategies were used
        if load_error:
            initialization_summary['fallback_strategies_used'].append('load_fallback')
        if not validation_passed:
            initialization_summary['fallback_strategies_used'].append('validation_recovery')
        if not save_successful:
            initialization_summary['fallback_strategies_used'].append('save_fallback')
        
        merged_config['metadata']['initialization'] = initialization_summary
        
        # Enhanced logging with memory optimization summary
        preset_used = merged_config.get('presets', {}).get('current_preset', 'none')
        model_type = merged_config.get('model', {}).get('model_type', 'unknown')
        total_sections = len(merged_config)
        
        logger.info("Configuration initialization completed successfully:")
        logger.info(f"  - Load method: {load_method or 'template_only'}")
        logger.info(f"  - Template source: {template_source}")
        logger.info(f"  - Preset: {preset_used}")
        logger.info(f"  - Model type: {model_type}")
        logger.info(f"  - Total sections: {total_sections}")
        logger.info(f"  - Initialization time: {initialization_time:.3f}s")
        logger.info(f"  - Memory optimization: {total_memory_actions} actions across {len(memory_optimization_log.get('checkpoints', [])) + 2} stages")
        logger.info(f"  - Validation: {'PASSED' if validation_passed else 'RECOVERED'}")
        logger.info(f"  - Save: {'SUCCESSFUL' if save_successful else 'FALLBACK'}")
        
        if initialization_summary['fallback_strategies_used']:
            logger.info(f"  - Fallback strategies used: {', '.join(initialization_summary['fallback_strategies_used'])}")
        
        return merged_config
        
    except Exception as e:
        logger.error(f"Configuration initialization failed: {str(e)}", exc_info=True)
        
        # Final emergency cleanup
        try:
            emergency_clear = enhanced_clear_memory(aggressive=True, hardware_data=hardware_data)
            logger.debug("Emergency memory cleanup performed")
        except Exception as cleanup_error:
            logger.debug(f"Emergency cleanup failed: {cleanup_error}")
        
        # Emergency fallback configuration
        try:
            logger.critical("Attempting emergency fallback configuration")
            fallback_config = _create_minimal_fallback_config('emergency')
            
            # Add emergency metadata with memory optimization context
            fallback_config['metadata'] = fallback_config.get('metadata', {})
            fallback_config['metadata']['emergency_initialization'] = {
                'original_error': str(e),
                'emergency_fallback_used': True,
                'initialization_time': datetime.now().isoformat(),
                'risk_level': 'critical',
                'memory_optimization_attempted': 'memory_optimization_log' in locals()
            }
            
            logger.critical("Emergency fallback configuration activated - system may have limited functionality")
            return fallback_config
            
        except Exception as fallback_error:
            logger.critical(f"Even emergency fallback configuration failed: {fallback_error}")
            raise RuntimeError(f"Complete configuration initialization failure: {str(e)}. Emergency fallback also failed: {str(fallback_error)}") from e

def update_global_config(config: Dict[str, Any]) -> None:
    """Update module-level constants from config with enhanced validation, logging, preset support,
    and intelligent memory management.
    
    This function synchronizes global configuration variables with the provided configuration
    dictionary, ensuring type safety, value validation, comprehensive change tracking, and
    optimal memory usage during intensive configuration processing.
    
    Args:
        config: Configuration dictionary to update from
        
    Raises:
        ValueError: If any configuration values are invalid
        TypeError: If any configuration values are of incorrect type
        KeyError: If required configuration sections are missing
    """
    if not isinstance(config, dict):
        raise TypeError(f"Configuration must be a dictionary, got {type(config).__name__}")
    
    # INITIAL MEMORY OPTIMIZATION - Get hardware context early for memory-aware processing
    hardware_data = None
    total_ram_gb = 8.0  # Conservative default
    
    try:
        hardware_data = check_hardware(include_memory_usage=True)
        total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
        
        # Initial memory cleanup for memory-constrained systems before processing large configs
        if total_ram_gb < 16 and len(str(config)) > 100000:  # Large config (>100KB serialized)
            initial_clear_results = enhanced_clear_memory(
                aggressive=total_ram_gb < 8,  # More aggressive on low-memory systems
                hardware_data=hardware_data
            )
            
            if initial_clear_results.get('success'):
                logger.debug(f"Initial memory optimization for config processing: {', '.join(initial_clear_results.get('actions_taken', []))}")
    except Exception as e:
        logger.debug(f"Initial memory optimization failed: {e}")
    
    # Validate config structure with better error messages
    required_sections = ['training', 'model', 'security', 'data']
    missing_sections = [section for section in required_sections if section not in config]
    if missing_sections:
        raise KeyError(f"Missing required configuration sections: {missing_sections}")
    
    # Initialize comprehensive change tracking
    changes = {
        'metadata': {
            'config_version': '2.1',
            'update_time': datetime.now().isoformat(),
            'source': config.get('metadata', {}).get('preset_used', 'manual'),
            'total_changes': 0,
            'memory_optimizations_applied': 0,
            'large_config_processing': len(str(config)) > 50000
        },
        'training': {},
        'model': {},
        'security': {},
        'data': {},
        'system': {}
    }
    
    # Training configuration updates with memory optimization checkpoints
    try:
        training_config = config.get('training', {})
        
        # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before processing large training configs
        if len(str(training_config)) > 20000 and total_ram_gb < 16:
            try:
                training_clear_results = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
                if training_clear_results.get('success'):
                    changes['metadata']['memory_optimizations_applied'] += 1
                    logger.debug("Memory optimized before training config processing")
            except Exception as e:
                logger.debug(f"Training config memory optimization failed: {e}")
        
        # Training parameter updates
        global DEFAULT_BATCH_SIZE, DEFAULT_EPOCHS, EARLY_STOPPING_PATIENCE, LEARNING_RATE
        global WEIGHT_DECAY, GRADIENT_CLIP, GRADIENT_ACCUMULATION_STEPS, MIXED_PRECISION, NUM_WORKERS
        
        if 'batch_size' in training_config:
            old_value = DEFAULT_BATCH_SIZE
            new_value = max(1, min(2048, int(training_config['batch_size'])))
            if old_value != new_value:
                DEFAULT_BATCH_SIZE = new_value
                changes['training']['batch_size'] = {'from': old_value, 'to': new_value}
        
        if 'epochs' in training_config:
            old_value = DEFAULT_EPOCHS
            new_value = max(1, min(10000, int(training_config['epochs'])))
            if old_value != new_value:
                DEFAULT_EPOCHS = new_value
                changes['training']['epochs'] = {'from': old_value, 'to': new_value}
        
        if 'patience' in training_config:
            old_value = EARLY_STOPPING_PATIENCE
            new_value = max(1, min(1000, int(training_config['patience'])))
            if old_value != new_value:
                EARLY_STOPPING_PATIENCE = new_value
                changes['training']['patience'] = {'from': old_value, 'to': new_value}
        
        if 'learning_rate' in training_config:
            old_value = LEARNING_RATE
            new_value = max(1e-8, min(1.0, float(training_config['learning_rate'])))
            if old_value != new_value:
                LEARNING_RATE = new_value
                changes['training']['learning_rate'] = {'from': old_value, 'to': new_value}
        
        if 'weight_decay' in training_config:
            old_value = WEIGHT_DECAY
            new_value = max(0.0, min(1.0, float(training_config['weight_decay'])))
            if old_value != new_value:
                WEIGHT_DECAY = new_value
                changes['training']['weight_decay'] = {'from': old_value, 'to': new_value}
        
        if 'gradient_clip' in training_config:
            old_value = GRADIENT_CLIP
            new_value = max(0.01, min(100.0, float(training_config['gradient_clip'])))
            if old_value != new_value:
                GRADIENT_CLIP = new_value
                changes['training']['gradient_clip'] = {'from': old_value, 'to': new_value}
        
        if 'gradient_accumulation_steps' in training_config:
            old_value = GRADIENT_ACCUMULATION_STEPS
            new_value = max(1, min(256, int(training_config['gradient_accumulation_steps'])))
            if old_value != new_value:
                GRADIENT_ACCUMULATION_STEPS = new_value
                changes['training']['gradient_accumulation_steps'] = {'from': old_value, 'to': new_value}
        
        if 'mixed_precision' in training_config:
            old_value = MIXED_PRECISION
            new_value = bool(training_config['mixed_precision'])
            if old_value != new_value:
                MIXED_PRECISION = new_value
                changes['training']['mixed_precision'] = {'from': old_value, 'to': new_value}
        
        if 'num_workers' in training_config:
            old_value = NUM_WORKERS
            new_value = max(0, min(32, int(training_config['num_workers'])))
            if old_value != new_value:
                NUM_WORKERS = new_value
                changes['training']['num_workers'] = {'from': old_value, 'to': new_value}
                
    except Exception as e:
        logger.error(f"Error updating training configuration: {e}")
        raise ValueError(f"Invalid training configuration: {e}")
    
    # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before model config processing
    try:
        model_config = config.get('model', {})
        
        if len(str(model_config)) > 15000 and total_ram_gb < 16:
            model_clear_results = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if model_clear_results.get('success'):
                changes['metadata']['memory_optimizations_applied'] += 1
                logger.debug("Memory optimized before model config processing")
    
        # Model parameter updates
        global DEFAULT_ENCODING_DIM, HIDDEN_LAYER_SIZES, DROPOUT_RATES, ACTIVATION, ACTIVATION_PARAM
        global NORMALIZATION, USE_BATCH_NORM, USE_LAYER_NORM, DIVERSITY_FACTOR, MIN_FEATURES, NUM_MODELS
        
        if 'encoding_dim' in model_config:
            old_value = DEFAULT_ENCODING_DIM
            new_value = max(1, min(1024, int(model_config['encoding_dim'])))
            if old_value != new_value:
                DEFAULT_ENCODING_DIM = new_value
                changes['model']['encoding_dim'] = {'from': old_value, 'to': new_value}
        
        if 'hidden_dims' in model_config:
            old_value = HIDDEN_LAYER_SIZES
            new_dims = model_config['hidden_dims']
            if isinstance(new_dims, (list, tuple)) and all(isinstance(d, int) and d > 0 for d in new_dims):
                new_value = list(new_dims)
                if old_value != new_value:
                    HIDDEN_LAYER_SIZES = new_value
                    changes['model']['hidden_dims'] = {'from': old_value, 'to': new_value}
        
        if 'dropout_rates' in model_config:
            old_value = DROPOUT_RATES
            new_rates = model_config['dropout_rates']
            if isinstance(new_rates, (list, tuple)) and all(isinstance(r, (int, float)) and 0 <= r < 1 for r in new_rates):
                new_value = list(new_rates)
                if old_value != new_value:
                    DROPOUT_RATES = new_value
                    changes['model']['dropout_rates'] = {'from': old_value, 'to': new_value}
        
        valid_activations = ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu']
        if 'activation' in model_config:
            old_value = ACTIVATION
            new_value = model_config['activation']
            if isinstance(new_value, str) and new_value.lower() in valid_activations:
                new_value = new_value.lower()
                if old_value != new_value:
                    ACTIVATION = new_value
                    changes['model']['activation'] = {'from': old_value, 'to': new_value}
        
        if 'activation_param' in model_config:
            old_value = ACTIVATION_PARAM
            new_value = max(0.0, min(1.0, float(model_config['activation_param'])))
            if old_value != new_value:
                ACTIVATION_PARAM = new_value
                changes['model']['activation_param'] = {'from': old_value, 'to': new_value}
        
        valid_normalizations = ['batch', 'layer', 'instance', 'group', None, 'none']
        if 'normalization' in model_config:
            old_value = NORMALIZATION
            new_value = model_config['normalization']
            if new_value in valid_normalizations:
                if new_value == 'none':
                    new_value = None
                if old_value != new_value:
                    NORMALIZATION = new_value
                    changes['model']['normalization'] = {'from': old_value, 'to': new_value}
        
        if 'use_batch_norm' in model_config:
            old_value = USE_BATCH_NORM
            new_value = bool(model_config['use_batch_norm'])
            if old_value != new_value:
                USE_BATCH_NORM = new_value
                changes['model']['use_batch_norm'] = {'from': old_value, 'to': new_value}
        
        if 'use_layer_norm' in model_config:
            old_value = USE_LAYER_NORM
            new_value = bool(model_config['use_layer_norm'])
            if old_value != new_value:
                USE_LAYER_NORM = new_value
                changes['model']['use_layer_norm'] = {'from': old_value, 'to': new_value}
        
        if 'diversity_factor' in model_config:
            old_value = DIVERSITY_FACTOR
            new_value = max(0.0, min(1.0, float(model_config['diversity_factor'])))
            if old_value != new_value:
                DIVERSITY_FACTOR = new_value
                changes['model']['diversity_factor'] = {'from': old_value, 'to': new_value}
        
        if 'min_features' in model_config:
            old_value = MIN_FEATURES
            new_value = max(1, min(10000, int(model_config['min_features'])))
            if old_value != new_value:
                MIN_FEATURES = new_value
                changes['model']['min_features'] = {'from': old_value, 'to': new_value}
        
        if 'num_models' in model_config:
            old_value = NUM_MODELS
            new_value = max(1, min(20, int(model_config['num_models'])))
            if old_value != new_value:
                NUM_MODELS = new_value
                changes['model']['num_models'] = {'from': old_value, 'to': new_value}
    
    except Exception as e:
        logger.error(f"Error updating model configuration: {e}")
        raise ValueError(f"Invalid model configuration: {e}")
    
    # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before security config processing
    try:
        security_config = config.get('security', {})
        
        if len(str(security_config)) > 10000 and total_ram_gb < 16:
            security_clear_results = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if security_clear_results.get('success'):
                changes['metadata']['memory_optimizations_applied'] += 1
                logger.debug("Memory optimized before security config processing")
    
        # Security parameter updates
        global DEFAULT_PERCENTILE, DEFAULT_ATTACK_THRESHOLD, FALSE_NEGATIVE_COST, SECURITY_METRICS
        
        if 'percentile' in security_config:
            old_value = DEFAULT_PERCENTILE
            new_value = max(50.0, min(99.99, float(security_config['percentile'])))
            if old_value != new_value:
                DEFAULT_PERCENTILE = new_value
                changes['security']['percentile'] = {'from': old_value, 'to': new_value}
        
        if 'attack_threshold' in security_config:
            old_value = DEFAULT_ATTACK_THRESHOLD
            new_value = max(0.001, min(1.0, float(security_config['attack_threshold'])))
            if old_value != new_value:
                DEFAULT_ATTACK_THRESHOLD = new_value
                changes['security']['attack_threshold'] = {'from': old_value, 'to': new_value}
        
        if 'false_negative_cost' in security_config:
            old_value = FALSE_NEGATIVE_COST
            new_value = max(0.1, min(100.0, float(security_config['false_negative_cost'])))
            if old_value != new_value:
                FALSE_NEGATIVE_COST = new_value
                changes['security']['false_negative_cost'] = {'from': old_value, 'to': new_value}
        
        if 'enable_security_metrics' in security_config:
            old_value = SECURITY_METRICS
            new_value = bool(security_config['enable_security_metrics'])
            if old_value != new_value:
                SECURITY_METRICS = new_value
                changes['security']['enable_security_metrics'] = {'from': old_value, 'to': new_value}
    
    except Exception as e:
        logger.error(f"Error updating security configuration: {e}")
        raise ValueError(f"Invalid security configuration: {e}")
    
    # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before data config processing
    try:
        data_config = config.get('data', {})
        
        if len(str(data_config)) > 10000 and total_ram_gb < 16:
            data_clear_results = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if data_clear_results.get('success'):
                changes['metadata']['memory_optimizations_applied'] += 1
                logger.debug("Memory optimized before data config processing")
    
        # Data parameter updates
        global NORMAL_SAMPLES, ATTACK_SAMPLES, FEATURES, ANOMALY_FACTOR, RANDOM_STATE
        
        if 'normal_samples' in data_config:
            old_value = NORMAL_SAMPLES
            new_value = max(10, min(1000000, int(data_config['normal_samples'])))
            if old_value != new_value:
                NORMAL_SAMPLES = new_value
                changes['data']['normal_samples'] = {'from': old_value, 'to': new_value}
        
        if 'attack_samples' in data_config:
            old_value = ATTACK_SAMPLES
            new_value = max(5, min(100000, int(data_config['attack_samples'])))
            if old_value != new_value:
                ATTACK_SAMPLES = new_value
                changes['data']['attack_samples'] = {'from': old_value, 'to': new_value}
        
        if 'features' in data_config:
            old_value = FEATURES
            new_value = max(1, min(10000, int(data_config['features'])))
            if old_value != new_value:
                FEATURES = new_value
                changes['data']['features'] = {'from': old_value, 'to': new_value}
        
        if 'anomaly_factor' in data_config:
            old_value = ANOMALY_FACTOR
            new_value = max(0.1, min(100.0, float(data_config['anomaly_factor'])))
            if old_value != new_value:
                ANOMALY_FACTOR = new_value
                changes['data']['anomaly_factor'] = {'from': old_value, 'to': new_value}
        
        if 'random_state' in data_config:
            old_value = RANDOM_STATE
            new_value = int(data_config['random_state'])
            if old_value != new_value:
                RANDOM_STATE = new_value
                changes['data']['random_state'] = {'from': old_value, 'to': new_value}
    
    except Exception as e:
        logger.error(f"Error updating data configuration: {e}")
        raise ValueError(f"Invalid data configuration: {e}")
    
    # Handle preset application with comprehensive validation and memory optimization
    try:
        preset_config = config.get('presets', {})
        
        # Clear memory before intensive preset processing if needed
        if len(str(preset_config)) > 5000 and total_ram_gb < 16:
            preset_clear_results = enhanced_clear_memory(aggressive=False, hardware_data=hardware_data)
            if preset_clear_results.get('success'):
                changes['metadata']['memory_optimizations_applied'] += 1
                logger.debug("Memory optimized before preset processing")
        
        current_preset = preset_config.get('current_preset')
        if current_preset and current_preset in PRESET_CONFIGS:
            logger.debug(f"Applying preset configuration: {current_preset}")
            
            # Apply preset-specific optimizations
            preset_data = PRESET_CONFIGS[current_preset]
            if isinstance(preset_data, dict):
                # Record preset application
                changes['system']['preset_applied'] = {
                    'preset': current_preset,
                    'description': preset_data.get('metadata', {}).get('description', 'No description'),
                    'compatibility': preset_data.get('metadata', {}).get('compatibility', [])
                }
                
                # Update global constants from preset if they haven't been explicitly overridden
                preset_training = preset_data.get('training', {})
                preset_model = preset_data.get('model', {})
                
                # Only apply preset values if they haven't been changed by direct config
                if 'batch_size' not in changes['training'] and 'batch_size' in preset_training:
                    DEFAULT_BATCH_SIZE = max(1, min(2048, int(preset_training['batch_size'])))
                    changes['training']['batch_size_from_preset'] = DEFAULT_BATCH_SIZE
                
                if 'epochs' not in changes['training'] and 'epochs' in preset_training:
                    DEFAULT_EPOCHS = max(1, min(10000, int(preset_training['epochs'])))
                    changes['training']['epochs_from_preset'] = DEFAULT_EPOCHS
                
                if 'encoding_dim' not in changes['model'] and 'encoding_dim' in preset_model:
                    DEFAULT_ENCODING_DIM = max(1, min(1024, int(preset_model['encoding_dim'])))
                    changes['model']['encoding_dim_from_preset'] = DEFAULT_ENCODING_DIM
    
    except Exception as e:
        logger.warning(f"Error applying preset configuration: {e}")
        changes['system']['preset_error'] = str(e)
    
    # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
    # Aggressive cleanup after configuration processing completion
    try:
        final_clear_results = enhanced_clear_memory(
            aggressive=True,  # Aggressive final cleanup
            hardware_data=hardware_data
        )
        
        if final_clear_results.get('success'):
            changes['metadata']['final_memory_cleanup'] = {
                'actions': final_clear_results.get('actions_taken', []),
                'timestamp': datetime.now().isoformat(),
                'memory_impact': {
                    'optimization_effective': True,
                    'final_cleanup_performed': True
                }
            }
            
            changes['metadata']['memory_optimizations_applied'] += 1
            logger.debug(f"Final memory optimization: {', '.join(final_clear_results.get('actions_taken', []))}")
            
    except Exception as e:
        logger.debug(f"Final memory optimization failed: {e}")
        changes['metadata']['final_cleanup_error'] = str(e)
    
    # Calculate total changes and log summary
    total_changes = sum(len(section) for section in changes.values() 
                       if isinstance(section, dict) and section != changes['metadata'])
    changes['metadata']['total_changes'] = total_changes
    
    if total_changes > 0:
        logger.debug(f"Updated {total_changes} global configuration parameters")
        logger.debug(f"Applied {changes['metadata']['memory_optimizations_applied']} memory optimizations during processing")
        
        # Log significant changes
        for section_name, section_changes in changes.items():
            if isinstance(section_changes, dict) and section_changes and section_name != 'metadata':
                logger.debug(f"Updated {section_name}: {list(section_changes.keys())}")
        
        # Log preset application if it occurred
        if 'preset_applied' in changes.get('system', {}):
            preset_info = changes['system']['preset_applied']
            logger.debug(f"Applied preset '{preset_info['preset']}': {preset_info['description']}")
    else:
        logger.debug("No global configuration changes applied")
    
    # Final validation of global state
    try:
        validate_global_config_state()
        logger.debug("Global configuration state validation passed")
    except Exception as e:
        logger.error(f"Global configuration state validation failed: {e}")
        raise ValueError(f"Global configuration state became invalid: {e}")
    
    # Save change log for audit trail
    try:
        save_change_log(changes)
    except Exception as e:
        logger.debug(f"Failed to save configuration change log: {e}")

# Helper functions for the updated configuration system
def save_change_log(changes: Dict[str, Any]) -> None:
    """Save configuration change log for audit trail."""
    try:
        log_dir = Path(globals().get('LOG_DIR', './logs'))
        log_dir.mkdir(exist_ok=True)
        change_log_dir = log_dir / "deep_learning_config_changes"
        change_log_dir.mkdir(exist_ok=True)
        
        log_file = change_log_dir / f"change_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # Load existing log or create new
        if log_file.exists():
            with open(log_file, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
                logger.debug(f"Loaded existing change log from {log_file}")
        else:
            log_data = {'changes': []}
            logger.debug(f"Created new change log at {log_file}")
        
        # Add new changes
        log_data['changes'].append(changes)
        logger.debug(f"Added {len(changes)} changes to the log")
        
        # Keep only last 100 changes
        if len(log_data['changes']) > 100:
            logger.debug(f"Trimming change log to last 100 entries")
            log_data['changes'] = log_data['changes'][-100:]
        
        # Save updated log
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
            logger.debug(f"Saved change log to {log_file}")
            
    except Exception as e:
        logger.debug(f"Failed to save change log: {e}")

def validate_global_config_state() -> None:
    """Validate the current global configuration state for consistency."""
    try:
        # Check critical variables exist and have valid values
        required_globals = {
            'DEFAULT_BATCH_SIZE': (int, lambda x: x > 0),
            'DEFAULT_EPOCHS': (int, lambda x: x > 0),
            'LEARNING_RATE': ((int, float), lambda x: x > 0),
            'FEATURES': (int, lambda x: x > 0),
            'NORMAL_SAMPLES': (int, lambda x: x > 0)
        }
        
        for var_name, (expected_type, validator) in required_globals.items():
            if var_name not in globals():
                raise ValueError(f"Required global variable {var_name} is missing")
            
            value = globals()[var_name]
            if not isinstance(value, expected_type):
                raise TypeError(f"{var_name} must be of type {expected_type}, got {type(value)}")
            
            if not validator(value):
                raise ValueError(f"{var_name} has invalid value: {value}")
        
        # Check list length compatibility
        hidden_dims = globals().get('HIDDEN_LAYER_SIZES', [])
        dropout_rates = globals().get('DROPOUT_RATES', [])
        
        if len(hidden_dims) != len(dropout_rates):
            raise ValueError(f"HIDDEN_LAYER_SIZES and DROPOUT_RATES must have same length: "
                           f"{len(hidden_dims)} != {len(dropout_rates)}")
        
    except Exception as e:
        logger.error(f"Global configuration state validation failed: {e}")
        raise

# System initialization validation and setup
#def initialize_system() -> Tuple[Dict[str, Any], Dict[str, Any]]:
def initialize_system() -> Tuple[Dict[str, Any], Dict[str, Any], logging.Logger]:
    """
    Initialize the complete system with comprehensive setup and validation.
    
    This function performs a complete system initialization by leveraging the
    existing System Check Framework and loading_screen functionality.
    
    Returns:
        Tuple containing:
            - Dict: Comprehensive system status and configuration
            - Dict: Active configuration dictionary
            - Logger: Configured logger instance
    
    Raises:
        RuntimeError: If critical system components fail to initialize
        SystemExit: If user chooses to quit during initialization
    """
    initialization_start = time.time()
    console = Console()
    
    # Console capability detection
    is_tty = sys.stdout.isatty()
    supports_color = is_tty and hasattr(sys.stdout, 'isatty')
    console_width = max(60, getattr(console, 'width', 80))
    
    # Create initialization status table with enhanced styling
    init_table = Table(
        title=f"\n[bold]DEEP LEARNING SYSTEM INITIALIZATION[/bold]",
        box=box.DOUBLE_EDGE,
        header_style="bold yellow",
        border_style="bold cyan",
        title_style="bold green",
        title_justify="left",
        show_lines=True,
        expand=True,
        width=min(140, console.width - 4)
    )
    
    init_table.add_column("Step", style="bold cyan", width=28, no_wrap=True)
    init_table.add_column("Status", width=8, justify="left")
    init_table.add_column("Duration", width=8, justify="left", style="bold magenta")
    init_table.add_column("Details", style="bold", min_width=60)
    init_table.add_column("Health", width=10, justify="left", style="bold")
    
    initialization_steps = []
    
    def add_step(step_name: str, status: str, duration: float, details: str, health_score: Optional[float] = None):
        """Add a step to the initialization tracking with enhanced metadata."""
        status_styles = {
            "SUCCESS": "bold green",
            "FAILED": "bold red", 
            "WARNING": "bold yellow",
            "SKIPPED": "bold blue",
            "PARTIAL": "bold orange1"
        }
        status_style = status_styles.get(status, "bold white")
        
        # Format health score
        health_display = ""
        if health_score is not None:
            if health_score >= 90:
                health_display = f"[bold green]{health_score:.0f}%[/bold green]"
            elif health_score >= 70:
                health_display = f"[bold yellow]{health_score:.0f}%[/bold yellow]"
            else:
                health_display = f"[bold red]{health_score:.0f}%[/bold red]"
        
        initialization_steps.append((step_name, status, duration, details, status_style, health_display))
    
    # Initialize system state tracking
    system_state = {
        'initialization_start': datetime.now(),
        'steps_completed': [],
        'errors_encountered': [],
        'warnings_issued': [],
        'health_metrics': {}
    }
    
    try:
        # Step 1: Enhanced Early Setup - Basic configuration with hardware detection
        step_start = time.time()

        # Use the existing setup_directories function for consistency
        try:
            # Create a temporary logger for setup_directories if needed
            # temp_logger = logging.getLogger(__name__)
            # if not temp_logger.handlers:
            #     temp_logger.addHandler(logging.StreamHandler())
            #     temp_logger.setLevel(logging.WARNING)
            
            log_dir = Path(__file__).resolve().parent / "logs"
            logger = setup_logging(log_dir)
            
            # Use the existing directory setup function
            #essential_dirs = setup_directories(temp_logger)
            essential_dirs = setup_directories(logger)
            directories_created = list(essential_dirs.keys())
            
            # Validate write access for all directories
            write_access_issues = []
            for dir_name, dir_path in essential_dirs.items():
                try:
                    # Verify write access
                    test_file = dir_path / ".write_test"
                    test_file.touch()
                    test_file.unlink()
                except Exception as e:
                    write_access_issues.append(f"Write access failed for {dir_name}: {e}")
                    system_state['errors_encountered'].append(f"Write access failed for {dir_name}: {e}")
            
            # Remove directories that failed write access from directories_created
            if write_access_issues:
                for issue in write_access_issues:
                    # Extract directory name from error message more safely
                    try:
                        dir_name = issue.split(' ')[4].rstrip(':')
                        if dir_name in directories_created:
                            directories_created.remove(dir_name)
                    except (IndexError, ValueError):
                        # If parsing fails, log but continue
                        logger.debug(f"Could not parse directory name from error: {issue}")

        except Exception as e:
            # Fallback to minimal directory creation if setup_directories fails
            system_state['errors_encountered'].append(f"setup_directories failed: {e}")
            
            # Create minimal essential directories manually
            base_dir = Path(__file__).resolve().parent
            essential_dirs = {
                'logs': base_dir / "logs",
                'models': base_dir / "models",
                'data': base_dir / "data",
                'config': base_dir / "config",
                'reports': base_dir / "reports",
                'tensorboard': base_dir / "tensorboard",
                'cache': base_dir / "cache",
                'exports': base_dir / "exports",
                'checkpoints': base_dir / "checkpoints" / f"checkpoints_v{VERSION_INFO['torch']}"
            }
            
            directories_created = []
            for dir_name, dir_path in essential_dirs.items():
                try:
                    dir_path.mkdir(parents=True, exist_ok=True)
                    # Verify write access
                    test_file = dir_path / ".write_test"
                    test_file.touch()
                    test_file.unlink()
                    directories_created.append(dir_name)
                except Exception as e:
                    system_state['errors_encountered'].append(f"Directory creation failed for {dir_name}: {e}")
        
        # Early hardware detection for adaptive configuration
        try:
            initial_hardware = check_hardware(min_disk_gb=1.0, include_memory_usage=True)
            cuda_available = initial_hardware.get('cuda', {}).get('available', False)
            total_ram_gb = initial_hardware.get('system_ram', {}).get('ram_total_gb', 4.0)
            logical_cores = initial_hardware.get('cpu_cores', {}).get('logical_cores', 1)
            system_state['hardware_detected'] = initial_hardware
        except Exception as e:
            cuda_available = torch.cuda.is_available()
            total_ram_gb = psutil.virtual_memory().total / (1024**3)
            logical_cores = os.cpu_count() or 1
            system_state['errors_encountered'].append(f"Hardware detection failed: {e}")
        
        # Hardware-aware system configuration
        config_result = configure_system()
        
        # Enhanced seed configuration with hardware awareness
        seed_config = set_seed(42, system_state.get('hardware_detected'))
        
        # Setup logging with enhanced configuration
        # log_dir = essential_dirs.get('logs', Path(__file__).resolve().parent / "logs")
        # logger = setup_logging(log_dir)

        # Configure global directories to ensure consistency across the system
        try:
            configure_directories(logger)
        except Exception as e:
            logger.warning(f"configure_directories failed: {e}")
            system_state['warnings_issued'].append(f"Global directory configuration failed: {e}")

        step_duration = time.time() - step_start
        
        # Calculate early setup health score
        early_health = 100.0
        if system_state['errors_encountered']:
            early_health -= len(system_state['errors_encountered']) * 15
        if len(directories_created) < len(essential_dirs):
            early_health -= 20
        early_health = max(0, early_health)
        
        system_state['health_metrics']['early_setup'] = early_health
        
        add_step(
            "Enhanced Early Setup", 
            "SUCCESS" if early_health >= 80 else "WARNING", 
            step_duration,
            f"Hardware-aware configuration applied\n"
            f"Directories: {', '.join(directories_created)}\n" 
            f"Seed configured: {seed_config.get('base_seed', 42)}\n"
            f"Hardware context: {'CUDA' if cuda_available else 'CPU'} ({total_ram_gb:.1f}GB RAM)",
            early_health
        )
        
        # Enhanced logging initialization 
        logger.info("=" * 80)
        logger.info("DEEP LEARNING SYSTEM INITIALIZATION STARTING")
        logger.info("=" * 80)
        logger.info(f"Hardware detected: {logical_cores} CPU cores, {total_ram_gb:.1f}GB RAM, CUDA: {cuda_available}")
        logger.info(f"Essential directories: {len(directories_created)}/{len(essential_dirs)} created successfully")
        
        # Step 2: Comprehensive System Validation (OPTIMIZED - No Redundancy)
        step_start = time.time()
        
        # Set enhanced global exception handler
        sys.excepthook = enhanced_global_exception_handler
        
        # Initialize global function availability
        global enhanced_clear_memory, enhanced_monitor_performance, establish_performance_baseline
        
        # Run comprehensive system checks with extended diagnostics
        try:
            # Use loading_screen with all enhancements enabled - THIS IS THE SINGLE SOURCE OF TRUTH
            # All system checks are performed here once and only once
            system_ready, system_check_results = loading_screen(
                logger=logger,
                # Enable extended system checks
                extended=True,
                # Include performance baseline
                include_performance=True,
                # Pass detected hardware
                hardware_data=system_state.get('hardware_detected')
            )
            
            if not system_ready:
                raise RuntimeError("System checks failed or user cancelled initialization")
            
            # EXTRACT results from the loading_screen checks that were already run
            # This is lightweight since the actual validation was already done
            try:
                # Store check results in system state
                system_state['system_check_results'] = system_check_results or {}
                
                # Build summary from existing results (no duplicate checks)
                additional_checks = []
                
                if system_check_results:
                    if 'hardware' in system_check_results and system_check_results['hardware'].passed:
                        additional_checks.append("Hardware validation: PASSED")
                    else:
                        additional_checks.append("Hardware validation: ISSUES")
                    
                    if 'logging_setup' in system_check_results:
                        result = system_check_results['logging_setup']
                        compliance_score = result.details.get('compliance_score', 0) if isinstance(result.details, dict) else 0
                        additional_checks.append(f"Logging setup: {'PASSED' if result.passed else 'ISSUES'} ({compliance_score}% compliance)")
                    
                    if 'seed_config' in system_check_results:
                        result = system_check_results['seed_config']
                        compliance_score = result.metadata.get('compliance_score', 0) if result.metadata else 0
                        additional_checks.append(f"Reproducibility: {'PASSED' if result.passed else 'ISSUES'} ({compliance_score}% compliance)")
                    
                    if 'performance_monitoring' in system_check_results:
                        result = system_check_results['performance_monitoring']
                        additional_checks.append(f"Performance monitoring: {'AVAILABLE' if result.passed else 'LIMITED'}")
                    
                    if 'memory_management' in system_check_results:
                        result = system_check_results['memory_management']
                        comprehensive = False
                        if result.passed and isinstance(result.details, dict):
                            comprehensive = result.details.get('capabilities', {}).get('comprehensive_management', False)
                        additional_checks.append(f"Memory management: {'COMPREHENSIVE' if comprehensive else 'BASIC' if result.passed else 'LIMITED'}")
                else:
                    additional_checks.append("System check results not available")
                
            except Exception as e:
                additional_checks = [f"Error extracting system check results: {e}"]
                system_state['errors_encountered'].append(f"System check result extraction failed: {e}")
            
            step_duration = time.time() - step_start
            
            # Calculate system validation health score based on results
            validation_health = 100.0
            if system_check_results:
                passed_checks = sum(1 for result in system_check_results.values() if result and result.passed)
                total_checks = len(system_check_results)
                if total_checks > 0:
                    validation_health = (passed_checks / total_checks) * 100
            
            system_state['health_metrics']['system_validation'] = validation_health
            
            add_step(
                "Comprehensive System Validation", 
                "SUCCESS" if validation_health >= 80 else "PARTIAL" if validation_health >= 60 else "WARNING", 
                step_duration,
                f"Extended system checks completed\n" + "\n".join(additional_checks),
                validation_health
            )
            
        except Exception as e:
            step_duration = time.time() - step_start
            system_state['errors_encountered'].append(f"System validation failed: {e}")
            system_state['health_metrics']['system_validation'] = 0
            
            add_step(
                "Comprehensive System Validation", 
                "FAILED", 
                step_duration,
                f"System validation failed: {str(e)}\nCritical components may be unavailable",
                0
            )
            raise RuntimeError(f"System validation failed: {e}")
        
        # Step 3: Enhanced Configuration System with Validation
        step_start = time.time()
        config_details = []
        config_changes = []
        config_health = 100.0
        
        try:
            # Temporarily reduce logging verbosity for cleaner initialization display
            #original_level = logger.level
            #logger.setLevel(logging.WARNING)
            
            # Load and validate configuration with enhanced error handling
            try:
                config = initialize_config()
                config_details.append(f"Configuration loaded from: {CONFIG_FILE.name}")
                # Restore logging level
                #logger.setLevel(original_level)
                
                # Enhanced configuration validation
                try:
                    validate_config(config)
                    config_details.append("Configuration validation: PASSED")
                except ValueError as validation_error:
                    config_health -= 30
                    config_details.append(f"Configuration validation: FAILED - {str(validation_error)}")
                    system_state['warnings_issued'].append(f"Configuration validation failed: {validation_error}")
                    
                    # Enhanced interactive error handling
                    if sys.stdin.isatty():
                        console.print(Panel.fit(
                            f"[bold yellow][WARN] Configuration Validation Issue[/bold yellow]\n\n"
                            f"[white]Issue detected:[/white] {str(validation_error)}\n\n"
                            f"[dim]Available options:[/dim]\n"
                            f"[green]1.[/green] Use default configuration (recommended)\n"
                            f"[blue]2.[/blue] Continue with current configuration (may cause issues)\n"
                            f"[red]3.[/red] Exit initialization\n",
                            border_style="yellow",
                            title="[!] Configuration Problem",
                            padding=(1, 2)
                        ))
                        
                        choice = Prompt.ask(
                            "Choose an option",
                            choices=["1", "2", "3"],
                            default="1"
                        )
                        
                        if choice == "1":
                            config = get_default_config()
                            save_config(config)
                            config_details.append("Applied default configuration")
                            config_changes.append({
                                'section': 'SYSTEM',
                                'parameter': 'configuration_source',
                                'old_value': 'user_config',
                                'new_value': 'default_config',
                                'source': 'validation_fallback'
                            })
                            # Partial recovery
                            config_health = 85
                        elif choice == "2":
                            config_details.append("Continuing with invalid configuration (user choice)")
                            # Significant health reduction
                            config_health = 50
                        else:
                            # choice == "3"
                            logger.info("User chose to exit due to configuration issues")
                            raise SystemExit("Configuration validation failed and user chose to exit")
                    else:
                        # Non-interactive mode - use defaults
                        config = get_default_config()
                        save_config(config)
                        config_details.append("Applied default configuration (non-interactive mode)")
                        # Partial recovery
                        config_health = 85
                
            except Exception as config_error:
                # Ensure logging level is restored
                #logger.setLevel(original_level)
                config_health -= 50
                config = get_default_config()
                config_details.append(f"Emergency fallback to default config: {str(config_error)}")
                system_state['errors_encountered'].append(f"Configuration loading failed: {config_error}")
            
            # Apply configuration with change tracking
            previous_config = get_current_config() if hasattr(sys.modules[__name__], 'CURRENT_CONFIG') else {}
            update_global_config(config)
            
            # Enhanced configuration metadata
            preset_name = config.get('_preset_name', 'custom')
            config_details.append(f"Active preset: {preset_name}")
            config_details.append(f"Configuration parameters: {len(config)} total")
            
            # Detect and track configuration changes
            if hasattr(config, '_changes_applied'):
                config_changes.extend(config._changes_applied)
            
            # Hardware-specific configuration adjustments
            if system_state.get('hardware_detected'):
                hw = system_state['hardware_detected']
                if hw.get('cuda', {}).get('available'):
                    gpu_count = hw.get('cuda', {}).get('gpu_count', 0)
                    config_details.append(f"GPU acceleration: {gpu_count} device(s) detected")
                    if gpu_count > 1:
                        config_details.append("Multi-GPU configuration applied")
                else:
                    config_details.append("CPU-only configuration applied")
            
            # Display configuration changes if any were made
            if config_changes and len(config_changes) > 0:
                try:
                    display_configuration_changes(config_changes, console, logger)
                except Exception as display_error:
                    logger.debug(f"Could not display configuration changes: {display_error}")
            
            step_duration = time.time() - step_start
            system_state['health_metrics']['configuration'] = config_health
            system_state['active_config'] = config
            system_state['config_changes'] = config_changes
            
            add_step(
                "Enhanced Configuration System", 
                "SUCCESS" if config_health >= 80 else "PARTIAL" if config_health >= 60 else "WARNING", 
                step_duration,
                "\n".join(config_details),
                config_health
            )
            
        except Exception as e:
            #if 'original_level' in locals():
            #    logger.setLevel(original_level)
            
            step_duration = time.time() - step_start
            config = get_default_config()
            update_global_config(config)
            system_state['errors_encountered'].append(f"Configuration system failed: {e}")
            system_state['health_metrics']['configuration'] = 25
            system_state['active_config'] = config
            
            add_step(
                "Enhanced Configuration System", 
                "FAILED", 
                step_duration,
                f"Configuration system failed: {str(e)}\nUsing emergency default configuration",
                25
            )
        
        # Step 4: Enhanced Model Variants Initialization
        step_start = time.time()
        model_details = []
        model_health = 100.0
        
        try:
            # Initialize model variants with enhanced error handling
            #original_level = logger.level
            # Reduce verbosity
            #logger.setLevel(logging.WARNING)
            
            # Initialize model variants
            initialize_model_variants(silent=True)
            
            if not MODEL_VARIANTS:
                raise RuntimeError("No model variants could be initialized")
            
            # Enhanced validation of model variants
            #model_status = validate_model_variants(logger, silent=True)
            #variant_status = validate_model_variants(silent=False)
            variant_status = validate_model_variants(logger, silent=True)
            available_variants = [name for name, status in variant_status.items() if status == 'available']
            failed_variants = [name for name, status in variant_status.items() if status != 'available']
            
            # Restore logging level
            #logger.setLevel(original_level)
            
            if not available_variants:
                raise RuntimeError("No working model variants available")
            
            # Calculate model health based on availability
            total_variants = len(MODEL_VARIANTS)
            available_count = len(available_variants)
            model_health = (available_count / total_variants) * 100 if total_variants > 0 else 0
            
            model_details.append(f"Initialized: {available_count}/{total_variants} variants")
            model_details.append(f"Available variants: {', '.join(available_variants)}")
            
            if failed_variants:
                model_details.append(f"Failed variants: {', '.join(failed_variants)}")
                system_state['warnings_issued'].extend([f"Model variant failed: {name}" for name in failed_variants])
            
            # Enhanced model validation with performance testing
            if available_variants:
                try:
                    # Test a representative model variant
                    test_variant = available_variants[0]
                    test_model = MODEL_VARIANTS[test_variant]
                    
                    # Quick validation test
                    # Dummy input for model validation
                    test_input = torch.randn(1, 10)
                    if cuda_available and hasattr(test_model, 'cuda'):
                        try:
                            test_model.cuda()
                            test_input = test_input.cuda()
                        except Exception:
                            # Fall back to CPU
                            pass
                    
                    # Quick forward pass test
                    with torch.no_grad():
                        _ = test_model(test_input)
                    
                    model_details.append(f"Model validation: PASSED (tested {test_variant})")
                    
                except Exception as test_error:
                    model_health -= 20
                    model_details.append(f"Model validation: PARTIAL - {str(test_error)}")
                    system_state['warnings_issued'].append(f"Model validation test failed: {test_error}")
            
            step_duration = time.time() - step_start
            system_state['health_metrics']['models'] = model_health
            system_state['model_variants'] = {
                'total': total_variants,
                'available': available_count,
                'available_names': available_variants,
                'failed_names': failed_variants,
                'status': variant_status
            }
            
            add_step(
                "Enhanced Model Variants", 
                "SUCCESS" if model_health >= 80 else "PARTIAL" if model_health >= 60 else "WARNING", 
                step_duration,
                "\n".join(model_details),
                model_health
            )
            
        except Exception as e:
            #if 'original_level' in locals():
            #    logger.setLevel(original_level)
            
            step_duration = time.time() - step_start
            system_state['errors_encountered'].append(f"Model initialization failed: {e}")
            system_state['health_metrics']['models'] = 0
            
            add_step(
                "Enhanced Model Variants", 
                "FAILED", 
                step_duration,
                f"Model initialization failed: {str(e)}\nDeep learning models unavailable",
                0
            )
            raise RuntimeError(f"Model initialization failed: {e}")
        
        # Step 5: Enhanced Performance Baseline with Hardware Optimization
        step_start = time.time()
        performance_health = 100.0
        
        try:
            # Establish comprehensive performance baseline
            #original_level = logger.level
            # Reduce verbosity
            #logger.setLevel(logging.WARNING)
            
            # Pass hardware data for optimized testing
            performance_metrics = establish_performance_baseline(
                hardware_data=system_state.get('hardware_detected')
            )
            
            # Restore logging level
            #logger.setLevel(original_level)
            
            # Analyze performance results
            baseline_details = []
            if 'error' in performance_metrics:
                performance_health = 25
                baseline_details.append(f"Baseline failed: {performance_metrics['error']}")
                system_state['warnings_issued'].append(f"Performance baseline failed: {performance_metrics['error']}")
            else:
                # Analyze individual component performance
                baselines = performance_metrics.get('baselines', {})
                summary = performance_metrics.get('summary', {})
                
                # CPU performance analysis
                if 'cpu' in baselines and 'cpu_error' not in baselines:
                    cpu_gflops = baselines['cpu'].get('gflops', 0)
                    baseline_details.append(f"CPU: {cpu_gflops:.2f} GFLOPS")
                    if cpu_gflops < 1:
                        performance_health -= 15
                else:
                    baseline_details.append("CPU: benchmark failed")
                    performance_health -= 20
                
                # Memory performance analysis
                if 'memory' in baselines and 'memory_error' not in baselines:
                    mem_speeds = [
                        baseline.get('allocation_speed_mbs', 0) 
                        for baseline in baselines['memory'].values() 
                        if isinstance(baseline, dict)
                    ]
                    avg_speed = np.mean(mem_speeds) if mem_speeds else 0
                    baseline_details.append(f"Memory: {avg_speed:.1f} MB/s average")
                    if avg_speed < 100:
                        performance_health -= 10
                else:
                    baseline_details.append("Memory: benchmark failed")
                    performance_health -= 15
                
                # GPU performance analysis
                if cuda_available:
                    if 'gpu' in baselines and 'gpu_error' not in baselines:
                        gpu_gflops = []
                        for gpu_data in baselines['gpu'].values():
                            if isinstance(gpu_data, dict):
                                gpu_gflops.append(gpu_data.get('gflops', 0))
                        
                        if gpu_gflops:
                            max_gflops = max(gpu_gflops)
                            baseline_details.append(f"GPU: {max_gflops:.1f} GFLOPS (best)")
                            if max_gflops < 100:
                                performance_health -= 10
                        else:
                            baseline_details.append("GPU: no performance data")
                            performance_health -= 15
                    else:
                        baseline_details.append("GPU: benchmark failed")
                        performance_health -= 15
                else:
                    baseline_details.append("GPU: not available (CPU-only system)")
                
                # I/O performance analysis
                if 'io' in baselines and 'io_error' not in baselines:
                    io_data = baselines['io']
                    write_speed = io_data.get('write_speed_mbs', 0)
                    read_speed = io_data.get('read_speed_mbs', 0)
                    baseline_details.append(f"I/O: {write_speed:.1f}/{read_speed:.1f} MB/s (write/read)")
                    if write_speed < 50 or read_speed < 50:
                        performance_health -= 5
                else:
                    baseline_details.append("I/O: benchmark failed")
                    performance_health -= 10
                
                # Overall system capability
                overall_capability = summary.get('overall_capability', 'standard')
                baseline_details.append(f"System class: {overall_capability}")
            
            step_duration = time.time() - step_start
            system_state['health_metrics']['performance'] = max(0, performance_health)
            system_state['performance_baseline'] = performance_metrics
            
            add_step(
                "Enhanced Performance Baseline", 
                "SUCCESS" if performance_health >= 80 else "PARTIAL" if performance_health >= 60 else "WARNING", 
                step_duration,
                "\n".join(baseline_details),
                max(0, performance_health)
            )
            
        except Exception as e:
            #if 'original_level' in locals():
            #    logger.setLevel(original_level)
            
            step_duration = time.time() - step_start
            performance_metrics = {'baseline_failed': str(e)}
            system_state['errors_encountered'].append(f"Performance baseline failed: {e}")
            system_state['health_metrics']['performance'] = 25
            system_state['performance_baseline'] = performance_metrics
            
            add_step(
                "Enhanced Performance Baseline", 
                "WARNING", 
                step_duration,
                f"Performance baseline failed: {str(e)}\nContinuing with default performance assumptions",
                25
            )
        
        # Step 6: System Integration and Final Validation (SIMPLIFIED - no redundant checks)
        step_start = time.time()
        integration_health = 100.0
        integration_details = []
        
        try:
            # Validate system integration using EXISTING results instead of re-running checks
            integration_checks = []
            
            # Use results from the system checks that were already run
            existing_results = system_state.get('system_check_results', {})
            
            # Check global exception handler (extract from existing results)
            if 'exception_handler' in existing_results:
                exception_result = existing_results['exception_handler']
                if exception_result.passed:
                    integration_checks.append("Exception handler: CONFIGURED")
                else:
                    integration_checks.append("Exception handler: BASIC")
                    integration_health -= 10
            else:
                integration_checks.append("Exception handler: UNKNOWN")
                integration_health -= 15
            
            # Validate logging system integration (from existing results)
            if 'logging_setup' in existing_results:
                logging_check = existing_results['logging_setup']
                if logging_check.passed:
                    compliance = logging_check.details.get('compliance_score', 0) if isinstance(logging_check.details, dict) else 0
                    integration_checks.append(f"Logging integration: PASSED ({compliance}%)")
                    if compliance < 100:
                        integration_health -= (100 - compliance) * 0.1
                else:
                    integration_checks.append("Logging integration: ISSUES")
                    integration_health -= 20
            else:
                integration_checks.append("Logging integration: UNKNOWN")
                integration_health -= 20
            
            # Validate hardware integration (from existing results)
            if 'hardware' in existing_results:
                hardware_check = existing_results['hardware']
                if hardware_check.passed:
                    integration_checks.append("Hardware integration: OPTIMAL")
                else:
                    integration_checks.append("Hardware integration: SUBOPTIMAL")
                    integration_health -= 15
            else:
                integration_checks.append("Hardware integration: UNKNOWN")
                integration_health -= 15
            
            # Validate model-hardware integration
            if system_state.get('model_variants', {}).get('available', 0) > 0:
                if cuda_available:
                    integration_checks.append("Model-GPU integration: ENABLED")
                else:
                    integration_checks.append("Model-CPU integration: ENABLED")
            else:
                integration_checks.append("Model integration: FAILED")
                integration_health -= 25
            
            # Final memory cleanup and validation
            try:
                cleanup_result = enhanced_clear_memory(
                    aggressive=False, 
                    hardware_data=system_state.get('hardware_detected')
                )
                if cleanup_result.get('success', False):
                    integration_checks.append("Memory management: VALIDATED")
                else:
                    integration_checks.append("Memory management: ISSUES")
                    integration_health -= 10
            except Exception as e:
                integration_checks.append(f"Memory management: ERROR - {e}")
                integration_health -= 15
            
            integration_details = integration_checks
            
            step_duration = time.time() - step_start
            system_state['health_metrics']['integration'] = max(0, integration_health)
            
            add_step(
                "System Integration Validation", 
                "SUCCESS" if integration_health >= 90 else "PARTIAL" if integration_health >= 70 else "WARNING", 
                step_duration,
                "\n".join(integration_details),
                max(0, integration_health)
            )
            
        except Exception as e:
            step_duration = time.time() - step_start
            system_state['errors_encountered'].append(f"System integration failed: {e}")
            system_state['health_metrics']['integration'] = 50
            
            add_step(
                "System Integration Validation", 
                "FAILED", 
                step_duration,
                f"Integration validation failed: {str(e)}\nSystem may have compatibility issues",
                50
            )
        
        # Calculate overall system health and display results
        initialization_time = time.time() - initialization_start
        
        # Calculate weighted overall health score
        health_weights = {
            'early_setup': 0.15,
            'system_validation': 0.20, 
            'configuration': 0.20,
            'models': 0.20,
            'performance': 0.10,
            'integration': 0.15
        }
        
        overall_health = 0.0
        for metric, weight in health_weights.items():
            health_value = system_state['health_metrics'].get(metric, 0)
            overall_health += health_value * weight
        
        # Add initialization table rows
        for step_name, status, duration, details, status_style, health_display in initialization_steps:
            init_table.add_row(
                Text(step_name, style="bold cyan"),
                Text(status, style=status_style),
                Text(f"{duration:.2f}s", style="dim"),
                Text(details, style="dim"),
                health_display
            )
        
        # Add comprehensive summary row
        summary_status = "SUCCESS" if overall_health >= 80 else "PARTIAL" if overall_health >= 60 else "WARNING"
        summary_style = "bold white on green" if overall_health >= 80 else "bold white on yellow" if overall_health >= 60 else "bold white on red"
        
        init_table.add_row(
            Text("SYSTEM INITIALIZATION", style="bold white on yellow"),
            #Text(summary_status, style=f"bold white on {'green' if overall_health >= 80 else 'yellow' if overall_health >= 60 else 'red'}"),
            Text(summary_status, style=summary_style),
            Text(f"{initialization_time:.2f}s", style="bold white on yellow"),
            #Text(f"Overall system health: {overall_health:.1f}% | Ready for deep learning operations", style="bold white on green"),
            Text(f"Overall system health: {overall_health:.1f}% | Ready for deep learning operations", style=summary_style),
            f"[bold white on {'green' if overall_health >= 80 else 'yellow' if overall_health >= 60 else 'red'}]{overall_health:.0f}%[/]"
        )
        
        # Display the comprehensive initialization table
        console.print(init_table)
        
        # Add user prompt based on initialization outcome
        banner_width = min(console_width - 8, 100)
        
        try:
            if overall_health >= 80:
                # Success scenario - all systems optimal
                overall_health_score = f"[bold cyan]{overall_health:.1f}%[/]"
                overall_duration = f"[bold cyan]{initialization_time:.2f} seconds[/]"
                model_variants_available = f"[bold cyan]{system_state.get('model_variants', {}).get('available', 0)}[/]"
                hardware = f"[bold cyan]{'GPU-accelerated' if cuda_available else 'CPU-only'} ({logical_cores} cores, {total_ram_gb:.1f} GB RAM)[/]"
                
                success_message = (
                    f"SYSTEM INITIALIZATION COMPLETED SUCCESSFULLY\n\n"
                    f"Overall Health Score: {overall_health_score}\n"
                    f"All critical systems are operational and optimized.\n"
                    f"The system is ready for deep learning operations.\n\n"
                    f"Duration: {overall_duration}\n"
                    f"Model Variants Available: {model_variants_available}\n"
                    f"Hardware: {hardware}\n\n"
                    #f"Continue to system? (Y/n)"
                )
                
                try:
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            #f"[bold green]{success_message}[/]",
                            f"{success_message}",
                            border_style="green",
                            title="SUCCESS",
                            style="bold green",
                            padding=(1, 3),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        #console.print(f"\nSUCCESS:\n{success_message}")
                        print(Fore.GREEN + Style.BRIGHT + f"\nSUCCESS:\n{success_message}")
                except Exception:
                    #console.print(f"\nSUCCESS:\n{success_message}")
                    print(Fore.GREEN + Style.BRIGHT + f"\nSUCCESS:\n{success_message}")
                
                # Handle user choice with safe input
                user_choice = None
                max_attempts = 3
                
                for attempt in range(max_attempts):
                    try:
                        #response = input("\nYour choice: ").strip().lower()
                        #response = input(Fore.YELLOW + Style.BRIGHT + "\nYour choice: ").strip().lower()
                        response = input(Fore.YELLOW + Style.BRIGHT + "\nContinue to system? (Y/n/q): ").strip().lower()
                        
                        if response in ['y', 'yes', '']:
                            user_choice = True
                            break
                        elif response in ['n', 'no', 'q', 'quit']:
                            user_choice = False
                            break
                        else:
                            if attempt < max_attempts - 1:
                                #console.print("Please enter 'y' for yes or 'n' for no.")
                                print(Fore.YELLOW + Style.BRIGHT + "Please enter 'y' for yes or 'n' for no or 'q' for quit.")
                            
                    except (EOFError, KeyboardInterrupt):
                        user_choice = False
                        break
                    except Exception as input_error:
                        if logger:
                            logger.debug(f"Input error on attempt {attempt + 1}: {input_error}")
                        if attempt < max_attempts - 1:
                            #console.print("Input error, please try again.")
                            print(Fore.RED + Style.BRIGHT + "Input error, please try again.")
                
                # Default to continue if no valid choice after max attempts
                if user_choice is None:
                    user_choice = True
                    #console.print("Using default choice: continue")
                    print(Fore.CYAN + Style.BRIGHT + "Using default choice: continue")
                
                # Handle user choice
                if user_choice is False:
                    try:
                        quit_message = (
                            "USER CHOSE TO QUIT\n\n"
                            "You chose to quit despite successful initialization.\n"
                            "System initialization cancelled."
                        )
                        
                        if supports_color and banner_width > 60:
                            console.print(Panel.fit(
                                #f"[bold yellow]{quit_message}[/bold yellow]",
                                f"{quit_message}",
                                border_style="red",
                                style="bold red",
                                title="QUIT",
                                padding=(1, 3),
                                width=min(banner_width, console_width - 4)
                            ))
                        else:
                            #console.print(f"\nQUIT:\n{quit_message}")
                            print(Fore.RED + Style.BRIGHT + f"\nQUIT:\n{quit_message}")
                    except Exception:
                        #console.print(f"\nQUIT:\n{quit_message}")
                        print(Fore.RED + Style.BRIGHT + f"\nQUIT:\n{quit_message}")
                    
                    if logger:
                        logger.info("User chose to quit after successful system initialization")
                    
                    #console.print("Exiting system initialization...")
                    print(Fore.RED + Style.BRIGHT + "Exiting system initialization...")
                    
                    time.sleep(2)
                    sys.exit(0)
                
                # User chose to continue
                try:
                    continue_message = (
                        "CONTINUING TO SYSTEM\n\n"
                        "All systems initialized successfully - proceeding to main system."
                    )
                    
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            #f"[bold green]{continue_message}[/bold green]",
                            f"{continue_message}",
                            border_style="green",
                            title="PROCEEDING",
                            style="bold green",
                            padding=(1, 2),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        #console.print(f"\nPROCEEDING:\n{continue_message}")
                        print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{continue_message}")
                except Exception:
                    #console.print(f"\nPROCEEDING:\n{continue_message}")
                    print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{continue_message}")
                
                if logger:
                    logger.info(f"All systems initialized successfully (health: {overall_health:.1f}%) - user chose to continue")
                    logger.info(f"Model variants available: {system_state.get('model_variants', {}).get('available', 0)}")
                
            elif overall_health >= 60:
                # Partial success - some issues but system can continue
                warning_message = (
                    f"SYSTEM INITIALIZATION COMPLETED WITH WARNINGS\n\n"
                    f"Overall Health Score: {overall_health:.1f}%\n"
                    f"Some components have issues but the system can operate.\n"
                    f"Review the warnings above for details.\n\n"
                    f"Duration: {initialization_time:.2f} seconds\n"
                    f"Errors: {len(system_state.get('errors_encountered', []))}\n"
                    f"Warnings: {len(system_state.get('warnings_issued', []))}\n\n"
                    #f"Continue with reduced functionality? (Y/n)"
                )
                
                try:
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            #f"[bold yellow]{warning_message}[/bold yellow]",
                            f"{warning_message}",
                            border_style="yellow",
                            title="WARNING",
                            subtitle="Issues Detected",
                            style="bold yellow",
                            padding=(1, 3),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        #console.print(f"\nWARNING:\n{warning_message}")
                        print(Fore.YELLOW + Style.BRIGHT + f"\nWARNING:\n{warning_message}")
                except Exception:
                    #console.print(f"\nWARNING:\n{warning_message}")
                    print(Fore.YELLOW + Style.BRIGHT + f"\nWARNING:\n{warning_message}")
                
                # User choice with timeout
                user_choice = None
                max_attempts = 3
                
                for attempt in range(max_attempts):
                    try:
                        #response = input("\nYour choice: ").strip().lower()
                        #response = input(Fore.YELLOW + Style.BRIGHT + "\nYour choice: ").strip().lower()
                        response = input(Fore.YELLOW + Style.BRIGHT + "\nContinue with reduced functionality? (Y/n/q): ").strip().lower()
                        
                        if response in ['y', 'yes', '']:
                            user_choice = True
                            break
                        elif response in ['n', 'no', 'q', 'quit']:
                            user_choice = False
                            break
                        else:
                            if attempt < max_attempts - 1:
                                #console.print("Please enter 'y' for yes or 'n' for no.")
                                print(Fore.YELLOW + Style.BRIGHT + "Please enter 'y' for yes or 'n' for no or 'q' for quit.")
                            
                    except (EOFError, KeyboardInterrupt):
                        user_choice = False
                        break
                    except Exception as input_error:
                        if logger:
                            logger.debug(f"Input error on attempt {attempt + 1}: {input_error}")
                        if attempt < max_attempts - 1:
                            #console.print("Input error, please try again.")
                            print(Fore.RED + Style.BRIGHT + "Input error, please try again.")
                
                # Default to continue if no valid choice
                if user_choice is None:
                    user_choice = True
                    #console.print("Using default choice: continue")
                    print(Fore.CYAN + Style.BRIGHT + "Using default choice: continue")
                
                if not user_choice:
                    try:
                        cancel_message = (
                            "INITIALIZATION CANCELLED\n\n"
                            "You chose to exit due to initialization warnings.\n"
                            "Please review the issues and try again."
                        )
                        
                        if supports_color and banner_width > 60:
                            console.print(Panel.fit(
                                #f"[bold red]{cancel_message}[/bold red]",
                                f"{cancel_message}",
                                border_style="red",
                                title="CANCELLED",
                                style="bold red",
                                padding=(1, 3),
                                width=min(banner_width, console_width - 4)
                            ))
                        else:
                            #console.print(f"\nCANCELLED:\n{cancel_message}")
                            print(Fore.RED + Style.BRIGHT + f"\nCANCELLED:\n{cancel_message}")
                    except Exception:
                        #console.print(f"\nCANCELLED:\n{cancel_message}")
                        print(Fore.RED + Style.BRIGHT + f"\nCANCELLED:\n{cancel_message}")
                    
                    if logger:
                        logger.warning("User chose to exit after seeing initialization warnings")
                    
                    #console.print("Exiting system initialization...")
                    print(Fore.RED + Style.BRIGHT + "Exiting system initialization...")
                    
                    time.sleep(2)
                    sys.exit(0)
                
                # User chose to continue
                try:
                    continue_message = (
                        "CONTINUING WITH WARNINGS\n\n"
                        "You chose to continue despite the warnings.\n"
                        "Some functionality may be limited."
                    )
                    
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            #f"[bold green]{continue_message}[/bold green]",
                            f"{continue_message}",
                            border_style="green",
                            title="CONTINUING",
                            style="bold green",
                            padding=(1, 2),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        #console.print(f"\nCONTINUING:\n{continue_message}")
                        print(Fore.GREEN + Style.BRIGHT + f"\nCONTINUING:\n{continue_message}")
                except Exception:
                    #console.print(f"\nCONTINUING:\n{continue_message}")
                    print(Fore.GREEN + Style.BRIGHT + f"\nCONTINUING:\n{continue_message}")
                
                if logger:
                    logger.info(f"User chose to continue despite {overall_health:.1f}% health score")
                    logger.info(f"System has {len(system_state.get('errors_encountered', []))} errors and {len(system_state.get('warnings_issued', []))} warnings")
            
            else:
                # Critical issues - system health too low
                critical_message = (
                    f"CRITICAL SYSTEM INITIALIZATION ISSUES\n\n"
                    f"Overall Health Score: {overall_health:.1f}%\n"
                    f"The system has significant issues that may affect operation.\n"
                    f"Critical errors: {len([e for e in system_state.get('errors_encountered', []) if 'failed' in e.lower()])}\n"
                    f"Total errors: {len(system_state.get('errors_encountered', []))}\n"
                    f"Warnings: {len(system_state.get('warnings_issued', []))}\n\n"
                    f"It is strongly recommended to resolve these issues before continuing.\n\n"
                    #f"Continue anyway? (y/N)"
                )
                
                try:
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            #f"[bold red]{critical_message}[/bold red]",
                            f"{critical_message}",
                            border_style="red",
                            title="WARNING",
                            subtitle="Critical Issues Detected",
                            style="bold red",
                            padding=(1, 3),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        #console.print(f"\nCRITICAL ISSUES:\n{critical_message}")
                        print(Fore.RED + Style.BRIGHT + f"\nCRITICAL ISSUES:\n{critical_message}")
                except Exception:
                    #console.print(f"\nCRITICAL ISSUES:\n{critical_message}")
                    print(Fore.RED + Style.BRIGHT + f"\nCRITICAL ISSUES:\n{critical_message}")
                
                # User choice with default to No
                user_choice = None
                max_attempts = 3
                
                for attempt in range(max_attempts):
                    try:
                        #response = input("\nYour choice: ").strip().lower()
                        #response = input(Fore.YELLOW + Style.BRIGHT + "\nYour choice: ").strip().lower()
                        response = input(Fore.YELLOW + Style.BRIGHT + "\nContinue anyway? (y/N/q): ").strip().lower()
                        
                        if response in ['y', 'yes']:
                            user_choice = True
                            break
                        elif response in ['n', 'no', 'q', 'quit', '']:
                            user_choice = False
                            break
                        else:
                            if attempt < max_attempts - 1:
                                #console.print("Please enter 'y' for yes or 'n' for no. Default is 'n'.")
                                print(Fore.YELLOW + Style.BRIGHT + "Please enter 'y' for yes or 'n' for no or 'q' for quit. Default is 'n'.")
                            
                    except (EOFError, KeyboardInterrupt):
                        user_choice = False
                        break
                    except Exception as input_error:
                        if logger:
                            logger.debug(f"Input error on attempt {attempt + 1}: {input_error}")
                        if attempt < max_attempts - 1:
                            #console.print("Input error, please try again.")
                            print(Fore.RED + Style.BRIGHT + "Input error, please try again.")
                
                # Default to quit if no valid choice
                if user_choice is None:
                    user_choice = False
                    #console.print("Using default choice: quit")
                    print(Fore.RED + Style.BRIGHT + "Using default choice: quit")
                
                if not user_choice:
                    try:
                        quit_message = (
                            "INITIALIZATION TERMINATED\n\n"
                            "You chose to exit due to critical initialization issues.\n"
                            "Please review the error messages above and resolve the issues.\n\n"
                            "Check the logs and initialization report for detailed information."
                        )
                        
                        if supports_color and banner_width > 60:
                            console.print(Panel.fit(
                                #f"[bold red]{quit_message}[/bold red]",
                                f"{quit_message}",
                                border_style="red",
                                title="TERMINATED",
                                style="bold red",
                                padding=(1, 3),
                                width=min(banner_width, console_width - 4)
                            ))
                        else:
                            #console.print(f"\nTERMINATED:\n{quit_message}")
                            print(Fore.RED + Style.BRIGHT + f"\nTERMINATED:\n{quit_message}")
                    except Exception:
                        #console.print(f"\nTERMINATED:\n{quit_message}")
                        print(Fore.RED + Style.BRIGHT + f"\nTERMINATED:\n{quit_message}")
                    
                    if logger:
                        logger.critical("User chose to exit due to critical initialization issues")
                        logger.critical(f"System health score: {overall_health:.1f}%")
                        for error in system_state.get('errors_encountered', []):
                            logger.error(f"  - {error}")
                    
                    #console.print("Exiting due to critical system issues...")
                    print(Fore.RED + Style.BRIGHT + "Exiting due to critical system issues...")
                    
                    time.sleep(3)
                    sys.exit(1)
                
                # User chose to continue despite critical issues
                try:
                    risky_continue_message = (
                        "CONTINUING WITH CRITICAL ISSUES\n\n"
                        "You chose to continue despite critical system issues.\n"
                        "The system may not function correctly.\n\n"
                        "Proceed with caution and monitor for errors."
                    )
                    
                    if supports_color and banner_width > 60:
                        console.print(Panel.fit(
                            #f"[bold yellow]{risky_continue_message}[/bold yellow]",
                            f"{risky_continue_message}",
                            border_style="red",
                            title="PROCEEDING",
                            subtitle="With Caution",
                            style="bold yellow",
                            padding=(1, 2),
                            width=min(banner_width, console_width - 4)
                        ))
                    else:
                        #console.print(f"\nPROCEEDING:\n{risky_continue_message}")
                        print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{risky_continue_message}")
                except Exception:
                    #console.print(f"\nPROCEEDING:\n{risky_continue_message}")
                    print(Fore.GREEN + Style.BRIGHT + f"\nPROCEEDING:\n{risky_continue_message}")
                
                if logger:
                    logger.warning(f"User chose to continue despite critical issues (health: {overall_health:.1f}%)")
                    logger.warning("System may not function correctly - monitoring recommended")
        
        except Exception as prompt_error:
            # If prompt handling fails, continue with warning
            #console.print(f"Error in user prompt handling: {prompt_error}")
            print(Fore.RED + Style.BRIGHT + f"Error in user prompt handling: {prompt_error}")
            
            if logger:
                logger.error(f"Error in initialization prompt: {prompt_error}")
            
            #console.print("Continuing with system initialization...")
            print(Fore.GREEN + Style.BRIGHT + "Continuing with system initialization...")
        
        # Clean up input buffer
        try:
            time.sleep(0.05)
            sys.stdout.flush()
            sys.stderr.flush()
            
            if hasattr(select, 'select') and sys.stdin.isatty():
                while select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], []):
                    line = sys.stdin.readline()
                    if not line:
                        break
            
            try:
                import msvcrt
                while msvcrt.kbhit():
                    msvcrt.getch()
            except ImportError:
                pass
            
            sys.stdin.flush()
            
        except Exception as cleanup_error:
            if logger:
                logger.debug(f"Input buffer cleanup failed: {cleanup_error}")
        
        # Clear console before continuing (if user chose to continue)
        try:
            if is_tty:
                console.clear()
        except Exception:
            pass
        
        # Create comprehensive system status report (same as before)
        system_status = {
            'initialization': {
                'start_time': system_state['initialization_start'],
                'end_time': datetime.now(),
                'duration_seconds': initialization_time,
                'status': 'success',
                'method': 'optimized_system_check_framework',
                'overall_health_score': overall_health,
                'health_metrics': system_state['health_metrics'],
                'steps_completed': len(initialization_steps),
                'errors_count': len(system_state.get('errors_encountered', [])),
                'warnings_count': len(system_state.get('warnings_issued', []))
            },
            'system': {
                'platform': platform.platform(),
                'python_version': sys.version.split()[0],
                'pytorch_version': torch.__version__,
                'cuda_available': cuda_available,
                'cuda_device_count': torch.cuda.device_count() if cuda_available else 0,
                'cpu_cores': logical_cores,
                'total_ram_gb': total_ram_gb,
                'working_directory': str(Path.cwd()),
                'log_directory': str(essential_dirs.get('logs', Path(__file__).resolve().parent / "logs")),
                'model_directory': str(essential_dirs.get('models', Path(__file__).resolve().parent / "models")),
                'config_directory': str(essential_dirs.get('config', Path(__file__).resolve().parent / "config")),
                'report_directory': str(essential_dirs.get('reports', Path(__file__).resolve().parent / "reports")),
                'cache_directory': str(essential_dirs.get('cache', Path(__file__).resolve().parent / "cache")),
                'export_directory': str(essential_dirs.get('exports', Path(__file__).resolve().parent / "exports")),
                'checkpoint_directory': str(essential_dirs.get('checkpoints', Path(__file__).resolve().parent / "checkpoints")),
                'tensorboard_directory': str(essential_dirs.get('tensorboard', Path(__file__).resolve().parent / "tensorboard")),
                'data_directory': str(essential_dirs.get('data', Path(__file__).resolve().parent / "data"))
            },
            'config': {
                'active_config': system_state.get('active_config', config),
                'config_file': str(CONFIG_FILE),
                'preset_name': config.get('_preset_name', 'custom'),
                'available_presets': list(PRESET_CONFIGS.keys()),
                'validation_status': 'passed' if system_state['health_metrics'].get('configuration', 0) >= 80 else 'issues',
                'changes_applied': len(system_state.get('config_changes', [])),
                'hardware_optimized': True
            },
            'hardware': {
                'detailed_info': system_state.get('hardware_detected', {}),
                'cpu_count': logical_cores,
                'memory_gb': total_ram_gb,
                'disk_space_gb': shutil.disk_usage('.').free / (1024**3),
                'cuda_available': cuda_available,
                'cuda_devices': [
                    {
                        'id': i,
                        'name': torch.cuda.get_device_name(i),
                        'memory_gb': torch.cuda.get_device_properties(i).total_memory / (1024**3),
                        'compute_capability': f"{torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}"
                    } for i in range(torch.cuda.device_count())
                ] if cuda_available else []
            },
            'models': {
                'variants_total': system_state.get('model_variants', {}).get('total', 0),
                'variants_available': system_state.get('model_variants', {}).get('available', 0),
                'variant_names': system_state.get('model_variants', {}).get('available_names', []),
                'variant_status': system_state.get('model_variants', {}).get('status', {}),
                'validation_passed': system_state['health_metrics'].get('models', 0) >= 80
            },
            'performance': system_state.get('performance_baseline', {}),
            'dependencies': {
                'torch_version': torch.__version__,
                'python_version': sys.version_info[:3],
                'platform': platform.system(),
                'optional_available': {
                    name: available for name, available in OPTIONAL_DEPENDENCIES.items()
                },
                'check_results': system_state.get('system_check_results', {})
            },
            'diagnostics': {
                'errors_encountered': system_state.get('errors_encountered', []),
                'warnings_issued': system_state.get('warnings_issued', []),
                'system_checks_passed': sum(1 for result in system_state.get('system_check_results', {}).values() if result and result.passed),
                'system_checks_total': len(system_state.get('system_check_results', {})),
                'integration_status': 'optimal' if system_state['health_metrics'].get('integration', 0) >= 90 else 'good' if system_state['health_metrics'].get('integration', 0) >= 70 else 'suboptimal'
            }
        }
        
        # Save comprehensive initialization report
        try:
            save_initialization_report(system_status, essential_dirs['reports'])
            logger.debug(f"Comprehensive initialization report saved to {essential_dirs['reports']}")
        except Exception as e:
            logger.warning(f"Failed to save initialization report: {e}")
        
        # Final logging summary with health metrics
        logger.debug("=" * 50)
        logger.debug("DEEP LEARNING SYSTEM INITIALIZATION COMPLETED")
        logger.debug("=" * 50)
        logger.debug(f"Overall Health Score: {overall_health:.1f}%")
        logger.debug(f"Initialization Time: {initialization_time:.2f} seconds")
        logger.debug(f"Configuration: {config.get('_preset_name', 'custom')} preset")
        logger.debug(f"Model Variants: {system_state.get('model_variants', {}).get('available', 0)}/{system_state.get('model_variants', {}).get('total', 0)} available")
        logger.debug(f"Hardware: {'GPU-accelerated' if cuda_available else 'CPU-only'} ({logical_cores} cores, {total_ram_gb:.1f}GB RAM)")
        logger.debug(f"System Status: {'OPTIMAL' if overall_health >= 90 else 'GOOD' if overall_health >= 70 else 'SUBOPTIMAL' if overall_health >= 50 else 'CRITICAL'}")
        
        if system_state.get('errors_encountered'):
            logger.warning(f"Errors encountered: {len(system_state['errors_encountered'])}")
            for error in system_state['errors_encountered']:
                logger.warning(f"  - {error}")
        
        if system_state.get('warnings_issued'):
            logger.debug(f"Warnings issued: {len(system_state['warnings_issued'])}")
            # Limit to first 5
            for warning in system_state['warnings_issued'][:5]:
                logger.debug(f"  - {warning}")
        
        logger.debug("=" * 50)
        
        return system_status, config, logger
        
    except KeyboardInterrupt:
        # Handle user interruption gracefully
        for step_name, status, duration, details, status_style, health_display in initialization_steps:
            init_table.add_row(
                Text(step_name, style="bold cyan"),
                Text(status, style=status_style),
                Text(f"{duration:.2f}s", style="dim"),
                Text(details, style="dim"),
                #Text(health_display, justify="center") if health_display else ""
                health_display
            )
        
        init_table.add_row(
            Text("SYSTEM INITIALIZATION", style="bold white on red"),
            Text("INTERRUPTED", style="bold white on red"),
            Text(f"{time.time() - initialization_start:.2f}s", style="bold white"),
            Text("System initialization was cancelled by user (Ctrl+C)", style="bright_white"),
            #Text("[bold red]0%[/]", justify="center")
            "[bold red]0%[/]"
        )
        
        console.print(init_table)
        logger.warning("System initialization interrupted by user (Ctrl+C)")
        sys.exit(0)
        
    except SystemExit:
        # User chose to quit during loading_screen or configuration
        logger.debug("System initialization cancelled by user choice")
        raise
        
    except Exception as e:
        # Handle critical initialization failures
        initialization_time = time.time() - initialization_start
        
        # Add completed steps to table
        for step_name, status, duration, details, status_style, health_display in initialization_steps:
            init_table.add_row(
                Text(step_name, style="bold cyan"),
                Text(status, style=status_style),
                Text(f"{duration:.2f}s", style="dim"),
                Text(details, style="dim"),
                #Text(health_display, justify="center") if health_display else ""
                health_display
            )
        
        init_table.add_row(
            Text("SYSTEM INITIALIZATION", style="bold white on red"),
            Text("CRITICAL FAILURE", style="bold white on red"),
            Text(f"{initialization_time:.2f}s", style="bold white"),
            Text(f"Critical error: {str(e)}\nType: {type(e).__name__}\nSystem cannot continue", style="bright_white"),
            #Text("[bold red]0%[/]", justify="center")
            "[bold red]0%[/]"
        )
        
        console.print(init_table)
        
        # Create comprehensive error status report
        error_status = {
            'initialization': {
                'start_time': system_state.get('initialization_start', datetime.now()),
                'end_time': datetime.now(),
                'duration_seconds': initialization_time,
                'status': 'critical_failure',
                'error': str(e),
                'error_type': type(e).__name__,
                'overall_health_score': 0,
                'steps_attempted': len(initialization_steps),
                'errors_encountered': system_state.get('errors_encountered', []) + [str(e)],
                'warnings_issued': system_state.get('warnings_issued', [])
            },
            'system': {
                'platform': platform.platform(),
                'python_version': sys.version.split()[0],
                'hardware_detected': system_state.get('hardware_detected'),
                'final_state': 'unusable'
            },
            'diagnostics': {
                'critical_failure_point': initialization_steps[-1][0] if initialization_steps else 'unknown',
                'recovery_possible': False,
                'suggested_actions': [
                    "Check system requirements",
                    "Verify Python environment", 
                    "Check available disk space and memory",
                    "Review error logs for specific issues"
                ]
            }
        }
        
        logger.critical("=" * 80)
        logger.critical("CRITICAL: SYSTEM INITIALIZATION FAILED")
        logger.critical("=" * 80)
        logger.critical(f"Error: {str(e)}")
        logger.critical(f"Error Type: {type(e).__name__}")
        logger.critical(f"Duration: {initialization_time:.2f} seconds")
        logger.critical(f"Steps completed: {len(initialization_steps)}")
        logger.critical(f"Total errors: {len(system_state.get('errors_encountered', [])) + 1}")
        logger.exception("Detailed error information:")
        logger.critical("=" * 80)
        
        # Save comprehensive error report
        try:
            error_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            reports_dir = essential_dirs.get('reports', Path(__file__).resolve().parent / "reports")
            reports_dir.mkdir(parents=True, exist_ok=True)
            error_report_path = reports_dir / f"deep_init_critical_failure_{error_timestamp}.json"
            
            with open(error_report_path, 'w', encoding='utf-8') as f:
                json.dump(error_status, f, indent=2, default=str, ensure_ascii=False)
            
            logger.error(f"Critical failure report saved to {error_report_path}")
        except Exception as save_error:
            logger.error(f"Failed to save critical failure report: {save_error}")
        
        raise RuntimeError(f"System initialization failed: {e}") from e

def display_configuration_changes(changes: List[Dict], console: Console = None, logger: logging.Logger = None):
    """
    Display configuration changes in a rich table format with enhanced styling and comprehensive information.
    
    This updated function provides improved visual presentation of configuration changes
    that occur during system initialization, including change categorization, impact assessment,
    and detailed formatting that matches the enhanced initialization system's visual style.
    
    Args:
        changes: List of configuration change dictionaries with enhanced metadata
        console: Rich console instance (creates new if None)
        logger: Logger for summary and detailed change logging
    """
    if not changes:
        return
    
    if console is None:
        console = Console()
    
    # Enhanced validation of changes structure
    valid_changes = []
    invalid_changes = []
    
    for change in changes:
        if isinstance(change, dict) and all(key in change for key in ['section', 'parameter']):
            valid_changes.append(change)
        else:
            invalid_changes.append(change)
            if logger:
                logger.debug(f"Invalid change structure detected: {change}")
    
    if not valid_changes:
        if logger:
            logger.warning(f"No valid configuration changes to display ({len(invalid_changes)} invalid entries)")
        return
    
    # Create enhanced configuration changes table with improved styling
    config_table = Table(
        title=f"\n[bold bright_green]CONFIGURATION CHANGES APPLIED[/bold bright_green]",
        box=box.DOUBLE_EDGE,
        header_style="bold bright_white on blue",
        border_style="bright_cyan",
        title_style="bold bright_green",
        title_justify="center",
        show_lines=True,
        expand=True,
        width=min(120, console.width - 4),
        caption="[dim]Changes applied during system initialization[/dim]",
        caption_justify="center"
    )
    
    # Enhanced column structure with better width management
    config_table.add_column("Section", style="bold cyan", width=14, no_wrap=True)
    config_table.add_column("Parameter", style="bold yellow", width=22, no_wrap=False)
    config_table.add_column("Previous", style="dim red", width=18, justify="center", no_wrap=False)
    config_table.add_column("Current", style="bold green", width=18, justify="center", no_wrap=False)
    config_table.add_column("Source", style="dim blue", width=12, justify="center", no_wrap=True)
    config_table.add_column("Impact", style="bold", width=10, justify="center", no_wrap=True)
    
    # Enhanced change categorization and analysis
    change_categories = {
        'CRITICAL': [],      # Changes that significantly affect system behavior
        'IMPORTANT': [],     # Changes that modify key functionality
        'MINOR': [],         # Changes that have limited impact
        'AUTOMATIC': [],     # Changes applied automatically by the system
        'USER_REQUESTED': [], # Changes explicitly requested by user
        'FALLBACK': []       # Changes applied due to errors or fallbacks
    }
    
    section_groups = {}
    total_changes = len(valid_changes)
    
    # Process each change and categorize/assess impact
    for change in valid_changes:
        section = change.get('section', 'UNKNOWN').upper()
        parameter = change.get('parameter', 'unknown')
        old_value = change.get('old_value', 'N/A')
        new_value = change.get('new_value', 'N/A')
        source = change.get('source', 'unknown')
        
        # Categorize change
        category = 'MINOR'  # Default
        # Critical changes that significantly affect system behavior
        critical_params = {
            'SYSTEM': ['device', 'cuda_optimizations', 'random_seed', 'reproducible'],
            'TRAINING': ['batch_size', 'learning_rate', 'optimizer', 'mixed_precision'],
            'MODEL': ['model_type', 'encoding_dim', 'hidden_dims', 'num_models'],
            'SECURITY': ['percentile', 'attack_threshold', 'enable_security_metrics'],
            'DATA': ['normal_samples', 'attack_samples', 'features', 'use_real_data']
        }
        
        # Check if this is a critical parameter
        if parameter in critical_params.get(section, []):
            category = 'CRITICAL'
        # Check source-based categorization
        elif source in ['validation_fallback', 'error_recovery', 'emergency_default']:
            category = 'FALLBACK'
        elif source in ['user_input', 'interactive', 'manual']:
            category = 'USER_REQUESTED'
        elif source in ['auto_optimization', 'hardware_detection', 'preset_application']:
            category = 'AUTOMATIC'
        # Important changes that modify key functionality
        elif section in ['TRAINING', 'MODEL', 'SECURITY']:
            category = 'IMPORTANT'
        
        change_categories[category].append(change)
        
        # Assess change impact
        impact_level = 'LOW'  # Default
        
        # High impact parameters
        high_impact_params = {
            'model_type', 'device', 'cuda_optimizations', 'batch_size', 
            'learning_rate', 'num_models', 'encoding_dim', 'use_real_data'
        }
        
        if parameter in high_impact_params:
            impact_level = 'HIGH'
        # Medium impact parameters
        elif parameter in {
            'hidden_dims', 'dropout_rates', 'optimizer', 'mixed_precision',
            'percentile', 'attack_threshold', 'normalization'
        }:
            impact_level = 'MEDIUM'
        # Check for significant value changes
        elif isinstance(old_value, (int, float)) and isinstance(new_value, (int, float)):
            if abs(old_value - new_value) / max(abs(old_value), abs(new_value), 1) > 0.5:
                impact_level = 'MEDIUM'
        # Low impact for specific parameters
        elif parameter in ['verbose', 'debug', 'log_frequency', 'metrics_frequency']:
            impact_level = 'LOW'
        
        # Format values for display
        def format_value(value, max_length=15):
            if value is None:
                return "None"
            elif isinstance(value, bool):
                return "True" if value else "False"
            elif isinstance(value, (list, tuple)):
                if len(value) <= 3:
                    formatted = str(value)
                else:
                    formatted = f"[{len(value)} items]"
            elif isinstance(value, dict):
                formatted = f"{{{len(value)} keys}}"
            else:
                formatted = str(value)
            
            # Truncate if too long
            if len(formatted) > max_length:
                return formatted[:max_length-3] + "..."
            
            return formatted
        
        formatted_old = format_value(old_value)
        formatted_new = format_value(new_value)
        
        # Format source display
        source_mapping = {
            'validation_fallback': 'Fallback',
            'error_recovery': 'Recovery',
            'emergency_default': 'Emergency',
            'user_input': 'User',
            'interactive': 'User',
            'manual': 'Manual',
            'auto_optimization': 'Auto',
            'hardware_detection': 'Hardware',
            'preset_application': 'Preset',
            'system_default': 'Default',
            'config_file': 'Config',
            'unknown': 'Unknown'
        }
        source_display = source_mapping.get(source, source.title())
        
        # Group by section for organized display
        if section not in section_groups:
            section_groups[section] = []
        section_groups[section].append({
            **change,
            'category': category,
            'formatted_old': formatted_old,
            'formatted_new': formatted_new,
            'impact_level': impact_level,
            'source_display': source_display
        })
    
    # Sort sections by importance and alphabetically
    section_priority = {
        'SYSTEM': 1, 'TRAINING': 2, 'MODEL': 3, 'SECURITY': 4, 
        'DATA': 5, 'MONITORING': 6, 'HARDWARE': 7, 'PRESETS': 8
    }
    
    sorted_sections = sorted(section_groups.keys(), 
                           key=lambda x: (section_priority.get(x, 99), x))
    
    # Add table rows with enhanced formatting and organization
    for section_name in sorted_sections:
        section_changes = section_groups[section_name]
        
        # Add prominent section header with change count
        config_table.add_row(
            Text(f"{section_name}", style="bold white on blue"),
            Text(f"({len(section_changes)} changes)", style="bold white on blue", justify="center"),
            "",
            "",
            "",
            "",
            style="bold white on blue"
        )
        
        # Sort changes within section by impact level and parameter name
        impact_priority = {'HIGH': 1, 'MEDIUM': 2, 'LOW': 3, 'NONE': 4}
        sorted_changes = sorted(section_changes, 
                              key=lambda x: (impact_priority.get(x['impact_level'], 99), 
                                           x.get('parameter', '')))
        
        # Add changes for this section with enhanced styling
        for change_info in sorted_changes:
            parameter = change_info.get('parameter', 'unknown')
            formatted_old = change_info['formatted_old']
            formatted_new = change_info['formatted_new']
            source_display = change_info['source_display']
            impact_level = change_info['impact_level']
            category = change_info['category']
            
            # Format impact display
            impact_styles = {
                'HIGH': ("HIGH", "bold red"),
                'MEDIUM': ("MED", "bold yellow"),
                'LOW': ("LOW", "bold green"),
                'NONE': ("NONE", "dim")
            }
            impact_text, impact_style = impact_styles.get(impact_level, (impact_level, "dim"))
            impact_display = Text(impact_text, style=impact_style)
            
            # Get parameter style
            if category == 'CRITICAL':
                param_style = "bold red"
            elif category == 'IMPORTANT':
                param_style = "bold yellow"
            elif category == 'FALLBACK':
                param_style = "bold magenta"
            elif impact_level == 'HIGH':
                param_style = "bold bright_yellow"
            else:
                param_style = "bold white"
            
            # Get new value style
            new_value_styles = {
                'HIGH': "bold bright_green",
                'MEDIUM': "bold green",
                'LOW': "green",
                'NONE': "dim green"
            }
            new_value_style = new_value_styles.get(impact_level, "green")
            
            # Add parameter row with appropriate styling
            config_table.add_row(
                "",  # Empty section column for parameter rows
                Text(parameter, style=param_style),
                Text(formatted_old, style="dim red"),
                Text(formatted_new, style=new_value_style),
                Text(source_display, style="dim blue"),
                impact_display
            )
    
    # Add comprehensive summary row with statistics
    critical_count = len(change_categories['CRITICAL'])
    important_count = len(change_categories['IMPORTANT'])
    minor_count = len(change_categories['MINOR'])
    
    config_table.add_row(
        Text("SUMMARY", style="bold bright_white on black"),
        Text(f"{total_changes} total changes", style="bold white"),
        Text(f"Critical: {critical_count}", style="bold red" if critical_count > 0 else "dim"),
        Text(f"Important: {important_count}", style="bold yellow" if important_count > 0 else "dim"),
        Text(f"Minor: {minor_count}", style="bold green" if minor_count > 0 else "dim"),
        Text("APPLIED", style="bold bright_green"),
        style="bold bright_white on black"
    )
    
    # Display the enhanced table
    console.print(config_table)
    
    # Enhanced post-table information display
    critical_changes = change_categories['CRITICAL']
    important_changes = change_categories['IMPORTANT']
    fallback_changes = change_categories['FALLBACK']
    
    if critical_changes or important_changes or fallback_changes:
        console.print()  # Add spacing
        
        # Create impact summary panel
        summary_content = []
        
        if critical_changes:
            summary_content.append(f"[bold red]Critical Changes ({len(critical_changes)}):[/bold red]")
            for change in critical_changes[:3]:  # Show first 3
                param = change.get('parameter', 'unknown')
                section = change.get('section', 'unknown')
                summary_content.append(f"  - {section}.{param}")
            if len(critical_changes) > 3:
                summary_content.append(f"  - ... and {len(critical_changes) - 3} more")
            summary_content.append("")
        
        if important_changes:
            summary_content.append(f"[bold yellow]Important Changes ({len(important_changes)}):[/bold yellow]")
            for change in important_changes[:3]:  # Show first 3
                param = change.get('parameter', 'unknown')
                section = change.get('section', 'unknown')
                summary_content.append(f"  - {section}.{param}")
            if len(important_changes) > 3:
                summary_content.append(f"  - ... and {len(important_changes) - 3} more")
            summary_content.append("")
        
        if fallback_changes:
            summary_content.append(f"[bold magenta]Fallback Changes ({len(fallback_changes)}):[/bold magenta]")
            summary_content.append("These changes were applied due to errors or validation failures:")
            for change in fallback_changes[:2]:  # Show first 2
                param = change.get('parameter', 'unknown')
                section = change.get('section', 'unknown')
                source = change.get('source', 'unknown')
                summary_content.append(f"  - {section}.{param} ({source})")
            if len(fallback_changes) > 2:
                summary_content.append(f"  - ... and {len(fallback_changes) - 2} more")
        
        # Display impact summary panel
        console.print(Panel.fit(
            "\n".join(summary_content),
            title="[bold]Change Impact Summary[/bold]",
            border_style="yellow",
            padding=(1, 2)
        ))
    
    # Enhanced logging with comprehensive details
    if logger:
        # Log summary statistics
        total_changes = len(valid_changes)
        critical_count = len(change_categories['CRITICAL'])
        important_count = len(change_categories['IMPORTANT'])
        fallback_count = len(change_categories['FALLBACK'])
        
        logger.info(f"Configuration changes summary:")
        logger.info(f"  - Total valid changes: {total_changes}")
        logger.info(f"  - Critical changes: {critical_count}")
        logger.info(f"  - Important changes: {important_count}")
        logger.info(f"  - Fallback changes: {fallback_count}")
        
        if invalid_changes:
            logger.warning(f"  - Invalid change entries: {len(invalid_changes)}")
        
        # Log section breakdown
        section_counts = {}
        for change in valid_changes:
            section = change.get('section', 'UNKNOWN')
            section_counts[section] = section_counts.get(section, 0) + 1
        
        if section_counts:
            section_summary = ", ".join([f"{section}: {count}" for section, count in section_counts.items()])
            logger.info(f"  - Changes by section: {section_summary}")
        
        # Log critical changes in detail
        if critical_count > 0:
            logger.warning("Critical configuration changes detected:")
            for change in change_categories['CRITICAL']:
                section = change.get('section', 'unknown')
                parameter = change.get('parameter', 'unknown')
                old_val = change.get('old_value', 'N/A')
                new_val = change.get('new_value', 'N/A')
                source = change.get('source', 'unknown')
                logger.warning(f"  - {section}.{parameter}: {old_val} -> {new_val} (source: {source})")
        
        # Log fallback changes with warnings
        if fallback_count > 0:
            logger.warning("Fallback configuration changes applied due to errors:")
            for change in change_categories['FALLBACK']:
                section = change.get('section', 'unknown')
                parameter = change.get('parameter', 'unknown')
                source = change.get('source', 'unknown')
                logger.warning(f"  - {section}.{parameter} (fallback source: {source})")
        
        # Log detailed change information at debug level
        logger.debug("Detailed configuration changes:")
        for change in valid_changes:
            change_str = (
                f"  - {change.get('section', 'unknown')}.{change.get('parameter', 'unknown')}: "
                f"{change.get('old_value', 'N/A')} -> {change.get('new_value', 'N/A')} "
                f"(source: {change.get('source', 'unknown')})"
            )
            logger.debug(change_str)

def save_initialization_report(system_status: Dict[str, Any], report_dir: Path) -> None:
    """
    Save a comprehensive initialization report to disk with multiple formats using template-based architecture.
    
    This function creates both machine-readable (JSON) and human-readable (TXT) reports that match 
    the comprehensive system_status structure from initialize_system, plus generates an interactive 
    HTML dashboard using the modular template system.
    
    Args:
        system_status: Complete system status dictionary from initialize_system (contains all check results)
        report_dir: Directory to save the report (should be a Path object)
        
    Raises:
        Exception: If report saving fails (logged but not re-raised to avoid interrupting initialization)
    """
    from datetime import datetime
    
    # Define file variables at the top to ensure they're in scope for error handling
    consolidated_files = {}
    report_data, status_data, diagnostics_data, dashboard_json_file_data = None, None, None, None
    template_renderer = None
    
    try:
        # Determine report_dir (default: script's directory / reports)
        if report_dir is None:
            report_dir = Path(__file__).resolve().parent / "reports"
        report_dir.mkdir(parents=True, exist_ok=True)
        
        # Ensure report_dir is a Path object
        if not isinstance(report_dir, Path):
            report_dir = Path(__file__).resolve().parent / "reports"
        
        # Create timestamp for report files
        timestamp_obj = datetime.now()
        date_str = timestamp_obj.strftime('%Y%m%d')
        time_str = timestamp_obj.strftime('%H%M%S')
        timestamp = f"{date_str}_{time_str}"
        
        # Initialize Template Renderer
        try:
            # Try to import and initialize the template renderer
            template_dir = Path(__file__).resolve().parent / "templates"
            
            template_renderer_path = template_dir / "template_renderer.py"
            if template_renderer_path.exists():
                spec = importlib.util.spec_from_file_location("template_renderer", template_renderer_path)
                template_renderer_module = importlib.util.module_from_spec(spec)
                sys.modules["template_renderer"] = template_renderer_module
                spec.loader.exec_module(template_renderer_module)
                
                # Initialize the renderer
                template_renderer = template_renderer_module.DashboardTemplateRenderer(template_dir)
                logger.debug("Template-based dashboard renderer initialized successfully")
            else:
                logger.warning("Template renderer not found, will use fallback HTML generation")
                template_renderer = None
                
        except Exception as template_error:
            logger.warning(f"Failed to initialize template renderer: {template_error}")
            logger.debug("Template renderer initialization error:", exc_info=True)
            template_renderer = None
        
        # OPTIMIZED: Extract existing check results instead of re-running checks
        enhanced_status = system_status.copy()
        
        # Extract existing check results from system_status
        existing_check_results = system_status.get('dependencies', {}).get('check_results', {})
        
        # HARDWARE INFORMATION - Extract from existing data
        if 'hardware' in system_status:
            enhanced_status['detailed_hardware'] = system_status['hardware']['detailed_info']
        elif 'hardware' in existing_check_results:
            # Extract from system check results if available
            hardware_check = existing_check_results['hardware']
            if hasattr(hardware_check, 'details') and hardware_check.details:
                enhanced_status['detailed_hardware'] = hardware_check.details
            elif hasattr(hardware_check, 'metadata') and hardware_check.metadata:
                enhanced_status['detailed_hardware'] = hardware_check.metadata
            else:
                # Fallback to system_status hardware info
                enhanced_status['detailed_hardware'] = system_status.get('hardware', {})
        else:
            # Only run check if absolutely no data is available
            try:
                hardware_data = check_hardware(include_memory_usage=True)
                enhanced_status['detailed_hardware'] = hardware_data
                logger.debug("Hardware check run as fallback - no existing data found")
            except Exception as e:
                enhanced_status['detailed_hardware_error'] = str(e)
                logger.warning(f"Fallback hardware check failed: {e}")
        
        # VERSION INFORMATION - Extract from existing data
        if 'package_versions' in existing_check_results:
            version_check = existing_check_results['package_versions']
            if hasattr(version_check, 'metadata') and version_check.metadata.get('version_info'):
                enhanced_status['detailed_versions'] = version_check.metadata['version_info']
            elif hasattr(version_check, 'details') and isinstance(version_check.details, str):
                # Try to extract from details if it's structured
                try:
                    # Look for version info in dependencies section
                    deps_info = system_status.get('dependencies', {})
                    if 'check_results' in deps_info:
                        enhanced_status['detailed_versions'] = deps_info.get('version_summary', {})
                except Exception:
                    pass
        
        # If still no version info, extract from system dependencies
        if 'detailed_versions' not in enhanced_status:
            deps_info = system_status.get('dependencies', {})
            if 'torch_version' in deps_info:
                # Build version info from available dependency data
                enhanced_status['detailed_versions'] = {
                    'PyTorch': {
                        'version': deps_info.get('torch_version', 'unknown'),
                        'compatible': True,
                        'required': True,
                        'status': 'OK'
                    },
                    'Python': {
                        'version': str(deps_info.get('python_version', 'unknown')),
                        'compatible': True,
                        'required': True,
                        'status': 'OK'
                    },
                    'Platform': {
                        'version': deps_info.get('platform', 'unknown'),
                        'compatible': True,
                        'required': False,
                        'status': 'OK'
                    }
                }
                # Add optional dependencies
                optional_available = deps_info.get('optional_available', {})
                for name, available in optional_available.items():
                    enhanced_status['detailed_versions'][name] = {
                        'version': 'Available' if available else 'N/A',
                        'compatible': available,
                        'required': False,
                        'status': 'OK' if available else 'MISSING'
                    }
            else:
                # Only run if absolutely no data available
                try:
                    version_info = check_versions(include_optional=True)
                    enhanced_status['detailed_versions'] = version_info
                    logger.debug("Version check run as fallback - no existing data found")
                except Exception as e:
                    enhanced_status['detailed_versions_error'] = str(e)
                    logger.warning(f"Fallback version check failed: {e}")
        
        # SEED CONFIGURATION - Extract from existing data
        if 'seed_config' in existing_check_results:
            seed_check = existing_check_results['seed_config']
            enhanced_status['reproducibility'] = {
                'seed_config_passed': seed_check.passed,
                'compliance_score': seed_check.metadata.get('compliance_score', 0) if seed_check.metadata else 0,
                'recommendations': seed_check.metadata.get('recommendations', []) if seed_check.metadata else [],
                'details': seed_check.details if hasattr(seed_check, 'details') else {}
            }
        else:
            # Try to infer from system configuration
            if 'config' in system_status and system_status['config'].get('reproducible', False):
                enhanced_status['reproducibility'] = {
                    'seed_config_passed': True,
                    'compliance_score': 85,  # Reasonable assumption if reproducible=True
                    'recommendations': [],
                    'details': {'inferred_from_config': True, 'seed': system_status['config'].get('random_seed', 42)}
                }
            else:
                # Only run if no existing data and can't infer
                try:
                    seed_result = check_seed_config(enhanced_status.get('detailed_hardware'))
                    enhanced_status['reproducibility'] = {
                        'seed_config_passed': seed_result.passed,
                        'compliance_score': seed_result.metadata.get('compliance_score', 0) if seed_result.metadata else 0,
                        'recommendations': seed_result.metadata.get('recommendations', []) if seed_result.metadata else [],
                        'details': seed_result.details
                    }
                    logger.debug("Seed config check run as fallback - no existing data found")
                except Exception as e:
                    enhanced_status['reproducibility_error'] = str(e)
                    logger.warning(f"Fallback seed config check failed: {e}")
        
        # LOGGING CONFIGURATION - Extract from existing data
        if 'logging_setup' in existing_check_results:
            logging_check = existing_check_results['logging_setup']
            enhanced_status['logging_config'] = {
                'passed': logging_check.passed,
                'compliance_score': logging_check.details.get('compliance_score', 0) if isinstance(logging_check.details, dict) else 0,
                'handlers_count': len(logging_check.details.get('handlers', [])) if isinstance(logging_check.details, dict) else 0,
                'feedback': logging_check.details.get('feedback', []) if isinstance(logging_check.details, dict) else []
            }
        else:
            # Try to infer from system state
            if logger and logger.handlers:
                enhanced_status['logging_config'] = {
                    'passed': True,
                    'compliance_score': 90,  # Reasonable assumption if logger is working
                    'handlers_count': len(logger.handlers),
                    'feedback': []
                }
            else:
                # Only run if no existing data and can't infer
                try:
                    logging_result = check_logging_setup()
                    enhanced_status['logging_config'] = {
                        'passed': logging_result.passed,
                        'compliance_score': logging_result.details.get('compliance_score', 0) if isinstance(logging_result.details, dict) else 0,
                        'handlers_count': len(logging_result.details.get('handlers', [])) if isinstance(logging_result.details, dict) else 0,
                        'feedback': logging_result.details.get('feedback', []) if isinstance(logging_result.details, dict) else []
                    }
                    logger.debug("Logging setup check run as fallback - no existing data found")
                except Exception as e:
                    enhanced_status['logging_config_error'] = str(e)
                    logger.warning(f"Fallback logging setup check failed: {e}")
        
        # PERFORMANCE MONITORING - Extract from existing data
        if 'performance_monitoring' in existing_check_results:
            perf_check = existing_check_results['performance_monitoring']
            enhanced_status['performance_monitoring'] = {
                'available': perf_check.passed,
                'capabilities': perf_check.details.get('capabilities', {}) if isinstance(perf_check.details, dict) else {},
                'hardware_integration': perf_check.details.get('hardware_integration', False) if isinstance(perf_check.details, dict) else False
            }
        else:
            # Try to infer from available functions
            has_perf_functions = (
                'enhanced_monitor_performance' in globals() and
                'performance_monitor_wrapper' in globals()
            )
            if has_perf_functions:
                enhanced_status['performance_monitoring'] = {
                    'available': True,
                    'capabilities': {'basic_monitoring': True, 'decorator_available': True},
                    'hardware_integration': enhanced_status.get('detailed_hardware', {}).get('cuda', {}).get('available', False)
                }
            else:
                # Only run if no existing data and can't infer
                try:
                    perf_monitoring_result = check_performance_monitoring()
                    enhanced_status['performance_monitoring'] = {
                        'available': perf_monitoring_result.passed,
                        'capabilities': perf_monitoring_result.details.get('capabilities', {}) if isinstance(perf_monitoring_result.details, dict) else {},
                        'hardware_integration': perf_monitoring_result.details.get('hardware_integration', False) if isinstance(perf_monitoring_result.details, dict) else False
                    }
                    logger.debug("Performance monitoring check run as fallback - no existing data found")
                except Exception as e:
                    enhanced_status['performance_monitoring_error'] = str(e)
                    logger.warning(f"Fallback performance monitoring check failed: {e}")
        
        # MEMORY MANAGEMENT - Extract from existing data
        if 'memory_management' in existing_check_results:
            memory_check = existing_check_results['memory_management']
            enhanced_status['memory_management'] = {
                'comprehensive': memory_check.passed,
                'capabilities': memory_check.details.get('capabilities', {}) if isinstance(memory_check.details, dict) else {},
                'test_results': memory_check.details.get('test_results', {}) if isinstance(memory_check.details, dict) else {}
            }
        else:
            # Try to infer from available functions
            has_memory_functions = 'enhanced_clear_memory' in globals()
            if has_memory_functions:
                enhanced_status['memory_management'] = {
                    'comprehensive': True,
                    'capabilities': {'clear_memory': True, 'hardware_aware': True},
                    'test_results': {'basic_test_passed': True}
                }
            else:
                # Only run if no existing data and can't infer
                try:
                    memory_mgmt_result = check_memory_management()
                    enhanced_status['memory_management'] = {
                        'comprehensive': memory_mgmt_result.passed,
                        'capabilities': memory_mgmt_result.details.get('capabilities', {}) if isinstance(memory_mgmt_result.details, dict) else {},
                        'test_results': memory_mgmt_result.details.get('test_results', {}) if isinstance(memory_mgmt_result.details, dict) else {}
                    }
                    logger.debug("Memory management check run as fallback - no existing data found")
                except Exception as e:
                    enhanced_status['memory_management_error'] = str(e)
                    logger.warning(f"Fallback memory management check failed: {e}")
        
        # SYSTEM CONFIGURATION - Extract from existing data
        if 'system' in system_status:
            enhanced_status['comprehensive_system_info'] = {
                'platform': system_status['system'],
                'hardware': system_status.get('hardware', {}),
                'dependencies': system_status.get('dependencies', {}),
                'diagnostics': system_status.get('diagnostics', {})
            }
        else:
            # Only run if no existing data
            try:
                system_info = get_system_info(include_versions=True, include_hardware=True)
                enhanced_status['comprehensive_system_info'] = system_info
                logger.debug("System info check run as fallback - no existing data found")
            except Exception as e:
                enhanced_status['comprehensive_system_info_error'] = str(e)
                logger.warning(f"Fallback system info check failed: {e}")
        
        # Create a serializable version of the enhanced report
        serializable_status = {}
        for key, value in enhanced_status.items():
            try:
                # Test if the value is JSON serializable
                json.dumps(value, default=str)
                serializable_status[key] = value
            except (TypeError, ValueError):
                # Convert problematic values to strings
                serializable_status[key] = str(value)
        
        # Add metadata to the JSON report
        serializable_status['_metadata'] = {
            'report_version': '3.1',  # Updated version to indicate optimization
            'generated_at': timestamp_obj.isoformat(),
            'report_type': 'comprehensive_system_initialization',
            'format': 'json',
            'generator': 'initialize_system',
            'enhancement_level': 'optimized_extraction',
            'template_based': template_renderer is not None,
            'dashboard_architecture': 'template_based' if template_renderer else 'inline_fallback',
            'redundancy_eliminated': True,
            'data_source': 'existing_check_results',
            'optimization_level': 'high',
            'check_functions_used': [
                'extracted_from_initialize_system_checks',
                'minimal_fallback_checks_only_when_needed'
            ],
            'python_version': sys.version,
            'platform': platform.platform(),
            'working_directory': str(Path.cwd())
        }
        
        # Calculate system health score for summary (using existing data)
        checks_performed = []
        checks_passed = []
        
        detailed_hw = enhanced_status.get('detailed_hardware', {})
        detailed_versions = enhanced_status.get('detailed_versions', {})
        reproducibility = enhanced_status.get('reproducibility', {})
        logging_config = enhanced_status.get('logging_config', {})
        perf_monitoring = enhanced_status.get('performance_monitoring', {})
        memory_mgmt = enhanced_status.get('memory_management', {})
        
        if 'detailed_hardware' in enhanced_status and not enhanced_status.get('detailed_hardware_error'):
            checks_performed.append('Hardware Analysis')
            if detailed_hw.get('cuda', {}).get('status') in ['PASS', 'WARN'] or detailed_hw.get('available', False):
                checks_passed.append('Hardware Analysis')
        
        if 'detailed_versions' in enhanced_status and not enhanced_status.get('detailed_versions_error'):
            checks_performed.append('Version Validation')
            compatible_count = sum(1 for v in detailed_versions.values() if isinstance(v, dict) and v.get('compatible', False))
            if compatible_count > 0:
                checks_passed.append('Version Validation')
        
        if 'reproducibility' in enhanced_status and not enhanced_status.get('reproducibility_error'):
            checks_performed.append('Reproducibility Config')
            if reproducibility.get('seed_config_passed', False):
                checks_passed.append('Reproducibility Config')
        
        if 'logging_config' in enhanced_status and not enhanced_status.get('logging_config_error'):
            checks_performed.append('Logging Configuration')
            if logging_config.get('passed', False):
                checks_passed.append('Logging Configuration')
        
        if 'performance_monitoring' in enhanced_status and not enhanced_status.get('performance_monitoring_error'):
            checks_performed.append('Performance Monitoring')
            if perf_monitoring.get('available', False):
                checks_passed.append('Performance Monitoring')
        
        if 'memory_management' in enhanced_status and not enhanced_status.get('memory_management_error'):
            checks_performed.append('Memory Management')
            if memory_mgmt.get('comprehensive', False):
                checks_passed.append('Memory Management')
        
        system_health_score = (len(checks_passed) / max(len(checks_performed), 1) * 100) if checks_performed else 0
        
        # Create compact status data
        compact_status = {
            'status': enhanced_status.get('initialization', {}).get('status', 'unknown'),
            'timestamp': timestamp_obj.isoformat(),
            'duration_seconds': enhanced_status.get('initialization', {}).get('duration_seconds', 0),
            'cuda_available': enhanced_status.get('system', {}).get('cuda_available', False),
            'model_variants': enhanced_status.get('models', {}).get('variants_available', 0),
            'config_preset': enhanced_status.get('config', {}).get('preset_name', 'unknown'),
            'errors': enhanced_status.get('initialization', {}).get('error') is not None,
            'system_health_score': system_health_score,
            'hardware_status': enhanced_status.get('detailed_hardware', {}).get('cuda', {}).get('status', 'unknown'),
            'reproducibility_score': enhanced_status.get('reproducibility', {}).get('compliance_score', 0),
            'logging_compliance': enhanced_status.get('logging_config', {}).get('compliance_score', 0),
            'performance_monitoring': enhanced_status.get('performance_monitoring', {}).get('available', False),
            'memory_management': enhanced_status.get('memory_management', {}).get('comprehensive', False)
        }
        
        # Create diagnostic data
        diagnostic_data = {
            'metadata': {
                'timestamp': timestamp_obj.isoformat(),
                'report_type': 'diagnostic',
                'check_framework_version': '3.1',
                'template_based': template_renderer is not None
            },
            'hardware_analysis': enhanced_status.get('detailed_hardware', {}),
            'version_validation': enhanced_status.get('detailed_versions', {}),
            'reproducibility_config': enhanced_status.get('reproducibility', {}),
            'logging_config': enhanced_status.get('logging_config', {}),
            'performance_monitoring': enhanced_status.get('performance_monitoring', {}),
            'memory_management': enhanced_status.get('memory_management', {}),
            'system_info': enhanced_status.get('comprehensive_system_info', {}),
            'extraction_summary': {
                'checks_extracted': len(checks_performed),
                'checks_successful': len(checks_passed),
                'fallback_checks_used': sum(1 for key in enhanced_status.keys() if key.endswith('_error')),
                'data_source': 'existing_system_checks'
            }
        }
        
        # Create dashboard data for JSON storage
        dashboard_json_data = {
            'timestamp': timestamp_obj.isoformat(),
            'health_score': system_health_score,
            'status': compact_status['status'],
            'duration_seconds': compact_status['duration_seconds'],
            'cuda_available': compact_status['cuda_available'],
            'model_variants': compact_status['model_variants'],
            'reproducibility_score': compact_status.get('reproducibility_score', 0),
            'logging_compliance': compact_status.get('logging_compliance', 0),
            'performance_monitoring': compact_status.get('performance_monitoring', False),
            'memory_management': compact_status.get('memory_management', False)
        }
        
        # DAILY CONSOLIDATED FILES IMPLEMENTATION (unchanged from original)
        # Define all the consolidated file paths
        consolidated_files = {
            'report': report_dir / f"deep_init_report_{date_str}.json",
            'summary': report_dir / f"deep_init_summary_{date_str}.txt", 
            'status': report_dir / f"deep_init_status_{date_str}.json",
            'diagnostics': report_dir / f"deep_init_diagnostics_{date_str}.json",
            'dashboard_json': report_dir / f"deep_system_dashboard_data_{date_str}.json",
            'dashboard_html': report_dir / f"deep_system_dashboard_{date_str}.html"
        }
        
        # Create entry metadata
        entry_metadata = {
            'sequence_id': timestamp,
            'timestamp': timestamp_obj.isoformat(),
            'time_str': time_str
        }
        
        # 1. Save to consolidated JSON report (append as array element)
        report_entry = {
            **entry_metadata,
            'data': serializable_status
        }
        
        # Use proper file handling with context managers
        if consolidated_files['report'].exists():
            try:
                with open(consolidated_files['report'], 'r', encoding='utf-8') as f:
                    report_data = json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Failed to load existing consolidated file, creating new: {e}")
                report_data = {
                    'date': date_str,
                    'reports': [],
                    'metadata': {
                        'version': '3.1',
                        'created_at': timestamp_obj.isoformat(),
                        'total_reports': 0,
                        'last_updated': timestamp_obj.isoformat(),
                        'report_type': 'consolidated_daily',
                        'template_based': template_renderer is not None
                    }
                }
        else:
            report_data = {
                'date': date_str,
                'reports': [report_entry],
                'metadata': {
                    'version': '3.1',
                    'created_at': timestamp_obj.isoformat(),
                    'total_reports': 1,
                    'last_updated': timestamp_obj.isoformat(),
                    'report_type': 'consolidated_daily',
                    'template_based': template_renderer is not None
                }
            }
        
        # Only append if we loaded existing data
        if consolidated_files['report'].exists():
            report_data['reports'].append(report_entry)
            report_data['metadata']['total_reports'] = len(report_data['reports'])
            report_data['metadata']['last_updated'] = timestamp_obj.isoformat()
        
        # Save the report data
        with open(consolidated_files['report'], 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, default=str, ensure_ascii=False)
        
        # 2. Append to consolidated summary file
        summary_header = f"""
{'=' * 80}
INITIALIZATION REPORT - {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')}
Sequence: {time_str} (Total today: {report_data['metadata']['total_reports']})
Dashboard Architecture: {'Template-Based' if template_renderer else 'Inline Fallback'}
{'=' * 80}
"""
        
        # Create the complete summary content as a string first
        init_info = enhanced_status.get('initialization', {})
        sys_info = enhanced_status.get('system', {})
        detailed_hw = enhanced_status.get('detailed_hardware', {})
        config_info = enhanced_status.get('config', {})
        model_info = enhanced_status.get('models', {})
        performance = enhanced_status.get('performance', {})
        deps_info = enhanced_status.get('dependencies', {})
        
        # Build summary content as a single string to avoid file handle issues
        summary_content = summary_header
        
        # Template Architecture Status
        summary_content += "TEMPLATE ARCHITECTURE\n"
        summary_content += "-" * 22 + "\n"
        summary_content += f"Template Renderer: {'Available' if template_renderer else 'Not Available'}\n"
        summary_content += f"Dashboard Type: {'Modular Template-Based' if template_renderer else 'Inline HTML Fallback'}\n"
        summary_content += f"CSS: {'External Stylesheet' if template_renderer else 'Embedded Styles'}\n"
        summary_content += f"JavaScript: {'Template Variables' if template_renderer else 'Inline Generation'}\n"
        summary_content += f"Quick Actions: {'Partial Template' if template_renderer else 'Generated HTML'}\n"
        summary_content += "\n"
        
        # Initialization Status
        summary_content += "INITIALIZATION STATUS\n"
        summary_content += "-" * 30 + "\n"
        summary_content += f"Status: {init_info.get('status', 'unknown').upper()}\n"
        summary_content += f"Duration: {init_info.get('duration_seconds', 0):.3f} seconds\n"
        summary_content += f"Method: {init_info.get('method', 'unknown')}\n"
        summary_content += f"Start Time: {init_info.get('start_time', 'unknown')}\n"
        summary_content += f"End Time: {init_info.get('end_time', 'unknown')}\n"
        
        if 'error' in init_info:
            summary_content += f"Error: {init_info['error']}\n"
            summary_content += f"Error Type: {init_info.get('error_type', 'unknown')}\n"
        summary_content += "\n"
        
        # System Environment
        summary_content += "SYSTEM ENVIRONMENT\n"
        summary_content += "-" * 20 + "\n"
        summary_content += f"Platform: {sys_info.get('platform', 'unknown')}\n"
        summary_content += f"Python Version: {sys_info.get('python_version', 'unknown')}\n"
        summary_content += f"PyTorch Version: {sys_info.get('pytorch_version', 'unknown')}\n"
        
        # Enhanced hardware details
        cpu_info = detailed_hw.get('cpu_cores', {})
        if cpu_info.get('available'):
            summary_content += f"CPU: {cpu_info.get('logical_cores', '?')} logical cores, {cpu_info.get('physical_cores', '?')} physical\n"
            if 'capacity' in cpu_info:
                freq = cpu_info['capacity'].get('frequency_ghz')
                if freq:
                    summary_content += f"CPU Frequency: {freq} GHz\n"
        
        ram_info = detailed_hw.get('system_ram', {})
        if ram_info.get('available'):
            summary_content += f"System RAM: {ram_info.get('ram_total_gb', 0):.1f}GB total, {ram_info.get('ram_available_gb', 0):.1f}GB available\n"
            if ram_info.get('ram_percent'):
                summary_content += f"RAM Usage: {ram_info.get('ram_percent', 0):.1f}%\n"
        
        disk_info = detailed_hw.get('disk_space', {})
        if disk_info.get('available') is not None:
            summary_content += f"Disk Space: {disk_info.get('free_gb', 0):.1f}GB free of {disk_info.get('total_gb', 0):.1f}GB total\n"
        
        # Enhanced CUDA information
        cuda_info = detailed_hw.get('cuda', {})
        summary_content += f"CUDA Available: {cuda_info.get('available', False)}\n"
        if cuda_info.get('available'):
            summary_content += f"CUDA Version: {cuda_info.get('cuda_version', 'unknown')}\n"
            summary_content += f"cuDNN Version: {cuda_info.get('cudnn_version', 'unknown')}\n"
            summary_content += f"GPU Count: {cuda_info.get('gpu_count', 0)}\n"
            
            for i, gpu in enumerate(cuda_info.get('gpus', [])):
                summary_content += f"  GPU {i}: {gpu.get('name', 'Unknown')} ({gpu.get('memory_gb', 0):.1f}GB)\n"
                summary_content += f"    Compute Capability: {gpu.get('compute_capability', 'unknown')}\n"
                if 'current_usage' in gpu:
                    usage = gpu['current_usage']
                    summary_content += f"    Memory Usage: {usage.get('allocated_mb', 0):.0f}MB allocated ({usage.get('percent_allocated', 0):.1f}%)\n"
        
        summary_content += f"Working Directory: {sys_info.get('working_directory', 'unknown')}\n"
        summary_content += f"Log Directory: {sys_info.get('log_directory', 'unknown')}\n"
        summary_content += f"Model Directory: {sys_info.get('model_directory', 'unknown')}\n"
        summary_content += f"Config Directory: {sys_info.get('config_directory', 'unknown')}\n"
        summary_content += f"Report Directory: {sys_info.get('report_directory', 'unknown')}\n"
        summary_content += "\n"
        
        # Configuration Information
        summary_content += "CONFIGURATION\n"
        summary_content += "-" * 15 + "\n"
        summary_content += f"Preset Name: {config_info.get('preset_name', 'custom')}\n"
        summary_content += f"Validation Status: {config_info.get('validation_status', 'unknown')}\n"
        summary_content += f"Config File: {config_info.get('config_file', 'unknown')}\n"
        available_presets = config_info.get('available_presets', [])
        summary_content += f"Available Presets: {', '.join(available_presets) if available_presets else 'none'}\n"
        
        # Enhanced configuration parameters
        active_config = config_info.get('active_config', {})
        if isinstance(active_config, dict) and active_config:
            summary_content += "Key Configuration Parameters:\n"
            # Limit to first 20 for readability
            for key, value in list(active_config.items())[:20]:
                if not key.startswith('_'):
                    if isinstance(value, dict):
                        summary_content += f"  {key}: {len(value)} items\n"
                    elif isinstance(value, (list, tuple)):
                        summary_content += f"  {key}: [{len(value)} items]\n"
                    else:
                        str_value = str(value)
                        if len(str_value) > 60:
                            str_value = str_value[:57] + "..."
                        summary_content += f"  {key}: {str_value}\n"
            
            if len(active_config) > 20:
                summary_content += f"  ... and {len(active_config) - 20} more parameters\n"
        summary_content += "\n"
        
        # Model Variants Information
        summary_content += "MODEL VARIANTS\n"
        summary_content += "-" * 15 + "\n"
        summary_content += f"Available Model Variants: {model_info.get('variants_available', 0)}\n"
        
        variant_names = model_info.get('variant_names', [])
        if variant_names:
            summary_content += f"Model Variant Names: {', '.join(variant_names)}\n"
        else:
            summary_content += "Model Variant Names: None\n"
        
        # Enhanced variant status
        variant_status = model_info.get('variant_status', {})
        if variant_status:
            summary_content += "Model Variant Status Details:\n"
            for name, status in variant_status.items():
                status_indicator = "[OK]" if status == 'available' else "[MISSING]"
                summary_content += f"  {status_indicator} {name}: {status}\n"
        summary_content += "\n"
        
        # Enhanced Performance Metrics
        summary_content += "PERFORMANCE BASELINE\n"
        summary_content += "-" * 22 + "\n"
        
        if performance:
            if 'baseline_failed' in performance:
                summary_content += f"Baseline establishment failed: {performance['baseline_failed']}\n"
            elif 'summary' in performance:
                summary = performance['summary']
                summary_content += f"Overall System Capability: {summary.get('overall_capability', 'unknown').upper()}\n"
                summary_content += f"CPU Performance: {summary.get('cpu_performance', 'unknown')}\n"
                summary_content += f"Memory Performance: {summary.get('memory_performance', 'unknown')}\n"
                summary_content += f"GPU Available: {summary.get('gpu_available', False)}\n"
                summary_content += f"I/O Performance: {summary.get('io_performance', 'unknown')}\n"
                
                # Detailed baseline metrics
                baselines = performance.get('baselines', {})
                if 'cpu' in baselines:
                    cpu_baseline = baselines['cpu']
                    summary_content += f"\nCPU Benchmark:\n"
                    summary_content += f"  Matrix Size: {cpu_baseline.get('matrix_size', 'unknown')}\n"
                    summary_content += f"  Computation Time: {cpu_baseline.get('computation_time', 0):.4f}s\n"
                    summary_content += f"  GFLOPS: {cpu_baseline.get('gflops', 0):.2f}\n"
                
                if 'gpu' in baselines:
                    gpu_baselines = baselines['gpu']
                    summary_content += f"\nGPU Benchmarks:\n"
                    for gpu_name, gpu_data in gpu_baselines.items():
                        if isinstance(gpu_data, dict) and 'gflops' in gpu_data:
                            summary_content += f"  {gpu_name}: {gpu_data.get('gflops', 0):.2f} GFLOPS\n"
            
            else:
                summary_content += "Performance Metrics:\n"
                for metric, value in performance.items():
                    if isinstance(value, (int, float)):
                        summary_content += f"  {metric}: {value:.4f}\n"
                    else:
                        summary_content += f"  {metric}: {value}\n"
        else:
            summary_content += "No performance metrics available\n"
        summary_content += "\n"
        
        # Enhanced Dependencies Information
        summary_content += "DEPENDENCIES\n"
        summary_content += "-" * 14 + "\n"
        summary_content += f"PyTorch Version: {deps_info.get('torch_version', 'unknown')}\n"
        summary_content += f"Python Version: {deps_info.get('python_version', 'unknown')}\n"
        summary_content += f"Platform: {deps_info.get('platform', 'unknown')}\n"
        
        # Enhanced dependency status from check_versions
        if detailed_versions:
            summary_content += "\nCore Dependencies:\n"
            core_deps = {k: v for k, v in detailed_versions.items() if isinstance(v, dict) and v.get('required', False)}
            for name, info in core_deps.items():
                status_icon = "[OK]" if info.get('compatible', False) else "[ERROR]"
                summary_content += f"  {status_icon} {name}: {info.get('version', 'unknown')}"
                if info.get('required_version'):
                    summary_content += f" (requires {info['required_version']})"
                summary_content += f" - {info.get('status', 'unknown')}\n"
            
            optional_deps = {k: v for k, v in detailed_versions.items() if isinstance(v, dict) and not v.get('required', False)}
            if optional_deps:
                summary_content += "\nOptional Dependencies:\n"
                # Limit for readability
                for name, info in list(optional_deps.items())[:15]:
                    if info.get('available', False):
                        summary_content += f"  [OK] {name}: {info.get('version', 'Available')}\n"
                    else:
                        summary_content += f"  - {name}: Not Available\n"
                
                if len(optional_deps) > 15:
                    summary_content += f"  ... and {len(optional_deps) - 15} more optional dependencies\n"
        else:
            # Fallback to basic optional dependencies
            optional_deps = deps_info.get('optional_available', {})
            if optional_deps:
                summary_content += "Optional Dependencies:\n"
                for name, available in optional_deps.items():
                    status_indicator = "[OK]" if available else "[MISSING]"
                    summary_content += f"  {status_indicator} {name}: {'Available' if available else 'Not Available'}\n"
        summary_content += "\n"
        
        # Reproducibility Status
        summary_content += "REPRODUCIBILITY CONFIGURATION\n"
        summary_content += "-" * 32 + "\n"
        if reproducibility:
            summary_content += f"Seed Configuration: {'PASSED' if reproducibility.get('seed_config_passed', False) else 'FAILED'}\n"
            summary_content += f"Compliance Score: {reproducibility.get('compliance_score', 0):.1f}%\n"
            recommendations = reproducibility.get('recommendations', [])
            if recommendations:
                summary_content += "Recommendations:\n"
                # Limit to first 5
                for rec in recommendations[:5]:
                    summary_content += f"  - {rec}\n"
        else:
            summary_content += "Reproducibility check not performed\n"
        summary_content += "\n"
        
        # Logging Configuration Status
        summary_content += "LOGGING CONFIGURATION\n"
        summary_content += "-" * 22 + "\n"
        if logging_config:
            summary_content += f"Configuration Status: {'PASSED' if logging_config.get('passed', False) else 'FAILED'}\n"
            summary_content += f"Compliance Score: {logging_config.get('compliance_score', 0):.1f}%\n"
            summary_content += f"Active Handlers: {logging_config.get('handlers_count', 0)}\n"
            feedback = logging_config.get('feedback', [])
            if feedback:
                summary_content += "Configuration Issues:\n"
                for issue in feedback:
                    summary_content += f"  - {issue}\n"
        else:
            summary_content += "Logging configuration check not performed\n"
        summary_content += "\n"
        
        # Performance Monitoring Status
        summary_content += "PERFORMANCE MONITORING\n"
        summary_content += "-" * 23 + "\n"
        if perf_monitoring:
            summary_content += f"Monitoring Available: {'YES' if perf_monitoring.get('available', False) else 'NO'}\n"
            summary_content += f"Hardware Integration: {'YES' if perf_monitoring.get('hardware_integration', False) else 'NO'}\n"
            capabilities = perf_monitoring.get('capabilities', {})
            if capabilities:
                summary_content += "Capabilities:\n"
                for capability, enabled in capabilities.items():
                    status = "[OK]" if enabled else "[ERROR]"
                    summary_content += f"  {status} {capability.replace('_', ' ').title()}\n"
        else:
            summary_content += "Performance monitoring check not performed\n"
        summary_content += "\n"
        
        # Memory Management Status
        summary_content += "MEMORY MANAGEMENT\n"
        summary_content += "-" * 18 + "\n"
        if memory_mgmt:
            summary_content += f"Comprehensive Management: {'AVAILABLE' if memory_mgmt.get('comprehensive', False) else 'LIMITED'}\n"
            capabilities = memory_mgmt.get('capabilities', {})
            if capabilities:
                summary_content += "Capabilities:\n"
                for capability, enabled in capabilities.items():
                    status = "[OK]" if enabled else "[ERROR]"
                    summary_content += f"  {status} {capability.replace('_', ' ').title()}\n"
            
            test_results = memory_mgmt.get('test_results', {})
            if test_results.get('basic_test_passed'):
                summary_content += f"Memory Cleanup Test: PASSED\n"
                if 'cleanup_effectiveness' in test_results:
                    summary_content += f"Cleanup Effectiveness: {test_results['cleanup_effectiveness']:.1f}%\n"
        else:
            summary_content += "Memory management check not performed\n"
        summary_content += "\n"
        
        # System Architecture Details
        arch_info = detailed_hw.get('system_architecture', {})
        if arch_info.get('available'):
            summary_content += "SYSTEM ARCHITECTURE\n"
            summary_content += "-" * 20 + "\n"
            summary_content += f"Architecture: {arch_info.get('architecture', 'unknown')}\n"
            summary_content += f"Machine Type: {arch_info.get('machine', 'unknown')}\n"
            summary_content += f"System: {arch_info.get('system', 'unknown')} {arch_info.get('release', '')}\n"
            summary_content += f"Processor: {arch_info.get('processor', 'unknown')}\n"
            summary_content += f"Python Build: {arch_info.get('python_build', 'unknown')}\n"
            summary_content += "\n"
        
        # Enhanced Footer with Diagnostic Summary
        summary_content += "=" * 80 + "\n"
        summary_content += "DIAGNOSTIC SUMMARY\n"
        summary_content += "=" * 80 + "\n"
        
        summary_content += f"Diagnostic Checks Performed: {len(checks_performed)}\n"
        summary_content += f"Checks Passed: {len(checks_passed)}\n"
        summary_content += f"System Health Score: {system_health_score:.1f}%\n"
        summary_content += f"Template Architecture: {'Active' if template_renderer else 'Fallback'}\n"
        summary_content += f"\nPassed Checks: {', '.join(checks_passed) if checks_passed else 'None'}\n"
        
        failed_checks = [check for check in checks_performed if check not in checks_passed]
        if failed_checks:
            summary_content += f"Failed Checks: {', '.join(failed_checks)}\n"
        
        summary_content += f"\nReport Generated: {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')}\n"
        summary_content += f"Dashboard Architecture: {'Template-Based v3.0' if template_renderer else 'Inline Fallback'}\n"
        summary_content += "End of Report Entry\n"
        summary_content += "=" * 80 + "\n\n"
        
        # Write the complete summary content in a single operation
        with open(consolidated_files['summary'], 'a', encoding='utf-8') as f:
            f.write(summary_content)
        
        # 3. Save to consolidated status file (append as array element)
        status_entry = {
            **entry_metadata,
            'data': compact_status
        }
        
        if consolidated_files['status'].exists():
            try:
                with open(consolidated_files['status'], 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
                status_data['entries'].append(status_entry)
                status_data['metadata']['total_entries'] = len(status_data['entries'])
                status_data['metadata']['last_updated'] = timestamp_obj.isoformat()
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Failed to load status file, creating new: {e}")
                status_data = {
                    'date': date_str,
                    'entries': [status_entry],
                    'metadata': {
                        'version': '3.1',
                        'created_at': timestamp_obj.isoformat(),
                        'total_entries': 1,
                        'last_updated': timestamp_obj.isoformat(),
                        'report_type': 'consolidated_status',
                        'template_based': template_renderer is not None
                    }
                }
        else:
            status_data = {
                'date': date_str,
                'entries': [status_entry],
                'metadata': {
                    'version': '3.1',
                    'created_at': timestamp_obj.isoformat(),
                    'total_entries': 1,
                    'last_updated': timestamp_obj.isoformat(),
                    'report_type': 'consolidated_status',
                    'template_based': template_renderer is not None
                }
            }
        
        with open(consolidated_files['status'], 'w', encoding='utf-8') as f:
            json.dump(status_data, f, indent=2, default=str, ensure_ascii=False)
        
        # 4. Save to consolidated diagnostics file (append as array element)
        diagnostics_entry = {
            **entry_metadata,
            'data': diagnostic_data
        }
        
        if consolidated_files['diagnostics'].exists():
            try:
                with open(consolidated_files['diagnostics'], 'r', encoding='utf-8') as f:
                    diagnostics_data = json.load(f)
                diagnostics_data['entries'].append(diagnostics_entry)
                diagnostics_data['metadata']['total_entries'] = len(diagnostics_data['entries'])
                diagnostics_data['metadata']['last_updated'] = timestamp_obj.isoformat()
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Failed to load diagnostics file, creating new: {e}")
                diagnostics_data = {
                    'date': date_str,
                    'entries': [diagnostics_entry],
                    'metadata': {
                        'version': '3.1',
                        'created_at': timestamp_obj.isoformat(),
                        'total_entries': 1,
                        'last_updated': timestamp_obj.isoformat(),
                        'report_type': 'consolidated_diagnostics',
                        'template_based': template_renderer is not None
                    }
                }
        else:
            diagnostics_data = {
                'date': date_str,
                'entries': [diagnostics_entry],
                'metadata': {
                    'version': '3.1',
                    'created_at': timestamp_obj.isoformat(),
                    'total_entries': 1,
                    'last_updated': timestamp_obj.isoformat(),
                    'report_type': 'consolidated_diagnostics',
                    'template_based': template_renderer is not None
                }
            }
        
        with open(consolidated_files['diagnostics'], 'w', encoding='utf-8') as f:
            json.dump(diagnostics_data, f, indent=2, default=str, ensure_ascii=False)
        
        # 5. Save to consolidated dashboard JSON file (append as array element)
        dashboard_json_entry = {
            **entry_metadata,
            'data': dashboard_json_data
        }
        
        if consolidated_files['dashboard_json'].exists():
            try:
                with open(consolidated_files['dashboard_json'], 'r', encoding='utf-8') as f:
                    dashboard_json_file_data = json.load(f)
                dashboard_json_file_data['entries'].append(dashboard_json_entry)
                dashboard_json_file_data['metadata']['total_entries'] = len(dashboard_json_file_data['entries'])
                dashboard_json_file_data['metadata']['last_updated'] = timestamp_obj.isoformat()
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Failed to load dashboard JSON file, creating new: {e}")
                dashboard_json_file_data = {
                    'date': date_str,
                    'entries': [dashboard_json_entry],
                    'metadata': {
                        'version': '3.1',
                        'created_at': timestamp_obj.isoformat(),
                        'total_entries': 1,
                        'last_updated': timestamp_obj.isoformat(),
                        'report_type': 'consolidated_dashboard_data',
                        'template_based': template_renderer is not None
                    }
                }
        else:
            dashboard_json_file_data = {
                'date': date_str,
                'entries': [dashboard_json_entry],
                'metadata': {
                    'version': '3.1',
                    'created_at': timestamp_obj.isoformat(),
                    'total_entries': 1,
                    'last_updated': timestamp_obj.isoformat(),
                    'report_type': 'consolidated_dashboard_data',
                    'template_based': template_renderer is not None
                }
            }
        
        with open(consolidated_files['dashboard_json'], 'w', encoding='utf-8') as f:
            json.dump(dashboard_json_file_data, f, indent=2, default=str, ensure_ascii=False)
        
        # 6. Create/Update HTML dashboard file using Template-Based Architecture
        try:
            if template_renderer:
                # USE TEMPLATE-BASED RENDERING
                logger.debug("Generating dashboard using template-based architecture (optimized)")
                html_content = template_renderer.render_dashboard(
                    enhanced_status=enhanced_status,
                    compact_status=compact_status,
                    system_health_score=system_health_score,
                    dashboard_json_file_data=dashboard_json_file_data,
                    consolidated_files=consolidated_files,
                    report_data=report_data,
                    status_data=status_data,
                    diagnostics_data=diagnostics_data,
                    timestamp_obj=timestamp_obj
                )
                
                # Add template architecture metadata to HTML
                html_content = html_content.replace(
                    '<body>', 
                    f'<body><!-- Generated using Optimized Template-Based Architecture v3.1 at {timestamp_obj.isoformat()} - Redundancy Eliminated -->'
                )
                
                logger.debug("Template-based dashboard rendered successfully (optimized)")
                
            else:
                # FALLBACK TO INLINE HTML GENERATION
                logger.warning("Generating dashboard using inline HTML fallback (optimized)")
                html_content = _generate_inline_dashboard_fallback(
                    enhanced_status, compact_status, system_health_score,
                    dashboard_json_file_data, consolidated_files, report_data,
                    status_data, diagnostics_data, timestamp_obj, 
                    date_str, time_str
                )
                
                logger.warning("Inline fallback dashboard generated (optimized)")
            
            # Write the HTML file
            with open(consolidated_files['dashboard_html'], 'w', encoding='utf-8') as f:
                f.write(html_content)
                
        except Exception as dashboard_error:
            logger.error(f"Failed to generate dashboard HTML: {dashboard_error}")
            logger.debug("Dashboard generation error details:", exc_info=True)
            
            # Create minimal error dashboard
            error_html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Dashboard Generation Error - {date_str}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; background: #f5f5f5; }}
        .error-container {{ background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .error-title {{ color: #d32f2f; font-size: 1.5em; margin-bottom: 20px; }}
        .error-message {{ background: #ffebee; padding: 15px; border-radius: 4px; border-left: 4px solid #f44336; }}
        .optimization-note {{ background: #e8f5e8; padding: 15px; border-radius: 4px; border-left: 4px solid #4caf50; margin-top: 15px; }}
    </style>
</head>
<body>
    <div class="error-container">
        <h1 class="error-title">Dashboard Generation Error</h1>
        <div class="error-message">
            <strong>Error:</strong> {str(dashboard_error)}<br>
            <strong>Template Renderer:</strong> {'Available' if template_renderer else 'Not Available'}<br>
            <strong>Fallback Used:</strong> {'No' if template_renderer else 'Yes'}
        </div>
        <div class="optimization-note">
            <strong>Optimization Status:</strong> Redundant checks eliminated - Data extracted from existing system checks<br>
            <strong>Fallback Checks:</strong> {sum(1 for key in enhanced_status.keys() if key.endswith('_error'))}<br>
            <strong>System Health Score:</strong> {system_health_score:.1f}%
        </div>
        <p><strong>Status:</strong> {compact_status['status']}</p>
        <p><strong>Generated:</strong> {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
</body>
</html>"""
            
            with open(consolidated_files['dashboard_html'], 'w', encoding='utf-8') as f:
                f.write(error_html)
        
        # 7. Update index file
        index_path = report_dir / "deep_initialization_reports.txt"
        report_entry = (
            f"{timestamp}: {compact_status['status']} "
            f"({compact_status['duration_seconds']:.2f}s, "
            f"health: {system_health_score:.1f}%, "
            f"cuda: {compact_status['cuda_available']}, "
            f"models: {compact_status['model_variants']}, "
            f"template: {'OK' if template_renderer else 'ERROR'}, "
            f"optimized: YES)\n"
        )
        
        with open(index_path, 'a', encoding='utf-8') as f:
            f.write(report_entry)
        
        # 8. Update latest symlinks
        try:
            symlinks = {
                'deep_latest_init_report.json': consolidated_files['report'],
                'deep_latest_init_summary.txt': consolidated_files['summary'],
                'deep_latest_init_status.json': consolidated_files['status'],
                'deep_latest_init_diagnostics.json': consolidated_files['diagnostics'],
                'deep_latest_system_dashboard.json': consolidated_files['dashboard_json'],
                'deep_latest_system_dashboard.html': consolidated_files['dashboard_html']
            }
            
            for symlink_name, target_file in symlinks.items():
                symlink_path = report_dir / symlink_name
                if symlink_path.exists() or symlink_path.is_symlink():
                    symlink_path.unlink()
                symlink_path.symlink_to(target_file.name)
            
            logger.debug("Latest report symlinks updated successfully")
            
        except (OSError, NotImplementedError):
            # Create a text file with paths instead
            with open(report_dir / "latest_reports.txt", 'w') as f:
                f.write("Latest consolidated reports (optimized):\n")
                for file_type, file_path in consolidated_files.items():
                    f.write(f"{file_type}: {file_path.name}\n")
                f.write(f"Template Architecture: {'Active' if template_renderer else 'Fallback'}\n")
                f.write(f"Optimization Applied: YES - Redundant checks eliminated\n")
        
        # Log successful report generation with optimization details
        architecture_type = "Optimized Template-Based v3.1" if template_renderer else "Optimized Inline Fallback"
        fallback_count = sum(1 for key in enhanced_status.keys() if key.endswith('_error'))
        
        logger.debug(f"Initialization report saved using {architecture_type} architecture:")
        logger.debug(f"  - OPTIMIZATION: Redundant checks eliminated")
        logger.debug(f"  - DATA SOURCE: Extracted from existing initialize_system() results")
        logger.debug(f"  - FALLBACK CHECKS: {fallback_count} (only when data unavailable)")
        logger.debug(f"  - Full reports: {consolidated_files['report'].name}")
        logger.debug(f"  - Summary: {consolidated_files['summary'].name}")
        logger.debug(f"  - Status: {consolidated_files['status'].name}")
        logger.debug(f"  - Diagnostics: {consolidated_files['diagnostics'].name}")
        logger.debug(f"  - Dashboard Data: {consolidated_files['dashboard_json'].name}")
        logger.debug(f"  - Dashboard HTML: {consolidated_files['dashboard_html'].name}")
        logger.debug(f"  - Total entries today: {report_data['metadata']['total_reports']}")
        logger.debug(f"  - Template Renderer: {'Active' if template_renderer else 'Unavailable'}")
        
    except Exception as e:
        # Log the error but don't raise it - we don't want report saving
        # to interrupt the initialization process
        error_msg = f"Failed to save optimized initialization report: {str(e)}"
        logger.error(error_msg)
        logger.debug(f"Report save error details:", exc_info=True)
        
        # Try to save a minimal error report
        try:
            error_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            error_path = report_dir / f"deep_report_error_{error_timestamp}.txt"
            
            with open(error_path, 'w', encoding='utf-8') as f:
                f.write(f"OPTIMIZED INITIALIZATION REPORT SAVE FAILED\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Error: {str(e)}\n")
                f.write(f"Error Type: {type(e).__name__}\n")
                f.write(f"Optimization Status: Redundancy elimination attempted\n")
                f.write(f"Template Renderer: {'Available' if template_renderer else 'Not Available'}\n")
                f.write(f"Original system_status keys: {list(system_status.keys()) if isinstance(system_status, dict) else 'Not a dict'}\n")
                
                # Include file information if available
                if consolidated_files:
                    f.write(f"Target files: {[str(f) for f in consolidated_files.values()]}\n")
                
                f.write(f"Python version: {sys.version}\n")
                f.write(f"Platform: {platform.platform()}\n")
                f.write(f"Working directory: {Path.cwd()}\n")
                f.write(f"Report directory: {report_dir}\n")
            
            logger.info(f"Minimal error report saved to: {error_path}")
            
        except Exception as fallback_error:
            logger.critical(f"Failed to save even minimal error report: {fallback_error}")

def _generate_inline_dashboard_fallback(enhanced_status: Dict, compact_status: Dict, 
                                     system_health_score: float, dashboard_json_file_data: Dict,
                                     consolidated_files: Dict, report_data: Dict, 
                                     status_data: Dict, diagnostics_data: Dict,
                                     timestamp_obj: datetime, date_str: str, time_str: str) -> str:
    """
    Generate inline HTML dashboard as fallback when template renderer is not available.
    
    This function provides a simplified version of the dashboard with embedded styles
    and JavaScript, maintaining core functionality without template dependencies.
    """
    # Extract data safely
    init_info = enhanced_status.get('initialization', {})
    sys_info = enhanced_status.get('system', {})
    detailed_hw = enhanced_status.get('detailed_hardware', {})
    config_info = enhanced_status.get('config', {})
    model_info = enhanced_status.get('models', {})
    cuda_info = detailed_hw.get('cuda', {})
    cpu_info = detailed_hw.get('cpu_cores', {})
    ram_info = detailed_hw.get('system_ram', {})
    
    # Determine health score class
    if system_health_score > 80:
        health_class = 'excellent'
    elif system_health_score > 60:
        health_class = 'good'
    elif system_health_score > 40:
        health_class = 'fair'
    else:
        health_class = 'poor'
    
    # Generate basic GPU info
    gpu_info_html = ""
    if cuda_info.get('available'):
        gpu_info_html = f"""
        <div class="metric-card">
            <div class="metric-title">CUDA Information</div>
            <div class="metric-value">Available</div>
            <div class="metric-description">
                Version: {cuda_info.get('cuda_version', 'unknown')}<br>
                GPU Count: {cuda_info.get('gpu_count', 0)}<br>
                cuDNN: {cuda_info.get('cudnn_version', 'unknown')}
            </div>
        </div>
        """
    else:
        gpu_info_html = """
        <div class="metric-card">
            <div class="metric-title">CUDA Status</div>
            <div class="metric-value">Not Available</div>
            <div class="metric-description">CUDA is not available on this system</div>
        </div>
        """
    
    return f"""<!DOCTYPE html>
<html>
<head>
    <title>Deep Learning System Dashboard - {date_str} (Fallback)</title>
    <style>
        body {{ 
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
            margin: 0; 
            padding: 20px; 
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            color: #333;
        }}
        .container {{ 
            max-width: 1200px; 
            margin: 0 auto; 
            background: white; 
            padding: 30px; 
            border-radius: 12px; 
            box-shadow: 0 15px 35px rgba(0,0,0,0.1);
        }}
        .header {{ 
            text-align: center; 
            margin-bottom: 30px; 
            padding: 30px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 8px;
            margin: -30px -30px 30px -30px;
        }}
        .header h1 {{ margin: 0; font-size: 2.5em; }}
        .header .subtitle {{ font-size: 1.1em; margin-top: 10px; opacity: 0.9; }}
        .fallback-notice {{
            background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
            border-left: 5px solid #ff9800;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }}
        .health-score {{
            font-size: 3em; 
            font-weight: bold; 
            text-align: center; 
            padding: 30px; 
            border-radius: 10px; 
            margin: 20px 0;
            color: white;
        }}
        .health-score.excellent {{ background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%); }}
        .health-score.good {{ background: linear-gradient(135deg, #8BC34A 0%, #7CB342 100%); }}
        .health-score.fair {{ background: linear-gradient(135deg, #FF9800 0%, #F57C00 100%); }}
        .health-score.poor {{ background: linear-gradient(135deg, #F44336 0%, #D32F2F 100%); }}
        .metric-grid {{ 
            display: grid; 
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); 
            gap: 20px; 
            margin: 20px 0; 
        }}
        .metric-card {{ 
            background: #f9f9f9;
            padding: 20px; 
            border-radius: 8px; 
            border-left: 4px solid #2196f3;
            transition: transform 0.2s ease;
        }}
        .metric-card:hover {{ transform: translateY(-2px); }}
        .metric-title {{ font-weight: bold; color: #2196f3; margin-bottom: 10px; }}
        .metric-value {{ font-size: 1.8em; font-weight: bold; color: #333; margin-bottom: 5px; }}
        .metric-description {{ font-size: 0.9em; color: #666; }}
        .status-success {{ color: #4CAF50; }}
        .status-warning {{ color: #FF9800; }}
        .status-error {{ color: #F44336; }}
        .footer {{ 
            text-align: center; 
            margin-top: 40px; 
            padding: 20px; 
            border-top: 2px solid #eee; 
            color: #666; 
        }}
    </style>
    <script>
        const dashboardData = {json.dumps(dashboard_json_file_data, default=str, indent=2)};
        const currentSystemStatus = {json.dumps(compact_status, default=str, indent=2)};
        
        function exportDashboardData() {{
            const exportData = {{
                generated_at: new Date().toISOString(),
                dashboard_metadata: dashboardData.metadata,
                current_status: currentSystemStatus,
                all_entries: dashboardData.entries,
                export_type: 'fallback_dashboard_export',
                export_version: '1.0'
            }};
            
            const dataStr = "data:text/json;charset=utf-8," + encodeURIComponent(JSON.stringify(exportData, null, 2));
            const downloadAnchorNode = document.createElement('a');
            downloadAnchorNode.setAttribute("href", dataStr);
            downloadAnchorNode.setAttribute("download", "fallback_dashboard_export_" + new Date().toISOString().replace(/[:.]/g, '-') + ".json");
            document.body.appendChild(downloadAnchorNode);
            downloadAnchorNode.click();
            downloadAnchorNode.remove();
            alert('Dashboard data exported successfully!');
        }}
    </script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Deep Learning System Dashboard</h1>
            <div class="subtitle">Initialization Report - {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')}</div>
        </div>
        
        <div class="fallback-notice">
            <strong>⚠️ Fallback Mode:</strong> Template-based rendering is unavailable. 
            Using simplified inline HTML generation. Some advanced features may be limited.
        </div>
        
        <div class="health-score {health_class}">
            {system_health_score:.1f}%
            <div style="font-size: 0.4em; margin-top: 10px;">System Health Score</div>
        </div>
        
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-title">Initialization Status</div>
                <div class="metric-value {'status-success' if compact_status['status'] == 'success' else 'status-error'}">{compact_status['status'].upper()}</div>
                <div class="metric-description">Duration: {compact_status['duration_seconds']:.2f} seconds</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">CUDA GPU</div>
                <div class="metric-value {'status-success' if compact_status['cuda_available'] else 'status-error'}">
                    {'Available' if compact_status['cuda_available'] else 'Not Available'}
                </div>
                <div class="metric-description">GPU Count: {cuda_info.get('gpu_count', 0) if cuda_info.get('available') else 0}</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Model Variants</div>
                <div class="metric-value">{compact_status['model_variants']}</div>
                <div class="metric-description">Available model configurations</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">System Resources</div>
                <div class="metric-value">{cpu_info.get('logical_cores', '?')}</div>
                <div class="metric-description">
                    CPU Cores: {cpu_info.get('logical_cores', '?')} logical, {cpu_info.get('physical_cores', '?')} physical<br>
                    RAM: {ram_info.get('ram_total_gb', 0):.1f}GB total, {ram_info.get('ram_available_gb', 0):.1f}GB available
                </div>
            </div>
        </div>
        
        <h3>Configuration Scores</h3>
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-title">Reproducibility</div>
                <div class="metric-value">{compact_status.get('reproducibility_score', 0):.1f}%</div>
                <div class="metric-description">Seed configuration compliance</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Logging Compliance</div>
                <div class="metric-value">{compact_status.get('logging_compliance', 0):.1f}%</div>
                <div class="metric-description">Logging setup quality</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Performance Monitoring</div>
                <div class="metric-value {'status-success' if compact_status.get('performance_monitoring') else 'status-error'}">
                    {'Available' if compact_status.get('performance_monitoring') else 'Not Available'}
                </div>
                <div class="metric-description">System performance tracking</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Memory Management</div>
                <div class="metric-value {'status-success' if compact_status.get('memory_management') else 'status-warning'}">
                    {'Comprehensive' if compact_status.get('memory_management') else 'Basic'}
                </div>
                <div class="metric-description">Memory handling capabilities</div>
            </div>
        </div>
        
        <h3>Hardware Details</h3>
        <div class="metric-grid">
            {gpu_info_html}
            
            <div class="metric-card">
                <div class="metric-title">CPU Information</div>
                <div class="metric-value">{cpu_info.get('logical_cores', '?')}</div>
                <div class="metric-description">
                    Logical Cores: {cpu_info.get('logical_cores', 'Unknown')}<br>
                    Physical Cores: {cpu_info.get('physical_cores', 'Unknown')}<br>
                    Frequency: {cpu_info.get('capacity', {}).get('frequency_ghz', 'Unknown')} GHz
                </div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Total Initializations</div>
                <div class="metric-value">{dashboard_json_file_data['metadata']['total_entries']}</div>
                <div class="metric-description">Entries recorded today</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">Export Data</div>
                <div class="metric-value">
                    <button onclick="exportDashboardData()" style="background: #2196f3; color: white; border: none; padding: 10px 20px; border-radius: 4px; cursor: pointer;">
                        Export JSON
                    </button>
                </div>
                <div class="metric-description">Download complete system data</div>
            </div>
        </div>
        
        <div class="footer">
            <p><strong>Deep Learning System Dashboard - Fallback Mode</strong></p>
            <p>Generated: {timestamp_obj.strftime('%Y-%m-%d %H:%M:%S')} | Version: 3.0 (Inline Fallback)</p>
            <p>⚠️ Limited functionality - Template renderer unavailable</p>
        </div>
    </div>
</body>
</html>"""



def _display_text_report(file_path, title=None):
    """Display a text report with pagination."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if not title:
            title = f"Text Report: {file_path.name}"
        
        console.print(
            Panel.fit(
                f"[bold green]{title}[/bold green]\n"
                f"File: [cyan]{file_path.name}[/cyan]\n"
                f"Size: [cyan]{len(content)} characters[/cyan]",
                title="TEXT REPORT",
                border_style="green",
                padding=(1, 2)
            )
        )
        
        # Split into pages for large files
        lines = content.split('\n')
        page_size = 50
        
        if len(lines) <= page_size:
            print(f"\n{content}")
        else:
            current_page = 0
            while current_page * page_size < len(lines):
                start_idx = current_page * page_size
                end_idx = min((current_page + 1) * page_size, len(lines))
                
                page_content = '\n'.join(lines[start_idx:end_idx])
                print(f"\n{Fore.CYAN}--- Page {current_page + 1} of {(len(lines) - 1) // page_size + 1} ---{Style.RESET_ALL}")
                print(page_content)
                
                if end_idx < len(lines):
                    try:
                        user_input = input(f"\n{Fore.YELLOW}Press Enter for next page, 'q' to quit, or 'a' for all: {Style.RESET_ALL}").strip().lower()
                        if user_input == 'q':
                            break
                        elif user_input == 'a':
                            print('\n'.join(lines[end_idx:]))
                            break
                    except (EOFError, KeyboardInterrupt):
                        break
                
                current_page += 1
        
    except Exception as e:
        print(f"{Fore.RED}Error reading file: {str(e)}{Style.RESET_ALL}")

def _display_json_report(file_path):
    """Display a JSON report with formatted output."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        console.print(
            Panel.fit(
                f"[bold green]JSON Report: {file_path.name}[/bold green]\n"
                f"Type: [cyan]{data.get('metadata', {}).get('report_type', 'unknown')}[/cyan]\n"
                f"Version: [cyan]{data.get('metadata', {}).get('version', 'unknown')}[/cyan]\n"
                f"Entries: [cyan]{len(data.get('reports', []))}[/cyan]",
                title="JSON REPORT",
                border_style="green",
                padding=(1, 2)
            )
        )
        
        # Display summary of entries
        reports = data.get('reports', [])
        if reports:
            print(f"\n{Fore.CYAN}Report Entries:{Style.RESET_ALL}")
            # Show first 10
            for i, report in enumerate(reports[:10], 1):
                timestamp = report.get('timestamp', 'Unknown')
                status = report.get('data', {}).get('initialization', {}).get('status', 'unknown')
                duration = report.get('data', {}).get('initialization', {}).get('duration_seconds', 0)
                print(f"  {i}. {timestamp} - Status: {status}, Duration: {duration:.2f}s")
            
            if len(reports) > 10:
                print(f"  ... and {len(reports) - 10} more entries")
        
        # Ask if user wants to see full JSON
        try:
            show_full = input(f"\n{Fore.YELLOW}Show full JSON content? (y/N): {Style.RESET_ALL}").strip().lower()
            if show_full in ('y', 'yes'):
                formatted_json = json.dumps(data, indent=2, default=str)
                print(f"\n{formatted_json}")
        except (EOFError, KeyboardInterrupt):
            pass
            
    except Exception as e:
        print(f"{Fore.RED}Error reading JSON file: {str(e)}{Style.RESET_ALL}")

def _display_status_report(file_path):
    """Display a status report with compact formatting."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        entries = data.get('entries', [])
        metadata = data.get('metadata', {})
        
        console.print(
            Panel.fit(
                f"[bold green]Status Report: {file_path.name}[/bold green]\n"
                f"Date: [cyan]{data.get('date', 'unknown')}[/cyan]\n"
                f"Entries: [cyan]{len(entries)}[/cyan]\n"
                f"Last Updated: [cyan]{metadata.get('last_updated', 'unknown')}[/cyan]",
                title="STATUS REPORT",
                border_style="green",
                padding=(1, 2)
            )
        )
        
        if entries:
            print(f"\n{Fore.CYAN}Initialization Status Summary:{Style.RESET_ALL}")
            print(f"{'Time':<10} {'Status':<10} {'Health':<8} {'Duration':<10} {'CUDA':<6} {'Models':<7}")
            print("-" * 60)
            # Show last 20 entries
            for entry in entries[-20:]:
                entry_data = entry.get('data', {})
                time_str = entry.get('time_str', 'unknown')[:8]
                status = entry_data.get('status', 'unknown')[:9]
                health = f"{entry_data.get('system_health_score', 0):.1f}%"
                duration = f"{entry_data.get('duration_seconds', 0):.2f}s"
                cuda = "Yes" if entry_data.get('cuda_available', False) else "No"
                models = str(entry_data.get('model_variants', 0))
                
                print(f"{time_str:<10} {status:<10} {health:<8} {duration:<10} {cuda:<6} {models:<7}")
                
    except Exception as e:
        print(f"{Fore.RED}Error reading status file: {str(e)}{Style.RESET_ALL}")

def _display_diagnostic_report(file_path):
    """Display a diagnostic report with technical details."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        entries = data.get('entries', [])
        metadata = data.get('metadata', {})
        
        console.print(
            Panel.fit(
                f"[bold green]Diagnostic Report: {file_path.name}[/bold green]\n"
                f"Date: [cyan]{data.get('date', 'unknown')}[/cyan]\n"
                f"Entries: [cyan]{len(entries)}[/cyan]\n"
                f"Framework Version: [cyan]{metadata.get('version', 'unknown')}[/cyan]",
                title="DIAGNOSTIC REPORT",
                border_style="green",
                padding=(1, 2)
            )
        )
        
        if entries:
            # Show latest entry details
            latest_entry = entries[-1]
            entry_data = latest_entry.get('data', {})
            
            print(f"\n{Fore.CYAN}Latest Diagnostic Entry ({latest_entry.get('timestamp', 'unknown')}):{Style.RESET_ALL}")
            
            # Hardware analysis
            hw_analysis = entry_data.get('hardware_analysis', {})
            if hw_analysis:
                print(f"\n  {Fore.GREEN}Hardware Analysis:{Style.RESET_ALL}")
                cuda_info = hw_analysis.get('cuda', {})
                if cuda_info:
                    print(f"    CUDA: {cuda_info.get('status', 'unknown')} - {cuda_info.get('gpu_count', 0)} GPUs")
                cpu_info = hw_analysis.get('cpu_cores', {})
                if cpu_info:
                    print(f"    CPU: {cpu_info.get('logical_cores', '?')} logical cores")
            
            # Version validation
            version_validation = entry_data.get('version_validation', {})
            if version_validation:
                print(f"\n  {Fore.GREEN}Version Validation:{Style.RESET_ALL}")
                compatible_count = sum(1 for v in version_validation.values() if v.get('compatible', False))
                print(f"    Compatible dependencies: {compatible_count}/{len(version_validation)}")
            
            # Reproducibility config
            repro_config = entry_data.get('reproducibility_config', {})
            if repro_config:
                print(f"\n  {Fore.GREEN}Reproducibility:{Style.RESET_ALL}")
                print(f"    Seed config passed: {repro_config.get('seed_config_passed', False)}")
                print(f"    Compliance score: {repro_config.get('compliance_score', 0):.1f}%")
                
    except Exception as e:
        print(f"{Fore.RED}Error reading diagnostic file: {str(e)}{Style.RESET_ALL}")

def _display_dashboard_data(file_path):
    """Display dashboard data with visualization options."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        entries = data.get('entries', [])
        metadata = data.get('metadata', {})
        
        console.print(
            Panel.fit(
                f"[bold green]Dashboard Data: {file_path.name}[/bold green]\n"
                f"Date: [cyan]{data.get('date', 'unknown')}[/cyan]\n"
                f"Entries: [cyan]{len(entries)}[/cyan]\n"
                f"Last Updated: [cyan]{metadata.get('last_updated', 'unknown')}[/cyan]",
                title="DASHBOARD DATA",
                border_style="green",
                padding=(1, 2)
            )
        )
        
        if entries:
            # Calculate statistics
            health_scores = [e.get('data', {}).get('health_score', 0) for e in entries]
            durations = [e.get('data', {}).get('duration_seconds', 0) for e in entries]
            
            avg_health = sum(health_scores) / len(health_scores)
            avg_duration = sum(durations) / len(durations)
            success_count = sum(1 for e in entries if e.get('data', {}).get('status', '').lower() == 'success')
            
            print(f"\n{Fore.CYAN}Statistics Summary:{Style.RESET_ALL}")
            print(f"  Average Health Score: {avg_health:.1f}%")
            print(f"  Average Duration: {avg_duration:.2f}s")
            print(f"  Success Rate: {success_count}/{len(entries)} ({success_count/len(entries)*100:.1f}%)")
            
            # Show trend
            if len(entries) >= 2:
                recent_health = sum(e.get('data', {}).get('health_score', 0) for e in entries[-5:]) / min(5, len(entries))
                overall_health = sum(health_scores) / len(health_scores)
                trend = "↑" if recent_health > overall_health else "↓" if recent_health < overall_health else "→"
                print(f"  Health Trend: {trend} Recent avg: {recent_health:.1f}%")
                
    except Exception as e:
        print(f"{Fore.RED}Error reading dashboard data: {str(e)}{Style.RESET_ALL}")

def _show_all_report_files(report_dir, report_files):
    """Show all available report files with details."""
    print(f"\n{Fore.CYAN}All Available Report Files:{Style.RESET_ALL}")
    print(f"Location: {report_dir}\n")
    
    all_files = []
    for category, files in report_files.items():
        for file_path in files:
            if file_path.exists():
                file_size = file_path.stat().st_size / 1024
                mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                all_files.append((file_path, file_size, mod_time, category))
    
    # Sort by modification time (newest first)
    all_files.sort(key=lambda x: x[2], reverse=True)
    
    if all_files:
        print(f"{'File Name':<40} {'Size':<10} {'Modified':<20} {'Type':<15}")
        print("-" * 90)
        
        for file_path, file_size, mod_time, category in all_files:
            print(f"{file_path.name:<40} {file_size:>7.1f}KB {mod_time.strftime('%Y-%m-%d %H:%M'):<20} {category:<15}")
    else:
        print("No files found.")

# Model architecture comparison
def initialize_model_variants(silent: bool = False) -> None:
    """Initialize MODEL_VARIANTS dictionary with enhanced validation and error recovery.
    
    This function has been updated to leverage the model_instantiation_with_validation() helper
    for comprehensive model initialization with built-in validation and error handling.
    
    Args:
        silent: If True, suppress detailed logging messages and progress bars during system checks
    """
    global MODEL_VARIANTS
    
    if not silent:
        logger.info("Initializing model variants using enhanced helper functions")
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'successful_models': 0,
        'failed_models': 0,
        'current_model': None,
        'instantiation_method': None
    }
    
    # Calculate total work units for progress bar
    total_stages = 6  # System, Config, Params, Definitions, Models, Finalization
    total_models = 0  # Will be set after model_definitions is created
    
    try:
        # STAGE 1: System Preparation
        progress_data['current_stage'] = "System Preparation"
        
        hardware_data = None
        total_ram_gb = 8.0
        
        with alive_bar(total_stages, title='Initializing Model Variants\t', unit='stages') as bar:
            
            try:
                bar.text = "Checking system hardware..."
                hardware_data = check_hardware(include_memory_usage=True)
                total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
                
                # Initial memory cleanup for memory-constrained systems
                _optimize_memory_if_needed(
                    condition=total_ram_gb < 8,
                    hardware_data=hardware_data,
                    aggressive=total_ram_gb < 4,
                    silent=silent
                )
                bar.text = "System check complete"
            except Exception as e:
                if not silent:
                    logger.debug(f"Hardware detection failed: {e}")
                hardware_data = {}
                bar.text = "System check (using defaults)"
            
            # Clear existing variants
            MODEL_VARIANTS = {}
            bar()
            
            # STAGE 2: Configuration Loading
            progress_data['current_stage'] = "Loading Configuration"
            bar.text = "Loading configuration..."
            
            # Get current configuration with comprehensive fallbacks
            try:
                current_config = get_current_config()
                if not isinstance(current_config, dict):
                    current_config = {}
                
                model_config = current_config.get('model', {})
                data_config = current_config.get('data', {})
                training_config = current_config.get('training', {})
                hardware_config = current_config.get('hardware', {})
                system_config = current_config.get('system', {})
                
                if not silent:
                    logger.debug("Loaded comprehensive configuration for model variant initialization")
                bar.text = "Configuration loaded"
            except Exception as e:
                if not silent:
                    logger.warning(f"Could not load current config, using defaults: {e}")
                current_config = {}
                model_config = {}
                data_config = {}
                training_config = {}
                hardware_config = {}
                system_config = {}
                bar.text = "Configuration (using defaults)"
            
            # MEMORY OPTIMIZATION CHECKPOINT - Clear memory after config processing for large configs
            config_size_estimate = len(str(current_config))
            _optimize_memory_if_needed(
                condition=config_size_estimate > 50000 and total_ram_gb < 16,
                hardware_data=hardware_data,
                aggressive=config_size_estimate > 100000,
                silent=silent
            )
            
            bar()
            
            # STAGE 3: Parameter Extraction & Validation
            progress_data['current_stage'] = "Validating Parameters"
            bar.text = "Extracting parameters..."
            
            # Extract comprehensive configuration values using helper function
            test_input_dim = _extract_and_validate_config_param(
                data_config, 'features', 20, 'FEATURES',
                lambda x: isinstance(x, int) and x > 0,
                "input feature dimension", silent
            )
            
            base_encoding_dim = _extract_and_validate_config_param(
                model_config, 'encoding_dim', 16, 'DEFAULT_ENCODING_DIM',
                lambda x: isinstance(x, int) and x > 0,
                "latent encoding dimension", silent
            )
            
            base_hidden_dims = _extract_and_validate_config_param(
                model_config, 'hidden_dims', [128, 64], 'HIDDEN_LAYER_SIZES',
                lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(d, int) and d > 0 for d in x),
                "hidden layer dimensions", silent
            )
            
            base_dropout_rates = _extract_and_validate_config_param(
                model_config, 'dropout_rates', [0.2, 0.15], 'DROPOUT_RATES',
                lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(r, (int, float)) and 0 <= r < 1 for r in x),
                "dropout rates", silent
            )
            
            # Activation and normalization
            activation = _extract_and_validate_config_param(
                model_config, 'activation', 'leaky_relu', 'ACTIVATION',
                lambda x: x in ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu', 'prelu'],
                "activation function", silent
            )
            
            activation_param = _extract_and_validate_config_param(
                model_config, 'activation_param', 0.2, 'ACTIVATION_PARAM',
                lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
                "activation parameter", silent
            )
            
            normalization = _extract_and_validate_config_param(
                model_config, 'normalization', 'batch', 'NORMALIZATION',
                lambda x: x in ['batch', 'layer', 'instance', 'group', 'none', None],
                "normalization type", silent
            )
            
            # Architecture features
            use_batch_norm = _extract_and_validate_config_param(
                model_config, 'use_batch_norm', True, 'USE_BATCH_NORM',
                lambda x: isinstance(x, bool),
                "batch normalization flag", silent
            )
            
            use_layer_norm = _extract_and_validate_config_param(
                model_config, 'use_layer_norm', False, 'USE_LAYER_NORM',
                lambda x: isinstance(x, bool),
                "layer normalization flag", silent
            )
            
            skip_connection = _extract_and_validate_config_param(
                model_config, 'skip_connection', True, 'SKIP_CONNECTION',
                lambda x: isinstance(x, bool),
                "skip connections flag", silent
            )
            
            residual_blocks = _extract_and_validate_config_param(
                model_config, 'residual_blocks', False, 'RESIDUAL_BLOCKS',
                lambda x: isinstance(x, bool),
                "residual blocks flag", silent
            )
            
            use_attention = _extract_and_validate_config_param(
                model_config, 'use_attention', False, 'USE_ATTENTION',
                lambda x: isinstance(x, bool),
                "attention mechanism flag", silent
            )
            
            # Ensemble parameters
            num_models = _extract_and_validate_config_param(
                model_config, 'num_models', 3, 'NUM_MODELS',
                lambda x: isinstance(x, int) and 1 <= x <= 10,
                "ensemble size", silent
            )
            
            diversity_factor = _extract_and_validate_config_param(
                model_config, 'diversity_factor', 0.2, 'DIVERSITY_FACTOR',
                lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
                "ensemble diversity factor", silent
            )
            
            # Training parameters
            mixed_precision = _extract_and_validate_config_param(
                training_config, 'mixed_precision', False, 'MIXED_PRECISION',
                lambda x: isinstance(x, bool),
                "mixed precision training", silent
            )
            
            learning_rate = _extract_and_validate_config_param(
                training_config, 'learning_rate', 0.001, 'LEARNING_RATE',
                lambda x: isinstance(x, (int, float)) and x > 0,
                "learning rate", silent
            )
            
            batch_size = _extract_and_validate_config_param(
                training_config, 'batch_size', 32, 'BATCH_SIZE',
                lambda x: isinstance(x, int) and x > 0,
                "batch size", silent
            )
            
            # Hardware parameters
            device = _extract_and_validate_config_param(
                hardware_config, 'device', 'auto', 'DEVICE',
                lambda x: isinstance(x, str) and x in ['auto', 'cpu', 'cuda'] or x.startswith('cuda:'),
                "compute device", silent
            )
            
            # System parameters
            random_seed = _extract_and_validate_config_param(
                system_config, 'random_seed', 42, 'RANDOM_SEED',
                lambda x: isinstance(x, int),
                "random seed", silent
            )
            
            legacy_mode = _extract_and_validate_config_param(
                model_config, 'legacy_mode', False, 'LEGACY_MODE',
                lambda x: isinstance(x, bool),
                "legacy compatibility mode", silent
            )
            
            # Validate and adjust parameters using helper function
            base_hidden_dims, base_dropout_rates = _validate_and_adjust_parameters(
                base_hidden_dims, base_dropout_rates, silent
            )
            
            # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before intensive model testing
            _optimize_memory_if_needed(
                condition=len(str(base_hidden_dims) + str(base_dropout_rates)) > 1000 or total_ram_gb < 8,
                hardware_data=hardware_data,
                aggressive=total_ram_gb < 4,
                silent=silent
            )
            
            bar.text = "Parameters validated"
            bar()
            
            # STAGE 4: Model Definitions
            progress_data['current_stage'] = "Creating Model Definitions"
            bar.text = "Creating model definitions..."
            
            # Create model test definitions using helper function
            model_definitions = _create_model_test_definition(
                encoding_dim=base_encoding_dim,
                hidden_dims=base_hidden_dims,
                dropout_rates=base_dropout_rates,
                use_attention=use_attention,
                residual_blocks=residual_blocks,
                skip_connection=skip_connection,
                legacy_mode=legacy_mode,
                num_models=num_models,
                diversity_factor=diversity_factor,
                mixed_precision=mixed_precision
            )
            
            total_models = len(model_definitions)
            bar.text = f"Created {total_models} model definitions"
            bar()
            
            # Close the main progress bar and start model initialization bar
            bar.text = "Starting model initialization..."
        
        # STAGE 5: Model Initialization with detailed progress
        progress_data['current_stage'] = "Model Initialization"
        
        # Track comprehensive initialization statistics
        initialization_stats = {
            'attempted': 0,
            'successful': [],
            'failed': [],
            'skipped': [],
            'fallback_used': [],
            'minimal_used': [],
            'adaptive_used': [],
            'validation_passed': 0,
            'errors': [],
            'config_validation_passed': 0,
            'architecture_tests_passed': 0,
            'performance_tests_passed': 0,
            'memory_optimizations_performed': 0,
            'detailed_metrics': {}
        }
        
        # Status symbols for visual feedback
        status_symbols = {
            'success': '[OK]',
            'failure': '[FAIL]',
            'skip': '[SKIP]'
        }
        
        method_symbols = {
            'primary': '[PRIMARY]',
            'fallback': '[FALLBACK]',
            'minimal': '[MINIMAL]',
            'adaptive': '[ADAPTIVE]'
        }
        
        with alive_bar(total_models, title='Initializing Models\t\t', unit='models') as model_bar:
            
            # Initialize each model variant using the enhanced helper function
            for name, definition in model_definitions.items():
                initialization_stats['attempted'] += 1
                progress_data['current_model'] = name
                
                # Update progress bar with current model info
                successful_count = len(initialization_stats['successful'])
                failed_count = len(initialization_stats['failed'])
                model_bar.text = f"Testing {name}... ({successful_count} passed, {failed_count} failed)"
                
                try:
                    if not silent:
                        logger.debug(f"Attempting to initialize {name} using enhanced helper function")
                    
                    # Check if model class exists and is callable
                    if not definition['class_check']():
                        error_msg = f"Class not available or not callable"
                        if not silent:
                            logger.warning(f"{name}: {error_msg}")
                        initialization_stats['errors'].append(f"{name}: {error_msg}")
                        initialization_stats['skipped'].append(name)
                        model_bar.text = f"{status_symbols['skip']} {name} skipped"
                        progress_data['failed_models'] += 1
                        model_bar()
                        continue
                    
                    # Get the model class
                    model_class = definition['class_getter']()
                    
                    # Use model_instantiation_with_validation for comprehensive initialization
                    test_model, validation_results, performance_metrics, instantiation_details = model_instantiation_with_validation(
                        variant_class=model_class,
                        variant_name=name,
                        input_dim=test_input_dim,
                        base_config=definition['primary_config'],
                        fallback_config=definition['fallback_config'],
                        minimal_config=definition['minimal_config'],
                        validation_tests=['basic', 'forward_pass', 'parameters', 'config_methods', 'training_mode'],
                        comprehensive_validation=True,
                        hardware_data=hardware_data,
                        silent=silent,
                        logger=logger
                    )
                    
                    # Track instantiation method used
                    instantiation_method = instantiation_details.get('method', 'failed')
                    progress_data['instantiation_method'] = instantiation_method
                    
                    if instantiation_method == 'fallback':
                        initialization_stats['fallback_used'].append(name)
                    elif instantiation_method == 'minimal':
                        initialization_stats['minimal_used'].append(name)
                    elif instantiation_method == 'adaptive':
                        initialization_stats['adaptive_used'].append(name)
                    
                    # Process instantiation results
                    if test_model is not None:
                        try:
                            # Store detailed metrics
                            initialization_stats['detailed_metrics'][name] = {
                                'instantiation_method': instantiation_method,
                                'performance_metrics': performance_metrics,
                                'validation_results': validation_results,
                                'config_used': instantiation_details.get('config_used', {})
                            }
                            
                            # Record validation statistics from helper function
                            test_results = validation_results.get('test_results', {})
                            
                            # Test 1: Configuration validation (handled by helper)
                            if test_results.get('config_methods') == 'passed':
                                initialization_stats['config_validation_passed'] += 1
                            
                            # Test 2: Basic forward pass (handled by helper)
                            if test_results.get('forward_pass') == 'passed':
                                initialization_stats['architecture_tests_passed'] += 1
                            
                            # Test 3: Training mode compatibility (handled by helper)
                            if test_results.get('training_mode') == 'passed':
                                initialization_stats['performance_tests_passed'] += 1
                            
                            # Additional comprehensive validation
                            if hasattr(test_model, 'encode') and hasattr(test_model, 'decode'):
                                try:
                                    test_model.eval()
                                    test_input_encode = torch.randn(2, test_input_dim)
                                    if hasattr(test_model, 'device'):
                                        test_input_encode = test_input_encode.to(test_model.device)
                                    
                                    encoded = test_model.encode(test_input_encode)
                                    decoded = test_model.decode(encoded)
                                    
                                    if decoded.shape == test_input_encode.shape:
                                        initialization_stats['performance_tests_passed'] += 1
                                except Exception:
                                    pass  # Encode/decode is optional
                            
                            # Test parameter counting (already done by helper)
                            total_params = performance_metrics.get('total_parameters', 0)
                            if total_params > 0:
                                initialization_stats['performance_tests_passed'] += 1
                            
                            # Test device compatibility
                            device_str = str(test_model.device if hasattr(test_model, 'device') else 'cpu')
                            if 'cuda' in device_str and not torch.cuda.is_available():
                                if not silent:
                                    logger.warning(f"{name}: Model on CUDA device but CUDA not available")
                            
                            # Success - register the model variant
                            MODEL_VARIANTS[name] = model_class
                            initialization_stats['successful'].append(name)
                            initialization_stats['validation_passed'] += 1
                            progress_data['successful_models'] += 1
                            
                            method_symbol = method_symbols.get(instantiation_method, '[UNKNOWN]')
                            model_bar.text = f"{status_symbols['success']} {name} {method_symbol} ({total_params:,} params)"
                            
                            if not silent:
                                logger.info(f"{status_symbols['success']} {name}: Successfully initialized and validated "
                                          f"({instantiation_method}, {total_params:,} params, "
                                          f"score: {validation_results.get('overall_score', 0):.1%})")
                            
                        except Exception as validation_error:
                            error_msg = f"Post-instantiation validation failed: {str(validation_error)}"
                            if not silent:
                                logger.error(f"{status_symbols['failure']} {name}: {error_msg}")
                            initialization_stats['errors'].append(f"{name}: {error_msg}")
                            initialization_stats['failed'].append(name)
                            progress_data['failed_models'] += 1
                            model_bar.text = f"{status_symbols['failure']} {name} validation failed"
                    else:
                        # Instantiation failed
                        error_msg = f"Instantiation failed: {', '.join(validation_results.get('errors', []))}"
                        if not silent:
                            logger.error(f"{status_symbols['failure']} {name}: {error_msg}")
                        initialization_stats['errors'].append(f"{name}: {error_msg}")
                        initialization_stats['failed'].append(name)
                        progress_data['failed_models'] += 1
                        model_bar.text = f"{status_symbols['failure']} {name} instantiation failed"
                    
                    # Cleanup test model and free memory
                    if test_model is not None:
                        del test_model
                    
                    # MEMORY OPTIMIZATION CHECKPOINT - Enhanced cleanup between model tests
                    if _optimize_memory_if_needed(
                        condition=True,  # Always aggressive between models
                        hardware_data=hardware_data,
                        aggressive=True,
                        silent=silent
                    ):
                        initialization_stats['memory_optimizations_performed'] += 1
                    
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
                    
                except Exception as e:
                    if not silent:
                        logger.error(f"{status_symbols['failure']} Failed to initialize model variant {name}: {e}")
                    initialization_stats['errors'].append(f"{name}: Unexpected error: {str(e)}")
                    initialization_stats['failed'].append(name)
                    progress_data['failed_models'] += 1
                    model_bar.text = f"{status_symbols['failure']} {name} unexpected error"
                
                # Update progress bar
                model_bar()
            
            # Final update for model initialization
            successful_count = len(initialization_stats['successful'])
            failed_count = len(initialization_stats['failed'])
            model_bar.text = f"Models: {successful_count} passed, {failed_count} failed"
        
        # STAGE 6: Finalization
        progress_data['current_stage'] = "Finalizing"
        
        with alive_bar(1, title='Finalizing\t\t\t') as final_bar:
            
            final_bar.text = "Finalizing setup..."
            
            # MEMORY OPTIMIZATION CHECKPOINT - Clear memory before post-processing
            _optimize_memory_if_needed(
                condition=len(MODEL_VARIANTS) > 2,
                hardware_data=hardware_data,
                aggressive=len(MODEL_VARIANTS) > 2,
                silent=silent
            )
            
            # Log comprehensive initialization summary
            total_attempted = initialization_stats['attempted']
            successful_count = len(initialization_stats['successful'])
            failed_count = len(initialization_stats['failed'])
            fallback_count = len(initialization_stats['fallback_used'])
            minimal_count = len(initialization_stats['minimal_used'])
            adaptive_count = len(initialization_stats['adaptive_used'])
            memory_opts = initialization_stats['memory_optimizations_performed']
            
            if not silent:
                logger.info("Model variants initialization completed using enhanced helper functions:")
                logger.info(f"  - Total attempted: {total_attempted}")
                logger.info(f"  - Successful: {successful_count}")
                logger.info(f"  - Failed: {failed_count}")
                logger.info(f"  - Using primary config: {successful_count - fallback_count - minimal_count - adaptive_count}")
                logger.info(f"  - Using fallback config: {fallback_count}")
                logger.info(f"  - Using minimal config: {minimal_count}")
                logger.info(f"  - Using adaptive config: {adaptive_count}")
                logger.info(f"  - Configuration validation passed: {initialization_stats['config_validation_passed']}")
                logger.info(f"  - Architecture tests passed: {initialization_stats['architecture_tests_passed']}")
                logger.info(f"  - Performance tests passed: {initialization_stats['performance_tests_passed']}")
                logger.info(f"  - Memory optimizations performed: {memory_opts}")
                
                # Log detailed metrics for successful models
                if initialization_stats['detailed_metrics']:
                    logger.info("  - Detailed metrics available for successful models")
                    for model_name, metrics in initialization_stats['detailed_metrics'].items():
                        score = metrics['validation_results'].get('overall_score', 0)
                        params = metrics['performance_metrics'].get('total_parameters', 0)
                        logger.debug(f"    {model_name}: {score:.1%} score, {params:,} params, {metrics['instantiation_method']} config")
                
                if initialization_stats['successful']:
                    logger.info(f"  - Available models: {', '.join(initialization_stats['successful'])}")
                
                if initialization_stats['failed']:
                    logger.warning(f"  - Failed models: {', '.join(initialization_stats['failed'])}")
                
                # Log critical errors for debugging
                if initialization_stats['errors']:
                    logger.warning("Initialization errors encountered (showing first 5):")
                    for error in initialization_stats['errors'][:5]:
                        logger.warning(f"  - {error}")
                    if len(initialization_stats['errors']) > 5:
                        logger.warning(f"  ... and {len(initialization_stats['errors']) - 5} more errors")
            
            # Ensure at least one model variant is available
            if not MODEL_VARIANTS:
                error_msg = "No model variants could be initialized"
                if not silent:
                    logger.error(error_msg)
                    logger.error("This indicates a serious configuration or dependency issue")
                
                final_bar.text = "No models - emergency fallback"
                
                # Emergency fallback using the helper function
                try:
                    if SimpleAutoencoder is not None:
                        emergency_model, emergency_results, emergency_metrics, emergency_details = model_instantiation_with_validation(
                            variant_class=SimpleAutoencoder,
                            variant_name='SimpleAutoencoder',
                            input_dim=10,
                            base_config=_create_adaptive_config(
                                model_name='SimpleAutoencoder',
                                model_class=SimpleAutoencoder,
                                system_class='emergency',
                                input_dim=10,
                                encoding_dim=4,
                                hidden_dims=[16],
                                dropout_rates=[0.1],
                                activation='relu',
                                activation_param=0.0,
                                normalization=None,
                                use_batch_norm=False,
                                use_layer_norm=False,
                                skip_connection=False,
                                residual_blocks=False,
                                use_attention=False,
                                legacy_mode=True,
                                num_models=1,
                                diversity_factor=0.0,
                                learning_rate=0.001,
                                batch_size=32,
                                mixed_precision=False,
                                optimizer_type='Adam',
                                device_setting='cpu',
                                random_seed=42,
                                hardware_data=hardware_data
                            ),
                            validation_tests=['basic', 'forward_pass'],
                            comprehensive_validation=False,
                            hardware_data=hardware_data,
                            silent=silent,
                            logger=logger
                        )
                        
                        if emergency_model is not None:
                            MODEL_VARIANTS['SimpleAutoencoder'] = SimpleAutoencoder
                            initialization_stats['memory_optimizations_performed'] += 1
                            initialization_stats['successful'].append('SimpleAutoencoder')
                            progress_data['successful_models'] += 1
                            final_bar.text = "Emergency fallback activated"
                            if not silent:
                                logger.warning("Emergency fallback: Ultra-minimal SimpleAutoencoder available")
                        else:
                            raise RuntimeError("Emergency fallback instantiation failed")
                    else:
                        raise RuntimeError(error_msg)
                        
                except Exception as e:
                    if not silent:
                        logger.critical(f"Emergency fallback failed: {e}")
                    raise RuntimeError(f"{error_msg}: {str(e)}")
            
            # Run post-initialization validation if available
            try:
                if not silent:
                    final_bar.text = "Running final validation"
                
                # Use the enhanced validation function if available
                if 'validate_model_variants' in globals():
                    variant_validation_results = validate_model_variants(logger, silent=silent)
                    
                    fully_validated = [
                        name for name, status in variant_validation_results.items() 
                        if status == 'available'
                    ]
                    
                    if fully_validated:
                        if not silent:
                            logger.info(f"[SUCCESS] Fully validated model variants: {', '.join(fully_validated)}")
                    else:
                        if not silent:
                            logger.warning("[WARN] No model variants passed comprehensive post-validation")
                    final_bar.text = "Final validation complete"
                else:
                    if not silent:
                        logger.debug("Post-initialization validation function not available")
                    
            except Exception as validation_error:
                if not silent:
                    logger.error(f"Post-initialization validation failed: {validation_error}")
            
            # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
            _optimize_memory_if_needed(
                condition=True,  # Always aggressive final cleanup
                hardware_data=hardware_data,
                aggressive=True,
                silent=silent
            )
            
            final_bar.text = "Initialization complete!"
            final_bar()
    
    except Exception as e:
        # If we were using progress bars, they're already closed by the context managers
        raise e
    
    if not silent:
        logger.info(f"Model variants initialization completed successfully with "
                   f"{len(MODEL_VARIANTS)} available variants")
        logger.info(f"Memory optimizations: {initialization_stats['memory_optimizations_performed']} operations performed")
        
        # Log final summary of available capabilities
        capabilities_summary = []
        for name in MODEL_VARIANTS.keys():
            if name == 'SimpleAutoencoder':
                capabilities_summary.append("Simple: Basic encoder-decoder")
            elif name == 'EnhancedAutoencoder':
                capabilities_summary.append("Enhanced: Advanced features")
            elif name == 'AutoencoderEnsemble':
                capabilities_summary.append("Ensemble: Multiple models")
        
        if capabilities_summary:
            logger.info(f"Available capabilities: {', '.join(capabilities_summary)}")

def validate_model_variants(logger: logging.Logger, silent: bool = False) -> Dict[str, str]:
    """
    Comprehensive validation of all registered model variants with enhanced testing.
    
    This function has been updated to fully leverage the existing helper functions while
    preserving ALL original validation capabilities including advanced scenario testing,
    memory analysis, model-specific feature validation, and comprehensive diagnostics.
    
    Args:
        logger: Logger instance for reporting validation results
        silent: If True, suppress detailed logging messages and progress bars during validation
        
    Returns:
        Dictionary mapping model names to their validation status
    """
    variant_status = {}
    validation_start_time = time.time()
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'models_tested': 0,
        'models_passed': 0,
        'models_failed': 0,
        'current_model': None,
        'current_test': None
    }
    
    # Phase 1: Check if MODEL_VARIANTS exists and initialize if needed
    progress_data['current_stage'] = "Initial Setup"
    
    with alive_bar(1, title='Validation Setup\t\t') as setup_bar:
        
        setup_bar.text = "Checking model variants..."
        
        if not MODEL_VARIANTS:
            if not silent:
                logger.warning("MODEL_VARIANTS is empty, attempting initialization")
            try:
                initialize_model_variants(silent=silent)
                setup_bar.text = "Model variants initialized"
            except Exception as e:
                error_msg = f'initialization_failed: {str(e)}'
                if not silent:
                    logger.error(f"Failed to initialize model variants for validation: {e}")
                setup_bar.text = "Initialization failed"
                return {'error': error_msg}
        
        if not MODEL_VARIANTS:
            error_msg = 'no_models_available'
            if not silent:
                logger.error("No model variants available after initialization attempt")
            setup_bar.text = "No models available"
            return {'error': error_msg}
        
        setup_bar.text = f"Found {len(MODEL_VARIANTS)} model variants"
        setup_bar()
    
    # Phase 2: Get hardware context for memory optimization
    progress_data['current_stage'] = "Hardware Check"
    
    with alive_bar(1, title='Hardware Check\t\t\t') as hardware_bar:
        
        hardware_bar.text = "Checking hardware..."
        
        try:
            hardware_data = check_hardware(include_memory_usage=True)
            total_ram_gb = hardware_data.get('system_ram', {}).get('ram_total_gb', 8.0)
            
            # Initial memory cleanup for memory-constrained systems
            _optimize_memory_if_needed(
                condition=total_ram_gb < 8,
                hardware_data=hardware_data,
                aggressive=total_ram_gb < 4,
                silent=silent
            )
            hardware_bar.text = f"Hardware check complete ({total_ram_gb}GB RAM)"
        except Exception as e:
            if not silent:
                logger.debug(f"Hardware detection failed: {e}")
            hardware_data = {}
            total_ram_gb = 8.0
            hardware_bar.text = "Hardware check (using defaults)"
        
        hardware_bar()
    
    # Phase 3: Load configuration
    progress_data['current_stage'] = "Configuration Loading"
    
    with alive_bar(1, title='Loading Configuration\t\t') as config_bar:
        
        config_bar.text = "Loading configuration..."
        
        try:
            current_config = get_current_config()
            if not isinstance(current_config, dict):
                current_config = {}
            
            model_config = current_config.get('model', {})
            data_config = current_config.get('data', {})
            training_config = current_config.get('training', {})
            hardware_config = current_config.get('hardware', {})
            system_config = current_config.get('system', {})
            
            if not silent:
                logger.debug("Loaded comprehensive configuration for validation testing")
            config_bar.text = "Configuration loaded"
                
        except Exception as e:
            if not silent:
                logger.warning(f"Could not load configuration for validation, using defaults: {e}")
            current_config = {}
            model_config = {}
            data_config = {}
            training_config = {}
            hardware_config = {}
            system_config = {}
            config_bar.text = "Configuration (using defaults)"
        
        config_bar()
    
    # Phase 4: Extract configuration parameters using helper functions
    progress_data['current_stage'] = "Parameter Extraction"
    
    with alive_bar(1, title='Extracting Parameters\t\t') as param_bar:
        
        param_bar.text = "Extracting parameters..."
        
        test_input_dim = _extract_and_validate_config_param(
            model_config, 'input_dim', 20, 'FEATURES',
            lambda x: isinstance(x, int) and x > 0,
            "input feature dimension", silent
        )
        
        base_encoding_dim = _extract_and_validate_config_param(
            model_config, 'encoding_dim', 16, 'DEFAULT_ENCODING_DIM',
            lambda x: isinstance(x, int) and x > 0,
            "latent encoding dimension", silent
        )
        
        base_hidden_dims = _extract_and_validate_config_param(
            model_config, 'hidden_dims', [128, 64], 'HIDDEN_LAYER_SIZES',
            lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(d, int) and d > 0 for d in x),
            "hidden layer dimensions", silent
        )
        
        base_dropout_rates = _extract_and_validate_config_param(
            model_config, 'dropout_rates', [0.2, 0.15], 'DROPOUT_RATES',
            lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(r, (int, float)) and 0 <= r < 1 for r in x),
            "dropout rates", silent
        )
        
        # Additional parameters for comprehensive validation
        activation = _extract_and_validate_config_param(
            model_config, 'activation', 'leaky_relu', 'ACTIVATION',
            lambda x: x in ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu', 'prelu'],
            "activation function", silent
        )
        
        activation_param = _extract_and_validate_config_param(
            model_config, 'activation_param', 0.2, 'ACTIVATION_PARAM',
            lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
            "activation parameter", silent
        )
        
        normalization = _extract_and_validate_config_param(
            model_config, 'normalization', 'batch', 'NORMALIZATION',
            lambda x: x in ['batch', 'layer', 'instance', 'group', 'none', None],
            "normalization type", silent
        )
        
        use_batch_norm = _extract_and_validate_config_param(
            model_config, 'use_batch_norm', True, 'USE_BATCH_NORM',
            lambda x: isinstance(x, bool),
            "batch normalization flag", silent
        )
        
        use_layer_norm = _extract_and_validate_config_param(
            model_config, 'use_layer_norm', False, 'USE_LAYER_NORM',
            lambda x: isinstance(x, bool),
            "layer normalization flag", silent
        )
        
        skip_connection = _extract_and_validate_config_param(
            model_config, 'skip_connection', True, 'SKIP_CONNECTION',
            lambda x: isinstance(x, bool),
            "skip connections flag", silent
        )
        
        residual_blocks = _extract_and_validate_config_param(
            model_config, 'residual_blocks', False, 'RESIDUAL_BLOCKS',
            lambda x: isinstance(x, bool),
            "residual blocks flag", silent
        )
        
        use_attention = _extract_and_validate_config_param(
            model_config, 'use_attention', False, 'USE_ATTENTION',
            lambda x: isinstance(x, bool),
            "attention mechanism flag", silent
        )
        
        num_models = _extract_and_validate_config_param(
            model_config, 'num_models', 3, 'NUM_MODELS',
            lambda x: isinstance(x, int) and 1 <= x <= 10,
            "ensemble size", silent
        )
        
        diversity_factor = _extract_and_validate_config_param(
            model_config, 'diversity_factor', 0.2, 'DIVERSITY_FACTOR',
            lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
            "ensemble diversity factor", silent
        )
        
        mixed_precision = _extract_and_validate_config_param(
            training_config, 'mixed_precision', False, 'MIXED_PRECISION',
            lambda x: isinstance(x, bool),
            "mixed precision training", silent
        )
        
        learning_rate = _extract_and_validate_config_param(
            training_config, 'learning_rate', 0.001, 'LEARNING_RATE',
            lambda x: isinstance(x, (int, float)) and x > 0,
            "learning rate", silent
        )
        
        device_setting = _extract_and_validate_config_param(
            hardware_config, 'device', 'auto', 'DEVICE',
            lambda x: isinstance(x, str) and x in ['auto', 'cpu', 'cuda'] or x.startswith('cuda:'),
            "compute device", silent
        )
        
        random_seed = _extract_and_validate_config_param(
            system_config, 'random_seed', 42, 'RANDOM_SEED',
            lambda x: isinstance(x, int),
            "random seed", silent
        )
        
        legacy_mode = _extract_and_validate_config_param(
            model_config, 'legacy_mode', False, 'LEGACY_MODE',
            lambda x: isinstance(x, bool),
            "legacy compatibility mode", silent
        )
        
        # Phase 5: Validate and adjust parameters
        base_hidden_dims, base_dropout_rates = _validate_and_adjust_parameters(
            base_hidden_dims, base_dropout_rates, silent
        )
        
        param_bar.text = "Parameters validated"
        param_bar()
    
    # Phase 6: Create model test definitions
    progress_data['current_stage'] = "Creating Test Definitions"
    
    with alive_bar(1, title='Creating Test Definitions\t') as def_bar:
        
        def_bar.text = "Creating model definitions..."
        
        model_definitions = _create_model_test_definition(
            encoding_dim=base_encoding_dim,
            hidden_dims=base_hidden_dims,
            dropout_rates=base_dropout_rates,
            use_attention=use_attention,
            residual_blocks=residual_blocks,
            skip_connection=skip_connection,
            legacy_mode=legacy_mode,
            num_models=num_models,
            diversity_factor=diversity_factor,
            mixed_precision=mixed_precision
        )
        
        # Update configurations with validation-specific input dimension
        for definition in model_definitions.values():
            for config_type in ['primary_config', 'fallback_config', 'minimal_config']:
                if config_type in definition:
                    definition[config_type]['data']['features'] = test_input_dim
                    definition[config_type]['model']['input_dim'] = test_input_dim
        
        def_bar.text = f"Created {len(model_definitions)} test definitions"
        def_bar()
    
    # Define comprehensive validation test scenarios
    validation_scenarios = [
        {
            'name': 'batch_norm_compatible',
            'batch_size': 4,
            'description': 'Batch normalization compatibility test'
        },
        {
            'name': 'single_sample',
            'batch_size': 1,
            'description': 'Single sample inference test'
        },
        {
            'name': 'small_batch',
            'batch_size': 2,
            'description': 'Small batch processing test'
        },
        {
            'name': 'medium_batch',
            'batch_size': 8,
            'description': 'Medium batch processing test'
        },
        {
            'name': 'large_batch',
            'batch_size': 16,
            'description': 'Large batch processing test'
        }
    ]
    
    # Phase 7: Initialize comprehensive validation statistics
    validation_stats = {
        'models_attempted': 0,
        'models_successful': 0,
        'models_failed': 0,
        'models_warning': 0,
        'total_tests_performed': 0,
        'total_tests_passed': 0,
        'total_tests_failed': 0,
        'configuration_tests_passed': 0,
        'architecture_tests_passed': 0,
        'functionality_tests_passed': 0,
        'performance_tests_passed': 0,
        'robustness_tests_passed': 0,
        'memory_tests_passed': 0,
        'device_compatibility_tests': 0,
        'memory_optimizations_performed': 0,
        'detailed_metrics': {},
        'available_variants': [],
        'warning_variants': [],
        'failed_variants': []
    }
    
    # Status symbols for visual feedback
    status_symbols = {
        'success': '[OK]',
        'failure': '[FAIL]',
        'warning': '[WARN]',
        'skip': '[SKIP]'
    }
    
    # Phase 8: Validate each model variant
    progress_data['current_stage'] = "Model Validation"
    total_models = len(MODEL_VARIANTS)
    
    with alive_bar(total_models, title='Validating Models\t') as model_bar:
        
        for variant_name, variant_class in MODEL_VARIANTS.items():
            model_validation_start = time.time()
            validation_stats['models_attempted'] += 1
            progress_data['current_model'] = variant_name
            
            # Update progress bar with current model info
            passed_count = validation_stats['models_successful']
            failed_count = validation_stats['models_failed']
            model_bar.text = f"Testing {variant_name}... ({passed_count} passed, {failed_count} failed)"
            
            # Initialize detailed tracking for this variant
            overall_status = 'available'
            variant_details = {
                'class_name': variant_class.__name__ if variant_class else 'None',
                'tests_performed': [],
                'tests_passed': [],
                'tests_failed': [],
                'errors': [],
                'warnings': [],
                'performance_metrics': {},
                'compatibility_results': {},
                'configuration_validation': {},
                'memory_usage': {}
            }
            
            try:
                if not silent:
                    logger.debug(f"Starting validation for {variant_name} using helper functions")
                
                # Test 1: Class Availability and Callability
                if variant_class is None:
                    variant_status[variant_name] = 'class_not_found'
                    variant_details['errors'].append('Model class is None')
                    validation_stats['failed_variants'].append(variant_name)
                    validation_stats['models_failed'] += 1
                    model_bar.text = f"{status_symbols['failure']} {variant_name} class not found"
                    model_bar()
                    continue
                
                if not callable(variant_class):
                    variant_status[variant_name] = 'class_not_callable'
                    variant_details['errors'].append('Model class is not callable')
                    validation_stats['failed_variants'].append(variant_name)
                    validation_stats['models_failed'] += 1
                    model_bar.text = f"{status_symbols['failure']} {variant_name} not callable"
                    model_bar()
                    continue
                
                variant_details['tests_performed'].append('class_availability')
                variant_details['tests_passed'].append('class_availability')
                validation_stats['total_tests_performed'] += 1
                validation_stats['total_tests_passed'] += 1
                
                # Get the appropriate test definition for this model
                if variant_name in model_definitions:
                    definition = model_definitions[variant_name]
                else:
                    # Create adaptive definition for unknown model types
                    if not silent:
                        logger.debug(f"Creating adaptive configuration for unknown model type: {variant_name}")
                    definition = {
                        'primary_config': _create_adaptive_config(
                            model_name=variant_name,
                            model_class=variant_class,
                            system_class='validation',
                            input_dim=test_input_dim,
                            encoding_dim=base_encoding_dim,
                            hidden_dims=base_hidden_dims,
                            dropout_rates=base_dropout_rates,
                            activation=activation,
                            activation_param=activation_param,
                            normalization=normalization,
                            use_batch_norm=use_batch_norm,
                            use_layer_norm=use_layer_norm,
                            skip_connection=skip_connection,
                            residual_blocks=residual_blocks,
                            use_attention=use_attention,
                            legacy_mode=legacy_mode,
                            num_models=num_models,
                            diversity_factor=diversity_factor,
                            learning_rate=learning_rate,
                            batch_size=32,
                            mixed_precision=mixed_precision,
                            optimizer_type='AdamW',
                            device_setting=device_setting,
                            random_seed=random_seed,
                            hardware_data=hardware_data
                        ),
                        'fallback_config': _create_adaptive_config(
                            model_name=variant_name,
                            model_class=variant_class,
                            system_class='fallback',
                            input_dim=test_input_dim,
                            encoding_dim=max(4, base_encoding_dim // 2),
                            hidden_dims=[64],
                            dropout_rates=[0.2],
                            activation='relu',
                            activation_param=0.0,
                            normalization=None,
                            use_batch_norm=False,
                            use_layer_norm=False,
                            skip_connection=False,
                            residual_blocks=False,
                            use_attention=False,
                            legacy_mode=True,
                            num_models=1,
                            diversity_factor=0.0,
                            learning_rate=0.001,
                            batch_size=32,
                            mixed_precision=False,
                            optimizer_type='Adam',
                            device_setting='cpu',
                            random_seed=42,
                            hardware_data=hardware_data
                        ),
                        'minimal_config': _create_adaptive_config(
                            model_name=variant_name,
                            model_class=variant_class,
                            system_class='minimal',
                            input_dim=test_input_dim,
                            encoding_dim=8,
                            hidden_dims=[32],
                            dropout_rates=[0.1],
                            activation='relu',
                            activation_param=0.0,
                            normalization=None,
                            use_batch_norm=False,
                            use_layer_norm=False,
                            skip_connection=False,
                            residual_blocks=False,
                            use_attention=False,
                            legacy_mode=True,
                            num_models=1,
                            diversity_factor=0.0,
                            learning_rate=0.001,
                            batch_size=32,
                            mixed_precision=False,
                            optimizer_type='Adam',
                            device_setting='cpu',
                            random_seed=42,
                            hardware_data=hardware_data
                        )
                    }
                
                # Test 2: Configuration Creation
                variant_details['configuration_validation']['config_created'] = True
                variant_details['tests_performed'].append('configuration_creation')
                variant_details['tests_passed'].append('configuration_creation')
                validation_stats['total_tests_performed'] += 1
                validation_stats['total_tests_passed'] += 1
                validation_stats['configuration_tests_passed'] += 1
                
                # Use the comprehensive model_instantiation_with_validation helper function
                test_instance, validation_results, performance_metrics, instantiation_details = model_instantiation_with_validation(
                    variant_class=variant_class,
                    variant_name=variant_name,
                    input_dim=test_input_dim,
                    base_config=definition['primary_config'],
                    fallback_config=definition.get('fallback_config'),
                    minimal_config=definition.get('minimal_config'),
                    validation_tests=['basic', 'forward_pass', 'parameters', 'config_methods', 'training_mode'],
                    comprehensive_validation=True,
                    hardware_data=hardware_data,
                    silent=silent,
                    logger=logger
                )
                
                # Process results
                instantiation_method = instantiation_details.get('method', 'failed')
                validation_score = validation_results.get('overall_score', 0)
                test_results = validation_results.get('test_results', {})
                warnings = validation_results.get('warnings', [])
                errors = validation_results.get('errors', [])
                
                # Track instantiation method
                if instantiation_method == 'fallback':
                    variant_details['warnings'].append('Used fallback configuration')
                elif instantiation_method == 'minimal':
                    variant_details['warnings'].append('Used minimal configuration')
                elif instantiation_method == 'adaptive':
                    variant_details['warnings'].append('Used adaptive configuration')
                
                # Add all results
                variant_details['warnings'].extend(warnings)
                variant_details['errors'].extend(errors)
                variant_details['performance_metrics'].update(performance_metrics)
                variant_details['configuration_validation'].update(performance_metrics.get('config_validation', {}))
                variant_details['instantiation_method'] = instantiation_method
                
                # Track models for cleanup
                test_models = []
                if test_instance is not None:
                    test_models.append(test_instance)
                    
                    # Record instantiation success
                    variant_details['tests_performed'].append(f'instantiation_{instantiation_method}')
                    variant_details['tests_passed'].append(f'instantiation_{instantiation_method}')
                    validation_stats['total_tests_performed'] += 1
                    validation_stats['total_tests_passed'] += 1
                    
                    # Process comprehensive validation results
                    for test_name, test_result in test_results.items():
                        variant_details['tests_performed'].append(test_name)
                        if test_result == 'passed':
                            variant_details['tests_passed'].append(test_name)
                            validation_stats['total_tests_passed'] += 1
                        else:
                            variant_details['tests_failed'].append(test_name)
                            validation_stats['total_tests_failed'] += 1
                        validation_stats['total_tests_performed'] += 1
                    
                    # Update validation statistics based on helper results
                    if test_results.get('basic') == 'passed':
                        validation_stats['architecture_tests_passed'] += 1
                    if test_results.get('forward_pass') == 'passed':
                        validation_stats['functionality_tests_passed'] += 1
                    if test_results.get('parameters') == 'passed':
                        validation_stats['performance_tests_passed'] += 1
                    if test_results.get('config_methods') == 'passed':
                        validation_stats['configuration_tests_passed'] += 1
                    if test_results.get('comprehensive') == 'passed':
                        validation_stats['robustness_tests_passed'] += 1
                    
                    if not silent:
                        logger.debug(f"{status_symbols['success']} {variant_name}: Comprehensive validation completed with {instantiation_method} configuration")
                        
                else:
                    # Instantiation failed
                    variant_details['tests_performed'].append('instantiation')
                    variant_details['tests_failed'].append('instantiation')
                    validation_stats['total_tests_performed'] += 1
                    validation_stats['total_tests_failed'] += 1
                    validation_stats['models_failed'] += 1
                    
                    if not silent:
                        logger.error(f"{status_symbols['failure']} {variant_name}: Instantiation failed")
                    model_bar.text = f"{status_symbols['failure']} {variant_name} instantiation failed"
                    model_bar()
                    continue
                
                # Memory optimization after model instantiation
                _optimize_memory_if_needed(
                    condition=True,
                    hardware_data=hardware_data,
                    aggressive=total_ram_gb < 4,
                    silent=silent
                )
                
                # Additional specialized tests beyond the helper function
                if test_instance is not None:
                    # Test: Advanced Forward Pass Scenarios
                    try:
                        scenario_results = {}
                        for scenario in validation_scenarios:
                            try:
                                test_instance.eval()
                                batch_size = scenario['batch_size']
                                
                                # Adjust for batch norm requirements
                                test_config = definition['primary_config']
                                if ((test_config['model'].get('use_batch_norm', False) or 
                                     test_config['model'].get('normalization') == 'batch') and 
                                     batch_size == 1):
                                    batch_size = 2
                                
                                test_input = torch.randn(batch_size, test_input_dim)
                                if hasattr(test_instance, 'device'):
                                    test_input = test_input.to(test_instance.device)
                                
                                with torch.no_grad():
                                    output = test_instance(test_input)
                                
                                expected_shape = (batch_size, test_input_dim)
                                if output.shape == expected_shape:
                                    scenario_results[scenario['name']] = 'passed'
                                else:
                                    scenario_results[scenario['name']] = 'failed'
                                    variant_details['warnings'].append(f"Shape mismatch in {scenario['name']}")
                            except Exception as scenario_error:
                                scenario_results[scenario['name']] = 'failed'
                                variant_details['warnings'].append(f"{scenario['name']} failed: {str(scenario_error)}")
                        
                        # Record scenario results
                        passed_scenarios = sum(1 for result in scenario_results.values() if result == 'passed')
                        total_scenarios = len(scenario_results)
                        
                        variant_details['performance_metrics']['scenario_testing'] = scenario_results
                        variant_details['tests_performed'].append('advanced_scenarios')
                        
                        if passed_scenarios == total_scenarios:
                            variant_details['tests_passed'].append('advanced_scenarios')
                            validation_stats['total_tests_passed'] += 1
                        else:
                            variant_details['tests_failed'].append('advanced_scenarios')
                            validation_stats['total_tests_failed'] += 1
                        validation_stats['total_tests_performed'] += 1
                        
                    except Exception as scenario_test_error:
                        variant_details['warnings'].append(f"Advanced scenario testing failed: {str(scenario_test_error)}")
                    
                    # Test: Memory Usage Analysis
                    try:
                        # Use psutil if available for memory monitoring
                        try:
                            import psutil
                            process = psutil.Process()
                            memory_before = process.memory_info().rss
                            
                            memory_test_input = torch.randn(32, test_input_dim)
                            if hasattr(test_instance, 'device'):
                                memory_test_input = memory_test_input.to(test_instance.device)
                            
                            test_instance.eval()
                            with torch.no_grad():
                                _ = test_instance(memory_test_input)
                            
                            memory_after = process.memory_info().rss
                            memory_used_mb = (memory_after - memory_before) / (1024 * 1024)
                            
                            variant_details['memory_usage']['inference_memory_mb'] = memory_used_mb
                            
                            if memory_used_mb > 1000:
                                variant_details['warnings'].append(f'High memory usage: {memory_used_mb:.1f} MB')
                            
                            variant_details['tests_performed'].append('memory_analysis')
                            variant_details['tests_passed'].append('memory_analysis')
                            validation_stats['total_tests_performed'] += 1
                            validation_stats['total_tests_passed'] += 1
                            validation_stats['memory_tests_passed'] += 1
                            
                        except ImportError:
                            variant_details['memory_usage']['psutil_available'] = False
                            variant_details['warnings'].append('psutil not available for detailed memory analysis')
                        
                    except Exception as memory_error:
                        variant_details['warnings'].append(f'Memory analysis failed: {str(memory_error)}')
                    
                    # Test: Model-Specific Feature Validation
                    if variant_name == 'EnhancedAutoencoder' and test_instance is not None:
                        try:
                            enhanced_features = []
                            if hasattr(test_instance, 'attention') and test_instance.attention is not None:
                                enhanced_features.append('attention')
                            if hasattr(test_instance, 'skip_layers'):
                                enhanced_features.append('skip_connections')
                            test_config = definition.get('primary_config', {})
                            if test_config.get('model', {}).get('residual_blocks', False):
                                enhanced_features.append('residual_blocks')
                            
                            variant_details['compatibility_results']['enhanced_features'] = enhanced_features
                            variant_details['tests_performed'].append('enhanced_features')
                            variant_details['tests_passed'].append('enhanced_features')
                            validation_stats['total_tests_performed'] += 1
                            validation_stats['total_tests_passed'] += 1
                            
                        except Exception as enhanced_error:
                            variant_details['warnings'].append(f'Enhanced features validation failed: {str(enhanced_error)}')
                    
                    elif variant_name == 'AutoencoderEnsemble' and test_instance is not None:
                        try:
                            if hasattr(test_instance, 'models'):
                                ensemble_size = len(test_instance.models)
                                variant_details['compatibility_results']['ensemble_size'] = ensemble_size
                                
                                test_config = definition.get('primary_config', {})
                                expected_models = test_config.get('model', {}).get('num_models', 3)
                                
                                if ensemble_size == 0:
                                    variant_details['errors'].append('Empty ensemble')
                                elif ensemble_size < expected_models:
                                    variant_details['warnings'].append(f'Partial ensemble: {ensemble_size} models')
                                
                                variant_details['tests_performed'].append('ensemble_features')
                                variant_details['tests_passed'].append('ensemble_features')
                                validation_stats['total_tests_performed'] += 1
                                validation_stats['total_tests_passed'] += 1
                            
                        except Exception as ensemble_error:
                            variant_details['warnings'].append(f'Ensemble features validation failed: {str(ensemble_error)}')
                
                # Calculate model validation time
                model_validation_time = time.time() - model_validation_start
                variant_details['performance_metrics']['validation_time_seconds'] = model_validation_time
                
                # Determine final status based on results
                error_count = len(variant_details['errors'])
                warning_count = len(variant_details['warnings'])
                tests_passed = len(variant_details['tests_passed'])
                tests_total = len(variant_details['tests_performed'])
                
                if error_count > 0:
                    overall_status = 'error'
                    validation_stats['models_failed'] += 1
                    validation_stats['failed_variants'].append(variant_name)
                    model_bar.text = f"{status_symbols['failure']} {variant_name} ({error_count} errors)"
                elif warning_count > 0 or validation_score < 0.8:
                    overall_status = 'warning'
                    validation_stats['models_warning'] += 1
                    validation_stats['warning_variants'].append(variant_name)
                    model_bar.text = f"{status_symbols['warning']} {variant_name} ({warning_count} warnings)"
                else:
                    overall_status = 'available'
                    validation_stats['models_successful'] += 1
                    validation_stats['available_variants'].append(variant_name)
                    model_bar.text = f"{status_symbols['success']} {variant_name} ({tests_passed}/{tests_total} tests)"
                
                # Create detailed status message
                status_msg = overall_status
                if overall_status != 'available':
                    details = []
                    if error_count > 0:
                        details.append(f"{error_count} errors")
                    if warning_count > 0:
                        details.append(f"{warning_count} warnings")
                    if details:
                        status_msg += f": {', '.join(details)}"
                    status_msg += f" ({validation_score:.1%} score)"
                
                variant_status[variant_name] = status_msg
                
                # Store detailed metrics for reporting
                validation_stats['detailed_metrics'][variant_name] = {
                    'instantiation_method': instantiation_method,
                    'validation_score': validation_score,
                    'tests_passed': tests_passed,
                    'tests_total': tests_total,
                    'performance_metrics': variant_details['performance_metrics'],
                    'validation_time_seconds': model_validation_time
                }
                
                # Cleanup test model and apply memory optimization
                if test_instance is not None:
                    del test_instance
                    
                # Memory optimization after each model test
                if _optimize_memory_if_needed(
                    condition=True,
                    hardware_data=hardware_data,
                    aggressive=total_ram_gb < 4,
                    silent=silent
                ):
                    validation_stats['memory_optimizations_performed'] += 1
                
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
                if not silent:
                    logger.info(f"Model {variant_name} validation completed: {status_msg} "
                               f"({tests_passed}/{tests_total} tests passed, "
                               f"{instantiation_method} config, "
                               f"{model_validation_time:.2f}s)")
                
            except Exception as e:
                error_msg = f'validation_error: {str(e)}'
                variant_status[variant_name] = error_msg
                validation_stats['failed_variants'].append(variant_name)
                validation_stats['models_failed'] += 1
                model_bar.text = f"{status_symbols['failure']} {variant_name} unexpected error"
                
                if not silent:
                    logger.error(f"Model variant {variant_name} validation failed with unexpected error: {e}")
            
            finally:
                # Store detailed results
                validation_stats[variant_name] = variant_details
                # Additional cleanup
                try:
                    if 'test_models' in locals():
                        for model in test_models:
                            del model
                    if 'test_instance' in locals():
                        del test_instance
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
                except Exception as cleanup_error:
                    if not silent:
                        logger.debug(f"Cleanup warning for {variant_name}: {cleanup_error}")
            
            # Update progress bar
            model_bar()
        
        # Final update for model validation
        passed_count = validation_stats['models_successful']
        failed_count = validation_stats['models_failed']
        model_bar.text = f"Validation complete: {passed_count} passed, {failed_count} failed"
    
    # Phase 9: Finalization
    progress_data['current_stage'] = "Finalizing"
    
    with alive_bar(1, title='Finalizing\t\t') as final_bar:
        
        final_bar.text = "Generating summary..."
        
        total_validation_time = time.time() - validation_start_time
        
        # Final comprehensive memory cleanup
        _optimize_memory_if_needed(
            condition=True,
            hardware_data=hardware_data,
            aggressive=True,
            silent=silent
        )
        
        # Phase 10: Log comprehensive validation summary
        if not silent:
            logger.info("="*70)
            logger.info("MODEL VARIANTS COMPREHENSIVE VALIDATION SUMMARY")
            logger.info("="*70)
            logger.info(f"Total Validation Time: {total_validation_time:.2f} seconds")
            logger.info(f"Models Attempted: {validation_stats['models_attempted']}")
            logger.info(f"Models Available: {validation_stats['models_successful']}")
            logger.info(f"Models with Warnings: {validation_stats['models_warning']}")
            logger.info(f"Models Failed: {validation_stats['models_failed']}")
            logger.info("-"*70)
            logger.info(f"Total Tests Performed: {validation_stats['total_tests_performed']}")
            logger.info(f"Total Tests Passed: {validation_stats['total_tests_passed']}")
            logger.info(f"Total Tests Failed: {validation_stats['total_tests_failed']}")
            success_rate = validation_stats['total_tests_passed']/max(1, validation_stats['total_tests_performed'])*100
            logger.info(f"Test Success Rate: {success_rate:.1f}%")
            logger.info("-"*70)
            logger.info(f"Configuration Tests Passed: {validation_stats['configuration_tests_passed']}")
            logger.info(f"Architecture Tests Passed: {validation_stats['architecture_tests_passed']}")
            logger.info(f"Functionality Tests Passed: {validation_stats['functionality_tests_passed']}")
            logger.info(f"Performance Tests Passed: {validation_stats['performance_tests_passed']}")
            logger.info(f"Robustness Tests Passed: {validation_stats['robustness_tests_passed']}")
            logger.info(f"Memory Tests Passed: {validation_stats['memory_tests_passed']}")
            logger.info(f"Device Compatibility Tests: {validation_stats['device_compatibility_tests']}")
            logger.info(f"Memory Optimizations Performed: {validation_stats['memory_optimizations_performed']}")
            
            # Log detailed metrics
            if validation_stats['detailed_metrics']:
                logger.info("-"*70)
                logger.info("DETAILED METRICS:")
                for model_name, metrics in validation_stats['detailed_metrics'].items():
                    score = metrics['validation_score']
                    tests_passed = metrics['tests_passed']
                    tests_total = metrics['tests_total']
                    method = metrics['instantiation_method']
                    logger.info(f"  {model_name}: {score:.1%} score, {tests_passed}/{tests_total} tests, {method} config")
            
            # Log individual model results
            logger.info("="*70)
            for model_name, status in variant_status.items():
                status_icon = "OK" if status == "available" else "WARN" if status.startswith("warning") else "FAIL"
                logger.info(f"{status_icon} {model_name}: {status}")
            
            # Log recommendations
            logger.info("="*70)
            available_models = [name for name, status in variant_status.items() if status == 'available']
            warning_models = [name for name, status in variant_status.items() if status.startswith('warning')]
            failed_models = [name for name, status in variant_status.items() if status.startswith('error') or 'failed' in status]
            
            if available_models:
                logger.info(f"RECOMMENDED MODELS: {', '.join(available_models)}")
            if warning_models:
                logger.info(f"MODELS WITH WARNINGS: {', '.join(warning_models)}")
            if failed_models:
                logger.info(f"FAILED MODELS: {', '.join(failed_models)}")
            
            logger.info("="*70)
        
        final_bar.text = "Validation complete!"
        final_bar()
    
    return variant_status

def model_instantiation(
    variant_class: Any,
    variant_name: str,
    input_dim: int,
    base_config: Dict[str, Any],
    fallback_config: Dict[str, Any] = None,
    minimal_config: Dict[str, Any] = None,
    hardware_data: Dict[str, Any] = None,
    silent: bool = False,
    logger: logging.Logger = None
) -> Tuple[Any, str, List[str], List[str], Dict[str, Any]]:
    """
    Enhanced helper function for model instantiation with comprehensive error handling and fallback support.
    
    This function can be used by validate_model_variants(), initialize_model_variants() and other functions 
    that need to instantiate model variants with robust error handling and configuration fallbacks.
    
    Args:
        variant_class: The model class to instantiate
        variant_name: Name of the model variant for logging
        input_dim: Input dimension for the model
        base_config: Primary configuration to try first
        fallback_config: Fallback configuration if primary fails (optional)
        minimal_config: Minimal configuration as last resort (optional)
        hardware_data: Hardware information for adaptive configuration (optional)
        silent: If True, suppress detailed logging
        logger: Logger instance for reporting results
        
    Returns:
        Tuple containing:
        - model_instance: Instantiated model or None if failed
        - instantiation_method: 'primary', 'fallback', 'minimal', 'adaptive', or 'failed'
        - warnings: List of warning messages
        - errors: List of error messages
        - config_used: The configuration that was successfully used
    """
    test_models = []
    warnings = []
    errors = []
    instantiation_method = 'failed'
    model_instance = None
    config_used = None
    
    if logger is None:
        # Create a basic logger if none provided
        logger = logging.getLogger(__name__)
    
    if hardware_data is None:
        hardware_data = {}
    
    # Try primary configuration first
    try:
        model_instance = variant_class(
            input_dim=input_dim,
            config=base_config
        )
        test_models.append(model_instance)
        instantiation_method = 'primary'
        config_used = base_config
        
        if not silent:
            logger.debug(f"[OK] {variant_name}: Instantiated with primary configuration")
            
    except Exception as primary_error:
        primary_error_msg = str(primary_error)
        if not silent:
            logger.debug(f"{variant_name}: Primary configuration failed: {primary_error_msg}")
        
        # Try fallback configuration if provided
        if fallback_config is not None:
            try:
                model_instance = variant_class(
                    input_dim=input_dim,
                    config=fallback_config
                )
                test_models.append(model_instance)
                instantiation_method = 'fallback'
                config_used = fallback_config
                warnings.append(f'Required fallback configuration: {primary_error_msg}')
                
                if not silent:
                    logger.info(f"[WARN] {variant_name}: Required fallback configuration")
                    
            except Exception as fallback_error:
                fallback_error_msg = str(fallback_error)
                if not silent:
                    logger.debug(f"{variant_name}: Fallback configuration failed: {fallback_error_msg}")
        
        # Try minimal configuration if provided and fallback failed
        if model_instance is None and minimal_config is not None:
            try:
                model_instance = variant_class(
                    input_dim=input_dim,
                    config=minimal_config
                )
                test_models.append(model_instance)
                instantiation_method = 'minimal'
                config_used = minimal_config
                warnings.append(f'Required minimal configuration: {primary_error_msg}')
                
                if not silent:
                    logger.info(f"[WARN] {variant_name}: Required minimal configuration")
                    
            except Exception as minimal_error:
                minimal_error_msg = str(minimal_error)
                if not silent:
                    logger.debug(f"{variant_name}: Minimal configuration failed: {minimal_error_msg}")
        
        # If all provided configurations failed, try adaptive configuration
        if model_instance is None:
            try:
                adaptive_config = _create_adaptive_config(
                    model_name=variant_name,
                    model_class=variant_class,
                    system_class='emergency',
                    input_dim=input_dim,
                    encoding_dim=max(4, base_config.get('model', {}).get('encoding_dim', 16) // 2),
                    hidden_dims=base_config.get('model', {}).get('hidden_dims', [64]),
                    dropout_rates=base_config.get('model', {}).get('dropout_rates', [0.2]),
                    activation=base_config.get('model', {}).get('activation', 'relu'),
                    activation_param=base_config.get('model', {}).get('activation_param', 0.0),
                    normalization=base_config.get('model', {}).get('normalization', None),
                    use_batch_norm=False,
                    use_layer_norm=False,
                    skip_connection=False,
                    residual_blocks=False,
                    use_attention=False,
                    legacy_mode=True,
                    num_models=1,
                    diversity_factor=0.0,
                    learning_rate=0.001,
                    batch_size=32,
                    mixed_precision=False,
                    optimizer_type='Adam',
                    device_setting='cpu',
                    random_seed=42,
                    hardware_data=hardware_data
                )
                
                model_instance = variant_class(
                    input_dim=input_dim,
                    config=adaptive_config
                )
                test_models.append(model_instance)
                instantiation_method = 'adaptive'
                config_used = adaptive_config
                warnings.append(f'Required adaptive configuration: {primary_error_msg}')
                
                if not silent:
                    logger.info(f"[WARN] {variant_name}: Required adaptive configuration")
                    
            except Exception as adaptive_error:
                adaptive_error_msg = str(adaptive_error)
                error_details = [f'Primary: {primary_error_msg}']
                if fallback_config is not None:
                    error_details.append(f'Fallback: {fallback_error_msg}')
                if minimal_config is not None:
                    error_details.append(f'Minimal: {minimal_error_msg}')
                error_details.append(f'Adaptive: {adaptive_error_msg}')
                
                errors.append(', '.join(error_details))
                
                if not silent:
                    logger.error(f"[FAIL] {variant_name}: All instantiation attempts failed")
    
    return model_instance, instantiation_method, warnings, errors, config_used

def model_instantiation_with_validation(
    variant_class: Any,
    variant_name: str,
    input_dim: int,
    base_config: Dict[str, Any],
    fallback_config: Dict[str, Any] = None,
    minimal_config: Dict[str, Any] = None,
    validation_tests: List[str] = None,
    comprehensive_validation: bool = False,
    hardware_data: Dict[str, Any] = None,
    silent: bool = False,
    logger: logging.Logger = None
) -> Tuple[Any, Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    """
    Enhanced model instantiation with built-in validation tests optimized for initialize_model_variants().
    
    This function combines instantiation with comprehensive validation and is useful for
    functions that need both instantiation and immediate validation, especially initialize_model_variants().
    
    Args:
        variant_class: The model class to instantiate
        variant_name: Name of the model variant for logging
        input_dim: Input dimension for the model
        base_config: Primary configuration to try
        fallback_config: Fallback configuration if primary fails (optional)
        minimal_config: Minimal configuration as last resort (optional)
        validation_tests: List of validation tests to perform
        comprehensive_validation: If True, perform comprehensive validation similar to validate_model_variants()
        hardware_data: Hardware information for adaptive configuration
        silent: If True, suppress detailed logging
        logger: Logger instance for reporting results
        
    Returns:
        Tuple containing:
        - model_instance: Instantiated model or None if failed
        - validation_results: Dictionary with test results
        - performance_metrics: Dictionary with performance metrics
        - instantiation_details: Dictionary with instantiation details
    """
    if validation_tests is None:
        validation_tests = ['basic', 'forward_pass', 'parameters', 'config_methods', 'training_mode']
    
    if logger is None:
        logger = logging.getLogger(__name__)
    
    validation_results = {}
    performance_metrics = {}
    instantiation_details = {}
    
    # Instantiate the model with enhanced error handling
    model_instance, method, warnings, errors, config_used = model_instantiation(
        variant_class=variant_class,
        variant_name=variant_name,
        input_dim=input_dim,
        base_config=base_config,
        fallback_config=fallback_config,
        minimal_config=minimal_config,
        hardware_data=hardware_data,
        silent=silent,
        logger=logger
    )
    
    instantiation_details['method'] = method
    instantiation_details['warnings'] = warnings
    instantiation_details['errors'] = errors
    instantiation_details['config_used'] = config_used
    instantiation_details['successful'] = model_instance is not None
    
    validation_results['instantiation_method'] = method
    validation_results['warnings'] = warnings
    validation_results['errors'] = errors
    validation_results['instantiation_successful'] = model_instance is not None
    
    if model_instance is None:
        return None, validation_results, performance_metrics, instantiation_details
    
    # Perform validation tests
    test_results = {}
    
    if 'basic' in validation_tests:
        try:
            # Basic model structure validation
            total_params = sum(p.numel() for p in model_instance.parameters())
            trainable_params = sum(p.numel() for p in model_instance.parameters() if p.requires_grad)
            
            performance_metrics['total_parameters'] = total_params
            performance_metrics['trainable_parameters'] = trainable_params
            
            if total_params == 0:
                validation_results['errors'].append('Model has no parameters')
                test_results['basic'] = 'failed'
            elif trainable_params == 0:
                validation_results['warnings'].append('Model has no trainable parameters')
                test_results['basic'] = 'warning'
            else:
                test_results['basic'] = 'passed'
            
        except Exception as e:
            validation_results['errors'].append(f'Basic validation failed: {str(e)}')
            test_results['basic'] = 'failed'
    
    if 'forward_pass' in validation_tests and model_instance is not None:
        try:
            # Basic forward pass test with different batch sizes
            model_instance.eval()
            
            # Test with different batch sizes for robustness
            batch_sizes = [1, 2, 4] if not comprehensive_validation else [1, 2, 4, 8, 16]
            forward_pass_results = {}
            
            for batch_size in batch_sizes:
                try:
                    test_input = torch.randn(batch_size, input_dim)
                    
                    if hasattr(model_instance, 'device'):
                        test_input = test_input.to(model_instance.device)
                    
                    with torch.no_grad():
                        output = model_instance(test_input)
                    
                    expected_shape = (batch_size, input_dim)
                    if output.shape == expected_shape:
                        forward_pass_results[f'batch_{batch_size}'] = 'passed'
                        
                        # Check for numerical issues
                        if torch.isnan(output).any():
                            validation_results['warnings'].append(f'NaN values in output (batch_size={batch_size})')
                            forward_pass_results[f'batch_{batch_size}'] = 'warning'
                        if torch.isinf(output).any():
                            validation_results['errors'].append(f'Infinite values in output (batch_size={batch_size})')
                            forward_pass_results[f'batch_{batch_size}'] = 'failed'
                    else:
                        validation_results['errors'].append(f'Shape mismatch: {output.shape} vs {expected_shape} (batch_size={batch_size})')
                        forward_pass_results[f'batch_{batch_size}'] = 'failed'
                        
                except Exception as batch_error:
                    validation_results['warnings'].append(f'Forward pass failed for batch_size={batch_size}: {str(batch_error)}')
                    forward_pass_results[f'batch_{batch_size}'] = 'failed'
            
            # Determine overall forward pass result
            if all(result == 'passed' for result in forward_pass_results.values()):
                test_results['forward_pass'] = 'passed'
            elif any(result == 'failed' for result in forward_pass_results.values()):
                test_results['forward_pass'] = 'failed'
            else:
                test_results['forward_pass'] = 'warning'
                
            performance_metrics['forward_pass_results'] = forward_pass_results
                
        except Exception as e:
            validation_results['errors'].append(f'Forward pass test failed: {str(e)}')
            test_results['forward_pass'] = 'failed'
    
    if 'parameters' in validation_tests and model_instance is not None:
        try:
            # Parameter validation with detailed statistics
            param_stats = {}
            total_params = 0
            has_nan_params = False
            has_inf_params = False
            
            for name, param in model_instance.named_parameters():
                param_stats[name] = {
                    'shape': list(param.shape),
                    'numel': param.numel(),
                    'requires_grad': param.requires_grad,
                    'has_nan': torch.isnan(param).any().item(),
                    'has_inf': torch.isinf(param).any().item(),
                    'mean': param.mean().item() if param.numel() > 0 else 0,
                    'std': param.std().item() if param.numel() > 0 else 0
                }
                
                total_params += param.numel()
                if param_stats[name]['has_nan']:
                    has_nan_params = True
                if param_stats[name]['has_inf']:
                    has_inf_params = True
            
            performance_metrics['parameter_stats'] = param_stats
            performance_metrics['total_parameters_detailed'] = total_params
            
            if has_nan_params:
                validation_results['errors'].append('Parameters contain NaN values')
                test_results['parameters'] = 'failed'
            elif has_inf_params:
                validation_results['errors'].append('Parameters contain infinite values')
                test_results['parameters'] = 'failed'
            else:
                test_results['parameters'] = 'passed'
            
        except Exception as e:
            validation_results['errors'].append(f'Parameter validation failed: {str(e)}')
            test_results['parameters'] = 'failed'
    
    if 'config_methods' in validation_tests and model_instance is not None:
        try:
            # Configuration methods validation
            config_validation = {}
            
            if hasattr(model_instance, 'get_config'):
                config_dict = model_instance.get_config()
                if not isinstance(config_dict, dict):
                    validation_results['warnings'].append('get_config() does not return a dictionary')
                    config_validation['get_config'] = 'warning'
                else:
                    config_validation['get_config'] = 'passed'
                    performance_metrics['config_structure'] = list(config_dict.keys()) if config_dict else []
            else:
                config_validation['get_config'] = 'not_available'
            
            if hasattr(model_instance, 'update_from_config'):
                # Test configuration update capability
                try:
                    test_update = {'model': {'test_update': True}}
                    model_instance.update_from_config(test_update)
                    config_validation['update_from_config'] = 'passed'
                except Exception as update_error:
                    validation_results['warnings'].append(f'update_from_config failed: {str(update_error)}')
                    config_validation['update_from_config'] = 'warning'
            else:
                config_validation['update_from_config'] = 'not_available'
            
            test_results['config_methods'] = 'passed' if all(
                result in ['passed', 'not_available'] for result in config_validation.values()
            ) else 'warning'
            
            performance_metrics['config_validation'] = config_validation
            
        except Exception as e:
            validation_results['errors'].append(f'Configuration methods test failed: {str(e)}')
            test_results['config_methods'] = 'failed'
    
    if 'training_mode' in validation_tests and model_instance is not None:
        try:
            # Training mode compatibility test
            model_instance.train()
            training_batch_size = 2  # Conservative batch size for training mode
            
            training_input = torch.randn(training_batch_size, input_dim)
            if hasattr(model_instance, 'device'):
                training_input = training_input.to(model_instance.device)
            
            with torch.no_grad():
                training_output = model_instance(training_input)
            
            expected_shape = (training_batch_size, input_dim)
            if training_output.shape == expected_shape:
                test_results['training_mode'] = 'passed'
            else:
                validation_results['warnings'].append('Training mode output shape mismatch')
                test_results['training_mode'] = 'warning'
                
            # Return to eval mode
            model_instance.eval()
            
        except Exception as e:
            validation_results['warnings'].append(f'Training mode test failed: {str(e)}')
            test_results['training_mode'] = 'failed'
    
    if comprehensive_validation and model_instance is not None:
        try:
            # Additional comprehensive validation tests
            comprehensive_results = {}
            
            # Encode/decode functionality if available
            if hasattr(model_instance, 'encode') and hasattr(model_instance, 'decode'):
                try:
                    model_instance.eval()
                    test_input_encode = torch.randn(2, input_dim)
                    if hasattr(model_instance, 'device'):
                        test_input_encode = test_input_encode.to(model_instance.device)
                    
                    encoded = model_instance.encode(test_input_encode)
                    decoded = model_instance.decode(encoded)
                    
                    if decoded.shape == test_input_encode.shape:
                        reconstruction_error = torch.mean((test_input_encode - decoded) ** 2).item()
                        comprehensive_results['encode_decode'] = 'passed'
                        performance_metrics['reconstruction_error'] = reconstruction_error
                    else:
                        comprehensive_results['encode_decode'] = 'failed'
                        validation_results['errors'].append('Encode/decode shape mismatch')
                except Exception as encode_error:
                    comprehensive_results['encode_decode'] = 'failed'
                    validation_results['warnings'].append(f'Encode/decode test failed: {str(encode_error)}')
            
            # Device compatibility if CUDA available
            if torch.cuda.is_available():
                try:
                    cuda_device = torch.device('cuda')
                    model_cuda = model_instance.to(cuda_device)
                    test_input_cuda = torch.randn(2, input_dim, device=cuda_device)
                    
                    with torch.no_grad():
                        cuda_output = model_cuda(test_input_cuda)
                    
                    comprehensive_results['cuda_compatibility'] = 'passed'
                    # Move back to original device
                    model_instance = model_cuda.cpu() if hasattr(model_instance, 'device') else model_cuda
                except Exception as cuda_error:
                    comprehensive_results['cuda_compatibility'] = 'failed'
                    validation_results['warnings'].append(f'CUDA compatibility test failed: {str(cuda_error)}')
            
            test_results['comprehensive'] = 'passed' if all(
                result == 'passed' for result in comprehensive_results.values()
            ) else 'warning'
            
        except Exception as e:
            validation_results['warnings'].append(f'Comprehensive validation failed: {str(e)}')
            test_results['comprehensive'] = 'failed'
    
    # Calculate overall validation score
    passed_tests = sum(1 for result in test_results.values() if result == 'passed')
    total_tests = len(test_results)
    
    performance_metrics['validation_score'] = passed_tests / total_tests if total_tests > 0 else 0
    performance_metrics['tests_performed'] = test_results
    
    validation_results['test_results'] = test_results
    validation_results['overall_score'] = performance_metrics['validation_score']
    
    return model_instance, validation_results, performance_metrics, instantiation_details

def _get_system_context(silent: bool = False) -> Dict[str, Any]:
    """
    Comprehensive system analysis and hardware context collection.
    
    This function gathers detailed system information including hardware capabilities,
    memory usage, performance baselines, and applies memory optimization if needed.
    
    Args:
        silent (bool): Whether to suppress logging output
        
    Returns:
        Dict[str, Any]: Comprehensive system context including hardware data, 
                       system class, and collection status
    """
    # Initialize context structure
    system_context = {
        'hardware_data': None,
        'total_ram_gb': 8.0,
        'system_class': 'unknown',
        'collection_success': False,
        'collection_error': None
    }
    
    try:
        # Comprehensive system information collection
        system_info = get_system_info(
            include_versions=True,
            include_hardware=True,
            include_memory_usage=True,
            include_detailed_analysis=True,
            include_performance_baseline=True
        )
        
        hardware_analysis = system_info.get('hardware_analysis', {})
        capabilities = hardware_analysis.get('capabilities', {})
        system_class = hardware_analysis.get('system_class', 'unknown')
        baseline_results = system_info.get('performance_baseline', {})
        
        # Extract comprehensive hardware information
        gpu_available = capabilities.get('gpu', {}).get('available', False)
        gpu_memory_gb = capabilities.get('gpu', {}).get('total_memory_gb', 0)
        cpu_count = capabilities.get('cpu', {}).get('logical_cores', os.cpu_count() or 1)
        system_memory_gb = capabilities.get('memory', {}).get('total_gb', 8)
        memory_usage_percent = capabilities.get('memory', {}).get('usage_percent', 0)
        
        # Create comprehensive hardware_data structure
        hardware_data = {
            'gpu_available': gpu_available,
            'gpu_memory_gb': gpu_memory_gb,
            'cpu_count': cpu_count,
            'system_memory_gb': system_memory_gb,
            'system_class': system_class,
            'memory_usage_percent': memory_usage_percent,
            'hardware_analysis': hardware_analysis,
            'capabilities': capabilities,
            'performance_baseline': baseline_results,
            'collection_method': 'comprehensive_system_info'
        }
        
        # Additional detailed information from system_info
        if 'cuda' in system_info:
            hardware_data['cuda'] = system_info['cuda']
        if 'cpu_cores' in system_info:
            hardware_data['cpu_cores'] = system_info['cpu_cores']
        if 'system_ram' in system_info:
            hardware_data['system_ram'] = system_info['system_ram']
        
        total_ram_gb = system_memory_gb
        
        if not silent:
            logger.debug(f"Comprehensive system analysis: "
                    f"Class={system_class}, "
                    f"GPU={gpu_available} ({gpu_memory_gb:.1f}GB), "
                    f"CPU={cpu_count} cores, "
                    f"RAM={system_memory_gb:.1f}GB ({memory_usage_percent:.1f}% used)")
        
        # Update system context with successful collection
        system_context.update({
            'hardware_data': hardware_data,
            'total_ram_gb': total_ram_gb,
            'system_class': system_class,
            'collection_success': True,
            'collection_error': None
        })
        
    except Exception as e:
        if not silent:
            logger.warning(f"Comprehensive system analysis failed, using fallbacks: {e}")
        
        # Fallback to basic hardware detection
        hardware_data = {
            'gpu_available': torch.cuda.is_available(),
            'gpu_memory_gb': 0,
            'cpu_count': os.cpu_count() or 1,
            'system_memory_gb': 8,
            'system_class': 'unknown',
            'memory_usage_percent': 0,
            'collection_error': str(e),
            'partial_data': True,
            'collection_method': 'fallback_basic'
        }
        
        # Try to get basic GPU information if available
        if torch.cuda.is_available():
            try:
                gpu_props = torch.cuda.get_device_properties(0)
                hardware_data['gpu_memory_gb'] = gpu_props.total_memory / (1024**3)
                
                # Basic CUDA information
                hardware_data['cuda'] = {
                    'available': True,
                    'device_count': torch.cuda.device_count(),
                    'current_device': torch.cuda.current_device(),
                    'device_name': gpu_props.name,
                    'memory_gb': hardware_data['gpu_memory_gb']
                }
            except Exception as gpu_error:
                hardware_data['gpu_memory_gb'] = 4
                hardware_data['cuda'] = {'available': False, 'error': str(gpu_error)}
        
        # Update system context with fallback values
        system_context.update({
            'hardware_data': hardware_data,
            'total_ram_gb': hardware_data['system_memory_gb'],
            'system_class': hardware_data['system_class'],
            'collection_success': False,
            'collection_error': str(e)
        })
    
    return system_context

def _optimize_memory_if_needed(condition: bool, hardware_data: Dict[str, Any], aggressive: bool = False, silent: bool = False) -> bool:
    """Shared memory optimization routine."""
    if condition:
        try:
            results = enhanced_clear_memory(
                aggressive=aggressive,
                hardware_data=hardware_data
            )
            if results.get('success') and not silent:
                logger.debug("Memory optimization completed")
            return results.get('success', False)
        except Exception as e:
            if not silent:
                logger.debug(f"Memory optimization failed: {e}")
    return False

def _create_model_test_config(
    encoding_dim: int,
    hidden_dims: list,
    dropout_rates: list,
    activation: str,
    activation_param: float,
    normalization: str,
    use_batch_norm: bool,
    use_layer_norm: bool,
    skip_connection: bool,
    residual_blocks: bool,
    use_attention: bool,
    legacy_mode: bool,
    num_models: int,
    diversity_factor: float,
    learning_rate: float,
    batch_size: int,
    mixed_precision: bool,
    optimizer_type: str,
    device_setting: str,
    random_seed: int,
    input_dim: int,
    hardware_data: Dict[str, Any],
    model_specific_overrides: Dict[str, Any] = None
) -> Dict[str, Any]:
    """Creates a comprehensive test configuration for model initialization."""
    # Get hardware data if not provided
    if not hardware_data:
        try:
            hardware_data = check_hardware(include_memory_usage=False) if 'check_hardware' in globals() else {}
        except:
            hardware_data = {}
    
    # Base configuration structure
    config = {
        'model': {
            'model_type': 'SimpleAutoencoder',
            'encoding_dim': encoding_dim,
            'hidden_dims': hidden_dims.copy(),
            'dropout_rates': dropout_rates.copy(),
            'activation': activation,
            'activation_param': activation_param,
            'normalization': normalization,
            'use_batch_norm': use_batch_norm or True if normalization == 'batch' else False,
            'use_layer_norm': use_layer_norm or True if normalization == 'layer' else False,
            'skip_connection': skip_connection,
            'residual_blocks': residual_blocks,
            'use_attention': use_attention,
            'bias': True,
            'weight_init': 'xavier_uniform',
            'legacy_mode': legacy_mode,
            'num_models': num_models,
            'diversity_factor': diversity_factor,
            'min_features': 5
        },
        'training': {
            'learning_rate': learning_rate,
            'batch_size': batch_size,
            'mixed_precision': mixed_precision and hardware_data.get('gpu_available', False),
            'optimizer': optimizer_type,
            'weight_decay': 1e-4,
            'scheduler': 'ReduceLROnPlateau',
            'scheduler_params': {'patience': 10, 'factor': 0.5}
        },
        'data': {
            'features': input_dim,
            'normalization': 'standard'
        },
        'hardware': {
            'device': device_setting,
            'cuda_optimizations': hardware_data.get('gpu_available', False)
        },
        'system': {
            'random_seed': random_seed,
            'reproducible': True,
            'debug': False,
            'verbose': False
        },
        'monitoring': {
            'metrics_to_track': ['loss', 'reconstruction_error'],
            'save_best_model': False,
            'save_checkpoints': False
        }
    }
    
    # Apply model-specific overrides if provided
    if model_specific_overrides:
        for section, params in model_specific_overrides.items():
            if section in config:
                config[section].update(params)
            else:
                config[section] = params
    
    return config

def _create_model_test_definition(
    encoding_dim: int,
    hidden_dims: list,
    dropout_rates: list,
    use_attention: bool,
    residual_blocks: bool,
    skip_connection: bool,
    legacy_mode: bool,
    num_models: int,
    diversity_factor: float,
    mixed_precision: bool
) -> Dict[str, Any]:
    """Creates a comprehensive model test definition dictionary."""
    
    model_definitions = {
        'SimpleAutoencoder': {
            'class_check': lambda: SimpleAutoencoder is not None and callable(SimpleAutoencoder),
            'class_getter': lambda: SimpleAutoencoder,
            'primary_config': _create_model_test_config(
                encoding_dim=encoding_dim,
                hidden_dims=[hidden_dims[0]] if hidden_dims else [128],
                dropout_rates=[dropout_rates[0]] if dropout_rates else [0.2],
                activation='relu',
                activation_param=0.0,
                normalization='batch',
                use_batch_norm=True,
                use_layer_norm=False,
                skip_connection=False,
                residual_blocks=False,
                use_attention=False,
                legacy_mode=legacy_mode,
                num_models=1,
                diversity_factor=0.0,
                learning_rate=0.001,
                batch_size=32,
                mixed_precision=mixed_precision,
                optimizer_type='adam',
                device_setting='auto',
                random_seed=42,
                input_dim=100,  # Will be overridden
                hardware_data={'gpu_available': torch.cuda.is_available()},
                model_specific_overrides={
                    'model': {
                        'model_type': 'SimpleAutoencoder',
                        'use_attention': False,
                        'residual_blocks': False
                    }
                }
            ),
            'fallback_config': _create_model_test_config(
                encoding_dim=max(4, encoding_dim // 2),
                hidden_dims=[64],
                dropout_rates=[0.2],
                activation='relu',
                activation_param=0.0,
                normalization=None,
                use_batch_norm=False,
                use_layer_norm=False,
                skip_connection=False,
                residual_blocks=False,
                use_attention=False,
                legacy_mode=True,
                num_models=1,
                diversity_factor=0.0,
                learning_rate=0.001,
                batch_size=32,
                mixed_precision=False,
                optimizer_type='adam',
                device_setting='auto',
                random_seed=42,
                input_dim=100,
                hardware_data={'gpu_available': torch.cuda.is_available()},
                model_specific_overrides={
                    'model': {
                        'model_type': 'SimpleAutoencoder',
                        'legacy_mode': True
                    }
                }
            ),
            'minimal_config': _create_model_test_config(
                encoding_dim=8,
                hidden_dims=[32],
                dropout_rates=[0.1],
                activation='relu',
                activation_param=0.0,
                normalization=None,
                use_batch_norm=False,
                use_layer_norm=False,
                skip_connection=False,
                residual_blocks=False,
                use_attention=False,
                legacy_mode=True,
                num_models=1,
                diversity_factor=0.0,
                learning_rate=0.001,
                batch_size=32,
                mixed_precision=False,
                optimizer_type='adam',
                device_setting='auto',
                random_seed=42,
                input_dim=20,
                hardware_data={'gpu_available': torch.cuda.is_available()},
                model_specific_overrides={
                    'model': {
                        'model_type': 'SimpleAutoencoder'
                    },
                    'data': {
                        'features': 20
                    }
                }
            ),
            'required_params': ['input_dim'],
            'description': 'Basic autoencoder with simple encoder-decoder architecture'
        },
        'EnhancedAutoencoder': {
            'class_check': lambda: EnhancedAutoencoder is not None and callable(EnhancedAutoencoder),
            'class_getter': lambda: EnhancedAutoencoder,
            'primary_config': _create_model_test_config(
                encoding_dim=encoding_dim,
                hidden_dims=hidden_dims.copy(),
                dropout_rates=dropout_rates.copy(),
                activation='leaky_relu',
                activation_param=0.01,
                normalization='batch',
                use_batch_norm=True,
                use_layer_norm=False,
                skip_connection=skip_connection,
                residual_blocks=residual_blocks,
                use_attention=use_attention and encoding_dim >= 32,
                legacy_mode=legacy_mode,
                num_models=1,
                diversity_factor=0.0,
                learning_rate=0.001,
                batch_size=32,
                mixed_precision=mixed_precision,
                optimizer_type='adam',
                device_setting='auto',
                random_seed=42,
                input_dim=100,
                hardware_data={'gpu_available': torch.cuda.is_available()},
                model_specific_overrides={
                    'model': {
                        'model_type': 'EnhancedAutoencoder'
                    }
                }
            ),
            'fallback_config': _create_model_test_config(
                encoding_dim=max(8, encoding_dim // 2),
                hidden_dims=[96, 48] if len(hidden_dims) > 1 else [64],
                dropout_rates=[0.2, 0.15] if len(dropout_rates) > 1 else [0.2],
                activation='leaky_relu',
                activation_param=0.01,
                normalization='batch',
                use_batch_norm=True,
                use_layer_norm=False,
                skip_connection=True,
                residual_blocks=False,
                use_attention=False,
                legacy_mode=False,
                num_models=1,
                diversity_factor=0.0,
                learning_rate=0.001,
                batch_size=32,
                mixed_precision=mixed_precision and torch.cuda.is_available(),
                optimizer_type='adam',
                device_setting='auto',
                random_seed=42,
                input_dim=100,
                hardware_data={'gpu_available': torch.cuda.is_available()},
                model_specific_overrides={
                    'model': {
                        'model_type': 'EnhancedAutoencoder'
                    }
                }
            ),
            'minimal_config': _create_model_test_config(
                encoding_dim=12,
                hidden_dims=[48],
                dropout_rates=[0.15],
                activation='relu',
                activation_param=0.0,
                normalization=None,
                use_batch_norm=False,
                use_layer_norm=False,
                skip_connection=False,
                residual_blocks=False,
                use_attention=False,
                legacy_mode=True,
                num_models=1,
                diversity_factor=0.0,
                learning_rate=0.001,
                batch_size=32,
                mixed_precision=False,
                optimizer_type='adam',
                device_setting='auto',
                random_seed=42,
                input_dim=100,
                hardware_data={'gpu_available': torch.cuda.is_available()},
                model_specific_overrides={
                    'model': {
                        'model_type': 'EnhancedAutoencoder'
                    }
                }
            ),
            'required_params': ['input_dim'],
            'description': 'Advanced autoencoder with enhanced features and configurable architecture'
        },
        'AutoencoderEnsemble': {
            'class_check': lambda: AutoencoderEnsemble is not None and callable(AutoencoderEnsemble),
            'class_getter': lambda: AutoencoderEnsemble,
            'primary_config': _create_model_test_config(
                encoding_dim=encoding_dim,
                hidden_dims=hidden_dims.copy(),
                dropout_rates=dropout_rates.copy(),
                activation='leaky_relu',
                activation_param=0.01,
                normalization='batch',
                use_batch_norm=True,
                use_layer_norm=False,
                skip_connection=skip_connection,
                residual_blocks=residual_blocks and not legacy_mode,
                use_attention=use_attention and not legacy_mode,
                legacy_mode=legacy_mode,
                num_models=min(3, num_models),
                diversity_factor=diversity_factor,
                learning_rate=0.001,
                batch_size=32,
                mixed_precision=mixed_precision,
                optimizer_type='adam',
                device_setting='auto',
                random_seed=42,
                input_dim=100,
                hardware_data={'gpu_available': torch.cuda.is_available()},
                model_specific_overrides={
                    'model': {
                        'model_type': 'AutoencoderEnsemble'
                    }
                }
            ),
            'fallback_config': _create_model_test_config(
                encoding_dim=max(8, encoding_dim // 2),
                hidden_dims=[80, 40] if len(hidden_dims) > 1 else [60],
                dropout_rates=[0.2, 0.15] if len(dropout_rates) > 1 else [0.2],
                activation='leaky_relu',
                activation_param=0.01,
                normalization='batch',
                use_batch_norm=True,
                use_layer_norm=False,
                skip_connection=False,
                residual_blocks=False,
                use_attention=False,
                legacy_mode=False,
                num_models=2,
                diversity_factor=0.1,
                learning_rate=0.001,
                batch_size=32,
                mixed_precision=False,
                optimizer_type='adam',
                device_setting='auto',
                random_seed=42,
                input_dim=100,
                hardware_data={'gpu_available': torch.cuda.is_available()},
                model_specific_overrides={
                    'model': {
                        'model_type': 'AutoencoderEnsemble'
                    }
                }
            ),
            'minimal_config': _create_model_test_config(
                encoding_dim=8,
                hidden_dims=[40],
                dropout_rates=[0.15],
                activation='relu',
                activation_param=0.0,
                normalization=None,
                use_batch_norm=False,
                use_layer_norm=False,
                skip_connection=False,
                residual_blocks=False,
                use_attention=False,
                legacy_mode=True,
                num_models=2,
                diversity_factor=0.05,
                learning_rate=0.001,
                batch_size=32,
                mixed_precision=False,
                optimizer_type='adam',
                device_setting='auto',
                random_seed=42,
                input_dim=100,
                hardware_data={'gpu_available': torch.cuda.is_available()},
                model_specific_overrides={
                    'model': {
                        'model_type': 'AutoencoderEnsemble'
                    }
                }
            ),
            'required_params': ['input_dim'],
            'description': 'Ensemble of diverse autoencoders for improved robustness and performance'
        }
    }
    
    return model_definitions

def _create_adaptive_config(model_name, model_class, system_class, input_dim, encoding_dim, 
                           hidden_dims, dropout_rates, activation, activation_param, 
                           normalization, use_batch_norm, use_layer_norm, skip_connection, 
                           residual_blocks, use_attention, legacy_mode, num_models, 
                           diversity_factor, learning_rate, batch_size, mixed_precision, 
                           optimizer_type, device_setting, random_seed, hardware_data):
    """
    Create adaptive configuration for a specific model type when no predefined test definitions are available.
    
    This function generates model-specific configurations using _create_model_test_config
    with appropriate parameters tailored to each model type's characteristics.
    
    Args:
        model_name (str): Name of the model to create configuration for
        model_class: The model class reference
        system_class: System classification for hardware adaptation
        input_dim (int): Input dimension size
        encoding_dim (int): Encoding/latent dimension size
        hidden_dims (list): List of hidden layer dimensions
        dropout_rates (list): List of dropout rates
        activation (str): Activation function name
        activation_param: Activation function parameters
        normalization (str): Normalization type
        use_batch_norm (bool): Whether to use batch normalization
        use_layer_norm (bool): Whether to use layer normalization
        skip_connection (bool): Whether to use skip connections
        residual_blocks (bool): Whether to use residual blocks
        use_attention (bool): Whether to use attention mechanisms
        legacy_mode (bool): Whether to use legacy mode
        num_models (int): Number of models for ensemble
        diversity_factor (float): Diversity factor for ensemble
        learning_rate (float): Learning rate
        batch_size (int): Batch size
        mixed_precision (bool): Whether to use mixed precision
        optimizer_type (str): Optimizer type
        device_setting (str): Device setting ('auto', 'cuda', 'cpu')
        random_seed (int): Random seed
        hardware_data (dict): Hardware information data
    
    Returns:
        dict: Adaptive configuration for the specified model
    """
    
    # Create model-specific configuration using _create_model_test_config
    if model_name == 'SimpleAutoencoder':
        adaptive_config = _create_model_test_config(
            encoding_dim=max(4, encoding_dim // 2),
            hidden_dims=[hidden_dims[0]] if hidden_dims else [64],
            dropout_rates=[dropout_rates[0]] if dropout_rates else [0.2],
            activation=activation,
            activation_param=activation_param,
            normalization=None,
            use_batch_norm=False,
            use_layer_norm=False,
            skip_connection=False,
            residual_blocks=False,
            use_attention=False,
            legacy_mode=True,
            num_models=num_models,
            diversity_factor=diversity_factor,
            learning_rate=learning_rate,
            batch_size=batch_size,
            mixed_precision=False,
            optimizer_type='Adam',
            device_setting=device_setting,
            random_seed=random_seed,
            input_dim=input_dim,
            hardware_data=hardware_data,
            model_specific_overrides={
                'model': {
                    'model_type': 'SimpleAutoencoder',
                    'use_batch_norm': False,
                    'use_layer_norm': False
                },
                'training': {
                    'mixed_precision': False,
                    'optimizer': 'Adam'
                }
            }
        )
    elif model_name == 'EnhancedAutoencoder':
        adaptive_config = _create_model_test_config(
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention and encoding_dim >= 32,
            legacy_mode=legacy_mode,
            num_models=num_models,
            diversity_factor=diversity_factor,
            learning_rate=learning_rate,
            batch_size=batch_size,
            mixed_precision=mixed_precision,
            optimizer_type=optimizer_type,
            device_setting=device_setting,
            random_seed=random_seed,
            input_dim=input_dim,
            hardware_data=hardware_data,
            model_specific_overrides={
                'model': {
                    'model_type': 'EnhancedAutoencoder',
                    'hardware_adaptive': True,
                    'system_class': system_class
                }
            }
        )
    elif model_name == 'AutoencoderEnsemble':
        adaptive_config = _create_model_test_config(
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention and encoding_dim >= 32,
            legacy_mode=legacy_mode,
            num_models=min(5, num_models),
            diversity_factor=diversity_factor,
            learning_rate=learning_rate * 0.8,
            batch_size=max(8, batch_size // 2),
            mixed_precision=mixed_precision,
            optimizer_type=optimizer_type,
            device_setting=device_setting,
            random_seed=random_seed,
            input_dim=input_dim,
            hardware_data=hardware_data,
            model_specific_overrides={
                'model': {
                    'model_type': 'AutoencoderEnsemble',
                    'num_models': min(5, num_models),
                    'diversity_factor': diversity_factor,
                    'hardware_adaptive': True,
                    'system_class': system_class
                }
            }
        )
    else:
        # Generic configuration for unknown model types
        adaptive_config = _create_model_test_config(
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention,
            legacy_mode=legacy_mode,
            num_models=num_models,
            diversity_factor=diversity_factor,
            learning_rate=learning_rate,
            batch_size=batch_size,
            mixed_precision=mixed_precision,
            optimizer_type=optimizer_type,
            device_setting=device_setting,
            random_seed=random_seed,
            input_dim=input_dim,
            hardware_data=hardware_data,
            model_specific_overrides={'model': {'model_type': model_name}}
        )
    
    return adaptive_config

def _extract_and_validate_config_param(config: Dict, key: str, default: Any, 
                                     global_name: str = None, validator=None, 
                                     description: str = "", silent: bool = False) -> Any:
    """Extract and validate configuration parameters from multiple config sections with comprehensive fallbacks and validation."""
    try:
        # Try configuration first
        if key in config:
            value = config[key]
            if value is not None:
                if validator is None or validator(value):
                    return value
                else:
                    if not silent:
                        logger.debug(f"Invalid config value for {key} ({description}): {value}, using default")
        
        # Try global variable
        if global_name and global_name in globals():
            global_value = globals()[global_name]
            if validator is None or validator(global_value):
                return global_value
            else:
                if not silent:
                    logger.debug(f"Invalid global value for {key} ({global_name}): {global_value}, using default")
        
        # Use default
        if validator and not validator(default):
            return default
        else:
            if not silent:
                logger.warning(f"Default value for {key} failed validation, using safe fallback")
            # Return safe fallback based on type
            if isinstance(default, int):
                return max(1, default)
            elif isinstance(default, list) and default:
                return [max(1, x) if isinstance(x, int) else x for x in default]
            elif isinstance(default, float):
                return max(0.0, min(1.0, default))
            else:
                return default
        
        return default
        
    except Exception as e:
        if not silent:
            logger.debug(f"Error extracting parameter {key}: {e}, using default: {default}")
        return default

def _validate_and_adjust_parameters(hidden_dims: list, dropout_rates: list, silent: bool = False) -> tuple:
    """Validates and adjusts parameters for consistency."""
    
    # Ensure hidden_dims is a valid list
    if not isinstance(hidden_dims, list) or not hidden_dims:
        if isinstance(hidden_dims, (int, float)) and hidden_dims > 0:
            hidden_dims = [int(hidden_dims)]
            if not silent:
                logger.warning(f"Converted hidden_dims to list: {hidden_dims}")
        else:
            hidden_dims = [128, 64]
            if not silent:
                logger.warning(f"Invalid hidden_dims, using default: {hidden_dims}")
    
    # Remove any invalid dimensions
    valid_dims = [dim for dim in hidden_dims if isinstance(dim, (int, float)) and dim > 0]
    if len(valid_dims) != len(hidden_dims):
        if valid_dims:
            hidden_dims = [int(dim) for dim in valid_dims]
            if not silent:
                logger.warning(f"Filtered invalid hidden dimensions: {hidden_dims}")
        else:
            hidden_dims = [64]
            if not silent:
                logger.warning(f"Invalid hidden_dims, using default: {hidden_dims}")
    
    # Ensure dropout_rates is a valid list
    if not isinstance(dropout_rates, list) or not dropout_rates:
        if isinstance(dropout_rates, (int, float)) and 0 <= dropout_rates < 1:
            dropout_rates = [float(dropout_rates)]
            if not silent:
                logger.info(f"Converted dropout_rates to list: {dropout_rates}")
        else:
            dropout_rates = [0.2, 0.15]
            if not silent:
                logger.warning(f"Invalid dropout_rates, using default: {dropout_rates}")
    
    # Remove any invalid rates
    valid_rates = [rate for rate in dropout_rates if isinstance(rate, (int, float)) and 0 <= rate < 1]
    if len(valid_rates) != len(dropout_rates):
        if valid_rates:
            dropout_rates = valid_rates
            if not silent:
                logger.warning(f"Filtered invalid dropout rates: {dropout_rates}")
        else:
            dropout_rates = [0.2]
            if not silent:
                logger.warning(f"Invalid dropout_rates, using default: {dropout_rates}")
    
    # Ensure matching lengths (allow auto-adjustment in classes)
    if len(hidden_dims) != len(dropout_rates):
        if not silent:
            logger.warning(f"Mismatch in lengths of hidden_dims and dropout_rates, proceeding with auto-adjustment in model classes")
    else:
        # Adjust lengths to match
        max_length = max(len(hidden_dims), len(dropout_rates))
        
        # Extend hidden_dims if needed
        while len(hidden_dims) < max_length:
            hidden_dims.append(hidden_dims[-1] // 2 if hidden_dims[-1] > 32 else 32)
        
        # Extend dropout_rates if needed
        while len(dropout_rates) < max_length:
            last_rate = dropout_rates[-1] * 0.8 if dropout_rates[-1] > 0.05 else 0.05
            dropout_rates.append(min(0.5, last_rate))
        
        # Truncate to matching length
        min_length = min(len(hidden_dims), len(dropout_rates))
        hidden_dims = hidden_dims[:min_length]
        dropout_rates = dropout_rates[:min_length]
        
        if not silent:
            logger.debug(f"Adjusted parameter lengths: hidden_dims={hidden_dims}, dropout_rates={dropout_rates}")
    
    return hidden_dims, dropout_rates

def compare_model_architectures(input_dim: int = None, silent: bool = False) -> Dict[str, Any]:
    """
    Compare parameter counts and complexity of different model architectures with comprehensive analysis.
    
    This function performs detailed architectural comparison, performance analysis, and resource requirements
    for all preset configurations and model variants following the initialization and validation of all the
    model variants registered/initialized in the MODEL_VARIANTS dictionary.
    
    Args:
        input_dim: Input dimension for comparison (uses config/default if None)
        silent: If True, suppress detailed logging messages during validation
        
    Returns:
        Dictionary containing detailed comparison results, recommendations, and optimization guidance
    """
    comparison_start_time = time.time()
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'successful_analyses': 0,
        'failed_analyses': 0,
        'current_model': None,
        'analysis_method': None,
        'memory_optimizations': 0
    }
    
    try:
        if not silent:
            logger.debug("Starting comprehensive model architecture comparison using helper functions")
        
        # Calculate total work units for progress tracking
        total_stages = 8  # System, Config, Validation, Parameters, Configs, Analysis, Summary, Finalization
        total_models = len(MODEL_VARIANTS) if MODEL_VARIANTS else 0
        
        # Initialize results structure
        results = {
            '_metadata': {
                'comparison_timestamp': datetime.now().isoformat(),
                'comparison_version': '3.2',
                'input_dimension': None,
                'config_source': 'unknown',
                'available_variants': 0,
                'successful_comparisons': 0,
                'failed_comparisons': 0,
                'hardware_context': None,
                'comparison_duration_seconds': 0,
                'helper_functions_utilized': [
                    '_get_system_context()',
                    '_optimize_memory_if_needed()', 
                    '_extract_and_validate_config_param()',
                    '_validate_and_adjust_parameters()',
                    '_create_model_test_definition()',
                    '_create_adaptive_config()',
                    'model_instantiation_with_validation()',
                    'analyze_model_layers()',
                    'calculate_architecture_efficiency()',
                    'execute_performance_testing()',
                    'estimate_flops()',
                    'analyze_scaling_behavior()',
                    'analyze_memory_usage()',
                    'estimate_training_resources()',
                    'model_specific_feature_analysis()',
                    'generate_model_recommendations()',
                    'generate_comparative_summary()'
                ],
                'memory_optimization_summary': {
                    'optimizations_performed': 0,
                    'hardware_aware': True,
                    'total_cleanup_actions': 0
                }
            },
            '_summary': {
                'recommendations': [],
                'warnings': [],
                'optimal_choices': {},
                'performance_ranking': {},
                'resource_efficiency': {},
                'use_case_recommendations': {},
                'compatibility_matrix': {},
                'optimization_suggestions': []
            },
            '_analysis_results': {
                'architectural_complexity': {},
                'computational_efficiency': {},
                'memory_utilization': {},
                'training_requirements': {},
                'inference_performance': {},
                'scalability_metrics': {}
            }
        }

        with alive_bar(total_stages, title='Model Architecture Comparison\t', unit='stages') as bar:
            
            # STAGE 1: System Analysis
            progress_data['current_stage'] = "System Analysis"
            bar.text = "Analyzing system hardware..."
            
            try:
                system_context = _get_system_context(silent=silent)
                hardware_data = system_context['hardware_data']
                total_ram_gb = system_context['total_ram_gb']
                system_class = system_context['system_class']
                collection_success = system_context['collection_success']
                
                # Extract nested hardware information with fallbacks
                baseline_results = hardware_data.get('performance_baseline', {}) if hardware_data else {}
                gpu_available = hardware_data.get('gpu_available', False) if hardware_data else False
                capabilities = hardware_data.get('capabilities', {}) if hardware_data else {}
                system_memory_gb = hardware_data.get('system_memory_gb', 8.0) if hardware_data else 8.0
                memory_usage_percent = hardware_data.get('memory_usage_percent', 0) if hardware_data else 0
                memory_pressure = memory_usage_percent > 70 or system_memory_gb < 8
                
                results['_metadata']['hardware_context'] = hardware_data
                bar.text = "System analysis complete"
            except Exception as e:
                if not silent:
                    logger.debug(f"System analysis failed: {e}")
                hardware_data = {}
                total_ram_gb = 8.0
                system_class = 'unknown'
                bar.text = "System analysis (using defaults)"
            
            bar()
            
            # STAGE 2: Configuration Loading
            progress_data['current_stage'] = "Loading Configuration"
            bar.text = "Loading configuration..."
            
            try:
                current_config = get_current_config()
                if not isinstance(current_config, dict):
                    current_config = {}
                
                model_config = current_config.get('model', {})
                data_config = current_config.get('data', {})
                training_config = current_config.get('training', {})
                hardware_config = current_config.get('hardware', {})
                system_config = current_config.get('system', {})
                
                results['_metadata']['config_source'] = 'current_config'
                if not silent:
                    logger.debug("Loaded configuration for architectural comparison")
                bar.text = "Configuration loaded"
            except Exception as e:
                if not silent:
                    logger.warning(f"Could not load current config, using defaults: {e}")
                current_config = get_default_config()
                if not isinstance(current_config, dict):
                    current_config = {}
                
                model_config = current_config.get('model', {})
                data_config = current_config.get('data', {})
                training_config = current_config.get('training', {})
                hardware_config = current_config.get('hardware', {})
                system_config = current_config.get('system', {})
                results['_metadata']['config_source'] = 'intelligent_defaults'
                if not silent:
                    logger.debug("Loaded default configuration for architectural comparison")
                bar.text = "Configuration (using defaults)"
            
            # Memory optimization after config processing
            config_size_estimate = len(str(current_config))
            config_memory_optimized = _optimize_memory_if_needed(
                condition=config_size_estimate > 50000 and total_ram_gb < 16,
                hardware_data=hardware_data,
                aggressive=config_size_estimate > 100000,
                silent=silent
            )
            if config_memory_optimized:
                results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                progress_data['memory_optimizations'] += 1
            
            bar()
            
            # STAGE 3: Model Variants Validation
            progress_data['current_stage'] = "Validating Model Variants"
            bar.text = "Validating model variants..."
            
            if not MODEL_VARIANTS:
                if not silent:
                    logger.info("MODEL_VARIANTS empty, performing initialization and validation")
                try:
                    initialize_model_variants(silent=silent)
                    if MODEL_VARIANTS:
                        validation_results = validate_model_variants(logger, silent=silent)
                        valid_variants = [name for name, status in validation_results.items() 
                                        if name != '_validation_summary' and (status == 'available' or status.startswith('warning'))]
                        results['_metadata']['validation_checks_performed'] = ['model_variants_validation']
                        results['_metadata']['valid_variants'] = valid_variants
                        if not silent:
                            logger.info(f"Validated {len(valid_variants)} model variants for comparison")
                        bar.text = f"Validated {len(valid_variants)} model variants"
                    else:
                        raise RuntimeError("No model variants available after initialization")
                except Exception as e:
                    error_msg = f"Model initialization and validation failed: {str(e)}"
                    results['initialization_error'] = error_msg
                    results['_metadata']['initialization_failure'] = True
                    if not silent:
                        logger.error(error_msg)
                    bar.text = "Model validation failed"
                    # Continue to allow partial analysis if possible
            else:
                bar.text = f"Using {len(MODEL_VARIANTS)} pre-validated variants"
            
            results['_metadata']['available_variants'] = len(MODEL_VARIANTS)
            total_models = len(MODEL_VARIANTS)
            
            bar()
            
            # STAGE 4: Parameter Extraction
            progress_data['current_stage'] = "Extracting Parameters"
            bar.text = "Extracting configuration parameters..."
            
            # Validate input dimension
            if input_dim is None:
                input_dim = _extract_and_validate_config_param(
                    data_config, 'features', 20, 'FEATURES',
                    lambda x: isinstance(x, int) and x > 0,
                    "input feature dimension", silent
                )
            
            if not isinstance(input_dim, int) or input_dim < 1:
                current_input_dim = input_dim
                input_dim = 20
                if not silent:
                    logger.warning(f"Invalid input_dim {current_input_dim}, using validated default {input_dim}")
            
            if input_dim > 10000:
                results['_metadata']['large_input_warning'] = True
                if not silent:
                    logger.warning(f"Very large input dimension ({input_dim}) detected - analysis may be memory intensive")
            
            results['_metadata']['input_dimension'] = input_dim
            
            # Extract all configuration parameters
            encoding_dim = _extract_and_validate_config_param(
                model_config, 'encoding_dim', 16, 'DEFAULT_ENCODING_DIM',
                lambda x: isinstance(x, int) and x > 0,
                "latent encoding dimension", silent
            )
            
            hidden_dims = _extract_and_validate_config_param(
                model_config, 'hidden_dims', [128, 64], 'HIDDEN_LAYER_SIZES',
                lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(d, int) and d > 0 for d in x),
                "hidden layer dimensions", silent
            )
            
            dropout_rates = _extract_and_validate_config_param(
                model_config, 'dropout_rates', [0.2, 0.15], 'DROPOUT_RATES',
                lambda x: isinstance(x, list) and len(x) > 0 and all(isinstance(r, (int, float)) and 0 <= r < 1 for r in x),
                "dropout rates", silent
            )
            
            activation = _extract_and_validate_config_param(
                model_config, 'activation', 'leaky_relu', 'ACTIVATION',
                lambda x: x in ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu', 'prelu'],
                "activation function", silent
            )
            
            activation_param = _extract_and_validate_config_param(
                model_config, 'activation_param', 0.2, 'ACTIVATION_PARAM',
                lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
                "activation parameter", silent
            )
            
            normalization = _extract_and_validate_config_param(
                model_config, 'normalization', 'batch', 'NORMALIZATION',
                lambda x: x in ['batch', 'layer', 'instance', 'group', 'none', None],
                "normalization type", silent
            )
            
            use_attention = _extract_and_validate_config_param(
                model_config, 'use_attention', True, 'USE_ATTENTION',
                lambda x: isinstance(x, bool),
                "attention mechanism flag", silent
            )
            
            residual_blocks = _extract_and_validate_config_param(
                model_config, 'residual_blocks', True, 'RESIDUAL_BLOCKS',
                lambda x: isinstance(x, bool),
                "residual blocks flag", silent
            )
            
            skip_connection = _extract_and_validate_config_param(
                model_config, 'skip_connection', True, 'SKIP_CONNECTION',
                lambda x: isinstance(x, bool),
                "skip connections flag", silent
            )
            
            num_models = _extract_and_validate_config_param(
                model_config, 'num_models', 3, 'NUM_MODELS',
                lambda x: isinstance(x, int) and 1 <= x <= 20,
                "ensemble size", silent
            )
            
            diversity_factor = _extract_and_validate_config_param(
                model_config, 'diversity_factor', 0.3, 'DIVERSITY_FACTOR',
                lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
                "ensemble diversity factor", silent
            )
            
            batch_size = _extract_and_validate_config_param(
                training_config, 'batch_size', 32, 'DEFAULT_BATCH_SIZE',
                lambda x: isinstance(x, int) and x > 0,
                "training batch size", silent
            )
            
            mixed_precision = _extract_and_validate_config_param(
                training_config, 'mixed_precision', True, 'MIXED_PRECISION',
                lambda x: isinstance(x, bool),
                "mixed precision training", silent
            )
            
            learning_rate = _extract_and_validate_config_param(
                training_config, 'learning_rate', 0.001, 'LEARNING_RATE',
                lambda x: isinstance(x, (int, float)) and x > 0,
                "learning rate", silent
            )
            
            optimizer_type = _extract_and_validate_config_param(
                training_config, 'optimizer', 'AdamW', 'OPTIMIZER',
                lambda x: x in ['Adam', 'AdamW', 'SGD', 'RMSprop', 'Adagrad'],
                "optimizer type", silent
            )
            
            device_setting = _extract_and_validate_config_param(
                hardware_config, 'device', 'auto', 'DEVICE',
                lambda x: isinstance(x, str) and x in ['auto', 'cpu', 'cuda'] or x.startswith('cuda:'),
                "compute device", silent
            )
            
            random_seed = _extract_and_validate_config_param(
                system_config, 'random_seed', 42, 'RANDOM_SEED',
                lambda x: isinstance(x, int),
                "random seed", silent
            )
            
            legacy_mode = _extract_and_validate_config_param(
                model_config, 'legacy_mode', False, 'LEGACY_MODE',
                lambda x: isinstance(x, bool),
                "legacy compatibility mode", silent
            )
            
            # Validate and adjust parameters
            hidden_dims, dropout_rates = _validate_and_adjust_parameters(hidden_dims, dropout_rates, silent)
            
            bar.text = "Parameters extracted and validated"
            bar()
            
            # STAGE 5: Test Configuration Creation
            progress_data['current_stage'] = "Creating Test Configurations"
            bar.text = "Creating test configurations..."
            
            # Generate test definitions
            model_test_definitions = _create_model_test_definition(
                encoding_dim=encoding_dim,
                hidden_dims=hidden_dims,
                dropout_rates=dropout_rates,
                use_attention=use_attention,
                residual_blocks=residual_blocks,
                skip_connection=skip_connection,
                legacy_mode=legacy_mode,
                num_models=num_models,
                diversity_factor=diversity_factor,
                mixed_precision=mixed_precision
            )
            
            # Create test configurations for each model variant
            test_configurations = {}
            for model_name, model_class in MODEL_VARIANTS.items():
                if model_name in model_test_definitions:
                    # Use test definition from helper function
                    test_def = model_test_definitions[model_name]
                    primary_config = test_def.get('primary_config', {})
                    
                    # Update input dimension in the configuration
                    if 'data' in primary_config:
                        primary_config['data']['features'] = input_dim
                    else:
                        primary_config['data'] = {'features': input_dim}
                    
                    if 'model' in primary_config:
                        primary_config['model']['input_dim'] = input_dim
                    else:
                        primary_config['model'] = {'input_dim': input_dim}
                    
                    test_configurations[model_name] = {
                        'params': {
                            'input_dim': input_dim,
                            'config': primary_config
                        },
                        'description': test_def.get('description', 'No description available'),
                        'use_cases': ['general purpose', 'production ready'],
                        'complexity_level': 'standard',
                        'computational_class': 'optimized',
                        'memory_efficiency': 'good',
                        'training_speed': 'moderate'
                    }
                else:
                    # Create adaptive configuration for unknown models
                    adaptive_config = _create_adaptive_config(
                        model_name=model_name,
                        model_class=model_class,
                        system_class=system_class,
                        input_dim=input_dim,
                        encoding_dim=encoding_dim,
                        hidden_dims=hidden_dims,
                        dropout_rates=dropout_rates,
                        activation=activation,
                        activation_param=activation_param,
                        normalization=normalization,
                        use_batch_norm=normalization == 'batch',
                        use_layer_norm=normalization == 'layer',
                        skip_connection=skip_connection,
                        residual_blocks=residual_blocks,
                        use_attention=use_attention,
                        legacy_mode=legacy_mode,
                        num_models=num_models if 'ensemble' in model_name.lower() else 1,
                        diversity_factor=diversity_factor,
                        learning_rate=learning_rate,
                        batch_size=batch_size,
                        mixed_precision=mixed_precision,
                        optimizer_type=optimizer_type,
                        device_setting=device_setting,
                        random_seed=random_seed,
                        hardware_data=hardware_data
                    )
                    
                    test_configurations[model_name] = {
                        'params': {
                            'input_dim': input_dim,
                            'config': adaptive_config
                        },
                        'description': f'Adaptive configuration for {model_name}',
                        'use_cases': ['general purpose', 'adaptive deployment'],
                        'complexity_level': 'adaptive',
                        'computational_class': 'adaptive',
                        'memory_efficiency': 'variable',
                        'training_speed': 'moderate'
                    }
            
            bar.text = f"Created {len(test_configurations)} test configurations"
            bar()
            
            # Close main progress bar and start model analysis bar
            bar.text = "Starting model analysis..."
        
        # STAGE 6: Comprehensive Model Analysis
        progress_data['current_stage'] = "Model Analysis"
        
        # Status symbols for visual feedback
        status_symbols = {
            'success': '[OK]',
            'failure': '[FAIL]',
            'skip': '[SKIP]'
        }
        
        method_symbols = {
            'primary': '[PRIMARY]',
            'fallback': '[FALLBACK]',
            'minimal': '[MINIMAL]',
            'adaptive': '[ADAPTIVE]'
        }
        
        with alive_bar(total_models, title='Analyzing Models\t\t', unit='models') as model_bar:
            
            for model_name, model_class in MODEL_VARIANTS.items():
                progress_data['current_model'] = model_name
                
                # Update progress bar with current model info
                successful_count = progress_data['successful_analyses']
                failed_count = progress_data['failed_analyses']
                model_bar.text = f"Analyzing {model_name}... ({successful_count} passed, {failed_count} failed)"
                
                if model_name not in test_configurations:
                    if not silent:
                        logger.warning(f"No test configuration for {model_name}, skipping analysis")
                    progress_data['failed_analyses'] += 1
                    model_bar.text = f"{status_symbols['skip']} {model_name} skipped"
                    model_bar()
                    continue
                
                test_config = test_configurations[model_name]
                analysis_start_time = time.time()
                model_results = {
                    'analysis_status': 'starting',
                    'errors': [],
                    'warnings': [],
                    'test_definition_used': model_name in model_test_definitions,
                    'helper_functions_used': []
                }
                
                try:
                    if not silent:
                        logger.debug(f"Starting comprehensive analysis for {model_name} using helper functions")
                    
                    # Model instantiation
                    model_results['helper_functions_used'].append('model_instantiation_with_validation')
                    
                    model_instance, validation_results, performance_metrics, instantiation_details = model_instantiation_with_validation(
                        variant_class=model_class,
                        variant_name=model_name,
                        input_dim=input_dim,
                        base_config=test_config['params']['config'],
                        fallback_config=test_config.get('fallback_config'),
                        minimal_config=test_config.get('minimal_config'),
                        validation_tests=['basic', 'forward_pass', 'parameters', 'config_methods'],
                        comprehensive_validation=False,
                        hardware_data=hardware_data,
                        silent=silent,
                        logger=logger
                    )
                    
                    if model_instance is None:
                        model_results['errors'].append('Model instantiation failed')
                        model_results['analysis_status'] = 'instantiation_failed'
                        results[model_name] = model_results
                        results['_metadata']['failed_comparisons'] += 1
                        progress_data['failed_analyses'] += 1
                        if not silent:
                            logger.error(f"Skipping analysis for {model_name} due to instantiation failure")
                        model_bar.text = f"{status_symbols['failure']} {model_name} instantiation failed"
                        model_bar()
                        continue
                    
                    instantiation_method = instantiation_details.get('method', 'primary')
                    validation_score = validation_results.get('overall_score', 0)
                    progress_data['analysis_method'] = instantiation_method
                    
                    # Track instantiation method
                    if instantiation_method != 'primary':
                        model_results['warnings'].append(f'Used {instantiation_method} configuration instead of primary')
                    
                    # Memory optimization after instantiation
                    post_instantiation_optimized = _optimize_memory_if_needed(
                        condition=total_ram_gb < 8,
                        hardware_data=hardware_data,
                        aggressive=total_ram_gb < 4,
                        silent=silent
                    )
                    if post_instantiation_optimized:
                        results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                        progress_data['memory_optimizations'] += 1
                        model_results['helper_functions_used'].append('_optimize_memory_if_needed')
                    
                    # Architecture Analysis using helper functions
                    total_params = sum(p.numel() for p in model_instance.parameters())
                    trainable_params = sum(p.numel() for p in model_instance.parameters() if p.requires_grad)
                    non_trainable_params = total_params - trainable_params
                    model_size_mb = total_params * 4 / (1024 * 1024)
                    
                    # Enhanced layer analysis using helper function
                    model_results['helper_functions_used'].append('analyze_model_layers')
                    layer_analysis = analyze_model_layers(model=model_instance)
                    
                    # Architecture efficiency using helper function
                    model_results['helper_functions_used'].append('calculate_architecture_efficiency')
                    architecture_efficiency = calculate_architecture_efficiency(total_params, layer_analysis)
                    
                    model_results['architecture'] = {
                        'total_params': total_params,
                        'trainable_params': trainable_params,
                        'non_trainable_params': non_trainable_params,
                        'model_size_mb': model_size_mb,
                        'complexity_level': test_config['complexity_level'],
                        'computational_class': test_config['computational_class'],
                        'description': test_config['description'],
                        'layer_count': layer_analysis['total_layers'],
                        'layer_types': layer_analysis['layer_types'],
                        'parameter_distribution': layer_analysis['parameter_distribution'],
                        'architecture_efficiency': architecture_efficiency,
                        'test_definition_based': model_name in model_test_definitions,
                        'instantiation_method': instantiation_method,
                        'validation_score': validation_score
                    }
                    
                    # Performance Testing with enhanced metrics using helper function
                    model_results['helper_functions_used'].append('execute_performance_testing')
                    performance_test_metrics = execute_performance_testing(
                        batch_size, input_dim, model_name, model_instance, hardware_data,
                        config_used=test_config['params']['config']
                    )
                    model_results['performance'] = performance_test_metrics
                    
                    # Memory optimization after performance testing
                    post_performance_optimized = _optimize_memory_if_needed(
                        condition=len(str(performance_test_metrics)) > 20000 or total_ram_gb < 8,
                        hardware_data=hardware_data,
                        aggressive=len(str(performance_test_metrics)) > 50000 or total_ram_gb < 4,
                        silent=silent
                    )
                    if post_performance_optimized:
                        results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                        progress_data['memory_optimizations'] += 1
                    
                    # FLOP Analysis using helper function
                    model_results['helper_functions_used'].append('estimate_flops')
                    flop_metrics = estimate_flops(input_dim, batch_size, model_instance)
                    model_results['computational_complexity'] = flop_metrics
                    
                    # Scaling Analysis using helper function
                    model_results['helper_functions_used'].append('analyze_scaling_behavior')
                    scaling_metrics = analyze_scaling_behavior(
                        model_class, test_config['params'], input_dim, model_name
                    )
                    model_results['scaling'] = scaling_metrics
                    
                    # Memory optimization after scaling analysis
                    post_scaling_optimized = _optimize_memory_if_needed(
                        condition=len(str(scaling_metrics)) > 30000 or total_ram_gb < 8,
                        hardware_data=hardware_data,
                        aggressive=len(str(scaling_metrics)) > 75000 or total_ram_gb < 4,
                        silent=silent
                    )
                    if post_scaling_optimized:
                        results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                        progress_data['memory_optimizations'] += 1
                    
                    # Memory Analysis with system awareness using helper function
                    model_results['helper_functions_used'].append('analyze_memory_usage')
                    memory_metrics = analyze_memory_usage(
                        batch_size, input_dim, model_name, model_instance, hardware_data
                    )
                    model_results['memory_analysis'] = memory_metrics
                    
                    # Training Resource Estimation using helper function
                    model_results['helper_functions_used'].append('estimate_training_resources')
                    training_resources = estimate_training_resources(
                        total_params, batch_size, input_dim, test_config['params']['config'],
                        model_name, hardware_data
                    )
                    model_results['resource_requirements'] = training_resources
                    
                    # Feature Analysis using helper function
                    model_results['helper_functions_used'].append('model_specific_feature_analysis')
                    feature_analysis = model_specific_feature_analysis(model_name, test_config, model_instance)
                    model_results['feature_analysis'] = feature_analysis
                    
                    # Generate Recommendations using helper function
                    model_results['helper_functions_used'].append('generate_model_recommendations')
                    recommendations = generate_model_recommendations(
                        model_name, total_params, hardware_data, performance_test_metrics
                    )
                    model_results['recommendations'] = recommendations
                    
                    # Finalize model results
                    model_results.update({
                        'use_cases': test_config['use_cases'],
                        'configuration_used': test_config['params'],
                        'analysis_metadata': {
                            'analysis_time_seconds': time.time() - analysis_start_time,
                            'test_batch_size': batch_size,
                            'analysis_version': '3.2',
                            'hardware_context_available': bool(hardware_data),
                            'system_class': system_class,
                            'memory_pressure': memory_pressure,
                            'validation_checks_passed': ['instantiation', 'forward_pass', 'parameter_count', 'memory_analysis'],
                            'test_definition_utilized': model_name in model_test_definitions,
                            'adaptive_configuration_used': model_name not in model_test_definitions,
                            'instantiation_method': instantiation_method,
                            'helper_functions_utilized': model_results['helper_functions_used']
                        },
                        'analysis_status': 'completed'
                    })
                    
                    results[model_name] = model_results
                    results['_metadata']['successful_comparisons'] += 1
                    progress_data['successful_analyses'] += 1
                    
                    method_symbol = method_symbols.get(instantiation_method, '[UNKNOWN]')
                    test_def_status = "TestDef" if model_name in model_test_definitions else "Adaptive"
                    model_bar.text = f"{status_symbols['success']} {model_name} {method_symbol} ({total_params:,} params)"
                    
                    if not silent:
                        logger.info(f"{status_symbols['success']} {model_name}: Comprehensive analysis completed "
                                  f"({test_def_status}, {instantiation_method}, {total_params:,} params, "
                                  f"score: {validation_score:.1%})")
                    
                except Exception as e:
                    error_msg = f"Analysis failed: {str(e)}"
                    if not silent:
                        logger.error(f"{model_name}: {error_msg}")
                    model_results['errors'].append(error_msg)
                    model_results['analysis_status'] = 'analysis_failed'
                    model_results['analysis_metadata'] = {
                        'analysis_time_seconds': time.time() - analysis_start_time,
                        'error_type': type(e).__name__,
                        'test_definition_utilized': model_name in model_test_definitions,
                        'helper_functions_utilized': model_results['helper_functions_used']
                    }
                    results[model_name] = model_results
                    results['_metadata']['failed_comparisons'] += 1
                    progress_data['failed_analyses'] += 1
                    model_bar.text = f"{status_symbols['failure']} {model_name} analysis failed"
                
                finally:
                    # Cleanup and memory optimization between models
                    try:
                        if 'model_instance' in locals() and model_instance is not None:
                            del model_instance
                        
                        inter_model_optimized = _optimize_memory_if_needed(
                            condition=True,
                            hardware_data=hardware_data,
                            aggressive=True,
                            silent=silent
                        )
                        if inter_model_optimized:
                            results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                            progress_data['memory_optimizations'] += 1
                        
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None
                    except Exception as cleanup_error:
                        if not silent:
                            logger.debug(f"Cleanup warning for {model_name}: {cleanup_error}")
                
                # Update progress bar
                model_bar()
            
            # Final update for model analysis
            successful_count = progress_data['successful_analyses']
            failed_count = progress_data['failed_analyses']
            model_bar.text = f"Analysis: {successful_count} passed, {failed_count} failed"
        
        # STAGE 7: Comparative Summary
        progress_data['current_stage'] = "Generating Summary"
        
        with alive_bar(1, title='Generating Summary\t\t') as summary_bar:
            
            summary_bar.text = "Generating comparative analysis..."
            
            # Memory optimization before comparative summary
            pre_summary_optimized = _optimize_memory_if_needed(
                condition=len(str(results)) > 100000,
                hardware_data=hardware_data,
                aggressive=len(str(results)) > 500000,
                silent=silent
            )
            if pre_summary_optimized:
                results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                progress_data['memory_optimizations'] += 1
            
            # Generate comparative analysis using helper function
            results['_summary'] = generate_comparative_summary(results, hardware_data)
            
            summary_bar.text = "Summary generated"
            summary_bar()
        
        # STAGE 8: Finalization
        progress_data['current_stage'] = "Finalizing"
        
        with alive_bar(1, title='Finalizing\t\t\t') as final_bar:
            
            final_bar.text = "Finalizing comparison..."
            
            # Final Metadata and Timing
            results['_metadata']['comparison_duration_seconds'] = time.time() - comparison_start_time
            results['_metadata']['analysis_completion_timestamp'] = datetime.now().isoformat()
            
            # FINAL COMPREHENSIVE MEMORY OPTIMIZATION
            final_optimized = _optimize_memory_if_needed(
                condition=True,
                hardware_data=hardware_data,
                aggressive=True,
                silent=silent
            )
            if final_optimized:
                results['_metadata']['memory_optimization_summary']['optimizations_performed'] += 1
                progress_data['memory_optimizations'] += 1
                results['_metadata']['memory_optimization_summary']['final_cleanup'] = True
            
            # Add helper function utilization summary
            total_optimizations = results['_metadata']['memory_optimization_summary']['optimizations_performed']
            results['_metadata']['helper_function_summary'] = {
                'total_helper_functions_used': len(results['_metadata']['helper_functions_utilized']),
                'memory_optimizations_performed': total_optimizations,
                'system_context_method': 'comprehensive_system_analysis',
                'parameter_extraction_method': 'helper_function_based',
                'test_definition_method': 'helper_function_generated',
                'instantiation_method': 'helper_function_validation',
                'configuration_validation': 'helper_function_based',
                'adaptive_configuration_support': True,
                'integration_level': 'full'
            }
            
            # Log comprehensive comparison summary
            successful = results['_metadata']['successful_comparisons']
            total = results['_metadata']['available_variants']
            duration = results['_metadata']['comparison_duration_seconds']
            helper_count = len(results['_metadata']['helper_functions_utilized'])
            test_def_count = sum(1 for m in MODEL_VARIANTS if m in model_test_definitions)
            
            if not silent:
                logger.debug("Model architecture comparison completed using enhanced helper functions:")
                logger.debug(f"  - Total models analyzed: {successful}/{total}")
                logger.debug(f"  - Comparison duration: {duration:.2f} seconds")
                logger.debug(f"  - Helper functions utilized: {helper_count}")
                logger.debug(f"  - Test definition based: {test_def_count}")
                logger.debug(f"  - Adaptive configurations: {total - test_def_count}")
                logger.debug(f"  - Memory optimizations: {progress_data['memory_optimizations']}")
                
                # Log available models and their key metrics
                if results['_metadata']['successful_comparisons'] > 0:
                    logger.debug("  - Successfully analyzed models:")
                    for model_name in MODEL_VARIANTS:
                        if model_name in results and results[model_name].get('analysis_status') == 'completed':
                            arch_data = results[model_name].get('architecture', {})
                            params = arch_data.get('total_params', 0)
                            method = arch_data.get('instantiation_method', 'unknown')
                            logger.debug(f"    {model_name}: {params:,} params ({method})")
                
                if results['_metadata']['failed_comparisons'] > 0:
                    logger.warning(f"  - Failed analyses: {results['_metadata']['failed_comparisons']}")
            
            final_bar.text = "Comparison complete!"
            final_bar()
    
    except Exception as e:
        if not silent:
            logger.error(f"Critical failure in model architecture comparison: {str(e)}", exc_info=True)
        
        # Emergency memory cleanup on critical failure
        emergency_optimized = _optimize_memory_if_needed(
            condition=True,
            hardware_data=hardware_data if 'hardware_data' in locals() else {},
            aggressive=True,
            silent=True
        )
        
        error_result = {
            'error': f'Critical comparison failure: {str(e)}',
            'timestamp': datetime.now().isoformat(),
            'error_type': type(e).__name__,
            'partial_results': results if 'results' in locals() else {},
            'recovery_suggestions': [
                'Check MODEL_VARIANTS initialization',
                'Verify configuration validity',
                'Ensure sufficient system resources',
                'Try with default parameters',
                'Review helper function implementations'
            ],
            'memory_optimization_attempted': True,
            'emergency_cleanup_performed': emergency_optimized,
            'helper_functions_utilized': [
                '_get_system_context()',
                '_optimize_memory_if_needed()', 
                '_create_model_test_definition()',
                '_create_adaptive_config()',
                '_extract_and_validate_config_param()',
                '_validate_and_adjust_parameters()',
                'model_instantiation_with_validation()',
                'analyze_model_layers()',
                'calculate_architecture_efficiency()',
                'execute_performance_testing()',
                'estimate_flops()',
                'analyze_scaling_behavior()',
                'analyze_memory_usage()',
                'estimate_training_resources()',
                'model_specific_feature_analysis()',
                'generate_model_recommendations()',
                'generate_comparative_summary()'
            ],
            'integration_status': 'failed_but_helper_functions_available'
        }
        return error_result

    return results

def analyze_model_layers(model: torch.nn.Module) -> Dict[str, Any]:
    """
    Analyze model layers and return comprehensive layer information.
    
    This function provides detailed layer analysis that harmonizes with the
    compare_model_architectures() function, focusing on parameter distribution,
    layer types, and architectural characteristics.
    
    Args:
        model: PyTorch model to analyze
        
    Returns:
        Dictionary containing layer analysis results
    """
    try:
        analysis = {
            'total_layers': 0,
            'layer_types': {},
            'parameter_distribution': {},
            'layer_details': [],
            'analysis_status': 'completed'
        }
        
        # Count layers and categorize by type
        layer_type_counts = {}
        layer_param_distribution = {}
        total_layers = 0
        layer_details = []
        
        for name, module in model.named_modules():
            if name == '':  # Skip root module
                continue
                
            total_layers += 1
            module_type = type(module).__name__
            
            # Count layer types
            layer_type_counts[module_type] = layer_type_counts.get(module_type, 0) + 1
            
            # Calculate parameters for this layer
            layer_params = sum(p.numel() for p in module.parameters(recurse=False))
            
            # Track parameter distribution by layer type
            if module_type not in layer_param_distribution:
                layer_param_distribution[module_type] = {
                    'total_params': 0,
                    'layer_count': 0,
                    'avg_params_per_layer': 0
                }
            
            layer_param_distribution[module_type]['total_params'] += layer_params
            layer_param_distribution[module_type]['layer_count'] += 1
            
            # Store layer details
            layer_info = {
                'name': name,
                'type': module_type,
                'parameters': layer_params,
                'trainable_params': sum(p.numel() for p in module.parameters(recurse=False) if p.requires_grad)
            }
            layer_details.append(layer_info)
        
        # Calculate average parameters per layer for each type
        for layer_type, info in layer_param_distribution.items():
            if info['layer_count'] > 0:
                info['avg_params_per_layer'] = info['total_params'] / info['layer_count']
        
        # Update analysis results
        analysis.update({
            'total_layers': total_layers,
            'layer_types': layer_type_counts,
            'parameter_distribution': layer_param_distribution,
            'layer_details': layer_details
        })
        
        return analysis
        
    except Exception as e:
        logger.debug(f"Layer analysis failed: {e}")
        return {
            'total_layers': 0,
            'layer_types': {},
            'parameter_distribution': {},
            'layer_details': [],
            'analysis_status': 'failed',
            'error': str(e)
        }

def calculate_architecture_efficiency(total_params: int, layer_analysis: Dict[str, Any]) -> float:
    """
    Calculate architecture efficiency score based on parameter count and layer analysis.
    
    This function provides an efficiency metric that harmonizes with the
    compare_model_architectures() function's architecture analysis, considering
    parameter distribution and layer structure.
    
    Args:
        total_params: Total number of model parameters
        layer_analysis: Layer analysis results from analyze_model_layers()
        
    Returns:
        Float efficiency score (0-100, higher is better)
    """
    try:
        if total_params <= 0 or not isinstance(layer_analysis, dict):
            return 0.0
        
        efficiency_score = 50.0  # Base score
        
        # Factor 1: Parameter distribution efficiency
        param_dist = layer_analysis.get('parameter_distribution', {})
        if param_dist:
            # Check if parameters are well distributed across layer types
            total_layers = layer_analysis.get('total_layers', 1)
            if total_layers > 0:
                # Reward balanced parameter distribution
                param_variance = 0
                layer_type_count = len(param_dist)
                
                if layer_type_count > 1:
                    avg_params_per_type = total_params / layer_type_count
                    for layer_type, info in param_dist.items():
                        type_params = info.get('total_params', 0)
                        param_variance += (type_params - avg_params_per_type) ** 2
                    
                    # Lower variance = better distribution = higher efficiency
                    param_variance = param_variance / layer_type_count
                    distribution_score = max(0, 20 - (param_variance / total_params) * 100)
                    efficiency_score += distribution_score
        
        # Factor 2: Layer structure efficiency
        layer_types = layer_analysis.get('layer_types', {})
        total_layers = layer_analysis.get('total_layers', 1)
        
        if total_layers > 0 and layer_types:
            # Reward efficient layer usage (not too many tiny layers)
            avg_params_per_layer = total_params / total_layers
            # Good parameter density
            if avg_params_per_layer > 1000:
                efficiency_score += 10
            elif avg_params_per_layer > 100:
                efficiency_score += 5
            
            # Reward diverse layer types (up to a point)
            unique_layer_types = len(layer_types)
            # Sweet spot for variety
            if 2 <= unique_layer_types <= 6:
                efficiency_score += min(15, unique_layer_types * 3)
        
        # Factor 3: Parameter efficiency relative to model size
        # Small model
        if total_params < 10000:
            # Bonus for efficiency
            efficiency_score += 15
        # Medium model
        elif total_params < 100000:
            efficiency_score += 10
        # Large model
        elif total_params < 1000000:
            efficiency_score += 5
        # Very large models get no bonus
        
        # Factor 4: Check for obviously inefficient patterns
        if layer_analysis.get('analysis_status') == 'failed':
            efficiency_score -= 20
        
        # Ensure score is within bounds
        efficiency_score = max(0.0, min(100.0, efficiency_score))
        
        return round(efficiency_score, 2)
        
    except Exception as e:
        logger.debug(f"Architecture efficiency calculation failed: {e}")
        # Default moderate efficiency score
        return 25.0

def execute_performance_testing(
    batch_size: int,
    input_dim: int,
    model_name: str,
    model: torch.nn.Module,
    hardware_data: Dict[str, Any],
    config_used: Dict[str, Any] = None,
    validation_mode: bool = False,
    scenario_filter: List[str] = None
) -> Dict[str, Any]:
    """
    Execute comprehensive performance testing leveraging full system analysis.
    
    This function is harmonized with compare_model_architectures() and uses
    the same parameter structure as called in the main comparison function.
    
    Args:
        batch_size: Base batch size for testing
        input_dim: Input dimension
        model_name: Name of the model for logging
        model: Model instance to test
        hardware_data: Hardware information from system context
        config_used: Configuration used for model creation
        validation_mode: If True, includes additional validation-specific tests
        scenario_filter: Optional list of scenario names to run (for targeted testing)
    
    Returns:
        Dictionary containing comprehensive performance metrics
    """
    performance_metrics = {}
    
    try:
        # Extract hardware information from the provided hardware_data
        gpu_available = hardware_data.get('gpu_available', False)
        system_memory_gb = hardware_data.get('system_memory_gb', 8.0)
        cpu_count = hardware_data.get('cpu_count', 1)
        
        # Determine system class based on hardware data
        if system_memory_gb >= 16 and gpu_available:
            system_class = 'high_performance'
        elif system_memory_gb < 8 or not gpu_available:
            system_class = 'limited'
        else:
            system_class = 'standard'
        
        # ADAPTIVE TEST SCENARIOS based on system capabilities and mode
        if system_class == 'high_performance':
            base_scenarios = [
                {'batch_size': 1, 'name': 'single_sample', 'description': 'Single sample inference test'},
                {'batch_size': 4, 'name': 'minimal_batch', 'description': 'Minimal batch processing'},
                {'batch_size': 16, 'name': 'small_batch', 'description': 'Small batch processing test'},
                {'batch_size': batch_size, 'name': 'standard_batch', 'description': 'Standard batch processing test'},
                {'batch_size': min(512, batch_size * 8), 'name': 'large_batch', 'description': 'Large batch processing test'},
                {'batch_size': min(1024, batch_size * 16), 'name': 'stress_test', 'description': 'Stress test with larger batch'}
            ]
        elif system_class == 'limited':
            base_scenarios = [
                {'batch_size': 1, 'name': 'single_sample', 'description': 'Single sample inference test'},
                {'batch_size': max(2, batch_size // 2), 'name': 'small_batch', 'description': 'Small batch processing test'},
                {'batch_size': batch_size, 'name': 'standard_batch', 'description': 'Standard batch processing test'}
            ]
        else:  # standard
            base_scenarios = [
                {'batch_size': 1, 'name': 'single_sample', 'description': 'Single sample inference test'},
                {'batch_size': max(2, batch_size // 2), 'name': 'small_batch', 'description': 'Small batch processing test'},
                {'batch_size': batch_size, 'name': 'standard_batch', 'description': 'Standard batch processing test'},
                {'batch_size': min(256, batch_size * 4), 'name': 'large_batch', 'description': 'Large batch processing test'}
            ]
        
        # Add validation-specific scenarios if in validation mode
        if validation_mode:
            # Add batch norm compatibility test
            if config_used and config_used.get('model', {}).get('use_batch_norm', False):
                base_scenarios.append({
                    'batch_size': max(4, batch_size), 
                    'name': 'batch_norm_compatible', 
                    'description': 'Batch normalization compatibility test'
                })
            
            # Add edge case scenarios for validation
            base_scenarios.extend([
                {'batch_size': 2, 'name': 'edge_case_small', 'description': 'Edge case: very small batch'},
                {'batch_size': max(8, batch_size * 2), 'name': 'medium_batch', 'description': 'Medium batch processing test'}
            ])
        
        # Filter scenarios if specified
        if scenario_filter:
            test_scenarios = [s for s in base_scenarios if s['name'] in scenario_filter]
        else:
            test_scenarios = base_scenarios
        
        # Remove duplicate batch sizes while preserving scenario names
        seen_batches = {}
        filtered_scenarios = []
        for scenario in test_scenarios:
            batch = scenario['batch_size']
            if batch not in seen_batches or scenario['name'] in ['batch_norm_compatible', 'single_sample']:
                seen_batches[batch] = True
                filtered_scenarios.append(scenario)
        
        test_scenarios = filtered_scenarios
        
        # SYSTEM-AWARE PERFORMANCE ANALYSIS
        performance_metrics.update({
            'system_class': system_class,
            'cpu_count': cpu_count,
            'system_memory_gb': system_memory_gb,
            'gpu_available': gpu_available,
            'model_name': model_name,
            'validation_mode': validation_mode,
            'scenarios_tested': len(test_scenarios),
            'test_configuration': {
                'batch_size': batch_size,
                'input_dim': input_dim,
                'config_used': config_used is not None
            }
        })
        
        # Execute performance testing for each scenario
        scenario_results = {}
        validation_results = {}
        
        for scenario in test_scenarios:
            scenario_name = scenario['name']
            scenario_batch_size = scenario['batch_size']
            scenario_description = scenario['description']
            
            try:
                # SYSTEM-AWARE BATCH SIZE ADJUSTMENT
                test_batch_size = scenario_batch_size
                
                # Adjust based on available memory
                if system_memory_gb < 8 and test_batch_size > 64:
                    original_batch_size = test_batch_size
                    test_batch_size = min(64, test_batch_size)
                    performance_metrics[f'{scenario_name}_batch_size_adjusted'] = {
                        'original': original_batch_size,
                        'adjusted': test_batch_size,
                        'reason': 'limited_system_memory'
                    }
                
                # Adjust for batch normalization requirements
                if ((config_used and config_used.get('model', {}).get('use_batch_norm', False)) or 
                    (config_used and config_used.get('model', {}).get('normalization') == 'batch')) and test_batch_size == 1:
                    test_batch_size = 2
                    performance_metrics[f'{scenario_name}_adjusted_batch_size'] = test_batch_size
                
                # Create test input with optimal device placement
                test_input = torch.randn(test_batch_size, input_dim)
                
                # ENHANCED DEVICE MANAGEMENT
                if hasattr(model, 'device'):
                    test_input = test_input.to(model.device)
                    device = model.device
                elif hasattr(next(model.parameters(), None), 'device'):
                    device = next(model.parameters()).device
                    test_input = test_input.to(device)
                else:
                    device = torch.device('cuda' if gpu_available else 'cpu')
                    test_input = test_input.to(device)
                    model = model.to(device)
                
                # SYSTEM-AWARE WARMUP
                warmup_runs = 10 if system_class == 'high_performance' else 5
                timing_runs = 30 if system_class == 'high_performance' else 20
                
                # Warmup with memory monitoring
                memory_before_warmup = None
                if torch.cuda.is_available() and device.type == 'cuda':
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    memory_before_warmup = torch.cuda.memory_allocated(device) / 1024**2
                
                for _ in range(warmup_runs):
                    with torch.no_grad():
                        _ = model(test_input)
                        if torch.cuda.is_available() and device.type == 'cuda':
                            torch.cuda.synchronize()
                
                # ENHANCED TIMING with system-aware statistics
                times = []
                outputs = []
                memory_usage = []
                
                for _ in range(timing_runs):
                    # Memory monitoring
                    if torch.cuda.is_available() and device.type == 'cuda':
                        torch.cuda.synchronize()
                        mem_before = torch.cuda.memory_allocated(device) / 1024**2
                    
                    #start_time = time.time()
                    start_time = time.perf_counter()  # Use perf_counter for better precision
                    with torch.no_grad():
                        output = model(test_input)
                        if torch.cuda.is_available() and device.type == 'cuda':
                            torch.cuda.synchronize()
                    #end_time = time.time()
                    end_time = time.perf_counter()
                    
                    inference_time = end_time - start_time
                    
                    #times.append(end_time - start_time)
                    times.append(max(inference_time, 1e-6))  # Ensure minimum time to prevent division by zero
                    outputs.append(output)
                    
                    if torch.cuda.is_available() and device.type == 'cuda':
                        mem_after = torch.cuda.memory_allocated(device) / 1024**2
                        memory_usage.append(mem_after - mem_before)
                
                # ENHANCED STATISTICS with system context
                #avg_time = sum(times) / len(times)
                avg_time = max(sum(times) / len(times), 1e-6)  # Minimum 1 microsecond
                #std_time = (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5
                std_time = max((sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5, 1e-6)
                #min_time = min(times)
                min_time = max(min(times), 1e-6)
                #max_time = max(times)
                max_time = max(max(times), 1e-6)
                #throughput = test_batch_size / avg_time
                # Safe throughput calculation with bounds checking
                if avg_time > 0 and test_batch_size > 0:
                    throughput = test_batch_size / avg_time
                    time_per_sample = avg_time / test_batch_size
                else:
                    # Fallback values for extremely fast inference
                    throughput = 1000000.0  # 1M samples/sec as fallback for very fast models
                    time_per_sample = 1e-6  # 1 microsecond per sample as fallback
                    performance_metrics[f'{scenario_name}_timing_warning'] = "Inference too fast for accurate measurement"

                # Ensure throughput is reasonable (not infinite)
                throughput = min(throughput, 1e8)  # Cap at 100M samples/sec
                time_per_sample = max(time_per_sample, 1e-9)  # Minimum 1 nanosecond
                
                # Rate performance based on throughput and system class
                if system_class == 'high_performance':
                    if throughput > 10000: 
                        performance_rating = 'excellent'
                    elif throughput > 5000: 
                        performance_rating = 'good'
                    elif throughput > 1000: 
                        performance_rating = 'fair'
                    else: 
                        performance_rating = 'poor'
                elif system_class == 'limited':
                    if throughput > 500: 
                        performance_rating = 'excellent'
                    elif throughput > 100: 
                        performance_rating = 'good'
                    elif throughput > 50: 
                        performance_rating = 'fair'
                    else: 
                        performance_rating = 'poor'
                else:  # standard
                    if throughput > 2000: 
                        performance_rating = 'excellent'
                    elif throughput > 1000: 
                        performance_rating = 'good'
                    elif throughput > 500: 
                        performance_rating = 'fair'
                    else: 
                        performance_rating = 'poor'
                
                # Store comprehensive performance metrics
                scenario_metrics = {
                    f'{scenario_name}_inference_time_ms': avg_time * 1000,
                    f'{scenario_name}_inference_std_ms': std_time * 1000,
                    f'{scenario_name}_inference_min_ms': min_time * 1000,
                    f'{scenario_name}_inference_max_ms': max_time * 1000,
                    f'{scenario_name}_throughput_samples_per_sec': throughput,
                    f'{scenario_name}_batch_size': test_batch_size,
                    f'{scenario_name}_device_type': device.type,
                    f'{scenario_name}_performance_rating': performance_rating,
                }
                
                # Add memory metrics if available
                if memory_usage:
                    scenario_metrics.update({
                        f'{scenario_name}_avg_memory_usage_mb': sum(memory_usage) / len(memory_usage),
                        f'{scenario_name}_max_memory_usage_mb': max(memory_usage),
                        f'{scenario_name}_memory_efficiency': test_batch_size / max(memory_usage) if max(memory_usage) > 0 else 0
                    })
                
                performance_metrics.update(scenario_metrics)
                
                # ENHANCED OUTPUT QUALITY ANALYSIS for validation mode
                output = outputs[-1]
                quality_metrics = {}
                validation_status = 'passed'
                validation_errors = []
                validation_warnings = []
                
                # Basic quality checks
                has_nan = torch.isnan(output).any().item()
                has_inf = torch.isinf(output).any().item()
                
                quality_metrics[f'{scenario_name}_has_nan'] = has_nan
                quality_metrics[f'{scenario_name}_has_inf'] = has_inf
                
                if validation_mode:
                    # Validation-specific checks
                    expected_shape = (test_batch_size, input_dim)
                    if output.shape != expected_shape:
                        validation_errors.append(f'Shape mismatch - expected {expected_shape}, got {output.shape}')
                        validation_status = 'failed'
                    
                    if has_nan:
                        validation_errors.append('NaN values detected in output')
                        validation_status = 'failed'
                    
                    if has_inf:
                        validation_errors.append('Infinite values detected in output')
                        validation_status = 'failed'
                
                if not has_nan and not has_inf:
                    # Enhanced statistics
                    output_range = (output.max() - output.min()).item()
                    output_mean = output.mean().item()
                    output_std = output.std().item()
                    output_median = output.median().item()
                    
                    # Detect potential issues
                    quality_issues = []
                    if output_std < 1e-6:
                        quality_issues.append('very_low_variance')
                        if validation_mode:
                            validation_warnings.append('Output has very small dynamic range')
                    
                    if abs(output_mean) > 100:
                        quality_issues.append('high_magnitude_values')
                        if validation_mode:
                            validation_warnings.append('Extreme output values detected')
                    
                    if output_range > 1000:
                        quality_issues.append('extreme_value_range')
                        if validation_mode:
                            validation_warnings.append('Output has very large value range')
                    
                    # Calculate quality score directly
                    quality_score = 1.0
                    
                    # Penalize extreme values
                    if abs(output_mean) > 10:
                        quality_score *= 0.8
                    if output_range > 100:
                        quality_score *= 0.7
                    if output_std < 1e-6:
                        quality_score *= 0.5
                    
                    # Ensure score is within valid range
                    quality_score = max(0.0, min(1.0, quality_score))
                    
                    quality_metrics.update({
                        f'{scenario_name}_output_range': output_range,
                        f'{scenario_name}_output_mean': output_mean,
                        f'{scenario_name}_output_std': output_std,
                        f'{scenario_name}_output_median': output_median,
                        f'{scenario_name}_quality_issues': quality_issues,
                        f'{scenario_name}_quality_score': quality_score
                    })
                
                performance_metrics.update(quality_metrics)
                
                # Store validation results if in validation mode
                if validation_mode:
                    validation_results[scenario_name] = {
                        'status': validation_status,
                        'errors': validation_errors,
                        'warnings': validation_warnings,
                        'batch_size_used': test_batch_size,
                        'output_shape': list(output.shape),
                        'device_used': str(device),
                        'scenario_description': scenario_description
                    }
                
                scenario_results[scenario_name] = {
                    'throughput': throughput,
                    'latency_ms': avg_time * 1000,
                    'performance_rating': performance_rating,
                    'quality_score': quality_metrics.get(f'{scenario_name}_quality_score', 0),
                    'validation_status': validation_status if validation_mode else 'not_tested'
                }
                
            except Exception as scenario_error:
                error_msg = f"{scenario_name}: {str(scenario_error)}"
                logger.warning(f"{model_name} performance test failed for {scenario_name}: {scenario_error}")
                performance_metrics[f'{scenario_name}_error'] = error_msg
                performance_metrics[f'{scenario_name}_inference_time_ms'] = float('nan')
                performance_metrics[f'{scenario_name}_throughput_samples_per_sec'] = float('nan')
                
                if validation_mode:
                    validation_results[scenario_name] = {
                        'status': 'failed',
                        'errors': [error_msg],
                        'warnings': [],
                        'scenario_description': scenario_description
                    }
                
                scenario_results[scenario_name] = {
                    'error': error_msg,
                    'validation_status': 'failed' if validation_mode else 'error'
                }
        
        # Generate comprehensive recommendations
        recommendations = []
        
        # Get average throughput
        throughputs = [v for k, v in performance_metrics.items() if k.endswith('_throughput_samples_per_sec') and not math.isnan(v)]
        if throughputs:
            avg_throughput = sum(throughputs) / len(throughputs)
            
            if avg_throughput < 100 and system_class != 'limited':
                recommendations.append("Performance is below expected for system class - consider model optimization")
            
            if system_class == 'high_performance' and avg_throughput < 1000:
                recommendations.append("High-performance system underutilized - enable mixed precision or larger batch sizes")
        
        # GPU-specific recommendations
        if gpu_available:
            gpu_memory_gb = hardware_data.get('gpu_memory_gb', 0)
            if gpu_memory_gb > 8:
                recommendations.append("Large GPU memory available - consider using larger batch sizes")
            elif gpu_memory_gb < 4:
                recommendations.append("Limited GPU memory - use gradient accumulation instead of large batch sizes")
        
        performance_metrics['recommendations'] = recommendations
        
        # Add validation-specific summary if in validation mode
        if validation_mode:
            passed_scenarios = [name for name, result in validation_results.items() if result['status'] == 'passed']
            failed_scenarios = [name for name, result in validation_results.items() if result['status'] == 'failed']
            
            performance_metrics['validation_summary'] = {
                'total_scenarios': len(validation_results),
                'passed_scenarios': len(passed_scenarios),
                'failed_scenarios': len(failed_scenarios),
                'success_rate': len(passed_scenarios) / max(len(validation_results), 1) * 100,
                'validation_results': validation_results
            }
        
        # Add scenario summary
        performance_metrics['scenario_summary'] = scenario_results
        
        return performance_metrics
        
    except Exception as e:
        logger.error(f"Performance testing failed for {model_name}: {e}")
        return {
            'error': f'Performance testing failed: {str(e)}',
            'model_name': model_name,
            'analysis_status': 'failed'
        }

def estimate_flops(
    input_dim: int,
    batch_size: int,
    model: torch.nn.Module
) -> Dict[str, Any]:
    """
    Comprehensive FLOP estimation with detailed analysis and breakdown.
    
    This function provides enhanced FLOP estimation including forward pass,
    backward pass, layer-wise breakdown, and computational complexity classification.
    Harmonized with compare_model_architectures() parameter structure.
    
    Args:
        input_dim: Input dimension size
        batch_size: Batch size for calculation
        model: PyTorch model to analyze
        
    Returns:
        Dictionary containing comprehensive FLOP analysis
    """
    flop_start_time = time.time()
    
    try:
        flop_analysis = {
            'forward_pass': {
                'total_flops': 0,
                'layer_breakdown': [],
                'operation_breakdown': {},
                'per_sample_flops': 0
            },
            'backward_pass': {
                'estimated_flops': 0,
                'gradient_computation_flops': 0,
                'parameter_update_flops': 0
            },
            'training_flops': {
                'total_per_step': 0,
                'per_epoch_estimate': 0,
                'optimizer_overhead': 0
            },
            'complexity_metrics': {
                'flops_per_parameter': 0,
                'computational_density': 0,
                'complexity_class': 'unknown',
                'efficiency_rating': 'unknown'
            },
            'comparative_metrics': {
                'flops_vs_mobilenet': 0,
                'flops_vs_resnet18': 0,
                'relative_complexity': 'unknown'
            },
            'optimization_analysis': {
                'bottleneck_layers': [],
                'optimization_opportunities': [],
                'memory_compute_ratio': 0
            },
            'analysis_metadata': {
                'estimation_method': 'comprehensive_layer_analysis',
                'accuracy_level': 'high',
                'analysis_time_seconds': 0,
                'warnings': []
            }
        }
        
        logger.debug(f"Starting comprehensive FLOP estimation for model with {sum(p.numel() for p in model.parameters())} parameters")
        
        # Phase 1: Detailed Layer-by-Layer Analysis
        model.eval()
        layer_details = []
        total_forward_flops = 0
        operation_counts = {
            'linear': 0,
            'convolution': 0,
            'activation': 0,
            'normalization': 0,
            'attention': 0,
            'dropout': 0,
            'other': 0
        }
        
        # Track tensor dimensions through the network
        current_shape = (batch_size, input_dim)
        
        for name, module in model.named_modules():
            if name == '':  # Skip root module
                continue
            
            layer_flops = 0
            layer_info = {
                'name': name,
                'type': type(module).__name__,
                'input_shape': current_shape,
                'output_shape': None,
                'flops': 0,
                'parameters': sum(p.numel() for p in module.parameters()),
                'trainable_params': sum(p.numel() for p in module.parameters() if p.requires_grad),
                'operation_class': 'other',
                'computational_intensity': 'low',
                'memory_access': 0,
                'arithmetic_intensity': 0
            }
            
            try:
                if isinstance(module, torch.nn.Linear):
                    # Linear layer: input_features × output_features × batch_size (MAC operations)
                    in_features = module.in_features
                    out_features = module.out_features
                    
                    # Multiply-accumulate operations (2 FLOPs each: multiply + add)
                    mac_operations = in_features * out_features * batch_size
                    layer_flops = mac_operations * 2
                    
                    # Bias addition if present
                    if module.bias is not None:
                        layer_flops += out_features * batch_size
                    
                    # Update shape tracking
                    current_shape = (batch_size, out_features)
                    layer_info['output_shape'] = current_shape
                    layer_info['operation_class'] = 'linear'
                    layer_info['computational_intensity'] = 'high'
                    
                    # Memory access estimation (weights + input + output)
                    layer_info['memory_access'] = (in_features * out_features + 
                                                 in_features * batch_size + 
                                                 out_features * batch_size) * 4  # bytes
                    
                    # Arithmetic intensity (FLOPs per byte)
                    layer_info['arithmetic_intensity'] = layer_flops / max(layer_info['memory_access'], 1)
                    
                    operation_counts['linear'] += layer_flops
                    
                    layer_info.update({
                        'in_features': in_features,
                        'out_features': out_features,
                        'has_bias': module.bias is not None,
                        'weight_flops': mac_operations * 2,
                        'bias_flops': out_features * batch_size if module.bias is not None else 0
                    })
                    
                elif isinstance(module, torch.nn.Conv1d):
                    # 1D Convolution analysis
                    kernel_size = module.kernel_size[0] if isinstance(module.kernel_size, tuple) else module.kernel_size
                    in_channels = module.in_channels
                    out_channels = module.out_channels
                    padding = module.padding[0] if isinstance(module.padding, tuple) else module.padding
                    stride = module.stride[0] if isinstance(module.stride, tuple) else module.stride
                    
                    # Calculate output length
                    input_length = current_shape[-1] if len(current_shape) > 2 else input_dim
                    output_length = (input_length + 2 * padding - kernel_size) // stride + 1
                    
                    # Convolution FLOPs: kernel_size × in_channels × out_channels × output_length × batch_size
                    conv_operations = kernel_size * in_channels * out_channels * output_length * batch_size
                    layer_flops = conv_operations * 2  # MAC operations
                    
                    if module.bias is not None:
                        layer_flops += out_channels * output_length * batch_size
                    
                    current_shape = (batch_size, out_channels, output_length)
                    layer_info['output_shape'] = current_shape
                    layer_info['operation_class'] = 'convolution'
                    layer_info['computational_intensity'] = 'high'
                    
                    operation_counts['convolution'] += layer_flops
                    
                elif isinstance(module, torch.nn.Conv2d):
                    # 2D Convolution analysis
                    kernel_h, kernel_w = (module.kernel_size if isinstance(module.kernel_size, tuple) 
                                        else (module.kernel_size, module.kernel_size))
                    in_channels = module.in_channels
                    out_channels = module.out_channels
                    
                    # Estimate spatial dimensions (simplified)
                    spatial_dim = int(input_dim ** 0.5)
                    output_h = output_w = spatial_dim  # Simplified
                    
                    conv_operations = (kernel_h * kernel_w * in_channels * out_channels * 
                                     output_h * output_w * batch_size)
                    layer_flops = conv_operations * 2
                    
                    if module.bias is not None:
                        layer_flops += out_channels * output_h * output_w * batch_size
                    
                    layer_info['operation_class'] = 'convolution'
                    layer_info['computational_intensity'] = 'very_high'
                    operation_counts['convolution'] += layer_flops
                    
                elif isinstance(module, (torch.nn.BatchNorm1d, torch.nn.LayerNorm)):
                    # Normalization layers
                    if hasattr(module, 'num_features'):
                        num_features = module.num_features
                    elif hasattr(module, 'normalized_shape'):
                        num_features = (module.normalized_shape[0] if isinstance(module.normalized_shape, tuple) 
                                      else module.normalized_shape)
                    else:
                        num_features = current_shape[-1] if current_shape else input_dim
                    
                    # Normalization operations:
                    # 1. Mean calculation: num_features * batch_size
                    # 2. Variance calculation: num_features * batch_size
                    # 3. Normalization: 4 * num_features * batch_size (subtract, divide, scale, shift)
                    layer_flops = 6 * num_features * batch_size
                    
                    layer_info['operation_class'] = 'normalization'
                    layer_info['computational_intensity'] = 'medium'
                    operation_counts['normalization'] += layer_flops
                    
                    layer_info.update({
                        'num_features': num_features,
                        'affine': getattr(module, 'affine', True)
                    })
                    
                elif isinstance(module, torch.nn.GroupNorm):
                    # Group normalization
                    num_groups = module.num_groups
                    num_channels = module.num_channels
                    
                    # Group normalization is more complex than batch norm
                    layer_flops = 8 * num_channels * batch_size
                    
                    layer_info['operation_class'] = 'normalization'
                    layer_info['computational_intensity'] = 'medium'
                    operation_counts['normalization'] += layer_flops
                    
                elif isinstance(module, torch.nn.Dropout):
                    # Dropout: minimal cost during inference (0 in eval mode)
                    layer_flops = 0
                    layer_info['operation_class'] = 'dropout'
                    layer_info['computational_intensity'] = 'none'
                    operation_counts['dropout'] += layer_flops
                    
                elif isinstance(module, (torch.nn.ReLU, torch.nn.LeakyReLU, torch.nn.ELU, 
                                        torch.nn.GELU, torch.nn.Sigmoid, torch.nn.Tanh)):
                    # Activation functions
                    estimated_neurons = current_shape[-1] if current_shape else input_dim
                    
                    if isinstance(module, torch.nn.ReLU):
                        layer_flops = estimated_neurons * batch_size  # Comparison operation
                    elif isinstance(module, torch.nn.LeakyReLU):
                        layer_flops = estimated_neurons * batch_size * 2  # Comparison + multiplication
                    elif isinstance(module, torch.nn.ELU):
                        layer_flops = estimated_neurons * batch_size * 4  # Exponential + conditional
                    elif isinstance(module, torch.nn.GELU):
                        layer_flops = estimated_neurons * batch_size * 8  # Complex approximation
                    elif isinstance(module, torch.nn.Sigmoid):
                        layer_flops = estimated_neurons * batch_size * 4  # Exponential + division
                    elif isinstance(module, torch.nn.Tanh):
                        layer_flops = estimated_neurons * batch_size * 6  # Hyperbolic function
                    else:
                        layer_flops = estimated_neurons * batch_size  # Default
                    
                    layer_info['operation_class'] = 'activation'
                    layer_info['computational_intensity'] = 'low' if layer_flops <= estimated_neurons * batch_size else 'medium'
                    operation_counts['activation'] += layer_flops
                    
                    layer_info['activation_type'] = type(module).__name__
                    
                elif hasattr(module, '__class__') and 'Attention' in module.__class__.__name__:
                    # Multi-head attention (custom implementation)
                    if hasattr(module, 'dim'):
                        dim = module.dim
                        num_heads = getattr(module, 'num_heads', 8)
                        seq_len = 1  # Assuming single token for autoencoder
                        
                        # Attention FLOPs: Q, K, V projections + attention computation + output projection
                        projection_flops = 3 * (dim * dim * batch_size)  # Q, K, V
                        attention_flops = (dim * seq_len * seq_len * batch_size * num_heads +  # Attention scores
                                         dim * seq_len * seq_len * batch_size * num_heads)  # Value combination
                        output_projection_flops = dim * dim * batch_size
                        
                        layer_flops = projection_flops + attention_flops + output_projection_flops
                        
                        layer_info['operation_class'] = 'attention'
                        layer_info['computational_intensity'] = 'very_high'
                        operation_counts['attention'] += layer_flops
                        
                        layer_info.update({
                            'attention_dim': dim,
                            'num_heads': num_heads,
                            'sequence_length': seq_len
                        })
                
                # Handle custom blocks (ResidualBlock, etc.)
                elif 'Block' in type(module).__name__ or 'Residual' in type(module).__name__:
                    # For custom blocks, estimate based on constituent operations
                    block_params = sum(p.numel() for p in module.parameters())
                    # Rough estimate: 2 FLOPs per parameter for complex blocks
                    layer_flops = block_params * 2 * batch_size
                    
                    layer_info['operation_class'] = 'other'
                    layer_info['computational_intensity'] = 'high'
                    operation_counts['other'] += layer_flops
                
                # Update layer information
                layer_info['flops'] = layer_flops
                layer_details.append(layer_info)
                total_forward_flops += layer_flops
                
                # Track computational bottlenecks
                if layer_flops > total_forward_flops * 0.1:  # More than 10% of total
                    flop_analysis['optimization_analysis']['bottleneck_layers'].append({
                        'layer_name': name,
                        'layer_type': type(module).__name__,
                        'flops': layer_flops,
                        'percentage_of_total': (layer_flops / max(total_forward_flops, 1)) * 100
                    })
                
            except Exception as e:
                logger.debug(f"FLOP estimation failed for layer {name}: {e}")
                layer_info['estimation_error'] = str(e)
                layer_details.append(layer_info)
        
        # Phase 2: Forward Pass Analysis
        flop_analysis['forward_pass'].update({
            'total_flops': total_forward_flops,
            'layer_breakdown': layer_details,
            'operation_breakdown': operation_counts,
            'per_sample_flops': total_forward_flops / max(batch_size, 1)
        })
        
        # Phase 3: Backward Pass Estimation
        # Backward pass typically requires 2x forward pass FLOPs for gradients
        gradient_flops = total_forward_flops * 2
        
        # Parameter update FLOPs (optimizer dependent)
        total_params = sum(p.numel() for p in model.parameters())
        # Adam optimizer: ~5 operations per parameter (momentum, variance, bias correction, update)
        optimizer_flops = total_params * 5
        
        flop_analysis['backward_pass'].update({
            'estimated_flops': gradient_flops,
            'gradient_computation_flops': gradient_flops,
            'parameter_update_flops': optimizer_flops
        })
        
        # Phase 4: Training FLOPs
        total_training_flops = total_forward_flops + gradient_flops + optimizer_flops
        
        # Estimate per-epoch FLOPs (assuming 1000 samples per epoch)
        samples_per_epoch = 1000
        steps_per_epoch = max(1, samples_per_epoch // batch_size)
        epoch_flops = total_training_flops * steps_per_epoch
        
        flop_analysis['training_flops'].update({
            'total_per_step': total_training_flops,
            'per_epoch_estimate': epoch_flops,
            'optimizer_overhead': optimizer_flops
        })
        
        # Phase 5: Complexity Metrics
        flops_per_param = total_forward_flops / max(total_params, 1)
        
        # Computational density (FLOPs per MB of model)
        model_size_mb = total_params * 4 / (1024 * 1024)
        computational_density = total_forward_flops / max(model_size_mb, 0.001)
        
        # Classify computational complexity
        def _classify_computational_complexity(flops: int) -> str:
            """Classify computational complexity based on FLOP count."""
            if flops < 1e3:
                return 'micro'
            elif flops < 1e4:
                return 'ultra_low'
            elif flops < 1e5:
                return 'very_low'
            elif flops < 1e6:
                return 'low'
            elif flops < 1e7:
                return 'low_medium'
            elif flops < 1e8:
                return 'medium'
            elif flops < 5e8:
                return 'medium_high'
            elif flops < 1e9:
                return 'high'
            elif flops < 5e9:
                return 'very_high'
            elif flops < 1e10:
                return 'ultra_high'
            elif flops < 1e11:
                return 'extreme'
            elif flops < 1e12:
                return 'super_extreme'
            else:
                return 'massive'
        
        complexity_class = _classify_computational_complexity(total_forward_flops)
        
        # Efficiency rating based on FLOPs per parameter
        if flops_per_param < 10:
            efficiency_rating = 'excellent'
        elif flops_per_param < 50:
            efficiency_rating = 'good'
        elif flops_per_param < 200:
            efficiency_rating = 'moderate'
        else:
            efficiency_rating = 'poor'
        
        flop_analysis['complexity_metrics'].update({
            'flops_per_parameter': flops_per_param,
            'computational_density': computational_density,
            'complexity_class': complexity_class,
            'efficiency_rating': efficiency_rating
        })
        
        # Phase 6: Comparative Analysis
        # Compare with standard architectures (rough estimates)
        mobilenet_flops = 569_000_000  # MobileNetV2
        resnet18_flops = 1_814_000_000  # ResNet-18
        
        flop_analysis['comparative_metrics'].update({
            'flops_vs_mobilenet': total_forward_flops / mobilenet_flops,
            'flops_vs_resnet18': total_forward_flops / resnet18_flops,
            'relative_complexity': 'lightweight' if total_forward_flops < mobilenet_flops else 
                                  'moderate' if total_forward_flops < resnet18_flops else 'heavy'
        })
        
        # Phase 7: Optimization Analysis
        optimization_opportunities = []
        
        # Check for inefficient operations
        if operation_counts['activation'] > total_forward_flops * 0.1:
            optimization_opportunities.append('Consider using more efficient activation functions')
        
        if operation_counts['normalization'] > total_forward_flops * 0.2:
            optimization_opportunities.append('Normalization overhead is significant - consider layer normalization')
        
        if len(flop_analysis['optimization_analysis']['bottleneck_layers']) > 0:
            optimization_opportunities.append('Focus optimization on identified bottleneck layers')
        
        # Memory-compute ratio
        total_memory_accesses = sum(layer['memory_access'] for layer in layer_details 
                                  if 'memory_access' in layer and layer['memory_access'] > 0)
        memory_compute_ratio = total_memory_accesses / max(total_forward_flops, 1)
        
        flop_analysis['optimization_analysis'].update({
            'optimization_opportunities': optimization_opportunities,
            'memory_compute_ratio': memory_compute_ratio
        })
        
        # Phase 8: Validation and Warnings
        warnings = []
        
        if total_forward_flops == 0:
            warnings.append('No FLOPs computed - model may be empty or estimation failed')
            # Fallback estimation
            total_forward_flops = total_params * 2 * batch_size
            flop_analysis['forward_pass']['total_flops'] = total_forward_flops
            warnings.append(f'Used fallback estimation: {total_forward_flops} FLOPs')
        
        if len(layer_details) == 0:
            warnings.append('No layers analyzed - check model structure')
        
        if total_forward_flops < 1000:
            warnings.append('Very low FLOP count - verify model complexity')
        
        # Final metadata
        flop_analysis['analysis_metadata'].update({
            'analysis_time_seconds': time.time() - flop_start_time,
            'total_layers_analyzed': len(layer_details),
            'total_parameters': total_params,
            'batch_size_used': batch_size,
            'warnings': warnings
        })
        
        logger.debug(f"Comprehensive FLOP estimation completed: {total_forward_flops:,} FLOPs "
                    f"for {len(layer_details)} layers in {time.time() - flop_start_time:.3f}s")
        
        return flop_analysis
        
    except Exception as e:
        logger.error(f"Comprehensive FLOP estimation failed: {e}")
        
        # Fallback estimation
        try:
            total_params = sum(p.numel() for p in model.parameters())
            fallback_flops = total_params * 2 * batch_size
            
            return {
                'forward_pass': {
                    'total_flops': fallback_flops,
                    'estimation_method': 'fallback'
                },
                'error': str(e),
                'fallback_used': True,
                'analysis_metadata': {
                    'estimation_method': 'parameter_based_fallback',
                    'accuracy_level': 'low',
                    'warnings': [f'Main estimation failed: {str(e)}', 'Using parameter-based fallback']
                }
            }
        except Exception as fallback_error:
            return {
                'error': f'Both main and fallback FLOP estimation failed: {str(e)}, {str(fallback_error)}',
                'forward_pass': {'total_flops': 10000},  # Minimal fallback
                'analysis_metadata': {
                    'estimation_method': 'minimal_fallback',
                    'accuracy_level': 'none',
                    'warnings': ['Complete estimation failure']
                }
            }

def analyze_scaling_behavior(
    model_class: type, 
    base_params: Dict[str, Any], 
    input_dim: int, 
    model_name: str
) -> Dict[str, Any]:
    """
    Comprehensive analysis of model scaling behavior with enhanced metrics and predictions.
    
    This function analyzes how models scale with different input dimensions, batch sizes,
    and architectural parameters, providing detailed scaling curves, performance predictions,
    and resource requirement forecasts. Harmonized with compare_model_architectures().
    
    Args:
        model_class: Model class to analyze
        base_params: Base parameters for model instantiation
        input_dim: Base input dimension
        model_name: Name of the model for logging
        
    Returns:
        Dictionary containing comprehensive scaling analysis
    """
    scaling_start_time = time.time()
    
    try:
        scaling_analysis = {
            'input_dimension_scaling': {
                'data_points': [],
                'scaling_coefficients': {},
                'performance_curves': {},
                'memory_curves': {},
                'recommendations': []
            },
            'batch_size_scaling': {
                'throughput_analysis': [],
                'memory_scaling': [],
                'efficiency_analysis': {},
                'optimal_batch_sizes': {}
            },
            'parameter_scaling': {
                'complexity_growth': 'unknown',
                'efficiency_trends': {},
                'scalability_limits': {},
                'architectural_impact': {}
            },
            'performance_predictions': {
                'inference_time_model': {},
                'memory_usage_model': {},
                'training_time_model': {},
                'confidence_intervals': {}
            },
            'resource_scaling': {
                'cpu_scaling': {},
                'memory_scaling': {},
                'gpu_utilization': {},
                'power_consumption_estimates': {}
            },
            'optimization_recommendations': {
                'scaling_efficiency': [],
                'bottleneck_predictions': [],
                'hardware_recommendations': [],
                'configuration_suggestions': []
            },
            'analysis_metadata': {
                'total_models_tested': 0,
                'successful_tests': 0,
                'failed_tests': 0,
                'analysis_time_seconds': 0,
                'scaling_dimensions_tested': [],
                'warnings': []
            }
        }
        
        logger.debug(f"Starting comprehensive scaling analysis for {model_name}")
        
        # Phase 1: Input Dimension Scaling Analysis
        logger.debug("Phase 1: Input dimension scaling analysis")
        
        # Define comprehensive test dimensions
        dimension_multipliers = [0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0, 4.0]
        test_input_dims = [max(5, int(input_dim * mult)) for mult in dimension_multipliers]
        
        # Add some specific interesting dimensions
        test_input_dims.extend([10, 20, 50, 100, 200, 500])
        test_input_dims = sorted(list(set(test_input_dims)))  # Remove duplicates and sort
        
        # Limit test dimensions to prevent excessive testing
        if len(test_input_dims) > 10:
            test_input_dims = test_input_dims[::max(1, len(test_input_dims)//10)]
        
        input_scaling_data = []
        successful_dim_tests = 0
        
        for test_dim in test_input_dims:
            dim_start_time = time.time()
            try:
                # Create test parameters with appropriate scaling
                test_params = base_params.copy()
                if 'config' in test_params and 'model' in test_params['config']:
                    test_model_config = test_params['config']['model'].copy()
                    
                    # Scale encoding dimension proportionally
                    original_encoding = test_model_config.get('encoding_dim', 16)
                    scaled_encoding = max(4, min(original_encoding, test_dim // 3))
                    test_model_config['encoding_dim'] = scaled_encoding
                    
                    # Scale hidden dimensions proportionally
                    if 'hidden_dims' in test_model_config:
                        original_hidden = test_model_config['hidden_dims']
                        if isinstance(original_hidden, list):
                            scale_factor = min(2.0, test_dim / max(input_dim, 1))
                            scaled_hidden = [max(8, int(dim * scale_factor)) for dim in original_hidden]
                            test_model_config['hidden_dims'] = scaled_hidden
                    
                    test_params['config']['model'] = test_model_config
                    test_params['input_dim'] = test_dim
                
                # Create and analyze test model
                test_model = model_class(**test_params)
                test_model.eval()
                
                # Basic metrics
                param_count = sum(p.numel() for p in test_model.parameters())
                model_size_mb = param_count * 4 / (1024 * 1024)
                
                # Performance testing with multiple batch sizes
                performance_results = []
                test_batch_sizes = [1, 4, 16, 32] if test_dim <= 1000 else [1, 4, 16]
                
                for test_batch in test_batch_sizes:
                    try:
                        test_input = torch.randn(test_batch, test_dim)
                        
                        # Warmup
                        with torch.no_grad():
                            _ = test_model(test_input)
                        
                        # Timing test
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        batch_start_time = time.time()
                        
                        with torch.no_grad():
                            _ = test_model(test_input)
                        
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        batch_time = time.time() - batch_start_time
                        
                        performance_results.append({
                            'batch_size': test_batch,
                            'inference_time_ms': batch_time * 1000,
                            'samples_per_second': test_batch / batch_time,
                            'time_per_sample_ms': (batch_time / test_batch) * 1000
                        })
                        
                    except Exception as batch_error:
                        logger.debug(f"Batch test failed for dim={test_dim}, batch={test_batch}: {batch_error}")
                
                # Memory analysis
                memory_metrics = {}
                try:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        memory_before = torch.cuda.memory_allocated()
                        
                        test_model_gpu = test_model.cuda()
                        test_input_gpu = torch.randn(16, test_dim).cuda()
                        
                        with torch.no_grad():
                            _ = test_model_gpu(test_input_gpu)
                        
                        memory_after = torch.cuda.memory_allocated()
                        gpu_memory_mb = (memory_after - memory_before) / (1024 * 1024)
                        
                        memory_metrics['gpu_memory_mb'] = gpu_memory_mb
                        memory_metrics['memory_per_param'] = gpu_memory_mb / max(param_count, 1) * 1024 * 1024
                        
                        # Clean up
                        del test_model_gpu, test_input_gpu
                        torch.cuda.empty_cache()
                        
                except Exception as memory_error:
                    logger.debug(f"GPU memory analysis failed for dim={test_dim}: {memory_error}")
                
                # FLOP estimation
                try:
                    flop_result = estimate_flops(test_dim, 16, test_model)
                    if isinstance(flop_result, dict):
                        total_flops = flop_result.get('forward_pass', {}).get('total_flops', 0)
                    else:
                        total_flops = flop_result
                    flop_efficiency = total_flops / max(param_count, 1)
                except Exception as flop_error:
                    logger.debug(f"FLOP estimation failed for dim={test_dim}: {flop_error}")
                    total_flops = param_count * 2  # Fallback
                    flop_efficiency = 2.0
                
                # Compile dimension scaling data point
                dim_data_point = {
                    'input_dimension': test_dim,
                    'parameter_count': param_count,
                    'model_size_mb': model_size_mb,
                    'total_flops': total_flops,
                    'flops_per_param': flop_efficiency,
                    'performance_results': performance_results,
                    'memory_metrics': memory_metrics,
                    'analysis_time_ms': (time.time() - dim_start_time) * 1000,
                    'scaling_factor': test_dim / input_dim
                }
                
                # Add best performance metrics for easier analysis
                if performance_results:
                    best_perf = max(performance_results, key=lambda x: x['samples_per_second'])
                    dim_data_point.update({
                        'best_throughput_sps': best_perf['samples_per_second'],
                        'best_batch_size': best_perf['batch_size'],
                        'fastest_inference_ms': min(p['time_per_sample_ms'] for p in performance_results)
                    })
                
                input_scaling_data.append(dim_data_point)
                successful_dim_tests += 1
                scaling_analysis['analysis_metadata']['total_models_tested'] += 1
                
                # Clean up
                del test_model
                
            except Exception as dim_error:
                logger.debug(f"Dimension scaling test failed for dim={test_dim}: {dim_error}")
                scaling_analysis['analysis_metadata']['failed_tests'] += 1
                continue
        
        scaling_analysis['input_dimension_scaling']['data_points'] = input_scaling_data
        scaling_analysis['analysis_metadata']['successful_tests'] += successful_dim_tests
        
        # Phase 2: Analyze Input Dimension Scaling Patterns
        if len(input_scaling_data) >= 3:  # Need minimum data points for analysis
            try:
                # Extract scaling metrics
                dims = [dp['input_dimension'] for dp in input_scaling_data]
                params = [dp['parameter_count'] for dp in input_scaling_data]
                flops = [dp['total_flops'] for dp in input_scaling_data]
                throughputs = [dp.get('best_throughput_sps', 0) for dp in input_scaling_data]
                
                # Calculate scaling coefficients using simple regression
                # Parameter scaling: params = a * dims^b
                log_dims = np.log(np.array(dims))
                log_params = np.log(np.array(params))
                param_coeffs = np.polyfit(log_dims, log_params, 1)
                param_scaling_exp = param_coeffs[0]  # Exponent
                
                # FLOP scaling
                log_flops = np.log(np.array(flops))
                flop_coeffs = np.polyfit(log_dims, log_flops, 1)
                flop_scaling_exp = flop_coeffs[0]
                
                # Performance scaling (inverse relationship expected)
                valid_throughputs = [t for t in throughputs if t > 0]
                if len(valid_throughputs) >= 3:
                    log_throughputs = np.log(np.array(valid_throughputs))
                    perf_dims = [dims[i] for i, t in enumerate(throughputs) if t > 0]
                    log_perf_dims = np.log(np.array(perf_dims))
                    perf_coeffs = np.polyfit(log_perf_dims, log_throughputs, 1)
                    perf_scaling_exp = perf_coeffs[0]
                else:
                    perf_scaling_exp = -1.0  # Default assumption
                
                # Classify scaling complexity directly
                def _classify_scaling_complexity(scaling_exponent: float) -> str:
                    """Classify scaling complexity based on scaling exponent."""
                    try:
                        if scaling_exponent <= 1.2:
                            return 'linear'
                        elif scaling_exponent <= 1.8:
                            return 'sub_quadratic'
                        elif scaling_exponent <= 2.5:
                            return 'quadratic'
                        elif scaling_exponent <= 3.5:
                            return 'cubic'
                        else:
                            return 'super_cubic'
                    except:
                        return 'unknown'
                
                scaling_analysis['input_dimension_scaling']['scaling_coefficients'] = {
                    'parameter_scaling_exponent': float(param_scaling_exp),
                    'flop_scaling_exponent': float(flop_scaling_exp),
                    'performance_scaling_exponent': float(perf_scaling_exp),
                    'parameter_complexity_class': _classify_scaling_complexity(param_scaling_exp),
                    'computational_complexity_class': _classify_scaling_complexity(flop_scaling_exp)
                }
                
                # Generate performance curves for prediction
                scaling_analysis['input_dimension_scaling']['performance_curves'] = {
                    'parameter_growth': f"O(n^{param_scaling_exp:.2f})",
                    'flop_growth': f"O(n^{flop_scaling_exp:.2f})",
                    'throughput_decay': f"O(n^{perf_scaling_exp:.2f})",
                    'memory_growth_estimate': f"O(n^{max(1.0, param_scaling_exp):.2f})"
                }
                
                # Memory scaling analysis
                gpu_memories = [dp['memory_metrics'].get('gpu_memory_mb', 0) for dp in input_scaling_data]
                valid_memories = [(dims[i], mem) for i, mem in enumerate(gpu_memories) if mem > 0]
                
                if len(valid_memories) >= 3:
                    mem_dims, mem_values = zip(*valid_memories)
                    log_mem_dims = np.log(np.array(mem_dims))
                    log_mem_values = np.log(np.array(mem_values))
                    mem_coeffs = np.polyfit(log_mem_dims, log_mem_values, 1)
                    memory_scaling_exp = mem_coeffs[0]
                    
                    scaling_analysis['input_dimension_scaling']['memory_curves'] = {
                        'memory_scaling_exponent': float(memory_scaling_exp),
                        'memory_growth': f"O(n^{memory_scaling_exp:.2f})"
                    }
                
            except Exception as e:
                logger.debug(f"Scaling coefficient analysis failed: {e}")
                scaling_analysis['analysis_metadata']['warnings'].append(f"Scaling analysis failed: {str(e)}")
        
        # Phase 3: Batch Size Scaling Analysis
        logger.debug("Phase 3: Batch size scaling analysis")
        
        try:
            # Test different batch sizes with the base input dimension
            test_batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]
            batch_scaling_data = []
            
            # Create base model for batch testing
            base_test_model = model_class(**base_params)
            base_test_model.eval()
            
            for batch_size in test_batch_sizes:
                try:
                    # Memory constraint check - skip large batches if input is very large
                    if input_dim * batch_size > 100000:  # Arbitrary threshold
                        continue
                    
                    test_input = torch.randn(batch_size, input_dim)
                    
                    # Warmup
                    with torch.no_grad():
                        _ = base_test_model(test_input)
                    
                    # Timing test
                    num_runs = max(3, 20 // batch_size)  # More runs for smaller batches
                    times = []
                    
                    for _ in range(num_runs):
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        start_time = time.time()
                        
                        with torch.no_grad():
                            _ = base_test_model(test_input)
                        
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        times.append(time.time() - start_time)
                    
                    avg_time = sum(times) / len(times)
                    throughput = batch_size / avg_time
                    time_per_sample = avg_time / batch_size
                    
                    # Memory measurement
                    memory_usage = 0
                    if torch.cuda.is_available():
                        try:
                            torch.cuda.empty_cache()
                            mem_before = torch.cuda.memory_allocated()
                            
                            test_model_gpu = base_test_model.cuda() if not next(base_test_model.parameters()).is_cuda else base_test_model
                            test_input_gpu = test_input.cuda()
                            
                            with torch.no_grad():
                                _ = test_model_gpu(test_input_gpu)
                            
                            mem_after = torch.cuda.memory_allocated()
                            memory_usage = (mem_after - mem_before) / (1024 * 1024)  # MB
                            
                            del test_input_gpu
                            torch.cuda.empty_cache()
                            
                        except Exception as mem_error:
                            logger.debug(f"GPU memory measurement failed for batch={batch_size}: {mem_error}")
                    
                    batch_data = {
                        'batch_size': batch_size,
                        'avg_inference_time_ms': avg_time * 1000,
                        'throughput_sps': throughput,
                        'time_per_sample_ms': time_per_sample * 1000,
                        'memory_usage_mb': memory_usage,
                        'memory_per_sample_mb': memory_usage / batch_size if batch_size > 0 else 0,
                        'efficiency_score': throughput / max(memory_usage, 1),
                        'std_time_ms': (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5 * 1000
                    }
                    
                    batch_scaling_data.append(batch_data)
                    
                except Exception as batch_error:
                    logger.debug(f"Batch scaling test failed for batch={batch_size}: {batch_error}")
                    continue
            
            scaling_analysis['batch_size_scaling']['throughput_analysis'] = batch_scaling_data
            
            # Analyze batch scaling patterns
            if len(batch_scaling_data) >= 3:
                try:
                    batch_sizes = [bd['batch_size'] for bd in batch_scaling_data]
                    throughputs = [bd['throughput_sps'] for bd in batch_scaling_data]
                    memories = [bd['memory_usage_mb'] for bd in batch_scaling_data]
                    efficiencies = [bd['efficiency_score'] for bd in batch_scaling_data]
                    
                    # Find optimal batch size
                    max_throughput_idx = throughputs.index(max(throughputs))
                    max_efficiency_idx = efficiencies.index(max(efficiencies))
                    
                    scaling_analysis['batch_size_scaling']['optimal_batch_sizes'] = {
                        'max_throughput': batch_sizes[max_throughput_idx],
                        'max_efficiency': batch_sizes[max_efficiency_idx],
                        'recommended': batch_sizes[max_efficiency_idx]  # Prefer efficiency
                    }
                    
                    # Batch efficiency analysis
                    scaling_analysis['batch_size_scaling']['efficiency_analysis'] = {
                        'linear_scaling_achieved': throughputs[-1] / throughputs[0] > len(batch_sizes) * 0.7,
                        'memory_scaling_linear': memories[-1] / memories[0] < len(batch_sizes) * 1.3,
                        'optimal_efficiency_range': [
                            batch_sizes[max(0, max_efficiency_idx - 1)],
                            batch_sizes[min(len(batch_sizes) - 1, max_efficiency_idx + 1)]
                        ]
                    }
                    
                except Exception as analysis_error:
                    logger.debug(f"Batch scaling analysis failed: {analysis_error}")
            
            # Clean up
            del base_test_model
            
        except Exception as batch_error:
            logger.debug(f"Batch size scaling analysis failed: {batch_error}")
            scaling_analysis['analysis_metadata']['warnings'].append(f"Batch scaling analysis failed: {str(batch_error)}")
        
        # Phase 4: Parameter Scaling Analysis
        logger.debug("Phase 4: Parameter scaling analysis")
        
        try:
            # Analyze how different architectural parameters affect scaling
            if model_name == 'EnhancedAutoencoder':
                # Test different encoding dimensions
                encoding_variants = [8, 16, 32, 64] if input_dim >= 32 else [4, 8, 16]
                encoding_scaling = []
                
                for enc_dim in encoding_variants:
                    try:
                        variant_params = base_params.copy()
                        if 'config' in variant_params and 'model' in variant_params['config']:
                            variant_params['config']['model']['encoding_dim'] = enc_dim
                        
                        variant_model = model_class(**variant_params)
                        variant_params_count = sum(p.numel() for p in variant_model.parameters())
                        
                        # Quick performance test
                        test_input = torch.randn(16, input_dim)
                        with torch.no_grad():
                            start_time = time.time()
                            _ = variant_model(test_input)
                            inference_time = time.time() - start_time
                        
                        encoding_scaling.append({
                            'encoding_dim': enc_dim,
                            'parameter_count': variant_params_count,
                            'inference_time_ms': inference_time * 1000,
                            'params_per_encoding_dim': variant_params_count / enc_dim
                        })
                        
                        del variant_model
                        
                    except Exception as enc_error:
                        logger.debug(f"Encoding dimension test failed for dim={enc_dim}: {enc_error}")
                
                if encoding_scaling:
                    scaling_analysis['parameter_scaling']['architectural_impact']['encoding_dimension'] = encoding_scaling
            
            elif model_name == 'AutoencoderEnsemble':
                # Test different ensemble sizes
                ensemble_sizes = [2, 3, 5] if base_params.get('config', {}).get('model', {}).get('num_models', 3) >= 3 else [2]
                ensemble_scaling = []
                
                for ensemble_size in ensemble_sizes:
                    try:
                        variant_params = base_params.copy()
                        if 'config' in variant_params and 'model' in variant_params['config']:
                            variant_params['config']['model']['num_models'] = ensemble_size
                        
                        variant_model = model_class(**variant_params)
                        variant_params_count = sum(p.numel() for p in variant_model.parameters())
                        
                        # Quick performance test
                        test_input = torch.randn(8, input_dim)  # Smaller batch for ensemble
                        with torch.no_grad():
                            start_time = time.time()
                            _ = variant_model(test_input)
                            inference_time = time.time() - start_time
                        
                        ensemble_scaling.append({
                            'ensemble_size': ensemble_size,
                            'parameter_count': variant_params_count,
                            'inference_time_ms': inference_time * 1000,
                            'params_per_model': variant_params_count / ensemble_size,
                            'time_scaling_factor': inference_time * 1000 / ensemble_size
                        })
                        
                        del variant_model
                        
                    except Exception as ens_error:
                        logger.debug(f"Ensemble size test failed for size={ensemble_size}: {ens_error}")
                
                if ensemble_scaling:
                    scaling_analysis['parameter_scaling']['architectural_impact']['ensemble_size'] = ensemble_scaling
            
            # Complexity growth analysis
            base_param_count = sum(p.numel() for p in model_class(**base_params).parameters())
            
            if input_scaling_data:
                max_params = max(dp['parameter_count'] for dp in input_scaling_data)
                min_params = min(dp['parameter_count'] for dp in input_scaling_data)
                param_growth_ratio = max_params / max(min_params, 1)
                
                if param_growth_ratio < 2:
                    complexity_growth = 'linear'
                elif param_growth_ratio < 10:
                    complexity_growth = 'polynomial'
                else:
                    complexity_growth = 'exponential'
                
                scaling_analysis['parameter_scaling']['complexity_growth'] = complexity_growth
                scaling_analysis['parameter_scaling']['parameter_growth_ratio'] = param_growth_ratio
            
        except Exception as param_error:
            logger.debug(f"Parameter scaling analysis failed: {param_error}")
            scaling_analysis['analysis_metadata']['warnings'].append(f"Parameter scaling analysis failed: {str(param_error)}")
        
        # Phase 5: Performance Predictions and Modeling
        logger.debug("Phase 5: Performance prediction modeling")
        
        try:
            if len(input_scaling_data) >= 3:
                # Simple polynomial fitting for predictions
                dims = np.array([dp['input_dimension'] for dp in input_scaling_data])
                
                # Inference time prediction model
                inference_times = np.array([dp.get('fastest_inference_ms', 0) for dp in input_scaling_data])
                valid_times = inference_times > 0
                
                if np.any(valid_times):
                    valid_dims = dims[valid_times]
                    valid_inference = inference_times[valid_times]
                    
                    try:
                        # Fit polynomial (degree 2)
                        time_coeffs = np.polyfit(valid_dims, valid_inference, 2)
                        
                        # Prediction for common input sizes
                        prediction_dims = [50, 100, 200, 500, 1000]
                        time_predictions = {}
                        
                        for pred_dim in prediction_dims:
                            if pred_dim not in dims:  # Don't predict for tested dimensions
                                pred_time = np.polyval(time_coeffs, pred_dim)
                                time_predictions[pred_dim] = max(0.1, pred_time)  # Minimum realistic time
                        
                        scaling_analysis['performance_predictions']['inference_time_model'] = {
                            'coefficients': time_coeffs.tolist(),
                            'predictions_ms': time_predictions,
                            'model_type': 'polynomial_degree_2'
                        }
                        
                    except Exception as pred_error:
                        logger.debug(f"Time prediction modeling failed: {pred_error}")
                
                # Memory usage prediction model
                mem_usages = np.array([dp['memory_metrics'].get('gpu_memory_mb', 0) for dp in input_scaling_data])
                valid_mem = mem_usages > 0
                
                if np.any(valid_mem):
                    valid_dims_mem = dims[valid_mem]
                    valid_memories = mem_usages[valid_mem]
                    
                    try:
                        mem_coeffs = np.polyfit(valid_dims_mem, valid_memories, 2)
                        
                        mem_predictions = {}
                        for pred_dim in prediction_dims:
                            if pred_dim not in dims:
                                pred_mem = np.polyval(mem_coeffs, pred_dim)
                                mem_predictions[pred_dim] = max(1.0, pred_mem)  # Minimum 1MB
                        
                        scaling_analysis['performance_predictions']['memory_usage_model'] = {
                            'coefficients': mem_coeffs.tolist(),
                            'predictions_mb': mem_predictions,
                            'model_type': 'polynomial_degree_2'
                        }
                        
                    except Exception as mem_pred_error:
                        logger.debug(f"Memory prediction modeling failed: {mem_pred_error}")
        
        except Exception as prediction_error:
            logger.debug(f"Performance prediction phase failed: {prediction_error}")
        
        # Phase 6: Optimization Recommendations
        logger.debug("Phase 6: Generating optimization recommendations")
        
        try:
            recommendations = {
                'scaling_efficiency': [],
                'bottleneck_predictions': [],
                'hardware_recommendations': [],
                'configuration_suggestions': []
            }
            
            # Scaling efficiency recommendations
            if scaling_analysis['input_dimension_scaling']['scaling_coefficients']:
                param_exp = scaling_analysis['input_dimension_scaling']['scaling_coefficients'].get('parameter_scaling_exponent', 2.0)
                flop_exp = scaling_analysis['input_dimension_scaling']['scaling_coefficients'].get('flop_scaling_exponent', 2.0)
                
                if param_exp > 3.0:
                    recommendations['scaling_efficiency'].append("High parameter growth - consider dimensionality reduction")
                elif param_exp < 1.5:
                    recommendations['scaling_efficiency'].append("Efficient parameter scaling - good for large inputs")
                
                if flop_exp > 3.0:
                    recommendations['scaling_efficiency'].append("High computational growth - optimize for smaller inputs")
                elif flop_exp < 2.0:
                    recommendations['scaling_efficiency'].append("Good computational scaling")
            
            # Bottleneck predictions
            if input_scaling_data:
                large_dim_data = [dp for dp in input_scaling_data if dp['input_dimension'] > input_dim * 2]
                if large_dim_data:
                    avg_large_throughput = sum(dp.get('best_throughput_sps', 0) for dp in large_dim_data) / len(large_dim_data)
                    small_dim_data = [dp for dp in input_scaling_data if dp['input_dimension'] <= input_dim]
                    if small_dim_data:
                        avg_small_throughput = sum(dp.get('best_throughput_sps', 0) for dp in small_dim_data) / len(small_dim_data)
                        
                        if avg_large_throughput < avg_small_throughput * 0.3:  # >70% performance drop
                            recommendations['bottleneck_predictions'].append("Severe performance degradation at large input sizes")
                        elif avg_large_throughput < avg_small_throughput * 0.6:  # >40% performance drop
                            recommendations['bottleneck_predictions'].append("Moderate performance degradation at scale")
            
            # Hardware recommendations
            max_memory_mb = 0
            if input_scaling_data:
                memory_usages = [dp['memory_metrics'].get('gpu_memory_mb', 0) for dp in input_scaling_data]
                max_memory_mb = max(memory_usages) if memory_usages else 0
            
            if max_memory_mb > 8000:  # >8GB
                recommendations['hardware_recommendations'].append("High-end GPU recommended for large inputs")
            elif max_memory_mb > 4000:  # >4GB
                recommendations['hardware_recommendations'].append("Mid-range GPU sufficient")
            else:
                recommendations['hardware_recommendations'].append("Entry-level GPU adequate")
            
            # Configuration suggestions
            if scaling_analysis['batch_size_scaling']['optimal_batch_sizes']:
                optimal_batch = scaling_analysis['batch_size_scaling']['optimal_batch_sizes'].get('recommended', 32)
                recommendations['configuration_suggestions'].append(f"Optimal batch size: {optimal_batch}")
            
            if model_name == 'AutoencoderEnsemble':
                recommendations['configuration_suggestions'].append("Consider ensemble size vs. resource trade-off")
            
            if len(input_scaling_data) > 0:
                param_counts = [dp['parameter_count'] for dp in input_scaling_data]
                if max(param_counts) > 1000000:  # >1M parameters
                    recommendations['configuration_suggestions'].append("Large model - consider mixed precision training")
            
            scaling_analysis['optimization_recommendations'] = recommendations
            
        except Exception as rec_error:
            logger.debug(f"Optimization recommendations generation failed: {rec_error}")
        
        # Final metadata and cleanup
        scaling_analysis['analysis_metadata'].update({
            'analysis_time_seconds': time.time() - scaling_start_time,
            'scaling_dimensions_tested': [dp['input_dimension'] for dp in input_scaling_data],
            'batch_sizes_tested': [bd['batch_size'] for bd in scaling_analysis['batch_size_scaling']['throughput_analysis']],
            'total_successful_tests': len(input_scaling_data) + len(scaling_analysis['batch_size_scaling']['throughput_analysis'])
        })
        
        # Clean up any remaining tensors
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.debug(f"Comprehensive scaling analysis completed for {model_name} in {time.time() - scaling_start_time:.2f}s")
        
        return scaling_analysis
        
    except Exception as e:
        logger.error(f"Comprehensive scaling analysis failed for {model_name}: {e}")
        
        return {
            'error': f'Scaling analysis failed: {str(e)}',
            'analysis_metadata': {
                'analysis_time_seconds': time.time() - scaling_start_time if 'scaling_start_time' in locals() else 0,
                'error_type': type(e).__name__,
                'warnings': [f'Complete analysis failure: {str(e)}']
            }
        }

def analyze_memory_usage(
    batch_size: int,
    input_dim: int,
    model_name: str,
    model: torch.nn.Module,
    hardware_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Comprehensive memory usage analysis with system awareness.
    
    This function provides detailed memory analysis including GPU memory usage,
    CPU memory estimation, and system-aware recommendations. Harmonized with
    compare_model_architectures() parameter structure.
    
    Args:
        batch_size: Batch size for memory analysis
        input_dim: Input dimension size
        model_name: Name of the model for logging
        model: PyTorch model to analyze
        hardware_data: Hardware information from system context
        
    Returns:
        Dictionary containing comprehensive memory analysis
    """
    try:
        memory_analysis = {
            'system_context': {
                'system_class': 'unknown',
                'total_system_memory_gb': 0,
                'memory_performance_class': 'unknown',
                'current_memory_usage_percent': 0,
                'gpu_available': False,
                'total_gpu_memory_gb': 0
            },
            'cpu_memory_analysis': {
                'parameter_memory_mb': 0,
                'estimated_activation_memory_mb': 0,
                'estimated_total_cpu_memory_mb': 0,
                'memory_pressure_score': 0,
                'system_memory_adequate': False
            },
            'gpu_memory_detailed': {
                'allocated_mb': 0,
                'peak_mb': 0,
                'per_sample_kb': 0,
                'memory_efficiency_score': 0,
                'snapshots': []
            },
            'model_complexity': {
                'total_parameters': 0,
                'model_size_mb': 0,
                'complexity_class': 'unknown',
                'memory_efficiency_score': 0
            },
            'recommendations': [],
            'analysis_metadata': {
                'analysis_method': 'comprehensive_system_aware',
                'gpu_analysis_performed': False,
                'warnings': []
            }
        }
        
        logger.debug(f"Starting comprehensive memory analysis for {model_name}")
        
        # Extract hardware information from the provided hardware_data
        gpu_available = hardware_data.get('gpu_available', False)
        system_memory_gb = hardware_data.get('system_memory_gb', 8.0)
        memory_usage_percent = hardware_data.get('memory_usage_percent', 0)
        gpu_memory_gb = hardware_data.get('gpu_memory_gb', 0)
        system_class = hardware_data.get('system_class', 'unknown')
        
        # SYSTEM CONTEXT INTEGRATION
        memory_analysis['system_context'].update({
            'system_class': system_class,
            'total_system_memory_gb': system_memory_gb,
            'current_memory_usage_percent': memory_usage_percent,
            'gpu_available': gpu_available,
            'total_gpu_memory_gb': gpu_memory_gb
        })
        
        # ENHANCED GPU MEMORY ANALYSIS with system awareness
        if gpu_available and torch.cuda.is_available():
            try:
                # Enhanced memory monitoring with system context
                torch.cuda.empty_cache()
                memory_before = torch.cuda.memory_allocated()
                
                # Move model and create input with optimal memory management
                model_gpu = model.cuda() if not next(model.parameters()).is_cuda else model
                
                # ADAPTIVE BATCH SIZE based on system memory
                test_batch_size = batch_size
                if system_memory_gb < 8:
                    test_batch_size = min(batch_size, 32)
                    memory_analysis['batch_size_adjusted'] = {
                        'original': batch_size,
                        'adjusted': test_batch_size,
                        'reason': 'limited_system_memory'
                    }
                
                test_input_gpu = torch.randn(test_batch_size, input_dim).cuda()
                
                # COMPREHENSIVE MEMORY PROFILING
                memory_snapshots = []
                
                # Baseline measurement
                torch.cuda.synchronize()
                baseline_memory = torch.cuda.memory_allocated()
                memory_snapshots.append(('baseline', baseline_memory))
                
                # Forward pass with detailed tracking
                with torch.no_grad():
                    torch.cuda.reset_peak_memory_stats()
                    output = model_gpu(test_input_gpu)
                    torch.cuda.synchronize()
                    
                    forward_memory = torch.cuda.memory_allocated()
                    peak_memory = torch.cuda.max_memory_allocated()
                    memory_snapshots.append(('forward', forward_memory))
                    memory_snapshots.append(('peak', peak_memory))
                
                # Calculate comprehensive memory metrics
                gpu_memory_mb = (forward_memory - memory_before) / (1024 * 1024)
                peak_memory_mb = peak_memory / (1024 * 1024)
                
                memory_analysis['gpu_memory_detailed'] = {
                    'allocated_mb': gpu_memory_mb,
                    'peak_mb': peak_memory_mb,
                    'per_sample_kb': (gpu_memory_mb * 1024) / test_batch_size,
                    'memory_efficiency_score': test_batch_size / max(gpu_memory_mb, 1),
                    'snapshots': [(name, mem / (1024 * 1024)) for name, mem in memory_snapshots]
                }
                
                # SYSTEM-AWARE MEMORY RECOMMENDATIONS
                memory_usage_percent = (peak_memory_mb / 1024) / max(gpu_memory_gb, 1) * 100 if gpu_memory_gb > 0 else 0
                
                memory_analysis['gpu_memory_recommendations'] = []
                
                if memory_usage_percent > 80:
                    memory_analysis['gpu_memory_recommendations'].append(
                        f"High GPU memory usage ({memory_usage_percent:.1f}%) - consider smaller batch sizes"
                    )
                elif memory_usage_percent < 30 and gpu_memory_gb > 4:
                    memory_analysis['gpu_memory_recommendations'].append(
                        f"Low GPU memory usage ({memory_usage_percent:.1f}%) - can use larger batch sizes"
                    )
                
                # Clean up GPU memory
                del test_input_gpu, output
                if model_gpu is not model:
                    del model_gpu
                
                torch.cuda.empty_cache()
                memory_analysis['analysis_metadata']['gpu_analysis_performed'] = True
                
            except Exception as gpu_error:
                logger.debug(f"GPU memory analysis failed: {gpu_error}")
                memory_analysis['analysis_metadata']['warnings'].append(f"GPU analysis failed: {str(gpu_error)}")
        
        # ENHANCED CPU MEMORY ESTIMATION with system awareness
        total_params = sum(p.numel() for p in model.parameters())
        param_memory_mb = total_params * 4 / (1024 * 1024)  # float32
        
        # SYSTEM-AWARE ACTIVATION MEMORY ESTIMATION
        # Try to estimate hidden dimensions from model structure
        estimated_hidden = input_dim * 2  # Conservative fallback
        
        # Check for common model attributes to improve estimation
        if hasattr(model, 'encoding_dim'):
            estimated_hidden = model.encoding_dim * 4
        elif hasattr(model, 'hidden_dims'):
            if hasattr(model.hidden_dims, '__len__') and len(model.hidden_dims) > 0:
                estimated_hidden = sum(model.hidden_dims) / len(model.hidden_dims)
        
        activation_memory_mb = estimated_hidden * batch_size * 4 / (1024 * 1024)
        estimated_cpu_memory_mb = param_memory_mb + activation_memory_mb
        
        # SYSTEM CONTEXT ANALYSIS
        memory_pressure_score = 0.0
        if system_memory_gb > 0:
            estimated_additional_percent = (estimated_cpu_memory_mb / 1024) / system_memory_gb * 100
            total_projected_usage = memory_usage_percent + estimated_additional_percent
            
            if total_projected_usage > 90:
                memory_pressure_score = 1.0  # High pressure
            elif total_projected_usage > 70:
                memory_pressure_score = 0.5  # Medium pressure
            else:
                memory_pressure_score = 0.0  # Low pressure
        
        # Classify model complexity directly
        if total_params < 10000:
            complexity_class = 'very_simple'
        elif total_params < 100000:
            complexity_class = 'simple'
        elif total_params < 1000000:
            complexity_class = 'medium'
        elif total_params < 10000000:
            complexity_class = 'complex'
        else:
            complexity_class = 'very_complex'
        
        memory_analysis['cpu_memory_analysis'].update({
            'parameter_memory_mb': param_memory_mb,
            'estimated_activation_memory_mb': activation_memory_mb,
            'estimated_total_cpu_memory_mb': estimated_cpu_memory_mb,
            'memory_pressure_score': memory_pressure_score,
            'system_memory_adequate': estimated_cpu_memory_mb < (system_memory_gb * 1024 * 0.3) if system_memory_gb > 0 else False
        })
        
        memory_analysis['model_complexity'].update({
            'total_parameters': total_params,
            'model_size_mb': param_memory_mb,
            'complexity_class': complexity_class,
            'memory_efficiency_score': 0  # Will be updated if throughput data is available
        })
        
        # COMPREHENSIVE RECOMMENDATIONS using system analysis
        recommendations = []
        
        # System class based recommendations
        if system_class == 'limited':
            recommendations.append("Limited system detected - consider using gradient checkpointing to reduce memory usage")
            if estimated_cpu_memory_mb > 500:
                recommendations.append("Model may be too large for system - consider using a smaller architecture")
        elif system_class == 'high_performance':
            if estimated_cpu_memory_mb < 1000 and system_memory_gb > 16:
                recommendations.append("High-performance system with ample memory - can handle larger models or batch sizes")
        
        # Memory pressure recommendations
        if memory_pressure_score > 0.5:
            recommendations.extend([
                "High memory pressure detected - consider reducing batch size",
                "Enable gradient accumulation to maintain effective batch size with lower memory usage"
            ])
        
        # GPU-specific recommendations
        if gpu_available:
            if gpu_memory_gb < 4:
                recommendations.append("Limited GPU memory - use mixed precision training to reduce memory usage")
            elif gpu_memory_gb > 8:
                recommendations.append("Ample GPU memory available - can use larger batch sizes or more complex models")
        
        memory_analysis['recommendations'] = recommendations
        
        logger.debug(f"Comprehensive memory analysis completed for {model_name}")
        
        return memory_analysis
        
    except Exception as e:
        logger.error(f"Comprehensive memory analysis failed for {model_name}: {e}")
        
        return {
            'error': f'Memory analysis failed: {str(e)}',
            'analysis_metadata': {
                'analysis_method': 'failed',
                'warnings': [f'Complete analysis failure: {str(e)}']
            }
        }

def estimate_training_resources(
    total_params: int,
    batch_size: int,
    input_dim: int,
    config: Dict[str, Any],
    model_name: str,
    hardware_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Comprehensive training resource estimation with detailed analysis and recommendations.
    
    This function provides enhanced resource estimation including memory requirements,
    computational needs, time estimates, hardware recommendations, and optimization
    strategies for different training scenarios. Harmonized with compare_model_architectures().
    
    Args:
        total_params: Number of model parameters
        batch_size: Training batch size
        input_dim: Input dimension size
        config: Configuration dictionary for detailed analysis
        model_name: Model name for logging and recommendations
        hardware_data: Hardware information from system context
        
    Returns:
        Dictionary containing comprehensive resource requirement estimates
    """
    estimation_start_time = time.time()
    
    try:
        logger.debug(f"Starting comprehensive training resource estimation for {model_name}")
        
        resources = {
            'memory': {
                'training_memory': {},
                'inference_memory': {},
                'optimization_memory': {},
                'total_estimates': {},
                'memory_breakdown': {},
                'memory_optimization': {}
            },
            'compute': {
                'training_compute': {},
                'inference_compute': {},
                'flops_analysis': {},
                'computational_intensity': {},
                'compute_optimization': {}
            },
            'time_estimates': {
                'training_time': {},
                'inference_time': {},
                'epoch_estimates': {},
                'convergence_estimates': {},
                'hardware_scaling': {}
            },
            'hardware_requirements': {
                'minimum_requirements': {},
                'recommended_requirements': {},
                'optimal_requirements': {},
                'hardware_compatibility': {},
                'upgrade_recommendations': {}
            },
            'optimization_strategies': {
                'memory_optimization': [],
                'compute_optimization': [],
                'training_optimization': [],
                'hardware_optimization': [],
                'cost_optimization': []
            },
            'scaling_analysis': {
                'batch_size_scaling': {},
                'parameter_scaling': {},
                'data_scaling': {},
                'hardware_scaling': {}
            },
            'cost_analysis': {
                'training_cost_estimates': {},
                'inference_cost_estimates': {},
                'hardware_cost_estimates': {},
                'optimization_savings': {}
            },
            'analysis_metadata': {
                'model_name': model_name,
                'param_count': total_params,
                'batch_size': batch_size,
                'input_dim': input_dim,
                'estimation_timestamp': datetime.now().isoformat(),
                'estimation_duration_seconds': 0,
                'hardware_analysis_available': bool(hardware_data),
                'config_analysis_available': bool(config),
                'warnings': [],
                'assumptions': [],
                'validation_checks': []
            }
        }
        
        # Extract configuration details if available
        model_config = config.get('model', {}) if config else {}
        training_config = config.get('training', {}) if config else {}
        
        # Basic validation and assumptions
        if total_params <= 0:
            resources['analysis_metadata']['warnings'].append("Invalid parameter count, using default estimate")
            total_params = 100000
        
        if batch_size <= 0:
            resources['analysis_metadata']['warnings'].append("Invalid batch size, using default")
            batch_size = 32
        
        if input_dim <= 0:
            resources['analysis_metadata']['warnings'].append("Invalid input dimension, using default")
            input_dim = 20
        
        # Phase 1: Comprehensive Memory Analysis
        try:
            logger.debug(f"Phase 1: Memory analysis for {model_name}")
            
            # Memory constants
            bytes_per_float32 = 4
            bytes_per_float16 = 2
            mixed_precision = training_config.get('mixed_precision', False)
            precision_bytes = bytes_per_float16 if mixed_precision else bytes_per_float32
            
            # Model parameters memory
            param_memory_bytes = total_params * bytes_per_float32  # Always FP32
            
            # Gradient memory (same size as parameters)
            gradient_memory_bytes = param_memory_bytes
            
            # Optimizer state memory
            optimizer_type = training_config.get('optimizer', 'AdamW').lower()
            if optimizer_type in ['adam', 'adamw']:
                # Adam/AdamW: momentum (FP32) + variance (FP32) + possibly more
                optimizer_state_multiplier = 2.0
                optimizer_memory_bytes = param_memory_bytes * optimizer_state_multiplier
            elif optimizer_type == 'sgd':
                # SGD with momentum: momentum buffer
                optimizer_state_multiplier = 1.0
                optimizer_memory_bytes = param_memory_bytes * optimizer_state_multiplier
            else:
                # Conservative estimate
                optimizer_state_multiplier = 1.5
                optimizer_memory_bytes = param_memory_bytes * optimizer_state_multiplier
            
            # Activation memory estimation
            # Estimate based on model architecture
            hidden_dims = model_config.get('hidden_dims', [128, 64])
            encoding_dim = model_config.get('encoding_dim', 32)
            
            # Calculate activation memory for forward pass
            activation_sizes = [input_dim] + hidden_dims + [encoding_dim] + list(reversed(hidden_dims)) + [input_dim]
            total_activation_elements = sum(activation_sizes) * batch_size
            activation_memory_bytes = total_activation_elements * precision_bytes
            
            # Additional memory for intermediate computations
            intermediate_memory_bytes = activation_memory_bytes * 0.5
            
            # Data batch memory (input + target)
            data_memory_bytes = input_dim * batch_size * precision_bytes * 2
            
            # Total training memory
            total_training_memory_bytes = (
                param_memory_bytes + 
                gradient_memory_bytes + 
                optimizer_memory_bytes + 
                activation_memory_bytes + 
                intermediate_memory_bytes + 
                data_memory_bytes
            )
            
            # System overhead and fragmentation
            overhead_factor = 1.5  # 50% overhead
            total_memory_with_overhead = total_training_memory_bytes * overhead_factor
            
            # Convert to more readable units
            def bytes_to_units(bytes_val):
                mb = bytes_val / (1024 ** 2)
                gb = bytes_val / (1024 ** 3)
                return {'bytes': bytes_val, 'mb': mb, 'gb': gb}
            
            resources['memory']['training_memory'] = {
                'parameters': bytes_to_units(param_memory_bytes),
                'gradients': bytes_to_units(gradient_memory_bytes),
                'optimizer_state': bytes_to_units(optimizer_memory_bytes),
                'activations': bytes_to_units(activation_memory_bytes),
                'intermediate_computations': bytes_to_units(intermediate_memory_bytes),
                'data_batch': bytes_to_units(data_memory_bytes),
                'total_training': bytes_to_units(total_training_memory_bytes),
                'total_with_overhead': bytes_to_units(total_memory_with_overhead),
                'optimizer_state_multiplier': optimizer_state_multiplier,
                'precision_used': 'FP16' if mixed_precision else 'FP32'
            }
            
            # Inference memory (much smaller)
            inference_memory_bytes = param_memory_bytes + activation_memory_bytes + data_memory_bytes
            resources['memory']['inference_memory'] = {
                'total_inference': bytes_to_units(inference_memory_bytes),
                'parameters': bytes_to_units(param_memory_bytes),
                'activations': bytes_to_units(activation_memory_bytes),
                'data_batch': bytes_to_units(data_memory_bytes)
            }
            
            # Memory breakdown percentages
            total_mem = total_memory_with_overhead
            resources['memory']['memory_breakdown'] = {
                'parameters_percent': (param_memory_bytes / total_mem) * 100,
                'gradients_percent': (gradient_memory_bytes / total_mem) * 100,
                'optimizer_percent': (optimizer_memory_bytes / total_mem) * 100,
                'activations_percent': (activation_memory_bytes / total_mem) * 100,
                'data_percent': (data_memory_bytes / total_mem) * 100,
                'overhead_percent': ((total_mem - total_training_memory_bytes) / total_mem) * 100
            }
            
            # Memory optimization suggestions
            memory_optimizations = []
            if mixed_precision:
                memory_optimizations.append("Using mixed precision (FP16) - saves ~50% activation memory")
            else:
                memory_optimizations.append("Consider enabling mixed precision to reduce memory usage")
            
            if batch_size > 64:
                memory_optimizations.append("Large batch size detected - consider gradient accumulation")
            
            if total_params > 1000000:
                memory_optimizations.append("Large model - consider gradient checkpointing")
            
            resources['memory']['memory_optimization'] = {
                'suggestions': memory_optimizations,
                'potential_savings': {
                    'mixed_precision_savings_mb': (activation_memory_bytes * 0.5) / (1024**2) if not mixed_precision else 0,
                    'gradient_checkpointing_savings_mb': (activation_memory_bytes * 0.7) / (1024**2),
                    'smaller_batch_savings_mb': ((batch_size - 16) * input_dim * precision_bytes * 2) / (1024**2) if batch_size > 16 else 0
                }
            }
            
            resources['analysis_metadata']['validation_checks'].append('memory_analysis_completed')
            
        except Exception as e:
            logger.error(f"Memory analysis failed for {model_name}: {e}")
            resources['memory']['error'] = str(e)
            resources['analysis_metadata']['warnings'].append(f"Memory analysis failed: {str(e)}")
        
        # Phase 2: Computational Requirements Analysis
        try:
            logger.debug(f"Phase 2: Computational analysis for {model_name}")
            
            # FLOP estimation for forward pass
            # Rough estimate based on matrix multiplications
            forward_flops = 0
            layer_dims = [input_dim] + hidden_dims + [encoding_dim]
            
            # Encoder FLOPs
            for i in range(len(layer_dims) - 1):
                # Matrix multiplication: input_dim × output_dim × batch_size × 2 (multiply-add)
                layer_flops = layer_dims[i] * layer_dims[i + 1] * batch_size * 2
                forward_flops += layer_flops
            
            # Decoder FLOPs (reverse)
            decoder_dims = [encoding_dim] + list(reversed(hidden_dims)) + [input_dim]
            for i in range(len(decoder_dims) - 1):
                layer_flops = decoder_dims[i] * decoder_dims[i + 1] * batch_size * 2
                forward_flops += layer_flops
            
            # Activation function FLOPs (approximate)
            activation_flops = sum(layer_dims[1:]) * batch_size  # One FLOP per activation
            activation_flops += sum(decoder_dims[1:-1]) * batch_size
            forward_flops += activation_flops
            
            # Backward pass FLOPs (typically 2x forward pass)
            backward_flops = forward_flops * 2
            
            # Optimizer FLOPs
            if optimizer_type in ['adam', 'adamw']:
                # Adam: momentum update, variance update, bias correction, parameter update
                optimizer_flops = total_params * 5
            elif optimizer_type == 'sgd':
                # SGD: momentum update, parameter update
                optimizer_flops = total_params * 2
            else:
                optimizer_flops = total_params * 3  # Conservative estimate
            
            total_training_flops = forward_flops + backward_flops + optimizer_flops
            
            # Classify computational intensity directly
            def _classify_computational_intensity(flops_per_step: int) -> str:
                """Classify computational intensity for training steps."""
                if flops_per_step < 1e3:
                    return 'negligible'
                elif flops_per_step < 1e4:
                    return 'minimal'
                elif flops_per_step < 1e5:
                    return 'very_light'
                elif flops_per_step < 1e6:
                    return 'light'
                elif flops_per_step < 1e7:
                    return 'light_moderate'
                elif flops_per_step < 1e8:
                    return 'moderate'
                elif flops_per_step < 5e8:
                    return 'moderate_high'
                elif flops_per_step < 1e9:
                    return 'high'
                elif flops_per_step < 5e9:
                    return 'very_high'
                elif flops_per_step < 1e10:
                    return 'intensive'
                elif flops_per_step < 5e10:
                    return 'very_intensive'
                elif flops_per_step < 1e11:
                    return 'ultra_intensive'
                elif flops_per_step < 1e12:
                    return 'extreme'
                else:
                    return 'massive'
            
            computational_intensity = _classify_computational_intensity(total_training_flops)
            
            resources['compute']['training_compute'] = {
                'forward_pass_flops': forward_flops,
                'backward_pass_flops': backward_flops,
                'optimizer_flops': optimizer_flops,
                'total_training_step_flops': total_training_flops,
                'flops_per_parameter': total_training_flops / total_params,
                'computational_intensity': computational_intensity
            }
            
            resources['compute']['inference_compute'] = {
                'inference_flops': forward_flops,
                'flops_per_sample': forward_flops / batch_size,
                'computational_efficiency': forward_flops / total_params
            }
            
            # FLOP analysis by operation type
            resources['compute']['flops_analysis'] = {
                'matrix_multiplication_percent': ((forward_flops + backward_flops - activation_flops * 3) / total_training_flops) * 100,
                'activation_function_percent': ((activation_flops * 3) / total_training_flops) * 100,
                'optimizer_percent': (optimizer_flops / total_training_flops) * 100,
                'floating_point_intensity': 'high' if total_training_flops > 1e9 else 'medium' if total_training_flops > 1e6 else 'low'
            }
            
            resources['analysis_metadata']['validation_checks'].append('compute_analysis_completed')
            
        except Exception as e:
            logger.error(f"Computational analysis failed for {model_name}: {e}")
            resources['compute']['error'] = str(e)
            resources['analysis_metadata']['warnings'].append(f"Computational analysis failed: {str(e)}")
        
        # Phase 3: Time Estimation Analysis
        try:
            logger.debug(f"Phase 3: Time estimation analysis for {model_name}")
            
            # Extract hardware information
            gpu_available = hardware_data.get('gpu_available', False)
            gpu_memory_gb = hardware_data.get('gpu_memory_gb', 0)
            cpu_count = hardware_data.get('cpu_count', os.cpu_count() or 1)
            
            # Estimate computational capacity
            if gpu_available and gpu_memory_gb > 0:
                # GPU performance estimates (FLOPS/second)
                if gpu_memory_gb >= 24:  # High-end GPU
                    gpu_flops_per_second = 15e12  # 15 TFLOPS
                elif gpu_memory_gb >= 8:   # Mid-range GPU
                    gpu_flops_per_second = 8e12   # 8 TFLOPS
                elif gpu_memory_gb >= 4:   # Entry-level GPU
                    gpu_flops_per_second = 4e12   # 4 TFLOPS
                else:
                    gpu_flops_per_second = 2e12   # Basic GPU
                
                # Memory constraint check
                required_memory_gb = resources['memory']['training_memory']['total_with_overhead']['gb']
                
                if required_memory_gb <= gpu_memory_gb:
                    computational_capacity = gpu_flops_per_second
                    device_type = 'GPU (unconstrained)'
                    memory_constraint_factor = 1.0
                else:
                    # Memory constrained - performance degradation
                    constraint_ratio = gpu_memory_gb / required_memory_gb
                    memory_constraint_factor = constraint_ratio * 0.7  # Significant degradation
                    computational_capacity = gpu_flops_per_second * memory_constraint_factor
                    device_type = 'GPU (memory constrained)'
            else:
                # CPU performance estimates
                cpu_flops_per_second = 1e10 * min(cpu_count, 16)  # ~10 GFLOPS per core, diminishing returns after 16
                computational_capacity = cpu_flops_per_second
                device_type = 'CPU'
                memory_constraint_factor = 1.0
            
            # Time per training step
            if 'total_training_step_flops' in resources['compute'].get('training_compute', {}):
                flops_per_step = resources['compute']['training_compute']['total_training_step_flops']
                time_per_step_seconds = flops_per_step / computational_capacity
                
                # Estimate samples per epoch and steps per epoch
                estimated_samples_per_epoch = training_config.get('samples_per_epoch', 10000)
                steps_per_epoch = max(1, estimated_samples_per_epoch // batch_size)
                
                time_per_epoch_seconds = time_per_step_seconds * steps_per_epoch
                time_per_epoch_minutes = time_per_epoch_seconds / 60
                time_per_epoch_hours = time_per_epoch_minutes / 60
                
                # Convergence estimates
                estimated_epochs_to_converge = training_config.get('epochs', 100)
                total_training_time_hours = time_per_epoch_hours * estimated_epochs_to_converge
                
                resources['time_estimates']['training_time'] = {
                    'time_per_step_ms': time_per_step_seconds * 1000,
                    'time_per_epoch_minutes': time_per_epoch_minutes,
                    'time_per_epoch_hours': time_per_epoch_hours,
                    'steps_per_epoch': steps_per_epoch,
                    'computational_capacity_flops': computational_capacity,
                    'device_type': device_type,
                    'memory_constraint_factor': memory_constraint_factor
                }
                
                resources['time_estimates']['convergence_estimates'] = {
                    'estimated_epochs': estimated_epochs_to_converge,
                    'total_training_time_hours': total_training_time_hours,
                    'total_training_time_days': total_training_time_hours / 24,
                    'epochs_per_day': 24 / max(time_per_epoch_hours, 0.001),
                    'training_throughput_samples_per_hour': (3600 / max(time_per_step_seconds, 0.001)) * batch_size
                }
            
            # Inference time estimates
            if 'inference_flops' in resources['compute'].get('inference_compute', {}):
                inference_flops = resources['compute']['inference_compute']['inference_flops']
                inference_time_seconds = inference_flops / computational_capacity
                
                resources['time_estimates']['inference_time'] = {
                    'inference_time_ms': inference_time_seconds * 1000,
                    'inference_throughput_samples_per_second': batch_size / inference_time_seconds,
                    'single_sample_latency_ms': (inference_time_seconds * 1000) / batch_size
                }
            
            # Hardware scaling analysis
            scaling_factors = {
                'double_gpu_memory': 1.8 if gpu_available else 1.0,
                'high_end_gpu': 2.5 if gpu_available else 3.0,
                'multi_gpu_2x': 1.7 if gpu_available else 1.0,
                'multi_gpu_4x': 3.2 if gpu_available else 1.0
            }
            
            resources['time_estimates']['hardware_scaling'] = {}
            for scenario, factor in scaling_factors.items():
                if 'time_per_epoch_hours' in resources['time_estimates'].get('training_time', {}):
                    original_time = resources['time_estimates']['training_time']['time_per_epoch_hours']
                    scaled_time = original_time / factor
                    resources['time_estimates']['hardware_scaling'][scenario] = {
                        'time_per_epoch_hours': scaled_time,
                        'speedup_factor': factor,
                        'total_training_time_reduction_hours': original_time - scaled_time
                    }
            
            resources['analysis_metadata']['validation_checks'].append('time_estimation_completed')
            
        except Exception as e:
            logger.error(f"Time estimation analysis failed for {model_name}: {e}")
            resources['time_estimates']['error'] = str(e)
            resources['analysis_metadata']['warnings'].append(f"Time estimation failed: {str(e)}")
        
        # Phase 4: Hardware Requirements and Recommendations
        try:
            logger.debug(f"Phase 4: Hardware requirements analysis for {model_name}")
            
            # Memory requirements
            required_memory_gb = resources['memory']['training_memory']['total_with_overhead']['gb']
            inference_memory_gb = resources['memory']['inference_memory']['total_inference']['gb']
            
            # Minimum requirements
            resources['hardware_requirements']['minimum_requirements'] = {
                'system_memory_gb': max(8, int(required_memory_gb * 1.5)),
                'gpu_memory_gb': max(4, int(required_memory_gb)) if gpu_available else None,
                'cpu_cores': max(4, cpu_count),
                'storage_gb': max(10, total_params * 8 / (1024**3)),  # Model + checkpoints + data
                'description': 'Minimum viable configuration for training'
            }
            
            # Recommended requirements
            resources['hardware_requirements']['recommended_requirements'] = {
                'system_memory_gb': max(16, int(required_memory_gb * 2)),
                'gpu_memory_gb': max(8, int(required_memory_gb * 1.5)) if gpu_available else None,
                'cpu_cores': max(8, cpu_count * 2),
                'storage_gb': max(50, total_params * 16 / (1024**3)),
                'gpu_compute_capability': 7.0 if gpu_available else None,
                'description': 'Recommended configuration for efficient training'
            }
            
            # Optimal requirements
            resources['hardware_requirements']['optimal_requirements'] = {
                'system_memory_gb': max(32, int(required_memory_gb * 3)),
                'gpu_memory_gb': max(16, int(required_memory_gb * 2)) if gpu_available else None,
                'cpu_cores': max(16, cpu_count * 4),
                'storage_gb': max(100, total_params * 32 / (1024**3)),
                'gpu_compute_capability': 8.0 if gpu_available else None,
                'nvme_storage': True,
                'description': 'Optimal configuration for maximum performance'
            }
            
            # Hardware compatibility analysis
            current_gpu_memory = hardware_data.get('gpu_memory_gb', 0)
            current_system_memory = hardware_data.get('system_memory_gb', 8)
            
            compatibility_status = 'unknown'
            compatibility_issues = []
            
            if required_memory_gb > current_gpu_memory and gpu_available:
                compatibility_issues.append(f"Insufficient GPU memory: need {required_memory_gb:.1f}GB, have {current_gpu_memory:.1f}GB")
                compatibility_status = 'insufficient_gpu_memory'
            elif required_memory_gb * 1.5 > current_system_memory:
                compatibility_issues.append(f"Insufficient system memory: need {required_memory_gb*1.5:.1f}GB, have {current_system_memory:.1f}GB")
                compatibility_status = 'insufficient_system_memory'
            elif not gpu_available and total_params > 1000000:
                compatibility_issues.append("Large model without GPU acceleration - training will be very slow")
                compatibility_status = 'cpu_only_large_model'
            else:
                compatibility_status = 'compatible'
            
            resources['hardware_requirements']['hardware_compatibility'] = {
                'status': compatibility_status,
                'issues': compatibility_issues,
                'current_gpu_memory_gb': current_gpu_memory,
                'current_system_memory_gb': current_system_memory,
                'memory_utilization_percent': (required_memory_gb / max(current_gpu_memory, current_system_memory)) * 100
            }
            
            # Upgrade recommendations
            upgrade_recommendations = []
            
            if compatibility_status != 'compatible':
                if 'gpu_memory' in compatibility_status:
                    upgrade_recommendations.append(f"Upgrade to GPU with at least {int(required_memory_gb * 1.5)}GB VRAM")
                if 'system_memory' in compatibility_status:
                    upgrade_recommendations.append(f"Increase system RAM to at least {int(required_memory_gb * 2)}GB")
                if not gpu_available and total_params > 500000:
                    upgrade_recommendations.append("Consider adding GPU acceleration for reasonable training times")
            
            # Performance improvement recommendations
            if gpu_available and current_gpu_memory < 16:
                upgrade_recommendations.append("GPU with more VRAM would allow larger batch sizes")
            
            if cpu_count < 8:
                upgrade_recommendations.append("More CPU cores would improve data loading performance")
            
            resources['hardware_requirements']['upgrade_recommendations'] = upgrade_recommendations
            
            resources['analysis_metadata']['validation_checks'].append('hardware_analysis_completed')
            
        except Exception as e:
            logger.error(f"Hardware requirements analysis failed for {model_name}: {e}")
            resources['hardware_requirements']['error'] = str(e)
            resources['analysis_metadata']['warnings'].append(f"Hardware analysis failed: {str(e)}")
        
        # Phase 5: Optimization Strategies
        try:
            logger.debug(f"Phase 5: Generating optimization strategies for {model_name}")
            
            # Memory optimization strategies
            memory_opts = []
            
            if not training_config.get('mixed_precision', False) and gpu_available:
                memory_opts.append("Enable mixed precision (FP16) training to reduce memory usage by ~50%")
            
            if batch_size > 32:
                memory_opts.append("Consider gradient accumulation with smaller batch sizes")
            
            if total_params > 1000000:
                memory_opts.append("Enable gradient checkpointing to trade compute for memory")
            
            if len(hidden_dims) > 3:
                memory_opts.append("Consider reducing model depth to decrease activation memory")
            
            resources['optimization_strategies']['memory_optimization'] = memory_opts
            
            # Compute optimization strategies
            compute_opts = []
            
            if not gpu_available and total_params > 100000:
                compute_opts.append("GPU acceleration strongly recommended for reasonable training times")
            
            if gpu_available and batch_size < 32:
                compute_opts.append("Increase batch size to better utilize GPU parallelism")
            
            if training_config.get('optimizer', '').lower() == 'sgd':
                compute_opts.append("Consider AdamW optimizer for potentially faster convergence")
            
            resources['optimization_strategies']['compute_optimization'] = compute_opts
            
            # Training optimization strategies
            training_opts = []
            
            training_opts.append("Implement early stopping to avoid overtraining")
            training_opts.append("Use learning rate scheduling for better convergence")
            
            if not training_config.get('gradient_clip'):
                training_opts.append("Consider gradient clipping for training stability")
            
            resources['optimization_strategies']['training_optimization'] = training_opts
            
            # Hardware optimization strategies
            hardware_opts = []
            
            if compatibility_status != 'compatible':
                hardware_opts.extend(resources['hardware_requirements']['upgrade_recommendations'])
            else:
                hardware_opts.append("Current hardware is compatible with training requirements")
                
                if gpu_available and current_gpu_memory > required_memory_gb * 2:
                    hardware_opts.append("GPU memory allows for larger batch sizes or model complexity")
            
            resources['optimization_strategies']['hardware_optimization'] = hardware_opts
            
            # Cost optimization strategies
            cost_opts = []
            
            if 'total_training_time_hours' in resources['time_estimates'].get('convergence_estimates', {}):
                training_hours = resources['time_estimates']['convergence_estimates']['total_training_time_hours']
                
                if training_hours > 24:
                    cost_opts.append("Long training time - consider cloud GPU instances for cost efficiency")
                
                if training_hours > 168:  # 1 week
                    cost_opts.append("Very long training - investigate model compression or architecture optimization")
            
            cost_opts.append("Monitor training progress to stop early if not improving")
            cost_opts.append("Use model checkpointing to resume training if interrupted")
            
            resources['optimization_strategies']['cost_optimization'] = cost_opts
            
            resources['analysis_metadata']['validation_checks'].append('optimization_strategies_generated')
            
        except Exception as e:
            logger.error(f"Optimization strategy generation failed for {model_name}: {e}")
            resources['optimization_strategies']['error'] = str(e)
            resources['analysis_metadata']['warnings'].append(f"Optimization strategy generation failed: {str(e)}")
        
        # Finalize analysis metadata
        resources['analysis_metadata']['estimation_duration_seconds'] = time.time() - estimation_start_time
        resources['analysis_metadata']['total_validation_checks'] = len(resources['analysis_metadata']['validation_checks'])
        resources['analysis_metadata']['total_warnings'] = len(resources['analysis_metadata']['warnings'])
        
        # Add summary statistics
        resources['analysis_metadata']['summary'] = {
            'parameter_count_formatted': f"{total_params:,}",
            'estimated_training_memory_gb': resources['memory']['training_memory']['total_with_overhead']['gb'],
            'estimated_training_time_hours': resources['time_estimates'].get('convergence_estimates', {}).get('total_training_time_hours', 0),
            'hardware_compatibility': resources['hardware_requirements']['hardware_compatibility']['status'],
            'primary_optimization_recommendation': resources['optimization_strategies']['memory_optimization'][0] if resources['optimization_strategies']['memory_optimization'] else "No specific recommendations",
            'analysis_completeness_percent': (len(resources['analysis_metadata']['validation_checks']) / 5) * 100  # 5 total phases
        }
        
        logger.debug(f"Training resource estimation completed for {model_name}: "
                    f"{resources['analysis_metadata']['total_validation_checks']} checks passed, "
                    f"{resources['analysis_metadata']['total_warnings']} warnings, "
                    f"duration: {resources['analysis_metadata']['estimation_duration_seconds']:.2f}s")
        
        return resources
        
    except Exception as e:
        logger.error(f"Critical training resource estimation failure for {model_name}: {e}")
        return {
            'error': f'Training resource estimation failed: {str(e)}',
            'model_name': model_name,
            'estimation_timestamp': datetime.now().isoformat(),
            'fallback_estimates': {
                'memory_mb': total_params * 16 / (1024**2),  # Very rough fallback
                'training_time_estimate': 'Unable to estimate due to analysis failure'
            },
            'recovery_suggestions': [
                'Check parameter validity (total_params, batch_size, input_dim)',
                'Verify hardware_data format',
                'Ensure configuration dictionary is properly formatted',
                'Try with default parameters'
            ]
        }
    
    finally:
        # Memory cleanup
        try:
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
        except Exception:
            pass

def model_specific_feature_analysis(
    model_name: str,
    test_config: Dict[str, Any],
    model: torch.nn.Module
) -> Dict[str, Any]:
    """
    Analyze model-specific features and capabilities.
    
    This function provides detailed analysis of model-specific features,
    architectural characteristics, and implementation details. Harmonized
    with compare_model_architectures() parameter structure.
    
    Args:
        model_name: Name of the model for feature analysis
        test_config: Configuration dictionary containing model parameters
        model: The model instance to analyze
        
    Returns:
        Dictionary containing model-specific feature analysis
    """
    try:
        logger.debug(f"Starting model-specific feature analysis for {model_name}")
        
        # Extract model configuration
        model_config = test_config.get('params', {}).get('config', {}).get('model', {})
        
        # Base feature analysis
        feature_analysis = {
            'supports_attention': getattr(model, 'use_attention', False) and hasattr(model, 'attention'),
            'supports_residual_blocks': getattr(model, 'residual_blocks', False),
            'supports_skip_connections': getattr(model, 'skip_connection', False) and hasattr(model, 'skip_layers'),
            'normalization_type': getattr(model, 'normalization', 'none'),
            'activation_function': getattr(model, 'activation', 'unknown'),
            'mixed_precision_compatible': getattr(model, 'mixed_precision', False),
            'ensemble_size': getattr(model, 'num_models', 1) if model_name == 'AutoencoderEnsemble' else 1,
            'model_type': model_name,
            'encoding_dim': getattr(model, 'encoding_dim', 0),
            'has_batch_norm': getattr(model, 'use_batch_norm', False),
            'has_layer_norm': getattr(model, 'use_layer_norm', False),
            'legacy_mode': getattr(model, 'legacy_mode', False),
            'feature_status': 'completed'
        }
        
        # Model-specific additional analysis
        if model_name == 'AutoencoderEnsemble':
            # Ensemble-specific features
            feature_analysis.update({
                'ensemble_diversity': getattr(model, 'diversity_factor', 0.0),
                'ensemble_voting': getattr(model, 'voting_method', 'average'),
                'individual_model_compatibility': all(hasattr(m, 'forward') for m in getattr(model, 'models', []) if hasattr(model, 'models'))
            })
        
        elif model_name == 'EnhancedAutoencoder':
            # Enhanced autoencoder specific features
            feature_analysis.update({
                'advanced_activations': hasattr(model, 'advanced_activation') or hasattr(model, 'activation_param'),
                'configurable_normalization': feature_analysis['normalization_type'] != 'none',
                'optimization_features': hasattr(model, 'cuda_optimizations') or hasattr(model, 'optimized_layers')
            })
        
        elif model_name == 'SimpleAutoencoder':
            # Simple autoencoder specific features
            feature_analysis.update({
                'minimal_architecture': True,
                'fast_inference': True,
                'low_memory_footprint': True
            })
        
        # Check if features are actually implemented (not just configured)
        feature_analysis.update({
            'attention_implemented': feature_analysis['supports_attention'] and hasattr(model, 'attention_layers'),
            'residual_implemented': feature_analysis['supports_residual_blocks'] and hasattr(model, 'residual_layers'),
            'skip_connections_implemented': feature_analysis['supports_skip_connections'] and hasattr(model, 'skip_connections')
        })
        
        # Analyze activation function specifics
        activation = feature_analysis['activation_function']
        if hasattr(model, 'activation_param'):
            feature_analysis['activation_parameters'] = getattr(model, 'activation_param', {})
        elif hasattr(model, 'activation_fn'):
            feature_analysis['activation_function'] = str(type(getattr(model, 'activation_fn', lambda x: x)).__name__)
        
        # Count normalization layers directly
        if feature_analysis['normalization_type'] == 'batch':
            batch_norm_count = 0
            for module in model.modules():
                if isinstance(module, torch.nn.BatchNorm1d):
                    batch_norm_count += 1
            feature_analysis['batch_norm_layers'] = batch_norm_count
        elif feature_analysis['normalization_type'] == 'layer':
            layer_norm_count = 0
            for module in model.modules():
                if isinstance(module, torch.nn.LayerNorm):
                    layer_norm_count += 1
            feature_analysis['layer_norm_layers'] = layer_norm_count
        
        # Calculate complexity score directly
        complexity = 0
        
        # Base complexity
        if feature_analysis.get('supports_attention', False):
            complexity += 2
        if feature_analysis.get('supports_residual_blocks', False):
            complexity += 2
        if feature_analysis.get('supports_skip_connections', False):
            complexity += 1
        if feature_analysis.get('has_batch_norm', False) or feature_analysis.get('has_layer_norm', False):
            complexity += 1
        if feature_analysis.get('ensemble_size', 1) > 1:
            complexity += 3
        if feature_analysis.get('mixed_precision_compatible', False):
            complexity += 1
        
        # Additional complexity factors
        norm_type = feature_analysis.get('normalization_type', 'none')
        if norm_type != 'none':
            complexity += 1
        
        # Model type specific complexity
        model_type = feature_analysis.get('model_type', '')
        if model_type == 'EnhancedAutoencoder':
            complexity += 2
        elif model_type == 'AutoencoderEnsemble':
            complexity += 3
        
        feature_analysis['complexity_score'] = complexity
        feature_analysis['complexity_level'] = (
            'high' if complexity > 7 else
            'medium' if complexity > 3 else
            'low'
        )
        
        logger.debug(f"Completed feature analysis for {model_name}")
        return feature_analysis
        
    except Exception as e:
        logger.error(f"Feature analysis failed for {model_name}: {str(e)}")
        # Return basic feature analysis with error information
        return {
            'feature_status': 'failed',
            'error': str(e),
            'model_type': model_name,
            'supports_attention': False,
            'supports_residual_blocks': False,
            'supports_skip_connections': False,
            'normalization_type': 'unknown',
            'activation_function': 'unknown',
            'mixed_precision_compatible': False,
            'ensemble_size': 1
        }

def generate_model_recommendations(
    model_name: str,
    total_params: int,
    hardware_data: Dict[str, Any],
    performance_metrics: Dict[str, Any]
) -> List[str]:
    """
    Generate specific recommendations for a model based on its characteristics.
    
    This function provides tailored recommendations based on model architecture,
    performance metrics, and hardware context. Harmonized with compare_model_architectures().
    
    Args:
        model_name: Name of the model
        total_params: Number of parameters
        hardware_data: Hardware information from system context
        performance_metrics: Performance measurement results
        
    Returns:
        List of recommendation strings
    """
    recommendations = []
    
    try:
        # Parameter-based recommendations
        if total_params < 10000:
            recommendations.extend([
                "Excellent for prototyping and development",
                "Minimal resource requirements",
                "Fast training and inference"
            ])
        elif total_params < 100000:
            recommendations.extend([
                "Good balance of complexity and efficiency",
                "Suitable for most production environments",
                "Reasonable training times"
            ])
        elif total_params < 1000000:
            recommendations.extend([
                "High capacity model for complex tasks",
                "Requires adequate hardware resources",
                "Consider mixed precision training"
            ])
        else:
            recommendations.extend([
                "Large model requiring significant resources",
                "Best suited for specialized hardware",
                "Distributed training may be beneficial"
            ])
        
        # Performance-based recommendations
        inference_fps = performance_metrics.get('inference_fps', 0)
        if inference_fps > 1000:
            recommendations.append("Excellent for real-time applications")
        elif inference_fps > 100:
            recommendations.append("Suitable for interactive applications")
        elif inference_fps < 10:
            recommendations.append("May not be suitable for real-time use")
        
        # Memory-based recommendations
        gpu_memory_mb = performance_metrics.get('gpu_memory_mb', 0)
        if gpu_memory_mb > 0:
            if gpu_memory_mb < 100:
                recommendations.append("Low GPU memory footprint")
            elif gpu_memory_mb < 1000:
                recommendations.append("Moderate GPU memory requirements")
            else:
                recommendations.append("High GPU memory requirements")
        
        # Hardware-specific recommendations
        gpu_available = hardware_data.get('gpu_available', False)
        gpu_memory_gb = hardware_data.get('gpu_memory_gb', 0)
        
        if not gpu_available:
            if total_params < 50000:
                recommendations.append("Suitable for CPU-only environments")
            else:
                recommendations.append("GPU strongly recommended for reasonable performance")
        elif gpu_memory_gb > 0:
            # Using >80% of GPU memory
            if gpu_memory_mb / 1024 > gpu_memory_gb * 0.8:
                recommendations.append("Consider reducing batch size or model complexity")
            # Using <30% of GPU memory
            elif gpu_memory_mb / 1024 < gpu_memory_gb * 0.3:
                recommendations.append("Could increase batch size for better GPU utilization")
        
        # Model-specific recommendations
        if model_name == 'SimpleAutoencoder':
            recommendations.extend([
                "Ideal for baseline comparisons",
                "Good starting point for hyperparameter tuning",
                "Limited capacity for complex patterns"
            ])
        elif model_name == 'EnhancedAutoencoder':
            recommendations.extend([
                "Configurable complexity for different requirements",
                "Production-ready with good performance",
                "Supports advanced training techniques"
            ])
        elif model_name == 'AutoencoderEnsemble':
            recommendations.extend([
                "Maximum accuracy for critical applications",
                "Requires more resources but provides robustness",
                "Consider ensemble size vs. resource trade-offs"
            ])
        
        # Training time recommendations
        avg_inference_time = performance_metrics.get('avg_inference_time_ms', 0)
        # >100ms inference time is considered long
        if avg_inference_time > 100:
            recommendations.append("Long inference time - consider model optimization")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_recommendations = []
        for rec in recommendations:
            if rec not in seen:
                seen.add(rec)
                unique_recommendations.append(rec)
        
        # Limit to top 10 recommendations
        return unique_recommendations[:10]
        
    except Exception as e:
        logger.debug(f"Error generating recommendations for {model_name}: {e}")
        return [f"Analysis completed for {model_name}", "Review performance metrics for details"]

def generate_comparative_summary(
    results: Dict[str, Any],
    hardware_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Generate comprehensive comparative summary and recommendations.
    
    This function analyzes all model results and provides comparative analysis,
    rankings, and recommendations. Harmonized with compare_model_architectures().
    
    Args:
        results: Complete results dictionary from model comparison
        hardware_data: Hardware context information from system analysis
        
    Returns:
        Dictionary containing comparative analysis and recommendations
    """
    try:
        summary = {
            'recommendations': [],
            'warnings': [],
            'optimal_choices': {},
            'performance_ranking': {},
            'resource_efficiency': {},
            'use_case_recommendations': {},
            'compatibility_matrix': {},
            'optimization_suggestions': []
        }
        
        # Extract successful model results
        model_results = {}
        for key, value in results.items():
            if not key.startswith('_') and isinstance(value, dict) and 'error' not in value:
                model_results[key] = value
        
        if not model_results:
            summary['warnings'].append("No models were successfully analyzed")
            return summary
        
        # Performance ranking
        performance_metrics = {}
        
        for model_name, data in model_results.items():
            arch = data.get('architecture', {})
            perf = data.get('performance', {})
            resources = data.get('resource_requirements', {})
            
            # Collect metrics for ranking
            performance_metrics[model_name] = {
                'param_count': arch.get('total_params', 0),
                'inference_fps': perf.get('inference_fps', 0),
                'model_size_mb': arch.get('model_size_mb', 0),
                'memory_requirement_mb': resources.get('memory', {}).get('total_with_overhead_mb', 0)
            }
        
        # Rank by different criteria
        rankings = {}
        
        # Speed ranking (highest FPS first)
        speed_ranking = sorted(
            performance_metrics.items(),
            key=lambda x: x[1]['inference_fps'],
            reverse=True
        )
        rankings['speed'] = [name for name, _ in speed_ranking]
        
        # Efficiency ranking (highest FPS per parameter)
        efficiency_ranking = sorted(
            performance_metrics.items(),
            key=lambda x: x[1]['inference_fps'] / max(x[1]['param_count'], 1),
            reverse=True
        )
        rankings['efficiency'] = [name for name, _ in efficiency_ranking]
        
        # Memory efficiency ranking (lowest memory usage)
        memory_ranking = sorted(
            performance_metrics.items(),
            key=lambda x: x[1]['memory_requirement_mb']
        )
        rankings['memory_efficiency'] = [name for name, _ in memory_ranking]
        
        # Size ranking (smallest model first)
        size_ranking = sorted(
            performance_metrics.items(),
            key=lambda x: x[1]['param_count']
        )
        rankings['size'] = [name for name, _ in size_ranking]
        
        summary['performance_ranking'] = rankings
        
        # Optimal choices for different scenarios
        optimal_choices = {}
        
        if speed_ranking:
            optimal_choices['fastest_inference'] = speed_ranking[0][0]
        
        if efficiency_ranking:
            optimal_choices['most_efficient'] = efficiency_ranking[0][0]
        
        if memory_ranking:
            optimal_choices['lowest_memory'] = memory_ranking[0][0]
        
        if size_ranking:
            optimal_choices['smallest_model'] = size_ranking[0][0]
        
        # Balanced recommendation (consider multiple factors)
        balanced_scores = {}
        for model_name, metrics in performance_metrics.items():
            # Normalize metrics (0-1 scale)
            max_fps = max(m['inference_fps'] for m in performance_metrics.values())
            max_params = max(m['param_count'] for m in performance_metrics.values())
            max_memory = max(m['memory_requirement_mb'] for m in performance_metrics.values())
            
            if max_fps > 0 and max_params > 0 and max_memory > 0:
                speed_score = metrics['inference_fps'] / max_fps
                # Smaller is better
                size_score = 1.0 - (metrics['param_count'] / max_params)
                # Less is better
                memory_score = 1.0 - (metrics['memory_requirement_mb'] / max_memory)
                
                # Weighted combination
                balanced_scores[model_name] = (speed_score * 0.4 + size_score * 0.3 + memory_score * 0.3)
        
        if balanced_scores:
            best_balanced = max(balanced_scores.items(), key=lambda x: x[1])
            optimal_choices['best_balanced'] = best_balanced[0]
        
        summary['optimal_choices'] = optimal_choices
        
        # Use case recommendations
        use_case_recs = {
            'prototyping_development': [],
            'production_deployment': [],
            'resource_constrained': [],
            'high_performance': [],
            'research_experimentation': []
        }
        
        for model_name, data in model_results.items():
            arch = data.get('architecture', {})
            param_count = arch.get('total_params', 0)
            complexity = arch.get('complexity_level', 'unknown')
            use_cases = data.get('use_cases', [])
            
            # Categorize by parameter count and complexity
            if param_count < 50000 or complexity == 'low':
                use_case_recs['prototyping_development'].append(model_name)
                use_case_recs['resource_constrained'].append(model_name)
            
            if 10000 < param_count < 500000 or complexity == 'medium':
                use_case_recs['production_deployment'].append(model_name)
            
            if param_count > 100000 or complexity in ['high', 'very_high']:
                use_case_recs['high_performance'].append(model_name)
                use_case_recs['research_experimentation'].append(model_name)
        
        summary['use_case_recommendations'] = use_case_recs
        
        # Generate overall recommendations
        recommendations = []
        
        # Hardware-based recommendations
        gpu_available = hardware_data.get('gpu_available', False)
        gpu_memory_gb = hardware_data.get('gpu_memory_gb', 0)
        
        if not gpu_available:
            recommendations.append("GPU acceleration not available - consider smallest models for reasonable performance")
            if use_case_recs['resource_constrained']:
                recommendations.append(f"For CPU-only: Recommended models: {', '.join(use_case_recs['resource_constrained'][:2])}")
        elif gpu_memory_gb < 4:
            recommendations.append("Limited GPU memory - avoid largest models or reduce batch sizes")
        elif gpu_memory_gb >= 8:
            recommendations.append("Adequate GPU memory - all models should run efficiently")
        
        # Performance-based recommendations
        if speed_ranking:
            fastest_model = speed_ranking[0][0]
            fastest_fps = speed_ranking[0][1]['inference_fps']
            recommendations.append(f"Fastest inference: {fastest_model} ({fastest_fps:.1f} samples/sec)")
        
        if optimal_choices.get('best_balanced'):
            recommendations.append(f"Best overall balance: {optimal_choices['best_balanced']}")
        
        # Resource efficiency recommendations
        if efficiency_ranking:
            most_efficient = efficiency_ranking[0][0]
            recommendations.append(f"Most parameter-efficient: {most_efficient}")
        
        summary['recommendations'] = recommendations
        
        # Generate warnings
        warnings = []
        
        # Check for potential issues
        for model_name, data in model_results.items():
            resources = data.get('resource_requirements', {})
            memory_req = resources.get('memory', {}).get('recommended_system_memory_gb', 0)
            
            if memory_req > 32:
                warnings.append(f"{model_name} requires >32GB RAM - ensure adequate system memory")
            
            time_estimates = resources.get('time_estimates', {})
            minutes_per_epoch = time_estimates.get('minutes_per_epoch', 0)
            
            # >1 hour per epoch
            if minutes_per_epoch > 60:
                warnings.append(f"{model_name} estimated >1 hour per training epoch")
        
        # Hardware warnings
        if not gpu_available and any(metrics['param_count'] > 100000 for metrics in performance_metrics.values()):
            warnings.append("Large models detected but no GPU available - training will be very slow")
        
        summary['warnings'] = warnings
        
        # Resource efficiency summary with integrated efficiency classification
        efficiency_summary = {}
        for model_name, metrics in performance_metrics.items():
            if metrics['param_count'] > 0:
                fps_per_param = metrics['inference_fps'] / metrics['param_count']
                fps_per_mb = metrics['inference_fps'] / max(metrics['memory_requirement_mb'], 1)
                
                # Classify efficiency directly
                try:
                    # Normalize and combine metrics
                    # Scale and cap at 10
                    param_score = min(fps_per_param * 1000, 10)
                    # Scale and cap at 10
                    memory_score = min(fps_per_mb / 10, 10)
                    
                    combined_score = (param_score + memory_score) / 2
                    
                    if combined_score >= 7:
                        efficiency_class = 'excellent'
                    elif combined_score >= 5:
                        efficiency_class = 'good'
                    elif combined_score >= 3:
                        efficiency_class = 'moderate'
                    elif combined_score >= 1:
                        efficiency_class = 'poor'
                    else:
                        efficiency_class = 'very_poor'
                except:
                    efficiency_class = 'unknown'
                
                efficiency_summary[model_name] = {
                    'fps_per_parameter': fps_per_param,
                    'fps_per_mb_memory': fps_per_mb,
                    'efficiency_class': efficiency_class
                }
        
        summary['resource_efficiency'] = efficiency_summary
        
        logger.debug(f"Comparative summary generated: {len(recommendations)} recommendations, {len(warnings)} warnings")
        
        return summary
        
    except Exception as e:
        logger.warning(f"Error generating comparative summary: {e}")
        return {
            'recommendations': ["Comparison completed with errors - review individual model results"],
            'warnings': [f"Summary generation failed: {str(e)}"],
            'optimal_choices': {},
            'performance_ranking': {},
            'resource_efficiency': {},
            'use_case_recommendations': {}
        }

# Display model comparison


# Model variants validation
def validate_model_preset_compatibility(model_type: str, config: Dict[str, Any]) -> bool:
    """
    Comprehensive validation of model type compatibility with preset configurations.
    
    This function has been updated to work harmoniously with initialize_model_variants() and 
    validate_model_variants(), providing thorough compatibility validation for all preset 
    configurations including DEFAULT_PRESET, STABILITY_PRESET, PERFORMANCE_PRESET, and 
    custom configurations while maintaining consistency with the validation approach.
    
    Args:
        model_type: The model type to validate (e.g., 'SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble')
        config: Configuration dictionary (can be preset or full config structure)
        
    Returns:
        bool: True if compatible, False otherwise with detailed logging of incompatibility reasons
    """
    compatibility_start_time = time.time()
    validation_details = {
        'model_type': model_type,
        'compatibility_checks': [],
        'warnings': [],
        'errors': [],
        'recommendations': []
    }
    
    try:
        # Phase 1: Basic Input Validation (aligned with validate_model_variants approach)
        if not model_type or not isinstance(model_type, str):
            validation_details['errors'].append('Invalid model_type: must be non-empty string')
            logger.debug("validate_model_preset_compatibility: Invalid model_type provided")
            return False
        
        if not config or not isinstance(config, dict):
            validation_details['errors'].append('Invalid config: must be non-empty dictionary')
            logger.debug("validate_model_preset_compatibility: Invalid config provided")
            return False
        
        validation_details['compatibility_checks'].append('basic_input_validation')
        
        # Phase 2: Model Variants Availability Check (harmonized with initialize_model_variants)
        if not MODEL_VARIANTS:
            logger.debug("MODEL_VARIANTS not initialized, attempting initialization for compatibility check")
            try:
                #initialize_model_variants(silent=True)
                initialize_model_variants(silent=False)
            except Exception as e:
                validation_details['warnings'].append(f'MODEL_VARIANTS initialization failed: {str(e)}')
                logger.warning(f"Failed to initialize model variants for compatibility check: {e}")
                # Fallback to basic string validation (consistent with existing approach)
                valid_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                if model_type not in valid_types:
                    validation_details['errors'].append(f'Model type "{model_type}" not in known types: {valid_types}')
                    return False
                validation_details['warnings'].append('Using fallback validation without MODEL_VARIANTS')
        
        if MODEL_VARIANTS and model_type not in MODEL_VARIANTS:
            validation_details['errors'].append(f'Model type "{model_type}" not found in MODEL_VARIANTS')
            logger.debug(f"Model type '{model_type}' not found in MODEL_VARIANTS: {list(MODEL_VARIANTS.keys())}")
            return False
        
        validation_details['compatibility_checks'].append('model_variants_validation')
        
        # Phase 3: Configuration Structure Analysis (enhanced but aligned)
        try:
            config_structure_type = 'unknown'
            metadata = config.get('metadata', {})
            model_config = config.get('model', {})
            training_config = config.get('training', {})
            
            # Determine configuration structure type
            if 'metadata' in config and 'model' in config and 'training' in config:
                config_structure_type = 'full_preset'
                preset_name = config.get('presets', {}).get('current_preset') or metadata.get('preset_used')
            elif 'preset_used' in config or 'preset_used' in metadata:
                config_structure_type = 'preset_reference'
                preset_name = config.get('preset_used') or metadata.get('preset_used')
            elif model_config:
                config_structure_type = 'partial_config'
                preset_name = model_config.get('preset_used')
            else:
                config_structure_type = 'minimal_config'
                preset_name = None
            
            validation_details['config_structure_type'] = config_structure_type
            validation_details['preset_name'] = preset_name
            validation_details['compatibility_checks'].append('config_structure_analysis')
            
        except Exception as e:
            validation_details['warnings'].append(f'Config structure analysis failed: {str(e)}')
            logger.debug(f"Config structure analysis failed: {e}")
        
        # Phase 4: Explicit Compatibility List Validation
        try:
            compatible_models = metadata.get('compatibility', [])
            if compatible_models and isinstance(compatible_models, list):
                if model_type not in compatible_models:
                    validation_details['errors'].append(
                        f'Model type "{model_type}" not in explicit compatibility list: {compatible_models}'
                    )
                    logger.debug(f"Model type '{model_type}' not in compatibility list: {compatible_models}")
                    return False
                else:
                    validation_details['compatibility_checks'].append('explicit_compatibility_list')
                    logger.debug(f"Model type '{model_type}' found in explicit compatibility list")
            
        except Exception as e:
            validation_details['warnings'].append(f'Explicit compatibility validation failed: {str(e)}')
            logger.debug(f"Error validating explicit compatibility: {e}")
        
        # Phase 5: Model-Specific Architecture Requirements Validation (aligned with test configurations)
        try:
            if model_type == 'SimpleAutoencoder':
                validation_details['compatibility_checks'].append('simpleautoencoder_validation')
                
                # Simple autoencoder requirements - basic parameters only (consistent with test config)
                encoding_dim = model_config.get('encoding_dim', 12)
                if not isinstance(encoding_dim, (int, float)) or encoding_dim <= 0:
                    validation_details['errors'].append(f'SimpleAutoencoder: Invalid encoding_dim: {encoding_dim}')
                    logger.debug(f"SimpleAutoencoder: Invalid encoding_dim: {encoding_dim}")
                    return False
                
                # Ensure simple architecture (no complex features) - aligned with _create_model_test_config
                if model_config.get('use_attention', False):
                    validation_details['warnings'].append('SimpleAutoencoder: use_attention should be False for simple architecture')
                
                if model_config.get('residual_blocks', False):
                    validation_details['warnings'].append('SimpleAutoencoder: residual_blocks should be False for simple architecture')
                
                # Validate hidden dimensions for simple architecture
                hidden_dims = model_config.get('hidden_dims', [128])
                if isinstance(hidden_dims, list) and len(hidden_dims) > 2:
                    validation_details['warnings'].append(f'SimpleAutoencoder: Deep architecture with {len(hidden_dims)} layers may be too complex')
                
                # Validate dropout rates (consistent with _validate_and_adjust_parameters approach)
                dropout_rates = model_config.get('dropout_rates', [0.2])
                if isinstance(dropout_rates, list):
                    invalid_rates = [r for r in dropout_rates if not isinstance(r, (int, float)) or r < 0 or r >= 1]
                    if invalid_rates:
                        validation_details['errors'].append(f'SimpleAutoencoder: Invalid dropout rates: {invalid_rates}')
                        return False
                
                validation_details['recommendations'].append('SimpleAutoencoder works best with simple architectures and basic features')
                
            elif model_type == 'EnhancedAutoencoder':
                validation_details['compatibility_checks'].append('enhancedautoencoder_validation')
                
                # Enhanced autoencoder requirements - supports advanced features
                encoding_dim = model_config.get('encoding_dim', 32)
                if not isinstance(encoding_dim, (int, float)) or encoding_dim <= 0:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Invalid encoding_dim: {encoding_dim}')
                    return False
                
                # Validate hidden dimensions (consistent with parameter validation)
                hidden_dims = model_config.get('hidden_dims', [256, 128, 64])
                if not isinstance(hidden_dims, list) or not hidden_dims:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Invalid hidden_dims: {hidden_dims}')
                    return False
                
                # Check for invalid dimension values (aligned with _validate_and_adjust_parameters)
                invalid_dims = [dim for dim in hidden_dims if not isinstance(dim, (int, float)) or dim <= 0]
                if invalid_dims:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Invalid dimension values: {invalid_dims}')
                    return False
                
                # Validate dropout rates (consistent with existing validation approach)
                dropout_rates = model_config.get('dropout_rates', [0.2, 0.15, 0.1])
                if not isinstance(dropout_rates, list) or not dropout_rates:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Invalid dropout_rates: {dropout_rates}')
                    return False
                
                invalid_rates = [r for r in dropout_rates if not isinstance(r, (int, float)) or r < 0 or r >= 1]
                if invalid_rates:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Invalid dropout rate values: {invalid_rates}')
                    return False
                
                # Advanced feature compatibility checks (aligned with test configurations)
                use_attention = model_config.get('use_attention', False)
                if use_attention and encoding_dim < 32:
                    validation_details['warnings'].append(f'EnhancedAutoencoder: Attention mechanism may not be effective with small encoding_dim: {encoding_dim}')
                
                residual_blocks = model_config.get('residual_blocks', False)
                if residual_blocks and not hidden_dims:
                    validation_details['warnings'].append('EnhancedAutoencoder: Residual blocks require hidden layers')
                
                # Normalization compatibility (consistent with _extract_and_validate_config_param)
                normalization = model_config.get('normalization', 'batch')
                available_normalizations = model_config.get('available_normalizations', ['batch', 'layer', 'instance', 'group', 'none'])
                if normalization and normalization not in available_normalizations:
                    validation_details['errors'].append(f'EnhancedAutoencoder: Normalization "{normalization}" not in available list: {available_normalizations}')
                    return False
                
                validation_details['recommendations'].append('EnhancedAutoencoder supports advanced features like attention and residual connections')
                
            elif model_type == 'AutoencoderEnsemble':
                validation_details['compatibility_checks'].append('autoencoder_ensemble_validation')
                
                # Ensemble-specific requirements (aligned with test configuration constraints)
                num_models = model_config.get('num_models', 3)
                if not isinstance(num_models, int) or num_models < 1:
                    validation_details['errors'].append(f'AutoencoderEnsemble: Invalid num_models: {num_models} (must be positive integer)')
                    return False
                
                if num_models > 20:
                    validation_details['warnings'].append(f'AutoencoderEnsemble: Large ensemble size ({num_models}) may be memory intensive')
                
                diversity_factor = model_config.get('diversity_factor', 0.3)
                if not isinstance(diversity_factor, (int, float)) or not 0 <= diversity_factor <= 1:
                    validation_details['errors'].append(f'AutoencoderEnsemble: Invalid diversity_factor: {diversity_factor} (must be between 0 and 1)')
                    return False
                
                # Basic architecture validation (same as enhanced, consistent with test configs)
                encoding_dim = model_config.get('encoding_dim', 24)
                if not isinstance(encoding_dim, (int, float)) or encoding_dim <= 0:
                    validation_details['errors'].append(f'AutoencoderEnsemble: Invalid encoding_dim: {encoding_dim}')
                    return False
                
                hidden_dims = model_config.get('hidden_dims', [192, 96, 48])
                if not isinstance(hidden_dims, list) or not hidden_dims:
                    validation_details['errors'].append(f'AutoencoderEnsemble: Invalid hidden_dims: {hidden_dims}')
                    return False
                
                # Memory and computational requirements (aligned with resource estimation approach)
                total_params_estimate = sum(hidden_dims) * num_models + encoding_dim * num_models
                if total_params_estimate > 1_000_000:  # 1M parameters threshold
                    validation_details['warnings'].append(f'AutoencoderEnsemble: Large parameter count estimate ({total_params_estimate:,}) may require significant memory')
                
                validation_details['recommendations'].append('AutoencoderEnsemble provides improved robustness through model diversity')
                
            else:
                validation_details['warnings'].append(f'Unknown model type "{model_type}" - using generic validation')
                
        except Exception as e:
            validation_details['errors'].append(f'Model-specific validation failed: {str(e)}')
            logger.error(f"Model-specific validation failed for {model_type}: {e}")
            return False
        
        # Phase 6: Preset-Specific Compatibility Validation (enhanced but consistent)
        try:
            if preset_name and preset_name in globals().get('PRESET_CONFIGS', {}):
                validation_details['compatibility_checks'].append('preset_specific_validation')
                
                preset_config = globals()['PRESET_CONFIGS'][preset_name]
                preset_metadata = preset_config.get('metadata', {})
                preset_compatible_models = preset_metadata.get('compatibility', [])
                
                if preset_compatible_models and model_type not in preset_compatible_models:
                    validation_details['errors'].append(f'Model type "{model_type}" not compatible with preset "{preset_name}" (compatible: {preset_compatible_models})')
                    logger.debug(f"Model type '{model_type}' not compatible with preset '{preset_name}'")
                    return False
                
                # Validate preset's model configuration compatibility
                preset_model_config = preset_config.get('model', {})
                preset_model_type = preset_model_config.get('model_type')
                
                if preset_model_type and preset_model_type != model_type:
                    validation_details['warnings'].append(f'Preset "{preset_name}" configured for "{preset_model_type}", requested "{model_type}" (may work but not optimal)')
                
                # Check preset-specific constraints (aligned with configuration validation)
                if preset_name == 'STABILITY_PRESET':
                    # Stability preset should use conservative settings
                    if model_config.get('dropout_rates', []):
                        high_dropout = [r for r in model_config['dropout_rates'] if isinstance(r, (int, float)) and r > 0.3]
                        if high_dropout:
                            validation_details['warnings'].append(f'STABILITY_PRESET: High dropout rates may impact stability: {high_dropout}')
                    
                elif preset_name == 'PERFORMANCE_PRESET':
                    # Performance preset should support advanced features
                    if model_type == 'SimpleAutoencoder':
                        validation_details['warnings'].append('PERFORMANCE_PRESET: SimpleAutoencoder may not utilize performance optimizations fully')
                    
                    mixed_precision = training_config.get('mixed_precision', False)
                    if not mixed_precision and torch.cuda.is_available():
                        validation_details['recommendations'].append('PERFORMANCE_PRESET: Consider enabling mixed_precision for better performance')
                
                elif preset_name == 'DEFAULT_PRESET':
                    # Default preset should work with all model types
                    pass
                
                validation_details['preset_validation_completed'] = preset_name
                
        except Exception as e:
            validation_details['warnings'].append(f'Preset-specific validation failed: {str(e)}')
            logger.debug(f"Error during preset validation: {e}")
        
        # Phase 7: Hardware Requirements Validation (aligned with system analysis approach)
        try:
            validation_details['compatibility_checks'].append('hardware_requirements_validation')
            
            hardware_config = config.get('hardware', {})
            if hardware_config:
                # Memory requirements estimation (consistent with resource estimation)
                min_gpu_memory = hardware_config.get('minimum_system_requirements', {}).get('gpu_memory_gb', 0)
                
                # Model-specific memory requirements in GB (aligned with resource analysis)
                memory_requirement = 0
                if model_type == 'SimpleAutoencoder':
                    memory_requirement = 1
                elif model_type == 'EnhancedAutoencoder':
                    memory_requirement = 2
                elif model_type == 'AutoencoderEnsemble':
                    num_models = model_config.get('num_models', 3)
                    memory_requirement = 1.5 * num_models
                
                if min_gpu_memory > 0 and memory_requirement > min_gpu_memory:
                    validation_details['warnings'].append(f'Estimated memory requirement ({memory_requirement:.1f}GB) exceeds minimum specified ({min_gpu_memory}GB)')
                
                # Device compatibility (consistent with hardware context validation)
                device = hardware_config.get('device', 'auto')
                mixed_precision = training_config.get('mixed_precision', False)
                
                if mixed_precision and device == 'cpu':
                    validation_details['warnings'].append('Mixed precision enabled but device is CPU (mixed precision requires CUDA)')
                
                # CUDA availability check (aligned with system validation)
                if device == 'cuda' and not torch.cuda.is_available():
                    validation_details['errors'].append('CUDA device specified but CUDA is not available')
                    return False
                
        except Exception as e:
            validation_details['warnings'].append(f'Hardware requirements validation failed: {str(e)}')
            logger.debug(f"Error during hardware validation: {e}")
        
        # Phase 8: Activation Function Compatibility (consistent with config parameter extraction)
        try:
            validation_details['compatibility_checks'].append('activation_compatibility_validation')
            
            activation = model_config.get('activation', 'leaky_relu')
            available_activations = model_config.get('available_activations', [
                'relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu', 'prelu'
            ])
            
            if activation and activation not in available_activations:
                validation_details['errors'].append(f'Activation function "{activation}" not in available list: {available_activations}')
                return False
            
            # Model-specific activation recommendations (aligned with test configurations)
            if model_type == 'SimpleAutoencoder' and activation in ['gelu', 'swish', 'selu']:
                validation_details['warnings'].append(f'SimpleAutoencoder: Advanced activation "{activation}" may be overkill for simple architecture')
            
            activation_param = model_config.get('activation_param', 0.2)
            if activation == 'leaky_relu' and not isinstance(activation_param, (int, float)):
                validation_details['errors'].append(f'LeakyReLU requires numeric activation_param, got: {activation_param}')
                return False
            
        except Exception as e:
            validation_details['warnings'].append(f'Activation compatibility validation failed: {str(e)}')
            logger.debug(f"Error during activation validation: {e}")
        
        # Phase 9: Training Configuration Compatibility (aligned with functional testing approach)
        try:
            validation_details['compatibility_checks'].append('training_compatibility_validation')
            
            if training_config:
                batch_size = training_config.get('batch_size', 32)
                
                # Batch normalization compatibility (consistent with test scenario filtering)
                use_batch_norm = model_config.get('use_batch_norm', False)
                normalization = model_config.get('normalization', 'batch')
                
                if (use_batch_norm or normalization == 'batch') and batch_size < 2:
                    validation_details['errors'].append(f'Batch size {batch_size} too small for batch normalization (minimum 2)')
                    return False
                
                if batch_size < 1:
                    validation_details['errors'].append(f'Invalid batch size: {batch_size}')
                    return False
                
                # Ensemble-specific training compatibility (aligned with scaling analysis)
                if model_type == 'AutoencoderEnsemble':
                    num_models = model_config.get('num_models', 3)
                    if batch_size < num_models:
                        validation_details['warnings'].append(f'Batch size ({batch_size}) smaller than ensemble size ({num_models}) may impact training efficiency')
                
                # Learning rate validation (consistent with parameter extraction validation)
                learning_rate = training_config.get('learning_rate', 0.001)
                if not isinstance(learning_rate, (int, float)) or learning_rate <= 0:
                    validation_details['errors'].append(f'Invalid learning rate: {learning_rate}')
                    return False
                
                # Optimizer compatibility (aligned with resource estimation approach)
                optimizer = training_config.get('optimizer', 'AdamW')
                available_optimizers = ['Adam', 'AdamW', 'SGD', 'RMSprop', 'Adagrad']
                if optimizer not in available_optimizers:
                    validation_details['warnings'].append(f'Optimizer "{optimizer}" may not be supported (available: {available_optimizers})')
                
        except Exception as e:
            validation_details['warnings'].append(f'Training compatibility validation failed: {str(e)}')
            logger.debug(f"Error during training validation: {e}")
        
        # Phase 10: Data Configuration Compatibility (aligned with input dimension validation)
        try:
            validation_details['compatibility_checks'].append('data_compatibility_validation')
            
            data_config = config.get('data', {})
            if data_config:
                features = data_config.get('features', 20)
                min_features = model_config.get('min_features', 5)
                
                if not isinstance(features, int) or features < min_features:
                    validation_details['errors'].append(f'Feature count {features} below minimum required {min_features}')
                    return False
                
                # Model-specific feature requirements (consistent with architectural analysis)
                if model_type == 'EnhancedAutoencoder' and features < 10:
                    validation_details['warnings'].append(f'EnhancedAutoencoder: Small feature count ({features}) may not benefit from advanced features')
                
                encoding_dim = model_config.get('encoding_dim', 16)
                if encoding_dim >= features:
                    validation_details['warnings'].append(f'Encoding dimension ({encoding_dim}) should be smaller than input features ({features}) for compression')
                
        except Exception as e:
            validation_details['warnings'].append(f'Data compatibility validation failed: {str(e)}')
            logger.debug(f"Error during data validation: {e}")
        
        # Phase 11: Experimental Features Validation (consistent with configuration analysis)
        try:
            validation_details['compatibility_checks'].append('experimental_features_validation')
            
            experimental_config = config.get('experimental', {})
            if experimental_config:
                experimental_features = experimental_config.get('experimental_features', {})
                
                if experimental_features and experimental_features.get('enabled', False):
                    if model_type == 'SimpleAutoencoder':
                        validation_details['warnings'].append('SimpleAutoencoder: Experimental features may not be supported')
                    
                    validation_details['recommendations'].append('Experimental features should be used with caution in production')
                
        except Exception as e:
            validation_details['warnings'].append(f'Experimental features validation failed: {str(e)}')
            logger.debug(f"Error during experimental features validation: {e}")
        
        # Phase 12: Comprehensive Results Analysis and Memory Optimization (aligned with existing functions)
        validation_time = time.time() - compatibility_start_time
        validation_details['validation_time_seconds'] = validation_time
        validation_details['total_checks_performed'] = len(validation_details['compatibility_checks'])
        validation_details['total_warnings'] = len(validation_details['warnings'])
        validation_details['total_errors'] = len(validation_details['errors'])
        
        # Determine final compatibility result (consistent with validate_model_variants approach)
        has_critical_errors = len(validation_details['errors']) > 0
        has_warnings = len(validation_details['warnings']) > 0
        
        if has_critical_errors:
            logger.debug(f"Model type '{model_type}' is NOT compatible: {len(validation_details['errors'])} errors found")
            # Log first 3 errors for debugging
            logger.debug(f"Compatibility errors: {validation_details['errors'][:3]}...")
            return False
        
        # Log detailed results (aligned with existing logging approach)
        if has_warnings:
            logger.debug(f"Model type '{model_type}' is compatible with {len(validation_details['warnings'])} warnings")
            # Log first 2 warnings for debugging
            logger.debug(f"Compatibility warnings: {validation_details['warnings'][:2]}...")
        else:
            logger.debug(f"Model type '{model_type}' is fully compatible - all {validation_details['total_checks_performed']} checks passed")
        
        # Store validation details for debugging (consistent with existing approach)
        if hasattr(validate_model_preset_compatibility, 'last_validation_details'):
            validate_model_preset_compatibility.last_validation_details = validation_details
        
        return True
        
    except Exception as e:
        validation_details['errors'].append(f'Unexpected validation error: {str(e)}')
        logger.error(f"Unexpected error during model-preset compatibility validation: {e}")
        logger.debug(f"Validation details when error occurred: {validation_details}")
        
        # In case of validation errors, default to compatible to avoid blocking functionality
        # but log the issue for investigation (consistent with existing error handling)
        logger.warning(f"Defaulting to compatible due to validation error for {model_type}")
        return True
    
    finally:
        # Always log final timing information (aligned with existing timing approach)
        total_time = time.time() - compatibility_start_time
        logger.debug(f"Model-preset compatibility validation completed in {total_time:.3f}s: "
                    f"model='{model_type}', checks={validation_details.get('total_checks_performed', 0)}, "
                    f"warnings={validation_details.get('total_warnings', 0)}, "
                    f"errors={validation_details.get('total_errors', 0)}")

def display_model_initialization_summary(
    model_instance,
    model_type: str,
    input_dim: int,
    architecture_info: Dict[str, Any],
    device: torch.device,
    mixed_precision: bool,
    preset_name: Optional[str] = None,
    training_config: Optional[Dict[str, Any]] = None,
    enhanced_features: Optional[Dict[str, bool]] = None,
    ensemble_info: Optional[Dict[str, Any]] = None,
    show_panels: Union[bool, Dict[str, bool]] = False
) -> None:
    """
    Display comprehensive model initialization summary with rich formatting.
    
    This function provides rich console output and logs detailed information to file
    for SimpleAutoencoder, EnhancedAutoencoder, and AutoencoderEnsemble classes.
    
    Args:
        model_instance: The model instance (for parameter counting)
        model_type: Type of model ('SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble')
        input_dim: Input dimension
        architecture_info: Dictionary containing architecture details
        device: PyTorch device
        mixed_precision: Whether mixed precision is enabled
        preset_name: Name of preset used (if any)
        training_config: Training configuration dictionary
        enhanced_features: Dictionary of enhanced features (for EnhancedAutoencoder)
        ensemble_info: Dictionary of ensemble information (for AutoencoderEnsemble)
        show_panels: Control which panels to display. Options:
            - True: Show all panels
            - False: Hide all panels
            - Dict: Selective control with keys:
                - "status": Status & recommendations panel
                - "quick_ref": Quick reference panel
                - "optimization": Optimization tips panel
    """
    try:
        # ENHANCED PRESET NAME DETECTION
        detected_preset_name = preset_name
        
        # Try multiple sources for preset information if not provided
        if not detected_preset_name and hasattr(model_instance, 'config'):
            config = model_instance.config
            # Check multiple locations for preset information
            detected_preset_name = (
                config.get('metadata', {}).get('preset_used') or
                config.get('presets', {}).get('current_preset') or
                config.get('runtime', {}).get('applied_preset') or
                config.get('runtime', {}).get('factory_preset_applied')
            )
        
        # Check model instance attributes
        if not detected_preset_name and hasattr(model_instance, 'preset_name'):
            detected_preset_name = model_instance.preset_name
        
        # Final fallback
        if not detected_preset_name:
            detected_preset_name = "Custom"
        
        # Calculate model parameters
        total_params = sum(p.numel() for p in model_instance.parameters())
        trainable_params = sum(p.numel() for p in model_instance.parameters() if p.requires_grad)
        
        # Parse panel display options
        if isinstance(show_panels, bool):
            show_status = show_panels
            show_quick_ref = show_panels
            show_optimization = show_panels
        else:
            show_status = show_panels.get("status", False)
            show_quick_ref = show_panels.get("quick_ref", False)
            show_optimization = show_panels.get("optimization", False)
        
        # CREATE COMPREHENSIVE LOG DATA DICTIONARY (for file logging only)
        log_data = {
            'model_initialization': {
                'model_type': model_type,
                'input_dim': input_dim,
                'total_parameters': total_params,
                'trainable_parameters': trainable_params,
                'device': str(device),
                'mixed_precision': mixed_precision,
                'preset_name': detected_preset_name,
                'initialization_timestamp': datetime.now().isoformat()
            },
            'architecture_details': {
                'input_dimension': input_dim,
                'hidden_dims': architecture_info.get('hidden_dims', []),
                'encoding_dim': architecture_info.get('encoding_dim', 'Unknown'),
                'architecture_summary': f"{input_dim} -> {architecture_info.get('hidden_dims', [])} -> {architecture_info.get('encoding_dim', 'Unknown')}"
            },
            'model_configuration': {},
            'enhanced_features': enhanced_features or {},
            'ensemble_configuration': ensemble_info or {},
            'training_configuration': training_config or {},
            'system_information': {
                'device': str(device),
                'device_type': device.type,
                'cuda_available': torch.cuda.is_available(),
                'mixed_precision_enabled': mixed_precision
            },
            'memory_estimates': {
                'parameter_memory_mb': total_params * 4 / (1024**2),
                'estimated_training_memory_mb': total_params * 16 / (1024**2),  # Rough estimate
                'model_size_mb': total_params * 4 / (1024**2)
            },
            'warnings_and_recommendations': []
        }
        
        # Add ensemble-specific information to log data
        if ensemble_info:
            log_data['ensemble_details'] = {
                'num_models': ensemble_info.get('num_models', 'Unknown'),
                'model_types': ensemble_info.get('model_types', {}),
                'diversity_factor': ensemble_info.get('diversity_factor', 'Unknown'),
                'ensemble_size': f"{ensemble_info.get('num_models', 'Unknown')} models"
            }
        
        # Add enhanced features details to log data
        if enhanced_features:
            features_enabled = []
            features_disabled = []
            for feature, enabled in enhanced_features.items():
                if enabled:
                    features_enabled.append(feature.replace('_', ' ').title())
                else:
                    features_disabled.append(feature.replace('_', ' ').title())
            
            log_data['enhanced_features_analysis'] = {
                'features_enabled': features_enabled,
                'features_disabled': features_disabled,
                'total_features': len(enhanced_features),
                'enabled_count': len(features_enabled),
                'disabled_count': len(features_disabled)
            }
        
        # Add training configuration details to log data
        if training_config:
            log_data['training_details'] = {
                'optimizer': training_config.get('optimizer', 'Unknown'),
                'learning_rate': training_config.get('learning_rate', 'Unknown'),
                'batch_size': training_config.get('batch_size', 'Unknown'),
                'configuration_complete': True
            }
        else:
            log_data['training_details'] = {
                'configuration_complete': False,
                'status': 'Training components not configured'
            }
        
        # Generate warnings and collect them in log data
        warnings = []
        if hasattr(model_instance, 'mixed_precision') and model_instance.mixed_precision and str(device) == 'cpu':
            warnings.append("Mixed precision enabled but using CPU device")
        
        if ensemble_info and ensemble_info.get('num_models', 0) > 10:
            warnings.append(f"Large ensemble with {ensemble_info.get('num_models')} models may be memory intensive")
        
        if model_type == 'EnhancedAutoencoder' and len(architecture_info.get('hidden_dims', [])) > 8:
            warnings.append(f"Very deep architecture with {len(architecture_info.get('hidden_dims', []))} hidden layers")
        
        if total_params > 10_000_000:  # 10M parameters
            warnings.append(f"Large model with {total_params:,} parameters may require significant memory")
        
        log_data['warnings_and_recommendations'] = warnings
        
        # Add performance analysis to log data
        memory_mb = total_params * 4 / (1024**2)
        log_data['performance_analysis'] = {
            'complexity_level': 'high' if total_params > 1_000_000 else 'medium' if total_params > 100_000 else 'low',
            'memory_classification': 'large' if memory_mb > 100 else 'medium' if memory_mb > 10 else 'small',
            'training_complexity': 'complex' if len(architecture_info.get('hidden_dims', [])) > 3 else 'moderate',
            'hardware_recommendations': []
        }
        
        # Add hardware recommendations to log data
        if torch.cuda.is_available() and "cpu" in str(device).lower():
            log_data['performance_analysis']['hardware_recommendations'].append("GPU available but using CPU - consider GPU acceleration")
        elif not torch.cuda.is_available():
            log_data['performance_analysis']['hardware_recommendations'].append("No GPU available - training will use CPU")
        
        if mixed_precision and torch.cuda.is_available():
            log_data['performance_analysis']['hardware_recommendations'].append("Mixed precision enabled for faster training")
        elif torch.cuda.is_available() and not mixed_precision:
            log_data['performance_analysis']['hardware_recommendations'].append("Consider enabling mixed precision for faster training")
        
        # LOG ALL DETAILED INFORMATION TO FILE ONLY (no console output)
        # Store the original logger level to suppress console handlers temporarily
        handlers_to_suppress = []
        for handler in logger.handlers:
            if isinstance(handler, logging.StreamHandler) and handler.stream.name in ['<stdout>', '<stderr>']:
                handlers_to_suppress.append(handler)
                handler.setLevel(logging.CRITICAL)  # Temporarily suppress console output
        
        try:
            # Log comprehensive information to file
            logger.info("=" * 80)
            logger.info(f"{model_type} INITIALIZATION SUMMARY")
            logger.info("=" * 80)
            
            # Basic information
            logger.info(f"Model Type: {model_type}")
            logger.info(f"Input Dimension: {input_dim}")
            logger.info(f"Architecture: {input_dim} -> {architecture_info.get('hidden_dims', [])} -> {architecture_info.get('encoding_dim', 'Unknown')}")
            logger.info(f"Parameters: {total_params:,} total, {trainable_params:,} trainable")
            logger.info(f"Model Size: {memory_mb:.3f} MB")
            logger.info(f"Device: {device}")
            logger.info(f"Mixed Precision: {mixed_precision}")
            logger.info(f"Preset Used: {detected_preset_name}")
            
            # Ensemble information
            if ensemble_info:
                logger.info(f"Ensemble Configuration:")
                logger.info(f"  - Size: {ensemble_info.get('num_models', 'Unknown')} models")
                logger.info(f"  - Model Types: {ensemble_info.get('model_types', {})}")
                logger.info(f"  - Diversity Factor: {ensemble_info.get('diversity_factor', 'Unknown')}")
            
            # Enhanced features
            if enhanced_features:
                features_list = [f"{k}={v}" for k, v in enhanced_features.items()]
                logger.info(f"Enhanced Features: {', '.join(features_list)}")
                logger.info(f"Features Enabled: {len([f for f, enabled in enhanced_features.items() if enabled])}")
                logger.info(f"Features Disabled: {len([f for f, enabled in enhanced_features.items() if not enabled])}")
            
            # Training configuration
            if training_config:
                logger.info(f"Training Configuration:")
                logger.info(f"  - Optimizer: {training_config.get('optimizer', 'Unknown')}")
                logger.info(f"  - Learning Rate: {training_config.get('learning_rate', 'Unknown')}")
                logger.info(f"  - Batch Size: {training_config.get('batch_size', 'Unknown')}")
            else:
                logger.info("Training Configuration: Not configured")
            
            # Performance analysis
            logger.info(f"Performance Analysis:")
            logger.info(f"  - Complexity Level: {log_data['performance_analysis']['complexity_level']}")
            logger.info(f"  - Memory Classification: {log_data['performance_analysis']['memory_classification']}")
            logger.info(f"  - Training Complexity: {log_data['performance_analysis']['training_complexity']}")
            
            # Hardware recommendations
            if log_data['performance_analysis']['hardware_recommendations']:
                logger.info(f"Hardware Recommendations:")
                for rec in log_data['performance_analysis']['hardware_recommendations']:
                    logger.info(f"  - {rec}")
            
            # Warnings
            if warnings:
                logger.info(f"Warnings:")
                for warning in warnings:
                    logger.warning(f"  - {warning}")
            
            # Memory estimates
            logger.info(f"Memory Estimates:")
            logger.info(f"  - Parameter Memory: {log_data['memory_estimates']['parameter_memory_mb']:.3f} MB")
            logger.info(f"  - Estimated Training Memory: {log_data['memory_estimates']['estimated_training_memory_mb']:.3f} MB")
            
            # Log the complete data structure for debugging
            logger.debug("Complete initialization data structure:")
            import json
            logger.debug(json.dumps(log_data, indent=2, default=str))
            
            logger.info("=" * 80)
            
        finally:
            # Restore original logger levels
            for handler in handlers_to_suppress:
                #handler.setLevel(logging.NOTSET)
                handler.setLevel(logging.ERROR)
        
        # RICH CONSOLE DISPLAY (only visual output)
        try:
            # Model type specific titles and colors
            if model_type == 'SimpleAutoencoder':
                title_color = "bold green"
                border_color = "green"
                title = "SimpleAutoencoder Initialization Complete"
            elif model_type == 'EnhancedAutoencoder':
                title_color = "bold cyan"
                border_color = "cyan"
                title = "EnhancedAutoencoder Initialization Complete"
            else:  # AutoencoderEnsemble
                title_color = "bold magenta"
                border_color = "magenta"
                title = "AutoencoderEnsemble Initialization Complete"
            
            # Create main model information table
            model_table = Table(
                title=f"[{title_color}]{title}[/{title_color}]",
                box=box.ROUNDED,
                header_style="bold white",
                border_style=border_color,
                title_style=f"{title_color}",
                title_justify="left",
                show_lines=True,
                expand=False
            )
            
            model_table.add_column("Attribute", style="bold cyan", width=20)
            model_table.add_column("Value", style="bold white", width=35)
            model_table.add_column("Details", style="bold white", width=25)
            
            # Format memory usage
            if memory_mb > 1000:
                memory_text = f"{memory_mb/1024:.3f} GB"
                memory_style = "bold red" if memory_mb > 5000 else "bold yellow"
            else:
                memory_text = f"{memory_mb:.3f} MB"
                memory_style = "bold green" if memory_mb < 500 else "bold yellow"
            
            # Format parameter count
            if total_params > 1_000_000:
                param_display = f"{total_params/1_000_000:.1f}M"
                param_style = "bold red" if total_params > 100_000_000 else "bold yellow"
            elif total_params > 1_000:
                param_display = f"{total_params/1_000:.1f}K"
                param_style = "bold green"
            else:
                param_display = f"{total_params:,}"
                param_style = "bold green"
            
            # Device status
            device_style = "bold green" if "cuda" in str(device).lower() else "bold yellow"
            device_text = str(device).upper()
            if torch.cuda.is_available() and "cpu" in device_text.lower():
                device_detail = "GPU available but using CPU"
            elif torch.cuda.is_available():
                device_detail = "GPU accelerated"
            else:
                device_detail = "CPU only"
            
            # Mixed precision status
            mp_style = "bold green" if mixed_precision else "bold yellow"
            mp_text = "Enabled" if mixed_precision else "Disabled"
            mp_detail = "FP16 acceleration" if mixed_precision else "FP32 precision"
            
            # Architecture summary
            hidden_dims = architecture_info.get('hidden_dims', [])
            encoding_dim = architecture_info.get('encoding_dim', 'Unknown')
            arch_text = f"{input_dim} - {' - '.join(map(str, hidden_dims))} - {encoding_dim}"
            
            # Add basic rows to table
            model_table.add_row(
                "Model Type", 
                Text(model_type, style="bold white"), 
                "Architecture class"
            )
            
            # Ensemble-specific information
            if ensemble_info:
                num_models = ensemble_info.get('num_models', 0)
                diversity = ensemble_info.get('diversity_factor', 0)
                model_types_dist = ensemble_info.get('model_types', {})
                
                model_table.add_row(
                    "Ensemble Size", 
                    Text(f"{num_models} models", style="bold white"), 
                    f"Diversity: {diversity}"
                )
                
                if isinstance(model_types_dist, dict):
                    dist_text = ", ".join([f"{count}x {name}" for name, count in model_types_dist.items()])
                else:
                    dist_text = str(model_types_dist)
                
                model_table.add_row(
                    "Model Distribution", 
                    Text(dist_text, style="bold white"), 
                    "Mixed architectures"
                )
            
            model_table.add_row(
                "Architecture", 
                Text(arch_text, style="bold white"), 
                "Layer dimensions"
            )
            
            model_table.add_row(
                "Parameters", 
                Text(param_display, style=param_style), 
                f"{trainable_params:,} trainable"
            )
            
            model_table.add_row(
                "Memory Usage", 
                Text(memory_text, style=memory_style), 
                "Estimated (FP32)"
            )
            
            model_table.add_row(
                "Device", 
                Text(device_text, style=device_style), 
                device_detail
            )
            
            model_table.add_row(
                "Mixed Precision", 
                Text(mp_text, style=mp_style), 
                mp_detail
            )
            
            # Enhanced features (for EnhancedAutoencoder)
            if enhanced_features:
                features_enabled = []
                features_disabled = []
                
                for feature, enabled in enhanced_features.items():
                    if enabled:
                        features_enabled.append(feature.replace('_', ' ').title())
                    else:
                        features_disabled.append(feature.replace('_', ' ').title())
                
                features_text = ", ".join(features_enabled) if features_enabled else "None"
                features_detail = f"Disabled: {', '.join(features_disabled)}" if features_disabled else "All enabled"
                
                model_table.add_row(
                    "Enhanced Features", 
                    Text(features_text, style="bold cyan"), 
                    features_detail
                )
            
            # Training configuration
            if training_config:
                optimizer = training_config.get('optimizer', 'Unknown')
                lr = training_config.get('learning_rate', 'Unknown')
                training_text = optimizer
                training_detail = f"LR: {lr}"
                training_style = "bold white"
            else:
                training_text = "Not configured"
                training_detail = "Training components not set up"
                training_style = "bold white"
            
            model_table.add_row(
                "Training Setup", 
                Text(training_text, style=training_style), 
                training_detail
            )
            
            model_table.add_row(
                "Preset Used", 
                Text(detected_preset_name, style="bold yellow" if detected_preset_name else "bold white"),
                "Configuration source"
            )
            
            # Display the main table
            console.print()
            console.print(model_table)
            
            # STATUS PANEL (only if enabled)
            if show_status:
                status_items = []
                
                # Performance status
                if total_params > 100_000_000:
                    status_items.append("[bold red]-WARNING-[/bold red] Very large model - may require significant memory and training time")
                elif total_params > 10_000_000:
                    status_items.append("[bold yellow]-NOTICE-[/bold yellow] Large model - monitor memory usage during training")
                else:
                    status_items.append("[bold green]-OK-[/bold green] Model size is reasonable for most hardware configurations")
                
                # Device optimization
                if torch.cuda.is_available() and "cpu" in str(device).lower():
                    status_items.append("[bold yellow]-OPTIMIZE-[/bold yellow] GPU available but using CPU - consider GPU acceleration for better performance")
                elif "cuda" in str(device).lower():
                    status_items.append("[bold green]-OPTIMIZED-[/bold green] Using GPU acceleration for optimal performance")
                
                # Mixed precision
                if mixed_precision:
                    status_items.append("[bold green]-OPTIMIZED-[/bold green] Mixed precision enabled - up to 50% faster training on compatible hardware")
                elif torch.cuda.is_available():
                    status_items.append("[bold yellow]-OPTIMIZE-[/bold yellow] Mixed precision available but disabled - enable for faster training")
                
                # Model-specific recommendations
                if model_type == 'AutoencoderEnsemble':
                    num_models = ensemble_info.get('num_models', 0) if ensemble_info else 0
                    if num_models > 5:
                        status_items.append("[bold yellow]-NOTICE-[/bold yellow] Large ensemble - excellent robustness but higher computational cost")
                    elif num_models >= 3:
                        status_items.append("[bold green]-BALANCED-[/bold green] Good ensemble size balancing robustness vs performance")
                    else:
                        status_items.append("[bold cyan]-INFO-[/bold cyan] Small ensemble - faster inference but potentially less robust")
                elif model_type == 'EnhancedAutoencoder':
                    if enhanced_features and enhanced_features.get('use_attention', False):
                        status_items.append("[bold green][ADVANCED][/bold green] Attention mechanisms enabled for enhanced feature learning")
                    if enhanced_features and enhanced_features.get('residual_blocks', False):
                        status_items.append("[bold green]-ADVANCED-[/bold green] Residual connections enabled for better gradient flow")
                else:  # SimpleAutoencoder
                    status_items.append("[bold green]-SIMPLE-[/bold green] Lightweight architecture - ideal for prototyping and resource-constrained environments")
                
                # Training configuration status
                if training_config:
                    status_items.append("[bold green]-CONFIGURED-[/bold green] Training components initialized and ready")
                else:
                    status_items.append("[bold cyan]-INFO-[/bold cyan] Training components not configured - call setup methods before training")
                
                # Architecture depth warnings
                if len(hidden_dims) > 8:
                    status_items.append("[bold yellow]-WARNING-[/bold yellow] Very deep architecture may be difficult to train - consider gradient clipping")
                elif len(hidden_dims) > 5:
                    status_items.append("[bold cyan]-INFO-[/bold cyan] Deep architecture detected - monitor for vanishing gradient issues")
                
                if status_items:
                    status_text = "\n".join(status_items)
                    status_panel = Panel.fit(
                        status_text,
                        #title="[bold yellow]Initialization Status & Recommendations[/bold yellow]",
                        title="Initialization Status & Recommendations",
                        border_style="bold yellow",
                        style="bold yellow",
                        title_align="left",
                        padding=(1, 2)
                    )
                    console.print()
                    console.print(status_panel)
            
            # QUICK REFERENCE PANEL (only if enabled)
            if show_quick_ref:
                if model_type == 'AutoencoderEnsemble':
                    quick_ref = [
                        "[bold cyan]Ensemble Quick Reference:[/bold cyan]",
                        f"Model summary: [bold green]ensemble.get_model_summary()[/bold green]",
                        f"Configuration: [bold green]ensemble.get_config()[/bold green]",
                        f"Forward pass: [bold green]output = ensemble(input_tensor)[/bold green]",
                        f"Individual models: [bold green]ensemble.models[i](input_tensor)[/bold green]",
                        f"Save ensemble: [bold green]ensemble.save_model('path.pth')[/bold green]"
                    ]
                elif model_type == 'EnhancedAutoencoder':
                    quick_ref = [
                        "[bold cyan]Enhanced Model Quick Reference:[/bold cyan]",
                        f"Model summary: [bold green]model.get_model_summary()[/bold green]",
                        f"Configuration: [bold green]model.get_config()[/bold green]",
                        f"Forward pass: [bold green]output = model(input_tensor)[/bold green]",
                        f"Encode only: [bold green]encoded = model.encode(input_tensor)[/bold green]",
                        f"Decode only: [bold green]decoded = model.decode(encoded)[/bold green]",
                        f"Save model: [bold green]model.save_model('path.pth')[/bold green]"
                    ]
                else:  # SimpleAutoencoder
                    quick_ref = [
                        "[bold cyan]Simple Model Quick Reference:[/bold cyan]",
                        f"Model summary: [bold green]model.get_model_summary()[/bold green]",
                        f"Configuration: [bold green]model.get_config()[/bold green]",
                        f"Forward pass: [bold green]output = model(input_tensor)[/bold green]",
                        f"Encode: [bold green]encoded = model.encode(input_tensor)[/bold green]",
                        f"Decode: [bold green]decoded = model.decode(encoded)[/bold green]",
                        f"Save model: [bold green]model.save_model('path.pth')[/bold green]"
                    ]
                
                quick_ref_text = "\n".join(quick_ref)
                quick_ref_panel = Panel.fit(
                    quick_ref_text,
                    #title="[bold cyan]Usage Guide[/bold cyan]",
                    title="Usage Guide",
                    border_style="bold cyan",
                    style="bold cyan",
                    title_align="left",
                    padding=(0, 2)
                )
                console.print()
                console.print(quick_ref_panel)
            
            # OPTIMIZATION TIPS PANEL (only if enabled)
            if show_optimization:
                optimization_tips = []
                
                if not mixed_precision and torch.cuda.is_available():
                    optimization_tips.append("Enable mixed precision: [bold green]config['training']['mixed_precision'] = True[/bold green]")
                
                if "cpu" in str(device).lower() and torch.cuda.is_available():
                    optimization_tips.append("Use GPU acceleration: [bold green]config['hardware']['device'] = 'cuda'[/bold green]")
                
                if total_params > 1_000_000:
                    optimization_tips.append("Consider gradient clipping: [bold green]config['training']['gradient_clip'] = 1.0[/bold green]")
                
                if model_type == 'AutoencoderEnsemble' and ensemble_info:
                    num_models = ensemble_info.get('num_models', 0)
                    if num_models > 5:
                        optimization_tips.append("For faster inference, consider reducing ensemble size")
                
                optimization_tips.extend([
                    "Monitor training: [bold green]config['monitoring']['tensorboard_logging'] = True[/bold green]",
                    "Enable early stopping: [bold green]config['training']['early_stopping'] = True[/bold green]",
                    "Use learning rate scheduling: [bold green]config['training']['scheduler'] = 'ReduceLROnPlateau'[/bold green]"
                ])
                
                if optimization_tips:
                    opt_text = "\n".join(optimization_tips)
                    opt_panel = Panel.fit(
                        opt_text,
                        #title="[bold green]Performance Optimization Tips[/bold green]",
                        title="Performance Optimization Tips",
                        border_style="green",
                        style="bold green",
                        title_align="left",
                        padding=(0, 2)
                    )
                    console.print()
                    console.print(opt_panel)
            
            console.print()
            
        except ImportError:
            # Fallback to simple formatted text if Rich is not available
            print("=" * 80)
            print(f"{model_type} Initialization Complete")
            print("=" * 80)
            print(f"Model Type: {model_type}")
            
            if ensemble_info:
                print(f"Ensemble Size: {ensemble_info.get('num_models', 'Unknown')} models")
                print(f"Model Types: {ensemble_info.get('model_types', {})}")
                print(f"Diversity Factor: {ensemble_info.get('diversity_factor', 'Unknown')}")
            
            print(f"Architecture: {input_dim} -> {architecture_info.get('hidden_dims', [])} -> {architecture_info.get('encoding_dim', 'Unknown')}")
            print(f"Parameters: {total_params:,} total, {trainable_params:,} trainable")
            print(f"Device: {device}")
            print(f"Mixed Precision: {mixed_precision}")
            print(f"Preset: {detected_preset_name}")
            
            if enhanced_features:
                features_list = [f"{k}={v}" for k, v in enhanced_features.items()]
                print(f"Enhanced Features: {', '.join(features_list)}")
            
            if training_config:
                print(f"Training: {training_config.get('optimizer', 'Unknown')} optimizer")
            
            print(f"Memory Usage: ~{memory_mb:.1f} MB (FP32)")
            
            if warnings:
                print("\nWarnings:")
                for warning in warnings:
                    print(f"  - {warning}")
            
            print("=" * 80)
            
        except Exception as display_error:
            logger.error(f"Failed to display rich initialization summary: {display_error}")
            # Minimal fallback
            print(f"{model_type} initialized: {total_params:,} parameters on {device}")
    
    except Exception as e:
        logger.error(f"Critical error in display_model_initialization_summary: {e}")
        print(f"Model initialization completed with errors. Check logs for details.")

def create_model_instance(
    model_type: str, 
    input_dim: int, 
    config: Dict[str, Any] = None,
    preset: str = None,
    **kwargs
) -> nn.Module:
    """
    Factory function that uses existing initialization/validation functions to create models.
    
    This eliminates the redundancy between SimpleAutoencoder's parameter processing and
    the functions initialize_model_variants() and validate_model_variants() by leveraging
    the existing comprehensive validation and instantiation infrastructure.
    
    Args:
        model_type: Type of model to create ('SimpleAutoencoder', 'EnhancedAutoencoder', etc.)
        input_dim: Input dimension for the model
        config: Configuration dictionary (optional, will use current config if None)
        preset: Preset name to apply (optional)
        **kwargs: Additional parameters that will be merged into config
        
    Returns:
        Instantiated and validated model instance
        
    Raises:
        ValueError: If model_type is not available or configuration is invalid
        RuntimeError: If model instantiation fails
    """
    try:
        # Ensure MODEL_VARIANTS is initialized
        if not MODEL_VARIANTS:
            logger.info("Initializing model variants for factory pattern")
            initialize_model_variants(silent=True)
        
        # Validate model type availability
        if model_type not in MODEL_VARIANTS:
            available_types = list(MODEL_VARIANTS.keys())
            raise ValueError(f"Model type '{model_type}' not available. Available types: {available_types}")
        
        # Get the model class
        model_class = MODEL_VARIANTS[model_type]
        
        # Get base configuration using existing helper
        if config is None:
            base_config = get_current_config()
        else:
            base_config = config.copy()
        
        # Apply preset if specified
        if preset and preset in PRESET_CONFIGS:
            preset_config = PRESET_CONFIGS[preset].copy()
            # Merge preset with base config (preset takes precedence)
            base_config = deep_update(base_config, preset_config)
            
            # EXPLICITLY SET PRESET INFORMATION IN MULTIPLE LOCATIONS
            if 'metadata' not in base_config:
                base_config['metadata'] = {}
            base_config['metadata']['preset_used'] = preset
            
            if 'presets' not in base_config:
                base_config['presets'] = {}
            base_config['presets']['current_preset'] = preset
            
            if 'runtime' not in base_config:
                base_config['runtime'] = {}
            base_config['runtime']['factory_preset_applied'] = preset
            
            logger.debug(f"Applied preset '{preset}' to configuration")
        
        # Merge any additional kwargs into config
        if kwargs:
            # Convert flat kwargs into structured config sections
            structured_kwargs = _structure_kwargs_into_config_sections(kwargs)
            base_config = deep_update(base_config, structured_kwargs)
            logger.debug(f"Merged {len(kwargs)} additional parameters into configuration")
        
        # Ensure input_dim is in the configuration
        if 'model' not in base_config:
            base_config['model'] = {}
        base_config['model']['input_dim'] = input_dim
        
        # Use existing model_instantiation_with_validation for comprehensive creation
        model_instance, validation_results, performance_metrics, instantiation_details = model_instantiation_with_validation(
            variant_class=model_class,
            variant_name=model_type,
            input_dim=input_dim,
            base_config=base_config,
            fallback_config=None,
            minimal_config=None,
            validation_tests=['basic', 'forward_pass', 'parameters', 'config_methods'],
            comprehensive_validation=True,
            hardware_data=None,
            #silent=False,
            silent=True,
            logger=logger
        )
        
        # Check instantiation success
        if model_instance is None:
            error_msg = f"Failed to instantiate {model_type}: {', '.join(validation_results.get('errors', []))}"
            raise RuntimeError(error_msg)
        
        # Log successful creation
        instantiation_method = instantiation_details.get('method', 'unknown')
        validation_score = validation_results.get('overall_score', 0)
        total_params = performance_metrics.get('total_parameters', 0)
        
        return model_instance
        
    except Exception as e:
        logger.error(f"Factory pattern model creation failed for {model_type}: {e}")
        raise RuntimeError(f"Model factory failed to create {model_type}: {str(e)}") from e

def _structure_kwargs_into_config_sections(kwargs: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convert flat kwargs into structured configuration sections.
    
    This helper function organizes flat parameters into the appropriate configuration
    sections, maintaining compatibility with all preset parameters.
    
    Args:
        kwargs: Flat dictionary of parameters
        
    Returns:
        Structured configuration dictionary with proper sections
    """
    # Parameter to section mapping (same as used in SimpleAutoencoder)
    param_sections = {
        'model': [
            'encoding_dim', 'hidden_dims', 'dropout_rates', 'activation', 'activation_param',
            'normalization', 'use_batch_norm', 'use_layer_norm', 'bias', 'weight_init',
            'skip_connection', 'residual_blocks', 'use_attention', 'model_type',
            'model_types', 'available_activations', 'available_normalizations',
            'available_initializers', 'legacy_mode', 'diversity_factor', 'min_features',
            'num_models'
        ],
        'training': [
            'batch_size', 'epochs', 'learning_rate', 'patience', 'weight_decay',
            'gradient_clip', 'gradient_accumulation_steps', 'mixed_precision',
            'num_workers', 'optimizer', 'scheduler', 'scheduler_params',
            'early_stopping', 'validation_split', 'shuffle', 'pin_memory',
            'persistent_workers', 'adam_betas', 'adam_eps', 'lr_patience',
            'lr_factor', 'min_lr'
        ],
        'data': [
            'normal_samples', 'attack_samples', 'features', 'use_real_data',
            'data_normalization', 'anomaly_factor', 'random_state', 'test_split',
            'stratified_split', 'data_path', 'artifacts_path', 'synthetic_generation',
            'preprocessing'
        ],
        'security': [
            'percentile', 'attack_threshold', 'false_negative_cost', 'enable_security_metrics',
            'anomaly_threshold_strategy', 'early_warning_threshold', 'adaptive_threshold',
            'confidence_interval', 'detection_methods', 'alert_levels', 'threshold_validation',
            'robust_detection', 'false_positive_tolerance', 'performance_optimized_detection',
            'real_time_monitoring', 'ensemble_voting', 'uncertainty_threshold'
        ],
        'monitoring': [
            'metrics_frequency', 'checkpoint_frequency', 'tensorboard_logging',
            'console_logging_level', 'save_best_model', 'save_model_history',
            'metrics_to_track', 'early_stopping_metric', 'checkpoint_format',
            'log_model_summary', 'tensorboard_dir', 'log_frequency', 'save_checkpoints',
            'tensorboard', 'stability_metrics', 'performance_metrics', 'profiling_enabled'
        ],
        'hardware': [
            'device', 'recommended_gpu_memory', 'minimum_system_requirements',
            'optimal_system_requirements', 'memory_management', 'performance_optimization',
            'detected_gpu_memory', 'detected_system_memory', 'system_performance_class',
            'optimization_recommendations'
        ],
        'system': [
            'model_dir', 'log_dir', 'config_dir', 'data_dir', 'checkpoint_dir', 'results_dir',
            'debug', 'verbose', 'random_seed', 'reproducible', 'parallel_processing',
            'max_workers', 'export_onnx', 'non_interactive', 'cuda_optimizations',
            'onnx_export', 'distributed_training', 'python_executable',
            'working_directory', 'environment_health'
        ],
        'presets': [
            'available_presets', 'current_preset', 'current_override', 'override_rules',
            'preset_configs', 'custom_presets_available', 'auto_apply',
            'validate_compatibility', 'system_recommended_preset', 'preset_compatibility'
        ],
        'hyperparameter_optimization': [
            'hpo_enabled', 'hpo_strategy', 'study_name', 'direction', 'n_trials',
            'timeout', 'sampler', 'pruner', 'objective_metric', 'optimization_space',
            'hpo_early_stopping', 'timeout_seconds', 'trial_epochs', 'trial_patience',
            'cleanup_trials', 'generate_plots', 'search_space', 'hpo_sampler',
            'hpo_pruner', 'scoring', 'storage'
        ],
        'validation': [
            'cross_validation', 'metrics', 'validation_frequency', 'save_validation_results',
            'detailed_metrics', 'robustness_testing', 'performance_benchmarking',
            'confidence_intervals'
        ],
        'experimental': [
            'experimental_features', 'experimental_settings'
        ],
        'metadata': [
            'description', 'version', 'config_version', 'config_type', 'created',
            'last_modified', 'preset_used', 'recommended_hardware', 'compatibility',
            'system_info', 'validation_info'
        ],
        'runtime': [
            'config_loaded_at', 'config_source', 'runtime_id', 'process_id',
            'system_analysis_completed', 'system_performance_score', 'system_class',
            'optimizations_applied', 'resource_status', 'system_warnings',
            'recommendations', 'configuration_health'
        ]
    }
    
    structured_config = {}
    
    # Organize parameters into sections
    for section, param_list in param_sections.items():
        section_params = {}
        for param in param_list:
            if param in kwargs:
                # Handle special parameter name mappings
                config_param = param
                if param == 'data_normalization':
                    config_param = 'normalization'
                elif param.startswith('hpo_'):
                    config_param = param[4:]  # Remove 'hpo_' prefix
                
                section_params[config_param] = kwargs[param]
        
        if section_params:
            structured_config[section] = section_params
    
    return structured_config

def _initialize_autoencoder_config(
    model_class_name: str,
    input_dim: Optional[int] = None,
    config: Optional[Dict[str, Any]] = None,
    preset: Optional[str] = None,
    
    # Core Model Architecture Parameters
    encoding_dim: Optional[int] = None,
    hidden_dims: Optional[List[int]] = None,
    dropout_rates: Optional[List[float]] = None,
    activation: Optional[str] = None,
    activation_param: Optional[float] = None,
    normalization: Optional[str] = None,
    use_batch_norm: Optional[bool] = None,
    use_layer_norm: Optional[bool] = None,
    bias: Optional[bool] = None,
    weight_init: Optional[str] = None,
    skip_connection: Optional[bool] = None,
    residual_blocks: Optional[bool] = None,
    use_attention: Optional[bool] = None,
    
    # Model Type and Variants
    model_type: Optional[str] = None,
    model_types: Optional[List[str]] = None,
    available_activations: Optional[List[str]] = None,
    available_normalizations: Optional[List[str]] = None,
    available_initializers: Optional[List[str]] = None,
    legacy_mode: Optional[bool] = None,
    
    # Ensemble Parameters
    diversity_factor: Optional[float] = None,
    min_features: Optional[int] = None,
    num_models: Optional[int] = None,
    
    # Training Parameters
    batch_size: Optional[int] = None,
    epochs: Optional[int] = None,
    learning_rate: Optional[float] = None,
    patience: Optional[int] = None,
    weight_decay: Optional[float] = None,
    gradient_clip: Optional[float] = None,
    gradient_accumulation_steps: Optional[int] = None,
    mixed_precision: Optional[bool] = None,
    num_workers: Optional[int] = None,
    optimizer: Optional[str] = None,
    scheduler: Optional[str] = None,
    scheduler_params: Optional[Dict[str, Any]] = None,
    early_stopping: Optional[bool] = None,
    validation_split: Optional[float] = None,
    shuffle: Optional[bool] = None,
    pin_memory: Optional[bool] = None,
    persistent_workers: Optional[bool] = None,
    adam_betas: Optional[Tuple[float, float]] = None,
    adam_eps: Optional[float] = None,
    lr_patience: Optional[int] = None,
    lr_factor: Optional[float] = None,
    min_lr: Optional[float] = None,
    
    # Data Parameters
    normal_samples: Optional[int] = None,
    attack_samples: Optional[int] = None,
    features: Optional[int] = None,
    use_real_data: Optional[bool] = None,
    data_normalization: Optional[str] = None,
    anomaly_factor: Optional[float] = None,
    random_state: Optional[int] = None,
    test_split: Optional[float] = None,
    stratified_split: Optional[bool] = None,
    data_path: Optional[str] = None,
    artifacts_path: Optional[str] = None,
    synthetic_generation: Optional[Dict[str, Any]] = None,
    preprocessing: Optional[Dict[str, Any]] = None,
    
    # Security Parameters
    percentile: Optional[float] = None,
    attack_threshold: Optional[float] = None,
    false_negative_cost: Optional[float] = None,
    enable_security_metrics: Optional[bool] = None,
    anomaly_threshold_strategy: Optional[str] = None,
    early_warning_threshold: Optional[float] = None,
    adaptive_threshold: Optional[bool] = None,
    confidence_interval: Optional[float] = None,
    detection_methods: Optional[List[str]] = None,
    alert_levels: Optional[List[str]] = None,
    threshold_validation: Optional[bool] = None,
    robust_detection: Optional[bool] = None,
    false_positive_tolerance: Optional[float] = None,
    performance_optimized_detection: Optional[bool] = None,
    real_time_monitoring: Optional[bool] = None,
    ensemble_voting: Optional[str] = None,
    uncertainty_threshold: Optional[float] = None,
    
    # Monitoring Parameters
    metrics_frequency: Optional[int] = None,
    checkpoint_frequency: Optional[int] = None,
    tensorboard_logging: Optional[bool] = None,
    console_logging_level: Optional[str] = None,
    save_best_model: Optional[bool] = None,
    save_model_history: Optional[bool] = None,
    metrics_to_track: Optional[List[str]] = None,
    early_stopping_metric: Optional[str] = None,
    checkpoint_format: Optional[str] = None,
    log_model_summary: Optional[bool] = None,
    tensorboard_dir: Optional[str] = None,
    log_frequency: Optional[int] = None,
    save_checkpoints: Optional[bool] = None,
    tensorboard: Optional[Dict[str, Any]] = None,
    stability_metrics: Optional[bool] = None,
    performance_metrics: Optional[bool] = None,
    profiling_enabled: Optional[bool] = None,
    
    # Hardware Parameters
    device: Optional[str] = None,
    recommended_gpu_memory: Optional[float] = None,
    minimum_system_requirements: Optional[Dict[str, Any]] = None,
    optimal_system_requirements: Optional[Dict[str, Any]] = None,
    memory_management: Optional[Dict[str, Any]] = None,
    performance_optimization: Optional[Dict[str, Any]] = None,
    detected_gpu_memory: Optional[float] = None,
    detected_system_memory: Optional[float] = None,
    system_performance_class: Optional[str] = None,
    optimization_recommendations: Optional[List[str]] = None,
    
    # System Parameters
    model_dir: Optional[str] = None,
    log_dir: Optional[str] = None,
    config_dir: Optional[str] = None,
    data_dir: Optional[str] = None,
    checkpoint_dir: Optional[str] = None,
    results_dir: Optional[str] = None,
    debug: Optional[bool] = None,
    verbose: Optional[bool] = None,
    random_seed: Optional[int] = None,
    reproducible: Optional[bool] = None,
    parallel_processing: Optional[bool] = None,
    max_workers: Optional[int] = None,
    export_onnx: Optional[bool] = None,
    non_interactive: Optional[bool] = None,
    cuda_optimizations: Optional[bool] = None,
    onnx_export: Optional[Dict[str, Any]] = None,
    distributed_training: Optional[bool] = None,
    python_executable: Optional[str] = None,
    working_directory: Optional[str] = None,
    environment_health: Optional[str] = None,
    
    # Preset Parameters
    available_presets: Optional[List[str]] = None,
    current_preset: Optional[str] = None,
    current_override: Optional[str] = None,
    override_rules: Optional[Dict[str, bool]] = None,
    preset_configs: Optional[Dict[str, str]] = None,
    custom_presets_available: Optional[List[str]] = None,
    auto_apply: Optional[bool] = None,
    validate_compatibility: Optional[bool] = None,
    system_recommended_preset: Optional[str] = None,
    preset_compatibility: Optional[Dict[str, Any]] = None,
    
    # Hyperparameter Optimization Parameters
    hpo_enabled: Optional[bool] = None,
    hpo_strategy: Optional[str] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    n_trials: Optional[int] = None,
    timeout: Optional[int] = None,
    sampler: Optional[str] = None,
    pruner: Optional[str] = None,
    objective_metric: Optional[str] = None,
    optimization_space: Optional[Dict[str, Any]] = None,
    hpo_early_stopping: Optional[Dict[str, Any]] = None,
    timeout_seconds: Optional[int] = None,
    trial_epochs: Optional[int] = None,
    trial_patience: Optional[int] = None,
    cleanup_trials: Optional[bool] = None,
    generate_plots: Optional[bool] = None,
    search_space: Optional[Dict[str, Any]] = None,
    hpo_sampler: Optional[Dict[str, Any]] = None,
    hpo_pruner: Optional[Dict[str, Any]] = None,
    scoring: Optional[Dict[str, Any]] = None,
    storage: Optional[Dict[str, Any]] = None,
    
    # Validation Parameters
    cross_validation: Optional[Dict[str, Any]] = None,
    metrics: Optional[List[str]] = None,
    validation_frequency: Optional[int] = None,
    save_validation_results: Optional[bool] = None,
    detailed_metrics: Optional[bool] = None,
    robustness_testing: Optional[bool] = None,
    performance_benchmarking: Optional[bool] = None,
    confidence_intervals: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_features: Optional[Dict[str, bool]] = None,
    experimental_settings: Optional[Dict[str, bool]] = None,
    
    # Metadata Parameters
    description: Optional[str] = None,
    version: Optional[str] = None,
    config_version: Optional[str] = None,
    config_type: Optional[str] = None,
    created: Optional[str] = None,
    last_modified: Optional[str] = None,
    preset_used: Optional[str] = None,
    recommended_hardware: Optional[Dict[str, Any]] = None,
    compatibility: Optional[List[str]] = None,
    system_info: Optional[Dict[str, Any]] = None,
    validation_info: Optional[Dict[str, Any]] = None,
    
    # Runtime Parameters
    config_loaded_at: Optional[str] = None,
    config_source: Optional[str] = None,
    runtime_id: Optional[str] = None,
    process_id: Optional[int] = None,
    system_analysis_completed: Optional[bool] = None,
    system_performance_score: Optional[float] = None,
    system_class: Optional[str] = None,
    optimizations_applied: Optional[Dict[str, bool]] = None,
    resource_status: Optional[Dict[str, bool]] = None,
    system_warnings: Optional[List[str]] = None,
    recommendations: Optional[List[str]] = None,
    configuration_health: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """
    Centralized autoencoder configuration initialization helper function.
    
    This function handles all the parameter processing, validation, and configuration
    setup that was previously duplicated across SimpleAutoencoder, EnhancedAutoencoder,
    and AutoencoderEnsemble classes.
    
    Args:
        model_class_name: Name of the model class ('SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble')
        input_dim: Input dimension for the model
        config: Configuration dictionary (optional, will use current config if None)
        preset: Preset name to apply (optional)
        **all other parameters: All the parameters that were previously handled in each class
        
    Returns:
        Dictionary containing:
        - 'config': Final processed configuration
        - 'processed_params': Dictionary of extracted and validated parameters
        - 'device': PyTorch device object
        - 'mixed_precision': Boolean indicating if mixed precision is enabled
        - 'initialization_timestamp': ISO timestamp of initialization
        - 'preset_name': Name of preset used (if any)
    """
    from datetime import datetime
    
    # Store initialization metadata
    initialization_timestamp = datetime.now().isoformat()
    
    # PRESERVE PRESET NAME EARLY
    actual_preset_used = preset
    
    # Validate input_dim
    if input_dim is None:
        # Try to extract from config sources in priority order
        if config and 'model' in config and 'input_dim' in config['model']:
            input_dim = config['model']['input_dim']
            logger.debug("input_dim extracted from config['model']['input_dim']")
        elif config and 'data' in config and 'features' in config['data']:
            input_dim = config['data']['features']
            logger.debug("input_dim extracted from config['data']['features']")
        else:
            input_dim = 20
            logger.warning("input_dim not provided in config, using default: 20")
    
    # Enhanced tensor/array handling with better error messages
    original_input_dim = input_dim  # Keep track for logging
    
    # Handle different input_dim types using existing helper pattern
    if isinstance(input_dim, (torch.Tensor, np.ndarray)):
        if hasattr(input_dim, 'shape') and len(input_dim.shape) >= 2:
            # For 2D+ tensors, use the last dimension (feature dimension)
            input_dim = input_dim.shape[-1]
            logger.info(f"input_dim inferred from tensor shape: {original_input_dim.shape} -> {input_dim}")
        elif hasattr(input_dim, 'shape') and len(input_dim.shape) == 1:
            # For 1D tensors, use the length
            input_dim = len(input_dim)
            logger.info(f"input_dim inferred from 1D tensor length: {len(original_input_dim)} -> {input_dim}")
        else:
            try:
                # Try to get total elements for other tensor types
                input_dim = input_dim.numel() if hasattr(input_dim, 'numel') else len(input_dim)
                logger.info(f"input_dim inferred from tensor elements: {input_dim}")
            except Exception as e:
                logger.error(f"Could not infer input_dim from tensor: {e}")
                input_dim = 20
                logger.warning("Using fallback input_dim: 20")
    
    elif isinstance(input_dim, (list, tuple)):
        if len(input_dim) > 0:
            # For lists/tuples, check if it's data or just dimensions
            if all(isinstance(x, (int, float)) for x in input_dim):
                # Assume it's a feature vector, use length
                input_dim = len(input_dim)
                logger.info(f"input_dim inferred from feature vector length: {input_dim}")
            else:
                # Assume it's nested data, use last dimension of first element
                try:
                    if hasattr(input_dim[0], '__len__'):
                        input_dim = len(input_dim[0])
                    else:
                        input_dim = len(input_dim)
                    logger.info(f"input_dim inferred from nested structure: {input_dim}")
                except Exception:
                    input_dim = len(original_input_dim)
                    logger.warning(f"Using length of input structure: {input_dim}")
        else:
            logger.error("input_dim was empty list/tuple, using default")
            input_dim = 20
    
    elif isinstance(input_dim, (float, np.number)):
        # Convert float to int
        input_dim = int(input_dim)
        logger.debug(f"Converted input_dim from {type(original_input_dim).__name__} to int: {input_dim}")
    
    # Final validation
    if not isinstance(input_dim, int) or input_dim <= 0:
        logger.error(f"input_dim must be a positive integer, got {type(input_dim).__name__}: {input_dim}")
        raise ValueError(f"input_dim must be a positive integer, got {type(input_dim).__name__}: {input_dim}")
    
    # Log the final input_dim for debugging
    logger.debug(f"Final input_dim: {input_dim}")
    
    # Use existing configuration processing helper
    if config is None:
        config = get_current_config()
    
    # Apply preset if specified using existing helper
    if preset and preset in PRESET_CONFIGS:
        preset_config = deepcopy(PRESET_CONFIGS[preset])
        config = deep_update(preset_config, config)
        logger.debug(f"Applied preset '{preset}' to configuration")
        
        # Ensure input_dim consistency between model and data sections
        if 'model' in config:
            config['model']['input_dim'] = input_dim
        if 'data' in config:
            config['data']['features'] = input_dim
        
        # ENSURE PRESET NAME IS RECORDED IN MULTIPLE PLACES
        if 'metadata' not in config:
            config['metadata'] = {}
        config['metadata']['preset_used'] = preset
        
        if 'presets' not in config:
            config['presets'] = {}
        config['presets']['current_preset'] = preset
        
        # Also store in runtime for easy access
        if 'runtime' not in config:
            config['runtime'] = {}
        config['runtime']['applied_preset'] = preset
    
    # Ensure input_dim consistency across all relevant sections
    if 'model' not in config:
        config['model'] = {}
    config['model']['input_dim'] = input_dim
    
    if 'data' not in config:
        config['data'] = {}
    config['data']['features'] = input_dim
    
    #preset_name = preset if preset in PRESET_CONFIGS else None
    actual_preset_used = preset if preset in PRESET_CONFIGS else None
    
    
    # Collect all non-None parameters for processing
    local_params = locals().copy()
    params_to_remove = {'model_class_name', 'config', 'preset', 'kwargs', 'local_params', 'initialization_timestamp'}
    individual_params = {k: v for k, v in local_params.items() 
                       if k not in params_to_remove and v is not None}
    
    # Add kwargs
    if kwargs:
        individual_params.update(kwargs)
    
    # Structure individual parameters using existing helper
    if individual_params:
        structured_params = _structure_kwargs_into_config_sections(individual_params)
        config = deep_update(config, structured_params)
        logger.debug(f"Processed {len(individual_params)} individual parameters")
    
    # Extract and validate core parameters using existing helpers
    model_config = config.get('model', {})
    
    # Set model-specific defaults based on class
    if model_class_name == 'SimpleAutoencoder':
        default_encoding_dim = 12
        default_hidden_dims = [128, 64]
        default_dropout_rates = [0.2, 0.15]
        default_mixed_precision = False
        default_use_attention = False
        default_residual_blocks = False
        default_skip_connection = True
        default_legacy_mode = False
    elif model_class_name == 'EnhancedAutoencoder':
        default_encoding_dim = 32
        default_hidden_dims = [256, 128, 64]
        default_dropout_rates = [0.2, 0.15, 0.1]
        default_mixed_precision = True
        default_use_attention = True
        default_residual_blocks = True
        default_skip_connection = True
        default_legacy_mode = False
    elif model_class_name == 'AutoencoderEnsemble':
        default_encoding_dim = 24
        default_hidden_dims = [192, 96, 48]
        default_dropout_rates = [0.25, 0.2, 0.15]
        default_mixed_precision = True
        default_use_attention = True
        default_residual_blocks = True
        default_skip_connection = True
        default_legacy_mode = False
    else:
        # Fallback defaults
        default_encoding_dim = 16
        default_hidden_dims = [128, 64]
        default_dropout_rates = [0.2, 0.15]
        default_mixed_precision = False
        default_use_attention = False
        default_residual_blocks = False
        default_skip_connection = False
        default_legacy_mode = False
    
    # Extract and validate parameters using existing helper functions
    processed_params = {}
    
    processed_params['encoding_dim'] = _extract_and_validate_config_param(
        model_config, 'encoding_dim', default_encoding_dim, 'DEFAULT_ENCODING_DIM',
        lambda x: isinstance(x, int) and x > 0,
        "encoding dimension"
    )
    
    processed_params['hidden_dims'] = _extract_and_validate_config_param(
        model_config, 'hidden_dims', default_hidden_dims, 'HIDDEN_LAYER_SIZES',
        lambda x: isinstance(x, list) and len(x) > 0,
        "hidden dimensions"
    )
    
    processed_params['dropout_rates'] = _extract_and_validate_config_param(
        model_config, 'dropout_rates', default_dropout_rates, 'DROPOUT_RATES',
        lambda x: isinstance(x, list) and len(x) > 0,
        "dropout rates"
    )
    
    processed_params['activation'] = _extract_and_validate_config_param(
        model_config, 'activation', 'leaky_relu', 'ACTIVATION',
        lambda x: x in ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu', 'selu', 'prelu'],
        "activation function"
    )
    
    processed_params['activation_param'] = _extract_and_validate_config_param(
        model_config, 'activation_param', 0.2, 'ACTIVATION_PARAM',
        lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
        "activation parameter"
    )
    
    processed_params['normalization'] = _extract_and_validate_config_param(
        model_config, 'normalization', 'batch', 'NORMALIZATION',
        lambda x: x in ['batch', 'layer', 'instance', 'group', 'none', None],
        "normalization type"
    )
    
    processed_params['use_batch_norm'] = _extract_and_validate_config_param(
        model_config, 'use_batch_norm', True, 'USE_BATCH_NORM',
        lambda x: isinstance(x, bool),
        "batch normalization flag"
    )
    
    processed_params['use_layer_norm'] = _extract_and_validate_config_param(
        model_config, 'use_layer_norm', False, 'USE_LAYER_NORM',
        lambda x: isinstance(x, bool),
        "layer normalization flag"
    )
    
    processed_params['bias'] = _extract_and_validate_config_param(
        model_config, 'bias', True, 'BIAS',
        lambda x: isinstance(x, bool),
        "bias flag"
    )
    
    processed_params['weight_init'] = _extract_and_validate_config_param(
        model_config, 'weight_init', 'xavier_uniform', 'WEIGHT_INIT',
        lambda x: x in ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'orthogonal', 'he_uniform', 'he_normal'],
        "weight initialization"
    )
    
    processed_params['skip_connection'] = _extract_and_validate_config_param(
        model_config, 'skip_connection', default_skip_connection, 'SKIP_CONNECTION',
        lambda x: isinstance(x, bool),
        "skip connection flag"
    )
    
    processed_params['residual_blocks'] = _extract_and_validate_config_param(
        model_config, 'residual_blocks', default_residual_blocks, 'RESIDUAL_BLOCKS',
        lambda x: isinstance(x, bool),
        "residual blocks flag"
    )
    
    processed_params['use_attention'] = _extract_and_validate_config_param(
        model_config, 'use_attention', default_use_attention, 'USE_ATTENTION',
        lambda x: isinstance(x, bool),
        "attention mechanism flag"
    )
    
    processed_params['model_type'] = model_config.get('model_type', model_class_name)
    processed_params['min_features'] = model_config.get('min_features', 5)
    processed_params['legacy_mode'] = _extract_and_validate_config_param(
        model_config, 'legacy_mode', default_legacy_mode, 'LEGACY_MODE',
        lambda x: isinstance(x, bool),
        "legacy compatibility mode"
    )
    
    # Ensemble-specific parameters
    if model_class_name == 'AutoencoderEnsemble':
        processed_params['num_models'] = _extract_and_validate_config_param(
            model_config, 'num_models', 3, 'NUM_MODELS',
            lambda x: isinstance(x, int) and 1 <= x <= 10,
            "ensemble size"
        )
        
        processed_params['diversity_factor'] = _extract_and_validate_config_param(
            model_config, 'diversity_factor', 0.3, 'DIVERSITY_FACTOR',
            lambda x: isinstance(x, (int, float)) and 0 <= x <= 1,
            "ensemble diversity factor"
        )
    else:
        processed_params['num_models'] = 1
        processed_params['diversity_factor'] = 0.0
    
    # Validate and adjust parameters using existing helper
    processed_params['hidden_dims'], processed_params['dropout_rates'] = _validate_and_adjust_parameters(
        processed_params['hidden_dims'], processed_params['dropout_rates']
    )
    
    # Model-specific adjustments
    if model_class_name == 'SimpleAutoencoder':
        # SimpleAutoencoder-specific adjustments
        if len(processed_params['hidden_dims']) > 1:
            processed_params['hidden_dims'] = [processed_params['hidden_dims'][0]]
            processed_params['dropout_rates'] = [processed_params['dropout_rates'][0]]
            logger.debug("Simplified architecture for SimpleAutoencoder")
        
        # Disable advanced features for SimpleAutoencoder
        processed_params['use_attention'] = False
        processed_params['residual_blocks'] = False
    
    elif model_class_name == 'EnhancedAutoencoder':
        # EnhancedAutoencoder-specific validations
        if input_dim < processed_params['min_features']:
            raise ValueError(f"Input dimension ({input_dim}) must be at least {processed_params['min_features']}")
            
        # Legacy mode adjustments
        if processed_params['legacy_mode']:
            processed_params['use_attention'] = False
            processed_params['residual_blocks'] = False
            logger.debug("Simplified architecture for legacy mode")
    
    elif model_class_name == 'AutoencoderEnsemble':
        # Ensemble-specific validations
        if input_dim < processed_params['min_features']:
            raise ValueError(f"Input dimension ({input_dim}) must be at least {processed_params['min_features']}")
        
        if not isinstance(processed_params['num_models'], int) or processed_params['num_models'] < 1:
            raise ValueError(f"num_models must be a positive integer, got {processed_params['num_models']}")
        
        if not isinstance(processed_params['diversity_factor'], (int, float)) or not 0 <= processed_params['diversity_factor'] <= 1:
            raise ValueError(f"diversity_factor must be between 0 and 1, got {processed_params['diversity_factor']}")
        
        # Legacy mode adjustments
        if processed_params['legacy_mode']:
            processed_params['use_attention'] = False
            processed_params['residual_blocks'] = False
            if len(processed_params['hidden_dims']) > 2:
                processed_params['hidden_dims'] = processed_params['hidden_dims'][:2]
                processed_params['dropout_rates'] = processed_params['dropout_rates'][:2]
            if processed_params['num_models'] > 3:
                processed_params['num_models'] = 3
            logger.debug("Simplified ensemble architecture for legacy mode")
    
    # Fix normalization consistency
    if processed_params['normalization'] == 'batch':
        processed_params['use_batch_norm'] = True
        processed_params['use_layer_norm'] = False
    elif processed_params['normalization'] == 'layer':
        processed_params['use_batch_norm'] = False
        processed_params['use_layer_norm'] = True
    elif processed_params['normalization'] in [None, 'none']:
        processed_params['normalization'] = None
        processed_params['use_batch_norm'] = False
        processed_params['use_layer_norm'] = False
    elif processed_params['normalization'] in ['instance', 'group'] and processed_params['legacy_mode']:
        logger.debug(f"Normalization '{processed_params['normalization']}' not supported in legacy mode, disabling")
        processed_params['normalization'] = None
        processed_params['use_batch_norm'] = False
        processed_params['use_layer_norm'] = False
    
    # Setup device using existing system context helper
    system_context = _get_system_context(silent=True)
    hardware_data = system_context.get('hardware_data', {})
    
    # Determine device
    device_setting = config.get('hardware', {}).get('device', 'auto')
    if device_setting == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    elif device_setting == 'cpu':
        device = torch.device('cpu')
    elif device_setting == 'cuda':
        if torch.cuda.is_available():
            device = torch.device('cuda')
        else:
            logger.warning("CUDA requested but not available, falling back to CPU")
            device = torch.device('cpu')
    else:
        try:
            device = torch.device(device_setting)
        except Exception as e:
            logger.error(f"Invalid device '{device_setting}': {e}, falling back to auto")
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Mixed precision settings
    training_config = config.get('training', {})
    mixed_precision_requested = training_config.get('mixed_precision', default_mixed_precision)
    mixed_precision = mixed_precision_requested and torch.cuda.is_available()
    
    if mixed_precision_requested and not torch.cuda.is_available():
        logger.warning("Mixed precision requested but CUDA not available")
    
    # Setup reproducibility
    system_config = config.get('system', {})
    random_seed = system_config.get('random_seed', 42)
    reproducible = system_config.get('reproducible', True)
    
    if reproducible:
        torch.manual_seed(random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(random_seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
    
    # Store processed input_dim
    processed_params['input_dim'] = input_dim
    
    # Ensure input_dim is in the configuration
    if 'model' not in config:
        config['model'] = {}
    config['model']['input_dim'] = input_dim
    
    # Return comprehensive initialization data
    return {
        'config': config,
        'processed_params': processed_params,
        'device': device,
        'mixed_precision': mixed_precision,
        'mixed_precision_requested': mixed_precision_requested,
        'initialization_timestamp': initialization_timestamp,
        #'preset_name': preset,
        'preset_name': actual_preset_used,
        #'preset_name': preset_name,
        'training_config': training_config,
        'hardware_data': hardware_data,
        'system_context': system_context
    }

class SimpleAutoencoder(nn.Module):
    """
    Streamlined SimpleAutoencoder using centralized configuration initialization.
    
    This class leverages the _initialize_autoencoder_config() helper function to eliminate
    redundancy and provide consistent parameter processing across all autoencoder variants.
    """
    
    def __init__(
        self,
        # Core parameter - always required
        input_dim: Optional[int] = None,
        
        # Configuration override - can contain any/all preset parameters
        config: Optional[Dict[str, Any]] = None,
        preset: Optional[str] = None,
        
        # ALL preset parameters accepted for backward compatibility
        # Core Model Architecture Parameters
        encoding_dim: Optional[int] = None,
        hidden_dims: Optional[List[int]] = None,
        dropout_rates: Optional[List[float]] = None,
        activation: Optional[str] = None,
        activation_param: Optional[float] = None,
        normalization: Optional[str] = None,
        use_batch_norm: Optional[bool] = None,
        use_layer_norm: Optional[bool] = None,
        bias: Optional[bool] = None,
        weight_init: Optional[str] = None,
        skip_connection: Optional[bool] = None,
        residual_blocks: Optional[bool] = None,
        use_attention: Optional[bool] = None,
        
        # Model Type and Variants
        model_type: Optional[str] = None,
        model_types: Optional[List[str]] = None,
        available_activations: Optional[List[str]] = None,
        available_normalizations: Optional[List[str]] = None,
        available_initializers: Optional[List[str]] = None,
        legacy_mode: Optional[bool] = None,
        
        # Ensemble Parameters (accepted but ignored for SimpleAutoencoder)
        diversity_factor: Optional[float] = None,
        min_features: Optional[int] = None,
        num_models: Optional[int] = None,
        
        # Training Parameters
        batch_size: Optional[int] = None,
        epochs: Optional[int] = None,
        learning_rate: Optional[float] = None,
        patience: Optional[int] = None,
        weight_decay: Optional[float] = None,
        gradient_clip: Optional[float] = None,
        gradient_accumulation_steps: Optional[int] = None,
        mixed_precision: Optional[bool] = None,
        num_workers: Optional[int] = None,
        optimizer: Optional[str] = None,
        scheduler: Optional[str] = None,
        scheduler_params: Optional[Dict[str, Any]] = None,
        early_stopping: Optional[bool] = None,
        validation_split: Optional[float] = None,
        shuffle: Optional[bool] = None,
        pin_memory: Optional[bool] = None,
        persistent_workers: Optional[bool] = None,
        adam_betas: Optional[Tuple[float, float]] = None,
        adam_eps: Optional[float] = None,
        lr_patience: Optional[int] = None,
        lr_factor: Optional[float] = None,
        min_lr: Optional[float] = None,
        
        # Data Parameters
        normal_samples: Optional[int] = None,
        attack_samples: Optional[int] = None,
        features: Optional[int] = None,
        use_real_data: Optional[bool] = None,
        data_normalization: Optional[str] = None,
        anomaly_factor: Optional[float] = None,
        random_state: Optional[int] = None,
        test_split: Optional[float] = None,
        stratified_split: Optional[bool] = None,
        data_path: Optional[str] = None,
        artifacts_path: Optional[str] = None,
        synthetic_generation: Optional[Dict[str, Any]] = None,
        preprocessing: Optional[Dict[str, Any]] = None,
        
        # Security Parameters
        percentile: Optional[float] = None,
        attack_threshold: Optional[float] = None,
        false_negative_cost: Optional[float] = None,
        enable_security_metrics: Optional[bool] = None,
        anomaly_threshold_strategy: Optional[str] = None,
        early_warning_threshold: Optional[float] = None,
        adaptive_threshold: Optional[bool] = None,
        confidence_interval: Optional[float] = None,
        detection_methods: Optional[List[str]] = None,
        alert_levels: Optional[List[str]] = None,
        threshold_validation: Optional[bool] = None,
        robust_detection: Optional[bool] = None,
        false_positive_tolerance: Optional[float] = None,
        performance_optimized_detection: Optional[bool] = None,
        real_time_monitoring: Optional[bool] = None,
        ensemble_voting: Optional[str] = None,
        uncertainty_threshold: Optional[float] = None,
        
        # Monitoring Parameters
        metrics_frequency: Optional[int] = None,
        checkpoint_frequency: Optional[int] = None,
        tensorboard_logging: Optional[bool] = None,
        console_logging_level: Optional[str] = None,
        save_best_model: Optional[bool] = None,
        save_model_history: Optional[bool] = None,
        metrics_to_track: Optional[List[str]] = None,
        early_stopping_metric: Optional[str] = None,
        checkpoint_format: Optional[str] = None,
        log_model_summary: Optional[bool] = None,
        tensorboard_dir: Optional[str] = None,
        log_frequency: Optional[int] = None,
        save_checkpoints: Optional[bool] = None,
        tensorboard: Optional[Dict[str, Any]] = None,
        stability_metrics: Optional[bool] = None,
        performance_metrics: Optional[bool] = None,
        profiling_enabled: Optional[bool] = None,
        
        # Hardware Parameters
        device: Optional[str] = None,
        recommended_gpu_memory: Optional[float] = None,
        minimum_system_requirements: Optional[Dict[str, Any]] = None,
        optimal_system_requirements: Optional[Dict[str, Any]] = None,
        memory_management: Optional[Dict[str, Any]] = None,
        performance_optimization: Optional[Dict[str, Any]] = None,
        detected_gpu_memory: Optional[float] = None,
        detected_system_memory: Optional[float] = None,
        system_performance_class: Optional[str] = None,
        optimization_recommendations: Optional[List[str]] = None,
        
        # System Parameters
        model_dir: Optional[str] = None,
        log_dir: Optional[str] = None,
        config_dir: Optional[str] = None,
        data_dir: Optional[str] = None,
        checkpoint_dir: Optional[str] = None,
        results_dir: Optional[str] = None,
        debug: Optional[bool] = None,
        verbose: Optional[bool] = None,
        random_seed: Optional[int] = None,
        reproducible: Optional[bool] = None,
        parallel_processing: Optional[bool] = None,
        max_workers: Optional[int] = None,
        export_onnx: Optional[bool] = None,
        non_interactive: Optional[bool] = None,
        cuda_optimizations: Optional[bool] = None,
        onnx_export: Optional[Dict[str, Any]] = None,
        distributed_training: Optional[bool] = None,
        python_executable: Optional[str] = None,
        working_directory: Optional[str] = None,
        environment_health: Optional[str] = None,
        
        # Preset Parameters
        available_presets: Optional[List[str]] = None,
        current_preset: Optional[str] = None,
        current_override: Optional[str] = None,
        override_rules: Optional[Dict[str, bool]] = None,
        preset_configs: Optional[Dict[str, str]] = None,
        custom_presets_available: Optional[List[str]] = None,
        auto_apply: Optional[bool] = None,
        validate_compatibility: Optional[bool] = None,
        system_recommended_preset: Optional[str] = None,
        preset_compatibility: Optional[Dict[str, Any]] = None,
        
        # Hyperparameter Optimization Parameters
        hpo_enabled: Optional[bool] = None,
        hpo_strategy: Optional[str] = None,
        study_name: Optional[str] = None,
        direction: Optional[str] = None,
        n_trials: Optional[int] = None,
        timeout: Optional[int] = None,
        sampler: Optional[str] = None,
        pruner: Optional[str] = None,
        objective_metric: Optional[str] = None,
        optimization_space: Optional[Dict[str, Any]] = None,
        hpo_early_stopping: Optional[Dict[str, Any]] = None,
        timeout_seconds: Optional[int] = None,
        trial_epochs: Optional[int] = None,
        trial_patience: Optional[int] = None,
        cleanup_trials: Optional[bool] = None,
        generate_plots: Optional[bool] = None,
        search_space: Optional[Dict[str, Any]] = None,
        hpo_sampler: Optional[Dict[str, Any]] = None,
        hpo_pruner: Optional[Dict[str, Any]] = None,
        scoring: Optional[Dict[str, Any]] = None,
        storage: Optional[Dict[str, Any]] = None,
        
        # Validation Parameters
        cross_validation: Optional[Dict[str, Any]] = None,
        metrics: Optional[List[str]] = None,
        validation_frequency: Optional[int] = None,
        save_validation_results: Optional[bool] = None,
        detailed_metrics: Optional[bool] = None,
        robustness_testing: Optional[bool] = None,
        performance_benchmarking: Optional[bool] = None,
        confidence_intervals: Optional[bool] = None,
        
        # Experimental Parameters
        experimental_features: Optional[Dict[str, bool]] = None,
        experimental_settings: Optional[Dict[str, bool]] = None,
        
        # Metadata Parameters
        description: Optional[str] = None,
        version: Optional[str] = None,
        config_version: Optional[str] = None,
        config_type: Optional[str] = None,
        created: Optional[str] = None,
        last_modified: Optional[str] = None,
        preset_used: Optional[str] = None,
        recommended_hardware: Optional[Dict[str, Any]] = None,
        compatibility: Optional[List[str]] = None,
        system_info: Optional[Dict[str, Any]] = None,
        validation_info: Optional[Dict[str, Any]] = None,
        
        # Runtime Parameters
        config_loaded_at: Optional[str] = None,
        config_source: Optional[str] = None,
        runtime_id: Optional[str] = None,
        process_id: Optional[int] = None,
        system_analysis_completed: Optional[bool] = None,
        system_performance_score: Optional[float] = None,
        system_class: Optional[str] = None,
        optimizations_applied: Optional[Dict[str, bool]] = None,
        resource_status: Optional[Dict[str, bool]] = None,
        system_warnings: Optional[List[str]] = None,
        recommendations: Optional[List[str]] = None,
        configuration_health: Optional[Dict[str, Any]] = None,
        
        **kwargs  # Catch any additional parameters
    ):
        super(SimpleAutoencoder, self).__init__()
        
        # Use centralized initialization helper
        init_data = _initialize_autoencoder_config(
            model_class_name='SimpleAutoencoder',
            input_dim=input_dim,
            config=config,
            preset=preset,
            # Pass through all parameters
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            bias=bias,
            weight_init=weight_init,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention,
            model_type=model_type,
            model_types=model_types,
            available_activations=available_activations,
            available_normalizations=available_normalizations,
            available_initializers=available_initializers,
            legacy_mode=legacy_mode,
            diversity_factor=diversity_factor,
            min_features=min_features,
            num_models=num_models,
            batch_size=batch_size,
            epochs=epochs,
            learning_rate=learning_rate,
            patience=patience,
            weight_decay=weight_decay,
            gradient_clip=gradient_clip,
            gradient_accumulation_steps=gradient_accumulation_steps,
            mixed_precision=mixed_precision,
            num_workers=num_workers,
            optimizer=optimizer,
            scheduler=scheduler,
            scheduler_params=scheduler_params,
            early_stopping=early_stopping,
            validation_split=validation_split,
            shuffle=shuffle,
            pin_memory=pin_memory,
            persistent_workers=persistent_workers,
            adam_betas=adam_betas,
            adam_eps=adam_eps,
            lr_patience=lr_patience,
            lr_factor=lr_factor,
            min_lr=min_lr,
            normal_samples=normal_samples,
            attack_samples=attack_samples,
            features=features,
            use_real_data=use_real_data,
            data_normalization=data_normalization,
            anomaly_factor=anomaly_factor,
            random_state=random_state,
            test_split=test_split,
            stratified_split=stratified_split,
            data_path=data_path,
            artifacts_path=artifacts_path,
            synthetic_generation=synthetic_generation,
            preprocessing=preprocessing,
            percentile=percentile,
            attack_threshold=attack_threshold,
            false_negative_cost=false_negative_cost,
            enable_security_metrics=enable_security_metrics,
            anomaly_threshold_strategy=anomaly_threshold_strategy,
            early_warning_threshold=early_warning_threshold,
            adaptive_threshold=adaptive_threshold,
            confidence_interval=confidence_interval,
            detection_methods=detection_methods,
            alert_levels=alert_levels,
            threshold_validation=threshold_validation,
            robust_detection=robust_detection,
            false_positive_tolerance=false_positive_tolerance,
            performance_optimized_detection=performance_optimized_detection,
            real_time_monitoring=real_time_monitoring,
            ensemble_voting=ensemble_voting,
            uncertainty_threshold=uncertainty_threshold,
            metrics_frequency=metrics_frequency,
            checkpoint_frequency=checkpoint_frequency,
            tensorboard_logging=tensorboard_logging,
            console_logging_level=console_logging_level,
            save_best_model=save_best_model,
            save_model_history=save_model_history,
            metrics_to_track=metrics_to_track,
            early_stopping_metric=early_stopping_metric,
            checkpoint_format=checkpoint_format,
            log_model_summary=log_model_summary,
            tensorboard_dir=tensorboard_dir,
            log_frequency=log_frequency,
            save_checkpoints=save_checkpoints,
            tensorboard=tensorboard,
            stability_metrics=stability_metrics,
            performance_metrics=performance_metrics,
            profiling_enabled=profiling_enabled,
            device=device,
            recommended_gpu_memory=recommended_gpu_memory,
            minimum_system_requirements=minimum_system_requirements,
            optimal_system_requirements=optimal_system_requirements,
            memory_management=memory_management,
            performance_optimization=performance_optimization,
            detected_gpu_memory=detected_gpu_memory,
            detected_system_memory=detected_system_memory,
            system_performance_class=system_performance_class,
            optimization_recommendations=optimization_recommendations,
            model_dir=model_dir,
            log_dir=log_dir,
            config_dir=config_dir,
            data_dir=data_dir,
            checkpoint_dir=checkpoint_dir,
            results_dir=results_dir,
            debug=debug,
            verbose=verbose,
            random_seed=random_seed,
            reproducible=reproducible,
            parallel_processing=parallel_processing,
            max_workers=max_workers,
            export_onnx=export_onnx,
            non_interactive=non_interactive,
            cuda_optimizations=cuda_optimizations,
            onnx_export=onnx_export,
            distributed_training=distributed_training,
            python_executable=python_executable,
            working_directory=working_directory,
            environment_health=environment_health,
            available_presets=available_presets,
            current_preset=current_preset,
            current_override=current_override,
            override_rules=override_rules,
            preset_configs=preset_configs,
            custom_presets_available=custom_presets_available,
            auto_apply=auto_apply,
            validate_compatibility=validate_compatibility,
            system_recommended_preset=system_recommended_preset,
            preset_compatibility=preset_compatibility,
            hpo_enabled=hpo_enabled,
            hpo_strategy=hpo_strategy,
            study_name=study_name,
            direction=direction,
            n_trials=n_trials,
            timeout=timeout,
            sampler=sampler,
            pruner=pruner,
            objective_metric=objective_metric,
            optimization_space=optimization_space,
            hpo_early_stopping=hpo_early_stopping,
            timeout_seconds=timeout_seconds,
            trial_epochs=trial_epochs,
            trial_patience=trial_patience,
            cleanup_trials=cleanup_trials,
            generate_plots=generate_plots,
            search_space=search_space,
            hpo_sampler=hpo_sampler,
            hpo_pruner=hpo_pruner,
            scoring=scoring,
            storage=storage,
            cross_validation=cross_validation,
            metrics=metrics,
            validation_frequency=validation_frequency,
            save_validation_results=save_validation_results,
            detailed_metrics=detailed_metrics,
            robustness_testing=robustness_testing,
            performance_benchmarking=performance_benchmarking,
            confidence_intervals=confidence_intervals,
            experimental_features=experimental_features,
            experimental_settings=experimental_settings,
            description=description,
            version=version,
            config_version=config_version,
            config_type=config_type,
            created=created,
            last_modified=last_modified,
            preset_used=preset_used,
            recommended_hardware=recommended_hardware,
            compatibility=compatibility,
            system_info=system_info,
            validation_info=validation_info,
            config_loaded_at=config_loaded_at,
            config_source=config_source,
            runtime_id=runtime_id,
            process_id=process_id,
            system_analysis_completed=system_analysis_completed,
            system_performance_score=system_performance_score,
            system_class=system_class,
            optimizations_applied=optimizations_applied,
            resource_status=resource_status,
            system_warnings=system_warnings,
            recommendations=recommendations,
            configuration_health=configuration_health,
            **kwargs
        )
        
        # Extract processed data from centralized initialization
        self.config = init_data['config']
        params = init_data['processed_params']
        self.device = init_data['device']
        self.mixed_precision = init_data['mixed_precision']
        self.initialization_timestamp = init_data['initialization_timestamp']
        #self.preset_name = init_data['preset_name']
        # PROPERLY STORE PRESET NAME WITH FALLBACKS
        self.preset_name = init_data.get('preset_name') or preset
        #self.preset_name = init_data['preset_name'] or preset
        
        # If still no preset name, try to extract from config
        if not self.preset_name:
            self.preset_name = (
                self.config.get('metadata', {}).get('preset_used') or
                self.config.get('presets', {}).get('current_preset') or
                self.config.get('runtime', {}).get('applied_preset') or
                "Custom"
            )
        
        # ENSURE PRESET NAME IS STORED IN CONFIG FOR CONSISTENCY
        if self.preset_name and self.preset_name != "Custom":
            if 'metadata' not in self.config:
                self.config['metadata'] = {}
            self.config['metadata']['preset_used'] = self.preset_name
            
            if 'presets' not in self.config:
                self.config['presets'] = {}
            self.config['presets']['current_preset'] = self.preset_name
        
        training_config = init_data['training_config']
        
        # Set instance attributes from processed parameters
        self.input_dim = params['input_dim']
        self.encoding_dim = params['encoding_dim']
        self.hidden_dims = params['hidden_dims']
        self.dropout_rates = params['dropout_rates']
        self.activation = params['activation']
        self.activation_param = params['activation_param']
        self.normalization = params['normalization']
        self.use_batch_norm = params['use_batch_norm']
        self.use_layer_norm = params['use_layer_norm']
        self.bias = params['bias']
        self.weight_init = params['weight_init']
        self.skip_connection = params['skip_connection']
        self.residual_blocks = params['residual_blocks']
        self.use_attention = params['use_attention']
        self.model_type = params['model_type']
        self.min_features = params['min_features']
        
        # Build network architecture
        self._build_network()
        
        # Initialize weights
        self._initialize_weights()
        
        # Move to device
        self.to(self.device)
        
        # Log successful initialization
        if hasattr(globals().get('display_model_initialization_summary'), '__call__'):
            try:
                architecture_info = {
                    'hidden_dims': self.hidden_dims,
                    'encoding_dim': self.encoding_dim,
                    'dropout_rates': self.dropout_rates,
                    'activation': self.activation,
                    'normalization': self.normalization
                }
                
                display_model_initialization_summary(
                    model_instance=self,
                    model_type='SimpleAutoencoder',
                    input_dim=self.input_dim,
                    architecture_info=architecture_info,
                    device=self.device,
                    mixed_precision=self.mixed_precision,
                    preset_name=self.preset_name,
                    training_config=training_config
                )
            except Exception as e:
                logger.debug(f"Could not display initialization summary: {e}")
    
    def _build_network(self) -> None:
        """Build the autoencoder network architecture."""
        try:
            # Encoder layers
            encoder_layers = []
            dims = [self.input_dim] + self.hidden_dims + [self.encoding_dim]
            
            for i in range(len(dims) - 1):
                # Linear layer
                encoder_layers.append(nn.Linear(dims[i], dims[i + 1], bias=self.bias))
                
                # Normalization (except for last layer)
                if i < len(dims) - 2:
                    norm_layer = self._get_normalization_layer(dims[i + 1])
                    if norm_layer is not None:
                        encoder_layers.append(norm_layer)
                    
                    # Activation
                    encoder_layers.append(self._get_activation_function())
                    
                    # Dropout
                    if i < len(self.dropout_rates):
                        dropout_rate = self.dropout_rates[i]
                        if dropout_rate > 0:
                            encoder_layers.append(nn.Dropout(dropout_rate))
            
            self.encoder = nn.Sequential(*encoder_layers)
            
            # Decoder layers (reverse of encoder)
            decoder_layers = []
            dims = [self.encoding_dim] + list(reversed(self.hidden_dims)) + [self.input_dim]
            
            for i in range(len(dims) - 1):
                # Linear layer
                decoder_layers.append(nn.Linear(dims[i], dims[i + 1], bias=self.bias))
                
                # Normalization (except for last layer)
                if i < len(dims) - 2:
                    norm_layer = self._get_normalization_layer(dims[i + 1])
                    if norm_layer is not None:
                        decoder_layers.append(norm_layer)
                    
                    # Activation
                    decoder_layers.append(self._get_activation_function())
                    
                    # Dropout (reversed order)
                    if i < len(self.dropout_rates):
                        dropout_idx = len(self.dropout_rates) - 1 - i
                        dropout_rate = self.dropout_rates[dropout_idx]
                        if dropout_rate > 0:
                            decoder_layers.append(nn.Dropout(dropout_rate))
            
            # Final activation for decoder output
            decoder_layers.append(nn.Sigmoid())
            
            self.decoder = nn.Sequential(*decoder_layers)
            
            logger.debug(f"Built SimpleAutoencoder: {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}")
            
        except Exception as e:
            logger.error(f"Failed to build SimpleAutoencoder network: {e}")
            raise RuntimeError(f"Network construction failed: {e}")
    
    def _get_activation_function(self) -> nn.Module:
        """Get activation function based on configuration."""
        activation_map = {
            'relu': nn.ReLU(inplace=True),
            'leaky_relu': nn.LeakyReLU(negative_slope=self.activation_param, inplace=True),
            'gelu': nn.GELU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid(),
            'swish': nn.SiLU(),
            'elu': nn.ELU(alpha=self.activation_param, inplace=True)
        }
        
        if self.activation not in activation_map:
            logger.warning(f"Unknown activation '{self.activation}', using ReLU")
            return nn.ReLU(inplace=True)
        
        return activation_map[self.activation]
    
    def _get_normalization_layer(self, num_features: int) -> Optional[nn.Module]:
        """Get normalization layer based on configuration."""
        if self.normalization is None or self.normalization == 'none':
            return None
        
        if self.normalization == 'batch' and self.use_batch_norm:
            return nn.BatchNorm1d(num_features)
        elif self.normalization == 'layer' and self.use_layer_norm:
            return nn.LayerNorm(num_features)
        elif self.normalization == 'instance':
            return nn.InstanceNorm1d(num_features)
        else:
            return None
    
    def _initialize_weights(self) -> None:
        """Initialize network weights based on configuration."""
        try:
            init_method = self.weight_init
            
            for module in self.modules():
                if isinstance(module, nn.Linear):
                    if init_method == 'xavier_uniform':
                        nn.init.xavier_uniform_(module.weight)
                    elif init_method == 'xavier_normal':
                        nn.init.xavier_normal_(module.weight)
                    elif init_method == 'kaiming_uniform':
                        nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')
                    elif init_method == 'kaiming_normal':
                        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')
                    elif init_method == 'orthogonal':
                        nn.init.orthogonal_(module.weight)
                    else:
                        logger.warning(f"Unknown weight init '{init_method}', using xavier_uniform")
                        nn.init.xavier_uniform_(module.weight)
                    
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0)
                
                elif isinstance(module, (nn.BatchNorm1d, nn.LayerNorm)):
                    if hasattr(module, 'weight') and module.weight is not None:
                        nn.init.constant_(module.weight, 1)
                    if hasattr(module, 'bias') and module.bias is not None:
                        nn.init.constant_(module.bias, 0)
            
            logger.debug(f"Initialized weights using {init_method}")
            
        except Exception as e:
            logger.error(f"Weight initialization failed: {e}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the autoencoder."""
        # Input validation
        if not isinstance(x, torch.Tensor):
            raise ValueError(f"Expected torch.Tensor input, got {type(x)}")
        
        if x.dim() != 2:
            raise ValueError(f"Expected 2D input (batch_size, features), got {x.dim()}D")
        
        if x.size(-1) != self.input_dim:
            raise ValueError(f"Input size {x.size(-1)} doesn't match expected {self.input_dim}")
        
        try:
            # Mixed precision forward pass
            #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
            device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
            with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
                # Encode
                encoded = self.encoder(x)
                
                # Decode
                decoded = self.decoder(encoded)
                
                return decoded
                
        except Exception as e:
            logger.error(f"Forward pass failed: {e}")
            raise RuntimeError(f"Forward pass error: {e}")
    
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode input data to latent representation."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            return self.encoder(x)
    
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode latent representation to original space."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            return self.decoder(z)
    
    def get_config(self) -> Dict[str, Any]:
        """Get comprehensive model configuration."""
        config = self.config.copy()
        
        # Add runtime information
        config.setdefault('runtime', {}).update({
            'model_initialized_at': self.initialization_timestamp,
            'total_parameters': sum(p.numel() for p in self.parameters()),
            'trainable_parameters': sum(p.numel() for p in self.parameters() if p.requires_grad),
            'device': str(self.device),
            'mixed_precision_active': self.mixed_precision,
            'preset_used': self.preset_name,
            'input_dim': self.input_dim,
            'architecture_summary': f"{self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            'centralized_config_used': True,
            'helper_functions_leveraged': True
        })
        
        return config
    
    def update_config(self, new_config: Dict[str, Any], reinitialize: bool = False) -> None:
        """Update configuration and optionally reinitialize components."""
        old_config = self.config.copy()
        
        try:
            # Update configuration using existing helper
            self.config = deep_update(self.config, new_config)
            
            # Reinitialize components if requested
            if reinitialize:
                logger.debug("Reinitializing model components due to config update")
                
                # Check if architecture changed
                architecture_changed = any(
                    key in new_config.get('model', {}) 
                    for key in ['encoding_dim', 'hidden_dims', 'dropout_rates']
                )
                
                if architecture_changed:
                    logger.warning("Architecture parameters changed - full model rebuild required")
                    raise ValueError("Architecture changes require creating a new model instance")
                
                # Update device configuration if changed
                if 'hardware' in new_config:
                    old_device = self.device
                    device_setting = new_config['hardware'].get('device')
                    if device_setting:
                        if device_setting == 'auto':
                            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                        elif device_setting in ['cpu', 'cuda']:
                            self.device = torch.device(device_setting)
                        
                        if self.device != old_device:
                            self.to(self.device)
                            logger.debug(f"Device changed from {old_device} to {self.device}")
            
            logger.debug("Configuration updated successfully using centralized helper")
            
        except Exception as e:
            # Restore old configuration on error
            self.config = old_config
            logger.error(f"Configuration update failed, restored previous config: {e}")
            raise
    
    def save_model(self, path: str, include_config: bool = True) -> None:
        """Save model state and configuration."""
        save_dict = {
            'model_state_dict': self.state_dict(),
            'model_class': self.__class__.__name__,
            'input_dim': self.input_dim,
            'architecture': {
                'encoding_dim': self.encoding_dim,
                'hidden_dims': self.hidden_dims,
                'dropout_rates': self.dropout_rates
            },
            'centralized_config_used': True
        }
        
        if include_config:
            save_dict['config'] = self.config
        
        torch.save(save_dict, path)
        logger.debug(f"SimpleAutoencoder saved to {path} (centralized config)")
    
    @classmethod
    def load_model(cls, path: str, **kwargs):
        """Load model from saved state."""
        checkpoint = torch.load(path, map_location='cpu')
        
        # Extract architecture parameters
        input_dim = checkpoint['input_dim']
        config = checkpoint.get('config', {})
        
        # Create model instance using centralized config if available
        if checkpoint.get('centralized_config_used', False):
            try:
                model = create_model_instance('SimpleAutoencoder', input_dim, config, **kwargs)
            except:
                # Fallback to direct instantiation
                model = cls(input_dim=input_dim, config=config, **kwargs)
        else:
            model = cls(input_dim=input_dim, config=config, **kwargs)
        
        # Load state dict
        model.load_state_dict(checkpoint['model_state_dict'])
        
        logger.debug(f"SimpleAutoencoder loaded from {path}")
        return model
    
    def get_model_summary(self) -> str:
        """Get detailed model summary."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        summary = [
            f"SimpleAutoencoder Summary (Factory Pattern)",
            f"{'='*60}",
            f"Model Type: {self.model_type}",
            f"Preset: {self.preset_name or 'Custom'}",
            f"Architecture: {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            f"Activation: {self.activation}",
            f"Normalization: {self.normalization}",
            f"Device: {self.device}",
            f"Mixed Precision: {self.mixed_precision}",
            f"Parameters: {total_params:,} total, {trainable_params:,} trainable",
            f"Memory Usage: ~{total_params * 4 / (1024**2):.1f} MB (FP32)",
            f"Factory Pattern: Enabled",
            f"Centralized Config: Enabled",
            f"Helper Functions: Leveraged",
            f"{'='*60}"
        ]
        
        return '\n'.join(summary)
    
    def __repr__(self) -> str:
        """String representation of the model."""
        return (f"SimpleAutoencoder(input_dim={self.input_dim}, "
                f"encoding_dim={self.encoding_dim}, "
                f"hidden_dims={self.hidden_dims}, "
                f"device={self.device}, "
                f"preset='{self.preset_name or 'Custom'}', "
                f"factory_pattern=True, "
                f"centralized_config=True)")

class EnhancedAutoencoder(nn.Module):
    """
    Streamlined EnhancedAutoencoder using centralized configuration initialization.
    
    This class leverages the _initialize_autoencoder_config() helper function to eliminate
    redundancy and provide consistent parameter processing across all autoencoder variants.
    Enhanced features include attention mechanisms, residual blocks, skip connections, and
    advanced architectures while maintaining full compatibility with all preset parameters.
    """
    
    def __init__(
        self,
        # Core parameter - always required
        input_dim: Optional[int] = None,
        
        # Configuration override - can contain any/all preset parameters
        config: Optional[Dict[str, Any]] = None,
        preset: Optional[str] = None,
        
        # ALL preset parameters accepted for backward compatibility
        # Core Model Architecture Parameters
        encoding_dim: Optional[int] = None,
        hidden_dims: Optional[List[int]] = None,
        dropout_rates: Optional[List[float]] = None,
        activation: Optional[str] = None,
        activation_param: Optional[float] = None,
        normalization: Optional[str] = None,
        use_batch_norm: Optional[bool] = None,
        use_layer_norm: Optional[bool] = None,
        bias: Optional[bool] = None,
        weight_init: Optional[str] = None,
        skip_connection: Optional[bool] = None,
        residual_blocks: Optional[bool] = None,
        use_attention: Optional[bool] = None,
        
        # Model Type and Variants
        model_type: Optional[str] = None,
        model_types: Optional[List[str]] = None,
        available_activations: Optional[List[str]] = None,
        available_normalizations: Optional[List[str]] = None,
        available_initializers: Optional[List[str]] = None,
        legacy_mode: Optional[bool] = None,
        
        # Ensemble Parameters (accepted for compatibility)
        diversity_factor: Optional[float] = None,
        min_features: Optional[int] = None,
        num_models: Optional[int] = None,
        
        # Training Parameters
        batch_size: Optional[int] = None,
        epochs: Optional[int] = None,
        learning_rate: Optional[float] = None,
        patience: Optional[int] = None,
        weight_decay: Optional[float] = None,
        gradient_clip: Optional[float] = None,
        gradient_accumulation_steps: Optional[int] = None,
        mixed_precision: Optional[bool] = None,
        num_workers: Optional[int] = None,
        optimizer: Optional[str] = None,
        scheduler: Optional[str] = None,
        scheduler_params: Optional[Dict[str, Any]] = None,
        early_stopping: Optional[bool] = None,
        validation_split: Optional[float] = None,
        shuffle: Optional[bool] = None,
        pin_memory: Optional[bool] = None,
        persistent_workers: Optional[bool] = None,
        adam_betas: Optional[Tuple[float, float]] = None,
        adam_eps: Optional[float] = None,
        lr_patience: Optional[int] = None,
        lr_factor: Optional[float] = None,
        min_lr: Optional[float] = None,
        
        # Data Parameters
        normal_samples: Optional[int] = None,
        attack_samples: Optional[int] = None,
        features: Optional[int] = None,
        use_real_data: Optional[bool] = None,
        data_normalization: Optional[str] = None,
        anomaly_factor: Optional[float] = None,
        random_state: Optional[int] = None,
        test_split: Optional[float] = None,
        stratified_split: Optional[bool] = None,
        data_path: Optional[str] = None,
        artifacts_path: Optional[str] = None,
        synthetic_generation: Optional[Dict[str, Any]] = None,
        preprocessing: Optional[Dict[str, Any]] = None,
        
        # Security Parameters
        percentile: Optional[float] = None,
        attack_threshold: Optional[float] = None,
        false_negative_cost: Optional[float] = None,
        enable_security_metrics: Optional[bool] = None,
        anomaly_threshold_strategy: Optional[str] = None,
        early_warning_threshold: Optional[float] = None,
        adaptive_threshold: Optional[bool] = None,
        confidence_interval: Optional[float] = None,
        detection_methods: Optional[List[str]] = None,
        alert_levels: Optional[List[str]] = None,
        threshold_validation: Optional[bool] = None,
        robust_detection: Optional[bool] = None,
        false_positive_tolerance: Optional[float] = None,
        performance_optimized_detection: Optional[bool] = None,
        real_time_monitoring: Optional[bool] = None,
        ensemble_voting: Optional[str] = None,
        uncertainty_threshold: Optional[float] = None,
        
        # Monitoring Parameters
        metrics_frequency: Optional[int] = None,
        checkpoint_frequency: Optional[int] = None,
        tensorboard_logging: Optional[bool] = None,
        console_logging_level: Optional[str] = None,
        save_best_model: Optional[bool] = None,
        save_model_history: Optional[bool] = None,
        metrics_to_track: Optional[List[str]] = None,
        early_stopping_metric: Optional[str] = None,
        checkpoint_format: Optional[str] = None,
        log_model_summary: Optional[bool] = None,
        tensorboard_dir: Optional[str] = None,
        log_frequency: Optional[int] = None,
        save_checkpoints: Optional[bool] = None,
        tensorboard: Optional[Dict[str, Any]] = None,
        stability_metrics: Optional[bool] = None,
        performance_metrics: Optional[bool] = None,
        profiling_enabled: Optional[bool] = None,
        
        # Hardware Parameters
        device: Optional[str] = None,
        recommended_gpu_memory: Optional[float] = None,
        minimum_system_requirements: Optional[Dict[str, Any]] = None,
        optimal_system_requirements: Optional[Dict[str, Any]] = None,
        memory_management: Optional[Dict[str, Any]] = None,
        performance_optimization: Optional[Dict[str, Any]] = None,
        detected_gpu_memory: Optional[float] = None,
        detected_system_memory: Optional[float] = None,
        system_performance_class: Optional[str] = None,
        optimization_recommendations: Optional[List[str]] = None,
        
        # System Parameters
        model_dir: Optional[str] = None,
        log_dir: Optional[str] = None,
        config_dir: Optional[str] = None,
        data_dir: Optional[str] = None,
        checkpoint_dir: Optional[str] = None,
        results_dir: Optional[str] = None,
        debug: Optional[bool] = None,
        verbose: Optional[bool] = None,
        random_seed: Optional[int] = None,
        reproducible: Optional[bool] = None,
        parallel_processing: Optional[bool] = None,
        max_workers: Optional[int] = None,
        export_onnx: Optional[bool] = None,
        non_interactive: Optional[bool] = None,
        cuda_optimizations: Optional[bool] = None,
        onnx_export: Optional[Dict[str, Any]] = None,
        distributed_training: Optional[bool] = None,
        python_executable: Optional[str] = None,
        working_directory: Optional[str] = None,
        environment_health: Optional[str] = None,
        
        # Preset Parameters
        available_presets: Optional[List[str]] = None,
        current_preset: Optional[str] = None,
        current_override: Optional[str] = None,
        override_rules: Optional[Dict[str, bool]] = None,
        preset_configs: Optional[Dict[str, str]] = None,
        custom_presets_available: Optional[List[str]] = None,
        auto_apply: Optional[bool] = None,
        validate_compatibility: Optional[bool] = None,
        system_recommended_preset: Optional[str] = None,
        preset_compatibility: Optional[Dict[str, Any]] = None,
        
        # Hyperparameter Optimization Parameters
        hpo_enabled: Optional[bool] = None,
        hpo_strategy: Optional[str] = None,
        study_name: Optional[str] = None,
        direction: Optional[str] = None,
        n_trials: Optional[int] = None,
        timeout: Optional[int] = None,
        sampler: Optional[str] = None,
        pruner: Optional[str] = None,
        objective_metric: Optional[str] = None,
        optimization_space: Optional[Dict[str, Any]] = None,
        hpo_early_stopping: Optional[Dict[str, Any]] = None,
        timeout_seconds: Optional[int] = None,
        trial_epochs: Optional[int] = None,
        trial_patience: Optional[int] = None,
        cleanup_trials: Optional[bool] = None,
        generate_plots: Optional[bool] = None,
        search_space: Optional[Dict[str, Any]] = None,
        hpo_sampler: Optional[Dict[str, Any]] = None,
        hpo_pruner: Optional[Dict[str, Any]] = None,
        scoring: Optional[Dict[str, Any]] = None,
        storage: Optional[Dict[str, Any]] = None,
        
        # Validation Parameters
        cross_validation: Optional[Dict[str, Any]] = None,
        metrics: Optional[List[str]] = None,
        validation_frequency: Optional[int] = None,
        save_validation_results: Optional[bool] = None,
        detailed_metrics: Optional[bool] = None,
        robustness_testing: Optional[bool] = None,
        performance_benchmarking: Optional[bool] = None,
        confidence_intervals: Optional[bool] = None,
        
        # Experimental Parameters
        experimental_features: Optional[Dict[str, bool]] = None,
        experimental_settings: Optional[Dict[str, bool]] = None,
        
        # Metadata Parameters
        description: Optional[str] = None,
        version: Optional[str] = None,
        config_version: Optional[str] = None,
        config_type: Optional[str] = None,
        created: Optional[str] = None,
        last_modified: Optional[str] = None,
        preset_used: Optional[str] = None,
        recommended_hardware: Optional[Dict[str, Any]] = None,
        compatibility: Optional[List[str]] = None,
        system_info: Optional[Dict[str, Any]] = None,
        validation_info: Optional[Dict[str, Any]] = None,
        
        # Runtime Parameters
        config_loaded_at: Optional[str] = None,
        config_source: Optional[str] = None,
        runtime_id: Optional[str] = None,
        process_id: Optional[int] = None,
        system_analysis_completed: Optional[bool] = None,
        system_performance_score: Optional[float] = None,
        system_class: Optional[str] = None,
        optimizations_applied: Optional[Dict[str, bool]] = None,
        resource_status: Optional[Dict[str, bool]] = None,
        system_warnings: Optional[List[str]] = None,
        recommendations: Optional[List[str]] = None,
        configuration_health: Optional[Dict[str, Any]] = None,
        
        **kwargs  # Catch any additional parameters
    ):
        super(EnhancedAutoencoder, self).__init__()
        
        # Use centralized initialization helper
        init_data = _initialize_autoencoder_config(
            model_class_name='EnhancedAutoencoder',
            input_dim=input_dim,
            config=config,
            preset=preset,
            # Pass through all parameters
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            bias=bias,
            weight_init=weight_init,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention,
            model_type=model_type,
            model_types=model_types,
            available_activations=available_activations,
            available_normalizations=available_normalizations,
            available_initializers=available_initializers,
            legacy_mode=legacy_mode,
            diversity_factor=diversity_factor,
            min_features=min_features,
            num_models=num_models,
            batch_size=batch_size,
            epochs=epochs,
            learning_rate=learning_rate,
            patience=patience,
            weight_decay=weight_decay,
            gradient_clip=gradient_clip,
            gradient_accumulation_steps=gradient_accumulation_steps,
            mixed_precision=mixed_precision,
            num_workers=num_workers,
            optimizer=optimizer,
            scheduler=scheduler,
            scheduler_params=scheduler_params,
            early_stopping=early_stopping,
            validation_split=validation_split,
            shuffle=shuffle,
            pin_memory=pin_memory,
            persistent_workers=persistent_workers,
            adam_betas=adam_betas,
            adam_eps=adam_eps,
            lr_patience=lr_patience,
            lr_factor=lr_factor,
            min_lr=min_lr,
            normal_samples=normal_samples,
            attack_samples=attack_samples,
            features=features,
            use_real_data=use_real_data,
            data_normalization=data_normalization,
            anomaly_factor=anomaly_factor,
            random_state=random_state,
            test_split=test_split,
            stratified_split=stratified_split,
            data_path=data_path,
            artifacts_path=artifacts_path,
            synthetic_generation=synthetic_generation,
            preprocessing=preprocessing,
            percentile=percentile,
            attack_threshold=attack_threshold,
            false_negative_cost=false_negative_cost,
            enable_security_metrics=enable_security_metrics,
            anomaly_threshold_strategy=anomaly_threshold_strategy,
            early_warning_threshold=early_warning_threshold,
            adaptive_threshold=adaptive_threshold,
            confidence_interval=confidence_interval,
            detection_methods=detection_methods,
            alert_levels=alert_levels,
            threshold_validation=threshold_validation,
            robust_detection=robust_detection,
            false_positive_tolerance=false_positive_tolerance,
            performance_optimized_detection=performance_optimized_detection,
            real_time_monitoring=real_time_monitoring,
            ensemble_voting=ensemble_voting,
            uncertainty_threshold=uncertainty_threshold,
            metrics_frequency=metrics_frequency,
            checkpoint_frequency=checkpoint_frequency,
            tensorboard_logging=tensorboard_logging,
            console_logging_level=console_logging_level,
            save_best_model=save_best_model,
            save_model_history=save_model_history,
            metrics_to_track=metrics_to_track,
            early_stopping_metric=early_stopping_metric,
            checkpoint_format=checkpoint_format,
            log_model_summary=log_model_summary,
            tensorboard_dir=tensorboard_dir,
            log_frequency=log_frequency,
            save_checkpoints=save_checkpoints,
            tensorboard=tensorboard,
            stability_metrics=stability_metrics,
            performance_metrics=performance_metrics,
            profiling_enabled=profiling_enabled,
            device=device,
            recommended_gpu_memory=recommended_gpu_memory,
            minimum_system_requirements=minimum_system_requirements,
            optimal_system_requirements=optimal_system_requirements,
            memory_management=memory_management,
            performance_optimization=performance_optimization,
            detected_gpu_memory=detected_gpu_memory,
            detected_system_memory=detected_system_memory,
            system_performance_class=system_performance_class,
            optimization_recommendations=optimization_recommendations,
            model_dir=model_dir,
            log_dir=log_dir,
            config_dir=config_dir,
            data_dir=data_dir,
            checkpoint_dir=checkpoint_dir,
            results_dir=results_dir,
            debug=debug,
            verbose=verbose,
            random_seed=random_seed,
            reproducible=reproducible,
            parallel_processing=parallel_processing,
            max_workers=max_workers,
            export_onnx=export_onnx,
            non_interactive=non_interactive,
            cuda_optimizations=cuda_optimizations,
            onnx_export=onnx_export,
            distributed_training=distributed_training,
            python_executable=python_executable,
            working_directory=working_directory,
            environment_health=environment_health,
            available_presets=available_presets,
            current_preset=current_preset,
            current_override=current_override,
            override_rules=override_rules,
            preset_configs=preset_configs,
            custom_presets_available=custom_presets_available,
            auto_apply=auto_apply,
            validate_compatibility=validate_compatibility,
            system_recommended_preset=system_recommended_preset,
            preset_compatibility=preset_compatibility,
            hpo_enabled=hpo_enabled,
            hpo_strategy=hpo_strategy,
            study_name=study_name,
            direction=direction,
            n_trials=n_trials,
            timeout=timeout,
            sampler=sampler,
            pruner=pruner,
            objective_metric=objective_metric,
            optimization_space=optimization_space,
            hpo_early_stopping=hpo_early_stopping,
            timeout_seconds=timeout_seconds,
            trial_epochs=trial_epochs,
            trial_patience=trial_patience,
            cleanup_trials=cleanup_trials,
            generate_plots=generate_plots,
            search_space=search_space,
            hpo_sampler=hpo_sampler,
            hpo_pruner=hpo_pruner,
            scoring=scoring,
            storage=storage,
            cross_validation=cross_validation,
            metrics=metrics,
            validation_frequency=validation_frequency,
            save_validation_results=save_validation_results,
            detailed_metrics=detailed_metrics,
            robustness_testing=robustness_testing,
            performance_benchmarking=performance_benchmarking,
            confidence_intervals=confidence_intervals,
            experimental_features=experimental_features,
            experimental_settings=experimental_settings,
            description=description,
            version=version,
            config_version=config_version,
            config_type=config_type,
            created=created,
            last_modified=last_modified,
            preset_used=preset_used,
            recommended_hardware=recommended_hardware,
            compatibility=compatibility,
            system_info=system_info,
            validation_info=validation_info,
            config_loaded_at=config_loaded_at,
            config_source=config_source,
            runtime_id=runtime_id,
            process_id=process_id,
            system_analysis_completed=system_analysis_completed,
            system_performance_score=system_performance_score,
            system_class=system_class,
            optimizations_applied=optimizations_applied,
            resource_status=resource_status,
            system_warnings=system_warnings,
            recommendations=recommendations,
            configuration_health=configuration_health,
            **kwargs
        )
        
        # Extract processed data from centralized initialization
        self.config = init_data['config']
        params = init_data['processed_params']
        self.device = init_data['device']
        self.mixed_precision = init_data['mixed_precision']
        self.mixed_precision_requested = init_data['mixed_precision_requested']
        self.initialization_timestamp = init_data['initialization_timestamp']
        #self.preset_name = init_data['preset_name']
        # PROPERLY STORE PRESET NAME WITH FALLBACKS
        self.preset_name = init_data.get('preset_name') or preset
        #self.preset_name = init_data['preset_name'] or preset
        
        # If still no preset name, try to extract from config
        if not self.preset_name:
            self.preset_name = (
                self.config.get('metadata', {}).get('preset_used') or
                self.config.get('presets', {}).get('current_preset') or
                self.config.get('runtime', {}).get('applied_preset') or
                "Custom"
            )
        
        # ENSURE PRESET NAME IS STORED IN CONFIG FOR CONSISTENCY
        if self.preset_name and self.preset_name != "Custom":
            if 'metadata' not in self.config:
                self.config['metadata'] = {}
            self.config['metadata']['preset_used'] = self.preset_name
            
            if 'presets' not in self.config:
                self.config['presets'] = {}
            self.config['presets']['current_preset'] = self.preset_name
        
        training_config = init_data['training_config']
        
        # Set instance attributes from processed parameters
        self.input_dim = params['input_dim']
        self.encoding_dim = params['encoding_dim']
        self.hidden_dims = params['hidden_dims']
        self.dropout_rates = params['dropout_rates']
        self.activation = params['activation']
        self.activation_param = params['activation_param']
        self.normalization = params['normalization']
        self.use_batch_norm = params['use_batch_norm']
        self.use_layer_norm = params['use_layer_norm']
        self.bias = params['bias']
        self.weight_init = params['weight_init']
        self.skip_connection = params['skip_connection']
        self.residual_blocks = params['residual_blocks']
        self.use_attention = params['use_attention']
        self.model_type = params['model_type']
        self.min_features = params['min_features']
        self.legacy_mode = params['legacy_mode']
        
        # Build enhanced network architecture
        self._build_enhanced_architecture()
        
        # Initialize weights
        self._initialize_weights()
        
        # Move to device
        self.to(self.device)
        
        # Log successful initialization
        if hasattr(globals().get('display_model_initialization_summary'), '__call__'):
            try:
                architecture_info = {
                    'hidden_dims': self.hidden_dims,
                    'encoding_dim': self.encoding_dim,
                    'dropout_rates': self.dropout_rates,
                    'activation': self.activation,
                    'normalization': self.normalization
                }
                
                enhanced_features = {
                    'use_attention': self.use_attention and hasattr(self, 'attention') and self.attention is not None,
                    'residual_blocks': self.residual_blocks,
                    'skip_connections': self.skip_connection,
                    'legacy_mode': self.legacy_mode
                }
                
                display_model_initialization_summary(
                    model_instance=self,
                    model_type='EnhancedAutoencoder',
                    input_dim=self.input_dim,
                    architecture_info=architecture_info,
                    device=self.device,
                    mixed_precision=self.mixed_precision,
                    preset_name=self.preset_name,
                    training_config=training_config,
                    enhanced_features=enhanced_features
                )
            except Exception as e:
                logger.debug(f"Could not display initialization summary: {e}")
    
    def _build_enhanced_architecture(self) -> None:
        """Build the enhanced autoencoder network architecture."""
        try:
            if self.legacy_mode:
                # Simple architecture for backward compatibility
                self._build_simple_architecture()
                return
            
            # Build advanced encoder
            encoder_layers = []
            dims = [self.input_dim] + self.hidden_dims + [self.encoding_dim]
            
            for i in range(len(dims) - 1):
                # Linear layer
                encoder_layers.append(nn.Linear(dims[i], dims[i + 1], bias=self.bias))
                
                # Normalization (except for last layer)
                if i < len(dims) - 2:
                    norm_layer = self._get_normalization_layer(dims[i + 1])
                    if norm_layer is not None:
                        encoder_layers.append(norm_layer)
                    
                    # Activation
                    encoder_layers.append(self._get_activation_function())
                    
                    # Residual blocks if enabled
                    if self.residual_blocks and dims[i + 1] >= 64:
                        encoder_layers.append(self.ResidualBlock(
                            dims[i + 1],
                            self._get_activation_function(),
                            self._get_normalization_layer(dims[i + 1]),
                            self.dropout_rates[i] * 0.5 if i < len(self.dropout_rates) else 0.1
                        ))
                    
                    # Dropout
                    if i < len(self.dropout_rates):
                        dropout_rate = self.dropout_rates[i]
                        if dropout_rate > 0:
                            encoder_layers.append(nn.Dropout(dropout_rate))
            
            # Final encoding activation
            encoder_layers.append(nn.Tanh())
            
            self.encoder = nn.Sequential(*encoder_layers)
            
            # Attention mechanism if enabled
            if self.use_attention and self.encoding_dim >= 64:
                # Ensure encoding_dim is divisible by number of attention heads
                num_heads = 8 if self.encoding_dim >= 64 else 4
                while self.encoding_dim % num_heads != 0 and num_heads > 1:
                    num_heads -= 1
                
                if num_heads > 0:
                    self.attention = self.MultiHeadAttention(self.encoding_dim, num_heads)
                else:
                    self.attention = None
                    logger.warning(f"Could not create attention with encoding_dim={self.encoding_dim}")
            else:
                self.attention = None
            
            # Build decoder (reverse of encoder)
            decoder_layers = []
            dims = [self.encoding_dim] + list(reversed(self.hidden_dims)) + [self.input_dim]
            
            for i in range(len(dims) - 1):
                # Linear layer
                decoder_layers.append(nn.Linear(dims[i], dims[i + 1], bias=self.bias))
                
                # Normalization (except for last layer)
                if i < len(dims) - 2:
                    norm_layer = self._get_normalization_layer(dims[i + 1])
                    if norm_layer is not None:
                        decoder_layers.append(norm_layer)
                    
                    # Activation
                    decoder_layers.append(self._get_activation_function())
                    
                    # Residual blocks if enabled
                    if self.residual_blocks and dims[i + 1] >= 64:
                        dropout_idx = len(self.dropout_rates) - 1 - i
                        dropout_rate = self.dropout_rates[dropout_idx] if dropout_idx < len(self.dropout_rates) and dropout_idx >= 0 else 0.1
                        decoder_layers.append(self.ResidualBlock(
                            dims[i + 1],
                            self._get_activation_function(),
                            self._get_normalization_layer(dims[i + 1]),
                            dropout_rate * 0.5
                        ))
                    
                    # Dropout (reversed order)
                    if i < len(self.dropout_rates):
                        dropout_idx = len(self.dropout_rates) - 1 - i
                        dropout_rate = self.dropout_rates[dropout_idx] if dropout_idx >= 0 else 0.0
                        if dropout_rate > 0:
                            decoder_layers.append(nn.Dropout(dropout_rate))
            
            # Final decoder activation
            decoder_layers.append(nn.Sigmoid())
            
            self.decoder = nn.Sequential(*decoder_layers)
            
            # Skip connections if enabled
            if self.skip_connection:
                # Create skip connections for compatible dimensions
                self.skip_layers = nn.ModuleDict()
                
                # Input to hidden layers skip connections
                for i, dim in enumerate(self.hidden_dims):
                    if self.input_dim <= dim * 2:  # Only if architecturally reasonable
                        self.skip_layers[f'input_to_hidden_{i}'] = nn.Linear(self.input_dim, dim, bias=False)
                
                # Input to output skip connection
                self.skip_layers['input_to_output'] = nn.Linear(self.input_dim, self.input_dim, bias=False)
            
            logger.debug(f"Built enhanced network: {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}")
            logger.debug(f"Enhanced features: attention={self.attention is not None}, "
                        f"residual_blocks={self.residual_blocks}, skip_connections={len(getattr(self, 'skip_layers', {}))}")
            
        except Exception as e:
            logger.error(f"Failed to build enhanced architecture: {e}")
            logger.warning("Falling back to simple architecture")
            self._build_simple_architecture()
    
    def _build_simple_architecture(self) -> None:
        """Build simple fallback architecture for legacy mode."""
        try:
            # Simple encoder
            self.encoder = nn.Sequential(
                nn.Linear(self.input_dim, self.hidden_dims[0] if self.hidden_dims else 128),
                self._get_activation_function(),
                nn.Dropout(self.dropout_rates[0] if self.dropout_rates else 0.2),
                nn.Linear(self.hidden_dims[0] if self.hidden_dims else 128, self.encoding_dim),
                nn.Tanh()
            )
            
            # Simple decoder
            self.decoder = nn.Sequential(
                nn.Linear(self.encoding_dim, self.hidden_dims[0] if self.hidden_dims else 128),
                self._get_activation_function(),
                nn.Dropout(self.dropout_rates[0] if self.dropout_rates else 0.2),
                nn.Linear(self.hidden_dims[0] if self.hidden_dims else 128, self.input_dim),
                nn.Sigmoid()
            )
            
            # No advanced features in simple mode
            self.attention = None
            self.skip_layers = None
            
            logger.debug("Built simple fallback architecture")
            
        except Exception as e:
            logger.error(f"Failed to build simple architecture: {e}")
            raise RuntimeError(f"Architecture construction completely failed: {e}")
    
    def _get_activation_function(self) -> nn.Module:
        """Get activation function based on configuration."""
        activation_map = {
            'relu': nn.ReLU(inplace=True),
            'leaky_relu': nn.LeakyReLU(negative_slope=self.activation_param, inplace=True),
            'gelu': nn.GELU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid(),
            'swish': nn.SiLU(),  # SiLU is PyTorch's implementation of Swish
            'elu': nn.ELU(alpha=self.activation_param, inplace=True),
            'selu': nn.SELU(inplace=True),
            'prelu': nn.PReLU()
        }
        
        if self.activation not in activation_map:
            logger.warning(f"Unknown activation '{self.activation}', using LeakyReLU")
            return nn.LeakyReLU(negative_slope=0.2, inplace=True)
        
        return activation_map[self.activation]
    
    def _get_normalization_layer(self, num_features: int) -> Optional[nn.Module]:
        """Get normalization layer based on configuration."""
        if self.normalization is None or self.normalization == 'none':
            return None
        
        if self.normalization == 'batch' and self.use_batch_norm:
            return nn.BatchNorm1d(num_features)
        elif self.normalization == 'layer' and self.use_layer_norm:
            return nn.LayerNorm(num_features)
        elif self.normalization == 'instance':
            return nn.InstanceNorm1d(num_features)
        elif self.normalization == 'group':
            # Use 8 groups or num_features, whichever is smaller
            num_groups = min(8, num_features)
            return nn.GroupNorm(num_groups, num_features)
        else:
            return None
    
    class ResidualBlock(nn.Module):
        """Residual block for enhanced architecture."""
        def __init__(self, dim: int, activation: nn.Module, normalization: Optional[nn.Module] = None, dropout: float = 0.0):
            super().__init__()
            self.dim = dim
            self.linear1 = nn.Linear(dim, dim)
            self.linear2 = nn.Linear(dim, dim)
            self.activation = activation
            self.normalization = normalization
            self.dropout = nn.Dropout(dropout) if dropout > 0 else None
            
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            residual = x
            
            out = self.linear1(x)
            if self.normalization is not None:
                out = self.normalization(out)
            out = self.activation(out)
            
            if self.dropout is not None:
                out = self.dropout(out)
            
            out = self.linear2(out)
            if self.normalization is not None and hasattr(self, 'normalization2'):
                out = self.normalization2(out)
            
            # Add residual connection
            out = out + residual
            
            return self.activation(out)
    
    class MultiHeadAttention(nn.Module):
        """Multi-head attention mechanism for enhanced features."""
        def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):
            super().__init__()
            self.dim = dim
            self.num_heads = num_heads
            self.head_dim = dim // num_heads
            
            if self.head_dim * num_heads != dim:
                raise ValueError(f"dim ({dim}) must be divisible by num_heads ({num_heads})")
            
            self.query = nn.Linear(dim, dim)
            self.key = nn.Linear(dim, dim)
            self.value = nn.Linear(dim, dim)
            self.out = nn.Linear(dim, dim)
            self.dropout = nn.Dropout(dropout)
            
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            batch_size = x.size(0)
            
            # Generate queries, keys, and values
            q = self.query(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
            k = self.key(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
            v = self.value(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
            
            # Attention mechanism
            scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
            attn_weights = torch.softmax(scores, dim=-1)
            attn_weights = self.dropout(attn_weights)
            
            # Apply attention
            context = torch.matmul(attn_weights, v)
            context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.dim)
            
            return self.out(context)
    
    def _initialize_weights(self) -> None:
        """Initialize network weights based on configuration."""
        try:
            init_method = self.weight_init
            
            for module in self.modules():
                if isinstance(module, nn.Linear):
                    if init_method == 'xavier_uniform':
                        nn.init.xavier_uniform_(module.weight)
                    elif init_method == 'xavier_normal':
                        nn.init.xavier_normal_(module.weight)
                    elif init_method == 'kaiming_uniform':
                        nn.init.kaiming_uniform_(module.weight, nonlinearity='leaky_relu' if self.activation == 'leaky_relu' else 'relu')
                    elif init_method == 'kaiming_normal':
                        nn.init.kaiming_normal_(module.weight, nonlinearity='leaky_relu' if self.activation == 'leaky_relu' else 'relu')
                    elif init_method == 'orthogonal':
                        nn.init.orthogonal_(module.weight)
                    elif init_method == 'he_uniform':
                        nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='relu')
                    elif init_method == 'he_normal':
                        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
                    else:
                        logger.warning(f"Unknown weight init '{init_method}', using xavier_uniform")
                        nn.init.xavier_uniform_(module.weight)
                    
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0)
                
                elif isinstance(module, (nn.BatchNorm1d, nn.LayerNorm, nn.GroupNorm)):
                    if hasattr(module, 'weight') and module.weight is not None:
                        nn.init.constant_(module.weight, 1)
                    if hasattr(module, 'bias') and module.bias is not None:
                        nn.init.constant_(module.bias, 0)
                
                elif isinstance(module, nn.InstanceNorm1d):
                    if hasattr(module, 'weight') and module.weight is not None:
                        nn.init.constant_(module.weight, 1)
                    if hasattr(module, 'bias') and module.bias is not None:
                        nn.init.constant_(module.bias, 0)
            
            logger.debug(f"Initialized weights using {init_method}")
            
        except Exception as e:
            logger.error(f"Weight initialization failed: {e}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Enhanced forward pass with attention and skip connections."""
        # Input validation
        if not isinstance(x, torch.Tensor):
            raise ValueError(f"Expected torch.Tensor input, got {type(x)}")
        
        if x.dim() != 2:
            raise ValueError(f"Expected 2D input (batch_size, features), got {x.dim()}D")
        
        if x.size(-1) != self.input_dim:
            raise ValueError(f"Input size {x.size(-1)} doesn't match expected {self.input_dim}")
        
        try:
            # Mixed precision forward pass
            #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
            device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
            with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
                # Store input for skip connections
                input_residual = x
                
                # Encode
                encoded = self.encoder(x)
                
                # Apply attention if available
                if hasattr(self, 'attention') and self.attention is not None:
                    # Reshape for attention (add sequence dimension)
                    batch_size = encoded.size(0)
                    encoded_reshaped = encoded.unsqueeze(1)  # [batch_size, 1, encoding_dim]
                    attended = self.attention(encoded_reshaped)
                    encoded = attended.squeeze(1) + encoded  # Residual connection
                
                # Decode
                decoded = self.decoder(encoded)
                
                # Apply skip connections if available
                if hasattr(self, 'skip_layers') and self.skip_layers is not None and not self.legacy_mode:
                    if 'input_to_output' in self.skip_layers:
                        skip_connection = self.skip_layers['input_to_output'](input_residual)
                        decoded = decoded + skip_connection
                
                return decoded
                
        except Exception as e:
            logger.error(f"EnhancedAutoencoder forward pass failed: {e}")
            raise RuntimeError(f"Forward pass error: {e}")
    
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode input data to latent representation with enhanced features."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            encoded = self.encoder(x)
            
            # Apply attention if available
            if hasattr(self, 'attention') and self.attention is not None:
                batch_size = encoded.size(0)
                encoded_reshaped = encoded.unsqueeze(1)
                attended = self.attention(encoded_reshaped)
                encoded = attended.squeeze(1) + encoded  # Residual connection
            
            return encoded
    
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode latent representation to original space."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            return self.decoder(z)
    
    @property
    def original_mixed_precision_setting(self) -> bool:
        """Returns the originally requested mixed precision setting."""
        return getattr(self, 'mixed_precision_requested', True)
    
    def get_config(self) -> Dict[str, Any]:
        """Get comprehensive model configuration."""
        config = self.config.copy()
        
        # Add runtime information
        config.setdefault('runtime', {}).update({
            'model_initialized_at': self.initialization_timestamp,
            'total_parameters': sum(p.numel() for p in self.parameters()),
            'trainable_parameters': sum(p.numel() for p in self.parameters() if p.requires_grad),
            'device': str(self.device),
            'mixed_precision_active': self.mixed_precision,
            'mixed_precision_requested': self.original_mixed_precision_setting,
            'preset_used': self.preset_name,
            'input_dim': self.input_dim,
            'architecture_summary': f"{self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            'centralized_config_used': True,
            'helper_functions_leveraged': True,
            'enhanced_features': {
                'attention': self.use_attention and hasattr(self, 'attention') and self.attention is not None,
                'residual_blocks': self.residual_blocks,
                'skip_connections': self.skip_connection and hasattr(self, 'skip_layers'),
                'legacy_mode': self.legacy_mode
            }
        })
        
        return config
    
    def update_config(self, new_config: Dict[str, Any], reinitialize: bool = False) -> None:
        """Update configuration and optionally reinitialize components."""
        old_config = self.config.copy()
        
        try:
            # Update configuration using existing helper
            self.config = deep_update(self.config, new_config)
            
            # Reinitialize components if requested
            if reinitialize:
                logger.debug("Reinitializing model components due to config update")
                
                # Check if architecture changed
                architecture_changed = any(
                    key in new_config.get('model', {}) 
                    for key in ['encoding_dim', 'hidden_dims', 'dropout_rates', 'use_attention', 'residual_blocks']
                )
                
                if architecture_changed:
                    logger.warning("Architecture parameters changed - full model rebuild required")
                    raise ValueError("Architecture changes require creating a new model instance")
                
                # Update device configuration if changed
                if 'hardware' in new_config:
                    old_device = self.device
                    device_setting = new_config['hardware'].get('device')
                    if device_setting:
                        if device_setting == 'auto':
                            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                        elif device_setting in ['cpu', 'cuda']:
                            self.device = torch.device(device_setting)
                        
                        if self.device != old_device:
                            self.to(self.device)
                            logger.debug(f"Device changed from {old_device} to {self.device}")
            
            logger.debug("Configuration updated successfully using centralized helper")
            
        except Exception as e:
            # Restore old configuration on error
            self.config = old_config
            logger.error(f"Configuration update failed, restored previous config: {e}")
            raise
    
    def save_model(self, path: str, include_config: bool = True) -> None:
        """Save model state and configuration."""
        save_dict = {
            'model_state_dict': self.state_dict(),
            'model_class': self.__class__.__name__,
            'input_dim': self.input_dim,
            'architecture': {
                'encoding_dim': self.encoding_dim,
                'hidden_dims': self.hidden_dims,
                'dropout_rates': self.dropout_rates,
                'use_attention': self.use_attention,
                'residual_blocks': self.residual_blocks,
                'skip_connection': self.skip_connection,
                'legacy_mode': self.legacy_mode
            },
            'centralized_config_used': True
        }
        
        if include_config:
            save_dict['config'] = self.config
        
        torch.save(save_dict, path)
        logger.debug(f"EnhancedAutoencoder saved to {path} (centralized config)")
    
    @classmethod
    def load_model(cls, path: str, **kwargs):
        """Load model from saved state."""
        checkpoint = torch.load(path, map_location='cpu')
        
        # Extract architecture parameters
        input_dim = checkpoint['input_dim']
        config = checkpoint.get('config', {})
        
        # Create model instance using centralized config if available
        if checkpoint.get('centralized_config_used', False):
            try:
                model = create_model_instance('EnhancedAutoencoder', input_dim, config, **kwargs)
            except:
                # Fallback to direct instantiation
                model = cls(input_dim=input_dim, config=config, **kwargs)
        else:
            model = cls(input_dim=input_dim, config=config, **kwargs)
        
        # Load state dict
        model.load_state_dict(checkpoint['model_state_dict'])
        
        logger.debug(f"EnhancedAutoencoder loaded from {path}")
        return model
    
    def get_model_summary(self) -> str:
        """Get detailed model summary."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        # Enhanced features summary
        enhanced_features = []
        if self.use_attention and hasattr(self, 'attention') and self.attention is not None:
            enhanced_features.append("Multi-Head Attention")
        if self.residual_blocks:
            enhanced_features.append("Residual Blocks")
        if self.skip_connection and hasattr(self, 'skip_layers'):
            enhanced_features.append("Skip Connections")
        if self.normalization:
            enhanced_features.append(f"{self.normalization.title()} Normalization")
        
        summary = [
            f"EnhancedAutoencoder Summary (Factory Pattern)",
            f"{'='*65}",
            f"Model Type: {self.model_type}",
            f"Preset: {self.preset_name or 'Custom'}",
            f"Architecture: {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            f"Activation: {self.activation}",
            f"Enhanced Features: {', '.join(enhanced_features) if enhanced_features else 'None'}",
            f"Legacy Mode: {self.legacy_mode}",
            f"Device: {self.device}",
            f"Mixed Precision: {self.mixed_precision}",
            f"Parameters: {total_params:,} total, {trainable_params:,} trainable",
            f"Memory Usage: ~{total_params * 4 / (1024**2):.1f} MB (FP32)",
            f"Factory Pattern: Enabled",
            f"Centralized Config: Enabled",
            f"Helper Functions: Leveraged",
            f"{'='*65}"
        ]
        
        return '\n'.join(summary)
    
    def __repr__(self) -> str:
        """String representation of the model."""
        enhanced_features = []
        if self.use_attention:
            enhanced_features.append("attention")
        if self.residual_blocks:
            enhanced_features.append("residual")
        if self.skip_connection:
            enhanced_features.append("skip")
        
        features_str = f", features=[{','.join(enhanced_features)}]" if enhanced_features else ""
        
        return (f"EnhancedAutoencoder(input_dim={self.input_dim}, "
                f"encoding_dim={self.encoding_dim}, "
                f"hidden_dims={self.hidden_dims}, "
                f"device={self.device}, "
                f"preset='{self.preset_name or 'Custom'}', "
                f"factory_pattern=True{features_str}, "
                f"centralized_config=True)")

class AutoencoderEnsemble(nn.Module):
    """
    Streamlined AutoencoderEnsemble using centralized configuration initialization.
    
    This class leverages the _initialize_autoencoder_config() helper function to eliminate
    redundancy and provide consistent parameter processing across all autoencoder variants.
    The ensemble creates diverse autoencoder models for improved robustness and performance
    while maintaining full compatibility with all preset parameters.
    """
    
    def __init__(
        self,
        # Core parameter - always required
        input_dim: Optional[int] = None,
        
        # Configuration override - can contain any/all preset parameters
        config: Optional[Dict[str, Any]] = None,
        preset: Optional[str] = None,
        
        # ALL preset parameters accepted for backward compatibility
        # Core Model Architecture Parameters
        encoding_dim: Optional[int] = None,
        hidden_dims: Optional[List[int]] = None,
        dropout_rates: Optional[List[float]] = None,
        activation: Optional[str] = None,
        activation_param: Optional[float] = None,
        normalization: Optional[str] = None,
        use_batch_norm: Optional[bool] = None,
        use_layer_norm: Optional[bool] = None,
        bias: Optional[bool] = None,
        weight_init: Optional[str] = None,
        skip_connection: Optional[bool] = None,
        residual_blocks: Optional[bool] = None,
        use_attention: Optional[bool] = None,
        
        # Model Type and Variants
        model_type: Optional[str] = None,
        model_types: Optional[List[str]] = None,
        available_activations: Optional[List[str]] = None,
        available_normalizations: Optional[List[str]] = None,
        available_initializers: Optional[List[str]] = None,
        legacy_mode: Optional[bool] = None,
        
        # Ensemble Parameters
        diversity_factor: Optional[float] = None,
        min_features: Optional[int] = None,
        num_models: Optional[int] = None,
        
        # Training Parameters
        batch_size: Optional[int] = None,
        epochs: Optional[int] = None,
        learning_rate: Optional[float] = None,
        patience: Optional[int] = None,
        weight_decay: Optional[float] = None,
        gradient_clip: Optional[float] = None,
        gradient_accumulation_steps: Optional[int] = None,
        mixed_precision: Optional[bool] = None,
        num_workers: Optional[int] = None,
        optimizer: Optional[str] = None,
        scheduler: Optional[str] = None,
        scheduler_params: Optional[Dict[str, Any]] = None,
        early_stopping: Optional[bool] = None,
        validation_split: Optional[float] = None,
        shuffle: Optional[bool] = None,
        pin_memory: Optional[bool] = None,
        persistent_workers: Optional[bool] = None,
        adam_betas: Optional[Tuple[float, float]] = None,
        adam_eps: Optional[float] = None,
        lr_patience: Optional[int] = None,
        lr_factor: Optional[float] = None,
        min_lr: Optional[float] = None,
        
        # Data Parameters
        normal_samples: Optional[int] = None,
        attack_samples: Optional[int] = None,
        features: Optional[int] = None,
        use_real_data: Optional[bool] = None,
        data_normalization: Optional[str] = None,
        anomaly_factor: Optional[float] = None,
        random_state: Optional[int] = None,
        test_split: Optional[float] = None,
        stratified_split: Optional[bool] = None,
        data_path: Optional[str] = None,
        artifacts_path: Optional[str] = None,
        synthetic_generation: Optional[Dict[str, Any]] = None,
        preprocessing: Optional[Dict[str, Any]] = None,
        
        # Security Parameters
        percentile: Optional[float] = None,
        attack_threshold: Optional[float] = None,
        false_negative_cost: Optional[float] = None,
        enable_security_metrics: Optional[bool] = None,
        anomaly_threshold_strategy: Optional[str] = None,
        early_warning_threshold: Optional[float] = None,
        adaptive_threshold: Optional[bool] = None,
        confidence_interval: Optional[float] = None,
        detection_methods: Optional[List[str]] = None,
        alert_levels: Optional[List[str]] = None,
        threshold_validation: Optional[bool] = None,
        robust_detection: Optional[bool] = None,
        false_positive_tolerance: Optional[float] = None,
        performance_optimized_detection: Optional[bool] = None,
        real_time_monitoring: Optional[bool] = None,
        ensemble_voting: Optional[str] = None,
        uncertainty_threshold: Optional[float] = None,
        
        # Monitoring Parameters
        metrics_frequency: Optional[int] = None,
        checkpoint_frequency: Optional[int] = None,
        tensorboard_logging: Optional[bool] = None,
        console_logging_level: Optional[str] = None,
        save_best_model: Optional[bool] = None,
        save_model_history: Optional[bool] = None,
        metrics_to_track: Optional[List[str]] = None,
        early_stopping_metric: Optional[str] = None,
        checkpoint_format: Optional[str] = None,
        log_model_summary: Optional[bool] = None,
        tensorboard_dir: Optional[str] = None,
        log_frequency: Optional[int] = None,
        save_checkpoints: Optional[bool] = None,
        tensorboard: Optional[Dict[str, Any]] = None,
        stability_metrics: Optional[bool] = None,
        performance_metrics: Optional[bool] = None,
        profiling_enabled: Optional[bool] = None,
        
        # Hardware Parameters
        device: Optional[str] = None,
        recommended_gpu_memory: Optional[float] = None,
        minimum_system_requirements: Optional[Dict[str, Any]] = None,
        optimal_system_requirements: Optional[Dict[str, Any]] = None,
        memory_management: Optional[Dict[str, Any]] = None,
        performance_optimization: Optional[Dict[str, Any]] = None,
        detected_gpu_memory: Optional[float] = None,
        detected_system_memory: Optional[float] = None,
        system_performance_class: Optional[str] = None,
        optimization_recommendations: Optional[List[str]] = None,
        
        # System Parameters
        model_dir: Optional[str] = None,
        log_dir: Optional[str] = None,
        config_dir: Optional[str] = None,
        data_dir: Optional[str] = None,
        checkpoint_dir: Optional[str] = None,
        results_dir: Optional[str] = None,
        debug: Optional[bool] = None,
        verbose: Optional[bool] = None,
        random_seed: Optional[int] = None,
        reproducible: Optional[bool] = None,
        parallel_processing: Optional[bool] = None,
        max_workers: Optional[int] = None,
        export_onnx: Optional[bool] = None,
        non_interactive: Optional[bool] = None,
        cuda_optimizations: Optional[bool] = None,
        onnx_export: Optional[Dict[str, Any]] = None,
        distributed_training: Optional[bool] = None,
        python_executable: Optional[str] = None,
        working_directory: Optional[str] = None,
        environment_health: Optional[str] = None,
        
        # Preset Parameters
        available_presets: Optional[List[str]] = None,
        current_preset: Optional[str] = None,
        current_override: Optional[str] = None,
        override_rules: Optional[Dict[str, bool]] = None,
        preset_configs: Optional[Dict[str, str]] = None,
        custom_presets_available: Optional[List[str]] = None,
        auto_apply: Optional[bool] = None,
        validate_compatibility: Optional[bool] = None,
        system_recommended_preset: Optional[str] = None,
        preset_compatibility: Optional[Dict[str, Any]] = None,
        
        # Hyperparameter Optimization Parameters
        hpo_enabled: Optional[bool] = None,
        hpo_strategy: Optional[str] = None,
        study_name: Optional[str] = None,
        direction: Optional[str] = None,
        n_trials: Optional[int] = None,
        timeout: Optional[int] = None,
        sampler: Optional[str] = None,
        pruner: Optional[str] = None,
        objective_metric: Optional[str] = None,
        optimization_space: Optional[Dict[str, Any]] = None,
        hpo_early_stopping: Optional[Dict[str, Any]] = None,
        timeout_seconds: Optional[int] = None,
        trial_epochs: Optional[int] = None,
        trial_patience: Optional[int] = None,
        cleanup_trials: Optional[bool] = None,
        generate_plots: Optional[bool] = None,
        search_space: Optional[Dict[str, Any]] = None,
        hpo_sampler: Optional[Dict[str, Any]] = None,
        hpo_pruner: Optional[Dict[str, Any]] = None,
        scoring: Optional[Dict[str, Any]] = None,
        storage: Optional[Dict[str, Any]] = None,
        
        # Validation Parameters
        cross_validation: Optional[Dict[str, Any]] = None,
        metrics: Optional[List[str]] = None,
        validation_frequency: Optional[int] = None,
        save_validation_results: Optional[bool] = None,
        detailed_metrics: Optional[bool] = None,
        robustness_testing: Optional[bool] = None,
        performance_benchmarking: Optional[bool] = None,
        confidence_intervals: Optional[bool] = None,
        
        # Experimental Parameters
        experimental_features: Optional[Dict[str, bool]] = None,
        experimental_settings: Optional[Dict[str, bool]] = None,
        
        # Metadata Parameters
        description: Optional[str] = None,
        version: Optional[str] = None,
        config_version: Optional[str] = None,
        config_type: Optional[str] = None,
        created: Optional[str] = None,
        last_modified: Optional[str] = None,
        preset_used: Optional[str] = None,
        recommended_hardware: Optional[Dict[str, Any]] = None,
        compatibility: Optional[List[str]] = None,
        system_info: Optional[Dict[str, Any]] = None,
        validation_info: Optional[Dict[str, Any]] = None,
        
        # Runtime Parameters
        config_loaded_at: Optional[str] = None,
        config_source: Optional[str] = None,
        runtime_id: Optional[str] = None,
        process_id: Optional[int] = None,
        system_analysis_completed: Optional[bool] = None,
        system_performance_score: Optional[float] = None,
        system_class: Optional[str] = None,
        optimizations_applied: Optional[Dict[str, bool]] = None,
        resource_status: Optional[Dict[str, bool]] = None,
        system_warnings: Optional[List[str]] = None,
        recommendations: Optional[List[str]] = None,
        configuration_health: Optional[Dict[str, Any]] = None,
        
        **kwargs  # Catch any additional parameters
    ):
        super(AutoencoderEnsemble, self).__init__()
        
        # Use centralized initialization helper
        init_data = _initialize_autoencoder_config(
            model_class_name='AutoencoderEnsemble',
            input_dim=input_dim,
            config=config,
            preset=preset,
            # Pass through all parameters
            encoding_dim=encoding_dim,
            hidden_dims=hidden_dims,
            dropout_rates=dropout_rates,
            activation=activation,
            activation_param=activation_param,
            normalization=normalization,
            use_batch_norm=use_batch_norm,
            use_layer_norm=use_layer_norm,
            bias=bias,
            weight_init=weight_init,
            skip_connection=skip_connection,
            residual_blocks=residual_blocks,
            use_attention=use_attention,
            model_type=model_type,
            model_types=model_types,
            available_activations=available_activations,
            available_normalizations=available_normalizations,
            available_initializers=available_initializers,
            legacy_mode=legacy_mode,
            diversity_factor=diversity_factor,
            min_features=min_features,
            num_models=num_models,
            batch_size=batch_size,
            epochs=epochs,
            learning_rate=learning_rate,
            patience=patience,
            weight_decay=weight_decay,
            gradient_clip=gradient_clip,
            gradient_accumulation_steps=gradient_accumulation_steps,
            mixed_precision=mixed_precision,
            num_workers=num_workers,
            optimizer=optimizer,
            scheduler=scheduler,
            scheduler_params=scheduler_params,
            early_stopping=early_stopping,
            validation_split=validation_split,
            shuffle=shuffle,
            pin_memory=pin_memory,
            persistent_workers=persistent_workers,
            adam_betas=adam_betas,
            adam_eps=adam_eps,
            lr_patience=lr_patience,
            lr_factor=lr_factor,
            min_lr=min_lr,
            normal_samples=normal_samples,
            attack_samples=attack_samples,
            features=features,
            use_real_data=use_real_data,
            data_normalization=data_normalization,
            anomaly_factor=anomaly_factor,
            random_state=random_state,
            test_split=test_split,
            stratified_split=stratified_split,
            data_path=data_path,
            artifacts_path=artifacts_path,
            synthetic_generation=synthetic_generation,
            preprocessing=preprocessing,
            percentile=percentile,
            attack_threshold=attack_threshold,
            false_negative_cost=false_negative_cost,
            enable_security_metrics=enable_security_metrics,
            anomaly_threshold_strategy=anomaly_threshold_strategy,
            early_warning_threshold=early_warning_threshold,
            adaptive_threshold=adaptive_threshold,
            confidence_interval=confidence_interval,
            detection_methods=detection_methods,
            alert_levels=alert_levels,
            threshold_validation=threshold_validation,
            robust_detection=robust_detection,
            false_positive_tolerance=false_positive_tolerance,
            performance_optimized_detection=performance_optimized_detection,
            real_time_monitoring=real_time_monitoring,
            ensemble_voting=ensemble_voting,
            uncertainty_threshold=uncertainty_threshold,
            metrics_frequency=metrics_frequency,
            checkpoint_frequency=checkpoint_frequency,
            tensorboard_logging=tensorboard_logging,
            console_logging_level=console_logging_level,
            save_best_model=save_best_model,
            save_model_history=save_model_history,
            metrics_to_track=metrics_to_track,
            early_stopping_metric=early_stopping_metric,
            checkpoint_format=checkpoint_format,
            log_model_summary=log_model_summary,
            tensorboard_dir=tensorboard_dir,
            log_frequency=log_frequency,
            save_checkpoints=save_checkpoints,
            tensorboard=tensorboard,
            stability_metrics=stability_metrics,
            performance_metrics=performance_metrics,
            profiling_enabled=profiling_enabled,
            device=device,
            recommended_gpu_memory=recommended_gpu_memory,
            minimum_system_requirements=minimum_system_requirements,
            optimal_system_requirements=optimal_system_requirements,
            memory_management=memory_management,
            performance_optimization=performance_optimization,
            detected_gpu_memory=detected_gpu_memory,
            detected_system_memory=detected_system_memory,
            system_performance_class=system_performance_class,
            optimization_recommendations=optimization_recommendations,
            model_dir=model_dir,
            log_dir=log_dir,
            config_dir=config_dir,
            data_dir=data_dir,
            checkpoint_dir=checkpoint_dir,
            results_dir=results_dir,
            debug=debug,
            verbose=verbose,
            random_seed=random_seed,
            reproducible=reproducible,
            parallel_processing=parallel_processing,
            max_workers=max_workers,
            export_onnx=export_onnx,
            non_interactive=non_interactive,
            cuda_optimizations=cuda_optimizations,
            onnx_export=onnx_export,
            distributed_training=distributed_training,
            python_executable=python_executable,
            working_directory=working_directory,
            environment_health=environment_health,
            available_presets=available_presets,
            current_preset=current_preset,
            current_override=current_override,
            override_rules=override_rules,
            preset_configs=preset_configs,
            custom_presets_available=custom_presets_available,
            auto_apply=auto_apply,
            validate_compatibility=validate_compatibility,
            system_recommended_preset=system_recommended_preset,
            preset_compatibility=preset_compatibility,
            hpo_enabled=hpo_enabled,
            hpo_strategy=hpo_strategy,
            study_name=study_name,
            direction=direction,
            n_trials=n_trials,
            timeout=timeout,
            sampler=sampler,
            pruner=pruner,
            objective_metric=objective_metric,
            optimization_space=optimization_space,
            hpo_early_stopping=hpo_early_stopping,
            timeout_seconds=timeout_seconds,
            trial_epochs=trial_epochs,
            trial_patience=trial_patience,
            cleanup_trials=cleanup_trials,
            generate_plots=generate_plots,
            search_space=search_space,
            hpo_sampler=hpo_sampler,
            hpo_pruner=hpo_pruner,
            scoring=scoring,
            storage=storage,
            cross_validation=cross_validation,
            metrics=metrics,
            validation_frequency=validation_frequency,
            save_validation_results=save_validation_results,
            detailed_metrics=detailed_metrics,
            robustness_testing=robustness_testing,
            performance_benchmarking=performance_benchmarking,
            confidence_intervals=confidence_intervals,
            experimental_features=experimental_features,
            experimental_settings=experimental_settings,
            description=description,
            version=version,
            config_version=config_version,
            config_type=config_type,
            created=created,
            last_modified=last_modified,
            preset_used=preset_used,
            recommended_hardware=recommended_hardware,
            compatibility=compatibility,
            system_info=system_info,
            validation_info=validation_info,
            config_loaded_at=config_loaded_at,
            config_source=config_source,
            runtime_id=runtime_id,
            process_id=process_id,
            system_analysis_completed=system_analysis_completed,
            system_performance_score=system_performance_score,
            system_class=system_class,
            optimizations_applied=optimizations_applied,
            resource_status=resource_status,
            system_warnings=system_warnings,
            recommendations=recommendations,
            configuration_health=configuration_health,
            **kwargs
        )
        
        # Extract processed data from centralized initialization
        self.config = init_data['config']
        params = init_data['processed_params']
        self.device = init_data['device']
        self.mixed_precision = init_data['mixed_precision']
        self.mixed_precision_requested = init_data['mixed_precision_requested']
        self.initialization_timestamp = init_data['initialization_timestamp']
        #self.preset_name = init_data['preset_name']
        # PROPERLY STORE PRESET NAME WITH FALLBACKS
        self.preset_name = init_data.get('preset_name') or preset
        #self.preset_name = init_data['preset_name'] or preset
        
        # If still no preset name, try to extract from config
        if not self.preset_name:
            self.preset_name = (
                self.config.get('metadata', {}).get('preset_used') or
                self.config.get('presets', {}).get('current_preset') or
                self.config.get('runtime', {}).get('applied_preset') or
                "Custom"
            )
        
        # ENSURE PRESET NAME IS STORED IN CONFIG FOR CONSISTENCY
        if self.preset_name and self.preset_name != "Custom":
            if 'metadata' not in self.config:
                self.config['metadata'] = {}
            self.config['metadata']['preset_used'] = self.preset_name
            
            if 'presets' not in self.config:
                self.config['presets'] = {}
            self.config['presets']['current_preset'] = self.preset_name
        
        training_config = init_data['training_config']
        
        # Set instance attributes from processed parameters
        self.input_dim = params['input_dim']
        self.encoding_dim = params['encoding_dim']
        self.hidden_dims = params['hidden_dims']
        self.dropout_rates = params['dropout_rates']
        self.activation = params['activation']
        self.activation_param = params['activation_param']
        self.normalization = params['normalization']
        self.use_batch_norm = params['use_batch_norm']
        self.use_layer_norm = params['use_layer_norm']
        self.bias = params['bias']
        self.weight_init = params['weight_init']
        self.skip_connection = params['skip_connection']
        self.residual_blocks = params['residual_blocks']
        self.use_attention = params['use_attention']
        self.model_type = params['model_type']
        self.min_features = params['min_features']
        self.legacy_mode = params['legacy_mode']
        self.num_models = params['num_models']
        self.diversity_factor = params['diversity_factor']
        
        # Build ensemble of diverse autoencoder models
        self._build_ensemble()
        
        # Setup training components if specified
        self._setup_training_components()
        
        # Setup monitoring and logging
        self._setup_monitoring()
        
        # Move to device
        self.to(self.device)
        
        # Log successful initialization
        if hasattr(globals().get('display_model_initialization_summary'), '__call__'):
            try:
                architecture_info = {
                    'hidden_dims': self.hidden_dims,
                    'encoding_dim': self.encoding_dim,
                    'dropout_rates': self.dropout_rates,
                    'activation': self.activation,
                    'normalization': self.normalization
                }
                
                ensemble_info = {
                    'num_models': self.num_models,
                    'model_types': [type(model).__name__ for model in self.models] if hasattr(self, 'models') else [],
                    'diversity_factor': self.diversity_factor,
                    'actual_models': len(self.models) if hasattr(self, 'models') else 0,
                    'use_attention': self.use_attention,
                    'residual_blocks': self.residual_blocks,
                    'skip_connections': self.skip_connection,
                    'legacy_mode': self.legacy_mode
                }
                
                display_model_initialization_summary(
                    model_instance=self,
                    model_type='AutoencoderEnsemble',
                    input_dim=self.input_dim,
                    architecture_info=architecture_info,
                    device=self.device,
                    mixed_precision=self.mixed_precision,
                    preset_name=self.preset_name,
                    training_config=training_config,
                    ensemble_info=ensemble_info
                )
            except Exception as e:
                logger.debug(f"Could not display initialization summary: {e}")
    
    def _build_ensemble(self) -> None:
        """Build the ensemble of diverse autoencoder models using centralized factory pattern."""
        try:
            self.models = nn.ModuleList()
            successful_models = 0
            
            # Prepare base configuration for ensemble members
            base_config = self.config.copy()
            
            for i in range(self.num_models):
                try:
                    # Calculate diversity parameters
                    diversity_offset = (i - self.num_models // 2) * self.diversity_factor
                    encoding_dim_variant = max(8, int(self.encoding_dim * (1 + diversity_offset * 0.5)))
                    
                    # Create diverse hidden layer configurations
                    hidden_diversity = 1 + diversity_offset * 0.3
                    hidden_dims_variant = [max(16, int(dim * hidden_diversity)) for dim in self.hidden_dims]
                    
                    # Adjust dropout rates with diversity
                    dropout_diversity = max(0.05, min(0.4, diversity_offset * 0.1))
                    dropout_rates_variant = [max(0.05, min(0.5, rate + dropout_diversity)) for rate in self.dropout_rates]
                    
                    # Different architectures for diversity
                    if i % 3 == 0:
                        # Enhanced architecture
                        model_type_variant = 'EnhancedAutoencoder'
                        model_config = base_config.copy()
                    elif i % 3 == 1:
                        # Enhanced with different features
                        model_type_variant = 'EnhancedAutoencoder'
                        model_config = base_config.copy()
                        model_config.setdefault('model', {}).update({
                            'use_attention': not self.legacy_mode and (i % 2 == 0),
                            'residual_blocks': not self.legacy_mode and (i % 2 == 1),
                            'skip_connection': i % 2 == 0,
                            'normalization': 'batch' if i % 2 == 0 else 'layer'
                        })
                    else:
                        # Simple architecture for diversity
                        model_type_variant = 'SimpleAutoencoder'
                        model_config = base_config.copy()
                        model_config.setdefault('model', {}).update({
                            'use_attention': False,
                            'residual_blocks': False
                        })
                    
                    # Update model configuration with variants
                    model_config['model'].update({
                        'encoding_dim': encoding_dim_variant,
                        'hidden_dims': hidden_dims_variant,
                        'dropout_rates': dropout_rates_variant,
                        'activation_param': max(0.1, self.activation_param + diversity_offset * 0.1),
                        'model_type': model_type_variant
                    })
                    
                    # Create ensemble member using factory pattern
                    try:
                        model = create_model_instance(
                            model_type=model_type_variant,
                            input_dim=self.input_dim,
                            config=model_config
                        )
                    except Exception as factory_error:
                        # Fallback to direct instantiation if factory fails
                        logger.debug(f"Factory creation failed for model {i}, using direct instantiation: {factory_error}")
                        if model_type_variant == 'EnhancedAutoencoder':
                            model = EnhancedAutoencoder(input_dim=self.input_dim, config=model_config)
                        else:
                            model = SimpleAutoencoder(input_dim=self.input_dim, config=model_config)
                    
                    # Move to device
                    model.to(self.device)
                    
                    self.models.append(model)
                    successful_models += 1
                    
                    logger.debug(f"Ensemble model {i} ({model_type_variant}) created successfully: "
                                f"encoding_dim={encoding_dim_variant}, hidden_dims={hidden_dims_variant}")
                    
                except Exception as e:
                    logger.error(f"Failed to create ensemble model {i}: {e}")
                    
                    # Try to create a simple fallback model
                    try:
                        fallback_config = {
                            'model': {
                                'encoding_dim': max(4, self.encoding_dim // 2),
                                'hidden_dims': [max(32, self.hidden_dims[0] // 2)],
                                'dropout_rates': [0.2],
                                'model_type': 'SimpleAutoencoder',
                                'use_attention': False,
                                'residual_blocks': False,
                                'skip_connection': False,
                                'legacy_mode': True
                            },
                            'training': {'mixed_precision': self.mixed_precision},
                            'hardware': {'device': str(self.device)},
                            'system': self.config.get('system', {})
                        }
                        
                        fallback_model = SimpleAutoencoder(
                            input_dim=self.input_dim,
                            config=fallback_config
                        )
                        fallback_model.to(self.device)
                        
                        self.models.append(fallback_model)
                        successful_models += 1
                        logger.warning(f"Created fallback SimpleAutoencoder for ensemble position {i}")
                        
                    except Exception as fallback_error:
                        logger.error(f"Failed to create fallback model for position {i}: {fallback_error}")
            
            # Validate that at least one model was created successfully
            if successful_models == 0:
                raise RuntimeError("Failed to create any ensemble models")
            
            if successful_models < self.num_models:
                logger.warning(f"Only {successful_models} out of {self.num_models} ensemble models created successfully")
                self.num_models = successful_models  # Update to reflect actual count
            
        except Exception as e:
            logger.error(f"Failed to build ensemble: {e}")
            raise RuntimeError(f"Ensemble construction failed: {e}")
    
    def _setup_training_components(self) -> None:
        """Setup training components for ensemble members."""
        training_config = self.config.get('training', {})
        
        if not training_config or not hasattr(self, 'models'):
            return
        
        # Setup ensemble-level training components
        all_parameters = []
        for model in self.models:
            all_parameters.extend(list(model.parameters()))
        
        if not all_parameters:
            logger.warning("No parameters found in ensemble models")
            return
        
        # Setup optimizer for entire ensemble
        optimizer_type = training_config.get('optimizer', 'AdamW').lower()
        learning_rate = training_config.get('learning_rate', 0.001)
        weight_decay = training_config.get('weight_decay', 1e-4)
        
        try:
            if optimizer_type == 'adam':
                adam_betas = training_config.get('adam_betas', (0.9, 0.999))
                adam_eps = training_config.get('adam_eps', 1e-8)
                self.optimizer = torch.optim.Adam(
                    all_parameters,
                    lr=learning_rate,
                    betas=adam_betas,
                    eps=adam_eps,
                    weight_decay=weight_decay
                )
            elif optimizer_type == 'adamw':
                adam_betas = training_config.get('adam_betas', (0.9, 0.999))
                adam_eps = training_config.get('adam_eps', 1e-8)
                self.optimizer = torch.optim.AdamW(
                    all_parameters,
                    lr=learning_rate,
                    betas=adam_betas,
                    eps=adam_eps,
                    weight_decay=weight_decay
                )
            elif optimizer_type == 'sgd':
                self.optimizer = torch.optim.SGD(
                    all_parameters,
                    lr=learning_rate,
                    momentum=0.9,
                    weight_decay=weight_decay
                )
            else:
                logger.warning(f"Unknown optimizer '{optimizer_type}', using AdamW")
                self.optimizer = torch.optim.AdamW(all_parameters, lr=learning_rate, weight_decay=weight_decay)
        except Exception as e:
            logger.error(f"Failed to setup ensemble optimizer: {e}")
            self.optimizer = None
        
        # Setup scheduler
        scheduler_type = training_config.get('scheduler')
        scheduler_params = training_config.get('scheduler_params', {})
        
        if scheduler_type and hasattr(self, 'optimizer') and self.optimizer:
            try:
                if scheduler_type == 'ReduceLROnPlateau':
                    self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                        self.optimizer,
                        mode=scheduler_params.get('mode', 'min'),
                        factor=scheduler_params.get('factor', 0.5),
                        patience=scheduler_params.get('patience', 10),
                        min_lr=scheduler_params.get('min_lr', 1e-6)
                    )
                elif scheduler_type == 'StepLR':
                    self.scheduler = torch.optim.lr_scheduler.StepLR(
                        self.optimizer,
                        step_size=scheduler_params.get('step_size', 30),
                        gamma=scheduler_params.get('gamma', 0.1)
                    )
                else:
                    logger.warning(f"Scheduler '{scheduler_type}' not implemented for ensemble")
                    self.scheduler = None
            except Exception as e:
                logger.error(f"Failed to setup ensemble scheduler: {e}")
                self.scheduler = None
        else:
            self.scheduler = None
        
        # Setup loss function
        self.loss_fn = nn.MSELoss()
        
        # Setup scaler for mixed precision
        if self.mixed_precision:
            #self.scaler = torch.cuda.amp.GradScaler()
            #self.scaler = torch.amp.GradScaler()
            device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
            self.scaler = torch.amp.GradScaler(device_type=device_type)
        
        logger.debug(f"Setup ensemble training components: {optimizer_type} optimizer, {scheduler_type} scheduler")
    
    def _setup_monitoring(self) -> None:
        """Setup monitoring and logging based on configuration."""
        monitoring_config = self.config.get('monitoring', {})
        
        if not monitoring_config:
            return
        
        # Setup metrics tracking
        self.metrics_to_track = monitoring_config.get('metrics_to_track', ['loss', 'reconstruction_error', 'ensemble_variance'])
        self.metrics_frequency = monitoring_config.get('metrics_frequency', 10)
        self.save_best_model = monitoring_config.get('save_best_model', True)
        
        # Training history for ensemble
        self.training_history = {metric: [] for metric in self.metrics_to_track}
        self.best_loss = float('inf')
        self.patience_counter = 0
        
        # Checkpoint settings
        self.checkpoint_frequency = monitoring_config.get('checkpoint_frequency', 10)
        self.save_checkpoints = monitoring_config.get('save_checkpoints', True)
        
        logger.debug(f"Setup ensemble monitoring: tracking {len(self.metrics_to_track)} metrics")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Enhanced forward pass with mixed precision support and robust error handling."""
        # Input validation
        if not isinstance(x, torch.Tensor):
            raise ValueError(f"Expected torch.Tensor input, got {type(x)}")
        
        if x.dim() != 2:
            raise ValueError(f"Expected 2D input tensor (batch_size, input_dim), got shape {x.shape}")
        
        if x.size(-1) != self.input_dim:
            raise ValueError(f"Input feature dimension {x.size(-1)} doesn't match expected {self.input_dim}")
        
        try:
            #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
            device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
            with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
                reconstructions = []
                successful_outputs = 0
                
                for i, model in enumerate(self.models):
                    try:
                        output = model(x)
                        reconstructions.append(output)
                        successful_outputs += 1
                    except Exception as e:
                        logger.warning(f"Ensemble model {i} failed during forward pass: {e}")
                        continue
                
                if successful_outputs == 0:
                    raise RuntimeError("All ensemble models failed during forward pass")
                
                if successful_outputs < len(self.models):
                    logger.debug(f"Only {successful_outputs} out of {len(self.models)} ensemble models "
                                f"produced outputs")
                
                # Average the successful reconstructions
                ensemble_output = torch.stack(reconstructions).mean(dim=0)
                return ensemble_output
                
        except Exception as e:
            logger.error(f"AutoencoderEnsemble forward pass failed: {e}")
            raise RuntimeError(f"Ensemble forward pass error: {e}")
    
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode input data using ensemble average of latent representations."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            encodings = []
            for model in self.models:
                try:
                    encoding = model.encode(x)
                    encodings.append(encoding)
                except Exception as e:
                    logger.warning(f"Model encoding failed: {e}")
                    continue
            
            if not encodings:
                raise RuntimeError("All ensemble models failed during encoding")
            
            return torch.stack(encodings).mean(dim=0)
    
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode latent representation using ensemble average."""
        #with torch.cuda.amp.autocast(enabled=self.mixed_precision):
        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'
        with torch.amp.autocast(device_type=device_type, enabled=self.mixed_precision):
            decodings = []
            for model in self.models:
                try:
                    decoding = model.decode(z)
                    decodings.append(decoding)
                except Exception as e:
                    logger.warning(f"Model decoding failed: {e}")
                    continue
            
            if not decodings:
                raise RuntimeError("All ensemble models failed during decoding")
            
            return torch.stack(decodings).mean(dim=0)
    
    @property
    def original_mixed_precision_setting(self) -> bool:
        """Returns the originally requested mixed precision setting."""
        return getattr(self, 'mixed_precision_requested', True)
    
    def get_config(self) -> Dict[str, Any]:
        """Get comprehensive ensemble configuration."""
        config = self.config.copy()
        
        # Get individual model configs
        model_configs = []
        for i, model in enumerate(self.models):
            try:
                model_configs.append(model.get_config())
            except Exception as e:
                logger.warning(f"Failed to get config for ensemble model {i}: {e}")
                model_configs.append({"error": str(e), "model_index": i})
        
        # Add runtime information
        config.setdefault('runtime', {}).update({
            'model_initialized_at': self.initialization_timestamp,
            'total_parameters': sum(p.numel() for p in self.parameters()),
            'trainable_parameters': sum(p.numel() for p in self.parameters() if p.requires_grad),
            'device': str(self.device),
            'mixed_precision_active': self.mixed_precision,
            'mixed_precision_requested': self.original_mixed_precision_setting,
            'preset_used': self.preset_name,
            'input_dim': self.input_dim,
            'ensemble_size': self.num_models,
            'actual_models': len(self.models),
            'diversity_factor': self.diversity_factor,
            'model_types': [type(model).__name__ for model in self.models],
            'architecture_summary': f"Ensemble({self.num_models}): {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            'centralized_config_used': True,
            'helper_functions_leveraged': True,
            'ensemble_configs': model_configs
        })
        
        return config
    
    def update_config(self, new_config: Dict[str, Any], reinitialize: bool = False) -> None:
        """Update configuration and optionally reinitialize components."""
        old_config = self.config.copy()
        
        try:
            # Update configuration using existing helper
            self.config = deep_update(self.config, new_config)
            
            # Reinitialize components if requested
            if reinitialize:
                logger.info("Reinitializing ensemble components due to config update")
                
                # Check if ensemble architecture changed
                ensemble_changed = any(
                    key in new_config.get('model', {}) 
                    for key in ['num_models', 'diversity_factor', 'encoding_dim', 'hidden_dims']
                )
                
                if ensemble_changed:
                    logger.warning("Ensemble architecture parameters changed - full rebuild required")
                    raise ValueError("Ensemble architecture changes require creating a new instance")
                
                # Update training components
                if 'training' in new_config:
                    self._setup_training_components()
                
                # Update device configuration
                if 'hardware' in new_config:
                    old_device = self.device
                    device_setting = new_config['hardware'].get('device')
                    if device_setting:
                        if device_setting == 'auto':
                            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                        elif device_setting in ['cpu', 'cuda']:
                            self.device = torch.device(device_setting)
                        
                        if self.device != old_device:
                            # Move all models to new device
                            for model in self.models:
                                model.to(self.device)
                            logger.info(f"Moved ensemble from {old_device} to {self.device}")
                
                # Update individual model configs
                for model in self.models:
                    try:
                        model.update_config(new_config, reinitialize=False)
                    except Exception as e:
                        logger.warning(f"Failed to update model config: {e}")
            
            logger.debug("Configuration updated successfully using centralized helper")
            
        except Exception as e:
            # Restore old configuration on error
            self.config = old_config
            logger.error(f"Configuration update failed, restored previous config: {e}")
            raise
    
    def save_model(self, path: str, include_config: bool = True) -> None:
        """Save ensemble state and configuration."""
        save_dict = {
            'model_state_dict': self.state_dict(),
            'model_class': self.__class__.__name__,
            'input_dim': self.input_dim,
            'ensemble_info': {
                'num_models': self.num_models,
                'diversity_factor': self.diversity_factor,
                'encoding_dim': self.encoding_dim,
                'hidden_dims': self.hidden_dims,
                'dropout_rates': self.dropout_rates,
                'model_types': [type(model).__name__ for model in self.models],
                'use_attention': self.use_attention,
                'residual_blocks': self.residual_blocks,
                'skip_connection': self.skip_connection,
                'legacy_mode': self.legacy_mode
            },
            'centralized_config_used': True
        }
        
        if include_config:
            save_dict['config'] = self.config
        
        if hasattr(self, 'optimizer') and self.optimizer:
            save_dict['optimizer_state_dict'] = self.optimizer.state_dict()
        
        if hasattr(self, 'scheduler') and self.scheduler is not None:
            save_dict['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(save_dict, path)
        logger.debug(f"AutoencoderEnsemble saved to {path} (centralized config)")
    
    @classmethod
    def load_model(cls, path: str, **kwargs):
        """Load ensemble from saved state."""
        checkpoint = torch.load(path, map_location='cpu')
        
        # Extract parameters
        input_dim = checkpoint['input_dim']
        config = checkpoint.get('config', {})
        ensemble_info = checkpoint.get('ensemble_info', {})
        
        # Merge ensemble info into config
        if ensemble_info:
            config.setdefault('model', {}).update(ensemble_info)
        
        # Create ensemble instance using centralized config if available
        if checkpoint.get('centralized_config_used', False):
            try:
                ensemble = create_model_instance('AutoencoderEnsemble', input_dim, config, **kwargs)
            except:
                # Fallback to direct instantiation
                ensemble = cls(input_dim=input_dim, config=config, **kwargs)
        else:
            ensemble = cls(input_dim=input_dim, config=config, **kwargs)
        
        # Load state dict
        ensemble.load_state_dict(checkpoint['model_state_dict'])
        
        # Load optimizer and scheduler if available
        if hasattr(ensemble, 'optimizer') and 'optimizer_state_dict' in checkpoint:
            ensemble.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if hasattr(ensemble, 'scheduler') and ensemble.scheduler is not None and 'scheduler_state_dict' in checkpoint:
            ensemble.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        logger.debug(f"AutoencoderEnsemble loaded from {path}")
        return ensemble
    
    def get_model_summary(self) -> str:
        """Get detailed ensemble summary."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        # Model type distribution
        model_types = [type(model).__name__ for model in self.models]
        type_counts = {}
        for model_type in model_types:
            type_counts[model_type] = type_counts.get(model_type, 0) + 1
        
        # Enhanced features summary
        enhanced_features = []
        if self.use_attention:
            enhanced_features.append("Multi-Head Attention")
        if self.residual_blocks:
            enhanced_features.append("Residual Blocks")
        if self.skip_connection:
            enhanced_features.append("Skip Connections")
        if self.normalization:
            enhanced_features.append(f"{self.normalization.title()} Normalization")
        
        summary = [
            f"AutoencoderEnsemble Summary (Factory Pattern)",
            f"{'='*75}",
            f"Ensemble Size: {self.num_models} models",
            f"Model Distribution: {dict(type_counts)}",
            f"Preset: {self.preset_name or 'Custom'}",
            f"Base Architecture: {self.input_dim} -> {self.hidden_dims} -> {self.encoding_dim}",
            f"Diversity Factor: {self.diversity_factor}",
            f"Base Activation: {self.activation}",
            f"Enhanced Features: {', '.join(enhanced_features) if enhanced_features else 'None'}",
            f"Legacy Mode: {self.legacy_mode}",
            f"Device: {self.device}",
            f"Mixed Precision: {self.mixed_precision}",
            f"Parameters: {total_params:,} total, {trainable_params:,} trainable",
            f"Memory Usage: ~{total_params * 4 / (1024**2):.1f} MB (FP32)",
            f"Factory Pattern: Enabled",
            f"Centralized Config: Enabled",
            f"Helper Functions: Leveraged",
            f"{'='*75}"
        ]
        
        return '\n'.join(summary)
    
    def __repr__(self) -> str:
        """String representation of the ensemble."""
        model_types = [type(model).__name__ for model in self.models]
        type_counts = {}
        for model_type in model_types:
            type_counts[model_type] = type_counts.get(model_type, 0) + 1
        
        enhanced_features = []
        if self.use_attention:
            enhanced_features.append("attention")
        if self.residual_blocks:
            enhanced_features.append("residual")
        if self.skip_connection:
            enhanced_features.append("skip")
        
        features_str = f", features=[{','.join(enhanced_features)}]" if enhanced_features else ""
        
        return (f"AutoencoderEnsemble(input_dim={self.input_dim}, "
                f"num_models={self.num_models}, "
                f"encoding_dim={self.encoding_dim}, "
                f"diversity_factor={self.diversity_factor}, "
                f"model_types={dict(type_counts)}, "
                f"device={self.device}, "
                f"preset='{self.preset_name or 'Custom'}', "
                f"centralized_config=True, "
                f"factory_pattern=True{features_str})")

def load_autoencoder_model(
    model_path: Path,
    input_dim: Optional[int] = None,
    encoding_dim: Optional[int] = None,
    config: Optional[Dict] = None,
    model_type: Optional[str] = None,
    **kwargs
) -> Union[SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble]:
    """
    Load autoencoder with automatic architecture detection and comprehensive config handling.
    
    This function supports loading all three autoencoder types (SimpleAutoencoder,
    EnhancedAutoencoder, AutoencoderEnsemble) with full configuration compatibility.
    
    Args:
        model_path: Path to the saved model file
        input_dim: Expected input dimension (optional, will be inferred if None)
        encoding_dim: Default encoding dimension if not found in saved model
        config: Optional configuration dictionary for model parameters
        model_type: Force specific model type ('SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble')
        **kwargs: Additional parameters passed to model constructors
        
    Returns:
        Loaded model instance with proper configuration
        
    Raises:
        FileNotFoundError: If model file doesn't exist
        RuntimeError: If model loading fails
        ValueError: If architecture parameters are invalid
    """
    from datetime import datetime
    
    # Validate input path
    if not isinstance(model_path, Path):
        model_path = Path(model_path)
    
    if not model_path.exists():
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    logger.info(f"Loading autoencoder model from: {model_path}")
    
    try:
        # Load checkpoint with error handling
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
        except Exception as e:
            logger.error(f"Failed to load checkpoint from {model_path}: {e}")
            raise RuntimeError(f"Failed to load model file: {str(e)}")
        
        # Handle different checkpoint formats
        if isinstance(checkpoint, dict):
            state_dict = checkpoint.get('model_state_dict', checkpoint)
            saved_config = checkpoint.get('config', {})
            saved_architecture = checkpoint.get('architecture', {})
            saved_ensemble_info = checkpoint.get('ensemble_info', {})
            saved_model_class = checkpoint.get('model_class', None)
            saved_input_dim = checkpoint.get('input_dim', None)
        else:
            # Legacy format - assume the checkpoint is the state dict
            state_dict = checkpoint
            saved_config = {}
            saved_architecture = {}
            saved_ensemble_info = {}
            saved_model_class = None
            saved_input_dim = None
        
        # Merge configurations with precedence: kwargs > config > saved_config > defaults
        final_config = {}
        
        # Start with default configuration
        try:
            default_config = get_current_config() if 'get_current_config' in globals() else {}
            final_config.update(default_config)
        except Exception as e:
            logger.warning(f"Could not load default config: {e}")
        
        # Apply saved configuration
        if saved_config:
            final_config.update(saved_config)
            logger.debug("Applied saved model configuration")
        
        # Apply provided configuration
        if config:
            final_config.update(config)
            logger.debug("Applied provided configuration")
        
        # Apply kwargs
        if kwargs:
            # Organize kwargs into appropriate sections
            for key, value in kwargs.items():
                # Try to place in appropriate config section
                if key in ['input_dim', 'encoding_dim', 'hidden_dims', 'dropout_rates', 
                          'activation', 'normalization', 'num_models', 'diversity_factor']:
                    final_config.setdefault('model', {})[key] = value
                elif key in ['batch_size', 'learning_rate', 'epochs', 'optimizer']:
                    final_config.setdefault('training', {})[key] = value
                elif key in ['device', 'mixed_precision']:
                    final_config.setdefault('hardware', {})[key] = value
                else:
                    # Put in model section by default
                    final_config.setdefault('model', {})[key] = value
        
        # Determine input dimension with multiple fallback methods
        final_input_dim = input_dim
        
        if final_input_dim is None:
            # Try to get from saved model
            final_input_dim = saved_input_dim
        
        if final_input_dim is None:
            # Try to get from config
            final_input_dim = final_config.get('data', {}).get('features') or \
                             final_config.get('model', {}).get('input_dim')
        
        if final_input_dim is None:
            # Try to infer from state dict
            try:
                # Look for encoder input layer in various possible locations
                encoder_patterns = [
                    'encoder.0.weight',           # SimpleAutoencoder
                    'encoder.net.0.weight',       # Alternative format
                    'models.0.encoder.0.weight',  # AutoencoderEnsemble
                ]
                
                for pattern in encoder_patterns:
                    if pattern in state_dict:
                        final_input_dim = state_dict[pattern].shape[1]
                        logger.debug(f"Inferred input_dim={final_input_dim} from {pattern}")
                        break
                
                # If not found in patterns, search dynamically
                if final_input_dim is None:
                    for key, tensor in state_dict.items():
                        if 'encoder' in key and 'weight' in key and len(tensor.shape) == 2:
                            final_input_dim = tensor.shape[1]
                            logger.debug(f"Inferred input_dim={final_input_dim} from {key}")
                            break
                            
            except Exception as e:
                logger.warning(f"Failed to infer input_dim from state_dict: {e}")
        
        if final_input_dim is None:
            raise ValueError("Could not determine input_dim. Please provide it explicitly.")
        
        # Determine model type with multiple detection methods
        detected_model_type = model_type or saved_model_class
        
        if detected_model_type is None:
            # Detect from state dict structure
            if any(k.startswith('models.') for k in state_dict.keys()):
                detected_model_type = 'AutoencoderEnsemble'
                logger.debug("Detected AutoencoderEnsemble from state dict keys")
            elif any(k.startswith('attention.') for k in state_dict.keys()) or \
                 any('ResidualBlock' in str(type(v)) for v in state_dict.values() if hasattr(v, 'dtype')):
                detected_model_type = 'EnhancedAutoencoder'
                logger.debug("Detected EnhancedAutoencoder from state dict structure")
            else:
                # Check complexity - simple models have fewer layers
                encoder_layers = [k for k in state_dict.keys() if 'encoder' in k and 'weight' in k]
                # Simple encoder typically has 2 linear layers
                if len(encoder_layers) <= 4:
                    detected_model_type = 'SimpleAutoencoder'
                    logger.debug("Detected SimpleAutoencoder from layer count")
                else:
                    detected_model_type = 'EnhancedAutoencoder'
                    logger.debug("Detected EnhancedAutoencoder from layer complexity")
        
        logger.info(f"Loading {detected_model_type} with input_dim={final_input_dim}")
        
        # Extract architecture parameters with fallbacks
        model_config = final_config.setdefault('model', {})
        
        # Set input_dim in config
        model_config['input_dim'] = final_input_dim
        final_config.setdefault('data', {})['features'] = final_input_dim
        
        # Handle encoding_dim
        final_encoding_dim = encoding_dim
        if final_encoding_dim is None:
            final_encoding_dim = saved_architecture.get('encoding_dim') or \
                                saved_ensemble_info.get('encoding_dim') or \
                                model_config.get('encoding_dim')
        
        if final_encoding_dim is None:
            # Try to infer from state dict
            try:
                # Look for encoder output or decoder input
                # SimpleAutoencoder final layer
                if 'encoder.2.weight' in state_dict:
                    final_encoding_dim = state_dict['encoder.2.weight'].shape[0]
                # Decoder input
                elif 'decoder.0.weight' in state_dict:
                    final_encoding_dim = state_dict['decoder.0.weight'].shape[1]
                else:
                    # Search dynamically
                    for key, tensor in state_dict.items():
                        if 'decoder' in key and 'weight' in key and len(tensor.shape) == 2:
                            final_encoding_dim = tensor.shape[1]
                            break
            except Exception as e:
                logger.warning(f"Failed to infer encoding_dim: {e}")
        
        if final_encoding_dim is None:
            # Use defaults based on model type
            if detected_model_type == 'SimpleAutoencoder':
                final_encoding_dim = max(4, final_input_dim // 2)
            elif detected_model_type == 'EnhancedAutoencoder':
                final_encoding_dim = max(8, final_input_dim // 3)
            else:  # AutoencoderEnsemble
                final_encoding_dim = max(12, final_input_dim // 4)
        
        model_config['encoding_dim'] = final_encoding_dim
        
        # Extract other architecture parameters
        if saved_architecture:
            for key, value in saved_architecture.items():
                if key not in model_config:
                    model_config[key] = value
        
        if saved_ensemble_info:
            for key, value in saved_ensemble_info.items():
                if key not in model_config:
                    model_config[key] = value
        
        # Set default architecture parameters if not present
        architecture_defaults = {
            'SimpleAutoencoder': {
                'hidden_dims': [max(32, final_input_dim // 2)],
                'dropout_rates': [0.2],
                'activation': 'leaky_relu',
                'activation_param': 0.2,
                'normalization': None,
                'use_batch_norm': False,
                'use_layer_norm': False,
                'skip_connection': False,
                'residual_blocks': False,
                'use_attention': False,
                'model_type': 'SimpleAutoencoder'
            },
            'EnhancedAutoencoder': {
                'hidden_dims': [256, 128, 64],
                'dropout_rates': [0.2, 0.15, 0.1],
                'activation': 'leaky_relu',
                'activation_param': 0.2,
                'normalization': 'batch',
                'use_batch_norm': True,
                'use_layer_norm': False,
                'skip_connection': True,
                'residual_blocks': True,
                'use_attention': True,
                'model_type': 'EnhancedAutoencoder',
                'legacy_mode': False
            },
            'AutoencoderEnsemble': {
                'hidden_dims': [192, 96, 48],
                'dropout_rates': [0.25, 0.2, 0.15],
                'activation': 'leaky_relu',
                'activation_param': 0.2,
                'normalization': 'batch',
                'use_batch_norm': True,
                'use_layer_norm': False,
                'skip_connection': True,
                'residual_blocks': True,
                'use_attention': True,
                'num_models': 3,
                'diversity_factor': 0.3,
                'model_type': 'AutoencoderEnsemble'
            }
        }
        
        defaults = architecture_defaults.get(detected_model_type, architecture_defaults['EnhancedAutoencoder'])
        for key, default_value in defaults.items():
            if key not in model_config:
                model_config[key] = default_value
        
        # Add runtime metadata
        final_config.setdefault('runtime', {}).update({
            'loaded_at': datetime.now().isoformat(),
            'loaded_from': str(model_path),
            'detected_model_type': detected_model_type,
            'inferred_input_dim': final_input_dim,
            'inferred_encoding_dim': final_encoding_dim
        })
        
        # Create model instance based on detected type
        logger.info(f"Creating {detected_model_type} instance")
        
        try:
            if detected_model_type == 'SimpleAutoencoder':
                model = SimpleAutoencoder(
                    input_dim=final_input_dim,
                    config=final_config
                )
            elif detected_model_type == 'EnhancedAutoencoder':
                model = EnhancedAutoencoder(
                    input_dim=final_input_dim,
                    config=final_config
                )
            elif detected_model_type == 'AutoencoderEnsemble':
                model = AutoencoderEnsemble(
                    input_dim=final_input_dim,
                    config=final_config
                )
            else:
                raise ValueError(f"Unknown model type: {detected_model_type}")
                
        except Exception as e:
            logger.error(f"Failed to create {detected_model_type} instance: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            # Try fallback to SimpleAutoencoder
            logger.warning("Attempting fallback to SimpleAutoencoder")
            try:
                fallback_config = final_config.copy()
                fallback_config['model'].update({
                    'model_type': 'SimpleAutoencoder',
                    'hidden_dims': [max(32, final_input_dim // 2)],
                    'dropout_rates': [0.2],
                    'use_attention': False,
                    'residual_blocks': False,
                    'skip_connection': False,
                    'normalization': None
                })
                
                model = SimpleAutoencoder(
                    input_dim=final_input_dim,
                    config=fallback_config
                )
                logger.info("Successfully created fallback SimpleAutoencoder")
                
            except Exception as fallback_error:
                raise RuntimeError(f"Failed to create model instance and fallback failed: {fallback_error}")
        
        # Load state dict with comprehensive error handling
        logger.info("Loading model state dict")
        try:
            # Get current model state dict for comparison
            model_state_dict = model.state_dict()
            
            # Filter state dict to only include compatible keys
            compatible_state_dict = {}
            incompatible_keys = []
            missing_keys = []
            
            for key, tensor in state_dict.items():
                if key in model_state_dict:
                    if tensor.shape == model_state_dict[key].shape:
                        compatible_state_dict[key] = tensor
                    else:
                        incompatible_keys.append(f"{key}: saved={tensor.shape}, model={model_state_dict[key].shape}")
                        logger.warning(f"Shape mismatch for {key}: saved={tensor.shape}, model={model_state_dict[key].shape}")
            
            # Check for missing keys
            for key in model_state_dict:
                if key not in state_dict:
                    missing_keys.append(key)
            
            # Load compatible state dict
            if compatible_state_dict:
                model.load_state_dict(compatible_state_dict, strict=False)
                loaded_keys = len(compatible_state_dict)
                total_keys = len(model_state_dict)
                
                logger.info(f"Loaded {loaded_keys}/{total_keys} parameters successfully")
                
                if incompatible_keys:
                    logger.warning(f"Skipped {len(incompatible_keys)} incompatible keys")
                    if len(incompatible_keys) <= 5:
                        for key_info in incompatible_keys:
                            logger.warning(f"  Incompatible: {key_info}")
                    else:
                        logger.warning(f"  First 5 incompatible keys:")
                        for key_info in incompatible_keys[:5]:
                            logger.warning(f"    {key_info}")
                
                if missing_keys:
                    logger.info(f"Initialized {len(missing_keys)} missing keys with random values")
                    if len(missing_keys) <= 10:
                        logger.debug(f"Missing keys: {missing_keys}")
            else:
                logger.warning("No compatible parameters found - model will use random initialization")
        
        except Exception as e:
            logger.error(f"Failed to load state dict: {e}")
            logger.warning("Model created with random initialization")
        
        # Load optimizer and scheduler states if present and model has them
        if hasattr(model, 'optimizer') and 'optimizer_state_dict' in checkpoint:
            try:
                model.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                logger.debug("Loaded optimizer state")
            except Exception as e:
                logger.warning(f"Failed to load optimizer state: {e}")
        
        if hasattr(model, 'scheduler') and model.scheduler is not None and 'scheduler_state_dict' in checkpoint:
            try:
                model.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                logger.debug("Loaded scheduler state")
            except Exception as e:
                logger.warning(f"Failed to load scheduler state: {e}")
        
        # Set model to evaluation mode
        model.eval()
        
        # Log successful loading summary
        logger.info(f"Successfully loaded {type(model).__name__}")
        logger.info(f"Model summary: input_dim={final_input_dim}, encoding_dim={final_encoding_dim}")
        logger.info(f"Device: {model.device}, Mixed precision: {model.mixed_precision}")
        
        if hasattr(model, 'num_models'):
            logger.info(f"Ensemble size: {model.num_models} models")
        
        return model
        
    except FileNotFoundError:
        raise
    except ValueError:
        raise
    except Exception as e:
        error_msg = f"Failed to load autoencoder model from {model_path}: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        raise RuntimeError(error_msg)

def save_autoencoder_model(
    model: Union[SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble],
    model_path: Path,
    include_config: bool = True,
    include_training_state: bool = True
) -> None:
    """
    Save autoencoder model with comprehensive metadata and configuration.
    
    Args:
        model: Model instance to save
        model_path: Path where to save the model
        include_config: Whether to include full configuration
        include_training_state: Whether to include optimizer and scheduler states
    """
    try:
        model_path = Path(model_path)
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Use the model's built-in save method
        model.save_model(
            str(model_path),
            include_config=include_config
        )
        
        logger.info(f"Model saved successfully to {model_path}")
        
    except Exception as e:
        logger.error(f"Failed to save model to {model_path}: {e}")
        raise RuntimeError(f"Model saving failed: {str(e)}")

# Helper function for backward compatibility
def get_autocast_context(device, mixed_precision=True, enabled=True):
    """
    Get appropriate autocast context for different PyTorch versions and devices.
    
    Args:
        device: torch.device object
        mixed_precision: bool, whether mixed precision is enabled
        enabled: bool, whether autocast should be enabled
    
    Returns:
        Context manager for autocast or nullcontext
    """
    if not mixed_precision or not enabled:
        return nullcontext()
    
    if device.type == 'cuda':
        try:
            # Try new API first (PyTorch 1.10+)
            return torch.cuda.amp.autocast(enabled=True)
        except TypeError:
            try:
                # Fallback for older PyTorch versions
                return torch.cuda.amp.autocast()
            except Exception:
                return nullcontext()
    else:
        try:
            # Try CPU autocast (PyTorch 1.10+)
            return torch.cpu.amp.autocast(enabled=True)
        except (TypeError, AttributeError):
            # CPU autocast not available or different API
            return nullcontext()

def create_autoencoder_from_config(
    config: Dict[str, Any],
    model_type: Optional[str] = None
) -> Union[SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble]:
    """
    Create autoencoder model from configuration dictionary.
    
    Args:
        config: Configuration dictionary
        model_type: Force specific model type
        
    Returns:
        Created model instance
    """
    model_config = config.get('model', {})
    data_config = config.get('data', {})
    
    # Determine input dimension
    input_dim = model_config.get('input_dim') or data_config.get('features', 20)
    
    # Determine model type
    if model_type is None:
        model_type = model_config.get('model_type', 'EnhancedAutoencoder')
    
    try:
        if model_type == 'SimpleAutoencoder':
            return SimpleAutoencoder(input_dim=input_dim, config=config)
        elif model_type == 'EnhancedAutoencoder':
            return EnhancedAutoencoder(input_dim=input_dim, config=config)
        elif model_type == 'AutoencoderEnsemble':
            return AutoencoderEnsemble(input_dim=input_dim, config=config)
        else:
            raise ValueError(f"Unknown model type: {model_type}")
            
    except Exception as e:
        logger.error(f"Failed to create {model_type}: {e}")
        raise RuntimeError(f"Model creation failed: {str(e)}")

def load_and_validate_data(
    # Core Data Parameters
    data_path: Optional[Union[str, Path]] = None,
    artifacts_path: Optional[Union[str, Path]] = None,
    config: Optional[Dict[str, Any]] = None,
    
    # Data Loading Parameters
    use_real_data: Optional[bool] = None,
    data_format: Optional[str] = None,
    encoding: Optional[str] = None,
    delimiter: Optional[str] = None,
    header: Optional[Union[int, str]] = None,
    index_col: Optional[Union[int, str]] = None,
    skiprows: Optional[Union[int, List[int]]] = None,
    nrows: Optional[int] = None,
    chunk_size: Optional[int] = None,
    low_memory: Optional[bool] = None,
    memory_map: Optional[bool] = None,
    
    # Data Validation Parameters
    min_samples: Optional[int] = None,
    max_samples: Optional[int] = None,
    min_features: Optional[int] = None,
    max_features: Optional[int] = None,
    required_columns: Optional[List[str]] = None,
    label_column: Optional[str] = None,
    feature_columns: Optional[List[str]] = None,
    exclude_columns: Optional[List[str]] = None,
    
    # Data Processing Parameters
    normalization: Optional[str] = None,
    scaling_method: Optional[str] = None,
    handle_missing: Optional[str] = None,
    missing_value_strategy: Optional[str] = None,
    outlier_detection: Optional[bool] = None,
    outlier_method: Optional[str] = None,
    outlier_threshold: Optional[float] = None,
    
    # Data Splitting Parameters
    train_split: Optional[float] = None,
    validation_split: Optional[float] = None,
    test_split: Optional[float] = None,
    stratified_split: Optional[bool] = None,
    shuffle: Optional[bool] = None,
    random_state: Optional[int] = None,
    
    # Class Balance Parameters
    balance_classes: Optional[bool] = None,
    balance_method: Optional[str] = None,
    min_class_samples: Optional[int] = None,
    max_class_ratio: Optional[float] = None,
    
    # Feature Engineering Parameters
    feature_selection: Optional[bool] = None,
    feature_selection_method: Optional[str] = None,
    n_features_select: Optional[int] = None,
    correlation_threshold: Optional[float] = None,
    variance_threshold: Optional[float] = None,
    
    # Synthetic Data Parameters
    synthetic_normal_samples: Optional[int] = None,
    synthetic_attack_samples: Optional[int] = None,
    synthetic_generation_method: Optional[str] = None,
    synthetic_noise_level: Optional[float] = None,
    synthetic_seed: Optional[int] = None,
    
    # Performance Parameters
    parallel_loading: Optional[bool] = None,
    n_jobs: Optional[int] = None,
    batch_processing: Optional[bool] = None,
    memory_efficient: Optional[bool] = None,
    cache_data: Optional[bool] = None,
    
    # Security Parameters
    validate_data_integrity: Optional[bool] = None,
    check_data_corruption: Optional[bool] = None,
    anomaly_detection_threshold: Optional[float] = None,
    security_validation: Optional[bool] = None,
    
    # Monitoring Parameters
    verbose: Optional[bool] = None,
    log_level: Optional[str] = None,
    progress_bar: Optional[bool] = None,
    save_statistics: Optional[bool] = None,
    statistics_path: Optional[Union[str, Path]] = None,
    
    # Compatibility Parameters
    legacy_format: Optional[bool] = None,
    backward_compatibility: Optional[bool] = None,
    version_check: Optional[bool] = None,
    
    # Advanced Parameters
    data_quality_checks: Optional[bool] = None,
    statistical_validation: Optional[bool] = None,
    distribution_checks: Optional[bool] = None,
    cross_validation_ready: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_features: Optional[bool] = None,
    advanced_preprocessing: Optional[bool] = None,
    
    **kwargs
) -> Dict[str, Union[np.ndarray, Dict[str, Any]]]:

    # Start timing
    start_time = datetime.now()
    
    # Initialize configuration with comprehensive defaults
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply all parameters to configuration
    final_config = {}
    
    # Merge with existing config
    final_config.update(config)
    
    # Apply individual parameters with intelligent organization
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {'config', 'kwargs', 'start_time', 'datetime', 'traceback', 
                       'hashlib', 'train_test_split', 'StratifiedShuffleSplit',
                       'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'QuantileTransformer',
                       'SelectKBest', 'f_classif', 'VarianceThreshold', 'SimpleImputer', 
                       'KNNImputer', 'IsolationForest', 'stats'}
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Organize parameters into logical sections
    param_sections = {
        'data_loading': [
            'use_real_data', 'data_format', 'encoding', 'delimiter', 'header', 
            'index_col', 'skiprows', 'nrows', 'chunk_size', 'low_memory', 'memory_map'
        ],
        'data_validation': [
            'min_samples', 'max_samples', 'min_features', 'max_features', 
            'required_columns', 'label_column', 'feature_columns', 'exclude_columns'
        ],
        'data_processing': [
            'normalization', 'scaling_method', 'handle_missing', 'missing_value_strategy',
            'outlier_detection', 'outlier_method', 'outlier_threshold'
        ],
        'data_splitting': [
            'train_split', 'validation_split', 'test_split', 'stratified_split',
            'shuffle', 'random_state'
        ],
        'class_balance': [
            'balance_classes', 'balance_method', 'min_class_samples', 'max_class_ratio'
        ],
        'feature_engineering': [
            'feature_selection', 'feature_selection_method', 'n_features_select',
            'correlation_threshold', 'variance_threshold'
        ],
        'synthetic_data': [
            'synthetic_normal_samples', 'synthetic_attack_samples', 
            'synthetic_generation_method', 'synthetic_noise_level', 'synthetic_seed'
        ],
        'performance': [
            'parallel_loading', 'n_jobs', 'batch_processing', 'memory_efficient', 'cache_data'
        ],
        'security': [
            'validate_data_integrity', 'check_data_corruption', 
            'anomaly_detection_threshold', 'security_validation'
        ],
        'monitoring': [
            'verbose', 'log_level', 'progress_bar', 'save_statistics', 'statistics_path'
        ],
        'compatibility': [
            'legacy_format', 'backward_compatibility', 'version_check'
        ],
        'advanced': [
            'data_quality_checks', 'statistical_validation', 'distribution_checks',
            'cross_validation_ready', 'experimental_features', 'advanced_preprocessing'
        ]
    }
    
    # Apply parameters to appropriate sections
    for section, param_list in param_sections.items():
        section_config = final_config.setdefault(section, {})
        for param in param_list:
            if param in cleaned_params:
                section_config[param] = cleaned_params[param]
    
    # Set up comprehensive defaults
    data_loading_config = final_config.setdefault('data_loading', {})
    data_validation_config = final_config.setdefault('data_validation', {})
    data_processing_config = final_config.setdefault('data_processing', {})
    data_splitting_config = final_config.setdefault('data_splitting', {})
    class_balance_config = final_config.setdefault('class_balance', {})
    feature_engineering_config = final_config.setdefault('feature_engineering', {})
    synthetic_data_config = final_config.setdefault('synthetic_data', {})
    performance_config = final_config.setdefault('performance', {})
    security_config = final_config.setdefault('security', {})
    monitoring_config = final_config.setdefault('monitoring', {})
    compatibility_config = final_config.setdefault('compatibility', {})
    advanced_config = final_config.setdefault('advanced', {})
    
    # Set intelligent defaults
    use_real_data = data_loading_config.setdefault('use_real_data', True)
    data_format = data_loading_config.setdefault('data_format', 'csv')
    encoding = data_loading_config.setdefault('encoding', 'utf-8')
    delimiter = data_loading_config.setdefault('delimiter', ',')
    header = data_loading_config.setdefault('header', 0)
    low_memory = data_loading_config.setdefault('low_memory', True)
    
    # Validation parameters
    min_features = data_validation_config.setdefault('min_features', MIN_FEATURES)
    min_samples = data_validation_config.setdefault('min_samples', 100)
    label_column = data_validation_config.setdefault('label_column', 'Label')
    required_columns = data_validation_config.setdefault('required_columns', [label_column])
    
    # Processing parameters
    normalization = data_processing_config.setdefault('normalization', 'standard')
    handle_missing = data_processing_config.setdefault('handle_missing', 'drop')
    outlier_detection = data_processing_config.setdefault('outlier_detection', False)
    outlier_method = data_processing_config.setdefault('outlier_method', 'iqr')
    outlier_threshold = data_processing_config.setdefault('outlier_threshold', 1.5)
    
    # Splitting parameters
    validation_split = data_splitting_config.setdefault('validation_split', 0.2)
    test_split = data_splitting_config.setdefault('test_split', 0.2)
    stratified_split = data_splitting_config.setdefault('stratified_split', True)
    shuffle_data = data_splitting_config.setdefault('shuffle', True)
    random_state = data_splitting_config.setdefault('random_state', 42)
    
    # Performance parameters
    parallel_loading = performance_config.setdefault('parallel_loading', False)
    n_jobs = performance_config.setdefault('n_jobs', -1)
    memory_efficient = performance_config.setdefault('memory_efficient', True)
    cache_data = performance_config.setdefault('cache_data', False)
    
    # Security parameters
    validate_data_integrity = security_config.setdefault('validate_data_integrity', True)
    check_data_corruption = security_config.setdefault('check_data_corruption', False)
    
    # Monitoring parameters
    verbose = monitoring_config.setdefault('verbose', False)
    progress_bar = monitoring_config.setdefault('progress_bar', True)
    save_statistics = monitoring_config.setdefault('save_statistics', True)
    
    # Advanced parameters
    data_quality_checks = advanced_config.setdefault('data_quality_checks', True)
    statistical_validation = advanced_config.setdefault('statistical_validation', False)
    
    # Set up logging level
    # if verbose:
    #     original_level = logger.level
    #     logger.setLevel(logging.INFO)
    
    logger.info("Starting comprehensive data loading and validation")
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'current_substage': None,
        'rows_processed': 0,
        'features_processed': 0,
        'data_quality_score': 0.0,
        'validation_passed': 0,
        'validation_failed': 0
    }
    
    # Initialize comprehensive statistics
    loading_stats = {
        'start_time': start_time.isoformat(),
        'data_path': None,
        'artifacts_path': None,
        'use_real_data': use_real_data,
        'config_applied': final_config,
        'stages_completed': [],
        'errors_encountered': [],
        'warnings_encountered': [],
        'performance_metrics': {}
    }
    
    try:
        # Calculate total stages for progress tracking
        total_stages = 8  # Configuration, Paths, Loading, Validation, Processing, Splitting, Statistics, Finalization
        
        with alive_bar(total_stages, title='Data Loading & Validation\t', unit='stages') as main_bar:
            
            # STAGE 1: Configuration and Setup
            progress_data['current_stage'] = "Configuration Setup"
            main_bar.text = "Setting up configuration and parameters..."
            
            # Determine paths with multiple fallback strategies
            if data_path is None:
                # Try to get from config
                data_path = final_config.get('data', {}).get('data_path') or \
                           final_config.get('system', {}).get('data_dir')
                
                if data_path is None:
                    # Use default path
                    data_path = DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"
                else:
                    data_path = Path(data_path) / "preprocessed_dataset.csv"
            else:
                data_path = Path(data_path)
            
            if artifacts_path is None:
                # Try to get from config or derive from data path
                artifacts_path = final_config.get('data', {}).get('artifacts_path')
                if artifacts_path is None:
                    artifacts_path = data_path.parent / "preprocessing_artifacts.pkl"
                else:
                    artifacts_path = Path(artifacts_path)
            else:
                artifacts_path = Path(artifacts_path)
            
            loading_stats['data_path'] = str(data_path)
            loading_stats['artifacts_path'] = str(artifacts_path)
            
            logger.info(f"Data path: {data_path}")
            logger.info(f"Artifacts path: {artifacts_path}")
            
            main_bar.text = "Configuration complete"
            loading_stats['stages_completed'].append('configuration')
            main_bar()
            
            # STAGE 2: Path Validation and File Discovery
            progress_data['current_stage'] = "Path Validation"
            main_bar.text = "Validating paths and discovering files..."
            
            # Validate file existence with helpful error messages
            if use_real_data:
                if not data_path.exists():
                    # Try alternative locations
                    alternative_paths = [
                        data_path.parent / "dataset.csv",
                        data_path.parent / "data.csv",
                        Path.cwd() / "data" / data_path.name,
                        Path.cwd() / data_path.name
                    ]
                    
                    found_alternative = None
                    for alt_path in alternative_paths:
                        if alt_path.exists():
                            found_alternative = alt_path
                            break
                    
                    if found_alternative:
                        logger.warning(f"Data file not found at {data_path}, using {found_alternative}")
                        data_path = found_alternative
                        loading_stats['data_path'] = str(data_path)
                        loading_stats['warnings_encountered'].append(f"Used alternative path: {found_alternative}")
                    else:
                        error_msg = f"Data file not found: {data_path}\nTried alternatives: {alternative_paths}"
                        loading_stats['errors_encountered'].append(error_msg)
                        raise FileNotFoundError(error_msg)
                
                if not artifacts_path.exists() and normalization != 'none':
                    logger.warning(f"Artifacts file not found: {artifacts_path}")
                    logger.info("Will attempt to load data without preprocessing artifacts")
                    loading_stats['warnings_encountered'].append("Artifacts file not found")
            
            main_bar.text = "Path validation complete"
            loading_stats['stages_completed'].append('path_validation')
            main_bar()
            
            # STAGE 3: Data Loading
            progress_data['current_stage'] = "Data Loading"
            main_bar.text = "Loading data from source..."
            
            # Load data based on format and parameters
            logger.info(f"Loading data in {data_format} format")
            
            if use_real_data:
                try:
                    # Load data based on format
                    if data_format.lower() == 'csv':
                        load_params = {
                            'encoding': encoding,
                            'sep': delimiter,
                            'header': header,
                            'low_memory': low_memory
                        }
                        
                        if index_col is not None:
                            load_params['index_col'] = index_col
                        if skiprows is not None:
                            load_params['skiprows'] = skiprows
                        if nrows is not None:
                            load_params['nrows'] = nrows
                        
                        if chunk_size is not None:
                            # Handle large files with chunking
                            logger.info(f"Loading data in chunks of size {chunk_size}")
                            chunk_iter = pd.read_csv(data_path, chunksize=chunk_size, **load_params)
                            df_chunks = []
                            
                            # Show chunk loading progress
                            total_chunks = (os.path.getsize(data_path) // (chunk_size * 1000)) + 1
                            with alive_bar(total_chunks, title='Loading Chunks\t\t', unit='chunks') as chunk_bar:
                                for i, chunk in enumerate(chunk_iter):
                                    df_chunks.append(chunk)
                                    chunk_bar.text = f"Chunk {i+1}: {len(chunk)} rows"
                                    chunk_bar()
                                    if len(df_chunks) * chunk_size >= (max_samples or float('inf')):
                                        break
                            
                            df = pd.concat(df_chunks, ignore_index=True)
                        else:
                            df = pd.read_csv(data_path, **load_params)
                            
                    elif data_format.lower() == 'parquet':
                        df = pd.read_parquet(data_path)
                    elif data_format.lower() == 'json':
                        df = pd.read_json(data_path, encoding=encoding)
                    elif data_format.lower() == 'pickle':
                        df = pd.read_pickle(data_path)
                    else:
                        raise ValueError(f"Unsupported data format: {data_format}")
                    
                    loading_stats['rows_loaded'] = len(df)
                    loading_stats['columns_loaded'] = len(df.columns)
                    progress_data['rows_processed'] = len(df)
                    progress_data['features_processed'] = len(df.columns)
                    
                    logger.info(f"Loaded {len(df)} rows and {len(df.columns)} columns")
                    
                except Exception as e:
                    logger.error(f"Failed to load data from {data_path}: {e}")
                    loading_stats['errors_encountered'].append(f"Data loading failed: {str(e)}")
                    
                    if not use_real_data:
                        logger.info("Falling back to synthetic data generation")
                    else:
                        raise RuntimeError(f"Data loading failed: {str(e)}")
            
            # Generate synthetic data if needed
            if not use_real_data or (use_real_data and 'df' not in locals()):
                progress_data['current_substage'] = "Synthetic Data Generation"
                main_bar.text = "Generating synthetic data..."
                
                logger.info("Generating synthetic data")
                
                normal_samples = synthetic_data_config.get('synthetic_normal_samples', 1000)
                attack_samples = synthetic_data_config.get('synthetic_attack_samples', 200)
                n_features = min_features
                generation_method = synthetic_data_config.get('synthetic_generation_method', 'gaussian')
                noise_level = synthetic_data_config.get('synthetic_noise_level', 0.1)
                synthetic_seed = synthetic_data_config.get('synthetic_seed', random_state)
                
                np.random.seed(synthetic_seed)
                
                if generation_method == 'gaussian':
                    # Generate normal data
                    X_normal = np.random.normal(0, 1, (normal_samples, n_features))
                    
                    # Generate attack data with different distribution
                    X_attack = np.random.normal(2, 1.5, (attack_samples, n_features))
                    X_attack += np.random.normal(0, noise_level, X_attack.shape)
                    
                elif generation_method == 'mixed':
                    # More complex synthetic data
                    X_normal = np.random.multivariate_normal(
                        np.zeros(n_features), 
                        np.eye(n_features), 
                        normal_samples
                    )
                    
                    # Create correlated attack features
                    attack_mean = np.random.uniform(-2, 2, n_features)
                    attack_cov = np.eye(n_features) * np.random.uniform(0.5, 2, n_features)
                    X_attack = np.random.multivariate_normal(attack_mean, attack_cov, attack_samples)
                    
                else:
                    raise ValueError(f"Unknown synthetic generation method: {generation_method}")
                
                # Combine data
                X = np.vstack([X_normal, X_attack])
                y = np.hstack([np.zeros(normal_samples), np.ones(attack_samples)])
                
                # Create DataFrame
                feature_names = [f'feature_{i}' for i in range(n_features)]
                df = pd.DataFrame(X, columns=feature_names)
                df[label_column] = y
                
                loading_stats.update({
                    'synthetic_data_generated': True,
                    'normal_samples': normal_samples,
                    'attack_samples': attack_samples,
                    'generation_method': generation_method,
                    'noise_level': noise_level
                })
                
                progress_data['rows_processed'] = len(df)
                progress_data['features_processed'] = len(df.columns)
                
                logger.info(f"Generated synthetic data: {normal_samples} normal, {attack_samples} attack samples")
            
            main_bar.text = "Data loading complete"
            loading_stats['stages_completed'].append('data_loading')
            main_bar()
            
            # STAGE 4: Data Validation
            progress_data['current_stage'] = "Data Validation"
            main_bar.text = "Validating data structure and quality..."
            
            logger.info("Performing comprehensive data validation")
            
            if df is None or df.empty:
                error_msg = "No data loaded or generated"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            # Check required columns
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                error_msg = f"Missing required columns: {missing_columns}"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            # Validate label column
            if label_column not in df.columns:
                error_msg = f"Label column '{label_column}' not found in dataset"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            # Check data types and handle mixed types
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            if label_column in numeric_columns:
                numeric_columns.remove(label_column)
            
            if len(numeric_columns) < min_features:
                logger.warning(f"Only {len(numeric_columns)} numeric features found, minimum is {min_features}")
                loading_stats['warnings_encountered'].append(f"Low numeric features: {len(numeric_columns)}")
                
                # Try to convert non-numeric columns
                conversion_attempts = 0
                for col in df.columns:
                    if col != label_column and col not in numeric_columns:
                        try:
                            df[col] = pd.to_numeric(df[col], errors='coerce')
                            if not df[col].isna().all():
                                numeric_columns.append(col)
                                conversion_attempts += 1
                        except Exception:
                            continue
                
                if conversion_attempts > 0:
                    logger.info(f"Converted {conversion_attempts} columns to numeric")
            
            # Update feature columns
            if feature_columns is None:
                feature_columns = [col for col in numeric_columns if col != label_column]
            
            # Exclude specified columns
            if exclude_columns:
                feature_columns = [col for col in feature_columns if col not in exclude_columns]
            
            # Validate final feature set
            if len(feature_columns) < min_features:
                error_msg = f"Too few valid features ({len(feature_columns)}), need at least {min_features}"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            if max_features and len(feature_columns) > max_features:
                logger.info(f"Reducing features from {len(feature_columns)} to {max_features}")
                feature_columns = feature_columns[:max_features]
            
            loading_stats['feature_columns'] = feature_columns
            loading_stats['n_features'] = len(feature_columns)
            progress_data['features_processed'] = len(feature_columns)
            
            # Extract features and labels
            X = df[feature_columns].values.astype(np.float32)
            y = df[label_column].values
            
            progress_data['validation_passed'] += 1
            main_bar.text = "Data validation complete"
            loading_stats['stages_completed'].append('data_validation')
            main_bar()
            
            # STAGE 5: Data Processing
            progress_data['current_stage'] = "Data Processing"
            main_bar.text = "Processing data (cleaning, scaling, etc.)..."
            
            # Validate data quality
            if data_quality_checks:
                progress_data['current_substage'] = "Quality Checks"
                main_bar.text = "Performing data quality checks..."
                
                logger.info("Performing comprehensive data quality checks")
                
                # Check for infinite values
                inf_mask = np.isinf(X)
                if inf_mask.any():
                    logger.warning(f"Found {inf_mask.sum()} infinite values, replacing with NaN")
                    X[inf_mask] = np.nan
                    loading_stats['warnings_encountered'].append(f"Fixed {inf_mask.sum()} infinite values")
                
                # Check for excessive missing values
                missing_per_feature = np.isnan(X).sum(axis=0) / len(X)
                problematic_features = np.where(missing_per_feature > 0.5)[0]
                if len(problematic_features) > 0:
                    warning_msg = f"Features with >50% missing values: {problematic_features}"
                    logger.warning(warning_msg)
                    loading_stats['warnings_encountered'].append(warning_msg)
                
                # Check for zero variance features
                if feature_engineering_config.get('variance_threshold', 0) > 0:
                    variances = np.var(X, axis=0)
                    zero_var_features = np.where(variances < feature_engineering_config['variance_threshold'])[0]
                    if len(zero_var_features) > 0:
                        warning_msg = f"Low variance features detected: {zero_var_features}"
                        logger.warning(warning_msg)
                        loading_stats['warnings_encountered'].append(warning_msg)
            
            # Handle missing values
            if handle_missing and np.isnan(X).any():
                progress_data['current_substage'] = "Missing Value Handling"
                main_bar.text = "Handling missing values..."
                
                logger.info(f"Handling missing values using strategy: {handle_missing}")
                
                if handle_missing == 'drop':
                    # Drop rows with any missing values
                    valid_mask = ~np.isnan(X).any(axis=1)
                    rows_dropped = (~valid_mask).sum()
                    X = X[valid_mask]
                    y = y[valid_mask]
                    logger.info(f"Dropped {rows_dropped} rows with missing values")
                    loading_stats['rows_dropped_missing'] = rows_dropped
                    
                elif handle_missing == 'fill':
                    strategy = data_processing_config.get('missing_value_strategy', 'mean')
                    if strategy in ['mean', 'median', 'most_frequent']:
                        imputer = SimpleImputer(strategy=strategy)
                    elif strategy == 'knn':
                        imputer = KNNImputer(n_neighbors=5)
                    else:
                        imputer = SimpleImputer(strategy='mean')
                    
                    X = imputer.fit_transform(X)
                    logger.info(f"Filled missing values using {strategy} strategy")
                    loading_stats['missing_values_filled'] = True
                    loading_stats['imputation_strategy'] = strategy
            
            # Validate sample sizes
            if len(X) < min_samples:
                error_msg = f"Too few samples ({len(X)}), need at least {min_samples}"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            if max_samples and len(X) > max_samples:
                logger.info(f"Limiting dataset to {max_samples} samples")
                indices = np.random.choice(len(X), max_samples, replace=False)
                X = X[indices]
                y = y[indices]
                loading_stats['samples_limited'] = max_samples
            
            # Validate label distribution
            unique_labels = np.unique(y)
            label_counts = {label: np.sum(y == label) for label in unique_labels}
            
            logger.info(f"Label distribution: {label_counts}")
            loading_stats['label_distribution'] = label_counts
            
            # Check for class balance issues
            if class_balance_config.get('min_class_samples', 10):
                min_class_size = min(label_counts.values())
                if min_class_size < class_balance_config['min_class_samples']:
                    warning_msg = f"Smallest class has only {min_class_size} samples"
                    logger.warning(warning_msg)
                    loading_stats['warnings_encountered'].append(warning_msg)
            
            if class_balance_config.get('max_class_ratio', 10):
                max_class_size = max(label_counts.values())
                min_class_size = min(label_counts.values())
                class_ratio = max_class_size / min_class_size if min_class_size > 0 else float('inf')
                if class_ratio > class_balance_config['max_class_ratio']:
                    warning_msg = f"Class imbalance ratio: {class_ratio:.2f}"
                    logger.warning(warning_msg)
                    loading_stats['warnings_encountered'].append(warning_msg)
            
            # Outlier detection and handling
            if outlier_detection:
                progress_data['current_substage'] = "Outlier Detection"
                main_bar.text = "Detecting and handling outliers..."
                
                logger.info(f"Detecting outliers using {outlier_method} method")
                
                if outlier_method == 'iqr':
                    Q1 = np.percentile(X, 25, axis=0)
                    Q3 = np.percentile(X, 75, axis=0)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - outlier_threshold * IQR
                    upper_bound = Q3 + outlier_threshold * IQR
                    
                    outlier_mask = ((X < lower_bound) | (X > upper_bound)).any(axis=1)
                    
                elif outlier_method == 'zscore':
                    z_scores = np.abs(stats.zscore(X, axis=0, nan_policy='omit'))
                    outlier_mask = (z_scores > outlier_threshold).any(axis=1)
                    
                elif outlier_method == 'isolation':
                    iso_forest = IsolationForest(contamination=0.1, random_state=random_state)
                    outlier_labels = iso_forest.fit_predict(X)
                    outlier_mask = outlier_labels == -1
                    
                else:
                    logger.warning(f"Unknown outlier method: {outlier_method}")
                    outlier_mask = np.zeros(len(X), dtype=bool)
                    loading_stats['warnings_encountered'].append(f"Unknown outlier method: {outlier_method}")
                
                n_outliers = outlier_mask.sum()
                if n_outliers > 0:
                    logger.info(f"Detected {n_outliers} outliers ({n_outliers/len(X)*100:.1f}%)")
                    # Remove outliers
                    X = X[~outlier_mask]
                    y = y[~outlier_mask]
                    logger.info(f"Removed outliers, dataset size: {len(X)}")
                    loading_stats['outliers_removed'] = n_outliers
                    loading_stats['outlier_percentage'] = n_outliers/len(X)*100
            
            # Load preprocessing artifacts if available
            scaler = None
            feature_selector = None
            artifacts = {}
            
            if artifacts_path.exists():
                try:
                    progress_data['current_substage'] = "Loading Artifacts"
                    main_bar.text = "Loading preprocessing artifacts..."
                    
                    logger.info("Loading preprocessing artifacts")
                    artifacts = joblib.load(artifacts_path)
                    
                    # Validate artifacts
                    artifacts_features = artifacts.get("feature_names", [])
                    if artifacts_features and set(artifacts_features) != set(feature_columns):
                        warning_msg = "Feature names in artifacts don't match current features"
                        logger.warning(warning_msg)
                        logger.warning(f"Artifacts features: {len(artifacts_features)}")
                        logger.warning(f"Current features: {len(feature_columns)}")
                        loading_stats['warnings_encountered'].append(warning_msg)
                    
                    scaler = artifacts.get("scaler")
                    feature_selector = artifacts.get("feature_selector")
                    
                    loading_stats['artifacts_loaded'] = True
                    loading_stats['scaler_type'] = type(scaler).__name__ if scaler else None
                    
                except Exception as e:
                    warning_msg = f"Failed to load artifacts: {e}"
                    logger.warning(warning_msg)
                    loading_stats['warnings_encountered'].append(warning_msg)
                    artifacts = {}
            
            # Apply or create scaling
            if normalization and normalization != 'none':
                progress_data['current_substage'] = "Feature Scaling"
                main_bar.text = "Applying feature scaling..."
                
                if scaler is not None:
                    logger.info(f"Applying existing {type(scaler).__name__} scaler")
                    try:
                        X_scaled = scaler.transform(X)
                        loading_stats['scaling_applied'] = 'existing'
                    except Exception as e:
                        warning_msg = f"Failed to apply existing scaler: {e}"
                        logger.warning(warning_msg)
                        loading_stats['warnings_encountered'].append(warning_msg)
                        scaler = None
                
                if scaler is None:
                    logger.info(f"Creating new {normalization} scaler")
                    
                    if normalization == 'standard':
                        scaler = StandardScaler()
                    elif normalization == 'minmax':
                        scaler = MinMaxScaler()
                    elif normalization == 'robust':
                        scaler = RobustScaler()
                    elif normalization == 'quantile':
                        scaler = QuantileTransformer()
                    else:
                        logger.warning(f"Unknown normalization method: {normalization}, using standard")
                        scaler = StandardScaler()
                    
                    X_scaled = scaler.fit_transform(X)
                    loading_stats['scaling_applied'] = 'new'
                
                X = X_scaled.astype(np.float32)
            else:
                loading_stats['scaling_applied'] = 'none'
            
            # Feature selection
            if feature_engineering_config.get('feature_selection', False):
                progress_data['current_substage'] = "Feature Selection"
                main_bar.text = "Performing feature selection..."
                
                n_features_select = feature_engineering_config.get('n_features_select', min(50, len(feature_columns)))
                selection_method = feature_engineering_config.get('feature_selection_method', 'k_best')
                
                logger.info(f"Performing feature selection: {selection_method}")
                
                if selection_method == 'k_best':
                    if feature_selector is None:
                        feature_selector = SelectKBest(f_classif, k=n_features_select)
                        X_selected = feature_selector.fit_transform(X, y)
                        loading_stats['feature_selection_applied'] = 'new'
                    else:
                        X_selected = feature_selector.transform(X)
                        loading_stats['feature_selection_applied'] = 'existing'
                    
                    selected_features = feature_selector.get_support()
                    selected_feature_names = [feature_columns[i] for i in range(len(feature_columns)) if selected_features[i]]
                    
                    X = X_selected
                    feature_columns = selected_feature_names
                    
                    logger.info(f"Selected {len(feature_columns)} features from {len(selected_features)}")
                    loading_stats['features_selected'] = len(feature_columns)
            
            progress_data['validation_passed'] += 1
            main_bar.text = "Data processing complete"
            loading_stats['stages_completed'].append('data_processing')
            main_bar()
            
            # STAGE 6: Data Splitting
            progress_data['current_stage'] = "Data Splitting"
            main_bar.text = "Splitting data into train/validation/test sets..."
            
            # Split data into normal and attack samples
            normal_mask = (y == 0)
            attack_mask = (y == 1) if len(unique_labels) == 2 else (~normal_mask)
            
            X_normal = X[normal_mask]
            X_attack = X[attack_mask]
            
            logger.info(f"Data split: {len(X_normal)} normal, {len(X_attack)} attack samples")
            
            # Perform train/validation/test splits
            if len(X_normal) == 0:
                error_msg = "No normal samples found in dataset"
                loading_stats['errors_encountered'].append(error_msg)
                raise ValueError(error_msg)
            
            # Split normal data for training and validation
            if validation_split > 0:
                X_train_normal, X_val_normal = train_test_split(
                    X_normal,
                    test_size=validation_split,
                    random_state=random_state,
                    shuffle=shuffle_data
                )
            else:
                X_train_normal = X_normal
                X_val_normal = np.array([]).reshape(0, X_normal.shape[1])
            
            # Handle test data
            if len(X_attack) > 0:
                if test_split > 0 and test_split < 1.0:
                    test_size = int(len(X_attack) * test_split)
                    X_test = X_attack[:test_size] if test_size > 0 else X_attack
                else:
                    X_test = X_attack
            else:
                # If no attack data, use a portion of normal data for testing
                if test_split > 0:
                    test_size = int(len(X_train_normal) * test_split)
                    if test_size > 0:
                        X_test = X_train_normal[:test_size]
                        X_train_normal = X_train_normal[test_size:]
                    else:
                        X_test = np.array([]).reshape(0, X_train_normal.shape[1])
                else:
                    X_test = np.array([]).reshape(0, X_train_normal.shape[1])
            
            loading_stats.update({
                'train_samples': len(X_train_normal),
                'val_samples': len(X_val_normal),
                'test_samples': len(X_test),
                'final_feature_count': len(feature_columns)
            })
            
            progress_data['validation_passed'] += 1
            main_bar.text = "Data splitting complete"
            loading_stats['stages_completed'].append('data_splitting')
            main_bar()
            
            # STAGE 7: Statistical Validation and Quality Assessment
            progress_data['current_stage'] = "Quality Assessment"
            main_bar.text = "Performing final quality assessment..."
            
            # Statistical validation
            if statistical_validation:
                logger.info("Performing statistical validation")
                
                # Check data distributions
                if advanced_config.get('distribution_checks', False):
                    # Kolmogorov-Smirnov test for normality
                    normality_results = []
                    for i, feature_name in enumerate(feature_columns):
                        # Ensure we don't exceed dimensions
                        if i < X_train_normal.shape[1]:
                            feature_data = X_train_normal[:, i]
                            ks_stat, p_value = stats.kstest(feature_data, 'norm')
                            normality_results.append({
                                'feature': feature_name,
                                'ks_statistic': ks_stat,
                                'p_value': p_value,
                                'normal': p_value >= 0.05
                            })
                            if p_value < 0.05:
                                logger.debug(f"Feature {feature_name} may not be normally distributed (p={p_value:.4f})")
                    
                    loading_stats['normality_tests'] = normality_results
            
            # Calculate data quality score
            quality_metrics = []
            
            # Sample size adequacy
            sample_score = min(1.0, len(X) / max(min_samples, 1000))
            quality_metrics.append(('sample_size', sample_score))
            
            # Feature adequacy
            feature_score = min(1.0, len(feature_columns) / max(min_features, 10))
            quality_metrics.append(('feature_count', feature_score))
            
            # Class balance (if applicable)
            if len(label_counts) > 1:
                min_class = min(label_counts.values())
                max_class = max(label_counts.values())
                balance_score = min_class / max_class if max_class > 0 else 1.0
                quality_metrics.append(('class_balance', balance_score))
            else:
                quality_metrics.append(('class_balance', 1.0))
            
            # Missing data handling
            missing_score = 1.0 if not np.isnan(X).any() else 0.8
            quality_metrics.append(('missing_data', missing_score))
            
            # Overall quality score (weighted average)
            weights = [0.3, 0.3, 0.2, 0.2]  # Adjust weights as needed
            quality_score = sum(score * weight for (_, score), weight in zip(quality_metrics, weights))
            
            progress_data['data_quality_score'] = quality_score
            loading_stats['data_quality_score'] = quality_score
            loading_stats['quality_metrics'] = dict(quality_metrics)
            
            main_bar.text = "Quality assessment complete"
            loading_stats['stages_completed'].append('quality_assessment')
            main_bar()
            
            # STAGE 8: Finalization and Statistics
            progress_data['current_stage'] = "Finalization"
            main_bar.text = "Finalizing data preparation..."
            
            # Prepare final data dictionary
            data_dict = {
                "X_train": X_train_normal.astype(np.float32),
                "X_val": X_val_normal.astype(np.float32),
                "X_test": X_test.astype(np.float32),
                "feature_names": feature_columns,
                "metadata": {
                    # Basic statistics
                    "total_samples": len(X),
                    "total_normal": len(X_normal),
                    "total_attack": len(X_attack),
                    "n_features": len(feature_columns),
                    "train_samples": len(X_train_normal),
                    "val_samples": len(X_val_normal),
                    "test_samples": len(X_test),
                    
                    # Configuration used
                    "validation_split": validation_split,
                    "test_split": test_split,
                    "stratified_split": stratified_split,
                    "random_state": random_state,
                    "normalization": normalization,
                    
                    # Processing applied
                    "scaler_applied": scaler is not None,
                    "scaler_type": type(scaler).__name__ if scaler else None,
                    "feature_selection_applied": feature_selector is not None,
                    "outliers_removed": outlier_detection,
                    "missing_values_handled": handle_missing if np.isnan(X).any() else 'none',
                    
                    # Data quality metrics
                    "class_balance_ratio": max(label_counts.values()) / min(label_counts.values()) if min(label_counts.values()) > 0 else float('inf'),
                    "data_quality_score": quality_score,
                    
                    # Loading statistics
                    "loading_time_seconds": (datetime.now() - start_time).total_seconds(),
                    "data_source": "real" if use_real_data else "synthetic",
                    "config_applied": final_config,
                    
                    # Artifacts information
                    "artifacts_available": scaler is not None or feature_selector is not None,
                    "preprocessing_pipeline": {
                        "scaler": type(scaler).__name__ if scaler else None,
                        "feature_selector": type(feature_selector).__name__ if feature_selector else None,
                        "steps_applied": loading_stats.get('steps_applied', [])
                    }
                },
                
                # Include preprocessing components for future use
                "scaler": scaler,
                "feature_selector": feature_selector,
                "label_encoder": artifacts.get("label_encoder"),
                
                # Raw data for advanced use cases
                "raw_data": {
                    "X_full": X,
                    "y_full": y,
                    "original_features": df.columns.tolist() if 'df' in locals() else feature_columns
                } if advanced_config.get('include_raw_data', False) else None
            }
            
            # Validate final data shapes and consistency
            logger.info("Performing final data validation")
            
            # Shape consistency checks
            expected_features = len(feature_columns)
            for key in ["X_train", "X_val", "X_test"]:
                # Only check non-empty arrays
                if data_dict[key].size > 0:
                    if data_dict[key].shape[1] != expected_features:
                        error_msg = f"{key} has {data_dict[key].shape[1]} features, expected {expected_features}"
                        loading_stats['errors_encountered'].append(error_msg)
                        raise ValueError(error_msg)
            
            # Data type consistency
            for key in ["X_train", "X_val", "X_test"]:
                if data_dict[key].dtype != np.float32:
                    logger.warning(f"{key} has dtype {data_dict[key].dtype}, converting to float32")
                    data_dict[key] = data_dict[key].astype(np.float32)
            
            # Save statistics if requested
            if save_statistics:
                stats_path = monitoring_config.get('statistics_path', data_path.parent / "data_loading_statistics.json")
                try:
                    import json
                    with open(stats_path, 'w') as f:
                        # Make loading_stats JSON serializable
                        serializable_stats = {}
                        for key, value in loading_stats.items():
                            if isinstance(value, (str, int, float, bool, list, dict)):
                                serializable_stats[key] = value
                            else:
                                serializable_stats[key] = str(value)
                        
                        json.dump(serializable_stats, f, indent=2)
                    logger.info(f"Saved loading statistics to {stats_path}")
                except Exception as e:
                    logger.warning(f"Failed to save statistics: {e}")
            
            main_bar.text = "Finalization complete"
            loading_stats['stages_completed'].append('finalization')
            main_bar()
        
        # Log comprehensive summary
        total_time = (datetime.now() - start_time).total_seconds()
        loading_stats['total_processing_time'] = total_time
        loading_stats['completion_status'] = 'success'
        
        logger.info("=" * 80)
        logger.info("DATA LOADING SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Data source: {'Real data' if use_real_data else 'Synthetic data'}")
        logger.info(f"Total samples: {len(X):,} ({len(X_normal):,} normal, {len(X_attack):,} attack)")
        logger.info(f"Features: {len(feature_columns)} (after processing)")
        logger.info(f"Train/Val/Test split: {len(X_train_normal)}/{len(X_val_normal)}/{len(X_test)}")
        logger.info(f"Normalization: {normalization} ({'applied' if scaler else 'none'})")
        logger.info(f"Feature selection: {'applied' if feature_selector else 'none'}")
        logger.info(f"Data quality score: {quality_score:.3f}")
        logger.info(f"Processing time: {total_time:.2f} seconds")
        logger.info(f"Stages completed: {len(loading_stats['stages_completed'])}/{total_stages}")
        logger.info(f"Warnings encountered: {len(loading_stats['warnings_encountered'])}")
        logger.info("=" * 80)
        
        # Restore original logging level
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        return data_dict
        
    except Exception as e:
        # Update loading stats with error information
        loading_stats['completion_status'] = 'failed'
        loading_stats['error_message'] = str(e)
        loading_stats['error_traceback'] = traceback.format_exc()
        
        # Restore original logging level on error
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        error_msg = f"Data loading and validation failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Provide helpful error context
        logger.error(f"Error occurred while processing: {data_path}")
        logger.error(f"Configuration used: {final_config}")
        logger.error(f"Stages completed: {loading_stats['stages_completed']}")
        
        raise RuntimeError(error_msg)

def validate_data_integrity(
    data_dict: Dict[str, Union[np.ndarray, Dict[str, Any]]],
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Perform comprehensive data integrity validation.
    
    Args:
        data_dict: Data dictionary from load_and_validate_data
        config: Configuration dictionary
        
    Returns:
        Dictionary with validation results and recommendations
    """
    validation_results = {
        'passed': True,
        'warnings': [],
        'errors': [],
        'recommendations': [],
        'quality_score': 1.0,
        'detailed_checks': {}
    }
    
    try:
        # Extract data components
        X_train = data_dict.get('X_train', np.array([]))
        X_val = data_dict.get('X_val', np.array([]))
        X_test = data_dict.get('X_test', np.array([]))
        metadata = data_dict.get('metadata', {})
        
        # Check data presence
        if X_train.size == 0:
            validation_results['errors'].append("No training data available")
            validation_results['passed'] = False
        
        # Check data shapes consistency
        if X_train.size > 0 and X_val.size > 0:
            if X_train.shape[1] != X_val.shape[1]:
                validation_results['errors'].append(f"Feature dimension mismatch: train={X_train.shape[1]}, val={X_val.shape[1]}")
                validation_results['passed'] = False
        
        # Check for data quality issues
        for name, data in [('train', X_train), ('val', X_val), ('test', X_test)]:
            if data.size > 0:
                # Check for NaN values
                nan_count = np.isnan(data).sum()
                if nan_count > 0:
                    validation_results['warnings'].append(f"{name} data contains {nan_count} NaN values")
                
                # Check for infinite values
                inf_count = np.isinf(data).sum()
                if inf_count > 0:
                    validation_results['warnings'].append(f"{name} data contains {inf_count} infinite values")
                
                # Check data ranges
                if np.any(data > 1e6) or np.any(data < -1e6):
                    validation_results['warnings'].append(f"{name} data contains extreme values")
        
        # Calculate quality score
        n_warnings = len(validation_results['warnings'])
        n_errors = len(validation_results['errors'])
        validation_results['quality_score'] = max(0.0, 1.0 - (n_warnings * 0.1 + n_errors * 0.5))
        
        return validation_results
        
    except Exception as e:
        validation_results['errors'].append(f"Validation process failed: {str(e)}")
        validation_results['passed'] = False
        validation_results['quality_score'] = 0.0
        return validation_results

def create_data_pipeline(
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Create a comprehensive data processing pipeline based on configuration.
    
    Args:
        config: Complete configuration dictionary
        
    Returns:
        Data pipeline configuration
    """
    pipeline_config = {
        'steps': [],
        'parameters': {},
        'validation_rules': {},
        'monitoring': {}
    }
    
    # Extract relevant configuration sections
    data_config = config.get('data_loading', {})
    processing_config = config.get('data_processing', {})
    validation_config = config.get('data_validation', {})
    
    # Build pipeline steps
    pipeline_steps = []
    
    # Step 1: Data Loading
    pipeline_steps.append({
        'name': 'data_loading',
        'function': 'load_raw_data',
        'parameters': data_config
    })
    
    # Step 2: Data Validation
    pipeline_steps.append({
        'name': 'validation',
        'function': 'validate_data_structure',
        'parameters': validation_config
    })
    
    # Step 3: Data Processing
    if processing_config.get('normalization', 'none') != 'none':
        pipeline_steps.append({
            'name': 'normalization',
            'function': 'apply_normalization',
            'parameters': {'method': processing_config['normalization']}
        })
    
    # Step 4: Feature Engineering
    if config.get('feature_engineering', {}).get('feature_selection', False):
        pipeline_steps.append({
            'name': 'feature_selection',
            'function': 'select_features',
            'parameters': config['feature_engineering']
        })
    
    # Step 5: Data Splitting
    pipeline_steps.append({
        'name': 'data_splitting',
        'function': 'split_data',
        'parameters': config.get('data_splitting', {})
    })
    
    pipeline_config['steps'] = pipeline_steps
    pipeline_config['parameters'] = config
    
    return pipeline_config

def generate_synthetic_data(
    # Core Data Generation Parameters
    normal_samples: Optional[int] = None,
    attack_samples: Optional[int] = None,
    features: Optional[int] = None,
    anomaly_factor: Optional[float] = None,
    random_state: Optional[int] = None,
    
    # Data Structure Parameters
    label_column: Optional[str] = None,
    feature_names: Optional[List[str]] = None,
    feature_prefix: Optional[str] = None,
    output_format: Optional[str] = None,
    data_type: Optional[str] = None,
    
    # Generation Method Parameters
    generation_method: Optional[str] = None,
    generation_strategy: Optional[str] = None,
    distribution_type: Optional[str] = None,
    cluster_centers: Optional[int] = None,
    cluster_variance: Optional[float] = None,
    cluster_separation: Optional[float] = None,
    
    # Normal Data Parameters
    normal_mean: Optional[float] = None,
    normal_std: Optional[float] = None,
    normal_distribution: Optional[str] = None,
    normal_clusters: Optional[int] = None,
    normal_cluster_variance: Optional[float] = None,
    normal_bounds: Optional[Tuple[float, float]] = None,
    normal_correlation: Optional[float] = None,
    
    # Attack Data Parameters
    attack_mean: Optional[float] = None,
    attack_std: Optional[float] = None,
    attack_distribution: Optional[str] = None,
    attack_clusters: Optional[int] = None,
    attack_cluster_variance: Optional[float] = None,
    attack_bounds: Optional[Tuple[float, float]] = None,
    attack_correlation: Optional[float] = None,
    attack_types: Optional[List[str]] = None,
    attack_proportions: Optional[List[float]] = None,
    
    # Anomaly Configuration Parameters
    anomaly_sparsity: Optional[float] = None,
    anomaly_intensity: Optional[float] = None,
    anomaly_patterns: Optional[List[str]] = None,
    anomaly_correlation: Optional[float] = None,
    seasonal_anomalies: Optional[bool] = None,
    contextual_anomalies: Optional[bool] = None,
    point_anomalies: Optional[bool] = None,
    collective_anomalies: Optional[bool] = None,
    
    # Data Splitting Parameters
    validation_split: Optional[float] = None,
    test_split: Optional[float] = None,
    stratified_split: Optional[bool] = None,
    shuffle: Optional[bool] = None,
    
    # Noise Parameters
    noise_level: Optional[float] = None,
    noise_type: Optional[str] = None,
    gaussian_noise_std: Optional[float] = None,
    uniform_noise_range: Optional[float] = None,
    salt_pepper_ratio: Optional[float] = None,
    
    # Feature Engineering Parameters
    correlated_features: Optional[bool] = None,
    correlation_matrix: Optional[np.ndarray] = None,
    feature_dependencies: Optional[Dict[str, List[str]]] = None,
    redundant_features: Optional[int] = None,
    informative_features: Optional[int] = None,
    feature_scaling: Optional[str] = None,
    
    # Temporal Parameters
    temporal_data: Optional[bool] = None,
    time_steps: Optional[int] = None,
    temporal_pattern: Optional[str] = None,
    trend_component: Optional[bool] = None,
    seasonal_component: Optional[bool] = None,
    temporal_noise: Optional[float] = None,
    
    # Quality Control Parameters
    outlier_contamination: Optional[float] = None,
    class_balance_ratio: Optional[float] = None,
    minimum_separation: Optional[float] = None,
    overlap_factor: Optional[float] = None,
    separability_score: Optional[float] = None,
    
    # Advanced Generation Parameters
    multimodal_data: Optional[bool] = None,
    mixture_components: Optional[int] = None,
    mixture_weights: Optional[List[float]] = None,
    non_linear_relationships: Optional[bool] = None,
    polynomial_degree: Optional[int] = None,
    interaction_features: Optional[bool] = None,
    
    # Statistical Properties Parameters
    skewness: Optional[float] = None,
    kurtosis: Optional[float] = None,
    heavy_tails: Optional[bool] = None,
    distribution_parameters: Optional[Dict[str, Any]] = None,
    moment_constraints: Optional[Dict[str, float]] = None,
    
    # Scalability Parameters
    batch_generation: Optional[bool] = None,
    batch_size: Optional[int] = None,
    memory_efficient: Optional[bool] = None,
    parallel_generation: Optional[bool] = None,
    n_jobs: Optional[int] = None,
    
    # Reproducibility Parameters
    deterministic: Optional[bool] = None,
    seed_sequence: Optional[List[int]] = None,
    generator_state: Optional[Dict[str, Any]] = None,
    
    # Validation Parameters
    validate_generation: Optional[bool] = None,
    statistical_tests: Optional[bool] = None,
    quality_metrics: Optional[List[str]] = None,
    distribution_tests: Optional[bool] = None,
    
    # Export Parameters
    save_data: Optional[bool] = None,
    output_path: Optional[Union[str, Path]] = None,
    file_format: Optional[str] = None,
    compression: Optional[str] = None,
    metadata_file: Optional[bool] = None,
    
    # Monitoring Parameters
    verbose: Optional[bool] = None,
    progress_bar: Optional[bool] = None,
    log_generation_stats: Optional[bool] = None,
    generation_report: Optional[bool] = None,
    
    # Compatibility Parameters
    sklearn_format: Optional[bool] = None,
    pandas_output: Optional[bool] = None,
    torch_tensors: Optional[bool] = None,
    numpy_arrays: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_methods: Optional[bool] = None,
    gan_based_generation: Optional[bool] = None,
    autoencoder_generation: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    synthetic_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Dict[str, Union[np.ndarray, pd.DataFrame, Dict[str, Any]]]:

    # Start timing
    start_time = datetime.now()
    
    # Initialize configuration with comprehensive defaults
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply synthetic configuration
    if synthetic_config:
        config.setdefault('synthetic_data', {}).update(synthetic_config)
    
    # Apply all parameters to configuration
    final_config = {}
    
    # Merge with existing config
    final_config.update(config)
    
    # Apply individual parameters with intelligent organization
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'synthetic_config', 'kwargs', 'start_time', 'datetime', 
        'traceback', 'stats', 'make_classification', 'make_blobs', 
        'StandardScaler', 'MinMaxScaler', 'train_test_split'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Organize parameters into logical sections
    param_sections = {
        'core_generation': [
            'normal_samples', 'attack_samples', 'features', 'anomaly_factor', 'random_state'
        ],
        'data_structure': [
            'label_column', 'feature_names', 'feature_prefix', 'output_format', 'data_type'
        ],
        'generation_methods': [
            'generation_method', 'generation_strategy', 'distribution_type', 'cluster_centers',
            'cluster_variance', 'cluster_separation'
        ],
        'normal_data': [
            'normal_mean', 'normal_std', 'normal_distribution', 'normal_clusters',
            'normal_cluster_variance', 'normal_bounds', 'normal_correlation'
        ],
        'attack_data': [
            'attack_mean', 'attack_std', 'attack_distribution', 'attack_clusters',
            'attack_cluster_variance', 'attack_bounds', 'attack_correlation',
            'attack_types', 'attack_proportions'
        ],
        'anomaly_config': [
            'anomaly_sparsity', 'anomaly_intensity', 'anomaly_patterns', 'anomaly_correlation',
            'seasonal_anomalies', 'contextual_anomalies', 'point_anomalies', 'collective_anomalies'
        ],
        'data_splitting': [
            'validation_split', 'test_split', 'stratified_split', 'shuffle'
        ],
        'noise_parameters': [
            'noise_level', 'noise_type', 'gaussian_noise_std', 'uniform_noise_range',
            'salt_pepper_ratio'
        ],
        'feature_engineering': [
            'correlated_features', 'correlation_matrix', 'feature_dependencies',
            'redundant_features', 'informative_features', 'feature_scaling'
        ],
        'temporal_parameters': [
            'temporal_data', 'time_steps', 'temporal_pattern', 'trend_component',
            'seasonal_component', 'temporal_noise'
        ],
        'quality_control': [
            'outlier_contamination', 'class_balance_ratio', 'minimum_separation',
            'overlap_factor', 'separability_score'
        ],
        'advanced_generation': [
            'multimodal_data', 'mixture_components', 'mixture_weights',
            'non_linear_relationships', 'polynomial_degree', 'interaction_features'
        ],
        'statistical_properties': [
            'skewness', 'kurtosis', 'heavy_tails', 'distribution_parameters',
            'moment_constraints'
        ],
        'scalability': [
            'batch_generation', 'batch_size', 'memory_efficient', 'parallel_generation', 'n_jobs'
        ],
        'reproducibility': [
            'deterministic', 'seed_sequence', 'generator_state'
        ],
        'validation': [
            'validate_generation', 'statistical_tests', 'quality_metrics', 'distribution_tests'
        ],
        'export': [
            'save_data', 'output_path', 'file_format', 'compression', 'metadata_file'
        ],
        'monitoring': [
            'verbose', 'progress_bar', 'log_generation_stats', 'generation_report'
        ],
        'compatibility': [
            'sklearn_format', 'pandas_output', 'torch_tensors', 'numpy_arrays'
        ],
        'experimental': [
            'experimental_methods', 'gan_based_generation', 'autoencoder_generation'
        ]
    }
    
    # Apply parameters to appropriate sections
    for section, param_list in param_sections.items():
        section_config = final_config.setdefault(section, {})
        for param in param_list:
            if param in cleaned_params:
                section_config[param] = cleaned_params[param]
    
    # Set up comprehensive defaults
    core_config = final_config.setdefault('core_generation', {})
    data_structure_config = final_config.setdefault('data_structure', {})
    generation_config = final_config.setdefault('generation_methods', {})
    normal_config = final_config.setdefault('normal_data', {})
    attack_config = final_config.setdefault('attack_data', {})
    anomaly_config = final_config.setdefault('anomaly_config', {})
    splitting_config = final_config.setdefault('data_splitting', {})
    noise_config = final_config.setdefault('noise_parameters', {})
    feature_config = final_config.setdefault('feature_engineering', {})
    quality_config = final_config.setdefault('quality_control', {})
    advanced_config = final_config.setdefault('advanced_generation', {})
    monitoring_config = final_config.setdefault('monitoring', {})
    
    # Set intelligent defaults
    normal_samples = core_config.setdefault('normal_samples', NORMAL_SAMPLES)
    attack_samples = core_config.setdefault('attack_samples', ATTACK_SAMPLES)
    features = core_config.setdefault('features', FEATURES)
    anomaly_factor = core_config.setdefault('anomaly_factor', ANOMALY_FACTOR)
    random_state = core_config.setdefault('random_state', RANDOM_STATE)
    
    # Data structure defaults
    label_column = data_structure_config.setdefault('label_column', 'Label')
    feature_prefix = data_structure_config.setdefault('feature_prefix', 'feature')
    output_format = data_structure_config.setdefault('output_format', 'dict')
    data_type = data_structure_config.setdefault('data_type', 'float32')
    
    # Generation method defaults
    generation_method = generation_config.setdefault('generation_method', 'mixed')
    distribution_type = generation_config.setdefault('distribution_type', 'gaussian')
    cluster_centers = generation_config.setdefault('cluster_centers', 3)
    cluster_variance = generation_config.setdefault('cluster_variance', 0.1)
    
    # Normal data defaults
    normal_mean = normal_config.setdefault('normal_mean', 0.5)
    normal_std = normal_config.setdefault('normal_std', 0.1)
    normal_distribution = normal_config.setdefault('normal_distribution', 'gaussian')
    normal_clusters = normal_config.setdefault('normal_clusters', 3)
    
    # Attack data defaults
    attack_mean = attack_config.setdefault('attack_mean', 0.7)
    attack_std = attack_config.setdefault('attack_std', 0.2)
    attack_distribution = attack_config.setdefault('attack_distribution', 'mixed')
    attack_types = attack_config.setdefault('attack_types', ['high_variance', 'shifted_mean', 'sparse_extreme', 'clustered_outliers'])
    
    # Anomaly configuration defaults
    anomaly_sparsity = anomaly_config.setdefault('anomaly_sparsity', 0.3)
    anomaly_intensity = anomaly_config.setdefault('anomaly_intensity', 2.0)
    point_anomalies = anomaly_config.setdefault('point_anomalies', True)
    collective_anomalies = anomaly_config.setdefault('collective_anomalies', False)
    
    # Splitting defaults
    validation_split = splitting_config.setdefault('validation_split', 0.2)
    # Use all attack data for test
    test_split = splitting_config.setdefault('test_split', 1.0)
    shuffle = splitting_config.setdefault('shuffle', True)
    
    # Noise defaults
    noise_level = noise_config.setdefault('noise_level', 0.01)
    noise_type = noise_config.setdefault('noise_type', 'gaussian')
    
    # Feature engineering defaults
    correlated_features = feature_config.setdefault('correlated_features', False)
    informative_features = feature_config.setdefault('informative_features', min(features, max(5, features // 2)))
    
    # Quality control defaults
    class_balance_ratio = quality_config.setdefault('class_balance_ratio', attack_samples / normal_samples)
    minimum_separation = quality_config.setdefault('minimum_separation', 0.1)
    
    # Monitoring defaults
    verbose = monitoring_config.setdefault('verbose', False)
    progress_bar = monitoring_config.setdefault('progress_bar', True)
    log_generation_stats = monitoring_config.setdefault('log_generation_stats', True)
    
    # Set up logging level
    # if verbose:
    #     original_level = logger.level
    #     logger.setLevel(logging.INFO)
    
    logger.info("Starting comprehensive synthetic data generation")
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'current_substage': None,
        'samples_generated': 0,
        'features_created': 0,
        'attack_types_processed': 0,
        'data_quality_score': 0.0,
        'generation_quality': 0.0
    }
    
    # Initialize generation statistics
    generation_stats = {
        'start_time': start_time.isoformat(),
        'normal_samples': normal_samples,
        'attack_samples': attack_samples,
        'features': features,
        'anomaly_factor': anomaly_factor,
        'random_state': random_state,
        'generation_method': generation_method,
        'config_applied': final_config,
        'stages_completed': [],
        'warnings_encountered': [],
        'performance_metrics': {}
    }
    
    try:
        # Validate parameters
        if normal_samples <= 0 or attack_samples <= 0 or features <= 0:
            raise ValueError("Sample counts and features must be positive")
        
        if not 0 < validation_split < 1:
            raise ValueError("Validation split must be between 0 and 1")
        
        if anomaly_factor <= 0:
            raise ValueError("Anomaly factor must be positive")
        
        # Initialize random number generator
        np.random.seed(random_state)
        
        # Calculate total stages for progress tracking
        total_stages = 10  # Configuration, Setup, Normal Data, Attack Data, Correlation, Noise, Scaling, Splitting, Validation, Finalization
        
        with alive_bar(total_stages, title='Synthetic Data Generation\t', unit='stages') as main_bar:
            
            # STAGE 1: Configuration and Validation
            progress_data['current_stage'] = "Configuration"
            main_bar.text = "Validating parameters and setting up configuration..."
            
            logger.info(f"Generating synthetic data: {normal_samples} normal, {attack_samples} attack samples")
            logger.info(f"Features: {features}, method: {generation_method}, anomaly_factor: {anomaly_factor}")
            
            # Generate feature names
            if feature_names is None:
                feature_names = [f"{feature_prefix}_{i}" for i in range(features)]
            elif len(feature_names) != features:
                logger.warning(f"Feature names length ({len(feature_names)}) doesn't match features ({features})")
                feature_names = [f"{feature_prefix}_{i}" for i in range(features)]
            
            progress_data['features_created'] = len(feature_names)
            main_bar.text = "Configuration complete"
            generation_stats['stages_completed'].append('configuration')
            main_bar()
            
            # STAGE 2: Data Generation Setup
            progress_data['current_stage'] = "Initialization"
            main_bar.text = "Initializing data generation framework..."
            
            # Initialize data containers
            X_normal = None
            X_attack_parts = []
            attack_type_counts = {}
            
            main_bar.text = "Initialization complete"
            generation_stats['stages_completed'].append('initialization')
            main_bar()
            
            # STAGE 3: Normal Data Generation
            progress_data['current_stage'] = "Normal Data Generation"
            main_bar.text = f"Generating {normal_samples} normal samples..."
            
            # Generate normal data based on method
            if generation_method == 'sklearn':
                logger.info("Using sklearn-based generation")
                main_bar.text = f"Using sklearn: {normal_samples} samples, {features} features..."
                X_normal, _ = make_classification(
                    n_samples=normal_samples,
                    n_features=features,
                    n_informative=informative_features,
                    n_redundant=max(0, features - informative_features),
                    n_clusters_per_class=normal_clusters,
                    class_sep=minimum_separation,
                    random_state=random_state
                )
                # Convert to probability-like values
                main_bar.text = "Scaling sklearn data to [0,1] range..."
                scaler = MinMaxScaler()
                X_normal = scaler.fit_transform(X_normal)
                
            elif generation_method == 'clustering':
                logger.info("Using clustering-based generation")
                main_bar.text = f"Using clustering: {normal_samples} samples, {normal_clusters} clusters..."
                X_normal, _ = make_blobs(
                    n_samples=normal_samples,
                    centers=normal_clusters,
                    n_features=features,
                    cluster_std=cluster_variance,
                    random_state=random_state
                )
                # Normalize to [0, 1] range
                main_bar.text = "Normalizing cluster data to [0,1] range..."
                X_normal = (X_normal - X_normal.min()) / (X_normal.max() - X_normal.min())
                
            elif generation_method == 'mixed':
                logger.info("Using mixed distribution generation")
                main_bar.text = f"Using {normal_distribution} distribution for normal data..."
                
                if normal_distribution == 'gaussian':
                    X_normal = np.random.normal(normal_mean, normal_std, (normal_samples, features))
                elif normal_distribution == 'uniform':
                    bounds = normal_config.get('normal_bounds', (0.0, 1.0))
                    X_normal = np.random.uniform(bounds[0], bounds[1], (normal_samples, features))
                elif normal_distribution == 'beta':
                    alpha, beta = normal_config.get('distribution_parameters', {}).get('beta_params', (2, 2))
                    X_normal = np.random.beta(alpha, beta, (normal_samples, features))
                elif normal_distribution == 'gamma':
                    shape, scale = normal_config.get('distribution_parameters', {}).get('gamma_params', (2, 0.1))
                    X_normal = np.random.gamma(shape, scale, (normal_samples, features))
                else:
                    logger.warning(f"Unknown normal distribution '{normal_distribution}', using gaussian")
                    X_normal = np.random.normal(normal_mean, normal_std, (normal_samples, features))
                
                # Ensure values are in reasonable range
                main_bar.text = "Clipping normal data to [0,1] range..."
                X_normal = np.clip(X_normal, 0.0, 1.0)
                
            else:
                raise ValueError(f"Unknown generation method: {generation_method}")
            
            progress_data['samples_generated'] += normal_samples
            main_bar.text = f"Generated {normal_samples} normal samples"
            generation_stats['stages_completed'].append('normal_data_generation')
            main_bar()
            
            # STAGE 4: Attack Data Generation
            progress_data['current_stage'] = "Attack Data Generation"
            main_bar.text = f"Generating {attack_samples} attack samples across {len(attack_types)} types..."
            
            logger.info(f"Generating attack data with types: {attack_types}")
            
            # Calculate samples per attack type
            samples_per_type = attack_samples // len(attack_types)
            remainder = attack_samples % len(attack_types)
            
            # Generate each attack type with progress tracking using the main bar
            for i, attack_type in enumerate(attack_types):
                progress_data['current_substage'] = f"Attack Type: {attack_type}"
                main_bar.text = f"Generating {attack_type}: {i+1}/{len(attack_types)} attack types..."
                
                n_samples = samples_per_type + (1 if i < remainder else 0)
                attack_type_counts[attack_type] = n_samples
                
                if attack_type == 'high_variance':
                    # High variance anomalies
                    X_attack_type = np.random.normal(
                        normal_mean, 
                        normal_std * anomaly_factor * 2, 
                        (n_samples, features)
                    )
                    
                elif attack_type == 'shifted_mean':
                    # Shifted mean anomalies
                    shift = anomaly_factor * 0.3
                    X_attack_type = np.random.normal(
                        normal_mean + shift, 
                        normal_std, 
                        (n_samples, features)
                    )
                    
                elif attack_type == 'sparse_extreme':
                    # Sparse extreme anomalies
                    X_attack_type = np.random.normal(normal_mean, normal_std, (n_samples, features))
                    # Make some features extreme
                    extreme_mask = np.random.random((n_samples, features)) < anomaly_sparsity
                    extreme_values = np.random.choice([0.05, 0.95], size=np.sum(extreme_mask))
                    X_attack_type[extreme_mask] = extreme_values * anomaly_intensity
                    
                elif attack_type == 'clustered_outliers':
                    # Clustered outliers
                    outlier_center = normal_mean + (0.4 * anomaly_factor)
                    X_attack_type = np.random.normal(
                        outlier_center, 
                        cluster_variance, 
                        (n_samples, features)
                    )
                    
                elif attack_type == 'mixed_distribution':
                    # Mixed distribution anomalies
                    n_mixed = n_samples // 2
                    X_part1 = np.random.uniform(0.8, 1.0, (n_mixed, features)) * anomaly_factor
                    X_part2 = np.random.exponential(0.1 * anomaly_factor, (n_samples - n_mixed, features))
                    X_attack_type = np.vstack([X_part1, X_part2])
                    
                elif attack_type == 'temporal_anomalies':
                    # Temporal pattern anomalies
                    X_attack_type = np.random.normal(normal_mean, normal_std, (n_samples, features))
                    # Add temporal patterns
                    for j in range(features):
                        temporal_pattern = np.sin(np.linspace(0, 2*np.pi, n_samples)) * anomaly_factor * 0.2
                        X_attack_type[:, j] += temporal_pattern
                        
                elif attack_type == 'contextual_anomalies':
                    # Context-dependent anomalies
                    X_attack_type = np.random.normal(normal_mean, normal_std, (n_samples, features))
                    # Make anomalous in specific feature combinations
                    context_features = np.random.choice(features, size=min(3, features), replace=False)
                    for cf in context_features:
                        mask = X_attack_type[:, cf] > normal_mean
                        X_attack_type[mask, cf] *= (1 + anomaly_factor)
                        
                else:
                    logger.warning(f"Unknown attack type '{attack_type}', using high_variance")
                    X_attack_type = np.random.normal(
                        normal_mean, 
                        normal_std * anomaly_factor * 2, 
                        (n_samples, features)
                    )
                
                # Clip to valid range
                X_attack_type = np.clip(X_attack_type, 0.0, 1.0)
                X_attack_parts.append(X_attack_type)
                
                progress_data['samples_generated'] += n_samples
                progress_data['attack_types_processed'] += 1
                main_bar.text = f"Generated {n_samples} {attack_type} samples ({i+1}/{len(attack_types)})"
            
            # Combine all attack data
            main_bar.text = "Combining all attack data..."
            X_attack = np.vstack(X_attack_parts)
            generation_stats['attack_type_counts'] = attack_type_counts
            
            main_bar.text = f"Generated {attack_samples} attack samples across {len(attack_types)} types"
            generation_stats['stages_completed'].append('attack_data_generation')
            main_bar()
            
            # STAGE 5: Feature Correlation
            progress_data['current_stage'] = "Feature Engineering"
            main_bar.text = "Applying feature correlation structure..."
            
            # Add correlation structure if requested
            if correlated_features and features > 1:
                logger.info("Adding correlation structure to normal data")
                
                correlation_strength = normal_config.get('normal_correlation', 0.3)
                
                if correlation_matrix is not None and correlation_matrix.shape == (features, features):
                    # Use provided correlation matrix
                    main_bar.text = "Applying provided correlation matrix..."
                    L = np.linalg.cholesky(correlation_matrix)
                    X_normal = X_normal @ L.T
                else:
                    # Create simple correlation structure
                    main_bar.text = "Creating correlation structure..."
                    correlation_indices = np.random.choice(features, size=min(features//2, 5), replace=False)
                    for i in range(len(correlation_indices)-1):
                        idx1, idx2 = correlation_indices[i], correlation_indices[i+1]
                        X_normal[:, idx2] = (X_normal[:, idx2] * (1 - correlation_strength) + 
                                           X_normal[:, idx1] * correlation_strength)
                
                # Renormalize
                main_bar.text = "Renormalizing correlated data..."
                X_normal = np.clip(X_normal, 0.0, 1.0)
                generation_stats['correlation_applied'] = True
            
            main_bar.text = "Feature engineering complete"
            generation_stats['stages_completed'].append('feature_engineering')
            main_bar()
            
            # STAGE 6: Noise Addition
            progress_data['current_stage'] = "Noise Addition"
            main_bar.text = f"Adding {noise_type} noise..."
            
            # Add noise if requested
            if noise_level > 0:
                logger.info(f"Adding {noise_type} noise with level {noise_level}")
                
                if noise_type == 'gaussian':
                    noise_std = noise_config.get('gaussian_noise_std', noise_level)
                    main_bar.text = f"Adding Gaussian noise (std={noise_std:.3f})..."
                    normal_noise = np.random.normal(0, noise_std, X_normal.shape)
                    attack_noise = np.random.normal(0, noise_std, X_attack.shape)
                    
                    X_normal = X_normal + normal_noise
                    X_attack = X_attack + attack_noise
                    
                elif noise_type == 'uniform':
                    noise_range = noise_config.get('uniform_noise_range', noise_level)
                    main_bar.text = f"Adding uniform noise (range=±{noise_range:.3f})..."
                    normal_noise = np.random.uniform(-noise_range, noise_range, X_normal.shape)
                    attack_noise = np.random.uniform(-noise_range, noise_range, X_attack.shape)
                    
                    X_normal = X_normal + normal_noise
                    X_attack = X_attack + attack_noise
                    
                elif noise_type == 'salt_pepper':
                    sp_ratio = noise_config.get('salt_pepper_ratio', 0.5)
                    main_bar.text = f"Adding salt & pepper noise (ratio={sp_ratio:.2f})..."
                    
                    # Apply salt and pepper noise
                    for X_data in [X_normal, X_attack]:
                        noise_mask = np.random.random(X_data.shape) < noise_level
                        salt_mask = noise_mask & (np.random.random(X_data.shape) < sp_ratio)
                        pepper_mask = noise_mask & ~salt_mask
                        
                        X_data[salt_mask] = 1.0
                        X_data[pepper_mask] = 0.0
                
                # Ensure values remain in valid range
                main_bar.text = "Clipping noisy data to [0,1] range..."
                X_normal = np.clip(X_normal, 0.0, 1.0)
                X_attack = np.clip(X_attack, 0.0, 1.0)
                generation_stats['noise_applied'] = True
                generation_stats['noise_type'] = noise_type
                generation_stats['noise_level'] = noise_level
            
            main_bar.text = "Noise addition complete"
            generation_stats['stages_completed'].append('noise_addition')
            main_bar()
            
            # STAGE 7: Feature Scaling
            progress_data['current_stage'] = "Feature Scaling"
            main_bar.text = "Applying feature scaling..."
            
            # Apply feature scaling if requested
            scaler = None
            if feature_scaling:
                logger.info(f"Applying {feature_scaling} scaling")
                main_bar.text = f"Applying {feature_scaling} scaling..."
                
                if feature_scaling == 'standard':
                    scaler = StandardScaler()
                elif feature_scaling == 'minmax':
                    scaler = MinMaxScaler()
                else:
                    logger.warning(f"Unknown scaling method '{feature_scaling}'")
                    scaler = MinMaxScaler()
                
                # Fit on normal data and transform both
                X_normal = scaler.fit_transform(X_normal)
                X_attack = scaler.transform(X_attack)
                generation_stats['scaling_applied'] = True
                generation_stats['scaling_method'] = feature_scaling
            
            main_bar.text = "Feature scaling complete"
            generation_stats['stages_completed'].append('feature_scaling')
            main_bar()
            
            # STAGE 8: Data Splitting
            progress_data['current_stage'] = "Data Splitting"
            main_bar.text = "Splitting data into train/validation/test sets..."
            
            # Create labels
            y_normal = np.zeros(normal_samples, dtype=np.int32)
            y_attack = np.ones(attack_samples, dtype=np.int32)
            
            # Perform data splitting
            logger.info(f"Splitting data: validation={validation_split}, test={test_split}")
            
            # Split normal data for training and validation
            if validation_split > 0:
                main_bar.text = f"Splitting normal data (validation={validation_split:.1%})..."
                X_train_normal, X_val_normal, y_train_normal, y_val_normal = train_test_split(
                    X_normal, y_normal,
                    test_size=validation_split,
                    random_state=random_state,
                    shuffle=shuffle
                )
            else:
                X_train_normal = X_normal
                X_val_normal = np.array([]).reshape(0, features)
                y_train_normal = y_normal
                y_val_normal = np.array([])
            
            # Handle test data
            if test_split > 0 and test_split <= 1.0:
                if test_split < 1.0:
                    main_bar.text = f"Splitting attack data (test={test_split:.1%})..."
                    test_size = int(len(X_attack) * test_split)
                    indices = np.random.choice(len(X_attack), size=test_size, replace=False)
                    X_test = X_attack[indices]
                    y_test = y_attack[indices]
                else:
                    main_bar.text = "Using all attack data for testing..."
                    X_test = X_attack
                    y_test = y_attack
            else:
                X_test = np.array([]).reshape(0, features)
                y_test = np.array([])
            
            generation_stats.update({
                'train_samples': len(X_train_normal),
                'val_samples': len(X_val_normal),
                'test_samples': len(X_test)
            })
            
            main_bar.text = "Data splitting complete"
            generation_stats['stages_completed'].append('data_splitting')
            main_bar()
            
            # STAGE 9: Data Validation
            progress_data['current_stage'] = "Data Validation"
            main_bar.text = "Validating generated data quality..."
            
            # Data quality validation if requested
            validation_results = {}
            quality_score = 0.5  # Default quality score
            
            if final_config.get('validation', {}).get('validate_generation', True):
                logger.info("Performing data quality validation")
                
                # Basic statistics
                validation_results['normal_data_stats'] = {
                    'mean': np.mean(X_train_normal, axis=0).tolist(),
                    'std': np.std(X_train_normal, axis=0).tolist(),
                    'min': np.min(X_train_normal, axis=0).tolist(),
                    'max': np.max(X_train_normal, axis=0).tolist()
                }
                
                if len(X_test) > 0:
                    validation_results['attack_data_stats'] = {
                        'mean': np.mean(X_test, axis=0).tolist(),
                        'std': np.std(X_test, axis=0).tolist(),
                        'min': np.min(X_test, axis=0).tolist(),
                        'max': np.max(X_test, axis=0).tolist()
                    }
                
                # Statistical tests if requested
                if final_config.get('validation', {}).get('statistical_tests', False):
                    logger.info("Performing statistical tests")
                    main_bar.text = "Running statistical tests..."
                    
                    validation_results['statistical_tests'] = {}
                    
                    # Normality tests on normal data
                    # Test first 5 features
                    for i in range(min(5, features)):
                        feature_data = X_train_normal[:, i]
                        if len(feature_data) > 3:
                            try:
                                stat, p_value = stats.normaltest(feature_data)
                                validation_results['statistical_tests'][f'normality_feature_{i}'] = {
                                    'statistic': float(stat),
                                    'p_value': float(p_value),
                                    'is_normal': p_value > 0.05
                                }
                            except Exception as e:
                                logger.warning(f"Normality test failed for feature {i}: {e}")
                    
                    # Separation test
                    if len(X_test) > 0:
                        try:
                            # Simple separation measure
                            normal_centroid = np.mean(X_train_normal, axis=0)
                            attack_centroid = np.mean(X_test, axis=0)
                            separation = np.linalg.norm(normal_centroid - attack_centroid)
                            
                            validation_results['statistical_tests']['class_separation'] = {
                                'euclidean_distance': float(separation),
                                'normalized_distance': float(separation / np.sqrt(features))
                            }
                        except Exception as e:
                            logger.warning(f"Separation test failed: {e}")
                
                # Calculate data quality score
                main_bar.text = "Calculating data quality metrics..."
                quality_metrics = []
                
                # Sample adequacy
                sample_score = min(1.0, (len(X_train_normal) + len(X_test)) / max(1000, normal_samples + attack_samples))
                quality_metrics.append(('sample_adequacy', sample_score))
                
                # Feature quality
                feature_score = min(1.0, features / max(10, features))
                quality_metrics.append(('feature_quality', feature_score))
                
                # Class separation (if test data exists)
                if len(X_test) > 0:
                    try:
                        separation = np.linalg.norm(np.mean(X_train_normal, axis=0) - np.mean(X_test, axis=0))
                        separation_score = min(1.0, separation / np.sqrt(features))
                        quality_metrics.append(('class_separation', separation_score))
                    except:
                        quality_metrics.append(('class_separation', 0.5))
                else:
                    quality_metrics.append(('class_separation', 0.5))
                
                # Overall quality score
                quality_score = sum(score for _, score in quality_metrics) / len(quality_metrics)
                progress_data['data_quality_score'] = quality_score
                progress_data['generation_quality'] = quality_score
                validation_results['quality_score'] = quality_score
                validation_results['quality_metrics'] = dict(quality_metrics)
            
            main_bar.text = f"Data validation complete (Quality: {quality_score:.3f})"
            generation_stats['stages_completed'].append('data_validation')
            main_bar()
            
            # STAGE 10: Finalization
            progress_data['current_stage'] = "Finalization"
            main_bar.text = "Finalizing data preparation and metadata..."
            
            # Convert to specified data type
            if data_type == 'float32':
                main_bar.text = "Converting to float32..."
                X_train_normal = X_train_normal.astype(np.float32)
                X_val_normal = X_val_normal.astype(np.float32)
                X_test = X_test.astype(np.float32)
            elif data_type == 'float64':
                main_bar.text = "Converting to float64..."
                X_train_normal = X_train_normal.astype(np.float64)
                X_val_normal = X_val_normal.astype(np.float64)
                X_test = X_test.astype(np.float64)
            
            # Prepare comprehensive metadata
            total_time = (datetime.now() - start_time).total_seconds()
            
            metadata = {
                # Basic generation info
                'generation_time_seconds': total_time,
                'generation_method': generation_method,
                'distribution_type': distribution_type,
                'random_state': random_state,
                
                # Sample statistics
                'normal_samples': normal_samples,
                'attack_samples': attack_samples,
                'total_samples': normal_samples + attack_samples,
                'features': features,
                'feature_names': feature_names,
                
                # Data splits
                'train_samples': len(X_train_normal),
                'val_samples': len(X_val_normal),
                'test_samples': len(X_test),
                'validation_split': validation_split,
                'test_split': test_split,
                
                # Generation parameters
                'anomaly_factor': anomaly_factor,
                'noise_level': noise_level,
                'noise_type': noise_type,
                'attack_types': attack_types,
                'attack_type_counts': generation_stats.get('attack_type_counts', {}),
                
                # Quality metrics
                'class_balance_ratio': class_balance_ratio,
                'correlated_features': correlated_features,
                'feature_scaling': feature_scaling,
                'data_quality_score': quality_score,
                
                # Configuration applied
                'config_applied': final_config,
                'generation_stats': generation_stats,
                
                # Validation results
                'validation_results': validation_results,
                
                # Data characteristics
                'data_bounds': (0.0, 1.0),
                'label_column': label_column,
                'data_type': data_type,
                'output_format': output_format,
                
                # Progress tracking summary
                'progress_summary': {
                    'stages_completed': len(generation_stats['stages_completed']),
                    'total_samples_generated': progress_data['samples_generated'],
                    'features_created': progress_data['features_created'],
                    'attack_types_processed': progress_data['attack_types_processed'],
                    'final_quality_score': progress_data['data_quality_score']
                }
            }
            
            main_bar.text = "Finalization complete"
            generation_stats['stages_completed'].append('finalization')
            main_bar()
        
        # Prepare output data structure
        if output_format == 'dict':
            result = {
                "X_train": X_train_normal,
                "X_val": X_val_normal,
                "X_test": X_test,
                "y_train": y_train_normal,
                "y_val": y_val_normal,
                "y_test": y_test,
                "feature_names": feature_names,
                "metadata": metadata
            }
            
        elif output_format == 'dataframe':
            # Create DataFrames
            main_bar.text = "Creating DataFrames..." if 'main_bar' in locals() else None
            train_df = pd.DataFrame(X_train_normal, columns=feature_names)
            train_df[label_column] = y_train_normal
            
            val_df = pd.DataFrame(X_val_normal, columns=feature_names)
            val_df[label_column] = y_val_normal
            
            test_df = pd.DataFrame(X_test, columns=feature_names)
            test_df[label_column] = y_test
            
            result = {
                "train_df": train_df,
                "val_df": val_df,
                "test_df": test_df,
                "metadata": metadata
            }
            
        elif output_format == 'sklearn':
            # sklearn-compatible format
            main_bar.text = "Creating sklearn-compatible format..." if 'main_bar' in locals() else None
            X_combined = np.vstack([X_train_normal, X_val_normal, X_test]) if len(X_val_normal) > 0 and len(X_test) > 0 else X_train_normal
            y_combined = np.hstack([y_train_normal, y_val_normal, y_test]) if len(y_val_normal) > 0 and len(y_test) > 0 else y_train_normal
            
            result = {
                "data": X_combined,
                "target": y_combined,
                "feature_names": feature_names,
                "target_names": ['normal', 'attack'],
                "DESCR": f"Synthetic dataset with {normal_samples} normal and {attack_samples} attack samples",
                "splits": {
                    "X_train": X_train_normal,
                    "X_val": X_val_normal,
                    "X_test": X_test,
                    "y_train": y_train_normal,
                    "y_val": y_val_normal,
                    "y_test": y_test
                },
                "metadata": metadata
            }
            
        else:
            raise ValueError(f"Unknown output format: {output_format}")
        
        # Save data if requested
        export_config = final_config.get('export', {})
        if export_config.get('save_data', False):
            output_path = export_config.get('output_path', 'synthetic_data.csv')
            file_format = export_config.get('file_format', 'csv')
            
            logger.info(f"Saving synthetic data to {output_path}")
            
            try:
                output_path = Path(output_path)
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                if file_format == 'csv':
                    # Save as CSV
                    main_bar.text = "Saving as CSV..." if 'main_bar' in locals() else None
                    combined_X = np.vstack([X_train_normal, X_val_normal, X_test])
                    combined_y = np.hstack([y_train_normal, y_val_normal, y_test])
                    
                    df_combined = pd.DataFrame(combined_X, columns=feature_names)
                    df_combined[label_column] = combined_y
                    df_combined.to_csv(output_path, index=False)
                    
                elif file_format == 'pickle':
                    # Save as pickle
                    main_bar.text = "Saving as pickle..." if 'main_bar' in locals() else None
                    with open(output_path, 'wb') as f:
                        pickle.dump(result, f)
                        
                elif file_format == 'numpy':
                    # Save as numpy arrays
                    main_bar.text = "Saving as numpy arrays..." if 'main_bar' in locals() else None
                    np.savez(
                        output_path,
                        X_train=X_train_normal,
                        X_val=X_val_normal,
                        X_test=X_test,
                        y_train=y_train_normal,
                        y_val=y_val_normal,
                        y_test=y_test,
                        feature_names=feature_names,
                        metadata=metadata
                    )
                
                # Save metadata if requested
                if export_config.get('metadata_file', False):
                    metadata_path = output_path.with_suffix('.json')
                    main_bar.text = "Saving metadata..." if 'main_bar' in locals() else None
                    with open(metadata_path, 'w') as f:
                        # Make metadata JSON serializable
                        serializable_metadata = {}
                        for key, value in metadata.items():
                            try:
                                # Test if serializable
                                json.dumps(value)
                                serializable_metadata[key] = value
                            except TypeError:
                                serializable_metadata[key] = str(value)
                        
                        json.dump(serializable_metadata, f, indent=2)
                    
                    logger.info(f"Saved metadata to {metadata_path}")
                
            except Exception as e:
                logger.error(f"Failed to save data: {e}")
        
        # Log comprehensive summary
        generation_stats['completion_status'] = 'success'
        generation_stats['total_processing_time'] = total_time
        
        logger.info("=" * 80)
        logger.info("SYNTHETIC DATA GENERATION SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Generation method: {generation_method}")
        logger.info(f"Total samples: {normal_samples + attack_samples:,} ({normal_samples:,} normal, {attack_samples:,} attack)")
        logger.info(f"Features: {features} ({data_type})")
        logger.info(f"Attack types: {len(attack_types)} types")
        logger.info(f"Train/Val/Test split: {len(X_train_normal)}/{len(X_val_normal)}/{len(X_test)}")
        logger.info(f"Anomaly factor: {anomaly_factor}")
        logger.info(f"Noise level: {noise_level} ({noise_type})")
        logger.info(f"Feature scaling: {feature_scaling or 'none'}")
        logger.info(f"Correlated features: {correlated_features}")
        logger.info(f"Data quality score: {quality_score:.3f}")
        logger.info(f"Output format: {output_format}")
        logger.info(f"Generation time: {total_time:.2f} seconds")
        logger.info(f"Stages completed: {len(generation_stats['stages_completed'])}/{total_stages}")
        logger.info("=" * 80)
        
        # Restore original logging level
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        return result
        
    except Exception as e:
        # Update generation stats with error information
        generation_stats['completion_status'] = 'failed'
        generation_stats['error_message'] = str(e)
        generation_stats['error_traceback'] = traceback.format_exc()
        
        # Restore original logging level on error
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        error_msg = f"Synthetic data generation failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Provide helpful error context
        logger.error(f"Generation parameters: normal={normal_samples}, attack={attack_samples}, features={features}")
        logger.error(f"Configuration used: {final_config}")
        logger.error(f"Stages completed: {generation_stats['stages_completed']}")
        
        raise RuntimeError(error_msg)

def validate_synthetic_data(
    data_dict: Dict[str, Union[np.ndarray, Dict[str, Any]]],
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Perform comprehensive validation of synthetic data quality and characteristics.
    
    Args:
        data_dict: Data dictionary from generate_synthetic_data
        config: Configuration dictionary
        
    Returns:
        Dictionary with validation results and quality metrics
    """
    validation_results = {
        'passed': True,
        'warnings': [],
        'errors': [],
        'quality_metrics': {},
        'statistical_tests': {},
        'recommendations': []
    }
    
    try:
        # Extract data components
        X_train = data_dict.get('X_train', np.array([]))
        X_val = data_dict.get('X_val', np.array([]))
        X_test = data_dict.get('X_test', np.array([]))
        metadata = data_dict.get('metadata', {})
        
        # Basic data presence checks
        if X_train.size == 0:
            validation_results['errors'].append("No training data found")
            validation_results['passed'] = False
            return validation_results
        
        # Shape consistency checks
        expected_features = X_train.shape[1]
        for name, data in [('validation', X_val), ('test', X_test)]:
            if data.size > 0 and data.shape[1] != expected_features:
                validation_results['errors'].append(
                    f"{name} data has {data.shape[1]} features, expected {expected_features}"
                )
                validation_results['passed'] = False
        
        # Data quality checks
        for name, data in [('train', X_train), ('val', X_val), ('test', X_test)]:
            if data.size > 0:
                # Check for invalid values
                if np.any(np.isnan(data)):
                    validation_results['warnings'].append(f"{name} data contains NaN values")
                
                if np.any(np.isinf(data)):
                    validation_results['warnings'].append(f"{name} data contains infinite values")
                
                # Check data ranges
                data_min, data_max = np.min(data), np.max(data)
                validation_results['quality_metrics'][f'{name}_range'] = (float(data_min), float(data_max))
                
                if data_min < -10 or data_max > 10:
                    validation_results['warnings'].append(f"{name} data has extreme values")
        
        # Statistical validation
        if len(X_train) > 10:
            # Distribution tests
            # Test first 3 features
            for i in range(min(3, X_train.shape[1])):
                feature_data = X_train[:, i]
                
                # Normality test
                try:
                    stat, p_value = stats.normaltest(feature_data)
                    validation_results['statistical_tests'][f'normality_feature_{i}'] = {
                        'statistic': float(stat),
                        'p_value': float(p_value)
                    }
                except Exception as e:
                    validation_results['warnings'].append(f"Statistical test failed for feature {i}: {e}")
        
        # Class separation analysis
        if X_test.size > 0:
            try:
                train_centroid = np.mean(X_train, axis=0)
                test_centroid = np.mean(X_test, axis=0)
                separation = np.linalg.norm(train_centroid - test_centroid)
                
                validation_results['quality_metrics']['class_separation'] = float(separation)
                validation_results['quality_metrics']['normalized_separation'] = float(separation / np.sqrt(expected_features))
                
                if separation < 0.1:
                    validation_results['warnings'].append("Low class separation detected")
                    validation_results['recommendations'].append("Consider increasing anomaly_factor")
                
            except Exception as e:
                validation_results['warnings'].append(f"Class separation analysis failed: {e}")
        
        # Metadata validation
        expected_samples = metadata.get('normal_samples', 0) + metadata.get('attack_samples', 0)
        actual_samples = len(X_train) + len(X_val) + len(X_test)
        
        if abs(expected_samples - actual_samples) > 1:
            validation_results['warnings'].append(
                f"Sample count mismatch: expected {expected_samples}, got {actual_samples}"
            )
        
        # Calculate overall quality score
        n_warnings = len(validation_results['warnings'])
        n_errors = len(validation_results['errors'])
        quality_score = max(0.0, 1.0 - (n_warnings * 0.1 + n_errors * 0.5))
        validation_results['quality_metrics']['overall_quality_score'] = quality_score
        
        if quality_score < 0.7:
            validation_results['recommendations'].append("Consider regenerating data with different parameters")
        
        return validation_results
        
    except Exception as e:
        validation_results['errors'].append(f"Validation process failed: {str(e)}")
        validation_results['passed'] = False
        validation_results['quality_metrics']['overall_quality_score'] = 0.0
        return validation_results

def create_synthetic_data_pipeline(
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Create a comprehensive synthetic data generation pipeline based on configuration.
    
    Args:
        config: Complete configuration dictionary
        
    Returns:
        Synthetic data pipeline configuration
    """
    pipeline_config = {
        'steps': [],
        'parameters': {},
        'validation_rules': {},
        'quality_checks': {}
    }
    
    # Extract relevant configuration sections
    core_config = config.get('core_generation', {})
    generation_config = config.get('generation_methods', {})
    quality_config = config.get('quality_control', {})
    
    # Build pipeline steps
    pipeline_steps = []
    
    # Step 1: Parameter validation
    pipeline_steps.append({
        'name': 'parameter_validation',
        'function': 'validate_generation_parameters',
        'parameters': core_config
    })
    
    # Step 2: Data generation
    pipeline_steps.append({
        'name': 'data_generation',
        'function': 'generate_base_data',
        'parameters': generation_config
    })
    
    # Step 3: Anomaly injection
    pipeline_steps.append({
        'name': 'anomaly_injection',
        'function': 'inject_anomalies',
        'parameters': config.get('anomaly_config', {})
    })
    
    # Step 4: Quality control
    if quality_config:
        pipeline_steps.append({
            'name': 'quality_control',
            'function': 'apply_quality_control',
            'parameters': quality_config
        })
    
    # Step 5: Data splitting
    pipeline_steps.append({
        'name': 'data_splitting',
        'function': 'split_synthetic_data',
        'parameters': config.get('data_splitting', {})
    })
    
    # Step 6: Validation
    pipeline_steps.append({
        'name': 'validation',
        'function': 'validate_synthetic_data',
        'parameters': config.get('validation', {})
    })
    
    pipeline_config['steps'] = pipeline_steps
    pipeline_config['parameters'] = config
    
    return pipeline_config

class EnhancedCollateFn:
    """
    Picklable collate function for DataLoader multiprocessing support.
    
    This class replaces the nested function approach to ensure compatibility
    with multiprocessing workers that require picklable objects.
    """
    
    def __init__(self, config=None, dtype=torch.float32, error_handling='graceful'):
        """
        Initialize the enhanced collate function with configuration.
        
        Args:
            config: Configuration dictionary containing processing parameters
            dtype: Target PyTorch data type for tensors
            error_handling: Error handling strategy ('strict' or 'graceful')
        """
        self.config = config or {}
        self.dtype = dtype
        self.error_handling = error_handling
        
        # Extract advanced features configuration
        advanced_config = self.config.get('advanced_features', {})
        self.variable_length_sequences = advanced_config.get('variable_length_sequences', False)
        self.max_sequence_length = advanced_config.get('max_sequence_length', 512)
        self.custom_collate = advanced_config.get('custom_collate')
        
        # Extract data format configuration
        data_format_config = self.config.get('data_format', {})
        self.squeeze_dims = data_format_config.get('squeeze_dims', False)
        self.unsqueeze_dims = data_format_config.get('unsqueeze_dims')
        
        # Store base collate function reference
        if self.custom_collate and callable(self.custom_collate):
            self.base_collate = self.custom_collate
        else:
            self.base_collate = default_collate
    
    def __call__(self, batch):
        """
        Process a batch of data with enhanced collation.
        
        Args:
            batch: List of samples to collate
            
        Returns:
            Collated batch tensor or tuple of tensors
        """
        try:
            # Apply base collation
            if self.base_collate == default_collate:
                collated = default_collate(batch)
            else:
                try:
                    collated = self.base_collate(batch)
                except Exception as e:
                    logger.warning(f"Custom collate function failed: {e}, using default")
                    collated = default_collate(batch)
            
            # Apply variable length sequence handling
            if self.variable_length_sequences:
                collated = self._handle_variable_sequences(collated)
            
            # Apply data type conversion and dimension adjustments
            collated = self._process_tensor_format(collated)
            
            return collated
            
        except Exception as e:
            if self.error_handling == 'strict':
                raise RuntimeError(f"Enhanced collate function failed: {e}")
            else:
                logger.warning(f"Collate function error, using default: {e}")
                return default_collate(batch)
    
    def _handle_variable_sequences(self, collated):
        """Handle variable length sequences by truncating to max length."""
        if isinstance(collated, torch.Tensor) and collated.dim() > 1:
            if collated.size(1) > self.max_sequence_length:
                collated = collated[:, :self.max_sequence_length]
        elif isinstance(collated, (tuple, list)) and len(collated) >= 2:
            processed = []
            for item in collated:
                if isinstance(item, torch.Tensor) and item.dim() > 1:
                    if item.size(1) > self.max_sequence_length:
                        item = item[:, :self.max_sequence_length]
                processed.append(item)
            collated = tuple(processed) if isinstance(collated, tuple) else processed
        
        return collated
    
    def _process_tensor_format(self, collated):
        """Apply dtype conversion and dimension adjustments."""
        if isinstance(collated, torch.Tensor):
            # Apply dtype conversion
            collated = collated.to(dtype=self.dtype)
            
            # Apply dimension adjustments
            if self.squeeze_dims:
                collated = collated.squeeze()
            
            if self.unsqueeze_dims:
                for dim in self.unsqueeze_dims:
                    collated = collated.unsqueeze(dim)
                    
        elif isinstance(collated, (tuple, list)) and len(collated) >= 2:
            # Handle tuple of tensors (input, target)
            processed = []
            for i, item in enumerate(collated):
                if isinstance(item, torch.Tensor):
                    # Apply dtype conversion
                    item = item.to(dtype=self.dtype)
                    
                    # Apply dimension adjustments
                    if self.squeeze_dims:
                        item = item.squeeze()
                    
                    if self.unsqueeze_dims:
                        for dim in self.unsqueeze_dims:
                            item = item.unsqueeze(dim)
                
                processed.append(item)
            
            collated = tuple(processed) if isinstance(collated, tuple) else processed
        
        return collated
    
    def __getstate__(self):
        """Custom pickling to handle non-serializable objects."""
        state = self.__dict__.copy()
        
        # Handle custom_collate function that might not be picklable
        if 'base_collate' in state and state['base_collate'] != default_collate:
            # Store a reference to the custom function name if possible
            if hasattr(state['base_collate'], '__name__'):
                state['custom_collate_name'] = state['base_collate'].__name__
            else:
                state['custom_collate_name'] = None
            # Remove the actual function reference
            state['base_collate'] = default_collate
        
        return state
    
    def __setstate__(self, state):
        """Custom unpickling to restore the object state."""
        self.__dict__.update(state)
        
        # Restore custom collate function if it was stored
        if hasattr(self, 'custom_collate_name') and self.custom_collate_name:
            # Try to restore the custom function (this is a best-effort approach)
            # In practice, you might need to register custom functions in a global registry
            if self.custom_collate and callable(self.custom_collate):
                self.base_collate = self.custom_collate
            else:
                self.base_collate = default_collate

class WorkerInitializer:
    """
    Picklable worker initialization class for DataLoader multiprocessing support.
    """
    
    def __init__(self, config=None):
        """
        Initialize the worker initializer with configuration.
        
        Args:
            config: Configuration dictionary containing worker parameters
        """
        self.config = config or {}
        
        # Extract configuration parameters
        self.shuffle_seed = self.config.get('sampling', {}).get('shuffle_seed')
        self.cpu_affinity = self.config.get('performance', {}).get('cpu_affinity')
        self.worker_memory_limit = self.config.get('memory_management', {}).get('worker_memory_limit')
        self.custom_worker_init_fn = self.config.get('core', {}).get('worker_init_fn')
        self.verbose = self.config.get('monitoring', {}).get('verbose', False)
    
    def __call__(self, worker_id):
        """
        Initialize a worker process.
        
        Args:
            worker_id: ID of the worker process
        """
        try:
            # Set worker-specific random seeds for reproducibility
            if self.shuffle_seed is not None:
                np.random.seed(self.shuffle_seed + worker_id)
                torch.manual_seed(self.shuffle_seed + worker_id)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(self.shuffle_seed + worker_id)
            
            # Apply CPU affinity if specified
            if self.cpu_affinity:
                try:
                    if isinstance(self.cpu_affinity, list) and self.cpu_affinity:
                        cpu_id = self.cpu_affinity[worker_id % len(self.cpu_affinity)]
                        psutil.Process().cpu_affinity([cpu_id])
                        if self.verbose:
                            print(f"Worker {worker_id} bound to CPU {cpu_id}")
                except Exception as e:
                    if self.verbose:
                        print(f"Failed to set CPU affinity for worker {worker_id}: {e}")
            
            # Memory management for workers
            if self.worker_memory_limit:
                try:
                    # Convert MB to bytes
                    memory_limit = self.worker_memory_limit * 1024 * 1024
                    import resource
                    resource.setrlimit(resource.RLIMIT_AS, (memory_limit, memory_limit))
                    if self.verbose:
                        print(f"Worker {worker_id} memory limit set to {self.worker_memory_limit}MB")
                except Exception as e:
                    if self.verbose:
                        print(f"Failed to set memory limit for worker {worker_id}: {e}")
            
            # Apply any custom worker initialization
            if self.custom_worker_init_fn and callable(self.custom_worker_init_fn):
                try:
                    self.custom_worker_init_fn(worker_id)
                except Exception as e:
                    if self.verbose:
                        print(f"Custom worker init failed for worker {worker_id}: {e}")
            
            if self.verbose:
                print(f"Initialized worker {worker_id}")
                
        except Exception as e:
            # Don't let worker initialization failures crash the process
            print(f"Worker {worker_id} initialization failed: {e}")

def create_enhanced_collate_fn(config=None, dtype=torch.float32, error_handling='graceful'):
    """
    Create a picklable enhanced collate function.
    
    This function creates an instance of the EnhancedCollateFn class that can be
    properly pickled for multiprocessing DataLoader workers.
    
    Args:
        config: Configuration dictionary containing processing parameters
        dtype: Target PyTorch data type for tensors
        error_handling: Error handling strategy ('strict' or 'graceful')
        
    Returns:
        EnhancedCollateFn instance that can be used as a collate function
    """
    # Extract configuration from current context if available
    final_config = config or {}
    
    # Try to get configuration from local context (if called from within create_dataloaders)
    import inspect
    frame = inspect.currentframe()
    try:
        # Look for configuration variables in the calling frame
        caller_locals = frame.f_back.f_locals if frame.f_back else {}
        
        # Extract relevant config variables from the calling context
        if 'advanced_config' in caller_locals:
            final_config.setdefault('advanced_features', {}).update(caller_locals['advanced_config'])
        
        if 'data_format_config' in caller_locals:
            final_config.setdefault('data_format', {}).update(caller_locals['data_format_config'])
        
        if 'error_handling' in caller_locals:
            error_handling = caller_locals['error_handling']
            
        if 'dtype' in caller_locals:
            dtype = caller_locals['dtype']
            
    except Exception as e:
        logger.debug(f"Could not extract context from calling frame: {e}")
    finally:
        del frame
    
    # Create and return the picklable collate function
    return EnhancedCollateFn(
        config=final_config,
        dtype=dtype,
        error_handling=error_handling
    )

def create_dataloaders(
    # Core Data Parameters
    data: Optional[Dict[str, np.ndarray]] = None,
    X_train: Optional[np.ndarray] = None,
    X_val: Optional[np.ndarray] = None,
    X_test: Optional[np.ndarray] = None,
    y_train: Optional[np.ndarray] = None,
    y_val: Optional[np.ndarray] = None,
    y_test: Optional[np.ndarray] = None,
    
    # Core DataLoader Parameters
    batch_size: Optional[int] = None,
    shuffle: Optional[bool] = None,
    num_workers: Optional[int] = None,
    pin_memory: Optional[bool] = None,
    drop_last: Optional[bool] = None,
    timeout: Optional[float] = None,
    worker_init_fn: Optional[Callable] = None,
    multiprocessing_context: Optional[str] = None,
    generator: Optional[torch.Generator] = None,
    prefetch_factor: Optional[int] = None,
    persistent_workers: Optional[bool] = None,
    
    # Batch Size Configuration
    train_batch_size: Optional[int] = None,
    val_batch_size: Optional[int] = None,
    test_batch_size: Optional[int] = None,
    eval_batch_size: Optional[int] = None,
    dynamic_batch_sizing: Optional[bool] = None,
    adaptive_batch_size: Optional[bool] = None,
    max_batch_size: Optional[int] = None,
    min_batch_size: Optional[int] = None,
    
    # Shuffling and Sampling
    train_shuffle: Optional[bool] = None,
    val_shuffle: Optional[bool] = None,
    test_shuffle: Optional[bool] = None,
    sampler: Optional[torch.utils.data.Sampler] = None,
    batch_sampler: Optional[torch.utils.data.BatchSampler] = None,
    shuffle_seed: Optional[int] = None,
    stratified_sampling: Optional[bool] = None,
    weighted_sampling: Optional[bool] = None,
    sample_weights: Optional[np.ndarray] = None,
    
    # Performance Optimization
    optimization_level: Optional[str] = None,
    memory_efficient: Optional[bool] = None,
    cpu_count: Optional[int] = None,
    max_workers: Optional[int] = None,
    worker_memory_limit: Optional[int] = None,
    dataloader_optimization: Optional[bool] = None,
    fast_dev_run: Optional[bool] = None,
    benchmark_mode: Optional[bool] = None,
    
    # Data Processing and Augmentation
    data_transforms: Optional[List[Callable]] = None,
    train_transforms: Optional[List[Callable]] = None,
    val_transforms: Optional[List[Callable]] = None,
    test_transforms: Optional[List[Callable]] = None,
    augmentation_pipeline: Optional[List[Callable]] = None,
    normalize_data: Optional[bool] = None,
    standardize_data: Optional[bool] = None,
    
    # Data Type and Format Parameters
    dtype: Optional[torch.dtype] = None,
    device: Optional[str] = None,
    tensor_format: Optional[str] = None,
    data_format: Optional[str] = None,
    squeeze_dims: Optional[bool] = None,
    unsqueeze_dims: Optional[List[int]] = None,
    transpose_dims: Optional[List[int]] = None,
    
    # Memory Management
    memory_management: Optional[str] = None,
    shared_memory: Optional[bool] = None,
    mmap_mode: Optional[str] = None,
    cache_datasets: Optional[bool] = None,
    preload_data: Optional[bool] = None,
    lazy_loading: Optional[bool] = None,
    memory_mapping: Optional[bool] = None,
    gc_collection: Optional[bool] = None,
    
    # Validation and Quality Control
    validate_data: Optional[bool] = None,
    check_data_integrity: Optional[bool] = None,
    handle_nan_values: Optional[str] = None,
    handle_inf_values: Optional[str] = None,
    data_consistency_checks: Optional[bool] = None,
    shape_validation: Optional[bool] = None,
    dtype_validation: Optional[bool] = None,
    
    # Distributed and Parallel Processing
    distributed: Optional[bool] = None,
    world_size: Optional[int] = None,
    rank: Optional[int] = None,
    local_rank: Optional[int] = None,
    distributed_sampler: Optional[bool] = None,
    ddp_backend: Optional[str] = None,
    sync_batchnorm: Optional[bool] = None,
    
    # Advanced Features
    collate_fn: Optional[Callable] = None,
    custom_collate: Optional[Callable] = None,
    variable_length_sequences: Optional[bool] = None,
    padding_strategy: Optional[str] = None,
    sequence_length: Optional[int] = None,
    max_sequence_length: Optional[int] = None,
    
    # Monitoring and Debugging
    profile_dataloaders: Optional[bool] = None,
    benchmark_dataloaders: Optional[bool] = None,
    dataloader_stats: Optional[bool] = None,
    timing_analysis: Optional[bool] = None,
    memory_profiling: Optional[bool] = None,
    bottleneck_detection: Optional[bool] = None,
    verbose: Optional[bool] = None,
    debug_mode: Optional[bool] = None,
    
    # Error Handling and Resilience
    error_handling: Optional[str] = None,
    retry_failed_batches: Optional[bool] = None,
    max_retries: Optional[int] = None,
    fallback_batch_size: Optional[int] = None,
    graceful_degradation: Optional[bool] = None,
    fault_tolerance: Optional[bool] = None,
    
    # Cross-validation Support
    cross_validation: Optional[bool] = None,
    cv_folds: Optional[int] = None,
    cv_strategy: Optional[str] = None,
    fold_dataloaders: Optional[bool] = None,
    stratified_cv: Optional[bool] = None,
    
    # Experimental and Advanced Options
    experimental_features: Optional[bool] = None,
    mixed_precision_loading: Optional[bool] = None,
    gradient_checkpointing: Optional[bool] = None,
    dataloader_sharding: Optional[bool] = None,
    pipeline_parallelism: Optional[bool] = None,
    
    # System Integration
    system_optimization: Optional[bool] = None,
    numa_awareness: Optional[bool] = None,
    cpu_affinity: Optional[List[int]] = None,
    gpu_affinity: Optional[List[int]] = None,
    io_optimization: Optional[bool] = None,
    
    # Compatibility and Legacy Support
    pytorch_version_check: Optional[bool] = None,
    legacy_compatibility: Optional[bool] = None,
    backwards_compatibility: Optional[bool] = None,
    version_specific_optimizations: Optional[bool] = None,
    
    # Configuration and Metadata
    config: Optional[Dict[str, Any]] = None,
    dataloader_config: Optional[Dict[str, Any]] = None,
    preset: Optional[str] = None,
    
    **kwargs
) -> Union[
    Tuple[DataLoader, DataLoader, DataLoader],
    Dict[str, DataLoader],
    DataLoader
]:
    # Start timing
    start_time = datetime.now()
    
    # Initialize configuration with comprehensive defaults
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply dataloader-specific configuration
    if dataloader_config:
        config.setdefault('dataloader', {}).update(dataloader_config)
    
    # Load preset configuration if specified
    if preset and preset in globals().get('PRESET_CONFIGS', {}):
        try:
            preset_config = globals()['PRESET_CONFIGS'][preset].copy()
            # Merge preset config, giving precedence to existing config
            for key, value in preset_config.items():
                if key not in config:
                    config[key] = value
            logger.info(f"Applied preset configuration: {preset}")
        except Exception as e:
            logger.warning(f"Failed to apply preset '{preset}': {e}")
    
    # Apply all parameters to configuration
    final_config = {}
    
    # Merge with existing config
    final_config.update(config)
    
    # Apply individual parameters with intelligent organization
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'dataloader_config', 'preset', 'kwargs', 'start_time', 'datetime',
        'traceback', 'psutil', 'gc', 'partial', 'WeightedRandomSampler', 'DistributedSampler',
        'default_collate', 'TensorDataset', 'DataLoader', 'torch'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Organize parameters into logical sections
    param_sections = {
        'core': [
            'batch_size', 'shuffle', 'num_workers', 'pin_memory', 'drop_last',
            'timeout', 'worker_init_fn', 'multiprocessing_context', 'generator',
            'prefetch_factor', 'persistent_workers'
        ],
        'batch_configuration': [
            'train_batch_size', 'val_batch_size', 'test_batch_size', 'eval_batch_size',
            'dynamic_batch_sizing', 'adaptive_batch_size', 'max_batch_size', 'min_batch_size'
        ],
        'sampling': [
            'train_shuffle', 'val_shuffle', 'test_shuffle', 'sampler', 'batch_sampler',
            'shuffle_seed', 'stratified_sampling', 'weighted_sampling', 'sample_weights'
        ],
        'performance': [
            'optimization_level', 'memory_efficient', 'cpu_count', 'max_workers',
            'worker_memory_limit', 'dataloader_optimization', 'fast_dev_run', 'benchmark_mode'
        ],
        'data_processing': [
            'data_transforms', 'train_transforms', 'val_transforms', 'test_transforms',
            'augmentation_pipeline', 'normalize_data', 'standardize_data'
        ],
        'data_format': [
            'dtype', 'device', 'tensor_format', 'data_format', 'squeeze_dims',
            'unsqueeze_dims', 'transpose_dims'
        ],
        'memory_management': [
            'memory_management', 'shared_memory', 'mmap_mode', 'cache_datasets',
            'preload_data', 'lazy_loading', 'memory_mapping', 'gc_collection'
        ],
        'validation': [
            'validate_data', 'check_data_integrity', 'handle_nan_values', 'handle_inf_values',
            'data_consistency_checks', 'shape_validation', 'dtype_validation'
        ],
        'distributed': [
            'distributed', 'world_size', 'rank', 'local_rank', 'distributed_sampler',
            'ddp_backend', 'sync_batchnorm'
        ],
        'advanced_features': [
            'collate_fn', 'custom_collate', 'variable_length_sequences', 'padding_strategy',
            'sequence_length', 'max_sequence_length'
        ],
        'monitoring': [
            'profile_dataloaders', 'benchmark_dataloaders', 'dataloader_stats',
            'timing_analysis', 'memory_profiling', 'bottleneck_detection', 'verbose', 'debug_mode'
        ],
        'error_handling': [
            'error_handling', 'retry_failed_batches', 'max_retries', 'fallback_batch_size',
            'graceful_degradation', 'fault_tolerance'
        ],
        'cross_validation': [
            'cross_validation', 'cv_folds', 'cv_strategy', 'fold_dataloaders', 'stratified_cv'
        ],
        'experimental': [
            'experimental_features', 'mixed_precision_loading', 'gradient_checkpointing',
            'dataloader_sharding', 'pipeline_parallelism'
        ],
        'system_integration': [
            'system_optimization', 'numa_awareness', 'cpu_affinity', 'gpu_affinity', 'io_optimization'
        ],
        'compatibility': [
            'pytorch_version_check', 'legacy_compatibility', 'backwards_compatibility',
            'version_specific_optimizations'
        ]
    }
    
    # Apply parameters to appropriate sections
    for section, param_list in param_sections.items():
        section_config = final_config.setdefault(section, {})
        for param in param_list:
            if param in cleaned_params:
                section_config[param] = cleaned_params[param]
    
    # Set up comprehensive defaults
    core_config = final_config.setdefault('core', {})
    batch_config = final_config.setdefault('batch_configuration', {})
    sampling_config = final_config.setdefault('sampling', {})
    performance_config = final_config.setdefault('performance', {})
    data_processing_config = final_config.setdefault('data_processing', {})
    data_format_config = final_config.setdefault('data_format', {})
    memory_config = final_config.setdefault('memory_management', {})
    validation_config = final_config.setdefault('validation', {})
    distributed_config = final_config.setdefault('distributed', {})
    advanced_config = final_config.setdefault('advanced_features', {})
    monitoring_config = final_config.setdefault('monitoring', {})
    error_config = final_config.setdefault('error_handling', {})
    cv_config = final_config.setdefault('cross_validation', {})
    experimental_config = final_config.setdefault('experimental', {})
    
    # Apply intelligent defaults with system awareness
    batch_size = core_config.setdefault('batch_size', DEFAULT_BATCH_SIZE)
    shuffle = core_config.setdefault('shuffle', True)
    num_workers = core_config.setdefault('num_workers', NUM_WORKERS)
    pin_memory = core_config.setdefault('pin_memory', torch.cuda.is_available())
    drop_last = core_config.setdefault('drop_last', False)
    timeout = core_config.setdefault('timeout', 0)
    #prefetch_factor = core_config.setdefault('prefetch_factor', 2)
    #persistent_workers = core_config.setdefault('persistent_workers', num_workers > 0)
    if num_workers > 0:
        prefetch_factor = core_config.setdefault('prefetch_factor', 2)
        persistent_workers = core_config.setdefault('persistent_workers', True)
    else:
        prefetch_factor = None
        persistent_workers = False
    
    # Performance defaults
    optimization_level = performance_config.setdefault('optimization_level', 'standard')
    memory_efficient = performance_config.setdefault('memory_efficient', True)
    dataloader_optimization = performance_config.setdefault('dataloader_optimization', True)
    fast_dev_run = performance_config.setdefault('fast_dev_run', False)
    benchmark_mode = performance_config.setdefault('benchmark_mode', False)
    
    # System optimization defaults
    system_cpu_count = os.cpu_count() or 1
    max_workers = performance_config.setdefault('max_workers', system_cpu_count)
    cpu_count = performance_config.setdefault('cpu_count', system_cpu_count)
    
    # Data format defaults
    dtype = data_format_config.setdefault('dtype', torch.float32)
    device = data_format_config.setdefault('device', 'auto')
    
    # Validation defaults
    validate_data = validation_config.setdefault('validate_data', True)
    check_data_integrity = validation_config.setdefault('check_data_integrity', True)
    handle_nan_values = validation_config.setdefault('handle_nan_values', 'error')
    handle_inf_values = validation_config.setdefault('handle_inf_values', 'error')
    
    # Monitoring defaults
    verbose = monitoring_config.setdefault('verbose', False)
    debug_mode = monitoring_config.setdefault('debug_mode', False)
    profile_dataloaders = monitoring_config.setdefault('profile_dataloaders', False)
    dataloader_stats = monitoring_config.setdefault('dataloader_stats', True)
    
    # Error handling defaults
    error_handling = error_config.setdefault('error_handling', 'strict')
    graceful_degradation = error_config.setdefault('graceful_degradation', True)
    max_retries = error_config.setdefault('max_retries', 3)
    
    # Set up logging level
    # if verbose:
    #     original_level = logger.level
    #     logger.setLevel(logging.INFO)
    
    logger.info("Starting comprehensive DataLoader creation")
    
    # Initialize progress tracking
    progress_data = {
        'current_stage': 'Starting...',
        'current_substage': None,
        'dataloaders_created': 0,
        'datasets_processed': 0,
        'transforms_applied': 0,
        'samplers_configured': 0,
        'performance_score': 0.0,
        'fallback_attempts': 0
    }
    
    # Initialize creation statistics
    creation_stats = {
        'start_time': start_time.isoformat(),
        'config_applied': final_config,
        'system_info': {
            'cpu_count': system_cpu_count,
            'available_memory_gb': psutil.virtual_memory().available / (1024**3),
            'cuda_available': torch.cuda.is_available(),
            'pytorch_version': torch.__version__
        },
        'stages_completed': [],
        'warnings_encountered': [],
        'performance_metrics': {}
    }
    
    try:
        # Calculate total stages for progress tracking
        total_stages = 12  # Configuration, Data Validation, System Optimization, Transforms, Datasets, Samplers, Collate, Training DL, Validation DL, Test DL, CV DL, Finalization
        
        with alive_bar(total_stages, title='DataLoader Creation\t\t', unit='stages') as main_bar:
            
            # STAGE 1: Configuration and Setup
            progress_data['current_stage'] = "Configuration"
            main_bar.text = "Setting up configuration and parameters..."
            
            # Determine device configuration
            if device == 'auto':
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            main_bar.text = "Configuration complete"
            creation_stats['stages_completed'].append('configuration')
            main_bar()
            
            # STAGE 2: Data Validation
            progress_data['current_stage'] = "Data Validation"
            main_bar.text = "Validating and preparing input data..."
            
            # Extract data from various input formats
            datasets = {}
            
            if data is not None:
                # Primary method: data dictionary
                X_train = data.get('X_train', X_train)
                X_val = data.get('X_val', X_val)
                X_test = data.get('X_test', X_test)
                y_train = data.get('y_train', y_train)
                y_val = data.get('y_val', y_val)
                y_test = data.get('y_test', y_test)
            
            # Validate required data
            if X_train is None:
                raise ValueError("Training data (X_train) is required")
            
            # Data validation if requested
            if validate_data:
                logger.info("Performing comprehensive data validation")
                
                def validate_array(arr, name):
                    if arr is None:
                        return None
                    
                    if not isinstance(arr, np.ndarray):
                        try:
                            arr = np.array(arr)
                        except Exception as e:
                            raise TypeError(f"{name} could not be converted to numpy array: {e}")
                    
                    if arr.size == 0:
                        logger.warning(f"{name} is empty")
                        return arr
                    
                    # Check for invalid values
                    if handle_nan_values != 'ignore':
                        nan_count = np.isnan(arr).sum()
                        if nan_count > 0:
                            if handle_nan_values == 'error':
                                raise ValueError(f"{name} contains {nan_count} NaN values")
                            elif handle_nan_values == 'remove':
                                valid_mask = ~np.isnan(arr).any(axis=1)
                                arr = arr[valid_mask]
                                logger.warning(f"Removed {(~valid_mask).sum()} samples with NaN values from {name}")
                            elif handle_nan_values == 'replace':
                                arr = np.nan_to_num(arr, nan=0.0)
                                logger.warning(f"Replaced {nan_count} NaN values with 0.0 in {name}")
                    
                    if handle_inf_values != 'ignore':
                        inf_count = np.isinf(arr).sum()
                        if inf_count > 0:
                            if handle_inf_values == 'error':
                                raise ValueError(f"{name} contains {inf_count} infinite values")
                            elif handle_inf_values == 'replace':
                                arr = np.nan_to_num(arr, posinf=1e6, neginf=-1e6)
                                logger.warning(f"Replaced {inf_count} infinite values in {name}")
                    
                    return arr
                
                # Validate all arrays
                X_train = validate_array(X_train, 'X_train')
                X_val = validate_array(X_val, 'X_val') if X_val is not None else None
                X_test = validate_array(X_test, 'X_test') if X_test is not None else None
                y_train = validate_array(y_train, 'y_train') if y_train is not None else None
                y_val = validate_array(y_val, 'y_val') if y_val is not None else None
                y_test = validate_array(y_test, 'y_test') if y_test is not None else None
                
                # Shape consistency validation
                if validation_config.get('shape_validation', True):
                    n_features = X_train.shape[1] if len(X_train.shape) > 1 else 1
                    
                    for name, arr in [('X_val', X_val), ('X_test', X_test)]:
                        if arr is not None and len(arr.shape) > 1:
                            if arr.shape[1] != n_features:
                                raise ValueError(f"Feature dimension mismatch: X_train has {n_features} features, {name} has {arr.shape[1]}")
                    
                    # Label consistency
                    if y_train is not None and len(y_train) != len(X_train):
                        raise ValueError(f"Training data size mismatch: X_train has {len(X_train)} samples, y_train has {len(y_train)}")
            
            creation_stats['data_validation_passed'] = True
            creation_stats['data_shapes'] = {
                'X_train': X_train.shape,
                'X_val': X_val.shape if X_val is not None else None,
                'X_test': X_test.shape if X_test is not None else None
            }
            
            progress_data['datasets_processed'] = 1 + (1 if X_val is not None else 0) + (1 if X_test is not None else 0)
            main_bar.text = f"Data validation complete ({progress_data['datasets_processed']} datasets)"
            creation_stats['stages_completed'].append('data_validation')
            main_bar()
            
            # STAGE 3: System Optimization
            progress_data['current_stage'] = "System Optimization"
            main_bar.text = "Optimizing system parameters..."
            
            # Optimize system parameters based on available resources
            if system_optimization:
                logger.info("Applying system optimizations")
                
                # Memory-based optimization
                available_memory_gb = psutil.virtual_memory().available / (1024**3)
                # Less than 4GB
                if available_memory_gb < 4:
                    num_workers = min(num_workers, 2)
                    batch_size = min(batch_size, 32)
                    if num_workers > 0:
                        prefetch_factor = 1
                    else:
                        prefetch_factor = None
                    logger.info("Applied memory-constrained optimizations")
                # More than 16GB
                elif available_memory_gb > 16:
                    num_workers = min(num_workers, max_workers)
                    prefetch_factor = min(prefetch_factor, 4)
                    logger.info("Applied high-memory optimizations")
                
                # CPU-based optimization
                if system_cpu_count <= 2:
                    num_workers = min(num_workers, 1)
                    persistent_workers = False
                elif system_cpu_count >= 8:
                    num_workers = min(num_workers, system_cpu_count - 1)
                    persistent_workers = True
            
            # Apply batch size configurations
            train_batch_size = batch_config.get('train_batch_size', batch_size)
            val_batch_size = batch_config.get('val_batch_size', batch_config.get('eval_batch_size', min(batch_size * 2, 1024)))
            test_batch_size = batch_config.get('test_batch_size', batch_config.get('eval_batch_size', min(batch_size * 2, 1024)))
            
            # Adaptive batch sizing based on data size and memory
            if batch_config.get('adaptive_batch_size', False):
                logger.info("Applying adaptive batch sizing")
                
                def calculate_optimal_batch_size(data_size, base_batch_size, memory_factor=1.0):
                    # Calculate based on data size and available memory
                    if data_size < 1000:
                        return min(base_batch_size, data_size // 4) if data_size > 4 else 1
                    elif data_size > 100000:
                        return min(base_batch_size * 2, batch_config.get('max_batch_size', 2048))
                    else:
                        return int(base_batch_size * memory_factor)
                
                memory_factor = min(2.0, available_memory_gb / 8.0) if 'available_memory_gb' in locals() else 1.0
                
                train_batch_size = calculate_optimal_batch_size(len(X_train), train_batch_size, memory_factor)
                if X_val is not None:
                    val_batch_size = calculate_optimal_batch_size(len(X_val), val_batch_size, memory_factor)
                if X_test is not None:
                    test_batch_size = calculate_optimal_batch_size(len(X_test), test_batch_size, memory_factor)
            
            # Apply batch size limits
            min_batch_size = batch_config.get('min_batch_size', 1)
            max_batch_size = batch_config.get('max_batch_size', 4096)
            
            train_batch_size = max(min_batch_size, min(train_batch_size, max_batch_size))
            val_batch_size = max(min_batch_size, min(val_batch_size, max_batch_size))
            test_batch_size = max(min_batch_size, min(test_batch_size, max_batch_size))
            
            # Shuffle configuration
            train_shuffle = sampling_config.get('train_shuffle', shuffle)
            val_shuffle = sampling_config.get('val_shuffle', False)
            test_shuffle = sampling_config.get('test_shuffle', False)
            
            main_bar.text = "System optimization complete"
            creation_stats['stages_completed'].append('system_optimization')
            main_bar()
            
            # STAGE 4: Transform Pipeline Setup
            progress_data['current_stage'] = "Transform Setup"
            main_bar.text = "Setting up data transforms..."
            
            # Setup multiprocessing context
            mp_context = core_config.get('multiprocessing_context')
            if mp_context and num_workers > 0:
                try:
                    import multiprocessing as mp
                    if mp_context in ['spawn', 'fork', 'forkserver']:
                        mp_ctx = mp.get_context(mp_context)
                        logger.info(f"Using multiprocessing context: {mp_context}")
                    else:
                        mp_ctx = None
                        logger.warning(f"Invalid multiprocessing context: {mp_context}")
                except Exception as e:
                    logger.warning(f"Failed to set multiprocessing context: {e}")
                    mp_ctx = None
            else:
                mp_ctx = None
            
            # Create data transforms pipeline
            def create_transform_pipeline(transforms_list):
                if not transforms_list:
                    return None
                
                def apply_transforms(tensor):
                    for transform in transforms_list:
                        tensor = transform(tensor)
                    return tensor
                return apply_transforms
            
            # Data processing transforms
            train_transform = create_transform_pipeline(data_processing_config.get('train_transforms'))
            val_transform = create_transform_pipeline(data_processing_config.get('val_transforms'))
            test_transform = create_transform_pipeline(data_processing_config.get('test_transforms'))
            
            # Generic transforms applied to all data
            if data_processing_config.get('data_transforms'):
                generic_transform = create_transform_pipeline(data_processing_config['data_transforms'])
            elif data_processing_config.get('normalize_data', False):
                # Simple normalization transform
                def normalize_transform(tensor):
                    return (tensor - tensor.mean()) / (tensor.std() + 1e-8)
                generic_transform = normalize_transform
            elif data_processing_config.get('standardize_data', False):
                # Standardization transform
                def standardize_transform(tensor):
                    return (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-8)
                generic_transform = standardize_transform
            else:
                generic_transform = None
            
            progress_data['transforms_applied'] = sum(1 for t in [train_transform, val_transform, test_transform, generic_transform] if t is not None)
            main_bar.text = f"Transform setup complete ({progress_data['transforms_applied']} transforms)"
            creation_stats['stages_completed'].append('transform_setup')
            main_bar()
            
            # STAGE 5: Dataset Creation
            progress_data['current_stage'] = "Dataset Creation"
            main_bar.text = "Creating tensor datasets..."
            
            # Create tensor datasets with enhanced functionality
            def create_enhanced_dataset(X, y=None, transform=None):
                # Convert to tensors
                X_tensor = torch.tensor(X, dtype=dtype)
                
                if y is not None:
                    y_tensor = torch.tensor(y, dtype=torch.long if y.dtype in [np.int32, np.int64] else dtype)
                    dataset = TensorDataset(X_tensor, y_tensor)
                else:
                    dataset = TensorDataset(X_tensor)
                
                # Apply transforms if specified
                if transform or generic_transform:
                    original_dataset = dataset
                    
                    def transformed_getitem(idx):
                        item = original_dataset[idx]
                        if generic_transform:
                            if isinstance(item, tuple):
                                item = (generic_transform(item[0]), *item[1:])
                            else:
                                item = generic_transform(item)
                        if transform:
                            if isinstance(item, tuple):
                                item = (transform(item[0]), *item[1:])
                            else:
                                item = transform(item)
                        return item
                    
                    # Create a custom dataset class that applies transforms
                    class TransformedDataset(torch.utils.data.Dataset):
                        def __init__(self, base_dataset, transform_fn):
                            self.base_dataset = base_dataset
                            self.transform_fn = transform_fn
                        
                        def __len__(self):
                            return len(self.base_dataset)
                        
                        def __getitem__(self, idx):
                            return self.transform_fn(idx)
                    
                    dataset = TransformedDataset(original_dataset, transformed_getitem)
                
                return dataset
            
            # Create datasets
            logger.info("Creating tensor datasets")
            
            train_dataset = create_enhanced_dataset(X_train, y_train, train_transform)
            val_dataset = create_enhanced_dataset(X_val, y_val, val_transform) if X_val is not None else None
            test_dataset = create_enhanced_dataset(X_test, y_test, test_transform) if X_test is not None else None
            
            main_bar.text = "Dataset creation complete"
            creation_stats['stages_completed'].append('dataset_creation')
            main_bar()
            
            # STAGE 6: Sampler Configuration
            progress_data['current_stage'] = "Sampler Configuration"
            main_bar.text = "Configuring data samplers..."
            
            # Setup samplers
            train_sampler = None
            val_sampler = None
            test_sampler = None
            
            # Distributed sampling
            if distributed_config.get('distributed', False) and distributed_config.get('distributed_sampler', True):
                world_size = distributed_config.get('world_size', 1)
                rank = distributed_config.get('rank', 0)
                
                logger.info(f"Setting up distributed samplers: world_size={world_size}, rank={rank}")
                
                train_sampler = DistributedSampler(
                    train_dataset,
                    num_replicas=world_size,
                    rank=rank,
                    shuffle=train_shuffle,
                    seed=sampling_config.get('shuffle_seed', 0)
                )
                
                if val_dataset:
                    val_sampler = DistributedSampler(
                        val_dataset,
                        num_replicas=world_size,
                        rank=rank,
                        shuffle=val_shuffle,
                        seed=sampling_config.get('shuffle_seed', 0)
                    )
                
                if test_dataset:
                    test_sampler = DistributedSampler(
                        test_dataset,
                        num_replicas=world_size,
                        rank=rank,
                        shuffle=test_shuffle,
                        seed=sampling_config.get('shuffle_seed', 0)
                    )
                
                # Override shuffle when using distributed sampler
                train_shuffle = False
                val_shuffle = False
                test_shuffle = False
            
            # Weighted sampling for training data
            elif sampling_config.get('weighted_sampling', False) and y_train is not None:
                logger.info("Setting up weighted sampling")
                
                if sampling_config.get('sample_weights') is not None:
                    sample_weights = sampling_config['sample_weights']
                else:
                    # Calculate class weights
                    class_counts = np.bincount(y_train)
                    class_weights = 1.0 / class_counts
                    sample_weights = class_weights[y_train]
                
                train_sampler = WeightedRandomSampler(
                    weights=sample_weights,
                    num_samples=len(sample_weights),
                    replacement=True
                )
                train_shuffle = False  # Don't shuffle when using custom sampler
            
            # Stratified sampling
            elif sampling_config.get('stratified_sampling', False) and y_train is not None:
                logger.info("Setting up stratified sampling")
                
                # Create stratified sampler (simplified implementation)
                class_indices = defaultdict(list)
                for idx, label in enumerate(y_train):
                    class_indices[label].append(idx)
                
                # Balance classes by sampling
                min_class_size = min(len(indices) for indices in class_indices.values())
                balanced_indices = []
                
                for indices in class_indices.values():
                    sampled_indices = np.random.choice(indices, size=min_class_size, replace=False)
                    balanced_indices.extend(sampled_indices)
                
                # Custom sampler that uses balanced indices
                class StratifiedSampler(torch.utils.data.Sampler):
                    def __init__(self, indices):
                        self.indices = indices
                    
                    def __iter__(self):
                        return iter(np.random.permutation(self.indices))
                    
                    def __len__(self):
                        return len(self.indices)
                
                train_sampler = StratifiedSampler(balanced_indices)
                train_shuffle = False
            
            # Custom samplers from configuration
            if sampling_config.get('sampler') is not None:
                train_sampler = sampling_config['sampler']
                train_shuffle = False
            
            if sampling_config.get('batch_sampler') is not None:
                batch_sampler = sampling_config['batch_sampler']
                train_sampler = None
                train_shuffle = False
            else:
                batch_sampler = None
            
            # Setup generator for reproducibility
            generator = core_config.get('generator')
            if generator is None and sampling_config.get('shuffle_seed') is not None:
                generator = torch.Generator()
                generator.manual_seed(sampling_config['shuffle_seed'])
            
            progress_data['samplers_configured'] = sum(1 for s in [train_sampler, val_sampler, test_sampler] if s is not None)
            main_bar.text = f"Sampler configuration complete ({progress_data['samplers_configured']} samplers)"
            creation_stats['stages_completed'].append('sampler_configuration')
            main_bar()
            
            # STAGE 7: Collate Function Setup
            progress_data['current_stage'] = "Collate Function"
            main_bar.text = "Setting up collate functions..."
            
            # Custom collate function
            enhanced_collate_fn = None
            
            # Only create enhanced collate if we have specific requirements or configurations
            needs_enhanced_collate = (
                advanced_config.get('variable_length_sequences', False) or 
                advanced_config.get('custom_collate') or
                data_format_config.get('squeeze_dims', False) or
                data_format_config.get('unsqueeze_dims') or
                dtype != torch.float32 or
                advanced_config.get('collate_fn') != default_collate
            )
            
            if needs_enhanced_collate:
                logger.debug("Creating enhanced collate function with custom processing")
                
                # Create the enhanced collate function with current configuration
                enhanced_collate_fn = EnhancedCollateFn(
                    config=final_config,
                    dtype=dtype,
                    error_handling=error_handling
                )
                
                # Test if the collate function is picklable when using multiprocessing
                if num_workers > 0:
                    try:
                        pickle.dumps(enhanced_collate_fn)
                        logger.debug("Enhanced collate function is picklable and ready for multiprocessing")
                    except Exception as pickle_error:
                        logger.warning(f"Enhanced collate function pickling test failed: {pickle_error}")
                        logger.info("Falling back to default collate function to avoid multiprocessing issues")
                        enhanced_collate_fn = None
                
            else:
                logger.debug("Using default PyTorch collate function (no custom processing needed)")
                # Use default collate
                enhanced_collate_fn = None
            
            # Additional fallback for problematic scenarios
            if enhanced_collate_fn is None and needs_enhanced_collate:
                logger.warning("Enhanced collate features requested but not available - some functionality may be limited")
            
            # Worker initialization function
            worker_init_fn = None
            
            if num_workers > 0:
                # Check if we need custom worker initialization
                needs_custom_worker_init = (
                    sampling_config.get('shuffle_seed') is not None or
                    performance_config.get('cpu_affinity') or
                    memory_config.get('worker_memory_limit') or
                    core_config.get('worker_init_fn')
                )
                
                if needs_custom_worker_init:
                    # Create picklable worker initializer
                    worker_init_fn = WorkerInitializer(config=final_config)
                    
                    # Test if the worker initializer is picklable
                    try:
                        pickle.dumps(worker_init_fn)
                        logger.debug("Worker initializer is picklable and ready for multiprocessing")
                    except Exception as pickle_error:
                        logger.warning(f"Worker initializer pickling test failed: {pickle_error}")
                        logger.info("Disabling custom worker initialization to avoid multiprocessing issues")
                        worker_init_fn = None
                else:
                    logger.debug("No custom worker initialization needed")
                    worker_init_fn = None
            else:
                worker_init_fn = None
            
            main_bar.text = "Collate function setup complete"
            creation_stats['stages_completed'].append('collate_setup')
            main_bar()
            
            # STAGE 8: Training DataLoader Creation
            progress_data['current_stage'] = "Training DataLoader"
            main_bar.text = "Creating training DataLoader..."
            
            # Common DataLoader parameters
            common_params = {
                'pin_memory': pin_memory,
                'timeout': timeout,
                'generator': generator
            }
            
            # CRITICAL FIX: Only add prefetch_factor if num_workers > 0
            if num_workers > 0:
                common_params['prefetch_factor'] = prefetch_factor
            
            # Add multiprocessing context if available
            if mp_ctx and num_workers > 0:
                common_params['multiprocessing_context'] = mp_ctx
            
            # Add collate function and worker configuration
            if num_workers > 0:
                common_params.update({
                    'num_workers': num_workers,
                    'worker_init_fn': worker_init_fn,  # This is now picklable
                    'persistent_workers': persistent_workers,
                    'collate_fn': enhanced_collate_fn
                })
            else:
                common_params.update({
                    'num_workers': 0,
                    'worker_init_fn': None,
                    'persistent_workers': False,
                    'collate_fn': enhanced_collate_fn
                })
            
            # Create training DataLoader with comprehensive error handling
            logger.info(f"Creating training DataLoader: batch_size={train_batch_size}, num_workers={num_workers}")
            
            train_loader_params = common_params.copy()
            train_loader_params.update({
                'batch_size': train_batch_size,
                'shuffle': train_shuffle,
                'drop_last': drop_last,
                'sampler': train_sampler,
                'batch_sampler': batch_sampler
            })
            
            # Multiple fallback attempts for training DataLoader
            train_loader = None
            fallback_attempts = 0
            
            for attempt in range(max_retries):
                try:
                    train_loader = DataLoader(train_dataset, **train_loader_params)
                    logger.debug(f"Training DataLoader created successfully on attempt {attempt + 1}")
                    break
                    
                except Exception as e:
                    fallback_attempts += 1
                    error_str = str(e).lower()
                    
                    # CRITICAL FIX: Handle prefetch_factor error specifically
                    if "prefetch_factor" in error_str and train_loader_params.get('num_workers', 0) == 0:
                        logger.warning(f"Attempt {attempt + 1}: Removing prefetch_factor for single-threaded mode")
                        if 'prefetch_factor' in train_loader_params:
                            del train_loader_params['prefetch_factor']
                    
                    elif "pickle" in error_str or "worker_init" in error_str:
                        logger.warning(f"Attempt {attempt + 1}: Worker initialization pickling error detected: {e}")
                        # Progressive fallback strategy
                        if attempt == 0:
                            # First attempt: Remove custom worker_init_fn and reduce workers
                            logger.info("Fallback 1: Removing custom worker initialization and reducing workers")
                            train_loader_params['worker_init_fn'] = None
                            train_loader_params['num_workers'] = min(2, num_workers)
                            train_loader_params['persistent_workers'] = False
                            # Remove prefetch_factor if switching to single-threaded
                            if train_loader_params['num_workers'] == 0 and 'prefetch_factor' in train_loader_params:
                                del train_loader_params['prefetch_factor']
                        elif attempt == 1:
                            # Second attempt: Single-threaded
                            logger.info("Fallback 2: Switching to single-threaded DataLoader")
                            train_loader_params.update({
                                'num_workers': 0,
                                'worker_init_fn': None,
                                'persistent_workers': False,
                                'pin_memory': False,
                                'collate_fn': None
                            })
                            # Remove prefetch_factor for single-threaded
                            if 'prefetch_factor' in train_loader_params:
                                del train_loader_params['prefetch_factor']
                            if 'multiprocessing_context' in train_loader_params:
                                del train_loader_params['multiprocessing_context']
                        else:
                            # Final attempt: Minimal configuration
                            logger.info("Fallback 3: Using minimal DataLoader configuration")
                            train_loader_params = {
                                'batch_size': error_config.get('fallback_batch_size', 32),
                                'shuffle': train_shuffle,
                                'num_workers': 0,
                                'pin_memory': False,
                                'drop_last': False
                            }
                    
                    elif "memory" in error_str or "out of memory" in error_str:
                        logger.warning(f"Memory error detected: {e}")
                        # Reduce batch size and workers
                        current_batch_size = train_loader_params.get('batch_size', batch_size)
                        new_batch_size = max(1, current_batch_size // 2)
                        train_loader_params['batch_size'] = new_batch_size
                        train_loader_params['num_workers'] = max(0, train_loader_params.get('num_workers', 0) - 1)
                        # Remove prefetch_factor if switching to single-threaded
                        if train_loader_params['num_workers'] == 0 and 'prefetch_factor' in train_loader_params:
                            del train_loader_params['prefetch_factor']
                        logger.info(f"Reduced batch size to {new_batch_size} and workers to {train_loader_params['num_workers']}")
                    
                    else:
                        logger.warning(f"Attempt {attempt + 1}: Unexpected error: {e}")
                        if attempt == max_retries - 1:
                            if graceful_degradation:
                                logger.warning("Using absolute minimal DataLoader configuration")
                                train_loader_params = {
                                    'batch_size': 1,
                                    'shuffle': False,
                                    'num_workers': 0
                                }
                            else:
                                raise RuntimeError(f"Failed to create training DataLoader after {max_retries} attempts: {e}")
            
            if train_loader is None:
                raise RuntimeError("Failed to create training DataLoader with all fallback attempts")
            
            progress_data['dataloaders_created'] += 1
            progress_data['fallback_attempts'] = fallback_attempts
            main_bar.text = f"Training DataLoader created (attempts: {fallback_attempts + 1})"
            creation_stats['stages_completed'].append('training_dataloader')
            main_bar()
            
            # STAGE 9: Validation DataLoader Creation
            progress_data['current_stage'] = "Validation DataLoader"
            main_bar.text = "Creating validation DataLoader..."
            
            # Create validation DataLoader
            val_loader = None
            if val_dataset is not None:
                logger.info(f"Creating validation DataLoader: batch_size={val_batch_size}")
                val_loader_params = common_params.copy()
                val_loader_params.update({
                    'batch_size': val_batch_size,
                    'shuffle': val_shuffle,
                    'drop_last': False,
                    'sampler': val_sampler
                })
                
                for attempt in range(max_retries):
                    try:
                        val_loader = DataLoader(val_dataset, **val_loader_params)
                        break
                    except Exception as e:
                        error_str = str(e).lower()
                        
                        if "prefetch_factor" in error_str and val_loader_params.get('num_workers', 0) == 0:
                            if 'prefetch_factor' in val_loader_params:
                                del val_loader_params['prefetch_factor']
                        elif attempt == max_retries - 1:
                            if graceful_degradation:
                                val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)
                            else:
                                raise RuntimeError(f"Failed to create validation DataLoader: {e}")
                
                progress_data['dataloaders_created'] += 1
            
            main_bar.text = f"Validation DataLoader {'created' if val_loader else 'skipped'}"
            creation_stats['stages_completed'].append('validation_dataloader')
            main_bar()
            
            # STAGE 10: Test DataLoader Creation
            progress_data['current_stage'] = "Test DataLoader"
            main_bar.text = "Creating test DataLoader..."
            
            # Create test DataLoader
            test_loader = None
            if test_dataset is not None:
                logger.info(f"Creating test DataLoader: batch_size={test_batch_size}")
                test_loader_params = common_params.copy()
                test_loader_params.update({
                    'batch_size': test_batch_size,
                    'shuffle': test_shuffle,
                    'drop_last': False,
                    'sampler': test_sampler
                })
                
                for attempt in range(max_retries):
                    try:
                        test_loader = DataLoader(test_dataset, **test_loader_params)
                        break
                    except Exception as e:
                        error_str = str(e).lower()
                        
                        if "prefetch_factor" in error_str and test_loader_params.get('num_workers', 0) == 0:
                            if 'prefetch_factor' in test_loader_params:
                                del test_loader_params['prefetch_factor']
                        elif attempt == max_retries - 1:
                            if graceful_degradation:
                                test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)
                            else:
                                raise RuntimeError(f"Failed to create test DataLoader: {e}")
                
                progress_data['dataloaders_created'] += 1
            
            main_bar.text = f"Test DataLoader {'created' if test_loader else 'skipped'}"
            creation_stats['stages_completed'].append('test_dataloader')
            main_bar()
            
            # STAGE 11: Cross-validation DataLoaders
            progress_data['current_stage'] = "Cross-validation"
            main_bar.text = "Creating cross-validation DataLoaders..."
            
            # Cross-validation DataLoaders
            cv_loaders = None
            if cv_config.get('cross_validation', False):
                logger.info("Creating cross-validation DataLoaders")
                
                cv_folds = cv_config.get('cv_folds', 5)
                cv_strategy = cv_config.get('cv_strategy', 'kfold')
                stratified_cv = cv_config.get('stratified_cv', False)
                
                try:
                    if stratified_cv and y_train is not None:
                        cv_splitter = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=sampling_config.get('shuffle_seed', 42))
                        splits = list(cv_splitter.split(X_train, y_train))
                    else:
                        cv_splitter = KFold(n_splits=cv_folds, shuffle=True, random_state=sampling_config.get('shuffle_seed', 42))
                        splits = list(cv_splitter.split(X_train))
                    
                    cv_loaders = []
                    
                    # Create progress bar for CV folds
                    with alive_bar(cv_folds, title='CV Folds\t\t\t', unit='folds') as cv_bar:
                        for fold_idx, (train_idx, val_idx) in enumerate(splits):
                            cv_bar.text = f"Creating fold {fold_idx + 1}/{cv_folds}"
                            
                            fold_train_X = X_train[train_idx]
                            fold_val_X = X_train[val_idx]
                            fold_train_y = y_train[train_idx] if y_train is not None else None
                            fold_val_y = y_train[val_idx] if y_train is not None else None
                            
                            fold_train_dataset = create_enhanced_dataset(fold_train_X, fold_train_y, train_transform)
                            fold_val_dataset = create_enhanced_dataset(fold_val_X, fold_val_y, val_transform)
                            
                            fold_train_loader = DataLoader(
                                fold_train_dataset,
                                batch_size=train_batch_size,
                                shuffle=train_shuffle,
                                # Reduce workers for CV
                                num_workers=max(1, num_workers // 2),
                                pin_memory=pin_memory,
                                collate_fn=enhanced_collate_fn
                            )
                            
                            fold_val_loader = DataLoader(
                                fold_val_dataset,
                                batch_size=val_batch_size,
                                shuffle=False,
                                num_workers=max(1, num_workers // 2),
                                pin_memory=pin_memory,
                                collate_fn=enhanced_collate_fn
                            )
                            
                            cv_loaders.append({
                                'fold': fold_idx,
                                'train': fold_train_loader,
                                'val': fold_val_loader
                            })
                            
                            cv_bar()
                    
                    logger.info(f"Created {cv_folds} cross-validation fold DataLoaders")
                    
                except Exception as e:
                    logger.error(f"Failed to create cross-validation DataLoaders: {e}")
                    cv_loaders = None
            
            main_bar.text = f"Cross-validation {'completed' if cv_loaders else 'skipped'}"
            creation_stats['stages_completed'].append('cross_validation')
            main_bar()
            
            # STAGE 12: Finalization and Performance Analysis
            progress_data['current_stage'] = "Finalization"
            main_bar.text = "Finalizing DataLoader creation..."
            
            # Performance profiling and benchmarking
            if profile_dataloaders or benchmark_dataloaders:
                logger.info("Performing DataLoader performance analysis")
                
                def benchmark_dataloader(dataloader, name, num_batches=10):
                    if dataloader is None:
                        return None
                    times = []
                    
                    dataloader_iter = iter(dataloader)
                    for i in range(min(num_batches, len(dataloader))):
                        start_time = time.time()
                        try:
                            batch = next(dataloader_iter)
                            end_time = time.time()
                            times.append(end_time - start_time)
                        except StopIteration:
                            break
                        except Exception as e:
                            logger.warning(f"Benchmark error for {name} batch {i}: {e}")
                            continue
                    
                    if times:
                        return {
                            'name': name,
                            'avg_batch_time': np.mean(times),
                            'std_batch_time': np.std(times),
                            'min_batch_time': np.min(times),
                            'max_batch_time': np.max(times),
                            'total_time': np.sum(times),
                            'batches_tested': len(times)
                        }
                    return None
                
                benchmark_results = {}
                if benchmark_dataloaders:
                    benchmark_results['train'] = benchmark_dataloader(train_loader, 'train', 5)
                    benchmark_results['val'] = benchmark_dataloader(val_loader, 'val', 3)
                    benchmark_results['test'] = benchmark_dataloader(test_loader, 'test', 3)
                    
                    creation_stats['benchmark_results'] = benchmark_results
                    
                    for name, result in benchmark_results.items():
                        if result:
                            logger.info(f"{name.title()} DataLoader: {result['avg_batch_time']:.4f}s avg batch time")
            
            # Collect DataLoader statistics
            if dataloader_stats:
                stats = {
                    'train': {
                        'dataset_size': len(train_dataset),
                        'batch_size': train_batch_size,
                        'num_batches': len(train_loader),
                        'shuffle': train_shuffle,
                        'sampler': type(train_sampler).__name__ if train_sampler else None,
                        'drop_last': drop_last
                    },
                    'val': {
                        'dataset_size': len(val_dataset) if val_dataset else 0,
                        'batch_size': val_batch_size,
                        'num_batches': len(val_loader) if val_loader else 0,
                        'shuffle': val_shuffle,
                        'sampler': type(val_sampler).__name__ if val_sampler else None
                    } if val_dataset else None,
                    'test': {
                        'dataset_size': len(test_dataset) if test_dataset else 0,
                        'batch_size': test_batch_size,
                        'num_batches': len(test_loader) if test_loader else 0,
                        'shuffle': test_shuffle,
                        'sampler': type(test_sampler).__name__ if test_sampler else None
                    } if test_dataset else None,
                    'common': {
                        'num_workers': num_workers,
                        'pin_memory': pin_memory,
                        'persistent_workers': persistent_workers,
                        'prefetch_factor': prefetch_factor,
                        'timeout': timeout,
                        'dtype': str(dtype),
                        'device': device
                    }
                }
                
                creation_stats['dataloader_stats'] = stats
            
            # Garbage collection if requested
            if memory_config.get('gc_collection', False):
                gc.collect()
                logger.debug("Performed garbage collection")
            
            # Calculate performance score
            performance_metrics = []
            
            # Worker efficiency
            worker_score = min(1.0, num_workers / max(1, system_cpu_count))
            performance_metrics.append(('worker_efficiency', worker_score))
            
            # Batch size adequacy
            batch_score = min(1.0, train_batch_size / max(32, train_batch_size))
            performance_metrics.append(('batch_adequacy', batch_score))
            
            # Memory optimization
            memory_score = 0.8 if pin_memory else 0.5
            performance_metrics.append(('memory_optimization', memory_score))
            
            # Overall performance score
            performance_score = sum(score for _, score in performance_metrics) / len(performance_metrics)
            progress_data['performance_score'] = performance_score
            creation_stats['performance_score'] = performance_score
            
            main_bar.text = f"Finalization complete (Performance: {performance_score:.3f})"
            creation_stats['stages_completed'].append('finalization')
            main_bar()
        
        # Prepare return value based on configuration
        total_time = (datetime.now() - start_time).total_seconds()
        creation_stats['total_processing_time'] = total_time
        creation_stats['completion_status'] = 'success'
        
        # Create comprehensive metadata
        metadata = {
            'creation_time_seconds': total_time,
            'dataloaders_created': {
                'train': True,
                'val': val_loader is not None,
                'test': test_loader is not None,
                'cv_folds': len(cv_loaders) if cv_loaders else 0
            },
            'configuration_applied': final_config,
            'creation_stats': creation_stats,
            'system_info': creation_stats['system_info'],
            'optimization_level': optimization_level,
            'performance_optimizations_applied': dataloader_optimization,
            'error_handling_mode': error_handling,
            'progress_summary': {
                'stages_completed': len(creation_stats['stages_completed']),
                'dataloaders_created': progress_data['dataloaders_created'],
                'datasets_processed': progress_data['datasets_processed'],
                'transforms_applied': progress_data['transforms_applied'],
                'samplers_configured': progress_data['samplers_configured'],
                'fallback_attempts': progress_data['fallback_attempts'],
                'final_performance_score': progress_data['performance_score']
            }
        }
        
        # Determine return format
        return_format = final_config.get('output', {}).get('format', 'tuple')
        
        if return_format == 'dict':
            result = {
                'train': train_loader,
                'val': val_loader,
                'test': test_loader,
                'metadata': metadata
            }
            
            if cv_loaders:
                result['cv_folds'] = cv_loaders
                
            return result
        
        elif return_format == 'single':
            # Return only training loader for specific use cases
            return train_loader
        
        else:
            # Default tuple format
            result = (train_loader, val_loader, test_loader)
            
            # Add metadata as attribute to train_loader for access
            train_loader.dataloader_metadata = metadata
            
            if cv_loaders:
                train_loader.cv_folds = cv_loaders
            
            return result
        
    except Exception as e:
        # Update creation stats with error information
        creation_stats['completion_status'] = 'failed'
        creation_stats['error_message'] = str(e)
        creation_stats['error_traceback'] = traceback.format_exc()
        
        # Restore original logging level on error
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        error_msg = f"DataLoader creation failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Provide helpful error context
        logger.error(f"Configuration used: {final_config}")
        logger.error(f"Stages completed: {creation_stats['stages_completed']}")
        
        # Attempt graceful fallback if enabled
        if graceful_degradation and data is not None and X_train is not None:
            logger.warning("Attempting graceful fallback to basic DataLoaders")
            
            try:
                # Create minimal DataLoaders
                train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32))
                val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32)) if X_val is not None else None
                test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32)) if X_test is not None else None
                
                fallback_batch_size = error_config.get('fallback_batch_size', 32)
                
                train_loader = DataLoader(train_data, batch_size=fallback_batch_size, shuffle=True, num_workers=0)
                val_loader = DataLoader(val_data, batch_size=fallback_batch_size, shuffle=False, num_workers=0) if val_data else None
                test_loader = DataLoader(test_data, batch_size=fallback_batch_size, shuffle=False, num_workers=0) if test_data else None
                
                logger.warning("Created fallback DataLoaders with minimal configuration")
                
                # Restore original logging level
                # if verbose and 'original_level' in locals():
                #     logger.setLevel(original_level)
                
                return (train_loader, val_loader, test_loader)
                
            except Exception as fallback_error:
                logger.error(f"Fallback DataLoader creation also failed: {fallback_error}")
                raise RuntimeError(f"Both primary and fallback DataLoader creation failed: {error_msg}")
        
        raise RuntimeError(error_msg)
    
    finally:
        # Restore original logging level
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        # Final cleanup
        if final_config.get('monitoring', {}).get('log_creation_summary', True):
            total_time = (datetime.now() - start_time).total_seconds()
            logger.info("=" * 80)
            logger.info("DATALOADER CREATION SUMMARY")
            logger.info("=" * 80)
            logger.info(f"Creation time: {total_time:.2f} seconds")
            logger.info(f"Training DataLoader: batch_size={train_batch_size}, num_batches={len(train_loader) if 'train_loader' in locals() else 'N/A'}")
            logger.info(f"Validation DataLoader: {'Created' if val_loader else 'Not created'}")
            logger.info(f"Test DataLoader: {'Created' if test_loader else 'Not created'}")
            logger.info(f"Workers: {num_workers}, Pin memory: {pin_memory}")
            logger.info(f"Optimization level: {optimization_level}")
            logger.info(f"Performance score: {progress_data.get('performance_score', 0):.3f}")
            logger.info(f"Fallback attempts: {progress_data.get('fallback_attempts', 0)}")
            if 'cv_loaders' in locals() and cv_loaders:
                logger.info(f"Cross-validation folds: {len(cv_loaders)}")
            logger.info("=" * 80)

def train_epoch(
    # Core Training Parameters
    model: Optional[nn.Module] = None,
    loader: Optional[DataLoader] = None,
    criterion: Optional[nn.Module] = None,
    optimizer: Optional[optim.Optimizer] = None,
    device: Optional[torch.device] = None,
    epoch: Optional[int] = None,
    
    # Training Configuration Parameters
    learning_rate: Optional[float] = None,
    batch_size: Optional[int] = None,
    gradient_clip: Optional[float] = None,
    gradient_accumulation_steps: Optional[int] = None,
    max_grad_norm: Optional[float] = None,
    gradient_clipping_mode: Optional[str] = None,
    gradient_scaling: Optional[bool] = None,
    
    # Mixed Precision Parameters
    mixed_precision: Optional[bool] = None,
    amp_enabled: Optional[bool] = None,
    scaler: Optional[GradScaler] = None,
    loss_scaling: Optional[str] = None,
    dynamic_loss_scaling: Optional[bool] = None,
    init_scale: Optional[float] = None,
    growth_factor: Optional[float] = None,
    backoff_factor: Optional[float] = None,
    growth_interval: Optional[int] = None,
    
    # Scheduler Parameters
    scheduler: Optional[Any] = None,
    scheduler_step_on_epoch: Optional[bool] = None,
    scheduler_step_on_batch: Optional[bool] = None,
    scheduler_step_after_epoch: Optional[bool] = None,
    scheduler_metric: Optional[str] = None,
    warmup_steps: Optional[int] = None,
    warmup_scheduler: Optional[Any] = None,
    
    # Loss Function Parameters
    loss_function: Optional[str] = None,
    loss_weights: Optional[Dict[str, float]] = None,
    multi_task_learning: Optional[bool] = None,
    auxiliary_loss_weight: Optional[float] = None,
    regularization_loss_weight: Optional[float] = None,
    reconstruction_loss_weight: Optional[float] = None,
    
    # Monitoring and Metrics Parameters
    track_metrics: Optional[bool] = None,
    metrics_to_track: Optional[List[str]] = None,
    calculate_detailed_metrics: Optional[bool] = None,
    metrics_frequency: Optional[int] = None,
    log_frequency: Optional[int] = None,
    progress_bar: Optional[bool] = None,
    progress_bar_desc: Optional[str] = None,
    
    # Performance and Optimization Parameters
    performance_mode: Optional[str] = None,
    benchmark_mode: Optional[bool] = None,
    cudnn_benchmark: Optional[bool] = None,
    cudnn_deterministic: Optional[bool] = None,
    channels_last: Optional[bool] = None,
    compile_model: Optional[bool] = None,
    torch_compile_mode: Optional[str] = None,
    
    # Memory Management Parameters
    memory_efficient: Optional[bool] = None,
    memory_optimization: Optional[str] = None,
    gradient_checkpointing: Optional[bool] = None,
    empty_cache_frequency: Optional[int] = None,
    gc_collection_frequency: Optional[int] = None,
    pin_memory: Optional[bool] = None,
    non_blocking_transfer: Optional[bool] = None,
    
    # Data Processing Parameters
    data_preprocessing: Optional[bool] = None,
    input_transforms: Optional[List[Callable]] = None,
    target_transforms: Optional[List[Callable]] = None,
    augmentation_during_training: Optional[bool] = None,
    mixup_alpha: Optional[float] = None,
    cutmix_alpha: Optional[float] = None,
    
    # Validation and Quality Control Parameters
    validate_inputs: Optional[bool] = None,
    check_finite: Optional[bool] = None,
    detect_anomaly: Optional[bool] = None,
    handle_nan_loss: Optional[str] = None,
    loss_spike_detection: Optional[bool] = None,
    loss_spike_threshold: Optional[float] = None,
    gradient_explosion_detection: Optional[bool] = None,
    
    # Distributed Training Parameters
    distributed: Optional[bool] = None,
    world_size: Optional[int] = None,
    rank: Optional[int] = None,
    local_rank: Optional[int] = None,
    ddp_backend: Optional[str] = None,
    find_unused_parameters: Optional[bool] = None,
    broadcast_buffers: Optional[bool] = None,
    gradient_as_bucket_view: Optional[bool] = None,
    
    # Checkpointing and Saving Parameters
    save_checkpoint: Optional[bool] = None,
    checkpoint_frequency: Optional[int] = None,
    checkpoint_path: Optional[str] = None,
    save_best_model: Optional[bool] = None,
    best_metric: Optional[str] = None,
    model_state_dict: Optional[bool] = None,
    save_optimizer_state: Optional[bool] = None,
    save_scheduler_state: Optional[bool] = None,
    
    # Early Stopping Parameters
    early_stopping: Optional[bool] = None,
    early_stopping_patience: Optional[int] = None,
    early_stopping_delta: Optional[float] = None,
    early_stopping_metric: Optional[str] = None,
    early_stopping_mode: Optional[str] = None,
    restore_best_weights: Optional[bool] = None,
    
    # Debugging and Profiling Parameters
    debug_mode: Optional[bool] = None,
    verbose: Optional[bool] = None,
    log_level: Optional[str] = None,
    profile_training: Optional[bool] = None,
    profile_memory: Optional[bool] = None,
    profile_compute: Optional[bool] = None,
    timing_analysis: Optional[bool] = None,
    bottleneck_detection: Optional[bool] = None,
    
    # Advanced Training Techniques Parameters
    teacher_forcing_ratio: Optional[float] = None,
    curriculum_learning: Optional[bool] = None,
    curriculum_schedule: Optional[str] = None,
    progressive_training: Optional[bool] = None,
    adaptive_training: Optional[bool] = None,
    self_supervised_pretext: Optional[bool] = None,
    
    # Regularization Parameters
    dropout_rate: Optional[float] = None,
    weight_decay: Optional[float] = None,
    l1_regularization: Optional[float] = None,
    l2_regularization: Optional[float] = None,
    spectral_normalization: Optional[bool] = None,
    batch_norm_momentum: Optional[float] = None,
    layer_norm_eps: Optional[float] = None,
    
    # Loss Smoothing and Stability Parameters
    label_smoothing: Optional[float] = None,
    focal_loss_alpha: Optional[float] = None,
    focal_loss_gamma: Optional[float] = None,
    loss_smoothing: Optional[str] = None,
    stable_loss_computation: Optional[bool] = None,
    
    # Batch Processing Parameters
    batch_processing_mode: Optional[str] = None,
    variable_batch_size: Optional[bool] = None,
    dynamic_batching: Optional[bool] = None,
    batch_size_adaptation: Optional[str] = None,
    max_tokens_per_batch: Optional[int] = None,
    
    # Hardware Optimization Parameters
    use_gpu: Optional[bool] = None,
    multi_gpu: Optional[bool] = None,
    gpu_ids: Optional[List[int]] = None,
    device_placement: Optional[str] = None,
    tensor_parallel: Optional[bool] = None,
    data_parallel: Optional[bool] = None,
    pipeline_parallel: Optional[bool] = None,
    
    # Random State and Reproducibility Parameters
    random_seed: Optional[int] = None,
    deterministic: Optional[bool] = None,
    reproducible: Optional[bool] = None,
    seed_workers: Optional[bool] = None,
    
    # Custom Callback Parameters
    callbacks: Optional[List[Callable]] = None,
    custom_training_step: Optional[Callable] = None,
    custom_loss_computation: Optional[Callable] = None,
    custom_metric_computation: Optional[Callable] = None,
    pre_batch_callback: Optional[Callable] = None,
    post_batch_callback: Optional[Callable] = None,
    
    # Export and Logging Parameters
    export_metrics: Optional[bool] = None,
    export_path: Optional[str] = None,
    tensorboard_logging: Optional[bool] = None,
    wandb_logging: Optional[bool] = None,
    mlflow_logging: Optional[bool] = None,
    log_images: Optional[bool] = None,
    log_gradients: Optional[bool] = None,
    log_weights: Optional[bool] = None,
    
    # Compatibility Parameters
    pytorch_version_check: Optional[bool] = None,
    legacy_mode: Optional[bool] = None,
    backward_compatibility: Optional[bool] = None,
    version_specific_optimizations: Optional[bool] = None,
    
    # Error Handling Parameters
    error_handling: Optional[str] = None,
    continue_on_error: Optional[bool] = None,
    max_retries: Optional[int] = None,
    fallback_mode: Optional[bool] = None,
    graceful_degradation: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_features: Optional[bool] = None,
    experimental_optimizations: Optional[bool] = None,
    beta_features: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    training_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Tuple[float, Dict[str, Any]]:
    
    # Initialize default return values FIRST
    default_loss = float('inf')
    default_metrics = {
        'loss': default_loss,
        'epoch': epoch if epoch is not None else 0,
        'num_batches': 0,
        'num_samples': 0,
        'training_completed': False,
        'error': None
    }
    
    # Start timing
    start_time = datetime.now()
    epoch_start_time = time.time()
    
    # Initialize variables that will be used in finally block
    num_batches = 0
    num_samples = 0
    total_loss = 0.0
    batch_idx = -1
    pbar = None
    profiler = None
    original_level = None
    
    try:
        # Initialize configuration with comprehensive defaults
        if config is None:
            try:
                config = get_current_config() if 'get_current_config' in globals() else {}
            except Exception:
                config = {}
        
        # Apply training-specific configuration
        if training_config:
            config.setdefault('training', {}).update(training_config)
        
        # Apply all parameters to configuration
        final_config = {}
        
        # Merge with existing config
        final_config.update(config)
        
        # Apply individual parameters with intelligent organization
        params = locals().copy()
        params.update(kwargs)
        
        # Remove non-parameter items
        params_to_remove = {
            'config', 'training_config', 'kwargs', 'start_time', 'epoch_start_time',
            'datetime', 'traceback', 'time', 'gc', 'warnings', 'defaultdict', 'deque',
            'nullcontext', 'nn', 'optim', 'DataLoader', 'GradScaler', 'autocast'
        }
        
        cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
        
        # Organize parameters into logical sections
        param_sections = {
            'core_training': [
                'learning_rate', 'batch_size', 'gradient_clip', 'gradient_accumulation_steps',
                'max_grad_norm', 'gradient_clipping_mode', 'gradient_scaling'
            ],
            'mixed_precision': [
                'mixed_precision', 'amp_enabled', 'scaler', 'loss_scaling',
                'dynamic_loss_scaling', 'init_scale', 'growth_factor', 'backoff_factor',
                'growth_interval'
            ],
            'scheduler': [
                'scheduler_step_on_epoch', 'scheduler_step_on_batch', 'scheduler_step_after_epoch',
                'scheduler_metric', 'warmup_steps', 'warmup_scheduler'
            ],
            'loss_function': [
                'loss_function', 'loss_weights', 'multi_task_learning', 'auxiliary_loss_weight',
                'regularization_loss_weight', 'reconstruction_loss_weight'
            ],
            'monitoring': [
                'track_metrics', 'metrics_to_track', 'calculate_detailed_metrics',
                'metrics_frequency', 'log_frequency', 'progress_bar', 'progress_bar_desc'
            ],
            'performance': [
                'performance_mode', 'benchmark_mode', 'cudnn_benchmark', 'cudnn_deterministic',
                'channels_last', 'compile_model', 'torch_compile_mode'
            ],
            'memory_management': [
                'memory_efficient', 'memory_optimization', 'gradient_checkpointing',
                'empty_cache_frequency', 'gc_collection_frequency', 'pin_memory',
                'non_blocking_transfer'
            ],
            'data_processing': [
                'data_preprocessing', 'input_transforms', 'target_transforms',
                'augmentation_during_training', 'mixup_alpha', 'cutmix_alpha'
            ],
            'validation': [
                'validate_inputs', 'check_finite', 'detect_anomaly', 'handle_nan_loss',
                'loss_spike_detection', 'loss_spike_threshold', 'gradient_explosion_detection'
            ],
            'distributed': [
                'distributed', 'world_size', 'rank', 'local_rank', 'ddp_backend',
                'find_unused_parameters', 'broadcast_buffers', 'gradient_as_bucket_view'
            ],
            'checkpointing': [
                'save_checkpoint', 'checkpoint_frequency', 'checkpoint_path',
                'save_best_model', 'best_metric', 'model_state_dict',
                'save_optimizer_state', 'save_scheduler_state'
            ],
            'early_stopping': [
                'early_stopping', 'early_stopping_patience', 'early_stopping_delta',
                'early_stopping_metric', 'early_stopping_mode', 'restore_best_weights'
            ],
            'debugging': [
                'debug_mode', 'verbose', 'log_level', 'profile_training',
                'profile_memory', 'profile_compute', 'timing_analysis', 'bottleneck_detection'
            ],
            'advanced_training': [
                'teacher_forcing_ratio', 'curriculum_learning', 'curriculum_schedule',
                'progressive_training', 'adaptive_training', 'self_supervised_pretext'
            ],
            'regularization': [
                'dropout_rate', 'weight_decay', 'l1_regularization', 'l2_regularization',
                'spectral_normalization', 'batch_norm_momentum', 'layer_norm_eps'
            ],
            'loss_stability': [
                'label_smoothing', 'focal_loss_alpha', 'focal_loss_gamma',
                'loss_smoothing', 'stable_loss_computation'
            ],
            'batch_processing': [
                'batch_processing_mode', 'variable_batch_size', 'dynamic_batching',
                'batch_size_adaptation', 'max_tokens_per_batch'
            ],
            'hardware_optimization': [
                'use_gpu', 'multi_gpu', 'gpu_ids', 'device_placement',
                'tensor_parallel', 'data_parallel', 'pipeline_parallel'
            ],
            'reproducibility': [
                'random_seed', 'deterministic', 'reproducible', 'seed_workers'
            ],
            'callbacks': [
                'callbacks', 'custom_training_step', 'custom_loss_computation',
                'custom_metric_computation', 'pre_batch_callback', 'post_batch_callback'
            ],
            'export_logging': [
                'export_metrics', 'export_path', 'tensorboard_logging', 'wandb_logging',
                'mlflow_logging', 'log_images', 'log_gradients', 'log_weights'
            ],
            'compatibility': [
                'pytorch_version_check', 'legacy_mode', 'backward_compatibility',
                'version_specific_optimizations'
            ],
            'error_handling': [
                'error_handling', 'continue_on_error', 'max_retries',
                'fallback_mode', 'graceful_degradation'
            ],
            'experimental': [
                'experimental_features', 'experimental_optimizations', 'beta_features'
            ]
        }
        
        # Apply parameters to appropriate sections
        for section, param_list in param_sections.items():
            section_config = final_config.setdefault(section, {})
            for param in param_list:
                if param in cleaned_params:
                    section_config[param] = cleaned_params[param]
        
        # Set up comprehensive defaults
        core_config = final_config.setdefault('core_training', {})
        mixed_precision_config = final_config.setdefault('mixed_precision', {})
        scheduler_config = final_config.setdefault('scheduler', {})
        loss_config = final_config.setdefault('loss_function', {})
        monitoring_config = final_config.setdefault('monitoring', {})
        performance_config = final_config.setdefault('performance', {})
        memory_config = final_config.setdefault('memory_management', {})
        data_processing_config = final_config.setdefault('data_processing', {})
        validation_config = final_config.setdefault('validation', {})
        distributed_config = final_config.setdefault('distributed', {})
        checkpoint_config = final_config.setdefault('checkpointing', {})
        early_stopping_config = final_config.setdefault('early_stopping', {})
        debug_config = final_config.setdefault('debugging', {})
        advanced_config = final_config.setdefault('advanced_training', {})
        regularization_config = final_config.setdefault('regularization', {})
        loss_stability_config = final_config.setdefault('loss_stability', {})
        batch_config = final_config.setdefault('batch_processing', {})
        hardware_config = final_config.setdefault('hardware_optimization', {})
        reproducibility_config = final_config.setdefault('reproducibility', {})
        callback_config = final_config.setdefault('callbacks', {})
        export_config = final_config.setdefault('export_logging', {})
        compatibility_config = final_config.setdefault('compatibility', {})
        error_config = final_config.setdefault('error_handling', {})
        experimental_config = final_config.setdefault('experimental', {})
        
        # Apply intelligent defaults with system awareness
        learning_rate = core_config.setdefault('learning_rate', LEARNING_RATE)
        batch_size = core_config.setdefault('batch_size', DEFAULT_BATCH_SIZE)
        gradient_clip = core_config.setdefault('gradient_clip', GRADIENT_CLIP)
        gradient_accumulation_steps = core_config.setdefault('gradient_accumulation_steps', GRADIENT_ACCUMULATION_STEPS)
        max_grad_norm = core_config.setdefault('max_grad_norm', gradient_clip)
        gradient_clipping_mode = core_config.setdefault('gradient_clipping_mode', 'norm')
        gradient_scaling = core_config.setdefault('gradient_scaling', True)
        
        # Mixed precision defaults
        mixed_precision = mixed_precision_config.setdefault('mixed_precision', 
                                                           MIXED_PRECISION and torch.cuda.is_available())
        amp_enabled = mixed_precision_config.setdefault('amp_enabled', mixed_precision)
        dynamic_loss_scaling = mixed_precision_config.setdefault('dynamic_loss_scaling', True)
        init_scale = mixed_precision_config.setdefault('init_scale', 65536.0)
        growth_factor = mixed_precision_config.setdefault('growth_factor', 2.0)
        backoff_factor = mixed_precision_config.setdefault('backoff_factor', 0.5)
        growth_interval = mixed_precision_config.setdefault('growth_interval', 2000)
        
        # Scheduler defaults
        scheduler_step_on_epoch = scheduler_config.setdefault('scheduler_step_on_epoch', True)
        scheduler_step_on_batch = scheduler_config.setdefault('scheduler_step_on_batch', False)
        scheduler_step_after_epoch = scheduler_config.setdefault('scheduler_step_after_epoch', False)
        scheduler_metric = scheduler_config.setdefault('scheduler_metric', 'loss')
        
        # Monitoring defaults
        track_metrics = monitoring_config.setdefault('track_metrics', True)
        metrics_to_track = monitoring_config.setdefault('metrics_to_track', 
                                                       ['loss', 'learning_rate', 'gradient_norm', 'batch_time'])
        calculate_detailed_metrics = monitoring_config.setdefault('calculate_detailed_metrics', False)
        metrics_frequency = monitoring_config.setdefault('metrics_frequency', 10)
        log_frequency = monitoring_config.setdefault('log_frequency', 10)
        progress_bar = monitoring_config.setdefault('progress_bar', True)
        progress_bar_desc = monitoring_config.setdefault('progress_bar_desc', f"Epoch {epoch if epoch is not None else 'N/A'}")
        
        # Performance defaults
        performance_mode = performance_config.setdefault('performance_mode', 'standard')
        benchmark_mode = performance_config.setdefault('benchmark_mode', False)
        cudnn_benchmark = performance_config.setdefault('cudnn_benchmark', torch.cuda.is_available())
        cudnn_deterministic = performance_config.setdefault('cudnn_deterministic', False)
        compile_model = performance_config.setdefault('compile_model', False)
        torch_compile_mode = performance_config.setdefault('torch_compile_mode', 'default')
        
        # Memory management defaults
        memory_efficient = memory_config.setdefault('memory_efficient', True)
        memory_optimization = memory_config.setdefault('memory_optimization', 'balanced')
        gradient_checkpointing = memory_config.setdefault('gradient_checkpointing', False)
        empty_cache_frequency = memory_config.setdefault('empty_cache_frequency', 10)
        gc_collection_frequency = memory_config.setdefault('gc_collection_frequency', 100)
        non_blocking_transfer = memory_config.setdefault('non_blocking_transfer', True)
        
        # Data processing defaults
        data_preprocessing = data_processing_config.setdefault('data_preprocessing', False)
        augmentation_during_training = data_processing_config.setdefault('augmentation_during_training', False)
        
        # Validation defaults
        validate_inputs = validation_config.setdefault('validate_inputs', True)
        check_finite = validation_config.setdefault('check_finite', True)
        detect_anomaly = validation_config.setdefault('detect_anomaly', debug_config.get('debug_mode', False))
        handle_nan_loss = validation_config.setdefault('handle_nan_loss', 'error')
        loss_spike_detection = validation_config.setdefault('loss_spike_detection', True)
        loss_spike_threshold = validation_config.setdefault('loss_spike_threshold', 10.0)
        gradient_explosion_detection = validation_config.setdefault('gradient_explosion_detection', True)
        
        # Distributed defaults
        distributed = distributed_config.setdefault('distributed', False)
        find_unused_parameters = distributed_config.setdefault('find_unused_parameters', False)
        broadcast_buffers = distributed_config.setdefault('broadcast_buffers', True)
        gradient_as_bucket_view = distributed_config.setdefault('gradient_as_bucket_view', False)
        
        # Debug defaults
        debug_mode = debug_config.setdefault('debug_mode', False)
        verbose = debug_config.setdefault('verbose', False)
        log_level = debug_config.setdefault('log_level', 'INFO' if verbose else 'WARNING')
        profile_training = debug_config.setdefault('profile_training', False)
        timing_analysis = debug_config.setdefault('timing_analysis', False)
        
        # Error handling defaults
        error_handling = error_config.setdefault('error_handling', 'strict')
        continue_on_error = error_config.setdefault('continue_on_error', False)
        max_retries = error_config.setdefault('max_retries', 3)
        graceful_degradation = error_config.setdefault('graceful_degradation', True)
        
        # Set up logging level
        # if verbose:
        #     original_level = logger.level
        #     logger.setLevel(getattr(logging, log_level.upper()))
        
        logger.info(f"Starting comprehensive training epoch {epoch if epoch is not None else 'N/A'}")
        
        # Parameter validation
        if model is None:
            raise ValueError("Model is required for training")
        if loader is None:
            raise ValueError("DataLoader is required for training")
        if criterion is None:
            raise ValueError("Loss criterion is required for training")
        if optimizer is None:
            raise ValueError("Optimizer is required for training")
        
        # Device configuration
        if device is None:
            device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')
        
        # Set up reproducibility
        if reproducibility_config.get('deterministic', False):
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            if reproducibility_config.get('random_seed'):
                torch.manual_seed(reproducibility_config['random_seed'])
                if torch.cuda.is_available():
                    torch.cuda.manual_seed_all(reproducibility_config['random_seed'])
        elif benchmark_mode or cudnn_benchmark:
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        
        # Initialize training statistics
        training_stats = {
            'start_time': start_time.isoformat(),
            'epoch': epoch,
            'config_applied': final_config,
            'device': str(device),
            'mixed_precision_enabled': mixed_precision,
            'gradient_accumulation_steps': gradient_accumulation_steps,
            'total_batches': len(loader),
            'batch_size': batch_size
        }
        
        # Set up mixed precision scaler
        if mixed_precision and scaler is None:
            scaler = GradScaler(
                enabled=amp_enabled,
                init_scale=init_scale,
                growth_factor=growth_factor,
                backoff_factor=backoff_factor,
                growth_interval=growth_interval
            )
        
        # Model compilation if requested
        if compile_model and hasattr(torch, 'compile'):
            try:
                logger.info(f"Compiling model with mode: {torch_compile_mode}")
                model = torch.compile(model, mode=torch_compile_mode)
                training_stats['model_compiled'] = True
            except Exception as e:
                logger.warning(f"Model compilation failed: {e}")
                training_stats['model_compiled'] = False
        
        # Set model to training mode
        model.train()
        
        # Memory optimization setup
        if memory_efficient:
            if gradient_checkpointing and hasattr(model, 'gradient_checkpointing_enable'):
                try:
                    model.gradient_checkpointing_enable()
                    logger.debug("Enabled gradient checkpointing")
                except Exception as e:
                    logger.warning(f"Failed to enable gradient checkpointing: {e}")
        
        # Initialize metrics tracking
        metrics_tracker = {
            'losses': [],
            'batch_times': [],
            'learning_rates': [],
            'gradient_norms': [],
            'memory_usage': [],
            'detailed_metrics': defaultdict(list) if calculate_detailed_metrics else None
        }
        
        # Initialize loss tracking for spike detection
        if loss_spike_detection:
            recent_losses = deque(maxlen=10)
        
        # Set up alive-progress bar
        if progress_bar:
            try:
                pbar = alive_bar(
                    total=len(loader), 
                    #title=f'Training Epoch {epoch if epoch is not None else "N/A"}\t', 
                    title=f'Training {progress_bar_desc}\t',
                    unit='batches',
                    bar='smooth',
                    spinner='dots',
                    stats=False,  # We'll handle stats manually for better control
                    monitor=True,
                    elapsed=True,
                    stats_end=False
                )
                pbar.__enter__()  # Manually enter the context since we're not using 'with' statement
            except ImportError:
                logger.warning("alive-progress not available, progress bar disabled")
                pbar = None
            except Exception as e:
                logger.warning(f"Failed to initialize alive-progress bar: {e}")
                pbar = None
        else:
            pbar = None
        
        # Initialize profiling if requested
        if profile_training:
            try:
                profiler = torch.profiler.profile(
                    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
                    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),
                    on_trace_ready=torch.profiler.tensorboard_trace_handler(
                        export_config.get('export_path', './profiler_logs')
                    ),
                    record_shapes=True,
                    profile_memory=memory_config.get('profile_memory', True),
                    with_stack=True
                )
                profiler.start()
                training_stats['profiling_enabled'] = True
            except Exception as e:
                logger.warning(f"Failed to initialize profiler: {e}")
                profiler = None
                training_stats['profiling_enabled'] = False
        else:
            profiler = None
        
        # Anomaly detection setup
        anomaly_context = torch.autograd.detect_anomaly() if detect_anomaly else nullcontext()
        
        # Initialize training variables
        total_loss = 0.0
        num_batches = 0
        num_samples = 0
        accumulated_steps = 0
        best_loss = float('inf')
        
        # Initialize batch processing variables
        batch_start_time = None
        data_loading_time = 0
        forward_time = 0
        backward_time = 0
        optimizer_time = 0
        
        # Custom callbacks setup
        callbacks = callback_config.get('callbacks', [])
        custom_training_step = callback_config.get('custom_training_step')
        pre_batch_callback = callback_config.get('pre_batch_callback')
        post_batch_callback = callback_config.get('post_batch_callback')
        
        logger.info(f"Starting epoch with {len(loader)} batches")
        
        # Main training loop
        with anomaly_context:
            # Zero gradients before starting
            optimizer.zero_grad()
            
            for batch_idx, batch in enumerate(loader):
                batch_start_time = time.time()
                
                try:
                    # Pre-batch callback
                    if pre_batch_callback:
                        pre_batch_callback(batch_idx, batch, model, optimizer)
                    
                    # Data loading timing
                    data_load_end_time = time.time()
                    if batch_idx > 0:  # Skip first batch for timing accuracy
                        data_loading_time += (data_load_end_time - batch_start_time)
                    
                    # Move data to device with timing
                    if isinstance(batch, (list, tuple)):
                        inputs = batch[0].to(device, non_blocking=non_blocking_transfer)
                        targets = batch[1].to(device, non_blocking=non_blocking_transfer) if len(batch) > 1 else inputs
                    else:
                        inputs = batch.to(device, non_blocking=non_blocking_transfer)
                        targets = inputs
                    
                    # Input validation
                    if validate_inputs:
                        if check_finite:
                            if not torch.isfinite(inputs).all():
                                if handle_nan_loss == 'error':
                                    raise ValueError(f"Non-finite input values detected in batch {batch_idx}")
                                elif handle_nan_loss == 'skip':
                                    logger.warning(f"Skipping batch {batch_idx} due to non-finite inputs")
                                    continue
                                elif handle_nan_loss == 'clip':
                                    inputs = torch.nan_to_num(inputs, nan=0.0, posinf=1e6, neginf=-1e6)
                    
                    # Apply input transforms if specified
                    if data_processing_config.get('input_transforms'):
                        for transform in data_processing_config['input_transforms']:
                            inputs = transform(inputs)
                    
                    # Apply data augmentation during training
                    if augmentation_during_training:
                        # Mixup augmentation
                        if data_processing_config.get('mixup_alpha', 0) > 0:
                            mixup_alpha = data_processing_config['mixup_alpha']
                            lam = np.random.beta(mixup_alpha, mixup_alpha)
                            batch_size_current = inputs.size(0)
                            index = torch.randperm(batch_size_current).to(device)
                            inputs = lam * inputs + (1 - lam) * inputs[index]
                            if isinstance(targets, torch.Tensor) and targets.numel() > 0:
                                targets = lam * targets + (1 - lam) * targets[index]
                    
                    current_batch_size = inputs.size(0)
                    num_samples += current_batch_size
                    
                    # Forward pass with mixed precision and timing
                    forward_start_time = time.time()
                    
                    if custom_training_step:
                        # Custom training step
                        loss = custom_training_step(model, inputs, targets, criterion, optimizer, scaler, batch_idx)
                    else:
                        autocast_context = get_autocast_context(device, mixed_precision, True)
                        
                        with autocast_context:
                            outputs = model(inputs)
                            
                            # CRITICAL: Ensure targets match outputs for autoencoder
                            if targets.shape != outputs.shape:
                                if hasattr(model, '__class__') and 'autoencoder' in model.__class__.__name__.lower():
                                    targets = inputs  # For autoencoder, target should be input
                                    logger.debug("Set targets = inputs for autoencoder training")
                                else:
                                    # Try to reshape
                                    if outputs.numel() == targets.numel():
                                        targets = targets.view_as(outputs)
                                        logger.debug(f"Reshaped targets to match outputs: {outputs.shape}")
                                    else:
                                        # Try to match dimensions intelligently
                                        batch_size = outputs.size(0)
                                        if len(outputs.shape) == 2:  # [batch_size, features]
                                            output_features = outputs.size(1)
                                            if targets.size(1) != output_features:
                                                if targets.size(1) > output_features:
                                                    # Truncate targets
                                                    targets = targets[:, :output_features]
                                                else:
                                                    # Pad targets
                                                    padding_size = output_features - targets.size(1)
                                                    padding = torch.zeros(batch_size, padding_size, device=device, dtype=targets.dtype)
                                                    targets = torch.cat([targets, padding], dim=1)
                                                logger.debug(f"Adjusted targets shape to {targets.shape}")
                                        else:
                                            # For complex shapes, use inputs as targets
                                            targets = inputs
                                            logger.warning(f"Using inputs as targets for complex shape mismatch")
                            
                            # Custom loss computation with error handling
                            if callback_config.get('custom_loss_computation'):
                                try:
                                    loss = callback_config['custom_loss_computation'](outputs, targets, criterion)
                                except Exception as loss_e:
                                    logger.error(f"Custom loss computation failed: {loss_e}")
                                    if handle_nan_loss == 'skip':
                                        logger.warning(f"Skipping batch {batch_idx} due to custom loss error")
                                        continue
                                    else:
                                        raise
                            else:
                                try:
                                    loss = criterion(outputs, targets)
                                except Exception as loss_e:
                                    logger.error(f"Standard loss computation failed: {loss_e}")
                                    logger.error(f"Shapes - outputs: {outputs.shape}, targets: {targets.shape}")
                                    
                                    # Emergency shape fix
                                    try:
                                        if 'autoencoder' in str(type(model)).lower():
                                            targets = inputs
                                            logger.info("Emergency fix: Set targets = inputs for autoencoder")
                                        else:
                                            # Force reshape
                                            if outputs.dim() == 2 and targets.dim() == 2:
                                                min_features = min(outputs.size(1), targets.size(1))
                                                outputs_fixed = outputs[:, :min_features]
                                                targets_fixed = targets[:, :min_features]
                                                loss = criterion(outputs_fixed, targets_fixed)
                                                logger.info(f"Emergency fix: Used {min_features} features for loss")
                                            else:
                                                targets = targets.view_as(outputs)
                                                loss = criterion(outputs, targets)
                                                logger.info("Emergency fix: Forced target reshape")
                                        
                                        if 'targets_fixed' not in locals():
                                            loss = criterion(outputs, targets)
                                        
                                    except Exception as final_e:
                                        logger.error(f"Final loss computation attempt failed: {final_e}")
                                        if handle_nan_loss == 'skip':
                                            logger.warning(f"Skipping batch {batch_idx} due to unresolvable shape mismatch")
                                            continue
                                        elif handle_nan_loss == 'error':
                                            raise RuntimeError(f"Unresolvable shape mismatch in batch {batch_idx}") from loss_e
                                        else:
                                            # Create a dummy loss to continue training
                                            loss = torch.tensor(0.0, device=device, requires_grad=True)
                                            logger.warning(f"Using dummy loss for batch {batch_idx}")
                            
                            # Scale loss for gradient accumulation
                            loss = loss / gradient_accumulation_steps
                    
                    forward_end_time = time.time()
                    forward_time += (forward_end_time - forward_start_time)
                    
                    # Loss validation
                    if check_finite:
                        if not torch.isfinite(loss):
                            if handle_nan_loss == 'error':
                                raise ValueError(f"Non-finite loss detected in batch {batch_idx}: {loss.item()}")
                            elif handle_nan_loss == 'skip':
                                logger.warning(f"Skipping batch {batch_idx} due to non-finite loss: {loss.item()}")
                                continue
                            elif handle_nan_loss == 'clip':
                                loss = torch.nan_to_num(loss, nan=0.0, posinf=1e6, neginf=-1e6)
                    
                    # Loss spike detection
                    if loss_spike_detection and len(recent_losses) > 0:
                        current_loss = loss.item() * gradient_accumulation_steps
                        avg_recent_loss = sum(recent_losses) / len(recent_losses)
                        if current_loss > avg_recent_loss * loss_spike_threshold:
                            logger.warning(f"Loss spike detected: current={current_loss:.4f}, avg_recent={avg_recent_loss:.4f}")
                            if handle_nan_loss == 'skip':
                                continue
                        recent_losses.append(current_loss)
                    elif loss_spike_detection:
                        recent_losses.append(loss.item() * gradient_accumulation_steps)
                    
                    # Backward pass with timing
                    backward_start_time = time.time()
                    
                    if mixed_precision and scaler is not None:
                        scaler.scale(loss).backward()
                    else:
                        loss.backward()
                    
                    backward_end_time = time.time()
                    backward_time += (backward_end_time - backward_start_time)
                    
                    accumulated_steps += 1
                    
                    # Optimizer step with gradient accumulation
                    if accumulated_steps % gradient_accumulation_steps == 0:
                        optimizer_start_time = time.time()
                        
                        # Gradient clipping
                        if gradient_clip > 0 or max_grad_norm > 0:
                            if mixed_precision and scaler is not None:
                                scaler.unscale_(optimizer)
                            
                            if gradient_clipping_mode == 'norm':
                                grad_norm = torch.nn.utils.clip_grad_norm_(
                                    model.parameters(), 
                                    max_grad_norm or gradient_clip
                                )
                            elif gradient_clipping_mode == 'value':
                                grad_norm = torch.nn.utils.clip_grad_value_(
                                    model.parameters(), 
                                    gradient_clip
                                )
                                # Calculate norm for tracking
                                grad_norm = torch.sqrt(sum(param.grad.data.norm()**2 for param in model.parameters() if param.grad is not None))
                            else:
                                grad_norm = torch.sqrt(sum(param.grad.data.norm()**2 for param in model.parameters() if param.grad is not None))
                        else:
                            grad_norm = torch.sqrt(sum(param.grad.data.norm()**2 for param in model.parameters() if param.grad is not None))
                        
                        # Gradient explosion detection
                        if gradient_explosion_detection and grad_norm > 100.0:
                            logger.warning(f"Large gradient norm detected: {grad_norm:.4f}")
                        
                        # Optimizer step
                        if mixed_precision and scaler is not None:
                            scaler.step(optimizer)
                            scaler.update()
                        else:
                            optimizer.step()
                        
                        # Scheduler step on batch if configured
                        if scheduler and scheduler_step_on_batch:
                            if hasattr(scheduler, 'step'):
                                try:
                                    if 'ReduceLROnPlateau' in str(type(scheduler)):
                                        # Don't step ReduceLROnPlateau on batch
                                        pass
                                    else:
                                        scheduler.step()
                                except Exception as e:
                                    logger.warning(f"Scheduler step failed: {e}")
                        
                        optimizer.zero_grad()
                        
                        optimizer_end_time = time.time()
                        optimizer_time += (optimizer_end_time - optimizer_start_time)
                        
                        # Store gradient norm for tracking
                        if track_metrics and 'gradient_norm' in metrics_to_track:
                            metrics_tracker['gradient_norms'].append(grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm)
                    
                    # Update running loss
                    total_loss += loss.item() * gradient_accumulation_steps
                    num_batches += 1
                    
                    # Store metrics
                    if track_metrics:
                        if 'loss' in metrics_to_track:
                            metrics_tracker['losses'].append(loss.item() * gradient_accumulation_steps)
                        
                        if 'learning_rate' in metrics_to_track:
                            current_lr = optimizer.param_groups[0]['lr']
                            metrics_tracker['learning_rates'].append(current_lr)
                        
                        if 'batch_time' in metrics_to_track and batch_start_time:
                            batch_time = time.time() - batch_start_time
                            metrics_tracker['batch_times'].append(batch_time)
                        
                        # Memory usage tracking
                        if 'memory_usage' in metrics_to_track and torch.cuda.is_available():
                            memory_allocated = torch.cuda.memory_allocated(device) / 1024**2  # MB
                            metrics_tracker['memory_usage'].append(memory_allocated)
                    
                    # Memory management
                    if memory_efficient:
                        if batch_idx % empty_cache_frequency == 0 and torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        
                        if batch_idx % gc_collection_frequency == 0:
                            gc.collect()
                    
                    # Logging
                    if batch_idx % log_frequency == 0 and batch_idx > 0:
                        avg_loss = total_loss / num_batches
                        current_lr = optimizer.param_groups[0]['lr']
                        
                        if timing_analysis:
                            logger.info(
                                f"Batch {batch_idx}/{len(loader)} | "
                                f"Loss: {avg_loss:.6f} | "
                                f"LR: {current_lr:.2e} | "
                                f"Forward: {forward_time/batch_idx:.3f}s | "
                                f"Backward: {backward_time/batch_idx:.3f}s | "
                                f"Optimizer: {optimizer_time/(accumulated_steps//gradient_accumulation_steps):.3f}s"
                            )
                        else:
                            logger.info(
                                f"Batch {batch_idx}/{len(loader)} | "
                                f"Loss: {avg_loss:.6f} | "
                                f"LR: {current_lr:.2e}"
                            )
                    
                    # Update alive-progress bar with enhanced styling
                    if pbar:
                        current_loss_display = loss.item() * gradient_accumulation_steps
                        avg_loss_display = total_loss / num_batches
                        current_lr_display = optimizer.param_groups[0]['lr']
                        
                        # Format the text for the progress bar
                        progress_text = (
                            f"Loss: {current_loss_display:.4f} | "
                            f"Avg: {avg_loss_display:.4f} | "
                            f"LR: {current_lr_display:.2e} | "
                            f"Batch: {batch_idx+1}/{len(loader)}"
                        )
                        
                        # Update the progress bar text
                        pbar.text(progress_text)
                        
                        # Update the progress
                        pbar()
                    
                    # Post-batch callback
                    if post_batch_callback:
                        post_batch_callback(batch_idx, batch, model, optimizer, loss)
                    
                    # Profiler step
                    if profiler:
                        profiler.step()
                    
                    # Execute callbacks
                    for callback in callbacks:
                        try:
                            callback(
                                batch_idx=batch_idx,
                                loss=loss.item() * gradient_accumulation_steps,
                                model=model,
                                optimizer=optimizer,
                                metrics=metrics_tracker
                            )
                        except Exception as e:
                            logger.warning(f"Callback execution failed: {e}")
                    
                except Exception as batch_error:
                    logger.error(f"Error in batch {batch_idx}: {batch_error}")
                    if continue_on_error:
                        if error_handling == 'skip':
                            continue
                        elif error_handling == 'retry':
                            retry_count = 0
                            while retry_count < max_retries:
                                try:
                                    logger.info(f"Retrying batch {batch_idx}, attempt {retry_count + 1}")
                                    optimizer.zero_grad()
                                    break
                                except Exception as retry_e:
                                    retry_count += 1
                                    logger.warning(f"Retry {retry_count} failed: {retry_e}")
                            
                            if retry_count >= max_retries:
                                logger.error(f"Max retries exceeded for batch {batch_idx}")
                                if graceful_degradation:
                                    continue
                                else:
                                    raise
                        else:
                            # Add dummy values to continue
                            total_loss += 1.0
                            num_batches += 1
                            continue
                    else:
                        # Return partial results
                        avg_loss = total_loss / max(num_batches, 1)
                        partial_metrics = default_metrics.copy()
                        partial_metrics.update({
                            'loss': avg_loss,
                            'num_batches': num_batches,
                            'num_samples': num_samples,
                            'error': f"Batch {batch_idx} failed: {str(batch_error)}",
                            'partial_completion': True
                        })
                        return avg_loss, partial_metrics
        
        # Handle remaining gradients after main loop
        if accumulated_steps % gradient_accumulation_steps != 0:
            if mixed_precision and scaler is not None:
                if gradient_clip > 0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                scaler.step(optimizer)
                scaler.update()
            else:
                if gradient_clip > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                optimizer.step()
        
        # Scheduler step after epoch
        if scheduler and (scheduler_step_on_epoch or scheduler_step_after_epoch):
            try:
                if 'ReduceLROnPlateau' in str(type(scheduler)):
                    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
                    scheduler.step(avg_loss)
                else:
                    scheduler.step()
                logger.debug("Scheduler step completed")
            except Exception as e:
                logger.warning(f"Scheduler step failed: {e}")
        
        # Calculate final metrics
        epoch_end_time = time.time()
        total_epoch_time = epoch_end_time - epoch_start_time
        avg_loss = total_loss / num_batches if num_batches > 0 else default_loss
        
        # Comprehensive metrics dictionary
        metrics = {
            'loss': avg_loss,
            'epoch': epoch if epoch is not None else 0,
            'num_batches': num_batches,
            'num_samples': num_samples,
            'total_epoch_time': total_epoch_time,
            'avg_batch_time': total_epoch_time / num_batches if num_batches > 0 else 0,
            'learning_rate': optimizer.param_groups[0]['lr'],
            'final_learning_rate': optimizer.param_groups[0]['lr'],
            'gradient_accumulation_steps': gradient_accumulation_steps,
            'effective_batch_size': batch_size * gradient_accumulation_steps,
            'mixed_precision_enabled': mixed_precision,
            'scaler_scale': scaler.get_scale() if scaler else None,
            'memory_allocated_mb': torch.cuda.memory_allocated(device) / 1024**2 if torch.cuda.is_available() else 0,
            'memory_reserved_mb': torch.cuda.memory_reserved(device) / 1024**2 if torch.cuda.is_available() else 0,
            'samples_per_second': num_samples / total_epoch_time if total_epoch_time > 0 else 0,
            'batches_per_second': num_batches / total_epoch_time if total_epoch_time > 0 else 0,
            'timing': {
                'forward_time': forward_time,
                'backward_time': backward_time,
                'optimizer_time': optimizer_time,
                'data_loading_time': data_loading_time,
                'avg_forward_time': forward_time / num_batches if num_batches > 0 else 0,
                'avg_backward_time': backward_time / num_batches if num_batches > 0 else 0,
                'avg_optimizer_time': optimizer_time / (accumulated_steps // gradient_accumulation_steps) if accumulated_steps > 0 else 0,
                'avg_data_loading_time': data_loading_time / num_batches if num_batches > 0 else 0
            },
            'config_applied': final_config,
            'device': str(device),
            'distributed_training': distributed,
            'training_stable': not any(loss > 1000 for loss in metrics_tracker['losses'][-10:]) if metrics_tracker['losses'] else True,
            'loss_spikes_detected': len([l for l in metrics_tracker['losses'] if l > avg_loss * loss_spike_threshold]) if loss_spike_detection else 0,
            'training_completed': True
        }
        
        # Add tracked metrics
        if track_metrics:
            if metrics_tracker['losses']:
                metrics['loss_std'] = float(np.std(metrics_tracker['losses']))
                metrics['min_loss'] = float(np.min(metrics_tracker['losses']))
                metrics['max_loss'] = float(np.max(metrics_tracker['losses']))
            
            if metrics_tracker['gradient_norms']:
                metrics['avg_gradient_norm'] = float(np.mean(metrics_tracker['gradient_norms']))
                metrics['max_gradient_norm'] = float(np.max(metrics_tracker['gradient_norms']))
                metrics['gradient_norm_std'] = float(np.std(metrics_tracker['gradient_norms']))
            
            if metrics_tracker['batch_times']:
                metrics['avg_batch_time'] = float(np.mean(metrics_tracker['batch_times']))
                metrics['batch_time_std'] = float(np.std(metrics_tracker['batch_times']))
            
            if metrics_tracker['memory_usage']:
                metrics['avg_memory_usage_mb'] = float(np.mean(metrics_tracker['memory_usage']))
                metrics['max_memory_usage_mb'] = float(np.max(metrics_tracker['memory_usage']))
        
        logger.info(f"Training epoch completed: loss={avg_loss:.6f}, batches={num_batches}")
        return avg_loss, metrics
        
    except Exception as e:
        logger.error(f"Training epoch failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Calculate what we can from available data
        try:
            final_loss = total_loss / max(num_batches, 1) if num_batches > 0 else default_loss
        except:
            final_loss = default_loss
        
        error_metrics = default_metrics.copy()
        error_metrics.update({
            'loss': final_loss,
            'num_batches': num_batches,
            'num_samples': num_samples,
            'error': str(e),
            'error_type': type(e).__name__,
            'training_failed': True
        })
        
        if graceful_degradation:
            logger.warning(f"Returning error results: {error_metrics}")
            return final_loss, error_metrics
        else:
            # Still return something to avoid unpacking errors
            return default_loss, error_metrics
    
    finally:
        # Restore original logging level
        # if verbose and original_level is not None:
        #     logger.setLevel(original_level)
        
        # Cleanup
        try:
            if pbar:
                # For alive_bar, we need to manually close it since we manually entered the context
                if hasattr(pbar, '__exit__'):
                    pbar.__exit__(None, None, None)
                else:
                    # Fallback for different alive_bar versions
                    pbar.close()
            
            if profiler:
                profiler.stop()
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        except:
            pass

def validate(
    # Core Validation Parameters
    model: Optional[nn.Module] = None,
    loader: Optional[DataLoader] = None,
    criterion: Optional[nn.Module] = None,
    device: Optional[torch.device] = None,
    epoch: Optional[int] = None,
    
    # Validation Configuration Parameters
    validation_type: Optional[str] = None,
    batch_size: Optional[int] = None,
    eval_mode: Optional[bool] = None,
    no_grad: Optional[bool] = None,
    inference_mode: Optional[bool] = None,
    
    # Mixed Precision Parameters
    mixed_precision: Optional[bool] = None,
    amp_enabled: Optional[bool] = None,
    scaler: Optional[GradScaler] = None,
    autocast_enabled: Optional[bool] = None,
    
    # Metrics and Evaluation Parameters
    calculate_metrics: Optional[bool] = None,
    metrics_to_calculate: Optional[List[str]] = None,
    detailed_metrics: Optional[bool] = None,
    per_sample_metrics: Optional[bool] = None,
    aggregate_metrics: Optional[bool] = None,
    statistical_metrics: Optional[bool] = None,
    distribution_analysis: Optional[bool] = None,
    anomaly_detection: Optional[bool] = None,
    threshold_analysis: Optional[bool] = None,
    
    # Performance and Optimization Parameters
    performance_mode: Optional[str] = None,
    benchmark_mode: Optional[bool] = None,
    memory_efficient: Optional[bool] = None,
    batch_processing: Optional[bool] = None,
    parallel_validation: Optional[bool] = None,
    
    # Data Processing Parameters
    data_preprocessing: Optional[bool] = None,
    input_transforms: Optional[List[Callable]] = None,
    target_transforms: Optional[List[Callable]] = None,
    normalize_outputs: Optional[bool] = None,
    denormalize_outputs: Optional[bool] = None,
    
    # Validation Quality Control Parameters
    validate_inputs: Optional[bool] = None,
    check_finite: Optional[bool] = None,
    handle_nan_outputs: Optional[str] = None,
    handle_inf_outputs: Optional[str] = None,
    output_validation: Optional[bool] = None,
    consistency_checks: Optional[bool] = None,
    
    # Reconstruction and Anomaly Parameters
    reconstruction_threshold: Optional[float] = None,
    anomaly_threshold: Optional[float] = None,
    percentile_threshold: Optional[float] = None,
    adaptive_threshold: Optional[bool] = None,
    threshold_method: Optional[str] = None,
    contamination_rate: Optional[float] = None,
    
    # Statistical Analysis Parameters
    confidence_interval: Optional[float] = None,
    significance_level: Optional[float] = None,
    statistical_tests: Optional[bool] = None,
    distribution_tests: Optional[bool] = None,
    normality_tests: Optional[bool] = None,
    outlier_analysis: Optional[bool] = None,
    
    # Monitoring and Logging Parameters
    progress_bar: Optional[bool] = None,
    progress_bar_desc: Optional[str] = None,
    log_frequency: Optional[int] = None,
    verbose: Optional[bool] = None,
    debug_mode: Optional[bool] = None,
    timing_analysis: Optional[bool] = None,
    
    # Memory Management Parameters
    empty_cache_frequency: Optional[int] = None,
    gc_collection_frequency: Optional[int] = None,
    memory_monitoring: Optional[bool] = None,
    
    # Distributed Validation Parameters
    distributed: Optional[bool] = None,
    world_size: Optional[int] = None,
    rank: Optional[int] = None,
    reduce_metrics: Optional[bool] = None,
    
    # Export and Visualization Parameters
    save_results: Optional[bool] = None,
    results_path: Optional[str] = None,
    save_predictions: Optional[bool] = None,
    save_reconstructions: Optional[bool] = None,
    visualization: Optional[bool] = None,
    plot_distributions: Optional[bool] = None,
    plot_reconstructions: Optional[bool] = None,
    
    # Cross-validation Parameters
    cross_validation: Optional[bool] = None,
    cv_folds: Optional[int] = None,
    stratified_cv: Optional[bool] = None,
    cv_metrics: Optional[bool] = None,
    
    # Advanced Analysis Parameters
    feature_importance: Optional[bool] = None,
    attention_analysis: Optional[bool] = None,
    gradient_analysis: Optional[bool] = None,
    activation_analysis: Optional[bool] = None,
    layer_analysis: Optional[bool] = None,
    
    # Error Handling Parameters
    error_handling: Optional[str] = None,
    continue_on_error: Optional[bool] = None,
    max_retries: Optional[int] = None,
    graceful_degradation: Optional[bool] = None,
    fallback_mode: Optional[bool] = None,
    
    # Compatibility Parameters
    legacy_mode: Optional[bool] = None,
    backward_compatibility: Optional[bool] = None,
    version_check: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_features: Optional[bool] = None,
    experimental_metrics: Optional[bool] = None,
    beta_features: Optional[bool] = None,
    
    # Custom Functions Parameters
    custom_metric_fn: Optional[Callable] = None,
    custom_threshold_fn: Optional[Callable] = None,
    custom_analysis_fn: Optional[Callable] = None,
    validation_callbacks: Optional[List[Callable]] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    validation_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Tuple[float, np.ndarray, Dict[str, Any]]:
    
    # Start timing
    start_time = datetime.now()
    validation_start_time = time.time()
    
    # Initialize configuration with comprehensive defaults
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply validation-specific configuration
    if validation_config:
        config.setdefault('validation', {}).update(validation_config)
    
    # Apply all parameters to configuration
    final_config = {}
    
    # Merge with existing config
    final_config.update(config)
    
    # Apply individual parameters with intelligent organization
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'validation_config', 'kwargs', 'start_time', 'validation_start_time',
        'datetime', 'traceback', 'time', 'gc', 'warnings', 'defaultdict', 'deque',
        'nullcontext', 'nn', 'optim', 'DataLoader', 'GradScaler', 'autocast', 'stats'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Organize parameters into logical sections
    param_sections = {
        'core_validation': [
            'validation_type', 'batch_size', 'eval_mode', 'no_grad', 'inference_mode'
        ],
        'mixed_precision': [
            'mixed_precision', 'amp_enabled', 'scaler', 'autocast_enabled'
        ],
        'metrics_evaluation': [
            'calculate_metrics', 'metrics_to_calculate', 'detailed_metrics',
            'per_sample_metrics', 'aggregate_metrics', 'statistical_metrics',
            'distribution_analysis', 'anomaly_detection', 'threshold_analysis'
        ],
        'performance': [
            'performance_mode', 'benchmark_mode', 'memory_efficient',
            'batch_processing', 'parallel_validation'
        ],
        'data_processing': [
            'data_preprocessing', 'input_transforms', 'target_transforms',
            'normalize_outputs', 'denormalize_outputs'
        ],
        'validation_quality': [
            'validate_inputs', 'check_finite', 'handle_nan_outputs',
            'handle_inf_outputs', 'output_validation', 'consistency_checks'
        ],
        'reconstruction_anomaly': [
            'reconstruction_threshold', 'anomaly_threshold', 'percentile_threshold',
            'adaptive_threshold', 'threshold_method', 'contamination_rate'
        ],
        'statistical_analysis': [
            'confidence_interval', 'significance_level', 'statistical_tests',
            'distribution_tests', 'normality_tests', 'outlier_analysis'
        ],
        'monitoring_logging': [
            'progress_bar', 'progress_bar_desc', 'log_frequency',
            'verbose', 'debug_mode', 'timing_analysis'
        ],
        'memory_management': [
            'empty_cache_frequency', 'gc_collection_frequency', 'memory_monitoring'
        ],
        'distributed': [
            'distributed', 'world_size', 'rank', 'reduce_metrics'
        ],
        'export_visualization': [
            'save_results', 'results_path', 'save_predictions', 'save_reconstructions',
            'visualization', 'plot_distributions', 'plot_reconstructions'
        ],
        'cross_validation': [
            'cross_validation', 'cv_folds', 'stratified_cv', 'cv_metrics'
        ],
        'advanced_analysis': [
            'feature_importance', 'attention_analysis', 'gradient_analysis',
            'activation_analysis', 'layer_analysis'
        ],
        'error_handling': [
            'error_handling', 'continue_on_error', 'max_retries',
            'graceful_degradation', 'fallback_mode'
        ],
        'compatibility': [
            'legacy_mode', 'backward_compatibility', 'version_check'
        ],
        'experimental': [
            'experimental_features', 'experimental_metrics', 'beta_features'
        ],
        'custom_functions': [
            'custom_metric_fn', 'custom_threshold_fn', 'custom_analysis_fn',
            'validation_callbacks'
        ]
    }
    
    # Apply parameters to appropriate sections
    for section, param_list in param_sections.items():
        section_config = final_config.setdefault(section, {})
        for param in param_list:
            if param in cleaned_params:
                section_config[param] = cleaned_params[param]
    
    # Set up comprehensive defaults
    core_config = final_config.setdefault('core_validation', {})
    mixed_precision_config = final_config.setdefault('mixed_precision', {})
    metrics_config = final_config.setdefault('metrics_evaluation', {})
    performance_config = final_config.setdefault('performance', {})
    data_processing_config = final_config.setdefault('data_processing', {})
    validation_quality_config = final_config.setdefault('validation_quality', {})
    reconstruction_config = final_config.setdefault('reconstruction_anomaly', {})
    statistical_config = final_config.setdefault('statistical_analysis', {})
    monitoring_config = final_config.setdefault('monitoring_logging', {})
    memory_config = final_config.setdefault('memory_management', {})
    distributed_config = final_config.setdefault('distributed', {})
    export_config = final_config.setdefault('export_visualization', {})
    cv_config = final_config.setdefault('cross_validation', {})
    advanced_config = final_config.setdefault('advanced_analysis', {})
    error_config = final_config.setdefault('error_handling', {})
    experimental_config = final_config.setdefault('experimental', {})
    custom_config = final_config.setdefault('custom_functions', {})
    
    # Apply intelligent defaults with system awareness
    validation_type = core_config.setdefault('validation_type', 'standard')
    eval_mode = core_config.setdefault('eval_mode', True)
    no_grad = core_config.setdefault('no_grad', True)
    inference_mode = core_config.setdefault('inference_mode', False)
    
    # Mixed precision defaults
    mixed_precision = mixed_precision_config.setdefault('mixed_precision', 
                                                       MIXED_PRECISION and torch.cuda.is_available())
    amp_enabled = mixed_precision_config.setdefault('amp_enabled', mixed_precision)
    autocast_enabled = mixed_precision_config.setdefault('autocast_enabled', mixed_precision)
    
    # Metrics defaults
    calculate_metrics = metrics_config.setdefault('calculate_metrics', True)
    metrics_to_calculate = metrics_config.setdefault('metrics_to_calculate', 
                                                    ['loss', 'mse', 'mae', 'reconstruction_error'])
    detailed_metrics = metrics_config.setdefault('detailed_metrics', True)
    per_sample_metrics = metrics_config.setdefault('per_sample_metrics', True)
    aggregate_metrics = metrics_config.setdefault('aggregate_metrics', True)
    statistical_metrics = metrics_config.setdefault('statistical_metrics', True)
    distribution_analysis = metrics_config.setdefault('distribution_analysis', False)
    anomaly_detection = metrics_config.setdefault('anomaly_detection', validation_type == 'anomaly_detection')
    threshold_analysis = metrics_config.setdefault('threshold_analysis', anomaly_detection)
    
    # Performance defaults
    performance_mode = performance_config.setdefault('performance_mode', 'standard')
    benchmark_mode = performance_config.setdefault('benchmark_mode', False)
    memory_efficient = performance_config.setdefault('memory_efficient', True)
    batch_processing = performance_config.setdefault('batch_processing', True)
    
    # Validation quality defaults
    validate_inputs = validation_quality_config.setdefault('validate_inputs', True)
    check_finite = validation_quality_config.setdefault('check_finite', True)
    handle_nan_outputs = validation_quality_config.setdefault('handle_nan_outputs', 'error')
    handle_inf_outputs = validation_quality_config.setdefault('handle_inf_outputs', 'error')
    output_validation = validation_quality_config.setdefault('output_validation', True)
    consistency_checks = validation_quality_config.setdefault('consistency_checks', True)
    
    # Reconstruction and anomaly defaults
    percentile_threshold = reconstruction_config.setdefault('percentile_threshold', 95.0)
    adaptive_threshold = reconstruction_config.setdefault('adaptive_threshold', True)
    threshold_method = reconstruction_config.setdefault('threshold_method', 'percentile')
    contamination_rate = reconstruction_config.setdefault('contamination_rate', 0.1)
    
    # Statistical analysis defaults
    confidence_interval = statistical_config.setdefault('confidence_interval', 0.95)
    significance_level = statistical_config.setdefault('significance_level', 0.05)
    statistical_tests = statistical_config.setdefault('statistical_tests', False)
    distribution_tests = statistical_config.setdefault('distribution_tests', False)
    outlier_analysis = statistical_config.setdefault('outlier_analysis', False)
    
    # Monitoring defaults
    progress_bar = monitoring_config.setdefault('progress_bar', True)
    progress_bar_desc = monitoring_config.setdefault('progress_bar_desc', f"{epoch if epoch is not None else ''}")
    log_frequency = monitoring_config.setdefault('log_frequency', 100)
    verbose = monitoring_config.setdefault('verbose', False)
    debug_mode = monitoring_config.setdefault('debug_mode', False)
    timing_analysis = monitoring_config.setdefault('timing_analysis', False)
    
    # Memory management defaults
    empty_cache_frequency = memory_config.setdefault('empty_cache_frequency', 50)
    gc_collection_frequency = memory_config.setdefault('gc_collection_frequency', 200)
    memory_monitoring = memory_config.setdefault('memory_monitoring', torch.cuda.is_available())
    
    # Distributed defaults
    distributed = distributed_config.setdefault('distributed', False)
    reduce_metrics = distributed_config.setdefault('reduce_metrics', distributed)
    
    # Error handling defaults
    error_handling = error_config.setdefault('error_handling', 'strict')
    continue_on_error = error_config.setdefault('continue_on_error', False)
    max_retries = error_config.setdefault('max_retries', 3)
    graceful_degradation = error_config.setdefault('graceful_degradation', True)
    
    # Set up logging level
    # if verbose:
    #     original_level = logger.level
    #     logger.setLevel(logging.INFO)
    
    logger.info(f"Starting comprehensive validation for epoch {epoch if epoch is not None else 'N/A'}")
    
    # Initialize variables for cleanup
    pbar = None
    
    try:
        # Parameter validation
        if model is None:
            raise ValueError("Model is required for validation")
        if loader is None:
            raise ValueError("DataLoader is required for validation")
        if criterion is None:
            raise ValueError("Loss criterion is required for validation")
        
        # Device configuration
        if device is None:
            device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')
        
        # Initialize validation statistics
        validation_stats = {
            'start_time': start_time.isoformat(),
            'epoch': epoch,
            'validation_type': validation_type,
            'config_applied': final_config,
            'device': str(device),
            'mixed_precision_enabled': mixed_precision,
            'total_batches': len(loader),
            'batch_size': getattr(loader, 'batch_size', 'unknown')
        }
        
        # Set model to appropriate mode
        if eval_mode:
            model.eval()
            logger.debug("Set model to eval mode")
        
        # Initialize metrics tracking
        metrics_tracker = {
            'losses': [],
            'batch_times': [],
            'memory_usage': [],
            'reconstruction_errors': [],
            'per_sample_mse': [],
            'per_sample_mae': [],
            'detailed_metrics': defaultdict(list) if detailed_metrics else None
        }
        
        # Initialize validation variables
        total_loss = 0.0
        num_batches = 0
        num_samples = 0
        all_reconstruction_errors = []
        all_predictions = []
        all_targets = []
        
        # Initialize batch processing variables
        validation_time = 0
        forward_time = 0
        metrics_time = 0
        
        # Set up alive-progress bar
        if progress_bar:
            try:
                pbar_context = alive_bar(
                    total=len(loader), 
                    title=f'Validation {progress_bar_desc}\t',
                    unit='batches',
                    bar='smooth',
                    spinner='dots',
                    stats=False,  # We'll handle stats manually for better control
                    monitor=True,
                    elapsed=True,
                    stats_end=False
                )
                #pbar.__enter__()  # Manually enter the context since we're not using 'with' statement
                pbar = pbar_context.__enter__()  # Get the actual progress bar iterator
            except ImportError:
                logger.warning("alive-progress not available, progress bar disabled")
                pbar = None
                pbar_context = None
            except Exception as e:
                logger.warning(f"Failed to initialize alive-progress bar: {e}")
                pbar = None
                pbar_context = None
        else:
            pbar = None
            pbar_context = None
        
        # Validation callbacks setup
        validation_callbacks = custom_config.get('validation_callbacks', [])
        
        logger.info(f"Starting validation with {len(loader)} batches")
        
        # Determine validation context
        if inference_mode and hasattr(torch, 'inference_mode'):
            validation_context = torch.inference_mode()
        elif no_grad:
            validation_context = torch.no_grad()
        else:
            validation_context = nullcontext()
        
        # Main validation loop
        with validation_context:
            for batch_idx, batch in enumerate(loader):
                batch_start_time = time.time()
                
                try:
                    # Update progress bar with current batch processing status
                    if pbar:
                        #current_status = f"Processing batch {batch_idx+1}/{len(loader)}"
                        #pbar.text(current_status)
                        #pbar.text = current_status
                        pbar.text = f"Processing batch {batch_idx+1}/{len(loader)}"
                    
                    # Move data to device
                    if isinstance(batch, (list, tuple)):
                        inputs = batch[0].to(device, non_blocking=True)
                        targets = batch[1].to(device, non_blocking=True) if len(batch) > 1 else inputs
                    else:
                        inputs = batch.to(device, non_blocking=True)
                        targets = inputs
                    
                    # Input validation
                    if validate_inputs:
                        if check_finite:
                            if not torch.isfinite(inputs).all():
                                if error_handling == 'strict':
                                    raise ValueError(f"Non-finite input values detected in batch {batch_idx}")
                                else:
                                    logger.warning(f"Skipping batch {batch_idx} due to non-finite inputs")
                                    continue
                    
                    # Apply input transforms if specified
                    if data_processing_config.get('input_transforms'):
                        for transform in data_processing_config['input_transforms']:
                            inputs = transform(inputs)
                    
                    current_batch_size = inputs.size(0)
                    num_samples += current_batch_size
                    
                    # Update progress bar with data loading status
                    if pbar:
                        #pbar.text(f"Batch {batch_idx+1}/{len(loader)} | Data loaded | Processing...")
                        pbar.text = f"Batch {batch_idx+1}/{len(loader)} | Data loaded | Processing..."
                    
                    # Forward pass with mixed precision and timing
                    forward_start_time = time.time()
                    
                    autocast_context = get_autocast_context(device, mixed_precision, autocast_enabled)
                    with autocast_context:
                        outputs = model(inputs)
                        
                        # CRITICAL: Ensure targets match outputs for loss calculation
                        if targets.shape != outputs.shape:
                            if hasattr(model, '__class__') and 'autoencoder' in model.__class__.__name__.lower():
                                # For autoencoder, target should be input
                                targets = inputs
                                logger.debug("Set targets = inputs for autoencoder validation")
                            else:
                                # Try to reshape targets to match outputs
                                if outputs.numel() == targets.numel():
                                    targets = targets.view_as(outputs)
                                    logger.debug(f"Reshaped targets from {targets.shape} to {outputs.shape}")
                                else:
                                    # Try to match batch size and feature dimensions
                                    batch_size = outputs.size(0)
                                    if len(outputs.shape) == 2:  # [batch_size, features]
                                        if targets.size(0) == batch_size:
                                            # Try to extract the correct number of features
                                            target_features = outputs.size(1)
                                            if targets.size(1) >= target_features:
                                                targets = targets[:, :target_features]
                                            else:
                                                # Pad or repeat features if needed
                                                targets = torch.cat([targets, targets[:, :target_features - targets.size(1)]], dim=1)
                                        else:
                                            # As last resort, use inputs as targets
                                            targets = inputs
                                            logger.warning(f"Using inputs as targets due to irreconcilable shape mismatch")
                                    else:
                                        # For other shapes, try to use inputs
                                        targets = inputs
                                        logger.warning(f"Complex shape mismatch, using inputs as targets")
                            
                            logger.debug(f"Fixed shape mismatch: outputs {outputs.shape}, targets {targets.shape}")
                        
                        # Calculate loss with proper error handling
                        try:
                            loss = criterion(outputs, targets)
                        except RuntimeError as loss_error:
                            logger.error(f"Loss calculation failed even after shape fixing: {loss_error}")
                            logger.error(f"Final shapes - outputs: {outputs.shape}, targets: {targets.shape}")
                            
                            # Final attempt: force targets to match outputs exactly
                            if 'autoencoder' in str(type(model)).lower():
                                targets = inputs
                            else:
                                targets = outputs.detach().clone()  # Use model output as target (will give 0 loss)
                            
                            try:
                                loss = criterion(outputs, targets)
                                logger.warning("Used fallback target matching for loss calculation")
                            except Exception as final_error:
                                logger.error(f"Final loss calculation attempt failed: {final_error}")
                                if handle_nan_outputs == 'skip':
                                    logger.warning(f"Skipping batch {batch_idx} due to unresolvable loss calculation")
                                    continue
                                elif graceful_degradation:
                                    # Create a dummy loss to continue
                                    loss = torch.tensor(0.0, device=device, requires_grad=True)
                                    logger.warning(f"Using dummy loss for batch {batch_idx}")
                                else:
                                    raise
                    
                    forward_end_time = time.time()
                    forward_time += (forward_end_time - forward_start_time)
                    
                    # Output validation
                    if output_validation:
                        if check_finite:
                            if not torch.isfinite(outputs).all():
                                if handle_nan_outputs == 'error':
                                    raise ValueError(f"Non-finite output values detected in batch {batch_idx}")
                                elif handle_nan_outputs == 'skip':
                                    logger.warning(f"Skipping batch {batch_idx} due to non-finite outputs")
                                    continue
                                elif handle_nan_outputs == 'replace':
                                    outputs = torch.nan_to_num(outputs, nan=0.0, posinf=1.0, neginf=0.0)
                    
                    # Update progress bar with forward pass completion
                    if pbar:
                        #pbar.text(f"Batch {batch_idx+1}/{len(loader)} | Forward pass completed | Calculating metrics...")
                        pbar.text = f"Batch {batch_idx+1}/{len(loader)} | Forward pass completed | Calculating metrics..."
                    
                    # Calculate metrics with timing
                    metrics_start_time = time.time()
                    
                    # Basic metrics
                    total_loss += loss.item()
                    num_batches += 1
                    
                    if per_sample_metrics:
                        # Calculate per-sample reconstruction error (MSE)
                        per_sample_mse = torch.mean((inputs - outputs)**2, dim=tuple(range(1, inputs.dim()))).cpu().numpy()
                        metrics_tracker['per_sample_mse'].extend(per_sample_mse)
                        all_reconstruction_errors.extend(per_sample_mse)
                        
                        # Calculate per-sample MAE
                        if 'mae' in metrics_to_calculate:
                            per_sample_mae = torch.mean(torch.abs(inputs - outputs), dim=tuple(range(1, inputs.dim()))).cpu().numpy()
                            metrics_tracker['per_sample_mae'].extend(per_sample_mae)
                    
                    # Store predictions and targets for advanced analysis
                    if advanced_config.get('feature_importance', False) or export_config.get('save_predictions', False):
                        all_predictions.append(outputs.cpu().numpy())
                        all_targets.append(targets.cpu().numpy())
                    
                    # Custom metric calculation
                    if custom_config.get('custom_metric_fn'):
                        try:
                            custom_metrics = custom_config['custom_metric_fn'](inputs, outputs, targets, loss)
                            if detailed_metrics and metrics_tracker['detailed_metrics'] is not None:
                                for metric_name, metric_value in custom_metrics.items():
                                    metrics_tracker['detailed_metrics'][metric_name].append(metric_value)
                        except Exception as e:
                            logger.warning(f"Custom metric calculation failed: {e}")
                    
                    # Detailed metrics calculation
                    if detailed_metrics and batch_idx % 10 == 0:
                        # Statistical metrics
                        if statistical_metrics:
                            reconstruction_error = torch.mean((inputs - outputs)**2)
                            reconstruction_std = torch.std((inputs - outputs)**2)
                            
                            if metrics_tracker['detailed_metrics'] is not None:
                                metrics_tracker['detailed_metrics']['reconstruction_mean'].append(reconstruction_error.item())
                                metrics_tracker['detailed_metrics']['reconstruction_std'].append(reconstruction_std.item())
                        
                        # Distribution analysis
                        if distribution_analysis:
                            input_mean = torch.mean(inputs).item()
                            output_mean = torch.mean(outputs).item()
                            input_std = torch.std(inputs).item()
                            output_std = torch.std(outputs).item()
                            
                            if metrics_tracker['detailed_metrics'] is not None:
                                metrics_tracker['detailed_metrics']['input_mean'].append(input_mean)
                                metrics_tracker['detailed_metrics']['output_mean'].append(output_mean)
                                metrics_tracker['detailed_metrics']['input_std'].append(input_std)
                                metrics_tracker['detailed_metrics']['output_std'].append(output_std)
                    
                    metrics_end_time = time.time()
                    metrics_time += (metrics_end_time - metrics_start_time)
                    
                    # Store batch metrics
                    if calculate_metrics:
                        batch_time = time.time() - batch_start_time
                        metrics_tracker['losses'].append(loss.item())
                        metrics_tracker['batch_times'].append(batch_time)
                        
                        # Memory usage tracking
                        if memory_monitoring and torch.cuda.is_available():
                            memory_allocated = torch.cuda.memory_allocated(device) / 1024**2  # MB
                            metrics_tracker['memory_usage'].append(memory_allocated)
                    
                    # Memory management
                    if memory_efficient:
                        if batch_idx % empty_cache_frequency == 0 and torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        
                        if batch_idx % gc_collection_frequency == 0:
                            gc.collect()
                    
                    # Update progress bar with comprehensive metrics
                    if pbar:
                        current_loss_display = loss.item()
                        avg_loss_display = total_loss / num_batches
                        
                        # Format the text for the progress bar with detailed status
                        progress_text = (
                            f"Batch {batch_idx+1}/{len(loader)} | "
                            f"Loss: {current_loss_display:.4f} | "
                            f"Avg: {avg_loss_display:.4f} | "
                            f"Samples: {num_samples:,} | "
                            f"Memory: {torch.cuda.memory_allocated(device)/1024**2:.0f}MB"
                        )
                        
                        # Update the progress bar text
                        #pbar.text(progress_text)
                        pbar.text = progress_text
                        
                        # Update the progress
                        pbar()
                    
                    # Logging
                    if batch_idx % log_frequency == 0 and batch_idx > 0:
                        avg_loss = total_loss / num_batches
                        
                        if timing_analysis:
                            logger.info(
                                f"Validation batch {batch_idx}/{len(loader)} | "
                                f"Loss: {avg_loss:.6f} | "
                                f"Forward: {forward_time/batch_idx:.3f}s | "
                                f"Metrics: {metrics_time/batch_idx:.3f}s"
                            )
                        else:
                            logger.info(
                                f"Validation batch {batch_idx}/{len(loader)} | "
                                f"Loss: {avg_loss:.6f}"
                            )
                    
                    # Execute validation callbacks
                    for callback in validation_callbacks:
                        try:
                            callback(
                                batch_idx=batch_idx,
                                inputs=inputs,
                                outputs=outputs,
                                loss=loss.item(),
                                model=model,
                                metrics=metrics_tracker
                            )
                        except Exception as e:
                            logger.warning(f"Validation callback execution failed: {e}")
                    
                except Exception as e:
                    if continue_on_error:
                        logger.error(f"Error in validation batch {batch_idx}: {e}")
                        if error_handling == 'skip':
                            continue
                        elif error_handling == 'retry':
                            retry_count = 0
                            while retry_count < max_retries:
                                try:
                                    logger.info(f"Retrying validation batch {batch_idx}, attempt {retry_count + 1}")
                                    # Re-process the batch (simplified retry logic)
                                    break
                                except Exception as retry_e:
                                    retry_count += 1
                                    logger.warning(f"Retry {retry_count} failed: {retry_e}")
                            
                            if retry_count >= max_retries:
                                logger.error(f"Max retries exceeded for batch {batch_idx}")
                                if graceful_degradation:
                                    continue
                                else:
                                    raise
                    else:
                        raise RuntimeError(f"Validation failed on batch {batch_idx}: {e}") from e
        
        # Update progress bar for post-processing phase
        if pbar:
            #pbar.text("Validation completed | Calculating final metrics...")
            pbar.text = "Validation completed | Calculating final metrics..."
        
        # Calculate final metrics
        validation_end_time = time.time()
        total_validation_time = validation_end_time - validation_start_time
        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
        
        # Convert reconstruction errors to numpy array
        reconstruction_errors_array = np.array(all_reconstruction_errors)
        
        # Update progress bar for anomaly detection
        if pbar and (anomaly_detection or threshold_analysis):
            #pbar.text("Validation completed | Performing anomaly detection...")
            pbar.text = "Validation completed | Performing anomaly detection..."
        
        # Anomaly detection and threshold analysis
        anomaly_results = {}
        if anomaly_detection or threshold_analysis:
            logger.info("Performing anomaly detection and threshold analysis")
            
            if len(reconstruction_errors_array) > 0:
                # Calculate threshold based on method
                if threshold_method == 'percentile':
                    threshold = np.percentile(reconstruction_errors_array, percentile_threshold)
                elif threshold_method == 'mean_std':
                    mean_error = np.mean(reconstruction_errors_array)
                    std_error = np.std(reconstruction_errors_array)
                    threshold = mean_error + 2 * std_error
                elif threshold_method == 'iqr':
                    Q1 = np.percentile(reconstruction_errors_array, 25)
                    Q3 = np.percentile(reconstruction_errors_array, 75)
                    IQR = Q3 - Q1
                    threshold = Q3 + 1.5 * IQR
                elif custom_config.get('custom_threshold_fn'):
                    threshold = custom_config['custom_threshold_fn'](reconstruction_errors_array)
                else:
                    threshold = np.percentile(reconstruction_errors_array, percentile_threshold)
                
                # Anomaly detection results
                anomalies = reconstruction_errors_array > threshold
                n_anomalies = np.sum(anomalies)
                anomaly_rate = n_anomalies / len(reconstruction_errors_array) if len(reconstruction_errors_array) > 0 else 0
                
                anomaly_results = {
                    'threshold': float(threshold),
                    'threshold_method': threshold_method,
                    'n_anomalies': int(n_anomalies),
                    'anomaly_rate': float(anomaly_rate),
                    'anomaly_indices': np.where(anomalies)[0].tolist() if export_config.get('save_predictions', False) else []
                }
            else:
                logger.warning("No reconstruction errors available for anomaly detection")
        
        # Update progress bar for statistical analysis
        if pbar and statistical_metrics:
            #pbar.text("Validation completed | Performing statistical analysis...")
            pbar.text = "Validation completed | Performing statistical analysis..."
        
        # Statistical analysis
        statistical_results = {}
        if statistical_metrics and len(reconstruction_errors_array) > 0:
            logger.info("Performing statistical analysis")
            
            # Basic statistics
            statistical_results.update({
                'mean_reconstruction_error': float(np.mean(reconstruction_errors_array)),
                'std_reconstruction_error': float(np.std(reconstruction_errors_array)),
                'min_reconstruction_error': float(np.min(reconstruction_errors_array)),
                'max_reconstruction_error': float(np.max(reconstruction_errors_array)),
                'median_reconstruction_error': float(np.median(reconstruction_errors_array)),
                'q25_reconstruction_error': float(np.percentile(reconstruction_errors_array, 25)),
                'q75_reconstruction_error': float(np.percentile(reconstruction_errors_array, 75))
            })
            
            # Advanced statistical analysis
            if statistical_tests and len(reconstruction_errors_array) > 10:
                try:
                    # Normality test
                    normality_stat, normality_p = stats.normaltest(reconstruction_errors_array)
                    statistical_results.update({
                        'normality_test_statistic': float(normality_stat),
                        'normality_test_pvalue': float(normality_p),
                        'is_normal': normality_p > significance_level
                    })
                    
                    # Skewness and kurtosis
                    statistical_results.update({
                        'skewness': float(stats.skew(reconstruction_errors_array)),
                        'kurtosis': float(stats.kurtosis(reconstruction_errors_array))
                    })
                    
                except Exception as e:
                    logger.warning(f"Statistical tests failed: {e}")
            
            # Confidence interval
            if confidence_interval < 1.0:
                alpha = 1 - confidence_interval
                ci_lower = np.percentile(reconstruction_errors_array, 100 * alpha / 2)
                ci_upper = np.percentile(reconstruction_errors_array, 100 * (1 - alpha / 2))
                statistical_results.update({
                    f'ci_{confidence_interval}_lower': float(ci_lower),
                    f'ci_{confidence_interval}_upper': float(ci_upper)
                })
        
        # Outlier analysis
        outlier_results = {}
        if outlier_analysis and len(reconstruction_errors_array) > 0:
            logger.info("Performing outlier analysis")
            
            # IQR-based outliers
            Q1 = np.percentile(reconstruction_errors_array, 25)
            Q3 = np.percentile(reconstruction_errors_array, 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            iqr_outliers = (reconstruction_errors_array < lower_bound) | (reconstruction_errors_array > upper_bound)
            n_iqr_outliers = np.sum(iqr_outliers)
            
            outlier_results.update({
                'iqr_outliers': int(n_iqr_outliers),
                'iqr_outlier_rate': float(n_iqr_outliers / len(reconstruction_errors_array)),
                'iqr_lower_bound': float(lower_bound),
                'iqr_upper_bound': float(upper_bound)
            })
        
        # Distributed metrics reduction
        if distributed and reduce_metrics:
            try:
                # Reduce loss across processes
                loss_tensor = torch.tensor(avg_loss, device=device)
                dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
                avg_loss = (loss_tensor / distributed_config.get('world_size', 1)).item()
                
                # Reduce sample count
                samples_tensor = torch.tensor(num_samples, device=device)
                dist.all_reduce(samples_tensor, op=dist.ReduceOp.SUM)
                num_samples = samples_tensor.item()
                
                logger.info("Reduced metrics across distributed processes")
                
            except Exception as e:
                logger.warning(f"Failed to reduce distributed metrics: {e}")
        
        # Update progress bar for custom analysis
        if pbar and custom_config.get('custom_analysis_fn'):
            pbar.text("Validation completed | Performing custom analysis...")
            pbar.text = "Validation completed | Performing custom analysis..."
        
        # Custom analysis
        custom_analysis_results = {}
        if custom_config.get('custom_analysis_fn'):
            try:
                custom_analysis_results = custom_config['custom_analysis_fn'](
                    model=model,
                    predictions=np.concatenate(all_predictions) if all_predictions else None,
                    targets=np.concatenate(all_targets) if all_targets else None,
                    reconstruction_errors=reconstruction_errors_array,
                    metrics_tracker=metrics_tracker
                )
                logger.info("Completed custom analysis")
            except Exception as e:
                logger.warning(f"Custom analysis failed: {e}")
        
        # Update progress bar for finalization
        if pbar:
            #pbar.text("Validation completed | Finalizing results...")
            pbar.text = "Validation completed | Finalizing results..."
        
        # Comprehensive metrics dictionary
        comprehensive_metrics = {
            # Core metrics
            'loss': avg_loss,
            'epoch': epoch,
            'validation_type': validation_type,
            'num_batches': num_batches,
            'num_samples': num_samples,
            'total_validation_time': total_validation_time,
            'avg_batch_time': total_validation_time / num_batches if num_batches > 0 else 0,
            
            # Basic reconstruction metrics
            'mean_mse': float(np.mean(reconstruction_errors_array)) if len(reconstruction_errors_array) > 0 else 0.0,
            'std_mse': float(np.std(reconstruction_errors_array)) if len(reconstruction_errors_array) > 0 else 0.0,
            'min_mse': float(np.min(reconstruction_errors_array)) if len(reconstruction_errors_array) > 0 else 0.0,
            'max_mse': float(np.max(reconstruction_errors_array)) if len(reconstruction_errors_array) > 0 else 0.0,
            'samples_validated': len(reconstruction_errors_array),
            
            # Mixed precision information
            'mixed_precision_enabled': mixed_precision,
            'autocast_enabled': autocast_enabled,
            
            # Memory information
            'memory_allocated_mb': torch.cuda.memory_allocated(device) / 1024**2 if torch.cuda.is_available() else 0,
            'memory_reserved_mb': torch.cuda.memory_reserved(device) / 1024**2 if torch.cuda.is_available() else 0,
            
            # Performance metrics
            'samples_per_second': num_samples / total_validation_time if total_validation_time > 0 else 0,
            'batches_per_second': num_batches / total_validation_time if total_validation_time > 0 else 0,
            
            # Timing breakdown
            'timing': {
                'forward_time': forward_time,
                'metrics_time': metrics_time,
                'total_time': total_validation_time,
                'avg_forward_time': forward_time / num_batches if num_batches > 0 else 0,
                'avg_metrics_time': metrics_time / num_batches if num_batches > 0 else 0
            },
            
            # Configuration info
            'config_applied': final_config,
            'device': str(device),
            'distributed_validation': distributed,
            
            # Quality metrics
            'validation_stable': not any(loss > 1000 for loss in metrics_tracker['losses'][-10:]) if metrics_tracker['losses'] else True
        }
        
        # Add tracked metrics
        if calculate_metrics:
            if metrics_tracker['losses']:
                comprehensive_metrics.update({
                    'loss_std': float(np.std(metrics_tracker['losses'])),
                    'min_batch_loss': float(np.min(metrics_tracker['losses'])),
                    'max_batch_loss': float(np.max(metrics_tracker['losses']))
                })
            
            if metrics_tracker['batch_times']:
                comprehensive_metrics.update({
                    'avg_batch_time': float(np.mean(metrics_tracker['batch_times'])),
                    'batch_time_std': float(np.std(metrics_tracker['batch_times']))
                })
            
            if metrics_tracker['memory_usage']:
                comprehensive_metrics.update({
                    'avg_memory_usage_mb': float(np.mean(metrics_tracker['memory_usage'])),
                    'max_memory_usage_mb': float(np.max(metrics_tracker['memory_usage']))
                })
        
        # Add detailed metrics
        if detailed_metrics and metrics_tracker['detailed_metrics']:
            comprehensive_metrics['detailed_metrics'] = {}
            for metric_name, values in metrics_tracker['detailed_metrics'].items():
                if values:
                    comprehensive_metrics['detailed_metrics'][metric_name] = {
                        'mean': float(np.mean(values)),
                        'std': float(np.std(values)),
                        'min': float(np.min(values)),
                        'max': float(np.max(values))
                    }
        
        # Add anomaly detection results
        if anomaly_results:
            comprehensive_metrics['anomaly_detection'] = anomaly_results
        
        # Add statistical results
        if statistical_results:
            comprehensive_metrics['statistics'] = statistical_results
        
        # Add outlier results
        if outlier_results:
            comprehensive_metrics['outliers'] = outlier_results
        
        # Add custom analysis results
        if custom_analysis_results:
            comprehensive_metrics['custom_analysis'] = custom_analysis_results
        
        # Save results if requested
        if export_config.get('save_results', False):
            results_path = export_config.get('results_path', f'./validation_results_epoch_{epoch}.json')
            try:
                with open(results_path, 'w') as f:
                    # Make metrics JSON serializable
                    serializable_metrics = {}
                    for key, value in comprehensive_metrics.items():
                        try:
                            json.dumps(value)
                            serializable_metrics[key] = value
                        except TypeError:
                            serializable_metrics[key] = str(value)
                    
                    json.dump(serializable_metrics, f, indent=2)
                logger.info(f"Saved validation results to {results_path}")
            except Exception as e:
                logger.warning(f"Failed to save validation results: {e}")
        
        # Save predictions if requested
        if export_config.get('save_predictions', False) and all_predictions:
            predictions_path = export_config.get('results_path', './').replace('.json', '_predictions.npy')
            try:
                np.save(predictions_path, np.concatenate(all_predictions))
                logger.info(f"Saved predictions to {predictions_path}")
            except Exception as e:
                logger.warning(f"Failed to save predictions: {e}")
        
        # Log comprehensive summary
        logger.info("=" * 80)
        logger.info(f"VALIDATION EPOCH {epoch if epoch is not None else 'N/A'} SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Validation Type: {validation_type}")
        logger.info(f"Average Loss: {avg_loss:.6f}")
        logger.info(f"Batches Processed: {num_batches:,}")
        logger.info(f"Samples Processed: {num_samples:,}")
        logger.info(f"Total Time: {total_validation_time:.2f}s")
        logger.info(f"Throughput: {comprehensive_metrics['samples_per_second']:.1f} samples/sec")
        
        if len(reconstruction_errors_array) > 0:
            logger.info(f"Mean Reconstruction Error: {comprehensive_metrics['mean_mse']:.6f}")
            logger.info(f"Std Reconstruction Error: {comprehensive_metrics['std_mse']:.6f}")
        
        if mixed_precision:
            logger.info(f"Mixed Precision: Enabled")
        
        if torch.cuda.is_available():
            logger.info(f"Memory: {comprehensive_metrics['memory_allocated_mb']:.1f}MB allocated")
        
        if anomaly_results:
            logger.info(f"Anomalies Detected: {anomaly_results['n_anomalies']} ({anomaly_results['anomaly_rate']*100:.1f}%)")
            logger.info(f"Anomaly Threshold: {anomaly_results['threshold']:.6f}")
        
        if timing_analysis:
            timing = comprehensive_metrics['timing']
            logger.info(f"Timing - Forward: {timing['forward_time']:.2f}s")
            logger.info(f"Timing - Metrics: {timing['metrics_time']:.2f}s")
        
        logger.info("=" * 80)
        
        # Final progress bar update
        if pbar:
            #pbar.text(f"Validation completed | Loss: {avg_loss:.4f} | Time: {total_validation_time:.1f}s")
            pbar.text = f"Validation completed | Loss: {avg_loss:.4f} | Time: {total_validation_time:.1f}s"
        
        # Restore original logging level
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        return avg_loss, reconstruction_errors_array, comprehensive_metrics
        
    except Exception as e:
        # Restore original logging level on error
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        error_msg = f"Validation failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Provide helpful error context
        logger.error(f"Error occurred at epoch {epoch}, batch {batch_idx if 'batch_idx' in locals() else 'N/A'}")
        logger.error(f"Configuration used: {final_config}")
        
        # Attempt graceful recovery if enabled
        if graceful_degradation and 'num_batches' in locals() and num_batches > 0:
            logger.warning("Attempting graceful recovery with partial results")
            try:
                # Return partial results
                avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
                partial_errors = np.array(all_reconstruction_errors)
                partial_metrics = {
                    'loss': avg_loss,
                    'epoch': epoch,
                    'num_batches': num_batches,
                    'num_samples': num_samples,
                    'error': str(e),
                    'partial_results': True
                }
                
                logger.warning(f"Returning partial results from {num_batches} processed batches")
                return avg_loss, partial_errors, partial_metrics
                
            except Exception as recovery_e:
                logger.error(f"Graceful recovery also failed: {recovery_e}")
        
        raise RuntimeError(error_msg) from e
    
    finally:
        # Final cleanup
        try:
            if pbar_context:
                pbar_context.__exit__(None, None, None)
                # # For alive_bar, we need to manually close it since we manually entered the context
                # if hasattr(pbar, '__exit__'):
                #     pbar.__exit__(None, None, None)
                # else:
                #     # Fallback for different alive_bar versions
                #     pbar.close()
            
            if memory_efficient:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
            
            # Restore model to original training state if it was changed
            if eval_mode and hasattr(model, 'training') and not model.training:
                # Don't automatically restore to training mode - let caller decide
                pass
        except:
            pass

def calculate_threshold(
    # Core Threshold Calculation Parameters
    model: Optional[nn.Module] = None,
    loader: Optional[DataLoader] = None,
    data: Optional[Union[np.ndarray, torch.Tensor]] = None,
    reconstruction_errors: Optional[np.ndarray] = None,
    percentile: Optional[float] = None,
    device: Optional[torch.device] = None,
    
    # Threshold Method Parameters
    threshold_method: Optional[str] = None,
    threshold_strategy: Optional[str] = None,
    threshold_type: Optional[str] = None,
    adaptive_threshold: Optional[bool] = None,
    dynamic_threshold: Optional[bool] = None,
    multi_threshold: Optional[bool] = None,
    hierarchical_threshold: Optional[bool] = None,
    
    # Statistical Threshold Parameters
    statistical_method: Optional[str] = None,
    confidence_level: Optional[float] = None,
    significance_level: Optional[float] = None,
    z_score_threshold: Optional[float] = None,
    iqr_multiplier: Optional[float] = None,
    mad_multiplier: Optional[float] = None,
    std_multiplier: Optional[float] = None,
    contamination_rate: Optional[float] = None,
    
    # Percentile-based Parameters
    percentile_method: Optional[str] = None,
    percentile_range: Optional[Tuple[float, float]] = None,
    multi_percentile: Optional[List[float]] = None,
    percentile_interpolation: Optional[str] = None,
    robust_percentile: Optional[bool] = None,
    
    # Distribution-based Parameters
    distribution_type: Optional[str] = None,
    distribution_params: Optional[Dict[str, float]] = None,
    distribution_fit_method: Optional[str] = None,
    use_empirical_distribution: Optional[bool] = None,
    kernel_density_estimation: Optional[bool] = None,
    kde_bandwidth: Optional[Union[str, float]] = None,
    
    # Machine Learning Threshold Parameters
    ml_threshold_method: Optional[str] = None,
    isolation_forest_contamination: Optional[float] = None,
    local_outlier_factor_neighbors: Optional[int] = None,
    one_class_svm_nu: Optional[float] = None,
    one_class_svm_kernel: Optional[str] = None,
    elliptic_envelope_contamination: Optional[float] = None,
    
    # Cross-validation Parameters
    cross_validation_threshold: Optional[bool] = None,
    cv_folds: Optional[int] = None,
    cv_strategy: Optional[str] = None,
    threshold_stability_analysis: Optional[bool] = None,
    bootstrap_threshold: Optional[bool] = None,
    bootstrap_samples: Optional[int] = None,
    
    # Time Series Threshold Parameters
    temporal_threshold: Optional[bool] = None,
    window_size: Optional[int] = None,
    sliding_window: Optional[bool] = None,
    seasonal_adjustment: Optional[bool] = None,
    trend_adjustment: Optional[bool] = None,
    change_point_detection: Optional[bool] = None,
    
    # Multi-modal Threshold Parameters
    mixture_model_threshold: Optional[bool] = None,
    n_components: Optional[int] = None,
    mixture_type: Optional[str] = None,
    component_weights: Optional[List[float]] = None,
    
    # Validation and Quality Parameters
    validate_threshold: Optional[bool] = None,
    threshold_validation_data: Optional[np.ndarray] = None,
    quality_metrics: Optional[List[str]] = None,
    stability_check: Optional[bool] = None,
    sensitivity_analysis: Optional[bool] = None,
    robustness_test: Optional[bool] = None,
    
    # Performance Parameters
    performance_mode: Optional[str] = None,
    batch_processing: Optional[bool] = None,
    parallel_processing: Optional[bool] = None,
    n_jobs: Optional[int] = None,
    memory_efficient: Optional[bool] = None,
    cache_results: Optional[bool] = None,
    
    # Data Processing Parameters
    data_preprocessing: Optional[bool] = None,
    outlier_removal: Optional[bool] = None,
    outlier_method: Optional[str] = None,
    data_transformation: Optional[str] = None,
    normalization: Optional[str] = None,
    scaling_method: Optional[str] = None,
    
    # Model Evaluation Parameters
    eval_mode: Optional[bool] = None,
    no_grad: Optional[bool] = None,
    mixed_precision: Optional[bool] = None,
    batch_size: Optional[int] = None,
    model_ensemble: Optional[bool] = None,
    ensemble_method: Optional[str] = None,
    
    # Advanced Analysis Parameters
    feature_importance: Optional[bool] = None,
    feature_weights: Optional[np.ndarray] = None,
    weighted_threshold: Optional[bool] = None,
    dimension_reduction: Optional[bool] = None,
    dimension_reduction_method: Optional[str] = None,
    n_components_pca: Optional[int] = None,
    
    # Uncertainty Quantification Parameters
    uncertainty_estimation: Optional[bool] = None,
    confidence_intervals: Optional[bool] = None,
    prediction_intervals: Optional[bool] = None,
    bayesian_threshold: Optional[bool] = None,
    mcmc_samples: Optional[int] = None,
    
    # Multi-class Threshold Parameters
    multi_class_threshold: Optional[bool] = None,
    class_specific_thresholds: Optional[bool] = None,
    class_weights: Optional[Dict[str, float]] = None,
    threshold_per_class: Optional[Dict[str, float]] = None,
    
    # Cost-sensitive Parameters
    cost_sensitive_threshold: Optional[bool] = None,
    false_positive_cost: Optional[float] = None,
    false_negative_cost: Optional[float] = None,
    cost_matrix: Optional[np.ndarray] = None,
    business_objective: Optional[str] = None,
    
    # Monitoring and Logging Parameters
    verbose: Optional[bool] = None,
    debug_mode: Optional[bool] = None,
    log_level: Optional[str] = None,
    progress_bar: Optional[bool] = None,
    timing_analysis: Optional[bool] = None,
    memory_profiling: Optional[bool] = None,
    
    # Export and Visualization Parameters
    save_results: Optional[bool] = None,
    results_path: Optional[str] = None,
    save_threshold_analysis: Optional[bool] = None,
    visualization: Optional[bool] = None,
    plot_distribution: Optional[bool] = None,
    plot_threshold: Optional[bool] = None,
    plot_roc_curve: Optional[bool] = None,
    
    # Error Handling Parameters
    error_handling: Optional[str] = None,
    handle_edge_cases: Optional[bool] = None,
    min_samples_required: Optional[int] = None,
    fallback_threshold: Optional[float] = None,
    graceful_degradation: Optional[bool] = None,
    
    # Experimental Parameters
    experimental_methods: Optional[bool] = None,
    deep_learning_threshold: Optional[bool] = None,
    autoencoder_threshold: Optional[bool] = None,
    gan_threshold: Optional[bool] = None,
    
    # System Parameters
    random_state: Optional[int] = None,
    reproducible: Optional[bool] = None,
    deterministic: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    threshold_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Tuple[Union[float, Dict[str, float]], Dict[str, Any]]:
    # Start timing
    start_time = datetime.now()
    
    # Initialize configuration with comprehensive defaults
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply threshold-specific configuration
    if threshold_config:
        config.setdefault('threshold', {}).update(threshold_config)
    
    # Apply all parameters to configuration
    final_config = {}
    
    # Merge with existing config
    final_config.update(config)
    
    # Apply individual parameters with intelligent organization
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'threshold_config', 'kwargs', 'start_time', 'datetime',
        'traceback', 'time', 'gc', 'warnings', 'defaultdict', 'deque',
        'nullcontext', 'nn', 'optim', 'DataLoader', 'stats', 'IsolationForest',
        'LocalOutlierFactor', 'OneClassSVM', 'EllipticEnvelope', 'GaussianMixture'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Organize parameters into logical sections
    param_sections = {
        'core_threshold': [
            'percentile', 'threshold_method', 'threshold_strategy', 'threshold_type',
            'adaptive_threshold', 'dynamic_threshold', 'multi_threshold', 'hierarchical_threshold'
        ],
        'statistical_methods': [
            'statistical_method', 'confidence_level', 'significance_level', 'z_score_threshold',
            'iqr_multiplier', 'mad_multiplier', 'std_multiplier', 'contamination_rate'
        ],
        'percentile_methods': [
            'percentile_method', 'percentile_range', 'multi_percentile',
            'percentile_interpolation', 'robust_percentile'
        ],
        'distribution_methods': [
            'distribution_type', 'distribution_params', 'distribution_fit_method',
            'use_empirical_distribution', 'kernel_density_estimation', 'kde_bandwidth'
        ],
        'ml_methods': [
            'ml_threshold_method', 'isolation_forest_contamination', 'local_outlier_factor_neighbors',
            'one_class_svm_nu', 'one_class_svm_kernel', 'elliptic_envelope_contamination'
        ],
        'cross_validation': [
            'cross_validation_threshold', 'cv_folds', 'cv_strategy',
            'threshold_stability_analysis', 'bootstrap_threshold', 'bootstrap_samples'
        ],
        'temporal': [
            'temporal_threshold', 'window_size', 'sliding_window',
            'seasonal_adjustment', 'trend_adjustment', 'change_point_detection'
        ],
        'multi_modal': [
            'mixture_model_threshold', 'n_components', 'mixture_type', 'component_weights'
        ],
        'validation': [
            'validate_threshold', 'threshold_validation_data', 'quality_metrics',
            'stability_check', 'sensitivity_analysis', 'robustness_test'
        ],
        'performance': [
            'performance_mode', 'batch_processing', 'parallel_processing',
            'n_jobs', 'memory_efficient', 'cache_results'
        ],
        'data_processing': [
            'data_preprocessing', 'outlier_removal', 'outlier_method',
            'data_transformation', 'normalization', 'scaling_method'
        ],
        'model_evaluation': [
            'eval_mode', 'no_grad', 'mixed_precision', 'batch_size',
            'model_ensemble', 'ensemble_method'
        ],
        'advanced_analysis': [
            'feature_importance', 'feature_weights', 'weighted_threshold',
            'dimension_reduction', 'dimension_reduction_method', 'n_components_pca'
        ],
        'uncertainty': [
            'uncertainty_estimation', 'confidence_intervals', 'prediction_intervals',
            'bayesian_threshold', 'mcmc_samples'
        ],
        'multi_class': [
            'multi_class_threshold', 'class_specific_thresholds', 'class_weights',
            'threshold_per_class'
        ],
        'cost_sensitive': [
            'cost_sensitive_threshold', 'false_positive_cost', 'false_negative_cost',
            'cost_matrix', 'business_objective'
        ],
        'monitoring': [
            'verbose', 'debug_mode', 'log_level', 'progress_bar',
            'timing_analysis', 'memory_profiling'
        ],
        'export': [
            'save_results', 'results_path', 'save_threshold_analysis',
            'visualization', 'plot_distribution', 'plot_threshold', 'plot_roc_curve'
        ],
        'error_handling': [
            'error_handling', 'handle_edge_cases', 'min_samples_required',
            'fallback_threshold', 'graceful_degradation'
        ],
        'experimental': [
            'experimental_methods', 'deep_learning_threshold', 'autoencoder_threshold',
            'gan_threshold'
        ],
        'system': [
            'random_state', 'reproducible', 'deterministic'
        ]
    }
    
    # Apply parameters to appropriate sections
    for section, param_list in param_sections.items():
        section_config = final_config.setdefault(section, {})
        for param in param_list:
            if param in cleaned_params:
                section_config[param] = cleaned_params[param]
    
    # Set up comprehensive defaults
    core_config = final_config.setdefault('core_threshold', {})
    statistical_config = final_config.setdefault('statistical_methods', {})
    percentile_config = final_config.setdefault('percentile_methods', {})
    distribution_config = final_config.setdefault('distribution_methods', {})
    ml_config = final_config.setdefault('ml_methods', {})
    validation_config = final_config.setdefault('validation', {})
    performance_config = final_config.setdefault('performance', {})
    data_processing_config = final_config.setdefault('data_processing', {})
    model_config = final_config.setdefault('model_evaluation', {})
    monitoring_config = final_config.setdefault('monitoring', {})
    error_config = final_config.setdefault('error_handling', {})
    system_config = final_config.setdefault('system', {})
    
    # Apply intelligent defaults
    percentile = core_config.setdefault('percentile', DEFAULT_PERCENTILE)
    threshold_method = core_config.setdefault('threshold_method', 'percentile')
    adaptive_threshold = core_config.setdefault('adaptive_threshold', True)
    contamination_rate = statistical_config.setdefault('contamination_rate', 0.1)
    
    # Statistical defaults
    confidence_level = statistical_config.setdefault('confidence_level', 0.95)
    z_score_threshold = statistical_config.setdefault('z_score_threshold', 2.0)
    iqr_multiplier = statistical_config.setdefault('iqr_multiplier', 1.5)
    std_multiplier = statistical_config.setdefault('std_multiplier', 2.0)
    
    # Performance defaults
    performance_mode = performance_config.setdefault('performance_mode', 'standard')
    batch_processing = performance_config.setdefault('batch_processing', True)
    memory_efficient = performance_config.setdefault('memory_efficient', True)
    n_jobs = performance_config.setdefault('n_jobs', -1)
    
    # Model evaluation defaults
    eval_mode = model_config.setdefault('eval_mode', True)
    no_grad = model_config.setdefault('no_grad', True)
    mixed_precision = model_config.setdefault('mixed_precision', torch.cuda.is_available())
    
    # Error handling defaults
    error_handling = error_config.setdefault('error_handling', 'strict')
    min_samples_required = error_config.setdefault('min_samples_required', 10)
    fallback_threshold = error_config.setdefault('fallback_threshold', 0.1)
    graceful_degradation = error_config.setdefault('graceful_degradation', True)
    
    # System defaults
    random_state = system_config.setdefault('random_state', 42)
    reproducible = system_config.setdefault('reproducible', True)
    
    # Monitoring defaults
    verbose = monitoring_config.setdefault('verbose', False)
    debug_mode = monitoring_config.setdefault('debug_mode', False)
    progress_bar = monitoring_config.setdefault('progress_bar', True)
    timing_analysis = monitoring_config.setdefault('timing_analysis', False)
    
    # Set up logging level
    # if verbose:
    #     original_level = logger.level
    #     logger.setLevel(logging.INFO)
    
    logger.info(f"Starting comprehensive threshold calculation using method: {threshold_method}")
    
    # Initialize variables for cleanup
    pbar = None
    
    try:
        # Initialize calculation statistics
        calculation_stats = {
            'start_time': start_time.isoformat(),
            'threshold_method': threshold_method,
            'config_applied': final_config
        }
        
        # Set random seed for reproducibility
        if reproducible:
            np.random.seed(random_state)
            if torch.cuda.is_available():
                torch.manual_seed(random_state)
        
        # Device configuration
        if device is None:
            if model is not None:
                device = next(model.parameters()).device
            else:
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Set up main progress bar for the entire threshold calculation process
        if progress_bar:
            try:
                pbar_context = alive_bar(
                    title='Threshold Calculation\t',
                    bar='smooth',
                    spinner='dots',
                    stats=False,
                    monitor=True,
                    elapsed=True,
                    stats_end=False
                )
                pbar = pbar_context.__enter__()  # Get the actual progress bar iterator
            except ImportError:
                logger.warning("alive-progress not available, progress bar disabled")
                pbar = None
                pbar_context = None
            except Exception as e:
                logger.warning(f"Failed to initialize alive-progress bar: {e}")
                pbar = None
                pbar_context = None
        
        # Data preparation and validation
        if pbar:
            pbar.text = "Preparing and validating input data..."
        logger.info("Preparing and validating input data")
        
        mse_values = None
        data_source = None
        
        # Extract reconstruction errors from various sources
        if reconstruction_errors is not None:
            mse_values = np.array(reconstruction_errors)
            data_source = "provided_reconstruction_errors"
            logger.debug(f"Using provided reconstruction errors: {len(mse_values)} samples")
            
        elif data is not None and model is not None:
            # Calculate reconstruction errors from data and model
            if pbar:
                pbar.text = "Calculating reconstruction errors from model and data..."
            logger.info("Calculating reconstruction errors from model and data")
            data_source = "calculated_from_model"
            
            if model_config.get('eval_mode', True):
                model.eval()
            
            mse_list = []
            
            # Convert data to tensor if needed
            if isinstance(data, np.ndarray):
                data_tensor = torch.tensor(data, dtype=torch.float32)
            else:
                data_tensor = data
            
            # Batch processing for memory efficiency
            if batch_processing:
                batch_size = model_config.get('batch_size', 32)
                n_samples = len(data_tensor)
                total_batches = n_samples // batch_size + 1
                
                context = torch.no_grad() if no_grad else nullcontext()
                
                with context:
                    for i in range(0, n_samples, batch_size):
                        current_batch = i // batch_size + 1
                        
                        # Update progress bar with batch processing status
                        if pbar:
                            progress_text = (
                                f"Processing batch {current_batch}/{total_batches} | "
                                f"Samples: {len(mse_list):,} | "
                                f"Avg MSE: {np.mean(mse_list) if mse_list else 0:.4f}"
                            )
                            pbar.text = progress_text
                        
                        batch_data = data_tensor[i:i+batch_size].to(device)
                        
                        # Mixed precision inference
                        autocast_context = get_autocast_context(device, mixed_precision, True)
                        with autocast_context:
                            outputs = model(batch_data)
                        
                        # Calculate MSE per sample with proper shape handling
                        if outputs.shape != batch_data.shape:
                            if outputs.numel() == batch_data.numel():
                                outputs = outputs.view_as(batch_data)
                            else:
                                # For autoencoders, ensure output matches input
                                min_features = min(outputs.size(-1), batch_data.size(-1))
                                batch_data_adjusted = batch_data[..., :min_features]
                                outputs_adjusted = outputs[..., :min_features]
                                outputs = outputs_adjusted
                                batch_data = batch_data_adjusted
                                batch_mse = torch.mean((batch_data - outputs)**2, dim=tuple(range(1, batch_data.dim()))).cpu().numpy()
                            if 'batch_mse' not in locals():
                                batch_mse = torch.mean((batch_data - outputs)**2, dim=tuple(range(1, batch_data.dim()))).cpu().numpy()
                        else:
                            batch_mse = torch.mean((batch_data - outputs)**2, dim=tuple(range(1, batch_data.dim()))).cpu().numpy()
                        mse_list.extend(batch_mse)
                        
                        # Memory management
                        if memory_efficient and torch.cuda.is_available():
                            torch.cuda.empty_cache()
                
                mse_values = np.array(mse_list)
                
            else:
                # Process all data at once
                data_tensor = data_tensor.to(device)
                
                context = torch.no_grad() if no_grad else nullcontext()
                
                with context:
                    autocast_context = get_autocast_context(device, mixed_precision, True)
                    with autocast_context:
                        outputs = model(data_tensor)
                    
                    # Calculate MSE per sample with proper shape handling
                    if data_tensor.shape != outputs.shape:
                        if outputs.numel() == data_tensor.numel():
                            outputs = outputs.view_as(data_tensor)
                        else:
                            min_features = min(outputs.size(-1), data_tensor.size(-1))
                            outputs_adjusted = outputs[..., :min_features]
                            outputs = outputs_adjusted
                            data_tensor_adjusted = data_tensor[..., :min_features]
                            data_tensor = data_tensor_adjusted
                            batch_mse = torch.mean((data_tensor - outputs)**2, dim=tuple(range(1, data_tensor.dim()))).cpu().numpy()
                        if 'batch_mse' not in locals():
                            batch_mse = torch.mean((data_tensor - outputs)**2, dim=tuple(range(1, data_tensor.dim()))).cpu().numpy()
                    else:
                        batch_mse = torch.mean((data_tensor - outputs)**2, dim=tuple(range(1, data_tensor.dim()))).cpu().numpy()
                    mse_list.extend(batch_mse)
                    
                    if memory_efficient and torch.cuda.is_available():
                        torch.cuda.empty_cache
                    
                    if 'mse_list' in locals():
                        mse_values = np.array(mse_list)
                    else:
                        mse_values = batch_mse
            
        elif loader is not None and model is not None:
            # Calculate reconstruction errors from DataLoader
            if pbar:
                pbar.text = "Calculating reconstruction errors from DataLoader..."
            logger.info("Calculating reconstruction errors from DataLoader")
            data_source = "calculated_from_dataloader"
            
            if model_config.get('eval_mode', True):
                model.eval()
            
            mse_list = []
            context = torch.no_grad() if no_grad else nullcontext()
            
            with context:
                for batch_idx, batch in enumerate(loader):
                    # Update progress bar with DataLoader processing status
                    if pbar:
                        progress_text = (
                            f"Processing batch {batch_idx+1}/{len(loader)} | "
                            f"Samples: {len(mse_list):,} | "
                            f"Avg MSE: {np.mean(mse_list) if mse_list else 0:.4f}"
                        )
                        pbar.text = progress_text
                    
                    if isinstance(batch, (list, tuple)):
                        inputs = batch[0].to(device)
                    else:
                        inputs = batch.to(device)
                    
                    autocast_context = get_autocast_context(device, mixed_precision, True)
                    with autocast_context:
                        outputs = model(inputs)
                    
                    batch_mse = torch.mean((inputs - outputs)**2, dim=tuple(range(1, inputs.dim()))).cpu().numpy()
                    mse_list.extend(batch_mse)
                    
                    # Memory management
                    if memory_efficient and torch.cuda.is_available():
                        torch.cuda.empty_cache()
            
            mse_values = np.array(mse_list)
            
        else:
            raise ValueError("Must provide either reconstruction_errors, or (model and data), or (model and loader)")
        
        # Validate data
        if mse_values is None or len(mse_values) == 0:
            raise ValueError("No reconstruction errors available for threshold calculation")
        
        if len(mse_values) < min_samples_required:
            if graceful_degradation:
                logger.warning(f"Only {len(mse_values)} samples available, less than minimum {min_samples_required}")
                logger.warning(f"Using fallback threshold: {fallback_threshold}")
                return fallback_threshold, {'method': 'fallback', 'samples': len(mse_values)}
            else:
                raise ValueError(f"Insufficient samples: {len(mse_values)}, minimum required: {min_samples_required}")
        
        calculation_stats['n_samples'] = len(mse_values)
        calculation_stats['data_source'] = data_source
        
        # Data preprocessing if requested
        if data_processing_config.get('data_preprocessing', False):
            if pbar:
                pbar.text = "Applying data preprocessing..."
            logger.info("Applying data preprocessing")
            
            # Remove outliers if requested
            if data_processing_config.get('outlier_removal', False):
                outlier_method = data_processing_config.get('outlier_method', 'iqr')
                original_size = len(mse_values)
                
                if outlier_method == 'iqr':
                    Q1 = np.percentile(mse_values, 25)
                    Q3 = np.percentile(mse_values, 75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    mse_values = mse_values[(mse_values >= lower_bound) & (mse_values <= upper_bound)]
                elif outlier_method == 'zscore':
                    z_scores = np.abs(stats.zscore(mse_values))
                    mse_values = mse_values[z_scores < 3]
                
                logger.info(f"Outlier removal: {original_size} -> {len(mse_values)} samples")
            
            # Apply data transformation
            transformation = data_processing_config.get('data_transformation')
            if transformation == 'log':
                mse_values = np.log(mse_values + 1e-8)
            elif transformation == 'sqrt':
                mse_values = np.sqrt(mse_values)
            elif transformation == 'box_cox':
                try:
                    mse_values, lambda_param = stats.boxcox(mse_values + 1e-8)
                    calculation_stats['box_cox_lambda'] = lambda_param
                except Exception as e:
                    logger.warning(f"Box-Cox transformation failed: {e}")
        
        # Calculate basic statistics
        if pbar:
            pbar.text = "Calculating basic statistics..."
        
        basic_stats = {
            'mean': float(np.mean(mse_values)),
            'std': float(np.std(mse_values)),
            'min': float(np.min(mse_values)),
            'max': float(np.max(mse_values)),
            'median': float(np.median(mse_values)),
            'q25': float(np.percentile(mse_values, 25)),
            'q75': float(np.percentile(mse_values, 75)),
            'skewness': float(stats.skew(mse_values)),
            'kurtosis': float(stats.kurtosis(mse_values))
        }
        
        calculation_stats['basic_statistics'] = basic_stats
        
        # Main threshold calculation based on method
        threshold_results = {}
        
        if threshold_method == 'percentile':
            if pbar:
                pbar.text = f"Calculating percentile-based threshold: P{percentile}..."
            logger.info(f"Calculating percentile-based threshold: P{percentile}")
            
            interpolation = percentile_config.get('percentile_interpolation', 'linear')
            threshold = np.percentile(mse_values, percentile, interpolation=interpolation)
            
            # Multi-percentile analysis if requested
            if percentile_config.get('multi_percentile'):
                multi_perc = percentile_config['multi_percentile']
                multi_thresholds = {}
                for p in multi_perc:
                    multi_thresholds[f'P{p}'] = float(np.percentile(mse_values, p, interpolation=interpolation))
                threshold_results['multi_percentile_thresholds'] = multi_thresholds
            
            threshold_results.update({
                'threshold': float(threshold),
                'method': 'percentile',
                'percentile_used': percentile,
                'interpolation': interpolation
            })
            
        elif threshold_method == 'statistical':
            statistical_method = statistical_config.get('statistical_method', 'mean_std')
            if pbar:
                pbar.text = f"Calculating statistical threshold using: {statistical_method}..."
            logger.info(f"Calculating statistical threshold using: {statistical_method}")
            
            if statistical_method == 'mean_std':
                threshold = basic_stats['mean'] + std_multiplier * basic_stats['std']
            elif statistical_method == 'median_mad':
                mad = np.median(np.abs(mse_values - basic_stats['median']))
                threshold = basic_stats['median'] + statistical_config.get('mad_multiplier', 2.0) * mad
                threshold_results['mad'] = float(mad)
            elif statistical_method == 'iqr':
                IQR = basic_stats['q75'] - basic_stats['q25']
                threshold = basic_stats['q75'] + iqr_multiplier * IQR
                threshold_results['iqr'] = float(IQR)
            elif statistical_method == 'z_score':
                threshold = basic_stats['mean'] + z_score_threshold * basic_stats['std']
            elif statistical_method == 'modified_z_score':
                mad = np.median(np.abs(mse_values - basic_stats['median']))
                modified_z_scores = 0.6745 * (mse_values - basic_stats['median']) / mad
                threshold = basic_stats['median'] + z_score_threshold * mad / 0.6745
                threshold_results['modified_z_mad'] = float(mad)
            else:
                threshold = basic_stats['mean'] + 2 * basic_stats['std']
            
            threshold_results.update({
                'threshold': float(threshold),
                'method': 'statistical',
                'statistical_method': statistical_method,
                'multiplier_used': std_multiplier if statistical_method == 'mean_std' else iqr_multiplier
            })
            
        elif threshold_method == 'distribution':
            distribution_type = distribution_config.get('distribution_type', 'normal')
            if pbar:
                pbar.text = f"Calculating distribution-based threshold using: {distribution_type}..."
            logger.info(f"Calculating distribution-based threshold using: {distribution_type}")
            
            if distribution_type == 'normal':
                # Fit normal distribution
                mu, sigma = stats.norm.fit(mse_values)
                threshold = stats.norm.ppf(confidence_level, mu, sigma)
                threshold_results.update({
                    'mu': float(mu),
                    'sigma': float(sigma),
                    'distribution_params': {'mu': mu, 'sigma': sigma}
                })
                
            elif distribution_type == 'gamma':
                # Fit gamma distribution
                shape, loc, scale = stats.gamma.fit(mse_values)
                threshold = stats.gamma.ppf(confidence_level, shape, loc, scale)
                threshold_results.update({
                    'shape': float(shape),
                    'loc': float(loc),
                    'scale': float(scale),
                    'distribution_params': {'shape': shape, 'loc': loc, 'scale': scale}
                })
                
            elif distribution_type == 'lognorm':
                # Fit lognormal distribution
                shape, loc, scale = stats.lognorm.fit(mse_values)
                threshold = stats.lognorm.ppf(confidence_level, shape, loc, scale)
                threshold_results.update({
                    'shape': float(shape),
                    'loc': float(loc),
                    'scale': float(scale),
                    'distribution_params': {'shape': shape, 'loc': loc, 'scale': scale}
                })
                
            elif distribution_type == 'exponential':
                # Fit exponential distribution
                loc, scale = stats.expon.fit(mse_values)
                threshold = stats.expon.ppf(confidence_level, loc, scale)
                threshold_results.update({
                    'loc': float(loc),
                    'scale': float(scale),
                    'distribution_params': {'loc': loc, 'scale': scale}
                })
                
            else:
                # Default to normal distribution
                mu, sigma = stats.norm.fit(mse_values)
                threshold = stats.norm.ppf(confidence_level, mu, sigma)
                threshold_results['distribution_params'] = {'mu': mu, 'sigma': sigma}
            
            threshold_results.update({
                'threshold': float(threshold),
                'method': 'distribution',
                'distribution_type': distribution_type,
                'confidence_level': confidence_level
            })
            
        elif threshold_method == 'ml':
            ml_method = ml_config.get('ml_threshold_method', 'isolation_forest')
            if pbar:
                pbar.text = f"Calculating ML-based threshold using: {ml_method}..."
            logger.info(f"Calculating ML-based threshold using: {ml_method}")
            
            # Reshape data for sklearn
            X = mse_values.reshape(-1, 1)
            
            if ml_method == 'isolation_forest':
                contamination = ml_config.get('isolation_forest_contamination', contamination_rate)
                clf = IsolationForest(contamination=contamination, random_state=random_state)
                clf.fit(X)
                
                # Get decision scores
                scores = clf.decision_function(X)
                threshold = np.percentile(scores, (1 - contamination) * 100)
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'ml',
                    'ml_method': ml_method,
                    'contamination': contamination,
                    'decision_scores_range': [float(np.min(scores)), float(np.max(scores))]
                })
                
            elif ml_method == 'local_outlier_factor':
                n_neighbors = ml_config.get('local_outlier_factor_neighbors', 20)
                contamination = ml_config.get('contamination_rate', contamination_rate)
                
                clf = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)
                outlier_labels = clf.fit_predict(X)
                
                # Get LOF scores
                lof_scores = -clf.negative_outlier_factor_
                threshold = np.percentile(lof_scores, (1 - contamination) * 100)
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'ml',
                    'ml_method': ml_method,
                    'n_neighbors': n_neighbors,
                    'contamination': contamination,
                    'lof_scores_range': [float(np.min(lof_scores)), float(np.max(lof_scores))]
                })
                
            elif ml_method == 'one_class_svm':
                nu = ml_config.get('one_class_svm_nu', contamination_rate)
                kernel = ml_config.get('one_class_svm_kernel', 'rbf')
                
                clf = OneClassSVM(nu=nu, kernel=kernel)
                clf.fit(X)
                
                # Get decision scores
                scores = clf.decision_function(X)
                threshold = np.percentile(scores, nu * 100)
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'ml',
                    'ml_method': ml_method,
                    'nu': nu,
                    'kernel': kernel,
                    'decision_scores_range': [float(np.min(scores)), float(np.max(scores))]
                })
                
            elif ml_method == 'elliptic_envelope':
                contamination = ml_config.get('elliptic_envelope_contamination', contamination_rate)
                
                clf = EllipticEnvelope(contamination=contamination, random_state=random_state)
                clf.fit(X)
                
                # Get Mahalanobis distances
                scores = clf.decision_function(X)
                threshold = np.percentile(scores, (1 - contamination) * 100)
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'ml',
                    'ml_method': ml_method,
                    'contamination': contamination,
                    'mahalanobis_distances_range': [float(np.min(scores)), float(np.max(scores))]
                })
                
            else:
                # Fallback to isolation forest
                clf = IsolationForest(contamination=contamination_rate, random_state=random_state)
                clf.fit(X)
                scores = clf.decision_function(X)
                threshold = np.percentile(scores, (1 - contamination_rate) * 100)
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'ml',
                    'ml_method': 'isolation_forest_fallback',
                    'contamination': contamination_rate
                })
                
        elif threshold_method == 'mixture':
            if pbar:
                pbar.text = "Calculating mixture model threshold..."
            logger.info("Calculating mixture model threshold")
            
            n_components = final_config.get('multi_modal', {}).get('n_components', 2)
            mixture_type = final_config.get('multi_modal', {}).get('mixture_type', 'gaussian')
            
            if mixture_type == 'gaussian':
                # Fit Gaussian Mixture Model
                gmm = GaussianMixture(n_components=n_components, random_state=random_state)
                gmm.fit(mse_values.reshape(-1, 1))
                
                # Get component with highest mean (anomaly component)
                component_means = gmm.means_.flatten()
                anomaly_component = np.argmax(component_means)
                
                # Calculate threshold based on anomaly component
                anomaly_mean = component_means[anomaly_component]
                anomaly_std = np.sqrt(gmm.covariances_[anomaly_component].flatten()[0])
                
                threshold = anomaly_mean - 2 * anomaly_std  # Conservative threshold
                
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'mixture',
                    'mixture_type': mixture_type,
                    'n_components': n_components,
                    'component_means': component_means.tolist(),
                    'component_weights': gmm.weights_.tolist(),
                    'anomaly_component': int(anomaly_component)
                })
            else:
                # Fallback to percentile method
                threshold = np.percentile(mse_values, percentile)
                threshold_results.update({
                    'threshold': float(threshold),
                    'method': 'mixture_fallback_percentile',
                    'percentile_used': percentile
                })
                
        elif threshold_method == 'adaptive':
            if pbar:
                pbar.text = "Calculating adaptive threshold..."
            logger.info("Calculating adaptive threshold")
            
            # Calculate multiple thresholds and select based on data characteristics
            percentile_thresh = np.percentile(mse_values, percentile)
            statistical_thresh = basic_stats['mean'] + 2 * basic_stats['std']
            iqr_thresh = basic_stats['q75'] + 1.5 * (basic_stats['q75'] - basic_stats['q25'])
            
            # Selection based on distribution characteristics
            if basic_stats['skewness'] > 1:  # Right-skewed
                threshold = percentile_thresh
                method_used = 'percentile_skewed'
            elif basic_stats['kurtosis'] > 3:  # Heavy-tailed
                threshold = iqr_thresh
                method_used = 'iqr_heavy_tailed'
            else:  # Approximately normal
                threshold = statistical_thresh
                method_used = 'statistical_normal'
            
            threshold_results.update({
                'threshold': float(threshold),
                'method': 'adaptive',
                'adaptive_method_used': method_used,
                'candidate_thresholds': {
                    'percentile': float(percentile_thresh),
                    'statistical': float(statistical_thresh),
                    'iqr': float(iqr_thresh)
                },
                'selection_criteria': {
                    'skewness': basic_stats['skewness'],
                    'kurtosis': basic_stats['kurtosis']
                }
            })
            
        else:
            # Default to percentile method
            logger.warning(f"Unknown threshold method '{threshold_method}', using percentile")
            threshold = np.percentile(mse_values, percentile)
            threshold_results.update({
                'threshold': float(threshold),
                'method': 'percentile_default',
                'percentile_used': percentile
            })
        
        # Cross-validation if requested
        cv_results = {}
        if final_config.get('cross_validation', {}).get('cross_validation_threshold', False):
            if pbar:
                pbar.text = "Performing cross-validation threshold analysis..."
            logger.info("Performing cross-validation threshold analysis")
            
            cv_folds = final_config.get('cross_validation', {}).get('cv_folds', 5)
            fold_thresholds = []
            
            # Simple k-fold validation
            fold_size = len(mse_values) // cv_folds
            for fold in range(cv_folds):
                start_idx = fold * fold_size
                end_idx = start_idx + fold_size if fold < cv_folds - 1 else len(mse_values)
                fold_data = mse_values[start_idx:end_idx]
                
                # Calculate threshold for this fold
                fold_threshold = np.percentile(fold_data, percentile)
                fold_thresholds.append(fold_threshold)
                
                # Update progress bar with CV status
                if pbar:
                    progress_text = (
                        f"Cross-validation: Fold {fold+1}/{cv_folds} | "
                        f"Threshold: {fold_threshold:.4f} | "
                        f"Samples: {len(fold_data):,}"
                    )
                    pbar.text = progress_text
            
            cv_results = {
                'cv_folds': cv_folds,
                'fold_thresholds': [float(t) for t in fold_thresholds],
                'cv_mean_threshold': float(np.mean(fold_thresholds)),
                'cv_std_threshold': float(np.std(fold_thresholds)),
                'cv_min_threshold': float(np.min(fold_thresholds)),
                'cv_max_threshold': float(np.max(fold_thresholds))
            }
        
        # Bootstrap confidence intervals if requested
        bootstrap_results = {}
        if final_config.get('cross_validation', {}).get('bootstrap_threshold', False):
            if pbar:
                pbar.text = "Performing bootstrap confidence interval analysis..."
            logger.info("Performing bootstrap confidence interval analysis")
            
            bootstrap_samples = final_config.get('cross_validation', {}).get('bootstrap_samples', 1000)
            bootstrap_thresholds = []
            
            for i in range(bootstrap_samples):
                # Bootstrap sample
                bootstrap_data = np.random.choice(mse_values, size=len(mse_values), replace=True)
                bootstrap_threshold = np.percentile(bootstrap_data, percentile)
                bootstrap_thresholds.append(bootstrap_threshold)
                
                # Update progress bar every 100 samples to avoid performance hit
                if pbar and i % 100 == 0:
                    progress_text = (
                        f"Bootstrap: {i+1}/{bootstrap_samples} | "
                        f"Current CI: [{np.percentile(bootstrap_thresholds, 2.5):.4f}, "
                        f"{np.percentile(bootstrap_thresholds, 97.5):.4f}]"
                    )
                    pbar.text = progress_text
            
            bootstrap_results = {
                'bootstrap_samples': bootstrap_samples,
                'bootstrap_mean': float(np.mean(bootstrap_thresholds)),
                'bootstrap_std': float(np.std(bootstrap_thresholds)),
                'bootstrap_ci_lower': float(np.percentile(bootstrap_thresholds, 2.5)),
                'bootstrap_ci_upper': float(np.percentile(bootstrap_thresholds, 97.5))
            }
        
        # Validation if requested
        validation_results = {}
        if validation_config.get('validate_threshold', False):
            if pbar:
                pbar.text = "Performing threshold validation..."
            logger.info("Performing threshold validation")
            
            # Calculate some validation metrics
            threshold_value = threshold_results['threshold']
            anomalies = mse_values > threshold_value
            
            validation_results = {
                'anomaly_rate': float(np.mean(anomalies)),
                'n_anomalies': int(np.sum(anomalies)),
                'n_normal': int(np.sum(~anomalies)),
                'threshold_percentile_actual': float(stats.percentileofscore(mse_values, threshold_value)),
                'validation_passed': True
            }
            
            # Additional validation checks
            quality_metrics = validation_config.get('quality_metrics', [])
            for metric in quality_metrics:
                if metric == 'stability':
                    # Simple stability check
                    validation_results['stability_score'] = 1.0 - (np.std(mse_values) / np.mean(mse_values))
                elif metric == 'sensitivity':
                    # Sensitivity to threshold changes
                    thresh_variations = [threshold_value * 0.9, threshold_value * 1.1]
                    anomaly_rates = []
                    for tv in thresh_variations:
                        anomaly_rates.append(np.mean(mse_values > tv))
                    validation_results['sensitivity_score'] = float(np.std(anomaly_rates))
        
        # Calculate final timing
        calculation_time = (datetime.now() - start_time).total_seconds()
        
        # Prepare comprehensive results
        final_threshold = threshold_results.get('threshold', fallback_threshold)
        
        comprehensive_results = {
            # Core results
            'threshold': final_threshold,
            'method': threshold_results.get('method', threshold_method),
            'calculation_time_seconds': calculation_time,
            
            # Data information
            'n_samples': len(mse_values),
            'data_source': data_source,
            'basic_statistics': basic_stats,
            
            # Method-specific results
            'method_details': threshold_results,
            
            # Configuration applied
            'config_applied': final_config,
            
            # Additional analyses
            'cross_validation': cv_results if cv_results else None,
            'bootstrap_analysis': bootstrap_results if bootstrap_results else None,
            'validation_results': validation_results if validation_results else None,
            
            # System information
            'device': str(device),
            'random_state': random_state,
            'reproducible': reproducible
        }
        
        # Add calculation statistics
        comprehensive_results.update(calculation_stats)
        
        # Save results if requested
        export_config = final_config.get('export', {})
        if export_config.get('save_results', False):
            if pbar:
                pbar.text = "Saving results..."
            results_path = export_config.get('results_path', f'./threshold_results.json')
            try:
                with open(results_path, 'w') as f:
                    # Make results JSON serializable
                    serializable_results = {}
                    for key, value in comprehensive_results.items():
                        try:
                            json.dumps(value)
                            serializable_results[key] = value
                        except TypeError:
                            serializable_results[key] = str(value)
                    
                    json.dump(serializable_results, f, indent=2)
                logger.info(f"Saved threshold results to {results_path}")
            except Exception as e:
                logger.warning(f"Failed to save results: {e}")
        
        # Visualization if requested
        if export_config.get('visualization', False):
            try:
                if export_config.get('plot_distribution', False):
                    if pbar:
                        pbar.text("Generating visualization...")
                    plt.figure(figsize=(10, 6))
                    plt.hist(mse_values, bins=50, alpha=0.7, density=True, label='Reconstruction Errors')
                    plt.axvline(final_threshold, color='red', linestyle='--', 
                              label=f'Threshold ({threshold_method}): {final_threshold:.4f}')
                    plt.xlabel('Reconstruction Error (MSE)')
                    plt.ylabel('Density')
                    plt.title('Reconstruction Error Distribution and Threshold')
                    plt.legend()
                    plt.grid(True, alpha=0.3)
                    
                    plot_path = export_config.get('results_path', './').replace('.json', '_distribution.png')
                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                    plt.close()
                    logger.info(f"Saved distribution plot to {plot_path}")
                    
            except ImportError:
                logger.warning("Matplotlib not available, skipping visualization")
            except Exception as e:
                logger.warning(f"Visualization failed: {e}")
        
        # Final progress bar update
        if pbar:
            pbar.text = f"Threshold calculation completed: {final_threshold:.4f}"
        
        # Log comprehensive summary
        logger.info("=" * 80)
        logger.info("THRESHOLD CALCULATION SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Method: {threshold_results.get('method', threshold_method)}")
        logger.info(f"Threshold: {final_threshold:.6f}")
        logger.info(f"Samples Used: {len(mse_values):,}")
        logger.info(f"Data Source: {data_source}")
        logger.info(f"Calculation Time: {calculation_time:.3f} seconds")
        
        if 'anomaly_rate' in validation_results:
            logger.info(f"Expected Anomaly Rate: {validation_results['anomaly_rate']*100:.2f}%")
        
        logger.info(f"Data Statistics:")
        logger.info(f"  Mean: {basic_stats['mean']:.6f}")
        logger.info(f"  Std: {basic_stats['std']:.6f}")
        logger.info(f"  Min: {basic_stats['min']:.6f}")
        logger.info(f"  Max: {basic_stats['max']:.6f}")
        logger.info(f"  Skewness: {basic_stats['skewness']:.3f}")
        
        if cv_results:
            logger.info(f"Cross-validation: {cv_results['cv_mean_threshold']:.6f} ± {cv_results['cv_std_threshold']:.6f}")
        
        if bootstrap_results:
            logger.info(f"Bootstrap CI: [{bootstrap_results['bootstrap_ci_lower']:.6f}, {bootstrap_results['bootstrap_ci_upper']:.6f}]")
        
        logger.info("=" * 80)
        
        # Restore original logging level
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        # Return threshold and comprehensive results
        return final_threshold, comprehensive_results
        
    except Exception as e:
        # Restore original logging level on error
        # if verbose and 'original_level' in locals():
        #     logger.setLevel(original_level)
        
        error_msg = f"Threshold calculation failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Provide helpful error context
        logger.error(f"Method used: {threshold_method}")
        logger.error(f"Configuration: {final_config}")
        
        # Attempt graceful recovery if enabled
        if graceful_degradation:
            logger.warning(f"Using fallback threshold: {fallback_threshold}")
            
            fallback_results = {
                'threshold': fallback_threshold,
                'method': 'fallback',
                'error': str(e),
                'graceful_degradation': True,
                'config_applied': final_config
            }
            
            return fallback_threshold, fallback_results
        
        raise RuntimeError(error_msg) from e
    
    finally:
        # Final cleanup
        try:
            if pbar_context:
                # Properly exit the context manager
                pbar_context.__exit__(None, None, None)
            
            if final_config.get('performance', {}).get('memory_efficient', True):
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
        except:
            pass

def train_model(
    # Core Training Parameters
    model_type: Optional[str] = None,
    input_dim: Optional[int] = None,
    
    # ALL Model Architecture Parameters (matching autoencoder classes)
    encoding_dim: Optional[int] = None,
    hidden_dims: Optional[List[int]] = None,
    dropout_rates: Optional[List[float]] = None,
    activation: Optional[str] = None,
    activation_param: Optional[float] = None,
    normalization: Optional[str] = None,
    use_batch_norm: Optional[bool] = None,
    use_layer_norm: Optional[bool] = None,
    bias: Optional[bool] = None,
    weight_init: Optional[str] = None,
    skip_connection: Optional[bool] = None,
    residual_blocks: Optional[bool] = None,
    use_attention: Optional[bool] = None,
    
    # Model Type and Variants
    model_types: Optional[List[str]] = None,
    available_activations: Optional[List[str]] = None,
    available_normalizations: Optional[List[str]] = None,
    available_initializers: Optional[List[str]] = None,
    legacy_mode: Optional[bool] = None,
    
    # Ensemble Parameters
    diversity_factor: Optional[float] = None,
    min_features: Optional[int] = None,
    num_models: Optional[int] = None,
    
    # Training Configuration Parameters
    batch_size: Optional[int] = None,
    epochs: Optional[int] = None,
    learning_rate: Optional[float] = None,
    patience: Optional[int] = None,
    weight_decay: Optional[float] = None,
    gradient_clip: Optional[float] = None,
    gradient_accumulation_steps: Optional[int] = None,
    mixed_precision: Optional[bool] = None,
    num_workers: Optional[int] = None,
    optimizer: Optional[str] = None,
    optimizer_type: Optional[str] = None,  # Keep both for backward compatibility
    scheduler: Optional[str] = None,
    scheduler_type: Optional[str] = None,  # Keep both for backward compatibility
    scheduler_params: Optional[Dict[str, Any]] = None,
    early_stopping: Optional[bool] = None,
    validation_split: Optional[float] = None,
    shuffle: Optional[bool] = None,
    pin_memory: Optional[bool] = None,
    persistent_workers: Optional[bool] = None,
    adam_betas: Optional[Tuple[float, float]] = None,
    adam_eps: Optional[float] = None,
    lr_patience: Optional[int] = None,
    lr_factor: Optional[float] = None,
    min_lr: Optional[float] = None,
    
    # Data Parameters
    normal_samples: Optional[int] = None,
    attack_samples: Optional[int] = None,
    features: Optional[int] = None,
    use_real_data: Optional[bool] = None,
    data_path: Optional[Union[str, Path]] = None,
    artifacts_path: Optional[Union[str, Path]] = None,
    data_normalization: Optional[str] = None,
    anomaly_factor: Optional[float] = None,
    random_state: Optional[int] = None,
    test_split: Optional[float] = None,
    stratified_split: Optional[bool] = None,
    synthetic_generation: Optional[Dict[str, Any]] = None,
    preprocessing: Optional[Dict[str, Any]] = None,
    synthetic_config: Optional[Dict[str, Any]] = None,  # Keep both for backward compatibility
    data_preprocessing: Optional[bool] = None,
    normalization_method: Optional[str] = None,
    
    # Security Parameters
    percentile: Optional[float] = None,
    attack_threshold: Optional[float] = None,
    false_negative_cost: Optional[float] = None,
    enable_security_metrics: Optional[bool] = None,
    anomaly_threshold_strategy: Optional[str] = None,
    early_warning_threshold: Optional[float] = None,
    adaptive_threshold: Optional[bool] = None,
    confidence_interval: Optional[float] = None,
    detection_methods: Optional[List[str]] = None,
    alert_levels: Optional[List[str]] = None,
    threshold_validation: Optional[bool] = None,
    robust_detection: Optional[bool] = None,
    false_positive_tolerance: Optional[float] = None,
    performance_optimized_detection: Optional[bool] = None,
    real_time_monitoring: Optional[bool] = None,
    ensemble_voting: Optional[str] = None,
    uncertainty_threshold: Optional[float] = None,
    threshold_method: Optional[str] = None,  # Keep for backward compatibility
    
    # Monitoring Parameters
    metrics_frequency: Optional[int] = None,
    checkpoint_frequency: Optional[int] = None,
    tensorboard_logging: Optional[bool] = None,
    console_logging_level: Optional[str] = None,
    save_best_model: Optional[bool] = None,
    save_model_history: Optional[bool] = None,
    metrics_to_track: Optional[List[str]] = None,
    early_stopping_metric: Optional[str] = None,
    checkpoint_format: Optional[str] = None,
    log_model_summary: Optional[bool] = None,
    tensorboard_dir: Optional[Union[str, Path]] = None,
    tb_dir: Optional[Union[str, Path]] = None,  # Keep both for backward compatibility
    log_frequency: Optional[int] = None,
    save_checkpoints: Optional[bool] = None,
    tensorboard: Optional[Dict[str, Any]] = None,
    stability_metrics: Optional[bool] = None,
    performance_metrics: Optional[bool] = None,
    profiling_enabled: Optional[bool] = None,
    
    # Hardware Parameters
    device: Optional[str] = None,
    recommended_gpu_memory: Optional[float] = None,
    minimum_system_requirements: Optional[Dict[str, Any]] = None,
    optimal_system_requirements: Optional[Dict[str, Any]] = None,
    memory_management: Optional[Dict[str, Any]] = None,
    performance_optimization: Optional[Dict[str, Any]] = None,
    detected_gpu_memory: Optional[float] = None,
    detected_system_memory: Optional[float] = None,
    system_performance_class: Optional[str] = None,
    optimization_recommendations: Optional[List[str]] = None,
    
    # System Parameters
    model_dir: Optional[Union[str, Path]] = None,
    log_dir: Optional[Union[str, Path]] = None,
    config_dir: Optional[Union[str, Path]] = None,
    data_dir: Optional[Union[str, Path]] = None,
    checkpoint_dir: Optional[Union[str, Path]] = None,
    results_dir: Optional[str] = None,
    random_seed: Optional[int] = None,
    reproducible: Optional[bool] = None,
    parallel_processing: Optional[bool] = None,
    max_workers: Optional[int] = None,
    export_onnx: Optional[bool] = None,
    non_interactive: Optional[bool] = None,
    cuda_optimizations: Optional[bool] = None,
    onnx_export: Optional[Dict[str, Any]] = None,
    distributed_training: Optional[bool] = None,
    python_executable: Optional[str] = None,
    working_directory: Optional[str] = None,
    environment_health: Optional[str] = None,
    
    # Preset Parameters
    available_presets: Optional[List[str]] = None,
    current_preset: Optional[str] = None,
    current_override: Optional[str] = None,
    override_rules: Optional[Dict[str, bool]] = None,
    preset_configs: Optional[Dict[str, str]] = None,
    custom_presets_available: Optional[List[str]] = None,
    auto_apply: Optional[bool] = None,
    validate_compatibility: Optional[bool] = None,
    system_recommended_preset: Optional[str] = None,
    preset_compatibility: Optional[Dict[str, Any]] = None,
    
    # Hyperparameter Optimization Parameters
    hpo_enabled: Optional[bool] = None,
    hpo_strategy: Optional[str] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    n_trials: Optional[int] = None,
    timeout: Optional[int] = None,
    sampler: Optional[str] = None,
    pruner: Optional[str] = None,
    objective_metric: Optional[str] = None,
    optimization_space: Optional[Dict[str, Any]] = None,
    hpo_early_stopping: Optional[Dict[str, Any]] = None,
    timeout_seconds: Optional[int] = None,
    trial_epochs: Optional[int] = None,
    trial_patience: Optional[int] = None,
    cleanup_trials: Optional[bool] = None,
    generate_plots: Optional[bool] = None,
    search_space: Optional[Dict[str, Any]] = None,
    hpo_sampler: Optional[Dict[str, Any]] = None,
    hpo_pruner: Optional[Dict[str, Any]] = None,
    scoring: Optional[Dict[str, Any]] = None,
    storage: Optional[Dict[str, Any]] = None,
    
    # Validation Parameters
    cross_validation: Optional[Dict[str, Any]] = None,
    cv_folds: Optional[int] = None,  # Keep both for backward compatibility
    metrics: Optional[List[str]] = None,
    validation_frequency: Optional[int] = None,
    save_validation_results: Optional[bool] = None,
    detailed_metrics: Optional[bool] = None,
    robustness_testing: Optional[bool] = None,
    performance_benchmarking: Optional[bool] = None,
    confidence_intervals: Optional[bool] = None,
    calculate_detailed_metrics: Optional[bool] = None,  # Keep for backward compatibility
    
    # Experimental Parameters
    experimental_features: Optional[Dict[str, bool]] = None,
    experimental_settings: Optional[Dict[str, bool]] = None,
    auto_optimize: Optional[bool] = None,  # Keep for backward compatibility
    
    # Metadata Parameters
    description: Optional[str] = None,
    version: Optional[str] = None,
    config_version: Optional[str] = None,
    config_type: Optional[str] = None,
    created: Optional[str] = None,
    last_modified: Optional[str] = None,
    preset_used: Optional[str] = None,
    recommended_hardware: Optional[Dict[str, Any]] = None,
    compatibility: Optional[List[str]] = None,
    system_info: Optional[Dict[str, Any]] = None,
    validation_info: Optional[Dict[str, Any]] = None,
    
    # Runtime Parameters
    config_loaded_at: Optional[str] = None,
    config_source: Optional[str] = None,
    runtime_id: Optional[str] = None,
    process_id: Optional[int] = None,
    system_analysis_completed: Optional[bool] = None,
    system_performance_score: Optional[float] = None,
    system_class: Optional[str] = None,
    optimizations_applied: Optional[Dict[str, bool]] = None,
    resource_status: Optional[Dict[str, bool]] = None,
    system_warnings: Optional[List[str]] = None,
    recommendations: Optional[List[str]] = None,
    configuration_health: Optional[Dict[str, Any]] = None,
    
    # Training System Parameters (keeping existing ones for compatibility)
    verbose: Optional[bool] = None,
    debug: Optional[bool] = None,
    debug_mode: Optional[bool] = None,
    progress_bar: Optional[bool] = None,
    
    # Export Parameters
    save_model: Optional[bool] = None,
    save_metadata: Optional[bool] = None,
    save_training_history: Optional[bool] = None,
    
    # Advanced Training Parameters
    compile_model: Optional[bool] = None,
    benchmark_mode: Optional[bool] = None,
    memory_efficient: Optional[bool] = None,
    gradient_checkpointing: Optional[bool] = None,
    
    # Error Handling Parameters
    error_handling: Optional[str] = None,
    continue_on_error: Optional[bool] = None,
    graceful_degradation: Optional[bool] = None,
    fallback_mode: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    training_config: Optional[Dict[str, Any]] = None,
    preset: Optional[str] = None,
    args: Optional[argparse.Namespace] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """
    Comprehensive model training function with full parameter compatibility.
    
    This function now accepts ALL parameters supported by the autoencoder classes
    (SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble) and uses the
    same centralized configuration and parameter processing infrastructure.
    
    Args:
        All parameters supported by the autoencoder model classes and their
        helper functions (_initialize_autoencoder_config, _structure_kwargs_into_config_sections)
        
    Returns:
        Dictionary containing comprehensive training results and metadata
    """
    # Start timing
    start_time = datetime.now()
    training_start_time = time.time()
    
    # Initialize configuration with comprehensive defaults
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply training-specific configuration
    if training_config:
        config.setdefault('training', {}).update(training_config)
    
    # Apply preset configuration if specified
    if preset and preset in globals().get('PRESET_CONFIGS', {}):
        try:
            preset_config = globals()['PRESET_CONFIGS'][preset].copy()
            # Merge preset config using the same deep_update pattern as autoencoder classes
            config = deep_update(preset_config, config)
            logger.info(f"Applied preset configuration: {preset}")
        except Exception as e:
            logger.warning(f"Failed to apply preset '{preset}': {e}")
    
    # Handle legacy args parameter for backward compatibility
    if args is not None:
        # Extract parameters from args object
        legacy_params = {}
        for attr_name in dir(args):
            if not attr_name.startswith('_'):
                value = getattr(args, attr_name)
                if value is not None:
                    legacy_params[attr_name] = value
        
        # Apply legacy parameters
        kwargs.update(legacy_params)
        logger.debug("Applied legacy args parameters")
    
    # Collect all non-None parameters for processing
    local_params = locals().copy()
    params_to_remove = {
        'config', 'training_config', 'preset', 'args', 'kwargs', 'start_time', 
        'training_start_time', 'datetime', 'traceback', 'time', 'gc', 'warnings', 
        'defaultdict', 'deque', 'nullcontext', 'nn', 'optim', 'DataLoader', 
        'GradScaler', 'autocast', 'SummaryWriter', 'Path', 'np', 'torch'
    }
    
    individual_params = {k: v for k, v in local_params.items() 
                       if k not in params_to_remove and v is not None}
    
    # Add kwargs
    if kwargs:
        individual_params.update(kwargs)
    
    # Use the SAME parameter structuring function as the autoencoder classes
    if individual_params:
        structured_params = _structure_kwargs_into_config_sections(individual_params)
        config = deep_update(config, structured_params)
        logger.debug(f"Processed {len(individual_params)} individual parameters using centralized helper")
    
    # Extract final configuration values using the same pattern as autoencoder classes
    model_config = config.get('model', {})
    training_config = config.get('training', {})
    data_config = config.get('data', {})
    security_config = config.get('security', {})
    system_config = config.get('system', {})
    monitoring_config = config.get('monitoring', {})
    export_config = config.get('export', {}) if 'export' in config else {}
    advanced_config = config.get('advanced_training', {}) if 'advanced_training' in config else {}
    validation_config = config.get('validation', {})
    error_config = config.get('error_handling', {}) if 'error_handling' in config else {}
    hardware_config = config.get('hardware', {})
    
    # Apply intelligent defaults using the same extraction pattern as autoencoder classes
    model_type = _extract_and_validate_config_param(
        model_config, 'model_type', 'EnhancedAutoencoder', 'MODEL_TYPE',
        lambda x: isinstance(x, str) and x in ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble'],
        "model type"
    )
    
    # Handle backward compatibility for optimizer/scheduler naming
    optimizer_type = training_config.get('optimizer_type') or training_config.get('optimizer', 'AdamW')
    scheduler_type = training_config.get('scheduler_type') or training_config.get('scheduler', 'ReduceLROnPlateau')
    training_config['optimizer'] = optimizer_type
    training_config['scheduler'] = scheduler_type
    
    # Handle backward compatibility for tensorboard directory naming
    if 'tb_dir' in system_config and 'tensorboard_dir' not in system_config:
        system_config['tensorboard_dir'] = system_config['tb_dir']
    
    # Set default values using existing constants and patterns
    input_dim = model_config.get('input_dim')  # Will be determined from data
    encoding_dim = model_config.setdefault('encoding_dim', DEFAULT_ENCODING_DIM)
    hidden_dims = model_config.setdefault('hidden_dims', HIDDEN_LAYER_SIZES)
    dropout_rates = model_config.setdefault('dropout_rates', DROPOUT_RATES)
    activation = model_config.setdefault('activation', ACTIVATION)
    normalization = model_config.setdefault('normalization', NORMALIZATION)
    
    # Training defaults
    batch_size = training_config.setdefault('batch_size', DEFAULT_BATCH_SIZE)
    epochs = training_config.setdefault('epochs', DEFAULT_EPOCHS)
    learning_rate = training_config.setdefault('learning_rate', LEARNING_RATE)
    patience = training_config.setdefault('patience', EARLY_STOPPING_PATIENCE)
    weight_decay = training_config.setdefault('weight_decay', WEIGHT_DECAY)
    gradient_clip = training_config.setdefault('gradient_clip', GRADIENT_CLIP)
    gradient_accumulation_steps = training_config.setdefault('gradient_accumulation_steps', GRADIENT_ACCUMULATION_STEPS)
    mixed_precision = training_config.setdefault('mixed_precision', MIXED_PRECISION)
    early_stopping = training_config.setdefault('early_stopping', True)
    validation_split = training_config.setdefault('validation_split', 0.2)
    
    # Data defaults
    normal_samples = data_config.setdefault('normal_samples', NORMAL_SAMPLES)
    attack_samples = data_config.setdefault('attack_samples', ATTACK_SAMPLES)
    features = data_config.setdefault('features', FEATURES)
    use_real_data = data_config.setdefault('use_real_data', False)
    data_preprocessing = data_config.setdefault('preprocessing', {}).get('enabled', True)
    
    # Security defaults
    percentile = security_config.setdefault('percentile', DEFAULT_PERCENTILE)
    threshold_method = security_config.setdefault('threshold_method', 'percentile')
    enable_security_metrics = security_config.setdefault('enable_security_metrics', SECURITY_METRICS)
    
    # System defaults
    config_dir = Path(system_config.setdefault('config_dir', CONFIG_DIR))
    if config_dir is None:
        config_dir = Path(__file__).resolve().parent / "config"
    #config_dir = Path(config_dir)
    
    model_dir = Path(system_config.setdefault('model_dir', DEFAULT_MODEL_DIR))
    log_dir = Path(system_config.setdefault('log_dir', LOG_DIR))
    tensorboard_dir = Path(monitoring_config.setdefault('tensorboard_dir', TB_DIR)) or Path(system_config.setdefault('tensorboard_dir', TB_DIR))
    checkpoint_dir = Path(system_config.setdefault('checkpoint_dir', CHECKPOINTS_DIR))
    data_dir = Path(system_config.setdefault('data_dir', DATA_DIR))
    results_dir = Path(system_config.setdefault('results_dir', RESULTS_DIR))
    device = system_config.setdefault('device', 'auto')
    random_seed = system_config.setdefault('random_seed', RANDOM_STATE)
    reproducible = system_config.setdefault('reproducible', True)
    
    # Monitoring defaults
    verbose = monitoring_config.setdefault('verbose', True)
    debug_mode = monitoring_config.setdefault('debug_mode', False)
    tensorboard_logging = monitoring_config.setdefault('tensorboard_logging', True)
    save_checkpoints = monitoring_config.setdefault('save_checkpoints', True)
    checkpoint_frequency = monitoring_config.setdefault('checkpoint_frequency', 10)
    log_frequency = monitoring_config.setdefault('log_frequency', 1)
    progress_bar = monitoring_config.setdefault('progress_bar', True)
    
    # Export defaults
    export_onnx = export_config.setdefault('export_onnx', False)
    save_model = export_config.setdefault('save_model', True)
    save_metadata = export_config.setdefault('save_metadata', True)
    save_training_history = export_config.setdefault('save_training_history', True)
    
    # Advanced training defaults
    num_workers = advanced_config.setdefault('num_workers', NUM_WORKERS)
    pin_memory = advanced_config.setdefault('pin_memory', torch.cuda.is_available())
    persistent_workers = advanced_config.setdefault('persistent_workers', num_workers > 0)
    compile_model = advanced_config.setdefault('compile_model', False)
    benchmark_mode = advanced_config.setdefault('benchmark_mode', False)
    memory_efficient = advanced_config.setdefault('memory_efficient', True)
    
    # Error handling defaults
    error_handling = error_config.setdefault('error_handling', 'strict')
    graceful_degradation = error_config.setdefault('graceful_degradation', True)
    
    logger.info("Starting comprehensive model training pipeline with full parameter compatibility")
    
    # Initialize alive_bar context managers
    main_pbar = None
    main_pbar_context = None
    epoch_pbar = None
    epoch_pbar_context = None
    
    try:
        # Initialize training statistics
        training_stats = {
            'start_time': start_time.isoformat(),
            'config_applied': config,
            'model_type': model_type,
            'training_mode': 'comprehensive',
            'parameter_compatibility': 'full_autoencoder_class_compatibility'
        }
        
        # Set up reproducibility
        if reproducible:
            torch.manual_seed(random_seed)
            np.random.seed(random_seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(random_seed)
                torch.backends.cudnn.deterministic = True
                torch.backends.cudnn.benchmark = False
            logger.info(f"Set random seed to {random_seed} for reproducibility")
        elif benchmark_mode:
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            logger.info("Enabled benchmark mode for performance")
        
        # Device configuration with comprehensive support
        if device == 'auto':
            if torch.cuda.is_available():
                device = torch.device('cuda')
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = torch.device('mps')
            else:
                device = torch.device('cpu')
        else:
            device = torch.device(device)
        
        training_stats['device'] = str(device)
        training_stats['device_type'] = device.type
        
        # Hardware optimization
        if device.type == 'cuda':
            training_stats['cuda_version'] = torch.version.cuda
            training_stats['gpu_count'] = torch.cuda.device_count()
            training_stats['gpu_name'] = torch.cuda.get_device_name() if torch.cuda.is_available() else None
        
        # Create directories
        model_dir.mkdir(parents=True, exist_ok=True)
        log_dir.mkdir(parents=True, exist_ok=True)
        tensorboard_dir.mkdir(parents=True, exist_ok=True)
        config_dir.mkdir(parents=True, exist_ok=True)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup experiment tracking
        timestamp = start_time.strftime("%Y%m%d_%H%M%S")
        run_id = f"train_{model_type}_{timestamp}"
        experiment_dir = tensorboard_dir / run_id
        experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize TensorBoard writer
        writer = None
        if tensorboard_logging:
            try:
                writer = SummaryWriter(log_dir=experiment_dir)
                logger.info(f"TensorBoard logging enabled: {experiment_dir}")
            except ImportError:
                logger.warning("TensorBoard not available, logging disabled")
                tensorboard_logging = False
        
        training_stats['run_id'] = run_id
        training_stats['experiment_dir'] = str(experiment_dir)
        
        logger.info("=" * 80)
        logger.info("COMPREHENSIVE TRAINING CONFIGURATION")
        logger.info("=" * 80)
        logger.info(f"Model Configuration:")
        logger.info(f"  Type: {model_type}")
        logger.info(f"  Encoding Dimension: {encoding_dim}")
        logger.info(f"  Hidden Dimensions: {hidden_dims}")
        logger.info(f"  Dropout Rates: {dropout_rates}")
        logger.info(f"  Activation: {activation}")
        logger.info(f"  Normalization: {normalization}")
        
        logger.info(f"Training Configuration:")
        logger.info(f"  Batch Size: {batch_size}")
        logger.info(f"  Epochs: {epochs}")
        logger.info(f"  Learning Rate: {learning_rate:.2e}")
        logger.info(f"  Weight Decay: {weight_decay:.2e}")
        logger.info(f"  Patience: {patience}")
        logger.info(f"  Mixed Precision: {mixed_precision}")
        logger.info(f"  Optimizer: {optimizer_type}")
        logger.info(f"  Scheduler: {scheduler_type}")
        
        logger.info(f"Data Configuration:")
        logger.info(f"  Use Real Data: {use_real_data}")
        logger.info(f"  Normal Samples: {normal_samples}")
        logger.info(f"  Attack Samples: {attack_samples}")
        logger.info(f"  Features: {features}")
        logger.info(f"  Validation Split: {validation_split}")
        
        logger.info(f"System Configuration:")
        logger.info(f"  Device: {device}")
        logger.info(f"  Model Directory: {model_dir}")
        logger.info(f"  Workers: {num_workers}")
        logger.info(f"  Pin Memory: {pin_memory}")
        logger.info(f"  Parameter Compatibility: Full Autoencoder Class Support")
        logger.info("=" * 80)
        
        # Data preparation with comprehensive configuration
        logger.info("PREPARING DATA")
        logger.info("-" * 40)
        
        data = None
        data_metadata = {}
        
        if use_real_data:
            try:
                logger.info("Loading real data...")
                data = load_and_validate_data(
                    data_path=data_config.get('data_path'),
                    artifacts_path=data_config.get('artifacts_path'),
                    config=config,
                    **{k: v for k, v in data_config.items() if k not in ['data_path', 'artifacts_path']}
                )
                
                # Update input dimension from real data
                actual_features = len(data["feature_names"])
                if input_dim is None:
                    input_dim = actual_features
                    model_config['input_dim'] = input_dim
                elif actual_features != input_dim:
                    logger.warning(f"Specified input_dim ({input_dim}) differs from data features ({actual_features})")
                    input_dim = actual_features
                    model_config['input_dim'] = input_dim
                
                data_metadata = data.get("metadata", {})
                logger.info(f"Loaded real data: {len(data['X_train'])} train, {len(data['X_val'])} val, {len(data['X_test'])} test samples")
                
            except Exception as e:
                logger.error(f"Failed to load real data: {e}")
                if graceful_degradation:
                    logger.warning("Falling back to synthetic data generation")
                    use_real_data = False
                else:
                    raise RuntimeError(f"Real data loading failed: {e}") from e
        
        if not use_real_data:
            try:
                logger.info("Generating synthetic data...")
                
                # Set input dimension if not specified
                if input_dim is None:
                    input_dim = features
                    model_config['input_dim'] = input_dim
                
                synthetic_params = {
                    'normal_samples': normal_samples,
                    'attack_samples': attack_samples,
                    'features': input_dim,
                    'validation_split': validation_split,
                    'random_state': random_seed,
                    'config': config
                }
                
                # Add synthetic-specific config
                if data_config.get('synthetic_generation'):
                    synthetic_params.update(data_config['synthetic_generation'])
                
                data = generate_synthetic_data(**synthetic_params)
                data_metadata = data.get("metadata", {})
                logger.info(f"Generated synthetic data: {len(data['X_train'])} train, {len(data['X_val'])} val, {len(data['X_test'])} test samples")
                
            except Exception as e:
                logger.error(f"Synthetic data generation failed: {e}")
                raise RuntimeError(f"Data preparation failed: {e}") from e
        
        # Validate that we have data
        if data is None or not data.get('X_train', np.array([])).size:
            raise RuntimeError("No valid data available for training")
        
        # Update training statistics with data information
        training_stats.update({
            'data_source': 'real' if use_real_data else 'synthetic',
            'train_samples': len(data['X_train']),
            'val_samples': len(data['X_val']),
            'test_samples': len(data['X_test']),
            'features': len(data['feature_names']),
            'input_dim': input_dim,
            'data_metadata': data_metadata
        })
        
        logger.info(f"Data prepared successfully: {input_dim} features, {len(data['X_train'])} training samples")
        
        # Create comprehensive dataloaders
        logger.info("CREATING DATA LOADERS")
        logger.info("-" * 40)
        
        try:
            dataloader_config = {
                'batch_size': batch_size,
                'num_workers': num_workers,
                'pin_memory': pin_memory,
                'persistent_workers': persistent_workers,
                'shuffle': True,
                'drop_last': False,
                'config': config
            }
            
            train_loader, val_loader, test_loader = create_dataloaders(
                data=data,
                **dataloader_config
            )
            
            logger.info(f"Created dataloaders: train={len(train_loader)} batches, val={len(val_loader)} batches, test={len(test_loader)} batches")
            
        except Exception as e:
            logger.error(f"DataLoader creation failed: {e}")
            if graceful_degradation:
                # Create basic dataloaders as fallback
                train_dataset = TensorDataset(
                    torch.tensor(data['X_train'], dtype=torch.float32)
                )
                val_dataset = TensorDataset(
                    torch.tensor(data['X_val'], dtype=torch.float32)
                )
                test_dataset = TensorDataset(
                    torch.tensor(data['X_test'], dtype=torch.float32)
                )
                
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
                
                logger.warning("Using basic fallback dataloaders")
            else:
                raise RuntimeError(f"DataLoader creation failed: {e}") from e
        
        # Model initialization using the SAME factory pattern as the autoencoder classes
        logger.info("INITIALIZING MODEL")
        logger.info("-" * 40)
        
        # Initialize model variants if needed
        if not globals().get('MODEL_VARIANTS'):
            initialize_model_variants(silent=False)
        
        try:
            # Use the SAME create_model_instance function as used by autoencoder classes
            model = create_model_instance(
                model_type=model_type,
                input_dim=input_dim,
                config=config,
                preset=preset
            )
            
            # Move to device
            model = model.to(device)
            
            logger.info(f"Created {model_type} successfully using factory pattern")
            
        except Exception as e:
            logger.error(f"Model creation failed: {e}")
            
            if graceful_degradation:
                logger.warning("Attempting to create fallback SimpleAutoencoder")
                try:
                    fallback_model = SimpleAutoencoder(
                        input_dim=input_dim,
                        encoding_dim=min(encoding_dim, input_dim // 2),
                        config={'model': {'model_type': 'SimpleAutoencoder'}}
                    )
                    model = fallback_model.to(device)
                    model_type = 'SimpleAutoencoder'
                    logger.warning("Using fallback SimpleAutoencoder")
                except Exception as fallback_error:
                    raise RuntimeError(f"Model creation failed and fallback failed: {fallback_error}") from e
            else:
                raise RuntimeError(f"Model creation failed: {e}") from e
        
        # Model compilation if requested
        if compile_model and hasattr(torch, 'compile'):
            try:
                logger.info("Compiling model with torch.compile")
                model = torch.compile(model, mode='default')
                training_stats['model_compiled'] = True
            except Exception as e:
                logger.warning(f"Model compilation failed: {e}")
                training_stats['model_compiled'] = False
        
        # Log model information
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        model_size_mb = total_params * 4 / 1024 / 1024
        
        training_stats.update({
            'model_class': type(model).__name__,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': model_size_mb,
            'factory_pattern_used': True,
            'centralized_config_used': True
        })
        
        logger.info(f"Model: {type(model).__name__}")
        logger.info(f"Total parameters: {total_params:,}")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        logger.info(f"Model size: {model_size_mb:.2f} MB")
        logger.info(f"Factory Pattern: Enabled")
        logger.info(f"Centralized Config: Enabled")
        
        # Training components setup
        logger.info("SETTING UP TRAINING COMPONENTS")
        logger.info("-" * 40)
        
        # Optimizer setup
        optimizer_params = {
            'lr': learning_rate,
            'weight_decay': weight_decay
        }
        
        if optimizer_type.lower() == 'adam':
            if 'adam_betas' in training_config:
                optimizer_params['betas'] = training_config['adam_betas']
            if 'adam_eps' in training_config:
                optimizer_params['eps'] = training_config['adam_eps']
            optimizer = optim.Adam(model.parameters(), **optimizer_params)
        elif optimizer_type.lower() == 'adamw':
            if 'adam_betas' in training_config:
                optimizer_params['betas'] = training_config['adam_betas']
            if 'adam_eps' in training_config:
                optimizer_params['eps'] = training_config['adam_eps']
            optimizer = optim.AdamW(model.parameters(), **optimizer_params)
        elif optimizer_type.lower() == 'sgd':
            optimizer_params['momentum'] = training_config.get('momentum', 0.9)
            optimizer = optim.SGD(model.parameters(), **optimizer_params)
        else:
            logger.warning(f"Unknown optimizer '{optimizer_type}', using AdamW")
            optimizer = optim.AdamW(model.parameters(), **optimizer_params)
        
        # Scheduler setup
        scheduler = None
        if scheduler_type:
            scheduler_params_config = training_config.get('scheduler_params', {})
            
            if scheduler_type == 'ReduceLROnPlateau':
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer,
                    mode='min',
                    patience=scheduler_params_config.get('patience', training_config.get('lr_patience', 5)),
                    factor=scheduler_params_config.get('factor', training_config.get('lr_factor', 0.5)),
                    min_lr=scheduler_params_config.get('min_lr', training_config.get('min_lr', 1e-7))
                )
            elif scheduler_type == 'StepLR':
                scheduler = optim.lr_scheduler.StepLR(
                    optimizer,
                    step_size=scheduler_params_config.get('step_size', 30),
                    gamma=scheduler_params_config.get('gamma', 0.1)
                )
            elif scheduler_type == 'CosineAnnealingLR':
                scheduler = optim.lr_scheduler.CosineAnnealingLR(
                    optimizer,
                    T_max=scheduler_params_config.get('T_max', epochs),
                    eta_min=scheduler_params_config.get('eta_min', 1e-7)
                )
        
        # Loss function
        criterion = nn.MSELoss()
        
        # Mixed precision scaler
        scaler = None
        if mixed_precision and device.type == 'cuda':
            try:
                scaler = GradScaler()
                logger.info("Mixed precision training enabled")
            except Exception as e:
                logger.warning(f"Failed to initialize mixed precision: {e}")
                mixed_precision = False
        
        logger.info(f"Optimizer: {optimizer_type}")
        logger.info(f"Scheduler: {scheduler_type or 'None'}")
        logger.info(f"Loss function: MSE")
        logger.info(f"Mixed precision: {'Enabled' if mixed_precision else 'Disabled'}")
        
        # Training loop
        logger.info("=" * 80)
        logger.info("STARTING TRAINING")
        logger.info("=" * 80)
        
        # Training state
        best_val_loss = float('inf')
        patience_counter = 0
        training_history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': [],
            'epoch_times': [],
            'memory_usage': [],
            'detailed_metrics': {}
        }
        
        # Set up epoch progress bar with alive_bar
        if progress_bar:
            try:
                epoch_pbar_context = alive_bar(
                    total=epochs,
                    title='Training Epochs\t',
                    bar='smooth',
                    spinner='dots_waves2',
                    stats=True,
                    monitor=True,
                    elapsed=True,
                    stats_end=True
                )
                epoch_pbar = epoch_pbar_context.__enter__()
            except ImportError:
                logger.warning("alive-progress not available, epoch progress bar disabled")
                epoch_pbar = None
                epoch_pbar_context = None
            except Exception as e:
                logger.warning(f"Failed to initialize alive-progress epoch bar: {e}")
                epoch_pbar = None
                epoch_pbar_context = None
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            
            try:
                # Update epoch progress bar
                if epoch_pbar:
                    epoch_pbar.text = (
                        f"Epoch {epoch+1}/{epochs} | "
                        f"Train: {training_history['train_loss'][-1]:.4f}" if training_history['train_loss'] else "Initializing..."
                    )
                
                # Training phase
                train_loss, train_metrics = train_epoch(
                    model=model,
                    loader=train_loader,
                    criterion=criterion,
                    optimizer=optimizer,
                    device=device,
                    epoch=epoch,
                    learning_rate=learning_rate,
                    gradient_clip=gradient_clip,
                    gradient_accumulation_steps=gradient_accumulation_steps,
                    mixed_precision=mixed_precision,
                    scaler=scaler,
                    scheduler=scheduler,
                    progress_bar=progress_bar and not epoch_pbar,
                    verbose=debug_mode,
                    config=config
                )
                
                # Validation phase
                val_loss, val_reconstruction_errors, val_metrics = validate(
                    model=model,
                    loader=val_loader,
                    criterion=criterion,
                    device=device,
                    epoch=epoch,
                    mixed_precision=mixed_precision,
                    calculate_metrics=True,
                    detailed_metrics=validation_config.get('detailed_metrics', False),
                    progress_bar=progress_bar and not epoch_pbar,
                    verbose=debug_mode,
                    config=config
                )
                
                # Update training history
                epoch_time = time.time() - epoch_start_time
                current_lr = optimizer.param_groups[0]['lr']
                
                training_history['train_loss'].append(train_loss)
                training_history['val_loss'].append(val_loss)
                training_history['learning_rate'].append(current_lr)
                training_history['epoch_times'].append(epoch_time)
                
                # Memory tracking
                if device.type == 'cuda':
                    memory_usage = torch.cuda.memory_allocated(device) / 1024**3  # GB
                    training_history['memory_usage'].append(memory_usage)
                
                # Store detailed metrics
                if train_metrics:
                    for key, value in train_metrics.items():
                        if isinstance(value, (int, float)):
                            training_history['detailed_metrics'].setdefault(f'train_{key}', []).append(value)
                
                if val_metrics:
                    for key, value in val_metrics.items():
                        if isinstance(value, (int, float)):
                            training_history['detailed_metrics'].setdefault(f'val_{key}', []).append(value)
                
                # Learning rate scheduling
                if scheduler:
                    if scheduler_type == 'ReduceLROnPlateau':
                        scheduler.step(val_loss)
                    else:
                        scheduler.step()
                
                # TensorBoard logging
                if tensorboard_logging and writer:
                    writer.add_scalar("Loss/Train", train_loss, epoch)
                    writer.add_scalar("Loss/Validation", val_loss, epoch)
                    writer.add_scalar("Learning_Rate", current_lr, epoch)
                    writer.add_scalar("Epoch_Time", epoch_time, epoch)
                    
                    # Additional metrics
                    for key, value in train_metrics.items():
                        if isinstance(value, (int, float)):
                            writer.add_scalar(f"Train/{key}", value, epoch)
                    
                    for key, value in val_metrics.items():
                        if isinstance(value, (int, float)):
                            writer.add_scalar(f"Validation/{key}", value, epoch)
                    
                    if device.type == 'cuda':
                        writer.add_scalar("System/GPU_Memory_GB", memory_usage, epoch)
                
                # Console logging
                if epoch % log_frequency == 0 or epoch == epochs - 1:
                    memory_info = f" | GPU: {memory_usage:.1f}GB" if device.type == 'cuda' and 'memory_usage' in locals() else ""
                    logger.info(
                        f"Epoch {epoch+1:3d}/{epochs} | "
                        f"Train: {train_loss:.6f} | "
                        f"Val: {val_loss:.6f} | "
                        f"LR: {current_lr:.2e} | "
                        f"Time: {epoch_time:.1f}s{memory_info}"
                    )
                
                # Model checkpointing and early stopping
                is_best = val_loss < best_val_loss
                if is_best:
                    best_val_loss = val_loss
                    patience_counter = 0
                    
                    # Save best model
                    if save_model:
                        best_model_path = model_dir / "best_model.pth"
                        torch.save({
                            'epoch': epoch,
                            'model_state_dict': model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                            'best_val_loss': best_val_loss,
                            'config': config,
                            'model_type': model_type,
                            'training_stats': training_stats
                        }, best_model_path)
                        
                        if epoch % (checkpoint_frequency * 5) == 0:
                            logger.info(f"New best model saved (epoch {epoch+1}, loss: {best_val_loss:.6f})")
                else:
                    patience_counter += 1
                
                # Early stopping check
                if early_stopping and patience_counter >= patience:
                    logger.info(f"Early stopping triggered at epoch {epoch+1} (patience: {patience})")
                    break
                
                # Periodic checkpointing
                if save_checkpoints and epoch % checkpoint_frequency == 0 and epoch > 0:
                    #checkpoint_path = model_dir / f"checkpoint_epoch_{epoch+1}.pth"
                    checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch+1}.pth"
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                        'val_loss': val_loss,
                        'training_history': training_history,
                        'config': config
                    }, checkpoint_path)
                
                # Update epoch progress bar
                if epoch_pbar:
                    epoch_pbar()
                    # Update the text with current metrics
                    epoch_pbar.text = (
                        f"Epoch {epoch+1}/{epochs} | "
                        f"Train: {train_loss:.4f} | "
                        f"Val: {val_loss:.4f} | "
                        f"Best: {best_val_loss:.4f} | "
                        f"LR: {current_lr:.2e}"
                    )
                
            except Exception as e:
                logger.error(f"Training error at epoch {epoch+1}: {e}")
                if error_config.get('continue_on_error', False):
                    logger.warning(f"Continuing training after error: {e}")
                    continue
                else:
                    raise RuntimeError(f"Training failed at epoch {epoch+1}: {e}") from e
        
        # Close epoch progress bar
        if epoch_pbar_context:
            epoch_pbar_context.__exit__(None, None, None)
        
        total_training_time = time.time() - training_start_time
        final_epoch = epoch + 1
        
        training_stats.update({
            'total_training_time': total_training_time,
            'final_epoch': final_epoch,
            'best_val_loss': best_val_loss,
            'early_stopped': patience_counter >= patience,
            'training_history': training_history
        })
        
        logger.info(f"Training completed in {total_training_time/60:.1f} minutes ({final_epoch} epochs)")
        
        # Load best model for evaluation
        if save_model and (model_dir / "best_model.pth").exists():
            logger.info("Loading best model for final evaluation...")
            try:
                checkpoint = torch.load(model_dir / "best_model.pth", map_location=device, weights_only=False)
                model.load_state_dict(checkpoint['model_state_dict'])
                logger.info(f"Loaded best model from epoch {checkpoint['epoch']+1}")
            except Exception as e:
                logger.warning(f"Failed to load best model: {e}, using final model")
        
        # Calculate anomaly threshold
        logger.info("CALCULATING ANOMALY THRESHOLD")
        logger.info("-" * 40)
        
        try:
            threshold, threshold_metadata = calculate_threshold(
                model=model,
                loader=val_loader,
                percentile=percentile,
                device=device,
                threshold_method=threshold_method,
                adaptive_threshold=security_config.get('adaptive_threshold', True),
                config=config
            )
            
            logger.info(f"Anomaly threshold calculated: {threshold:.6f} (method: {threshold_method})")
            training_stats['threshold_data'] = {
                'threshold': threshold,
                'method': threshold_method,
                'metadata': threshold_metadata
            }
            
        except Exception as e:
            logger.error(f"Threshold calculation failed: {e}")
            if graceful_degradation:
                # Use fallback threshold
                threshold = np.percentile(val_reconstruction_errors, percentile) if 'val_reconstruction_errors' in locals() else 0.1
                threshold_metadata = {'method': 'fallback', 'error': str(e)}
                logger.warning(f"Using fallback threshold: {threshold:.6f}")
                training_stats['threshold_data'] = {
                    'threshold': threshold,
                    'method': 'fallback',
                    'metadata': threshold_metadata
                }
            else:
                raise RuntimeError(f"Threshold calculation failed: {e}") from e
        
        # Final evaluation on test set
        logger.info("FINAL EVALUATION")
        logger.info("-" * 40)
        
        try:
            test_loss, test_reconstruction_errors, test_metrics = validate(
                model=model,
                loader=test_loader,
                criterion=criterion,
                device=device,
                mixed_precision=mixed_precision,
                calculate_metrics=True,
                detailed_metrics=True,
                statistical_metrics=True,
                anomaly_detection=True,
                anomaly_threshold=threshold,
                progress_bar=progress_bar,
                config=config
            )
            
            # Calculate detection metrics
            if len(test_reconstruction_errors) > 0:
                anomaly_predictions = test_reconstruction_errors > threshold
                anomaly_rate = anomaly_predictions.mean()
                
                # Additional evaluation metrics
                mse_stats = {
                    'mean': float(np.mean(test_reconstruction_errors)),
                    'std': float(np.std(test_reconstruction_errors)),
                    'min': float(np.min(test_reconstruction_errors)),
                    'max': float(np.max(test_reconstruction_errors)),
                    'median': float(np.median(test_reconstruction_errors)),
                    'p95': float(np.percentile(test_reconstruction_errors, 95)),
                    'p99': float(np.percentile(test_reconstruction_errors, 99))
                }
            else:
                anomaly_rate = 0.0
                mse_stats = {}
            
            training_stats['final_evaluation'] = {
                'test_loss': test_loss,
                'test_metrics': test_metrics,
                'anomaly_rate': anomaly_rate,
                'mse_statistics': mse_stats,
                'reconstruction_errors_count': len(test_reconstruction_errors)
            }
            
            logger.info(f"Test Loss: {test_loss:.6f}")
            logger.info(f"Anomaly Detection Rate: {anomaly_rate:.2%}")
            logger.info(f"MSE Statistics: mean={mse_stats.get('mean', 0):.6f}, std={mse_stats.get('std', 0):.6f}")
            
        except Exception as e:
            logger.error(f"Final evaluation failed: {e}")
            training_stats['final_evaluation'] = {'error': str(e)}
            if not graceful_degradation:
                raise RuntimeError(f"Final evaluation failed: {e}") from e
        
        # Save model and artifacts
        logger.info("SAVING ARTIFACTS")
        logger.info("-" * 40)
        
        saved_artifacts = {}
        
        # Save final model using the model's own save method if available
        if save_model:
            try:
                final_model_path = model_dir / "autoencoder_model.pth"
                
                # Use model's save method if it has one (from centralized config classes)
                if hasattr(model, 'save_model'):
                    model.save_model(str(final_model_path), include_config=True)
                    logger.info(f"Final model saved using model.save_model(): {final_model_path}")
                else:
                    torch.save(model.state_dict(), final_model_path)
                    logger.info(f"Final model saved: {final_model_path}")
                
                saved_artifacts['model_path'] = str(final_model_path)
                
            except Exception as e:
                logger.error(f"Failed to save model: {e}")
        
        # Save threshold data
        try:
            threshold_path = model_dir / "anomaly_threshold.pkl"
            threshold_data = {
                'threshold': threshold,
                'metadata': threshold_metadata,
                'mse_statistics': mse_stats if 'mse_stats' in locals() else {},
                'percentile': percentile,
                'method': threshold_method
            }
            joblib.dump(threshold_data, threshold_path)
            saved_artifacts['threshold_path'] = str(threshold_path)
            logger.info(f"Threshold data saved: {threshold_path}")
        except Exception as e:
            logger.error(f"Failed to save threshold data: {e}")
        
        # Export to ONNX if requested
        if export_onnx:
            try:
                # Get export configuration from config if available
                export_config_section = config.get('export', {}).get('onnx_export', {})
                
                # Call export_to_onnx with comprehensive configuration
                onnx_path = export_to_onnx(
                    model=model,
                    input_dim=input_dim,
                    device=device,
                    model_dir=model_dir,
                    opset_version=export_config_section.get('opset_version', 14),
                    config={
                        'system': {
                            'onnx_export': {
                                'max_ram_gb': export_config_section.get('max_ram_gb', 8),
                                'max_vram_gb': export_config_section.get('max_vram_gb', 2),
                                'chunk_size': export_config_section.get('chunk_size', min(128, input_dim)),
                                'runtime_validation': export_config_section.get('runtime_validation', True),
                                'validation_tolerance': export_config_section.get('validation_tolerance', 1e-5),
                                'strict_validation': export_config_section.get('strict_validation', False),
                                'dynamic_axes': export_config_section.get('dynamic_axes', {'input': {0: 'batch_size'}}),
                                'constant_folding': export_config_section.get('constant_folding', True),
                                'verbose': export_config_section.get('verbose', False),
                                'fail_silently': export_config_section.get('fail_silently', False)
                            }
                        }
                    }
                )

                if onnx_path:
                    saved_artifacts['onnx_path'] = str(onnx_path)
                    logger.info(f"ONNX model exported successfully: {onnx_path}")
                    
                    # Optionally validate the exported model
                    if export_config_section.get('validate_export', True):
                        try:
                            validation_result = validate_onnx_model(
                                model=model,
                                onnx_path=onnx_path,
                                device=device,
                                tolerance=export_config_section.get('validation_tolerance', 1e-5),
                                strict=export_config_section.get('strict_validation', False)
                            )
                            
                            if validation_result['status'] == 'passed':
                                logger.info("ONNX model validation passed")
                                saved_artifacts['onnx_validation'] = validation_result
                            else:
                                logger.warning(f"ONNX model validation {validation_result['status']}: {validation_result.get('error', 'unknown error')}")
                        except Exception as val_error:
                            logger.warning(f"ONNX model validation failed: {val_error}")
                else:
                    logger.warning("ONNX export completed but no model path returned")
                    
            except Exception as e:
                error_msg = f"ONNX export failed: {str(e)}"
                logger.warning(error_msg)
                saved_artifacts['onnx_export_error'] = error_msg
                
                if export_config_section.get('fail_silently', False):
                    logger.info("Continuing training process due to fail_silently=True")
                else:
                    raise RuntimeError(error_msg) from e
        
        # Save training metadata
        if save_training_history:
            try:
                history_path = model_dir / "training_history.pkl"
                joblib.dump(training_history, history_path)
                saved_artifacts['history_path'] = str(history_path)
                logger.info(f"Training history saved: {history_path}")
            except Exception as e:
                logger.error(f"Failed to save training history: {e}")
        
        # Save configuration used
        # try:
        #     config_path = config_dir / "training_config.json"
        #     with open(config_path, "w") as f:
        #         json.dump(config, f, indent=2, default=str)
        #     saved_artifacts['config_path'] = str(config_path)
        #     logger.info(f"Training configuration saved: {config_path}")
        # except Exception as e:
        #     logger.error(f"Failed to save config: {e}")
        
        # Save configuration used
        try:
            save_dir = config_dir if config_dir is not None else model_dir
            if isinstance(save_dir, str):
                save_dir = Path(save_dir)
            
            config_path = save_dir / "training_config.json"
            with open(config_path, "w") as f:
                json.dump(config, f, indent=2, default=str)
            saved_artifacts['config_path'] = str(config_path)
            logger.info(f"Training configuration saved: {config_path}")
        except Exception as e:
            logger.error(f"Failed to save config: {e}")
        
        # Prepare final results with comprehensive model class information
        final_results = {
            "success": True,
            "run_id": run_id,
            "timestamp": timestamp,
            "model_type": model_type,
            "training_time_minutes": total_training_time / 60,
            "final_metrics": {
                "best_validation_loss": best_val_loss,
                "test_loss": training_stats.get('final_evaluation', {}).get('test_loss', float('inf')),
                "anomaly_detection_rate": training_stats.get('final_evaluation', {}).get('anomaly_rate', 0.0),
                "threshold": threshold,
                "final_epoch": final_epoch
            },
            "model_info": {
                "type": model_type,
                "class_name": type(model).__name__,
                "parameters": total_params,
                "trainable_parameters": trainable_params,
                "size_mb": model_size_mb,
                "input_dim": input_dim,
                "encoding_dim": encoding_dim,
                "factory_pattern_used": True,
                "centralized_config_used": True,
                "parameter_compatibility": "full_autoencoder_class_support"
            },
            "data_info": {
                "source": 'real' if use_real_data else 'synthetic',
                "train_samples": len(data["X_train"]),
                "val_samples": len(data["X_val"]),
                "test_samples": len(data["X_test"]),
                "features": len(data["feature_names"])
            },
            "system_info": {
                "device": str(device),
                "device_type": device.type,
                "mixed_precision": mixed_precision,
                "pytorch_version": torch.__version__
            },
            "artifacts": saved_artifacts,
            "configuration": config,
            "training_stats": training_stats
        }
        
        # Save final results summary
        try:
            #results_path = model_dir / "training_results.json"
            #results_dir = Path(__file__).resolve().parent / "results"
            #results_path = results_dir / "training_results.json"
            #results_path.mkdir(parents=True, exist_ok=True)
            results_path = results_dir / "training_results.json"
            with open(results_path, "w") as f:
                json.dump(final_results, f, indent=2, default=str)
            saved_artifacts['results_path'] = str(results_path)
            logger.info(f"Final results summary saved: {results_path}")
        except Exception as e:
            logger.error(f"Failed to save results summary: {e}")
        
        # Close TensorBoard writer
        if writer:
            try:
                writer.close()
                logger.debug("TensorBoard writer closed successfully")
            except Exception as e:
                logger.warning(f"Failed to close TensorBoard writer: {e}")
        
        # Log final summary with comprehensive parameter compatibility information
        logger.info("=" * 80)
        logger.info("TRAINING COMPLETED SUCCESSFULLY")
        logger.info("=" * 80)
        logger.info(f"Model Type: {model_type}")
        logger.info(f"Training Time: {total_training_time/60:.1f} minutes ({final_epoch} epochs)")
        logger.info(f"Best Validation Loss: {best_val_loss:.6f}")
        logger.info(f"Test Loss: {training_stats.get('final_evaluation', {}).get('test_loss', 'N/A')}")
        logger.info(f"Anomaly Threshold: {threshold:.6f}")
        logger.info(f"Anomaly Detection Rate: {training_stats.get('final_evaluation', {}).get('anomaly_rate', 0)*100:.1f}%")
        logger.info(f"Model Parameters: {total_params:,}")
        logger.info(f"Model Size: {model_size_mb:.3f} MB")
        logger.info(f"Device: {device}")
        logger.info(f"Mixed Precision: {mixed_precision}")
        logger.info(f"Factory Pattern: Enabled")
        logger.info(f"Centralized Config: Enabled")
        logger.info(f"Parameter Compatibility: Full Autoencoder Class Support")
        logger.info("Saved Artifacts:")
        for artifact_type, artifact_path in saved_artifacts.items():
            logger.info(f"  {artifact_type}: {artifact_path}")
        logger.info("=" * 80)
        
        return final_results
        
    except Exception as e:
        error_msg = f"Training pipeline failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Save error information with comprehensive parameter compatibility context
        error_info = {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "timestamp": start_time.isoformat() if 'start_time' in locals() else datetime.now().isoformat(),
            "training_interrupted": True,
            "run_id": locals().get('run_id', f"error_{datetime.now().strftime('%Y%m%d_%H%M%S')}"),
            "model_type": locals().get('model_type', 'unknown'),
            "configuration": locals().get('config', config or {}),
            "training_stats": locals().get('training_stats', {}),
            "system_info": {
                "device": str(locals().get('device', 'unknown')),
                "pytorch_version": torch.__version__,
                "cuda_available": torch.cuda.is_available(),
                "parameter_compatibility": "full_autoencoder_class_support",
                "factory_pattern_used": True,
                "centralized_config_used": True
            },
            "traceback": traceback.format_exc(),
            "partial_results": {}
        }
        
        # Add partial training results if available
        if 'training_history' in locals() and training_history.get('train_loss'):
            error_info["partial_results"] = {
                "epochs_completed": len(training_history['train_loss']),
                "best_val_loss": locals().get('best_val_loss', float('inf')),
                "training_history": training_history,
                "last_train_loss": training_history['train_loss'][-1] if training_history['train_loss'] else None,
                "last_val_loss": training_history['val_loss'][-1] if training_history['val_loss'] else None
            }
            logger.info(f"Partial training completed: {len(training_history['train_loss'])} epochs")
        
        # Add model information if available
        if 'model' in locals() and hasattr(locals()['model'], 'parameters'):
            try:
                total_params = sum(p.numel() for p in locals()['model'].parameters())
                error_info["model_info"] = {
                    "class_name": type(locals()['model']).__name__,
                    "total_parameters": total_params,
                    "model_size_mb": total_params * 4 / 1024 / 1024,
                    "factory_pattern_used": True,
                    "centralized_config_used": True
                }
            except Exception:
                error_info["model_info"] = {"error": "Could not extract model info"}
        
        # Add data information if available
        if 'data' in locals() and data:
            try:
                error_info["data_info"] = {
                    "train_samples": len(data.get('X_train', [])),
                    "val_samples": len(data.get('X_val', [])),
                    "test_samples": len(data.get('X_test', [])),
                    "features": len(data.get('feature_names', [])),
                    "data_source": 'real' if locals().get('use_real_data', False) else 'synthetic'
                }
            except Exception:
                error_info["data_info"] = {"error": "Could not extract data info"}
        
        # Save error information to file
        try:
            error_dir = Path(locals().get('model_dir', DEFAULT_MODEL_DIR))
            error_dir.mkdir(parents=True, exist_ok=True)
            error_path = error_dir / f"training_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(error_path, "w") as f:
                json.dump(error_info, f, indent=2, default=str)
            
            logger.error(f"Error information saved to: {error_path}")
            error_info["error_log_path"] = str(error_path)
            
        except Exception as save_error:
            logger.error(f"Failed to save error information: {save_error}")
            error_info["save_error"] = str(save_error)
        
        # Close progress bars if they exist
        try:
            if 'epoch_pbar_context' in locals() and locals()['epoch_pbar_context']:
                locals()['epoch_pbar_context'].__exit__(None, None, None)
        except Exception:
            pass
        
        # Close TensorBoard writer if it exists
        if 'writer' in locals() and locals()['writer']:
            try:
                locals()['writer'].close()
                logger.debug("Closed TensorBoard writer")
            except Exception as writer_error:
                logger.warning(f"Failed to close TensorBoard writer: {writer_error}")
        
        # Attempt graceful recovery if enabled
        if locals().get('graceful_degradation', True):
            logger.warning("Attempting graceful recovery and partial results return")
            
            try:
                # Prepare partial results if any training was completed
                partial_results = {
                    "success": False,
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "graceful_recovery": True,
                    "timestamp": error_info["timestamp"],
                    "run_id": error_info["run_id"],
                    "model_type": error_info["model_type"],
                    "configuration": error_info["configuration"],
                    "system_info": error_info["system_info"],
                    "error_log_path": error_info.get("error_log_path"),
                    "parameter_compatibility": "full_autoencoder_class_support"
                }
                
                # Add partial training results if available
                if error_info["partial_results"]:
                    partial_results.update({
                        "partial_training_completed": True,
                        "epochs_completed": error_info["partial_results"]["epochs_completed"],
                        "training_metrics": {
                            "best_val_loss": error_info["partial_results"]["best_val_loss"],
                            "last_train_loss": error_info["partial_results"]["last_train_loss"],
                            "last_val_loss": error_info["partial_results"]["last_val_loss"]
                        },
                        "training_history": error_info["partial_results"]["training_history"]
                    })
                    
                    # Save partial model if it exists
                    if 'model' in locals() and locals().get('model_dir'):
                        try:
                            partial_model_path = Path(locals()['model_dir']) / "partial_model_error_recovery.pth"
                            
                            # Use model's save method if available (from centralized config classes)
                            if hasattr(locals()['model'], 'save_model'):
                                locals()['model'].save_model(str(partial_model_path), include_config=True)
                                logger.info(f"Partial model saved using model.save_model(): {partial_model_path}")
                            else:
                                torch.save(locals()['model'].state_dict(), partial_model_path)
                                logger.info(f"Partial model saved: {partial_model_path}")
                            
                            partial_results["partial_model_path"] = str(partial_model_path)
                        except Exception as model_save_error:
                            logger.warning(f"Failed to save partial model: {model_save_error}")
                
                # Add model and data info if available
                if error_info.get("model_info"):
                    partial_results["model_info"] = error_info["model_info"]
                
                if error_info.get("data_info"):
                    partial_results["data_info"] = error_info["data_info"]
                
                # Save partial results
                try:
                    if locals().get('model_dir'):
                        partial_results_path = Path(locals()['model_dir']) / "partial_training_results.json"
                        with open(partial_results_path, "w") as f:
                            json.dump(partial_results, f, indent=2, default=str)
                        partial_results["partial_results_path"] = str(partial_results_path)
                        logger.info(f"Partial results saved: {partial_results_path}")
                except Exception as partial_save_error:
                    logger.warning(f"Failed to save partial results: {partial_save_error}")
                
                logger.warning("Graceful recovery completed - returning partial results")
                return partial_results
                
            except Exception as recovery_error:
                logger.error(f"Graceful recovery failed: {recovery_error}")
                error_info["recovery_error"] = str(recovery_error)
        
        # If graceful recovery is disabled or failed, prepare error response
        error_response = {
            "success": False,
            "error": error_msg,
            "error_type": type(e).__name__,
            "timestamp": error_info["timestamp"],
            "run_id": error_info["run_id"],
            "model_type": error_info["model_type"],
            "configuration": error_info["configuration"],
            "system_info": error_info["system_info"],
            "error_details": error_info,
            "graceful_recovery_attempted": locals().get('graceful_degradation', True),
            "graceful_recovery_succeeded": False,
            "parameter_compatibility": "full_autoencoder_class_support"
        }
        
        # Add debugging information for development
        if locals().get('debug_mode', False):
            error_response["debug_info"] = {
                "local_variables": {k: str(v) for k, v in locals().items() 
                                  if not k.startswith('_') and not callable(v)},
                "exception_context": {
                    "filename": traceback.extract_tb(e.__traceback__)[-1].filename,
                    "line_number": traceback.extract_tb(e.__traceback__)[-1].lineno,
                    "function_name": traceback.extract_tb(e.__traceback__)[-1].name
                }
            }
        
        logger.error("Training failed - returning error response")
        
        # Determine whether to raise exception or return error response
        error_handling_mode = locals().get('error_handling', 'strict')
        
        if error_handling_mode == 'return_error':
            return error_response
        elif error_handling_mode == 'continue':
            logger.warning("Error handling set to 'continue' - returning error response instead of raising")
            return error_response
        else:  # strict mode (default)
            raise RuntimeError(error_msg) from e
    
    finally:
        # Final cleanup operations
        try:
            # Close progress bars in finally block to ensure cleanup
            if 'epoch_pbar_context' in locals() and locals()['epoch_pbar_context']:
                locals()['epoch_pbar_context'].__exit__(None, None, None)
            
            # Memory cleanup
            if 'model' in locals():
                del locals()['model']
            
            # Clear CUDA cache if available
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.debug("Cleared CUDA cache")
            
            # Clear MPS cache if available
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                try:
                    torch.mps.empty_cache()
                    logger.debug("Cleared MPS cache")
                except AttributeError:
                    pass  # MPS cache clearing not available in all versions
            
            # Garbage collection
            gc.collect()
            logger.debug("Performed garbage collection")
            
        except Exception as cleanup_error:
            logger.warning(f"Cleanup operations failed: {cleanup_error}")
        
        # Log final cleanup message
        try:
            logger.debug("Training function cleanup completed")
        except Exception:
            # Silent fail for final logging attempt
            pass

def train_model_interactive(
    use_real_data: Optional[bool] = None,
    use_current_config: bool = False,
    preset: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None,
    non_interactive: bool = False,
    **kwargs
) -> Optional[Dict[str, Any]]:
    """Interactive model training setup with context display and error handling."""
    try:
        # Clear screen and show banner with config
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Extract configuration context with error handling
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu header with context
        #print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "INTERACTIVE AUTOENCODER TRAINING SETUP")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"\nActive Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        
        if config:
            base_config = config.copy()
            print(Fore.GREEN + Style.BRIGHT + "\nUsing provided configuration as base:")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
        elif use_current_config:
            try:
                base_config = get_current_config() if 'get_current_config' in globals() else {}
                print(Fore.GREEN + Style.BRIGHT + "\nUsing current system configuration:")
                print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            except Exception as e:
                logger.warning(f"Failed to load current config: {e}")
                console.print(
                    Panel.fit(
                        #f"[bold yellow]Warning: Failed to load current config, using defaults: {str(e)}[/bold yellow]",
                        f"Failed to load current config, using defaults: {str(e)}",
                        title="WARNING",
                        style="bold red",
                        border_style="red",
                        padding=(1, 1),
                        box=box.ROUNDED
                    )
                )
                base_config = {}
        else:
            base_config = {}
            print(Fore.YELLOW + Style.BRIGHT + "\nUsing default configuration:")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
        
        if preset:
            try:
                if preset in globals().get('PRESET_CONFIGS', {}):
                    preset_config = globals()['PRESET_CONFIGS'][preset].copy()
                    for section, values in preset_config.items():
                        if section not in base_config:
                            base_config[section] = values
                        elif isinstance(values, dict):
                            base_config[section].update(values)
                    print(Fore.GREEN + Style.BRIGHT + f"\nApplied preset configuration: {preset}")
                    print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
                else:
                    print(Fore.RED + Style.BRIGHT + f"\nPreset '{preset}' not found, using base configuration:")
                    print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            except Exception as e:
                logger.warning(f"Failed to apply preset '{preset}': {e}")
                console.print(
                    Panel.fit(
                        #f"[bold yellow]Warning: Failed to apply preset '{preset}': {str(e)}[/bold yellow]",
                        f"Failed to apply preset '{preset}': {str(e)}",
                        title="WARNING",
                        style="bold red",
                        border_style="red",
                        padding=(1, 1),
                        box=box.ROUNDED
                    )
                )
        
        if non_interactive or use_current_config:
            
            # Display configuration summary with context
            model_config = base_config.get('model', {})
            training_config = base_config.get('training', {})
            data_config = base_config.get('data', {})
            hardware_config = base_config.get('hardware', {})
            system_config = base_config.get('system', {})
            
            # Apply defaults if not set
            model_type = model_config.get('model_type', 'EnhancedAutoencoder')
            epochs = training_config.get('epochs', DEFAULT_EPOCHS)
            batch_size = training_config.get('batch_size', DEFAULT_BATCH_SIZE)
            learning_rate = training_config.get('learning_rate', LEARNING_RATE)
            use_real_data_final = data_config.get('use_real_data', use_real_data if use_real_data is not None else False)
            mixed_precision = training_config.get('mixed_precision', torch.cuda.is_available())
            device_config = hardware_config.get('device', system_config.get('device', 'auto'))
            encoding_dim = model_config.get('encoding_dim', DEFAULT_ENCODING_DIM)
            hidden_dims = model_config.get('hidden_dims', HIDDEN_LAYER_SIZES)
            
            # Configuration display
            # print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
            # print(Fore.GREEN + Style.BRIGHT + "CONFIGURATION SUMMARY")
            # print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            
            # Core configuration
            print(Fore.YELLOW + Style.BRIGHT + "\nCore Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data Source: " + Fore.GREEN + f"{'Real Data' if use_real_data_final else 'Synthetic Data'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + f"{model_type}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training: " + Fore.GREEN + f"{epochs} epochs (~{_estimate_training_time(epochs, model_type)} min)")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + f"{batch_size}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + f"{learning_rate}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Mixed Precision: " + Fore.GREEN + f"{'Enabled' if mixed_precision else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Device: " + Fore.GREEN + f"{'GPU' if torch.cuda.is_available() else 'CPU'} ({device_config})")
            
            # Architecture details
            print(Fore.YELLOW + Style.BRIGHT + "\nArchitecture Details:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Encoding Dim: " + Fore.GREEN + f"{encoding_dim}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Hidden Dims: " + Fore.GREEN + f"{hidden_dims}")
            if model_type == 'AutoencoderEnsemble':
                num_models = model_config.get('num_models', 3)
                diversity_factor = model_config.get('diversity_factor', 0.3)
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Ensemble Size: " + Fore.GREEN + f"{num_models}")
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Diversity Factor: " + Fore.GREEN + f"{diversity_factor}")
            elif model_type == 'EnhancedAutoencoder':
                features = []
                if model_config.get('use_attention', True):
                    features.append("Attention")
                if model_config.get('residual_blocks', True):
                    features.append("Residual Blocks")
                if model_config.get('skip_connection', True):
                    features.append("Skip Connections")
                if features:
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Enhanced Features: " + Fore.GREEN + f"{', '.join(features)}")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Legacy Mode: " + Fore.GREEN + f"{model_config.get('legacy_mode', False)}")
            
            # Data configuration
            if use_real_data_final:
                data_path = data_config.get('data_path', 'Default')
                artifacts_path = data_config.get('artifacts_path', 'Default')
                print(Fore.YELLOW + Style.BRIGHT + "\nReal Data Configuration:")
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data Path: " + Fore.GREEN + f"{data_path}")
                if artifacts_path != 'Default':
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Artifacts Path: " + Fore.GREEN + f"{artifacts_path}")
            else:
                normal_samples = data_config.get('normal_samples', NORMAL_SAMPLES)
                attack_samples = data_config.get('attack_samples', ATTACK_SAMPLES)
                features = data_config.get('features', FEATURES)
                print(Fore.YELLOW + Style.BRIGHT + "\nSynthetic Data Configuration:")
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.GREEN + f"{normal_samples:,}")
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.GREEN + f"{attack_samples:,}")
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Features: " + Fore.GREEN + f"{features}")
            
            # System configuration
            model_dir = system_config.get('model_dir', DEFAULT_MODEL_DIR)
            reproducible = system_config.get('reproducible', True)
            random_seed = system_config.get('random_seed', RANDOM_STATE)
            print(Fore.YELLOW + Style.BRIGHT + "\nSystem Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Directory: " + Fore.GREEN + f"{model_dir}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Reproducible: " + Fore.GREEN + f"{reproducible}")
            if reproducible:
                print(Fore.CYAN + Style.BRIGHT + f"  └─ Random Seed: " + Fore.GREEN + f"{random_seed}")
            
            # Monitoring configuration
            monitoring_config = base_config.get('monitoring', {})
            tensorboard_logging = monitoring_config.get('tensorboard_logging', True)
            save_checkpoints = monitoring_config.get('save_checkpoints', True)
            verbose_mode = monitoring_config.get('verbose', True)
            print(Fore.YELLOW + Style.BRIGHT + "\nMonitoring Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ TensorBoard Logging: " + Fore.GREEN + f"{'Enabled' if tensorboard_logging else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Save Checkpoints: " + Fore.GREEN + f"{'Enabled' if save_checkpoints else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Verbose Output: " + Fore.GREEN + f"{'Enabled' if verbose_mode else 'Disabled'}")
            
            # Export configuration
            export_config = base_config.get('export', {})
            save_model = export_config.get('save_model', True)
            save_metadata = export_config.get('save_metadata', True)
            save_training_history = export_config.get('save_training_history', True)
            export_onnx = export_config.get('export_onnx', False)
            print(Fore.YELLOW + Style.BRIGHT + "\nExport Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Save Model: " + Fore.GREEN + f"{'Enabled' if save_model else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Save Metadata: " + Fore.GREEN + f"{'Enabled' if save_metadata else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Save Training History: " + Fore.GREEN + f"{'Enabled' if save_training_history else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Export ONNX: " + Fore.GREEN + f"{'Enabled' if export_onnx else 'Disabled'}")
            
            # Security configuration
            security_config = base_config.get('security', {})
            percentile = security_config.get('percentile', DEFAULT_PERCENTILE)
            threshold_method = security_config.get('anomaly_threshold_strategy', 'percentile')
            adaptive_threshold = security_config.get('adaptive_threshold', True)
            print(Fore.YELLOW + Style.BRIGHT + "\nSecurity Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Anomaly Threshold: " + Fore.GREEN + f"{percentile}th percentile")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Threshold Method: " + Fore.GREEN + f"{threshold_method}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Adaptive Threshold: " + Fore.GREEN + f"{'Enabled' if adaptive_threshold else 'Disabled'}")
            
            # Confirmation prompt
            print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart training with these settings? (Y/n/c to cancel): " + Style.RESET_ALL).strip().lower()
            
            if confirm in ('', 'y', 'yes'):
                print(Fore.GREEN + Style.BRIGHT + "\nLaunching training with configured defaults...")
                return _launch_training_with_config(config=base_config, **kwargs)
            elif confirm in ('c', 'cancel'):
                print(Fore.RED + Style.BRIGHT + "\nTraining cancelled by user")
                return None
            else:
                # Fallback options
                console.print(
                    Panel.fit(
                        "Would you like to switch setup mode?",
                        style="bold yellow",
                        border_style="yellow",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                print(Fore.WHITE + Style.BRIGHT + "\n1. Switch to interactive setup")
                print(Fore.WHITE + Style.BRIGHT + "2. Try again with current settings")
                print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
                
                while True:
                    try:
                        choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-2): " + Style.RESET_ALL).strip()
                        if choice in ['1', '2', '0']:
                            break
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                        return None
                
                if choice == '1':
                    print(Fore.GREEN + Style.BRIGHT + "\nSwitching to interactive setup...")
                    # Fall through to interactive mode
                    pass
                elif choice == '2':
                    print(Fore.GREEN + Style.BRIGHT + "\nRetrying with current settings...")
                    return train_model_interactive(
                        use_real_data=use_real_data,
                        use_current_config=use_current_config,
                        preset=preset,
                        config=config,
                        non_interactive=non_interactive,
                        **kwargs
                    )
                else:
                    print(Fore.RED + Style.BRIGHT + "\nCancelled by user")
                    return None
        
        # Interactive mode continues
        print(Fore.YELLOW + Style.BRIGHT + "\nSetup includes:")
        print(Fore.GREEN + Style.BRIGHT + "  ├─ Data source selection (real/synthetic)")
        print(Fore.GREEN + Style.BRIGHT + "  ├─ Model architecture configuration") 
        print(Fore.GREEN + Style.BRIGHT + "  ├─ Training parameters setup")
        print(Fore.GREEN + Style.BRIGHT + "  ├─ System and monitoring options")
        print(Fore.GREEN + Style.BRIGHT + "  └─ Export and saving preferences")
        
        print(Fore.YELLOW + Style.BRIGHT + "\nQuick Start Options:")
        print(Fore.WHITE + Style.BRIGHT + "1. Express Setup " + Fore.GREEN + Style.BRIGHT + "(recommended defaults)")
        print(Fore.WHITE + Style.BRIGHT + "2. Custom Configuration " + Fore.GREEN + Style.BRIGHT + "(full control)")
        print(Fore.WHITE + Style.BRIGHT + "3. Use Preset Configuration " + Fore.GREEN + Style.BRIGHT + f"(available: {len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'})")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        while True:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-3): " + Style.RESET_ALL).strip()
                if choice in ['1', '2', '3', '0']:
                    break
                print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                return None
        
        if choice == '1':
            print(Fore.GREEN + Style.BRIGHT + "\nEXPRESS SETUP")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            return _interactive_express_setup(base_config, use_real_data, **kwargs)
        elif choice == '2':
            print(Fore.GREEN + Style.BRIGHT + "\nCUSTOM CONFIGURATION")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            return _interactive_custom_setup(base_config, use_real_data, **kwargs)
        elif choice == '3':
            print(Fore.GREEN + Style.BRIGHT + "\nPRESET SELECTION")
            print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
            return _interactive_preset_setup(base_config, use_real_data, **kwargs)
        elif choice == '0':
            print(Fore.RED + Style.BRIGHT + "Training cancelled by user")
            return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\n\nTraining setup interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"Interactive training setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during interactive training setup: {str(e)}\n"
            f"Context:\n"
            f"- Use Real Data: {use_real_data}\n"
            f"- Use Current Config: {use_current_config}\n"
            f"- Preset: {preset}\n"
            f"- Non-interactive: {non_interactive}\n\n"
            f"This could be due to:\n"
            f"- Configuration file corruption\n"
            f"- Missing dependencies\n"
            f"- System resource issues\n"
            f"- Invalid parameter combinations"
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="TRAINING SETUP ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        return None

def _interactive_express_setup(
    base_config: Dict[str, Any], 
    use_real_data: Optional[bool],
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Interactive express setup for quick model training configuration.
    
    Provides a streamlined setup process with smart defaults while maintaining
    full compatibility with the centralized configuration system.
    """
    try:
        # Clear screen and show banner
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if base_config is None and banner_config is not None:
            base_config = banner_config
        else:
            base_config = base_config or {}
        
        # Extract context for display
        preset_name = "Custom/Default"
        model_type = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = base_config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = base_config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
            current_model_type = model_type
        
        # Clear screen and show context
        #print("\033c", end="")
        #print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "EXPRESS SETUP - QUICK CONFIGURATION")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Extract context from base_config for consistent display
        #preset_name = base_config.get('presets', {}).get('current_preset', 'Custom/Default')
        #current_model_type = base_config.get('model', {}).get('model_type', 'Unknown')
        #current_model_type = model_type
        
        print(Fore.YELLOW + Style.BRIGHT + f"\nBase Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Current Model: " + Fore.CYAN + Style.BRIGHT + f"{current_model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Mode: " + Fore.CYAN + Style.BRIGHT + f"Express Setup")
        
        #print(Fore.GREEN + Style.BRIGHT + "\nSetting up training with smart defaults...")
        
        # Data Source Selection
        if use_real_data is None:
            print(Fore.YELLOW + Style.BRIGHT + "\nDATA SOURCE SELECTION")
            #print(Fore.YELLOW + Style.BRIGHT + "-" * 30)
            print(Fore.WHITE + Style.BRIGHT + "\n1. Real network data " + Fore.GREEN + Style.BRIGHT + "(recommended for production)")
            print(Fore.WHITE + Style.BRIGHT + "2. Synthetic data " + Fore.GREEN + Style.BRIGHT + "(good for testing and development)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            while True:
                try:
                    data_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect data source (0-2): " + Style.RESET_ALL).strip()
                    if data_choice in ['1', '2', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                    return None
            
            if data_choice == '0':
                print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                return None
                
            use_real_data = data_choice == '1'
            print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {'Real Data' if use_real_data else 'Synthetic Data'}")
        
        # Model Architecture Selection
        print(Fore.YELLOW + Style.BRIGHT + "\nMODEL ARCHITECTURE SELECTION")
        #print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + "\n1. EnhancedAutoencoder " + Fore.GREEN + Style.BRIGHT + "(recommended - advanced features, good balance)")
        print(Fore.WHITE + Style.BRIGHT + "2. SimpleAutoencoder " + Fore.GREEN + Style.BRIGHT + "(fast and lightweight, minimal resources)")  
        print(Fore.WHITE + Style.BRIGHT + "3. AutoencoderEnsemble " + Fore.GREEN + Style.BRIGHT + "(best accuracy, slower, more resources)")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        while True:
            try:
                model_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect model type (0-3): " + Style.RESET_ALL).strip()
                if model_choice in ['1', '2', '3', '0']:
                    break
                print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nModel selection cancelled")
                return None
        
        if model_choice == '0':
            print(Fore.RED + Style.BRIGHT + "\nModel selection cancelled")
            return None
            
        model_types = ['EnhancedAutoencoder', 'SimpleAutoencoder', 'AutoencoderEnsemble']
        model_type = model_types[int(model_choice)-1]
        print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {model_type}")
        
        # Training Duration Selection
        print(Fore.YELLOW + Style.BRIGHT + "\nTRAINING DURATION SELECTION")
        #print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
        print(Fore.WHITE + Style.BRIGHT + "\n1. Quick Test " + Fore.GREEN + Style.BRIGHT + "(10 epochs - 1-3 minutes)")
        print(Fore.WHITE + Style.BRIGHT + "2. Standard " + Fore.GREEN + Style.BRIGHT + "(50 epochs - 5-15 minutes)")
        print(Fore.WHITE + Style.BRIGHT + "3. Thorough " + Fore.GREEN + Style.BRIGHT + "(100 epochs - 15-30 minutes)")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        while True:
            try:
                duration_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect training duration (0-3): " + Style.RESET_ALL).strip()
                if duration_choice in ['1', '2', '3', '0']:
                    break
                print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nDuration selection cancelled")
                return None
        
        if duration_choice == '0':
            print(Fore.RED + Style.BRIGHT + "\nDuration selection cancelled")
            return None
            
        epochs_map = {'1': 10, '2': 50, '3': 100}
        epochs = epochs_map.get(duration_choice, 50)
        print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {epochs} epochs")
        
        # Performance Configuration
        print(Fore.YELLOW + Style.BRIGHT + "\nPERFORMANCE CONFIGURATION")
        #print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
        mixed_precision = torch.cuda.is_available()
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name() if torch.cuda.is_available() else "Unknown"
            print(Fore.GREEN + Style.BRIGHT + f"\n✓ GPU detected: {gpu_name}")
            print(Fore.GREEN + Style.BRIGHT + "  Using mixed precision for faster training")
        else:
            print(Fore.YELLOW + Style.BRIGHT + "\n[i] CPU mode: Standard precision")
            print(Fore.YELLOW + Style.BRIGHT + "  Consider using GPU for better performance")
        
        # Build final configuration with smart defaults
        final_config = base_config.copy()
        
        # Model Configuration - using same structure as centralized config
        model_config = final_config.setdefault('model', {})
        model_config.update({
            'model_type': model_type,
            'activation': 'leaky_relu',
            'activation_param': 0.2,
            'normalization': 'batch',
            'use_batch_norm': True,
            'use_layer_norm': False,
            'bias': True,
            'weight_init': 'xavier_uniform',
            'skip_connection': True,
            # Enhanced features for better performance
            'use_attention': model_type != 'SimpleAutoencoder',
            'residual_blocks': model_type != 'SimpleAutoencoder',
            'legacy_mode': False
        })
        
        # Model-specific optimizations
        if model_type == 'SimpleAutoencoder':
            model_config.update({
                'encoding_dim': 16,
                'hidden_dims': [128, 64],
                'dropout_rates': [0.2, 0.15],
            })
        elif model_type == 'EnhancedAutoencoder':
            model_config.update({
                'encoding_dim': 32,
                'hidden_dims': [256, 128, 64],
                'dropout_rates': [0.2, 0.15, 0.1],
            })
        elif model_type == 'AutoencoderEnsemble':
            model_config.update({
                'encoding_dim': 24,
                'hidden_dims': [192, 96, 48],
                'dropout_rates': [0.25, 0.2, 0.15],
                'num_models': 3,
                'diversity_factor': 0.3,
            })
        
        # Training Configuration - aligned with train_model parameters
        training_config = final_config.setdefault('training', {})
        training_config.update({
            'epochs': epochs,
            'batch_size': 64,
            'learning_rate': 0.001,
            'patience': min(15, max(5, epochs // 3)),  # Adaptive patience
            'mixed_precision': mixed_precision,
            'optimizer': 'AdamW',
            'scheduler': 'ReduceLROnPlateau',
            'early_stopping': True,
            'validation_split': 0.2,
            'weight_decay': 1e-4,
            'adam_betas': (0.9, 0.999),
            'adam_eps': 1e-8,
            'gradient_clip': 1.0,
            'gradient_accumulation_steps': 1,
            'num_workers': 4 if torch.cuda.is_available() else 2,
            'shuffle': True,
            'pin_memory': torch.cuda.is_available(),
            'persistent_workers': True,
            'lr_patience': 5,
            'lr_factor': 0.5,
            'min_lr': 1e-6
        })
        
        # Data Configuration - consistent with interactive_main context
        data_config = final_config.setdefault('data', {})
        data_config.update({
            'use_real_data': use_real_data,
            'features': 20 if not use_real_data else None,
            'normal_samples': 8000 if not use_real_data else None,
            'attack_samples': 2000 if not use_real_data else None,
            'test_split': 0.2,
            'random_state': 42,
            'stratified_split': True,
            'data_normalization': 'standard',
            'anomaly_factor': 0.1,
            'data_preprocessing': True,
            'synthetic_generation': {
                'complexity': 'medium',
                'noise_level': 0.05,
                'correlation_strength': 0.3
            } if not use_real_data else {},
            'preprocessing': {
                'enabled': True,
                'feature_scaling': True,
                'outlier_handling': 'clip'
            }
        })
        
        # System Configuration - using same defaults as train_model_interactive
        system_config = final_config.setdefault('system', {})
        system_config.update({
            'reproducible': True,
            'random_seed': 42,
            'non_interactive': False,
            'model_dir': DEFAULT_MODEL_DIR,
            'log_dir': LOG_DIR,
            'config_dir': CONFIG_DIR,
            'data_dir': DATA_DIR,
            'checkpoint_dir': CHECKPOINTS_DIR,
            'results_dir': RESULTS_DIR,
            'debug': False,
            'parallel_processing': False,
            'max_workers': 4,
            'export_onnx': False,
            'cuda_optimizations': torch.cuda.is_available(),
            'onnx_export': {},
            'distributed_training': False,
            'python_executable': sys.executable,
            'working_directory': os.getcwd(),
            'environment_health': 'auto'
        })
        
        # Hardware Configuration - enhanced detection
        hardware_config = final_config.setdefault('hardware', {})
        hardware_config.update({
            'device': 'auto',
            'cuda_optimizations': torch.cuda.is_available(),
            'memory_management': {'enable_memory_efficient': True},
            'recommended_gpu_memory': 4.0,
            'minimum_system_requirements': {},
            'optimal_system_requirements': {},
            'performance_optimization': {},
            'detected_gpu_memory': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else None,
            'detected_system_memory': psutil.virtual_memory().total / 1e9 if 'psutil' in sys.modules else None,
            'system_performance_class': 'auto',
            'optimization_recommendations': []
        })
        
        # Monitoring Configuration - comprehensive logging
        monitoring_config = final_config.setdefault('monitoring', {})
        monitoring_config.update({
            'verbose': True,
            'debug_mode': False,
            'tensorboard_logging': True,
            'save_checkpoints': True,
            'save_best_model': True,
            'metrics_to_track': ['loss', 'reconstruction_error', 'learning_rate'],
            'checkpoint_frequency': max(10, epochs // 5),
            'log_frequency': 1,
            'metrics_frequency': 1,
            'console_logging_level': 'INFO',
            'save_model_history': True,
            'early_stopping_metric': 'val_loss',
            'checkpoint_format': 'pth',
            'log_model_summary': True,
            'tensorboard_dir': TB_DIR,
            'tensorboard': {},
            'stability_metrics': True,
            'performance_metrics': True,
            'profiling_enabled': False,
            'progress_bar': True
        })
        
        # Export Configuration
        export_config = final_config.setdefault('export', {})
        export_config.update({
            'save_model': True,
            'save_metadata': True,
            'save_training_history': True,
            'export_onnx': False
        })
        
        # Security Configuration - using same structure as main functions
        security_config = final_config.setdefault('security', {})
        security_config.update({
            'percentile': 95.0,
            'enable_security_metrics': True,
            'anomaly_threshold_strategy': 'percentile',
            'adaptive_threshold': True,
            'attack_threshold': None,
            'false_negative_cost': None,
            'early_warning_threshold': None,
            'confidence_interval': None,
            'detection_methods': ['reconstruction_error'],
            'alert_levels': ['low', 'medium', 'high'],
            'threshold_validation': True,
            'robust_detection': True,
            'false_positive_tolerance': None,
            'performance_optimized_detection': True,
            'real_time_monitoring': False,
            'ensemble_voting': 'average' if model_type == 'AutoencoderEnsemble' else 'single',
            'uncertainty_threshold': None
        })
        
        # Advanced Training Configuration
        advanced_config = final_config.setdefault('advanced_training', {})
        advanced_config.update({
            'memory_efficient': True,
            'compile_model': False,
            'benchmark_mode': False,
            'gradient_checkpointing': False
        })
        
        # Presets Configuration - maintain context
        presets_config = final_config.setdefault('presets', {})
        presets_config.update({
            'available_presets': list(globals().get('PRESET_CONFIGS', {}).keys()),
            'current_preset': 'express_setup',
            'current_override': None,
            'override_rules': {},
            'preset_configs': {},
            'custom_presets_available': [],
            'auto_apply': False,
            'validate_compatibility': True,
            'system_recommended_preset': None,
            'preset_compatibility': {}
        })
        
        # Runtime Configuration - track express setup
        runtime_config = final_config.setdefault('runtime', {})
        runtime_config.update({
            'config_loaded_at': datetime.now().isoformat(),
            'config_source': 'interactive_express_setup',
            'runtime_id': f"express_{int(time.time())}",
            'process_id': os.getpid(),
            'system_analysis_completed': True,
            'system_performance_score': None,
            'system_class': 'express',
            'optimizations_applied': {
                'mixed_precision': mixed_precision,
                'memory_efficient': True,
                'adaptive_patience': True
            },
            'resource_status': {
                'gpu_available': torch.cuda.is_available(),
                'mixed_precision_enabled': mixed_precision
            },
            'system_warnings': [],
            'recommendations': [
                "Express setup optimized for quick results",
                f"Using {model_type} with {epochs} epochs",
                "Monitor training progress in TensorBoard"
            ],
            'configuration_health': {
                'status': 'healthy',
                'checks_passed': True,
                'express_optimized': True
            }
        })
        
        # Display comprehensive configuration summary
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "EXPRESS SETUP - CONFIGURATION SUMMARY")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        
        # Core configuration display
        print(Fore.YELLOW + Style.BRIGHT + "\nCore Configuration:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Data Source: " + Fore.GREEN + f"{'Real Data' if use_real_data else 'Synthetic Data'}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Training Duration: " + Fore.GREEN + f"{epochs} epochs (~{_estimate_training_time(epochs, model_type)} min)")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + f"64")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + f"0.001")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Mixed Precision: " + Fore.GREEN + f"{'Enabled' if mixed_precision else 'Disabled'}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Device: " + Fore.GREEN + f"{'GPU' if torch.cuda.is_available() else 'CPU'}")
        
        # Architecture details
        print(Fore.YELLOW + Style.BRIGHT + "\nArchitecture Details:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Encoding Dimension: " + Fore.GREEN + f"{model_config['encoding_dim']}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Hidden Layers: " + Fore.GREEN + f"{model_config['hidden_dims']}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Dropout Rates: " + Fore.GREEN + f"{model_config['dropout_rates']}")
        if model_type == 'AutoencoderEnsemble':
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Ensemble Size: " + Fore.GREEN + f"{model_config['num_models']}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Diversity Factor: " + Fore.GREEN + f"{model_config['diversity_factor']}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Attention Mechanism: " + Fore.GREEN + f"{'Enabled' if model_config.get('use_attention', False) else 'Disabled'}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Residual Blocks: " + Fore.GREEN + f"{'Enabled' if model_config.get('residual_blocks', False) else 'Disabled'}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Skip Connections: " + Fore.GREEN + f"{'Enabled' if model_config.get('skip_connection', False) else 'Disabled'}")
        
        # Data configuration
        if use_real_data:
            data_path = data_config.get('data_path', 'Default')
            print(Fore.YELLOW + Style.BRIGHT + "\nReal Data Configuration:")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Data Path: " + Fore.GREEN + f"{data_path}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preprocessing: " + Fore.GREEN + f"Enabled")
            print(Fore.WHITE + Style.BRIGHT + f"  └─ Normalization: " + Fore.GREEN + f"Standard")
        else:
            print(Fore.YELLOW + Style.BRIGHT + "\nSynthetic Data Configuration:")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.GREEN + f"{data_config.get('normal_samples', 0):,}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.GREEN + f"{data_config.get('attack_samples', 0):,}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Features: " + Fore.GREEN + f"{data_config.get('features', 0)}")
            print(Fore.WHITE + Style.BRIGHT + f"  └─ Anomaly Factor: " + Fore.GREEN + f"{data_config.get('anomaly_factor', 0)}")
        
        # System configuration
        print(Fore.YELLOW + Style.BRIGHT + "\nSystem Configuration:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Reproducible: " + Fore.GREEN + f"{system_config.get('reproducible', True)}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Workers: " + Fore.GREEN + f"{training_config.get('num_workers', 0)}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ TensorBoard: " + Fore.GREEN + f"{'Enabled' if monitoring_config.get('tensorboard_logging', False) else 'Disabled'}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Checkpoints: " + Fore.GREEN + f"{'Enabled' if monitoring_config.get('save_checkpoints', False) else 'Disabled'}")
        
        # Confirmation with enhanced styling
        #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*50)
        try:
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart training with these express settings? (Y/n/c to cancel): " + Style.RESET_ALL).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print(Fore.RED + Style.BRIGHT + "\nConfiguration cancelled by user")
            return None
        
        if confirm in ('', 'y', 'yes'):
            print(Fore.GREEN + Style.BRIGHT + "\nLaunching training with express configuration...")
            return _launch_training_with_config(final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print(Fore.RED + Style.BRIGHT + "\nTraining cancelled")
            return None
        else:
            # Enhanced fallback options with styling
            print(Fore.YELLOW + Style.BRIGHT + "\nWould you like to?\n")
            print(Fore.WHITE + Style.BRIGHT + "1. Try express setup again with different settings")
            print(Fore.WHITE + Style.BRIGHT + "2. Switch to custom configuration for full control")
            print(Fore.RED + Style.BRIGHT + "0. Return to previous menu")
            
            while True:
                try:
                    retry_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-2): " + Style.RESET_ALL).strip()
                    if retry_choice in ['1', '2', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                    return None
            
            if retry_choice == '1':
                print(Fore.GREEN + Style.BRIGHT + "\nRestarting express setup...")
                return _interactive_express_setup(base_config, use_real_data, **kwargs)
            elif retry_choice == '2':
                print(Fore.GREEN + Style.BRIGHT + "\nSwitching to custom configuration...")
                return _interactive_custom_setup(base_config, use_real_data, **kwargs)
            else:
                print(Fore.RED + Style.BRIGHT + "\nReturning to previous menu")
                return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\nExpress setup interrupted by user")
        return None
    except Exception as e:
        logger.error(f"Express setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during express setup: {str(e)}\n"
            f"Context:\n"
            f"- Use Real Data: {use_real_data}\n"
            f"- Base Config: {bool(base_config)}\n\n"
            f"This could be due to:\n"
            f"- Configuration structure issues\n"
            f"- Missing required parameters\n"
            f"- System resource constraints\n"
            f"- Invalid user input handling"
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="EXPRESS SETUP ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        return None

def _interactive_preset_setup(
    base_config: Dict[str, Any],
    use_real_data: Optional[bool], 
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Interactive preset configuration setup with comprehensive context integration.
    
    Provides a streamlined preset selection experience while maintaining full
    compatibility with the centralized configuration system.
    """
    try:
        # Clear screen and show banner for consistency
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if base_config is None and banner_config is not None:
            base_config = banner_config
        else:
            base_config = base_config or {}
        
        # Extract context for display
        preset_name = "Custom/Default"
        model_type = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = base_config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
            current_preset = preset_name
        
        # Extract model type
        model_section = base_config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
            current_model_type = model_type
        
        # Clear screen and show context
        #print("\033c", end="")
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "PRESET CONFIGURATION SETUP")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        # Extract context from base_config for consistent display
        #current_preset = base_config.get('presets', {}).get('current_preset', 'Custom/Default')
        #current_model_type = base_config.get('model', {}).get('model_type', 'Unknown')
        
        print(Fore.GREEN + Style.BRIGHT + f"\nBase Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.CYAN + Style.BRIGHT + f"{current_preset}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Current Model: " + Fore.CYAN + Style.BRIGHT + f"{current_model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Mode: " + Fore.CYAN + Style.BRIGHT + f"Preset Selection")
        
        print(Fore.YELLOW + Style.BRIGHT + "\nAvailable Preset Configurations:\n")
        #print(Fore.YELLOW + Style.BRIGHT + "-" * 40)
        
        available_presets = list(globals().get('PRESET_CONFIGS', {}).keys())
        
        if not available_presets:
            message = (
                f"No preset configurations available.\n"
                f"Presets provide pre-configured settings for:\n"
                f"- Different model architectures\n"
                f"- Various use cases and scenarios\n"
                f"- Performance vs accuracy tradeoffs\n"
                f"- Hardware-specific optimizations\n\n"
                f"Switching to express setup as fallback..."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="NO PRESETS AVAILABLE",
                    style="bold yellow",
                    border_style="yellow",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            return _interactive_express_setup(base_config, use_real_data, **kwargs)
        
        # Display available presets with enhanced formatting
        for i, preset_name in enumerate(available_presets, 1):
            preset_config = globals()['PRESET_CONFIGS'][preset_name]
            metadata = preset_config.get('metadata', {})
            description = metadata.get('description', 'No description available')
            recommended_use = metadata.get('recommended_use', 'General purpose')
            model_type = preset_config.get('model', {}).get('model_type', 'Unknown')
            
            print(Fore.WHITE + Style.BRIGHT + f"{i}. {preset_name}\n")
            print(Fore.CYAN + Style.BRIGHT + f"   Model: " + Fore.GREEN + Style.BRIGHT + f"{model_type}")
            print(Fore.CYAN + Style.BRIGHT + f"   Description: " + Fore.MAGENTA + Style.BRIGHT + f"{description}")
            print(Fore.CYAN + Style.BRIGHT + f"   Best for: " + Fore.YELLOW + Style.BRIGHT + f"{recommended_use}")
            
            # Show key configuration highlights
            training_config = preset_config.get('training', {})
            epochs = training_config.get('epochs', 'Default')
            batch_size = training_config.get('batch_size', 'Default')
            learning_rate = training_config.get('learning_rate', 'Default')
            
            print(Fore.CYAN + Style.BRIGHT + f"   Configuration: " + Fore.GREEN + Style.BRIGHT + f"{epochs} epochs, " + Fore.GREEN + Style.BRIGHT + f"batch {batch_size}, " + Fore.GREEN + Style.BRIGHT + f"LR {learning_rate}")
            print()  # Empty line for spacing
        
        # Enhanced navigation options
        print(Fore.YELLOW + Style.BRIGHT + "\nNavigation Options:\n")
        print(Fore.WHITE + Style.BRIGHT + f"{len(available_presets)+1}. " + Fore.GREEN + Style.BRIGHT + "Switch to Express Setup")
        print(Fore.WHITE + Style.BRIGHT + f"{len(available_presets)+2}. " + Fore.GREEN + Style.BRIGHT + "Switch to Custom Configuration")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        # Preset selection with robust error handling
        while True:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect preset (1-{len(available_presets)}) or navigation option: " + Style.RESET_ALL).strip()
                
                if not choice:
                    continue
                    
                choice_num = int(choice)
                
                if 1 <= choice_num <= len(available_presets):
                    selected_preset = available_presets[choice_num-1]
                    break
                elif choice_num == 0:
                    print(Fore.RED + Style.BRIGHT + "\nPreset selection cancelled")
                    return None
                elif choice_num == len(available_presets) + 1:
                    print(Fore.GREEN + Style.BRIGHT + "\nSwitching to express setup...")
                    return _interactive_express_setup(base_config, use_real_data, **kwargs)
                elif choice_num == len(available_presets) + 2:
                    print(Fore.GREEN + Style.BRIGHT + "\nSwitching to custom configuration...")
                    return _interactive_custom_setup(base_config, use_real_data, **kwargs)
                else:
                    print(Fore.RED + Style.BRIGHT + f"\nInvalid choice. Please select 1-{len(available_presets)+2} or 0 to cancel.")
                    
            except ValueError:
                print(Fore.RED + Style.BRIGHT + f"\nInvalid input. Please enter a number 1-{len(available_presets)+2} or 0 to cancel.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nPreset selection cancelled")
                return None
        
        # Load and display selected preset details
        preset_config = globals()['PRESET_CONFIGS'][selected_preset].copy()
        
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + f"SELECTED PRESET: {selected_preset}")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        # Extract preset details for display
        model_config = preset_config.get('model', {})
        training_config = preset_config.get('training', {})
        data_config = preset_config.get('data', {})
        metadata = preset_config.get('metadata', {})
        
        model_type = model_config.get('model_type', 'Unknown')
        epochs = training_config.get('epochs', 'Default')
        batch_size = training_config.get('batch_size', 'Default')
        learning_rate = training_config.get('learning_rate', 'Default')
        description = metadata.get('description', 'No description available')
        recommended_use = metadata.get('recommended_use', 'General purpose')
        
        # Display preset overview
        print(Fore.YELLOW + Style.BRIGHT + "\nPreset Overview:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Description: " + Fore.WHITE + Style.BRIGHT + f"{description}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Recommended Use: " + Fore.GREEN + Style.BRIGHT + f"{recommended_use}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + Style.BRIGHT + f"{model_type}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training: " + Fore.GREEN + Style.BRIGHT + f"{epochs} epochs (~{_estimate_training_time(epochs, model_type)} min)")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + Style.BRIGHT + f"{batch_size}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Learning Rate: " + Fore.GREEN + Style.BRIGHT + f"{learning_rate}")
        
        # Architecture details
        if 'encoding_dim' in model_config:
            print(Fore.YELLOW + Style.BRIGHT + "\nArchitecture Details:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Encoding Dimension: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('encoding_dim', 'N/A')}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Hidden Layers: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('hidden_dims', 'N/A')}")
            
            if model_type == 'AutoencoderEnsemble':
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Ensemble Size: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('num_models', 'N/A')}")
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Diversity Factor: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('diversity_factor', 'N/A')}")
            
            # Enhanced features
            enhanced_features = []
            if model_config.get('use_attention', False):
                enhanced_features.append("Attention")
            if model_config.get('residual_blocks', False):
                enhanced_features.append("Residual Blocks")
            if model_config.get('skip_connection', False):
                enhanced_features.append("Skip Connections")
            
            if enhanced_features:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Enhanced Features: " + Fore.GREEN + Style.BRIGHT + f"{', '.join(enhanced_features)}")
            
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Normalization: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('normalization', 'Default')}")
        
        # Training configuration details
        print(Fore.YELLOW + Style.BRIGHT + "\nTraining Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Optimizer: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('optimizer', 'Default')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Scheduler: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('scheduler', 'Default')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Early Stopping: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if training_config.get('early_stopping', True) else 'Disabled'}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Mixed Precision: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if training_config.get('mixed_precision', False) else 'Disabled'}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Validation Split: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('validation_split', 'Default')}")
        
        # Customization options with enhanced descriptions
        print(Fore.YELLOW + Style.BRIGHT + "\nCustomization Options:\n")
        print(Fore.WHITE + Style.BRIGHT + "1. Use preset as-is " + Fore.GREEN + Style.BRIGHT + "(recommended for first use)")
        print(Fore.WHITE + Style.BRIGHT + "2. Customize data source only " + Fore.GREEN + Style.BRIGHT + "(keep preset, change data)")
        print(Fore.WHITE + Style.BRIGHT + "3. Customize training duration " + Fore.GREEN + Style.BRIGHT + "(adjust epochs and patience)")
        print(Fore.WHITE + Style.BRIGHT + "4. Customize model architecture " + Fore.GREEN + Style.BRIGHT + "(change model type or layers)")
        print(Fore.WHITE + Style.BRIGHT + "5. Full customization " + Fore.GREEN + Style.BRIGHT + "(complete control)")
        print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
        
        while True:
            try:
                custom_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect customization option (0-5): " + Style.RESET_ALL).strip()
                if custom_choice in ['1', '2', '3', '4', '5', '0']:
                    break
                print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-5.")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nCustomization selection cancelled")
                return None
        
        if custom_choice == '0':
            print(Fore.RED + Style.BRIGHT + "\nPreset selection cancelled")
            return None
        
        # Merge configurations using the same pattern as train_model_interactive
        final_config = _merge_configs(base_config, preset_config)
        
        # Data source customization
        if custom_choice in ['2', '3', '4', '5'] or use_real_data is None:
            if use_real_data is None:
                print(Fore.YELLOW + Style.BRIGHT + "\nDATA SOURCE SELECTION\n")
                #print(Fore.YELLOW + Style.BRIGHT + "-" * 30)
                print(Fore.WHITE + Style.BRIGHT + "1. Real network data " + Fore.GREEN + Style.BRIGHT + "(production use)")
                print(Fore.WHITE + Style.BRIGHT + "2. Synthetic data " + Fore.GREEN + Style.BRIGHT + "(testing/development)")
                print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
                
                while True:
                    try:
                        data_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect data source (0-2): " + Style.RESET_ALL).strip()
                        if data_choice in ['1', '2', '0']:
                            break
                        print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                        return None
                
                if data_choice == '0':
                    print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                    return None
                    
                use_real_data = data_choice == '1'
                print(Fore.GREEN + Style.BRIGHT + f"\nSelected: {'Real Data' if use_real_data else 'Synthetic Data'}")
            
            final_config.setdefault('data', {})['use_real_data'] = use_real_data
        
        # Training duration customization
        if custom_choice in ['3', '4', '5']:
            print(Fore.YELLOW + Style.BRIGHT + "\nTRAINING DURATION CUSTOMIZATION\n")
            #print(Fore.YELLOW + Style.BRIGHT + "-" * 30)
            current_epochs = training_config.get('epochs', 50)
            estimated_time = _estimate_training_time(current_epochs, model_type)
            
            print(Fore.CYAN + Style.BRIGHT + f"Current: " + Fore.GREEN + Style.BRIGHT + f"{current_epochs} epochs (~{estimated_time} min)")
            print(Fore.CYAN + Style.BRIGHT + "Quick: 10 epochs | Standard: 50 epochs | Thorough: 100+ epochs")
            
            try:
                new_epochs = input(Fore.YELLOW + Style.BRIGHT + f"\nNew epochs (Enter for {current_epochs}, 'c' to cancel): " + Style.RESET_ALL).strip()
                
                if new_epochs.lower() == 'c':
                    print(Fore.RED + Style.BRIGHT + "\nDuration customization cancelled")
                    return None
                elif new_epochs:
                    new_epochs_int = int(new_epochs)
                    final_config.setdefault('training', {})['epochs'] = new_epochs_int
                    # Adaptive patience based on epochs
                    final_config['training']['patience'] = min(20, max(5, new_epochs_int // 3))
                    
                    print(Fore.GREEN + Style.BRIGHT + f"\nUpdated: {new_epochs_int} epochs (~{_estimate_training_time(new_epochs_int, model_type)} min)")
                else:
                    print(Fore.GREEN + Style.BRIGHT + f"\nKeeping: {current_epochs} epochs")
                    
            except ValueError:
                print(Fore.RED + Style.BRIGHT + f"\nInvalid input, keeping {current_epochs} epochs")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nDuration customization cancelled")
                return None
        
        # Model architecture customization
        if custom_choice in ['4', '5']:
            print(Fore.YELLOW + Style.BRIGHT + "\nMODEL ARCHITECTURE CUSTOMIZATION\n")
            #print(Fore.YELLOW + Style.BRIGHT + "-" * 30)
            current_model_type = model_config.get('model_type', 'EnhancedAutoencoder')
            
            print(Fore.YELLOW + Style.BRIGHT + f"Current model: " + Fore.CYAN + Style.BRIGHT + f"{current_model_type}")
            print(Fore.WHITE + Style.BRIGHT + "1. Keep current model type")
            print(Fore.WHITE + Style.BRIGHT + "2. SimpleAutoencoder " + Fore.GREEN + Style.BRIGHT + "(fast, lightweight)")
            print(Fore.WHITE + Style.BRIGHT + "3. EnhancedAutoencoder " + Fore.GREEN + Style.BRIGHT + "(balanced, recommended)")
            print(Fore.WHITE + Style.BRIGHT + "4. AutoencoderEnsemble " + Fore.GREEN + Style.BRIGHT + "(accurate, slower)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            try:
                model_change = input(Fore.YELLOW + Style.BRIGHT + "\nSelect model type (0-4, default=1): " + Style.RESET_ALL).strip()
                
                if model_change == '0':
                    print(Fore.RED + Style.BRIGHT + "\nModel customization cancelled")
                    return None
                elif model_change in ['2', '3', '4']:
                    new_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                    new_model_type = new_types[int(model_change)-2]
                    final_config.setdefault('model', {})['model_type'] = new_model_type
                    
                    print(Fore.GREEN + Style.BRIGHT + f"\nChanged model type to: {new_model_type}")
                    
                    # Apply model-specific defaults
                    _apply_model_type_defaults(final_config, new_model_type)
                else:
                    print(Fore.GREEN + Style.BRIGHT + f"\nKeeping: {current_model_type}")
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nModel customization cancelled")
                return None
        
        # Full customization fallback
        if custom_choice == '5':
            print(Fore.GREEN + Style.BRIGHT + "\nSwitching to full customization mode...")
            return _interactive_custom_setup(final_config, use_real_data, **kwargs)
        
        # Final confirmation with comprehensive summary
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "FINAL CONFIGURATION SUMMARY")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        final_model_type = final_config.get('model', {}).get('model_type', model_type)
        final_epochs = final_config.get('training', {}).get('epochs', epochs)
        final_data_source = final_config.get('data', {}).get('use_real_data', use_real_data)
        
        print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Preset: " + Fore.GREEN + Style.BRIGHT + f"{selected_preset}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model: " + Fore.GREEN + Style.BRIGHT + f"{final_model_type}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + Style.BRIGHT + f"{'Real' if final_data_source else 'Synthetic'}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.GREEN + Style.BRIGHT + f"{final_epochs}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + Style.BRIGHT + f"~{_estimate_training_time(final_epochs, final_model_type)} min")
        
        if custom_choice != '1':
            print(Fore.YELLOW + Style.BRIGHT + f"\nCustomizations Applied:")
            if custom_choice in ['2', '3', '4', '5']:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data source customized")
            if custom_choice in ['3', '4', '5']:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training duration adjusted")
            if custom_choice in ['4', '5']:
                print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model architecture modified")
        
        # Enhanced confirmation with fallback options
        #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*50)
        try:
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart training with this configuration? (Y/n/c to cancel): " + Style.RESET_ALL).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print(Fore.RED + Style.BRIGHT + "\nTraining confirmation cancelled")
            return None
        
        if confirm in ('', 'y', 'yes'):
            print(Fore.GREEN + Style.BRIGHT + "\nLaunching training with preset configuration...")
            return _launch_training_with_config(final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print(Fore.RED + Style.BRIGHT + "\nTraining cancelled")
            return None
        else:
            # Enhanced fallback options
            print(Fore.YELLOW + Style.BRIGHT + "\nWould you like to?\n")
            print(Fore.WHITE + Style.BRIGHT + "1. Try preset selection again")
            print(Fore.WHITE + Style.BRIGHT + "2. Switch to express setup")
            print(Fore.WHITE + Style.BRIGHT + "3. Switch to custom configuration")
            print(Fore.RED + Style.BRIGHT + "0. Return to previous menu")
            
            while True:
                try:
                    retry_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-3): " + Style.RESET_ALL).strip()
                    if retry_choice in ['1', '2', '3', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-3.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                    return None
            
            if retry_choice == '1':
                print(Fore.GREEN + Style.BRIGHT + "\nRestarting preset selection...")
                return _interactive_preset_setup(base_config, use_real_data, **kwargs)
            elif retry_choice == '2':
                print(Fore.GREEN + Style.BRIGHT + "\nSwitching to express setup...")
                return _interactive_express_setup(base_config, use_real_data, **kwargs)
            elif retry_choice == '3':
                print(Fore.GREEN + Style.BRIGHT + "\nSwitching to custom configuration...")
                return _interactive_custom_setup(base_config, use_real_data, **kwargs)
            else:
                print(Fore.RED + Style.BRIGHT + "\nReturning to previous menu")
                return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\nPreset setup interrupted by user")
        return None
    except Exception as e:
        logger.error(f"Preset setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during preset setup: {str(e)}\n"
            f"Context:\n"
            f"- Base Config: {bool(base_config)}\n"
            f"- Use Real Data: {use_real_data}\n"
            f"- Available Presets: {len(available_presets) if 'available_presets' in locals() else 0}\n\n"
            f"This could be due to:\n"
            f"- Preset configuration corruption\n"
            f"- Invalid preset structure\n"
            f"- Configuration merging issues\n"
            f"- User input handling problems"
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="PRESET SETUP ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        return None

def _interactive_custom_setup(
    base_config: Dict[str, Any],
    use_real_data: Optional[bool],
    **kwargs
) -> Optional[Dict[str, Any]]:
    """Full interactive configuration with all options matching class parameters."""
    
    try:
        # Clear screen and show banner for consistency
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if base_config is None and banner_config is not None:
            base_config = banner_config
        else:
            base_config = base_config or {}
        
        # Extract context for display
        preset_name = "Custom/Default"
        model_type = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = base_config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = base_config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Menu header with context matching other functions
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "CUSTOM CONFIGURATION SETUP")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + f"Active Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Mode: " + Fore.CYAN + Style.BRIGHT + "Full Custom Control")
        
        print("\nFull interactive configuration with complete control over all parameters.\n")
        print("Press Enter to accept defaults shown in parentheses.\n")
        print("Enter 'c' at any prompt to cancel and return to previous menu")
        
        final_config = base_config.copy()
        
        # DATA CONFIGURATION SECTION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "DATA CONFIGURATION")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        if use_real_data is None:
            print(Fore.GREEN + Style.BRIGHT + "\nData Source Selection:\n")
            print(Fore.WHITE + Style.BRIGHT + "1. Real network data " + Fore.YELLOW + Style.BRIGHT + "(requires data files)")
            print(Fore.WHITE + Style.BRIGHT + "2. Synthetic data " + Fore.YELLOW + Style.BRIGHT + "(generated automatically)")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to previous menu")
            
            while True:
                try:
                    data_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect data source (0-2): " + Style.RESET_ALL).strip()
                    if data_choice in ['1', '2', '0']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nPlease select 1, 2, or 0")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                    return None
            
            if data_choice == '0':
                print(Fore.RED + Style.BRIGHT + "\nData selection cancelled")
                return None
                
            use_real_data = data_choice == '1'
        
        data_config = final_config.setdefault('data', {})
        data_config['use_real_data'] = use_real_data
        
        if not use_real_data:
            print(Fore.GREEN + Style.BRIGHT + "\nSynthetic Data Parameters:")
            cancel_msg = Fore.RED + Style.BRIGHT + "\nSynthetic data configuration cancelled"
            
            normal_samples = input(Fore.YELLOW + Style.BRIGHT + "Normal samples " + Fore.WHITE + Style.BRIGHT + "(8000): " + Style.RESET_ALL).strip()
            if normal_samples.lower() == 'c':
                print(cancel_msg)
                return None
            data_config['normal_samples'] = int(normal_samples) if normal_samples else 8000
            
            attack_samples = input(Fore.YELLOW + Style.BRIGHT + "Attack samples " + Fore.WHITE + Style.BRIGHT + "(2000): " + Style.RESET_ALL).strip()
            if attack_samples.lower() == 'c':
                print(cancel_msg)
                return None
            data_config['attack_samples'] = int(attack_samples) if attack_samples else 2000
            
            features = input(Fore.YELLOW + Style.BRIGHT + "Number of features " + Fore.WHITE + Style.BRIGHT + "(20): " + Style.RESET_ALL).strip()
            if features.lower() == 'c':
                print(cancel_msg)
                return None
            data_config['features'] = int(features) if features else 20
            
            anomaly_factor = input(Fore.YELLOW + Style.BRIGHT + "Anomaly factor " + Fore.WHITE + Style.BRIGHT + "(0.1): " + Style.RESET_ALL).strip()
            if anomaly_factor.lower() == 'c':
                print(cancel_msg)
                return None
            data_config['anomaly_factor'] = float(anomaly_factor) if anomaly_factor else 0.1
        else:
            print(Fore.GREEN + Style.BRIGHT + "\nReal Data Configuration:")
            cancel_msg = Fore.RED + Style.BRIGHT + "\nReal data configuration cancelled"
            
            data_path = input(Fore.YELLOW + Style.BRIGHT + "Data file path " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
            if data_path.lower() == 'c':
                print(cancel_msg)
                return None
            if data_path:
                data_config['data_path'] = data_path
            
            artifacts_path = input(Fore.YELLOW + Style.BRIGHT + "Artifacts path " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
            if artifacts_path.lower() == 'c':
                print(cancel_msg)
                return None
            if artifacts_path:
                data_config['artifacts_path'] = artifacts_path
        
        # Data processing parameters
        random_state = input(Fore.YELLOW + Style.BRIGHT + "Random state " + Fore.WHITE + Style.BRIGHT + "(42): " + Style.RESET_ALL).strip()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nData configuration cancelled"
        
        if random_state.lower() == 'c':
            print(cancel_msg)
            return None
        data_config['random_state'] = int(random_state) if random_state else 42
        
        test_split = input(Fore.YELLOW + Style.BRIGHT + "Test split ratio " + Fore.WHITE + Style.BRIGHT + "(0.2): " + Style.RESET_ALL).strip()
        if test_split.lower() == 'c':
            print(cancel_msg)
            return None
        data_config['test_split'] = float(test_split) if test_split else 0.2
        
        stratified = input(Fore.YELLOW + Style.BRIGHT + "Use stratified split? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if stratified.lower() == 'c':
            print(cancel_msg)
            return None
        data_config['stratified_split'] = stratified in ('', 'y', 'yes')
        
        normalization = input(Fore.YELLOW + Style.BRIGHT + "Data normalization " + Fore.WHITE + Style.BRIGHT + "(standard/minmax/none): " + Style.RESET_ALL).strip()
        if normalization.lower() == 'c':
            print(cancel_msg)
            return None
        data_config['data_normalization'] = normalization if normalization else 'standard'
        
        preprocessing = input(Fore.YELLOW + Style.BRIGHT + "Enable data preprocessing? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if preprocessing.lower() == 'c':
            print(cancel_msg)
            return None
        data_config['data_preprocessing'] = preprocessing in ('', 'y', 'yes')

        # MODEL ARCHITECTURE SECTION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "MODEL ARCHITECTURE")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        print(Fore.GREEN + Style.BRIGHT + "\nModel Type Selection:")
        model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
        for i, mtype in enumerate(model_types, 1):
            print(Fore.WHITE + Style.BRIGHT + f"{i}. {mtype}")
        print(Fore.RED + Style.BRIGHT + f"{len(model_types)+1}. Cancel and return to previous menu")
        
        while True:
            try:
                model_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect model (1-{len(model_types)+1}): " + Style.RESET_ALL).strip()
                if model_choice in [str(i) for i in range(1, len(model_types)+2)]:
                    break
                print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(model_types)+1}")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nModel selection cancelled")
                return None
        
        if model_choice == str(len(model_types)+1):
            print(Fore.RED + Style.BRIGHT + "\nModel selection cancelled")
            return None
        
        model_type = model_types[int(model_choice)-1]
        
        model_config = final_config.setdefault('model', {})
        model_config['model_type'] = model_type
        
        print(Fore.GREEN + Style.BRIGHT + f"\nArchitecture Configuration for {model_type}:")
        cancel_msg = Fore.RED + Style.BRIGHT + "\nArchitecture configuration cancelled"
        
        # Encoding dimension with model-specific defaults
        default_encoding = {
            'SimpleAutoencoder': 16,
            'EnhancedAutoencoder': 32, 
            'AutoencoderEnsemble': 24
        }
        encoding_dim = input(Fore.YELLOW + Style.BRIGHT + f"Encoding dimension " + Fore.WHITE + Style.BRIGHT + f"({default_encoding[model_type]}): " + Style.RESET_ALL).strip()
        if encoding_dim.lower() == 'c':
            print(cancel_msg)
            return None
        model_config['encoding_dim'] = int(encoding_dim) if encoding_dim else default_encoding[model_type]
        
        # Hidden dimensions with model-specific defaults
        default_hidden_dims = {
            'SimpleAutoencoder': [128, 64],
            'EnhancedAutoencoder': [256, 128, 64],
            'AutoencoderEnsemble': [192, 96, 48]
        }
        hidden_dims_input = input(Fore.YELLOW + Style.BRIGHT + "Hidden layers " + Fore.WHITE + Style.BRIGHT + f"(comma-separated, default: {','.join(map(str, default_hidden_dims[model_type]))}): " + Style.RESET_ALL).strip()
        if hidden_dims_input.lower() == 'c':
            print(cancel_msg)
            return None
        if hidden_dims_input:
            model_config['hidden_dims'] = [int(x.strip()) for x in hidden_dims_input.split(',')]
        else:
            model_config['hidden_dims'] = default_hidden_dims[model_type]
        
        # Dropout rates with intelligent defaults
        dropout_input = input(Fore.YELLOW + Style.BRIGHT + "Dropout rates " + Fore.WHITE + Style.BRIGHT + "(comma-separated, e.g., 0.2,0.15,0.1): " + Style.RESET_ALL).strip()
        if dropout_input.lower() == 'c':
            print(cancel_msg)
            return None
        if dropout_input:
            model_config['dropout_rates'] = [float(x.strip()) for x in dropout_input.split(',')]
        else:
            hidden_len = len(model_config['hidden_dims'])
            # Create decreasing dropout rates
            model_config['dropout_rates'] = [0.2 - i*0.05 for i in range(hidden_len)]
        
        print(Fore.GREEN + Style.BRIGHT + "\nNetwork Configuration:")
        cancel_msg = Fore.RED + Style.BRIGHT + "\nNetwork configuration cancelled"
        
        activation = input(Fore.YELLOW + Style.BRIGHT + "Activation function " + Fore.WHITE + Style.BRIGHT + "(leaky_relu/relu/gelu/tanh): " + Style.RESET_ALL).strip()
        if activation.lower() == 'c':
            print(cancel_msg)
            return None
        model_config['activation'] = activation if activation else 'leaky_relu'
        
        if model_config['activation'] == 'leaky_relu':
            activation_param = input(Fore.YELLOW + Style.BRIGHT + "Negative slope for LeakyReLU " + Fore.WHITE + Style.BRIGHT + "(0.2): " + Style.RESET_ALL).strip()
            if activation_param.lower() == 'c':
                print(cancel_msg)
                return None
            model_config['activation_param'] = float(activation_param) if activation_param else 0.2
        
        normalization = input(Fore.YELLOW + Style.BRIGHT + "Normalization " + Fore.WHITE + Style.BRIGHT + "(batch/layer/none): " + Style.RESET_ALL).strip()
        if normalization.lower() == 'c':
            print(cancel_msg)
            return None
        model_config['normalization'] = normalization if normalization else 'batch'
        
        if model_config['normalization'] == 'batch':
            model_config['use_batch_norm'] = True
            model_config['use_layer_norm'] = False
        elif model_config['normalization'] == 'layer':
            model_config['use_batch_norm'] = False
            model_config['use_layer_norm'] = True
        else:
            model_config['use_batch_norm'] = False
            model_config['use_layer_norm'] = False
        
        bias = input(Fore.YELLOW + Style.BRIGHT + "Use bias in layers? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if bias.lower() == 'c':
            print(cancel_msg)
            return None
        model_config['bias'] = bias in ('', 'y', 'yes')
        
        weight_init = input(Fore.YELLOW + Style.BRIGHT + "Weight initialization " + Fore.WHITE + Style.BRIGHT + "(xavier_uniform/xavier_normal/kaiming_uniform): " + Style.RESET_ALL).strip()
        if weight_init.lower() == 'c':
            print(cancel_msg)
            return None
        model_config['weight_init'] = weight_init if weight_init else 'xavier_uniform'
        
        # Enhanced features for advanced models
        if model_type in ['EnhancedAutoencoder', 'AutoencoderEnsemble']:
            print(Fore.GREEN + Style.BRIGHT + "\nEnhanced Features Configuration:")
            cancel_msg = Fore.RED + Style.BRIGHT + "\nEnhanced features configuration cancelled"
            
            attention = input(Fore.YELLOW + Style.BRIGHT + "Use attention mechanism? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if attention == 'c':
                print(cancel_msg)
                return None
            model_config['use_attention'] = attention in ('', 'y', 'yes')
            
            residual = input(Fore.YELLOW + Style.BRIGHT + "Use residual blocks? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if residual == 'c':
                print(cancel_msg)
                return None
            model_config['residual_blocks'] = residual in ('', 'y', 'yes')
            
            skip_conn = input(Fore.YELLOW + Style.BRIGHT + "Use skip connections? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if skip_conn == 'c':
                print(cancel_msg)
                return None
            model_config['skip_connection'] = skip_conn in ('', 'y', 'yes')
            
            if model_type == 'EnhancedAutoencoder':
                legacy = input(Fore.YELLOW + Style.BRIGHT + "Use legacy mode? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
                if legacy == 'c':
                    print(cancel_msg)
                    return None
                model_config['legacy_mode'] = legacy in ('y', 'yes')
        
        # Ensemble-specific configuration
        if model_type == 'AutoencoderEnsemble':
            print(Fore.GREEN + Style.BRIGHT + "\nEnsemble Configuration:")
            cancel_msg = Fore.RED + Style.BRIGHT + "\nEnsemble configuration cancelled"
            
            num_models = input(Fore.YELLOW + Style.BRIGHT + "Number of ensemble models " + Fore.WHITE + Style.BRIGHT + "(3): " + Style.RESET_ALL).strip()
            if num_models.lower() == 'c':
                print(cancel_msg)
                return None
            model_config['num_models'] = int(num_models) if num_models else 3
            
            diversity = input(Fore.YELLOW + Style.BRIGHT + "Diversity factor " + Fore.WHITE + Style.BRIGHT + "(0.3): " + Style.RESET_ALL).strip()
            if diversity.lower() == 'c':
                print(cancel_msg)
                return None
            model_config['diversity_factor'] = float(diversity) if diversity else 0.3
        
        # Validation configuration
        min_features = input(Fore.YELLOW + Style.BRIGHT + "Minimum features validation " + Fore.WHITE + Style.BRIGHT + "(5): " + Style.RESET_ALL).strip()
        if min_features.lower() == 'c':
            print(Fore.RED + Style.BRIGHT + "\nValidation configuration cancelled")
            return None
        model_config['min_features'] = int(min_features) if min_features else 5

        # TRAINING CONFIGURATION SECTION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "TRAINING CONFIGURATION")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        training_config = final_config.setdefault('training', {})
        
        print(Fore.GREEN + Style.BRIGHT + "\nBasic Training Parameters:")
        cancel_msg = Fore.RED + Style.BRIGHT + "\nTraining configuration cancelled"
        
        epochs = input(Fore.YELLOW + Style.BRIGHT + "Number of epochs " + Fore.WHITE + Style.BRIGHT + "(50): " + Style.RESET_ALL).strip()
        if epochs.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['epochs'] = int(epochs) if epochs else 50
        
        batch_size = input(Fore.YELLOW + Style.BRIGHT + "Batch size " + Fore.WHITE + Style.BRIGHT + "(64): " + Style.RESET_ALL).strip()
        if batch_size.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['batch_size'] = int(batch_size) if batch_size else 64
        
        lr = input(Fore.YELLOW + Style.BRIGHT + "Learning rate " + Fore.WHITE + Style.BRIGHT + "(0.001): " + Style.RESET_ALL).strip()
        if lr.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['learning_rate'] = float(lr) if lr else 0.001
        
        patience = input(Fore.YELLOW + Style.BRIGHT + "Early stopping patience " + Fore.WHITE + Style.BRIGHT + "(15): " + Style.RESET_ALL).strip()
        if patience.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['patience'] = int(patience) if patience else 15
        
        validation_split = input(Fore.YELLOW + Style.BRIGHT + "Validation split " + Fore.WHITE + Style.BRIGHT + "(0.2): " + Style.RESET_ALL).strip()
        if validation_split.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['validation_split'] = float(validation_split) if validation_split else 0.2
        
        print(Fore.GREEN + Style.BRIGHT + "\nAdvanced Training Configuration:")
        weight_decay = input(Fore.YELLOW + Style.BRIGHT + "Weight decay " + Fore.WHITE + Style.BRIGHT + "(1e-4): " + Style.RESET_ALL).strip()
        if weight_decay.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['weight_decay'] = float(weight_decay) if weight_decay else 1e-4
        
        gradient_clip = input(Fore.YELLOW + Style.BRIGHT + "Gradient clipping threshold " + Fore.WHITE + Style.BRIGHT + "(1.0): " + Style.RESET_ALL).strip()
        if gradient_clip.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['gradient_clip'] = float(gradient_clip) if gradient_clip else 1.0
        
        grad_accum = input(Fore.YELLOW + Style.BRIGHT + "Gradient accumulation steps " + Fore.WHITE + Style.BRIGHT + "(1): " + Style.RESET_ALL).strip()
        if grad_accum.lower() == 'c':
            print(cancel_msg)
            return None
        training_config['gradient_accumulation_steps'] = int(grad_accum) if grad_accum else 1
        
        # Optimizer selection with enhanced UI
        print(Fore.GREEN + Style.BRIGHT + "\nOptimizer Selection:")
        optimizers = ['AdamW', 'Adam', 'SGD', 'RMSprop']
        for i, opt in enumerate(optimizers, 1):
            print(Fore.WHITE + Style.BRIGHT + f"{i}. {opt}")
        print(Fore.RED + Style.BRIGHT + f"{len(optimizers)+1}. Cancel and return to previous menu")
        
        while True:
            try:
                opt_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect optimizer (1-{len(optimizers)+1}): " + Style.RESET_ALL).strip()
                if opt_choice in [str(i) for i in range(1, len(optimizers)+2)]:
                    break
                print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(optimizers)+1}")
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nOptimizer selection cancelled")
                return None
        
        if opt_choice == str(len(optimizers)+1):
            print(Fore.RED + Style.BRIGHT + "\nOptimizer selection cancelled")
            return None
        
        optimizer = optimizers[int(opt_choice)-1]
        training_config['optimizer'] = optimizer
        
        # Optimizer-specific parameters
        if optimizer in ['AdamW', 'Adam']:
            adam_betas_input = input(Fore.YELLOW + Style.BRIGHT + "Adam betas " + Fore.WHITE + Style.BRIGHT + "(0.9,0.999): " + Style.RESET_ALL).strip()
            if adam_betas_input.lower() == 'c':
                print(Fore.RED + Style.BRIGHT + "\nOptimizer configuration cancelled")
                return None
            if adam_betas_input:
                betas = [float(x.strip()) for x in adam_betas_input.split(',')]
                training_config['adam_betas'] = tuple(betas)
            else:
                training_config['adam_betas'] = (0.9, 0.999)
            
            adam_eps = input(Fore.YELLOW + Style.BRIGHT + "Adam epsilon " + Fore.WHITE + Style.BRIGHT + "(1e-8): " + Style.RESET_ALL).strip()
            if adam_eps.lower() == 'c':
                print(Fore.RED + Style.BRIGHT + "\nOptimizer configuration cancelled")
                return None
            training_config['adam_eps'] = float(adam_eps) if adam_eps else 1e-8
        
        # Learning rate scheduler
        scheduler = input(Fore.YELLOW + Style.BRIGHT + "Use learning rate scheduler? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nScheduler configuration cancelled"
        
        if scheduler == 'c':
            print(cancel_msg)
            return None
        if scheduler in ('', 'y', 'yes'):
            print(Fore.GREEN + Style.BRIGHT + "\nScheduler Options:")
            scheduler_types = ['ReduceLROnPlateau', 'StepLR', 'CosineAnnealingLR', 'ExponentialLR']
            for i, sched in enumerate(scheduler_types, 1):
                print(Fore.WHITE + Style.BRIGHT + f"{i}. {sched}")
            print(Fore.RED + Style.BRIGHT + f"{len(scheduler_types)+1}. Cancel and return to previous menu")
            
            while True:
                try:
                    sched_choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect scheduler (1-{len(scheduler_types)+1}): " + Style.RESET_ALL).strip()
                    if sched_choice in [str(i) for i in range(1, len(scheduler_types)+2)]:
                        break
                    print(Fore.RED + Style.BRIGHT + f"\nPlease select 1-{len(scheduler_types)+1}")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nScheduler selection cancelled")
                    return None
            
            if sched_choice == str(len(scheduler_types)+1):
                print(Fore.RED + Style.BRIGHT + "\nScheduler selection cancelled")
                return None
            
            training_config['scheduler'] = scheduler_types[int(sched_choice)-1]
            
            # Scheduler-specific parameters
            if training_config['scheduler'] == 'ReduceLROnPlateau':
                lr_patience = input(Fore.YELLOW + Style.BRIGHT + "LR scheduler patience " + Fore.WHITE + Style.BRIGHT + "(5): " + Style.RESET_ALL).strip()
                if lr_patience == 'c':
                    print(cancel_msg)
                    return None
                lr_factor = input(Fore.YELLOW + Style.BRIGHT + "LR reduction factor " + Fore.WHITE + Style.BRIGHT + "(0.5): " + Style.RESET_ALL).strip()
                if lr_factor == 'c':
                    print(cancel_msg)
                    return None
                min_lr = input(Fore.YELLOW + Style.BRIGHT + "Minimum learning rate " + Fore.WHITE + Style.BRIGHT + "(1e-6): " + Style.RESET_ALL).strip()
                if min_lr == 'c':
                    print(cancel_msg)
                    return None
                
                scheduler_params = {
                    'patience': int(lr_patience) if lr_patience else 5,
                    'factor': float(lr_factor) if lr_factor else 0.5,
                    'min_lr': float(min_lr) if min_lr else 1e-6
                }
                training_config['scheduler_params'] = scheduler_params
            elif training_config['scheduler'] == 'StepLR':
                step_size = input(Fore.YELLOW + Style.BRIGHT + "Step size " + Fore.WHITE + Style.BRIGHT + "(30): " + Style.RESET_ALL).strip()
                if step_size == 'c':
                    print(cancel_msg)
                    return None
                gamma = input(Fore.YELLOW + Style.BRIGHT + "Gamma " + Fore.WHITE + Style.BRIGHT + "(0.1): " + Style.RESET_ALL).strip()
                if gamma == 'c':
                    print(cancel_msg)
                    return None
                
                scheduler_params = {
                    'step_size': int(step_size) if step_size else 30,
                    'gamma': float(gamma) if gamma else 0.1
                }
                training_config['scheduler_params'] = scheduler_params
            elif training_config['scheduler'] == 'CosineAnnealingLR':
                t_max = input(Fore.YELLOW + Style.BRIGHT + f"T_max " + Fore.WHITE + Style.BRIGHT + f"({training_config['epochs']}): " + Style.RESET_ALL).strip()
                if t_max == 'c':
                    print(cancel_msg)
                    return None
                eta_min = input(Fore.YELLOW + Style.BRIGHT + "Eta min " + Fore.WHITE + Style.BRIGHT + "(1e-7): " + Style.RESET_ALL).strip()
                if eta_min == 'c':
                    print(cancel_msg)
                    return None
                
                scheduler_params = {
                    'T_max': int(t_max) if t_max else training_config['epochs'],
                    'eta_min': float(eta_min) if eta_min else 1e-7
                }
                training_config['scheduler_params'] = scheduler_params
            elif training_config['scheduler'] == 'ExponentialLR':
                gamma = input(Fore.YELLOW + Style.BRIGHT + "Gamma " + Fore.WHITE + Style.BRIGHT + "(0.95): " + Style.RESET_ALL).strip()
                if gamma == 'c':
                    print(cancel_msg)
                    return None
                
                scheduler_params = {
                    'gamma': float(gamma) if gamma else 0.95
                }
                training_config['scheduler_params'] = scheduler_params
        else:
            training_config['scheduler'] = None
        
        # Additional training options
        early_stopping = input(Fore.YELLOW + Style.BRIGHT + "Enable early stopping? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nTraining configuration cancelled"
        
        if early_stopping == 'c':
            print(cancel_msg)
            return None
        training_config['early_stopping'] = early_stopping in ('', 'y', 'yes')
        
        shuffle = input(Fore.YELLOW + Style.BRIGHT + "Shuffle training data? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if shuffle == 'c':
            print(cancel_msg)
            return None
        training_config['shuffle'] = shuffle in ('', 'y', 'yes')
        
        # Mixed precision training
        if torch.cuda.is_available():
            mixed_prec = input(Fore.YELLOW + Style.BRIGHT + "Use mixed precision training? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if mixed_prec == 'c':
                print(cancel_msg)
                return None
            training_config['mixed_precision'] = mixed_prec in ('', 'y', 'yes')
        else:
            training_config['mixed_precision'] = False

        # SYSTEM & HARDWARE CONFIGURATION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "SYSTEM & HARDWARE CONFIGURATION")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        hardware_config = final_config.setdefault('hardware', {})
        system_config = final_config.setdefault('system', {})
        advanced_config = final_config.setdefault('advanced_training', {})
        
        device_choice = input(Fore.YELLOW + Style.BRIGHT + "\nDevice " + Fore.WHITE + Style.BRIGHT + "(auto/cpu/cuda): " + Style.RESET_ALL).strip()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nHardware configuration cancelled"
        
        if device_choice.lower() == 'c':
            print(cancel_msg)
            return None
        hardware_config['device'] = device_choice if device_choice else 'auto'
        
        cuda_opt = input(Fore.YELLOW + Style.BRIGHT + "Enable CUDA optimizations? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if cuda_opt == 'c':
            print(cancel_msg)
            return None
        hardware_config['cuda_optimizations'] = cuda_opt in ('', 'y', 'yes') and torch.cuda.is_available()
        
        memory_mgmt = input(Fore.YELLOW + Style.BRIGHT + "Enable memory management? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if memory_mgmt == 'c':
            print(cancel_msg)
            return None
        hardware_config['memory_management'] = {'enable_memory_efficient': memory_mgmt in ('', 'y', 'yes')}
        
        gpu_memory = input(Fore.YELLOW + Style.BRIGHT + "Recommended GPU memory GB " + Fore.WHITE + Style.BRIGHT + "(4.0): " + Style.RESET_ALL).strip()
        if gpu_memory.lower() == 'c':
            print(cancel_msg)
            return None
        hardware_config['recommended_gpu_memory'] = float(gpu_memory) if gpu_memory else 4.0
        
        perf_class = input(Fore.YELLOW + Style.BRIGHT + "System performance class " + Fore.WHITE + Style.BRIGHT + "(auto/low/medium/high): " + Style.RESET_ALL).strip()
        if perf_class.lower() == 'c':
            print(cancel_msg)
            return None
        hardware_config['system_performance_class'] = perf_class if perf_class else 'auto'
        
        # Advanced training configuration
        num_workers = input(Fore.YELLOW + Style.BRIGHT + "Data loader workers " + Fore.WHITE + Style.BRIGHT + "(4): " + Style.RESET_ALL).strip()
        if num_workers.lower() == 'c':
            print(cancel_msg)
            return None
        advanced_config['num_workers'] = int(num_workers) if num_workers else 4
        
        pin_memory = input(Fore.YELLOW + Style.BRIGHT + "Pin memory for GPU? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if pin_memory == 'c':
            print(cancel_msg)
            return None
        advanced_config['pin_memory'] = pin_memory in ('', 'y', 'yes') and torch.cuda.is_available()
        
        persistent_workers = input(Fore.YELLOW + Style.BRIGHT + "Use persistent workers? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if persistent_workers == 'c':
            print(cancel_msg)
            return None
        advanced_config['persistent_workers'] = persistent_workers in ('', 'y', 'yes')
        
        memory_efficient = input(Fore.YELLOW + Style.BRIGHT + "Enable memory efficient training? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if memory_efficient == 'c':
            print(cancel_msg)
            return None
        advanced_config['memory_efficient'] = memory_efficient in ('', 'y', 'yes')
        
        compile_model = input(Fore.YELLOW + Style.BRIGHT + "Compile model with torch.compile? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if compile_model == 'c':
            print(cancel_msg)
            return None
        advanced_config['compile_model'] = compile_model in ('y', 'yes')
        
        benchmark_mode = input(Fore.YELLOW + Style.BRIGHT + "Enable benchmark mode? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if benchmark_mode == 'c':
            print(cancel_msg)
            return None
        advanced_config['benchmark_mode'] = benchmark_mode in ('y', 'yes')
        
        gradient_checkpointing = input(Fore.YELLOW + Style.BRIGHT + "Use gradient checkpointing? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if gradient_checkpointing == 'c':
            print(cancel_msg)
            return None
        advanced_config['gradient_checkpointing'] = gradient_checkpointing in ('y', 'yes')
        
        # System configuration
        parallel_processing = input(Fore.YELLOW + Style.BRIGHT + "Enable parallel processing? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nSystem configuration cancelled"
        
        if parallel_processing == 'c':
            print(cancel_msg)
            return None
        system_config['parallel_processing'] = parallel_processing in ('y', 'yes')
        
        max_workers = input(Fore.YELLOW + Style.BRIGHT + "Maximum workers " + Fore.WHITE + Style.BRIGHT + "(4): " + Style.RESET_ALL).strip()
        if max_workers.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['max_workers'] = int(max_workers) if max_workers else 4
        
        distributed_training = input(Fore.YELLOW + Style.BRIGHT + "Enable distributed training? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if distributed_training == 'c':
            print(cancel_msg)
            return None
        system_config['distributed_training'] = distributed_training in ('y', 'yes')

        # MONITORING & LOGGING CONFIGURATION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "MONITORING & LOGGING")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        monitoring_config = final_config.setdefault('monitoring', {})
        
        tensorboard = input(Fore.YELLOW + Style.BRIGHT + "\nEnable TensorBoard logging? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nMonitoring configuration cancelled"
        
        if tensorboard == 'c':
            print(cancel_msg)
            return None
        monitoring_config['tensorboard_logging'] = tensorboard in ('', 'y', 'yes')
        
        verbose = input(Fore.YELLOW + Style.BRIGHT + "Verbose output? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if verbose == 'c':
            print(cancel_msg)
            return None
        monitoring_config['verbose'] = verbose in ('', 'y', 'yes')
        
        debug = input(Fore.YELLOW + Style.BRIGHT + "Enable debug mode? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if debug == 'c':
            print(cancel_msg)
            return None
        system_config['debug'] = debug in ('y', 'yes')
        
        checkpoints = input(Fore.YELLOW + Style.BRIGHT + "Save training checkpoints? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if checkpoints == 'c':
            print(cancel_msg)
            return None
        monitoring_config['save_checkpoints'] = checkpoints in ('', 'y', 'yes')
        
        if monitoring_config['save_checkpoints']:
            checkpoint_freq = input(Fore.YELLOW + Style.BRIGHT + "Checkpoint frequency in epochs " + Fore.WHITE + Style.BRIGHT + "(10): " + Style.RESET_ALL).strip()
            if checkpoint_freq.lower() == 'c':
                print(cancel_msg)
                return None
            monitoring_config['checkpoint_frequency'] = int(checkpoint_freq) if checkpoint_freq else 10
        
        save_best = input(Fore.YELLOW + Style.BRIGHT + "Save best model? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if save_best == 'c':
            print(cancel_msg)
            return None
        monitoring_config['save_best_model'] = save_best in ('', 'y', 'yes')
        
        save_history = input(Fore.YELLOW + Style.BRIGHT + "Save model history? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if save_history == 'c':
            print(cancel_msg)
            return None
        monitoring_config['save_model_history'] = save_history in ('', 'y', 'yes')
        
        log_freq = input(Fore.YELLOW + Style.BRIGHT + "Log frequency " + Fore.WHITE + Style.BRIGHT + "(1): " + Style.RESET_ALL).strip()
        if log_freq.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['log_frequency'] = int(log_freq) if log_freq else 1
        
        metrics_freq = input(Fore.YELLOW + Style.BRIGHT + "Metrics frequency " + Fore.WHITE + Style.BRIGHT + "(1): " + Style.RESET_ALL).strip()
        if metrics_freq.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['metrics_frequency'] = int(metrics_freq) if metrics_freq else 1
        
        print(Fore.GREEN + Style.BRIGHT + "\nMetrics to track (space-separated):")
        print(Fore.WHITE + Style.BRIGHT + "Available: loss, reconstruction_error, anomaly_detection_rate, mse_statistics")
        metrics_input = input(Fore.YELLOW + Style.BRIGHT + "Metrics " + Fore.WHITE + Style.BRIGHT + "(default: loss reconstruction_error): " + Style.RESET_ALL).strip()
        if metrics_input.lower() == 'c':
            print(cancel_msg)
            return None
        if metrics_input:
            monitoring_config['metrics_to_track'] = metrics_input.split()
        else:
            monitoring_config['metrics_to_track'] = ['loss', 'reconstruction_error']
        
        console_level = input(Fore.YELLOW + Style.BRIGHT + "Console logging level " + Fore.WHITE + Style.BRIGHT + "(INFO/DEBUG/WARNING): " + Style.RESET_ALL).strip()
        if console_level.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['console_logging_level'] = console_level if console_level else 'INFO'
        
        early_metric = input(Fore.YELLOW + Style.BRIGHT + "Early stopping metric " + Fore.WHITE + Style.BRIGHT + "(val_loss): " + Style.RESET_ALL).strip()
        if early_metric.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['early_stopping_metric'] = early_metric if early_metric else 'val_loss'
        
        checkpoint_format = input(Fore.YELLOW + Style.BRIGHT + "Checkpoint format " + Fore.WHITE + Style.BRIGHT + "(pth/pt): " + Style.RESET_ALL).strip()
        if checkpoint_format.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['checkpoint_format'] = checkpoint_format if checkpoint_format else 'pth'
        
        log_summary = input(Fore.YELLOW + Style.BRIGHT + "Log model summary? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if log_summary == 'c':
            print(cancel_msg)
            return None
        monitoring_config['log_model_summary'] = log_summary in ('', 'y', 'yes')
        
        tensorboard_dir = input(Fore.YELLOW + Style.BRIGHT + f"TensorBoard directory " + Fore.WHITE + Style.BRIGHT + f"({TB_DIR}): " + Style.RESET_ALL).strip()
        if tensorboard_dir.lower() == 'c':
            print(cancel_msg)
            return None
        monitoring_config['tensorboard_dir'] = tensorboard_dir if tensorboard_dir else TB_DIR
        
        stability_metrics = input(Fore.YELLOW + Style.BRIGHT + "Track stability metrics? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if stability_metrics == 'c':
            print(cancel_msg)
            return None
        monitoring_config['stability_metrics'] = stability_metrics in ('', 'y', 'yes')
        
        performance_metrics = input(Fore.YELLOW + Style.BRIGHT + "Track performance metrics? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if performance_metrics == 'c':
            print(cancel_msg)
            return None
        monitoring_config['performance_metrics'] = performance_metrics in ('', 'y', 'yes')
        
        profiling = input(Fore.YELLOW + Style.BRIGHT + "Enable profiling? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if profiling == 'c':
            print(cancel_msg)
            return None
        monitoring_config['profiling_enabled'] = profiling in ('y', 'yes')

        # SECURITY & ANOMALY DETECTION CONFIGURATION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "SECURITY & ANOMALY DETECTION")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        security_config = final_config.setdefault('security', {})
        
        percentile = input(Fore.YELLOW + Style.BRIGHT + "\nAnomaly threshold percentile " + Fore.WHITE + Style.BRIGHT + "(95.0): " + Style.RESET_ALL).strip()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nSecurity configuration cancelled"
        
        if percentile.lower() == 'c':
            print(cancel_msg)
            return None
        security_config['percentile'] = float(percentile) if percentile else 95.0
        
        threshold_method = input(Fore.YELLOW + Style.BRIGHT + "Threshold calculation method " + Fore.WHITE + Style.BRIGHT + "(percentile/adaptive): " + Style.RESET_ALL).strip()
        if threshold_method.lower() == 'c':
            print(cancel_msg)
            return None
        security_config['anomaly_threshold_strategy'] = threshold_method if threshold_method else 'percentile'
        
        enable_security = input(Fore.YELLOW + Style.BRIGHT + "Enable security metrics? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if enable_security == 'c':
            print(cancel_msg)
            return None
        security_config['enable_security_metrics'] = enable_security in ('', 'y', 'yes')
        
        adaptive_thresh = input(Fore.YELLOW + Style.BRIGHT + "Use adaptive thresholding? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if adaptive_thresh == 'c':
            print(cancel_msg)
            return None
        security_config['adaptive_threshold'] = adaptive_thresh in ('', 'y', 'yes')
        
        attack_thresh = input(Fore.YELLOW + Style.BRIGHT + "Attack threshold " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        if attack_thresh.lower() == 'c':
            print(cancel_msg)
            return None
        if attack_thresh:
            security_config['attack_threshold'] = float(attack_thresh)
        
        false_neg_cost = input(Fore.YELLOW + Style.BRIGHT + "False negative cost " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        if false_neg_cost.lower() == 'c':
            print(cancel_msg)
            return None
        if false_neg_cost:
            security_config['false_negative_cost'] = float(false_neg_cost)
        
        early_warning = input(Fore.YELLOW + Style.BRIGHT + "Early warning threshold " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        if early_warning.lower() == 'c':
            print(cancel_msg)
            return None
        if early_warning:
            security_config['early_warning_threshold'] = float(early_warning)
        
        confidence_interval = input(Fore.YELLOW + Style.BRIGHT + "Confidence interval " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        if confidence_interval.lower() == 'c':
            print(cancel_msg)
            return None
        if confidence_interval:
            security_config['confidence_interval'] = float(confidence_interval)
        
        detection_methods = input(Fore.YELLOW + Style.BRIGHT + "Detection methods " + Fore.WHITE + Style.BRIGHT + "(space-separated, optional): " + Style.RESET_ALL).strip()
        if detection_methods.lower() == 'c':
            print(cancel_msg)
            return None
        if detection_methods:
            security_config['detection_methods'] = detection_methods.split()
        
        alert_levels = input(Fore.YELLOW + Style.BRIGHT + "Alert levels " + Fore.WHITE + Style.BRIGHT + "(space-separated, optional): " + Style.RESET_ALL).strip()
        if alert_levels.lower() == 'c':
            print(cancel_msg)
            return None
        if alert_levels:
            security_config['alert_levels'] = alert_levels.split()
        
        threshold_validation = input(Fore.YELLOW + Style.BRIGHT + "Enable threshold validation? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if threshold_validation == 'c':
            print(cancel_msg)
            return None
        security_config['threshold_validation'] = threshold_validation in ('', 'y', 'yes')
        
        robust_detection = input(Fore.YELLOW + Style.BRIGHT + "Enable robust detection? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if robust_detection == 'c':
            print(cancel_msg)
            return None
        security_config['robust_detection'] = robust_detection in ('', 'y', 'yes')
        
        fp_tolerance = input(Fore.YELLOW + Style.BRIGHT + "False positive tolerance " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        if fp_tolerance.lower() == 'c':
            print(cancel_msg)
            return None
        if fp_tolerance:
            security_config['false_positive_tolerance'] = float(fp_tolerance)
        
        perf_optimized = input(Fore.YELLOW + Style.BRIGHT + "Performance optimized detection? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if perf_optimized == 'c':
            print(cancel_msg)
            return None
        security_config['performance_optimized_detection'] = perf_optimized in ('', 'y', 'yes')
        
        real_time = input(Fore.YELLOW + Style.BRIGHT + "Real-time monitoring? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if real_time == 'c':
            print(cancel_msg)
            return None
        security_config['real_time_monitoring'] = real_time in ('y', 'yes')
        
        if model_type == 'AutoencoderEnsemble':
            voting = input(Fore.YELLOW + Style.BRIGHT + "Ensemble voting method " + Fore.WHITE + Style.BRIGHT + "(average/majority/weighted): " + Style.RESET_ALL).strip()
            if voting.lower() == 'c':
                print(cancel_msg)
                return None
            security_config['ensemble_voting'] = voting if voting else 'average'
            
            uncertainty = input(Fore.YELLOW + Style.BRIGHT + "Uncertainty threshold " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
            if uncertainty.lower() == 'c':
                print(cancel_msg)
                return None
            if uncertainty:
                security_config['uncertainty_threshold'] = float(uncertainty)

        # VALIDATION & TESTING CONFIGURATION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "VALIDATION & TESTING")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        validation_config = final_config.setdefault('validation', {})
        
        detailed_metrics = input(Fore.YELLOW + Style.BRIGHT + "\nCalculate detailed metrics? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nValidation configuration cancelled"
        
        if detailed_metrics == 'c':
            print(cancel_msg)
            return None
        validation_config['detailed_metrics'] = detailed_metrics in ('', 'y', 'yes')
        
        cross_validation = input(Fore.YELLOW + Style.BRIGHT + "Enable cross-validation? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if cross_validation == 'c':
            print(cancel_msg)
            return None
        if cross_validation in ('y', 'yes'):
            cv_folds = input(Fore.YELLOW + Style.BRIGHT + "Number of CV folds " + Fore.WHITE + Style.BRIGHT + "(5): " + Style.RESET_ALL).strip()
            if cv_folds.lower() == 'c':
                print(cancel_msg)
                return None
            validation_config['cross_validation'] = {
                'enabled': True,
                'folds': int(cv_folds) if cv_folds else 5
            }
        else:
            validation_config['cross_validation'] = {'enabled': False}
        
        val_freq = input(Fore.YELLOW + Style.BRIGHT + "Validation frequency " + Fore.WHITE + Style.BRIGHT + "(1): " + Style.RESET_ALL).strip()
        if val_freq.lower() == 'c':
            print(cancel_msg)
            return None
        validation_config['validation_frequency'] = int(val_freq) if val_freq else 1
        
        save_val_results = input(Fore.YELLOW + Style.BRIGHT + "Save validation results? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if save_val_results == 'c':
            print(cancel_msg)
            return None
        validation_config['save_validation_results'] = save_val_results in ('', 'y', 'yes')
        
        robustness = input(Fore.YELLOW + Style.BRIGHT + "Enable robustness testing? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if robustness == 'c':
            print(cancel_msg)
            return None
        validation_config['robustness_testing'] = robustness in ('y', 'yes')
        
        benchmarking = input(Fore.YELLOW + Style.BRIGHT + "Enable performance benchmarking? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if benchmarking == 'c':
            print(cancel_msg)
            return None
        validation_config['performance_benchmarking'] = benchmarking in ('y', 'yes')
        
        confidence_intervals = input(Fore.YELLOW + Style.BRIGHT + "Calculate confidence intervals? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if confidence_intervals == 'c':
            print(cancel_msg)
            return None
        validation_config['confidence_intervals'] = confidence_intervals in ('y', 'yes')
        
        validation_metrics = input(Fore.YELLOW + Style.BRIGHT + "Validation metrics " + Fore.WHITE + Style.BRIGHT + "(space-separated, default: loss reconstruction_error): " + Style.RESET_ALL).strip()
        if validation_metrics.lower() == 'c':
            print(cancel_msg)
            return None
        if validation_metrics:
            validation_config['metrics'] = validation_metrics.split()
        else:
            validation_config['metrics'] = ['loss', 'reconstruction_error']

        # EXPORT & SAVING CONFIGURATION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "EXPORT & SAVING")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        export_config = final_config.setdefault('export', {})
        
        save_model = input(Fore.YELLOW + Style.BRIGHT + "\nSave trained model? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nExport configuration cancelled"
        
        if save_model == 'c':
            print(cancel_msg)
            return None
        export_config['save_model'] = save_model in ('', 'y', 'yes')
        
        save_metadata = input(Fore.YELLOW + Style.BRIGHT + "Save training metadata? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if save_metadata == 'c':
            print(cancel_msg)
            return None
        export_config['save_metadata'] = save_metadata in ('', 'y', 'yes')
        
        save_history = input(Fore.YELLOW + Style.BRIGHT + "Save training history? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if save_history == 'c':
            print(cancel_msg)
            return None
        export_config['save_training_history'] = save_history in ('', 'y', 'yes')
        
        export_onnx = input(Fore.YELLOW + Style.BRIGHT + "Export to ONNX format? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if export_onnx == 'c':
            print(cancel_msg)
            return None
        export_config['export_onnx'] = export_onnx in ('y', 'yes')
        
        model_dir = input(Fore.YELLOW + Style.BRIGHT + f"Model directory " + Fore.WHITE + f"({DEFAULT_MODEL_DIR}): " + Style.RESET_ALL).strip()
        if model_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['model_dir'] = model_dir if model_dir else DEFAULT_MODEL_DIR
        
        log_dir = input(Fore.YELLOW + Style.BRIGHT + f"Log directory " + Fore.WHITE + f"({LOG_DIR}): " + Style.RESET_ALL).strip()
        if log_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['log_dir'] = log_dir if log_dir else LOG_DIR
        
        config_dir = input(Fore.YELLOW + Style.BRIGHT + f"Config directory " + Fore.WHITE + f"({DEFAULT_MODEL_DIR}): " + Style.RESET_ALL).strip()
        if config_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['config_dir'] = config_dir if config_dir else DEFAULT_MODEL_DIR
        
        data_dir = input(Fore.YELLOW + Style.BRIGHT + f"Data directory " + Fore.WHITE + f"({DEFAULT_MODEL_DIR}): " + Style.RESET_ALL).strip()
        if data_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['data_dir'] = data_dir if data_dir else DEFAULT_MODEL_DIR
        
        checkpoint_dir = input(Fore.YELLOW + Style.BRIGHT + f"Checkpoint directory " + Fore.WHITE + f"({DEFAULT_MODEL_DIR}): " + Style.RESET_ALL).strip()
        if checkpoint_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['checkpoint_dir'] = checkpoint_dir if checkpoint_dir else DEFAULT_MODEL_DIR

        # REPRODUCIBILITY & SYSTEM CONFIGURATION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "REPRODUCIBILITY & SYSTEM")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        reproducible = input(Fore.YELLOW + Style.BRIGHT + "\nEnable reproducible training? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nSystem configuration cancelled"
        
        if reproducible == 'c':
            print(cancel_msg)
            return None
        system_config['reproducible'] = reproducible in ('', 'y', 'yes')
        
        if system_config['reproducible']:
            random_seed = input(Fore.YELLOW + Style.BRIGHT + "Random seed " + Fore.WHITE + Style.BRIGHT + "(42): " + Style.RESET_ALL).strip()
            if random_seed.lower() == 'c':
                print(cancel_msg)
                return None
            system_config['random_seed'] = int(random_seed) if random_seed else 42
        
        non_interactive = input(Fore.YELLOW + Style.BRIGHT + "Set non-interactive mode? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if non_interactive == 'c':
            print(cancel_msg)
            return None
        system_config['non_interactive'] = non_interactive in ('y', 'yes')
        
        python_exec = input(Fore.YELLOW + Style.BRIGHT + f"Python executable " + Fore.WHITE + f"({sys.executable}): " + Style.RESET_ALL).strip()
        if python_exec.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['python_executable'] = python_exec if python_exec else sys.executable
        
        work_dir = input(Fore.YELLOW + Style.BRIGHT + f"Working directory " + Fore.WHITE + f"({os.getcwd()}): " + Style.RESET_ALL).strip()
        if work_dir.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['working_directory'] = work_dir if work_dir else os.getcwd()
        
        env_health = input(Fore.YELLOW + Style.BRIGHT + "Environment health check " + Fore.WHITE + Style.BRIGHT + "(auto/skip): " + Style.RESET_ALL).strip()
        if env_health.lower() == 'c':
            print(cancel_msg)
            return None
        system_config['environment_health'] = env_health if env_health else 'auto'

        # HYPERPARAMETER OPTIMIZATION CONFIGURATION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        
        hpo_enabled = input(Fore.YELLOW + Style.BRIGHT + "\nEnable hyperparameter optimization? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nHPO configuration cancelled"
        
        if hpo_enabled == 'c':
            print(cancel_msg)
            return None
        hpo_config['enabled'] = hpo_enabled in ('y', 'yes')
        
        if hpo_config['enabled']:
            hpo_strategy = input(Fore.YELLOW + Style.BRIGHT + "HPO strategy " + Fore.WHITE + Style.BRIGHT + "(optuna/hyperopt/random): " + Style.RESET_ALL).strip()
            if hpo_strategy.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['strategy'] = hpo_strategy if hpo_strategy else 'optuna'
            
            study_name = input(Fore.YELLOW + Style.BRIGHT + "Study name " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
            if study_name.lower() == 'c':
                print(cancel_msg)
                return None
            if study_name:
                hpo_config['study_name'] = study_name
            
            direction = input(Fore.YELLOW + Style.BRIGHT + "Optimization direction " + Fore.WHITE + Style.BRIGHT + "(minimize/maximize): " + Style.RESET_ALL).strip()
            if direction.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['direction'] = direction if direction else 'minimize'
            
            n_trials = input(Fore.YELLOW + Style.BRIGHT + "Number of trials " + Fore.WHITE + Style.BRIGHT + "(100): " + Style.RESET_ALL).strip()
            if n_trials.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['n_trials'] = int(n_trials) if n_trials else 100
            
            timeout_input = input(Fore.YELLOW + Style.BRIGHT + "Timeout in seconds " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
            if timeout_input.lower() == 'c':
                print(cancel_msg)
                return None
            if timeout_input:
                hpo_config['timeout'] = int(timeout_input)
            
            sampler = input(Fore.YELLOW + Style.BRIGHT + "Sampler " + Fore.WHITE + Style.BRIGHT + "(TPE/Random/Grid): " + Style.RESET_ALL).strip()
            if sampler.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['sampler'] = sampler if sampler else 'TPE'
            
            pruner = input(Fore.YELLOW + Style.BRIGHT + "Pruner " + Fore.WHITE + Style.BRIGHT + "(MedianPruner/HyperbandPruner/None): " + Style.RESET_ALL).strip()
            if pruner.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['pruner'] = pruner if pruner else 'MedianPruner'
            
            objective_metric = input(Fore.YELLOW + Style.BRIGHT + "Objective metric " + Fore.WHITE + Style.BRIGHT + "(val_loss): " + Style.RESET_ALL).strip()
            if objective_metric.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['objective_metric'] = objective_metric if objective_metric else 'val_loss'
            
            trial_epochs = input(Fore.YELLOW + Style.BRIGHT + "Trial epochs " + Fore.WHITE + Style.BRIGHT + "(10): " + Style.RESET_ALL).strip()
            if trial_epochs.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['trial_epochs'] = int(trial_epochs) if trial_epochs else 10
            
            trial_patience = input(Fore.YELLOW + Style.BRIGHT + "Trial patience " + Fore.WHITE + Style.BRIGHT + "(5): " + Style.RESET_ALL).strip()
            if trial_patience.lower() == 'c':
                print(cancel_msg)
                return None
            hpo_config['trial_patience'] = int(trial_patience) if trial_patience else 5
            
            cleanup_trials = input(Fore.YELLOW + Style.BRIGHT + "Cleanup trials after completion? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if cleanup_trials == 'c':
                print(cancel_msg)
                return None
            hpo_config['cleanup_trials'] = cleanup_trials in ('', 'y', 'yes')
            
            generate_plots = input(Fore.YELLOW + Style.BRIGHT + "Generate optimization plots? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
            if generate_plots == 'c':
                print(cancel_msg)
                return None
            hpo_config['generate_plots'] = generate_plots in ('', 'y', 'yes')

        # PRESETS & ADVANCED CONFIGURATION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "PRESETS & ADVANCED")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        presets_config = final_config.setdefault('presets', {})
        
        current_preset = input(Fore.YELLOW + Style.BRIGHT + "Current preset name " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nPresets configuration cancelled"
        
        if current_preset.lower() == 'c':
            print(cancel_msg)
            return None
        if current_preset:
            presets_config['current_preset'] = current_preset
        
        auto_apply = input(Fore.YELLOW + Style.BRIGHT + "Auto-apply compatible presets? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if auto_apply == 'c':
            print(cancel_msg)
            return None
        presets_config['auto_apply'] = auto_apply in ('y', 'yes')
        
        validate_compatibility = input(Fore.YELLOW + Style.BRIGHT + "Validate preset compatibility? " + Fore.WHITE + Style.BRIGHT + "(Y/n): " + Style.RESET_ALL).strip().lower()
        if validate_compatibility == 'c':
            print(cancel_msg)
            return None
        presets_config['validate_compatibility'] = validate_compatibility in ('', 'y', 'yes')
        
        experimental_config = final_config.setdefault('experimental', {})
        
        experimental_features = input(Fore.YELLOW + Style.BRIGHT + "\nEnable experimental features? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nExperimental configuration cancelled"
        
        if experimental_features == 'c':
            print(cancel_msg)
            return None
        if experimental_features in ('y', 'yes'):
            print(Fore.GREEN + Style.BRIGHT + "\nAvailable experimental features:")
            print(Fore.WHITE + Style.BRIGHT + "1. Advanced attention mechanisms")
            print(Fore.WHITE + Style.BRIGHT + "2. Dynamic architecture adjustment")
            print(Fore.WHITE + Style.BRIGHT + "3. Adaptive learning rates")
            print(Fore.WHITE + Style.BRIGHT + "4. Novel regularization techniques")
            
            exp_features_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect features " + Fore.WHITE + Style.BRIGHT + "(space-separated numbers, optional): " + Style.RESET_ALL).strip()
            if exp_features_input.lower() == 'c':
                print(cancel_msg)
                return None
            if exp_features_input:
                feature_map = {
                    '1': 'advanced_attention',
                    '2': 'dynamic_architecture',
                    '3': 'adaptive_lr',
                    '4': 'novel_regularization'
                }
                selected_features = {}
                for num in exp_features_input.split():
                    if num in feature_map:
                        selected_features[feature_map[num]] = True
                experimental_config['experimental_features'] = selected_features
        
        auto_optimize = input(Fore.YELLOW + Style.BRIGHT + "Enable automatic optimization? " + Fore.WHITE + Style.BRIGHT + "(y/N): " + Style.RESET_ALL).strip().lower()
        if auto_optimize == 'c':
            print(cancel_msg)
            return None
        experimental_config['auto_optimize'] = auto_optimize in ('y', 'yes')

        # METADATA & DOCUMENTATION CONFIGURATION
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "METADATA & DOCUMENTATION")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        
        metadata_config = final_config.setdefault('metadata', {})
        
        description = input(Fore.YELLOW + Style.BRIGHT + "Experiment description " + Fore.WHITE + Style.BRIGHT + "(optional): " + Style.RESET_ALL).strip()
        cancel_msg = Fore.RED + Style.BRIGHT + "\nMetadata configuration cancelled"
        
        if description.lower() == 'c':
            print(cancel_msg)
            return None
        if description:
            metadata_config['description'] = description
        
        version = input(Fore.YELLOW + Style.BRIGHT + "Configuration version " + Fore.WHITE + Style.BRIGHT + "(1.0): " + Style.RESET_ALL).strip()
        if version.lower() == 'c':
            print(cancel_msg)
            return None
        metadata_config['version'] = version if version else '1.0'
        
        config_version = input(Fore.YELLOW + Style.BRIGHT + "Config format version " + Fore.WHITE + Style.BRIGHT + "(1.0): " + Style.RESET_ALL).strip()
        if config_version.lower() == 'c':
            print(cancel_msg)
            return None
        metadata_config['config_version'] = config_version if config_version else '1.0'
        
        config_type = input(Fore.YELLOW + Style.BRIGHT + "Configuration type " + Fore.WHITE + Style.BRIGHT + "(custom): " + Style.RESET_ALL).strip()
        if config_type.lower() == 'c':
            print(cancel_msg)
            return None
        metadata_config['config_type'] = config_type if config_type else 'custom'
        
        metadata_config['created'] = datetime.now().isoformat()
        metadata_config['last_modified'] = datetime.now().isoformat()
        
        if final_config.get('presets', {}).get('current_preset'):
            metadata_config['preset_used'] = final_config['presets']['current_preset']
        
        compatibility = input(Fore.YELLOW + Style.BRIGHT + "Compatibility tags " + Fore.WHITE + Style.BRIGHT + "(space-separated, optional): " + Style.RESET_ALL).strip()
        if compatibility.lower() == 'c':
            print(cancel_msg)
            return None
        if compatibility:
            metadata_config['compatibility'] = compatibility.split()
        else:
            metadata_config['compatibility'] = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
        
        runtime_config = final_config.setdefault('runtime', {})
        runtime_config.update({
            'config_loaded_at': datetime.now().isoformat(),
            'config_source': 'interactive_custom',
            'runtime_id': f"custom_{int(time.time())}",
            'process_id': os.getpid(),
            'system_analysis_completed': False,
            'system_performance_score': None,
            'system_class': hardware_config.get('system_performance_class', 'auto'),
            'optimizations_applied': {},
            'resource_status': {},
            'system_warnings': [],
            'recommendations': [],
            'configuration_health': {}
        })

        # FINAL CONFIGURATION REVIEW
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*60)
        print(Fore.CYAN + Style.BRIGHT + "CONFIGURATION REVIEW")
        print(Fore.YELLOW + Style.BRIGHT + "-"*60)
        
        # Comprehensive configuration summary
        estimated_time = _estimate_training_time(
            training_config.get('epochs', 50), 
            model_type,
            training_config.get('batch_size', 64)
        )
        
        print(Fore.YELLOW + Style.BRIGHT + f"\nConfiguration Summary:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + Style.BRIGHT + f"{'Real Data' if use_real_data else 'Synthetic Data'}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model: " + Fore.GREEN + Style.BRIGHT + f"{model_type}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('epochs', 50)} epochs")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('batch_size', 64)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('learning_rate', 0.001)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Optimizer: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('optimizer', 'AdamW')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Scheduler: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('scheduler', 'None')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Mixed Precision: " + Fore.GREEN + Style.BRIGHT + f"{training_config.get('mixed_precision', False)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Device: " + Fore.GREEN + Style.BRIGHT + f"{hardware_config.get('device', 'auto')}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + Style.BRIGHT + f"~{estimated_time} minutes")
        
        print(Fore.YELLOW + Style.BRIGHT + f"\nArchitecture Details:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Encoding Dim: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('encoding_dim')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Hidden Dims: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('hidden_dims')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Dropout Rates: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('dropout_rates')}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Activation: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('activation')}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Normalization: " + Fore.GREEN + Style.BRIGHT + f"{model_config.get('normalization')}")
        
        # Enhanced features display
        if model_type in ['EnhancedAutoencoder', 'AutoencoderEnsemble']:
            enhanced_features = []
            if model_config.get('use_attention'):
                enhanced_features.append("Attention")
            if model_config.get('residual_blocks'):
                enhanced_features.append("Residual Blocks")
            if model_config.get('skip_connection'):
                enhanced_features.append("Skip Connections")
            
            if enhanced_features:
                print(Fore.MAGENTA + Style.BRIGHT + f"\n   Enhanced Features:")
                print(Fore.GREEN + Style.BRIGHT + f"     └─ Enhanced Features: " + Fore.CYAN + Style.BRIGHT + f"{', '.join(enhanced_features)}")
        
        if model_type == 'AutoencoderEnsemble':
            print(Fore.MAGENTA + Style.BRIGHT + f"\n   Ensemble Configuration:")
            print(Fore.GREEN + Style.BRIGHT + f"     ├─ Ensemble Size: " + Fore.CYAN + Style.BRIGHT + f"{model_config.get('num_models', 3)} models")
            print(Fore.GREEN + Style.BRIGHT + f"     └─ Diversity Factor: " + Fore.CYAN + Style.BRIGHT + f"{model_config.get('diversity_factor', 0.3)}")
        
        # Security configuration display
        print(Fore.YELLOW + Style.BRIGHT + f"\nSecurity Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Anomaly Threshold: " + Fore.GREEN + Style.BRIGHT + f"{security_config.get('percentile', 95.0)}th percentile")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Threshold Method: " + Fore.GREEN + Style.BRIGHT + f"{security_config.get('anomaly_threshold_strategy', 'percentile')}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Adaptive Threshold: " + Fore.GREEN + Style.BRIGHT + f"{security_config.get('adaptive_threshold', True)}")
        
        # System configuration display
        print(Fore.YELLOW + Style.BRIGHT + f"\nSystem Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Reproducible: " + Fore.GREEN + Style.BRIGHT + f"{system_config.get('reproducible', True)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Workers: " + Fore.GREEN + Style.BRIGHT + f"{advanced_config.get('num_workers', 4)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Directory: " + Fore.GREEN + Style.BRIGHT + f"{system_config.get('model_dir', DEFAULT_MODEL_DIR)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Config Directory: " + Fore.GREEN + Style.BRIGHT + f"{system_config.get('config_dir', CONFIG_DIR)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Log Directory: " + Fore.GREEN + Style.BRIGHT + f"{system_config.get('log_dir', LOG_DIR)}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Tensorboard Directory: " + Fore.GREEN + Style.BRIGHT + f"{system_config.get('tb_dir', TB_DIR)}")
        
        # Monitoring configuration display
        print(Fore.YELLOW + Style.BRIGHT + f"\nMonitoring Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ TensorBoard: " + Fore.GREEN + Style.BRIGHT + f"{monitoring_config.get('tensorboard_logging', True)}")
        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Checkpoints: " + Fore.GREEN + Style.BRIGHT + f"{monitoring_config.get('save_checkpoints', True)}")
        print(Fore.CYAN + Style.BRIGHT + f"  └─ Metrics: " + Fore.GREEN + Style.BRIGHT + f"{monitoring_config.get('metrics_to_track', ['loss', 'reconstruction_error'])}")
        
        # HPO configuration display
        if hpo_config.get('enabled'):
            print(Fore.YELLOW + Style.BRIGHT + f"\nHPO Configuration:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Strategy: " + Fore.GREEN + Style.BRIGHT + f"{hpo_config.get('strategy', 'optuna')}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Trials: " + Fore.GREEN + Style.BRIGHT + f"{hpo_config.get('n_trials', 100)}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Objective: " + Fore.GREEN + Style.BRIGHT + f"{hpo_config.get('objective_metric', 'val_loss')}")
        
        # Final confirmation with enhanced options
        #print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*60)
        confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart training with this configuration? " + Fore.WHITE + Style.BRIGHT + "(Y/n/c to cancel): " + Style.RESET_ALL).strip().lower()
        
        if confirm in ('', 'y', 'yes'):
            console.clear()
            
            launch_panel = Panel.fit(
                "Launching training with custom configuration...",
                style="bold green",
                border_style="green",
                padding=(1, 2),
                box=box.ROUNDED
            )
            # Small delay to ensure panel is rendered before proceeding
            time.sleep(3)
            console.clear()
            
            return _launch_training_with_config(final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print(Fore.RED + Style.BRIGHT + "\nTraining cancelled by user")
            return None
        else:
            # Enhanced fallback options matching other functions
            print(Fore.YELLOW + Style.BRIGHT + "\nWould you like to:")
            print(Fore.WHITE + Style.BRIGHT + "1. Try again with different settings")
            print(Fore.WHITE + Style.BRIGHT + "2. Return to training menu")
            print(Fore.RED + Style.BRIGHT + "0. Cancel completely")
            
            while True:
                try:
                    retry_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-2): " + Style.RESET_ALL).strip()
                    if retry_choice in ['0', '1', '2']:
                        break
                    print(Fore.RED + Style.BRIGHT + "\nInvalid choice. Please select 0-2.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                    return None
            
            if retry_choice == '1':
                print(Fore.CYAN + Style.BRIGHT + "\nRestarting custom configuration...")
                return _interactive_custom_setup(base_config, use_real_data, **kwargs)
            elif retry_choice == '2':
                print(Fore.YELLOW + Style.BRIGHT + "\nReturning to training menu...")
                return None
            else:
                print(Fore.RED + Style.BRIGHT + "\nConfiguration cancelled")
                return None
            
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\n\nCustom setup interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"Custom setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during custom configuration setup: {str(e)}\n"
            f"Context:\n"
            f"- Base Config: {bool(base_config)}\n"
            f"- Use Real Data: {use_real_data}\n\n"
            f"This could be due to:\n"
            f"- Invalid input values\n"
            f"- Configuration validation issues\n"
            f"- System resource constraints\n"
            f"- Interactive input handling problems"
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="CUSTOM SETUP ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        return None

def _launch_training_with_config(config: Dict[str, Any], **kwargs) -> Optional[Dict[str, Any]]:
    try:
        print("\nLAUNCHING TRAINING")
        print("="*50)
        
        training_params = {}
        
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        security_config = config.get('security', {})
        system_config = config.get('system', {})
        monitoring_config = config.get('monitoring', {})
        export_config = config.get('export', {})
        hardware_config = config.get('hardware', {})
        advanced_config = config.get('advanced_training', {})
        validation_config = config.get('validation', {})
        hpo_config = config.get('hyperparameter_optimization', {})
        presets_config = config.get('presets', {})
        experimental_config = config.get('experimental', {})
        metadata_config = config.get('metadata', {})
        runtime_config = config.get('runtime', {})
        
        for key, value in model_config.items():
            if key in ['model_type', 'encoding_dim', 'hidden_dims', 'dropout_rates', 
                      'activation', 'normalization', 'skip_connection', 'residual_blocks',
                      'use_attention', 'legacy_mode', 'num_models', 'diversity_factor',
                      'activation_param', 'use_batch_norm', 'use_layer_norm', 'bias',
                      'weight_init', 'available_activations', 'available_normalizations',
                      'available_initializers', 'model_types', 'min_features']:
                training_params[key] = value
        
        for key, value in training_config.items():
            if key in ['batch_size', 'epochs', 'learning_rate', 'patience', 'weight_decay',
                      'gradient_clip', 'gradient_accumulation_steps', 'mixed_precision',
                      'optimizer', 'scheduler', 'scheduler_params', 'early_stopping',
                      'validation_split', 'shuffle', 'pin_memory', 'persistent_workers',
                      'adam_betas', 'adam_eps', 'lr_patience', 'lr_factor', 'min_lr']:
                if key == 'optimizer':
                    training_params['optimizer_type'] = value
                elif key == 'scheduler':
                    training_params['scheduler_type'] = value
                else:
                    training_params[key] = value
        
        for key, value in data_config.items():
            if key in ['normal_samples', 'attack_samples', 'features', 'use_real_data',
                      'data_path', 'artifacts_path', 'synthetic_generation', 'preprocessing',
                      'anomaly_factor', 'random_state', 'test_split', 'stratified_split',
                      'data_preprocessing']:
                training_params[key] = value
            elif key == 'normalization':
                training_params['normalization_method'] = value
        
        for key, value in security_config.items():
            if key in ['percentile', 'attack_threshold', 'false_negative_cost',
                      'enable_security_metrics', 'adaptive_threshold', 'confidence_interval',
                      'detection_methods', 'alert_levels', 'threshold_validation',
                      'robust_detection', 'false_positive_tolerance',
                      'performance_optimized_detection', 'real_time_monitoring',
                      'ensemble_voting', 'uncertainty_threshold',
                      'early_warning_threshold']:
                training_params[key] = value
            elif key == 'anomaly_threshold_strategy':
                training_params['threshold_method'] = value
        
        for key, value in system_config.items():
            if key in ['model_dir', 'log_dir', 'config_dir', 'random_seed',
                      'reproducible', 'data_dir', 'checkpoint_dir', 'debug',
                      'verbose', 'parallel_processing', 'max_workers', 'export_onnx',
                      'non_interactive', 'cuda_optimizations', 'onnx_export',
                      'distributed_training', 'python_executable', 'working_directory',
                      'environment_health']:
                training_params[key] = value
        
        for key, value in monitoring_config.items():
            if key in ['verbose', 'tensorboard_logging', 'save_checkpoints',
                      'checkpoint_frequency', 'log_frequency', 'metrics_frequency',
                      'console_logging_level', 'save_best_model', 'save_model_history',
                      'metrics_to_track', 'early_stopping_metric', 'checkpoint_format',
                      'log_model_summary', 'tensorboard_dir', 'tensorboard',
                      'stability_metrics', 'performance_metrics', 'profiling_enabled']:
                training_params[key] = value
            elif key == 'verbose':
                training_params['debug_mode'] = value
        
        for key, value in export_config.items():
            if key in ['export_onnx', 'save_model', 'save_metadata',
                      'save_training_history']:
                training_params[key] = value
        
        for key, value in hardware_config.items():
            if key in ['device', 'recommended_gpu_memory', 'minimum_system_requirements',
                      'optimal_system_requirements', 'memory_management',
                      'performance_optimization', 'detected_gpu_memory',
                      'detected_system_memory', 'system_performance_class',
                      'optimization_recommendations', 'cuda_optimizations']:
                training_params[key] = value
        
        for key, value in advanced_config.items():
            if key in ['num_workers', 'pin_memory', 'persistent_workers',
                      'memory_efficient', 'compile_model', 'benchmark_mode',
                      'gradient_checkpointing']:
                training_params[key] = value
        
        for key, value in validation_config.items():
            if key in ['cross_validation', 'metrics', 'validation_frequency',
                      'save_validation_results', 'detailed_metrics', 'robustness_testing',
                      'performance_benchmarking', 'confidence_intervals']:
                if key == 'detailed_metrics':
                    training_params['calculate_detailed_metrics'] = value
                elif key == 'cross_validation' and isinstance(value, dict):
                    if value.get('enabled', False):
                        training_params['cross_validation'] = True
                        training_params['cv_folds'] = value.get('folds', 5)
                    else:
                        training_params['cross_validation'] = False
                else:
                    training_params[key] = value
        
        for key, value in hpo_config.items():
            if key in ['enabled', 'strategy', 'study_name', 'direction', 'n_trials',
                      'timeout', 'sampler', 'pruner', 'objective_metric',
                      'trial_epochs', 'trial_patience', 'cleanup_trials',
                      'generate_plots']:
                hpo_key = f'hpo_{key}' if key != 'enabled' else 'hpo_enabled'
                training_params[hpo_key] = value
        
        for key, value in experimental_config.items():
            if key in ['experimental_features', 'auto_optimize']:
                training_params[key] = value
        
        training_params.update(kwargs)
        training_params['config'] = config
        
        training_params.setdefault('model_dir', DEFAULT_MODEL_DIR)
        training_params.setdefault('log_dir', LOG_DIR)
        training_params.setdefault('tensorboard_dir', TB_DIR)
        
        training_params.setdefault('checkpoint_dir', CHECKPOINTS_DIR)
        training_params.setdefault('data_dir', DATA_DIR)
        training_params.setdefault('config_dir', CONFIG_DIR)
        
        if monitoring_config.get('tensorboard_dir'):
            training_params['tensorboard_dir'] = monitoring_config['tensorboard_dir']
        elif system_config.get('log_dir'):
            training_params['tensorboard_dir'] = system_config['log_dir']
        
        if 'tb_dir' in system_config:
            training_params['tb_dir'] = system_config['tb_dir']
        
        print("Calling train_model with comprehensive configuration...")
        results = train_model(**training_params)
        
        if results and results.get('success', False):
            print("\n" + "="*40)
            print("TRAINING COMPLETED SUCCESSFULLY!")
            print("="*40)
            
            _display_training_results(results)
            
        else:
            print("\nTRAINING FAILED")
            if results:
                error_msg = results.get('error', 'Unknown error')
                print(f"Error: {error_msg}")
                
                if results.get('partial_training_completed', False):
                    print(f"\nPartial training completed:")
                    print(f"   Epochs: {results.get('epochs_completed', 0)}")
                    partial_metrics = results.get('training_metrics', {})
                    if partial_metrics:
                        print(f"   Best Loss: {partial_metrics.get('best_val_loss', 'N/A')}")
        
        return results
        
    except Exception as e:
        logger.error(f"Training launch failed: {e}")
        print(f"\nFailed to launch training: {str(e)}")
        return None

def _merge_configs(base_config: Dict[str, Any], preset_config: Dict[str, Any]) -> Dict[str, Any]:
    merged = {}
    
    all_keys = set(base_config.keys()) | set(preset_config.keys())
    
    for key in all_keys:
        base_value = base_config.get(key)
        preset_value = preset_config.get(key)
        
        if base_value is None and preset_value is None:
            continue
        elif base_value is None:
            merged[key] = preset_value.copy() if isinstance(preset_value, dict) else preset_value
        elif preset_value is None:
            merged[key] = base_value.copy() if isinstance(base_value, dict) else base_value
        elif isinstance(base_value, dict) and isinstance(preset_value, dict):
            merged[key] = _merge_configs(base_value, preset_value)
        else:
            merged[key] = base_value
    
    return merged

def _apply_model_type_defaults(config: Dict[str, Any], model_type: str) -> None:
    model_config = config.setdefault('model', {})
    training_config = config.setdefault('training', {})
    hardware_config = config.setdefault('hardware', {})
    monitoring_config = config.setdefault('monitoring', {})
    security_config = config.setdefault('security', {})
    
    if model_type == 'SimpleAutoencoder':
        model_config.update({
            'model_type': 'SimpleAutoencoder',
            'encoding_dim': 16,
            'hidden_dims': [128, 64],
            'dropout_rates': [0.2, 0.15],
            'activation': 'leaky_relu',
            'activation_param': 0.2,
            'normalization': 'batch',
            'use_batch_norm': True,
            'use_layer_norm': False,
            'bias': True,
            'weight_init': 'xavier_uniform',
            'skip_connection': False,
            'residual_blocks': False,
            'use_attention': False,
            'legacy_mode': False,
            'diversity_factor': None,
            'min_features': 5,
            'num_models': None
        })
        
        training_config.update({
            'batch_size': 64,
            'epochs': 50,
            'learning_rate': 0.001,
            'patience': 15,
            'weight_decay': 1e-4,
            'gradient_clip': 1.0,
            'gradient_accumulation_steps': 1,
            'mixed_precision': torch.cuda.is_available(),
            'optimizer': 'Adam',
            'scheduler': 'ReduceLROnPlateau',
            'scheduler_params': {
                'patience': 5,
                'factor': 0.5,
                'min_lr': 1e-6
            },
            'early_stopping': True,
            'validation_split': 0.2,
            'shuffle': True,
            'pin_memory': torch.cuda.is_available(),
            'persistent_workers': True,
            'adam_betas': (0.9, 0.999),
            'adam_eps': 1e-8,
            'lr_patience': 5,
            'lr_factor': 0.5,
            'min_lr': 1e-6
        })
        
    elif model_type == 'EnhancedAutoencoder':
        model_config.update({
            'model_type': 'EnhancedAutoencoder',
            'encoding_dim': 32,
            'hidden_dims': [256, 128, 64],
            'dropout_rates': [0.2, 0.15, 0.1],
            'activation': 'leaky_relu',
            'activation_param': 0.2,
            'normalization': 'batch',
            'use_batch_norm': True,
            'use_layer_norm': False,
            'bias': True,
            'weight_init': 'xavier_uniform',
            'skip_connection': True,
            'residual_blocks': True,
            'use_attention': True,
            'legacy_mode': False,
            'diversity_factor': None,
            'min_features': 5,
            'num_models': None
        })
        
        training_config.update({
            'batch_size': 64,
            'epochs': 100,
            'learning_rate': 0.001,
            'patience': 20,
            'weight_decay': 1e-4,
            'gradient_clip': 1.0,
            'gradient_accumulation_steps': 1,
            'mixed_precision': torch.cuda.is_available(),
            'optimizer': 'AdamW',
            'scheduler': 'ReduceLROnPlateau',
            'scheduler_params': {
                'patience': 7,
                'factor': 0.5,
                'min_lr': 1e-6
            },
            'early_stopping': True,
            'validation_split': 0.2,
            'shuffle': True,
            'pin_memory': torch.cuda.is_available(),
            'persistent_workers': True,
            'adam_betas': (0.9, 0.999),
            'adam_eps': 1e-8,
            'lr_patience': 7,
            'lr_factor': 0.5,
            'min_lr': 1e-6
        })
        
    elif model_type == 'AutoencoderEnsemble':
        model_config.update({
            'model_type': 'AutoencoderEnsemble',
            'encoding_dim': 24,
            'hidden_dims': [192, 96, 48],
            'dropout_rates': [0.25, 0.2, 0.15],
            'activation': 'leaky_relu',
            'activation_param': 0.2,
            'normalization': 'batch',
            'use_batch_norm': True,
            'use_layer_norm': False,
            'bias': True,
            'weight_init': 'xavier_uniform',
            'skip_connection': True,
            'residual_blocks': True,
            'use_attention': True,
            'legacy_mode': False,
            'diversity_factor': 0.3,
            'min_features': 5,
            'num_models': 3
        })
        
        training_config.update({
            'batch_size': 64,
            'epochs': 150,
            'learning_rate': 0.001,
            'patience': 25,
            'weight_decay': 1e-4,
            'gradient_clip': 1.0,
            'gradient_accumulation_steps': 1,
            'mixed_precision': torch.cuda.is_available(),
            'optimizer': 'AdamW',
            'scheduler': 'ReduceLROnPlateau',
            'scheduler_params': {
                'patience': 10,
                'factor': 0.5,
                'min_lr': 1e-6
            },
            'early_stopping': True,
            'validation_split': 0.2,
            'shuffle': True,
            'pin_memory': torch.cuda.is_available(),
            'persistent_workers': True,
            'adam_betas': (0.9, 0.999),
            'adam_eps': 1e-8,
            'lr_patience': 10,
            'lr_factor': 0.5,
            'min_lr': 1e-6
        })
    
    hardware_config.setdefault('device', 'auto')
    hardware_config.setdefault('cuda_optimizations', torch.cuda.is_available())
    hardware_config.setdefault('memory_management', {'enable_memory_efficient': True})
    hardware_config.setdefault('recommended_gpu_memory', 4.0 if model_type == 'SimpleAutoencoder' else 8.0)
    hardware_config.setdefault('minimum_system_requirements', {})
    hardware_config.setdefault('optimal_system_requirements', {})
    hardware_config.setdefault('performance_optimization', {})
    hardware_config.setdefault('detected_gpu_memory', None)
    hardware_config.setdefault('detected_system_memory', None)
    hardware_config.setdefault('system_performance_class', 'auto')
    hardware_config.setdefault('optimization_recommendations', [])
    
    monitoring_config.setdefault('verbose', True)
    monitoring_config.setdefault('tensorboard_logging', True)
    monitoring_config.setdefault('save_checkpoints', True)
    monitoring_config.setdefault('save_best_model', True)
    monitoring_config.setdefault('metrics_to_track', ['loss', 'reconstruction_error'])
    monitoring_config.setdefault('checkpoint_frequency', 10)
    monitoring_config.setdefault('log_frequency', 1)
    monitoring_config.setdefault('metrics_frequency', 1)
    monitoring_config.setdefault('console_logging_level', 'INFO')
    monitoring_config.setdefault('save_model_history', True)
    monitoring_config.setdefault('early_stopping_metric', 'val_loss')
    monitoring_config.setdefault('checkpoint_format', 'pth')
    monitoring_config.setdefault('log_model_summary', True)
    monitoring_config.setdefault('tensorboard_dir', TB_DIR)
    monitoring_config.setdefault('tensorboard', {})
    monitoring_config.setdefault('stability_metrics', True)
    monitoring_config.setdefault('performance_metrics', True)
    monitoring_config.setdefault('profiling_enabled', False)
    
    security_config.setdefault('percentile', 95.0)
    security_config.setdefault('enable_security_metrics', True)
    security_config.setdefault('anomaly_threshold_strategy', 'percentile')
    security_config.setdefault('adaptive_threshold', True)
    security_config.setdefault('attack_threshold', None)
    security_config.setdefault('false_negative_cost', None)
    security_config.setdefault('early_warning_threshold', None)
    security_config.setdefault('confidence_interval', None)
    security_config.setdefault('detection_methods', [])
    security_config.setdefault('alert_levels', [])
    security_config.setdefault('threshold_validation', True)
    security_config.setdefault('robust_detection', True)
    security_config.setdefault('false_positive_tolerance', None)
    security_config.setdefault('performance_optimized_detection', True)
    security_config.setdefault('real_time_monitoring', False)
    security_config.setdefault('ensemble_voting', 'average' if model_type == 'AutoencoderEnsemble' else None)
    security_config.setdefault('uncertainty_threshold', None)

def _estimate_training_time(epochs: int, model_type: str, batch_size: int = 64) -> str:
    base_time_per_epoch = {
        'SimpleAutoencoder': 3.0,
        'EnhancedAutoencoder': 8.0,
        'AutoencoderEnsemble': 20.0
    }
    
    base_time = base_time_per_epoch.get(model_type, 8.0)
    
    batch_factor = (64.0 / batch_size) ** 0.7
    
    device_factor = 1.0
    if torch.cuda.is_available():
        device_factor = 0.25
        try:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            if gpu_memory >= 8:
                device_factor = 0.2
            elif gpu_memory >= 16:
                device_factor = 0.15
        except Exception:
            pass
    
    complexity_factor = 1.0
    if model_type == 'SimpleAutoencoder':
        complexity_factor = 0.6
    elif model_type == 'EnhancedAutoencoder':
        complexity_factor = 1.0
    elif model_type == 'AutoencoderEnsemble':
        complexity_factor = 1.8
    
    total_seconds = epochs * base_time * batch_factor * device_factor * complexity_factor
    total_minutes = total_seconds / 60.0
    
    if total_minutes < 0.5:
        return "< 1"
    elif total_minutes < 1:
        return "1"
    elif total_minutes < 5:
        return f"{total_minutes:.1f}"
    elif total_minutes < 60:
        return f"{int(total_minutes)}"
    else:
        hours = int(total_minutes // 60)
        remaining_minutes = int(total_minutes % 60)
        if hours < 24:
            return f"{hours}h {remaining_minutes}m"
        else:
            days = hours // 24
            remaining_hours = hours % 24
            return f"{days}d {remaining_hours}h"

def _display_training_results(results: Dict[str, Any]) -> None:
    try:
        print("\n" + "="*40)
        print("TRAINING RESULTS SUMMARY")
        print("="*40)
        
        success = results.get('success', False)
        timestamp = results.get('timestamp', 'Unknown')
        run_id = results.get('run_id', 'Unknown')
        
        print(f"Status: {'SUCCESS' if success else 'FAILED'}")
        print(f"Completed: {timestamp}")
        print(f"Run ID: {run_id}")
        
        if not success:
            error_msg = results.get('error', 'Unknown error')
            error_type = results.get('error_type', 'Unknown')
            print(f"Error Type: {error_type}")
            print(f"Error Message: {error_msg}")
            
            partial_training = results.get('partial_training_completed', False)
            if partial_training:
                epochs_completed = results.get('epochs_completed', 0)
                print(f"Partial Training: {epochs_completed} epochs completed")
                
                training_metrics = results.get('training_metrics', {})
                if training_metrics:
                    best_val_loss = training_metrics.get('best_val_loss', 'N/A')
                    last_train_loss = training_metrics.get('last_train_loss', 'N/A')
                    last_val_loss = training_metrics.get('last_val_loss', 'N/A')
                    print(f"Best Validation Loss: {best_val_loss}")
                    print(f"Last Training Loss: {last_train_loss}")
                    print(f"Last Validation Loss: {last_val_loss}")
            
            error_log_path = results.get('error_log_path')
            if error_log_path:
                print(f"Error Log: {error_log_path}")
            
            partial_model_path = results.get('partial_model_path')
            if partial_model_path:
                print(f"Partial Model: {partial_model_path}")
            
            graceful_recovery = results.get('graceful_recovery', False)
            if graceful_recovery:
                print("Graceful recovery was attempted")
            
            return
        
        training_time = results.get('training_time_minutes', 0)
        model_type = results.get('model_type', 'Unknown')
        
        print(f"Model Type: {model_type}")
        print(f"Training Duration: {training_time:.2f} minutes")
        
        final_metrics = results.get('final_metrics', {})
        if final_metrics:
            print(f"\nFINAL METRICS")
            print(f"-" * 40)
            
            best_val_loss = final_metrics.get('best_validation_loss', float('inf'))
            test_loss = final_metrics.get('test_loss', float('inf'))
            final_epoch = final_metrics.get('final_epoch', 'N/A')
            threshold = final_metrics.get('threshold', 'N/A')
            detection_rate = final_metrics.get('anomaly_detection_rate', 0)
            
            print(f"Best Validation Loss: {best_val_loss:.6f}")
            print(f"Test Loss: {test_loss:.6f}")
            print(f"Final Epoch: {final_epoch}")
            print(f"Anomaly Threshold: {threshold:.6f}" if isinstance(threshold, (int, float)) else f"Anomaly Threshold: {threshold}")
            print(f"Detection Rate: {detection_rate*100:.2f}%")
        
        model_info = results.get('model_info', {})
        if model_info:
            print(f"\nMODEL INFORMATION")
            print(f"-" * 40)
            
            model_class = model_info.get('class_name', 'N/A')
            total_params = model_info.get('parameters', 0)
            trainable_params = model_info.get('trainable_parameters', 0)
            model_size = model_info.get('size_mb', 0)
            input_dim = model_info.get('input_dim', 'N/A')
            encoding_dim = model_info.get('encoding_dim', 'N/A')
            
            print(f"Class: {model_class}")
            print(f"Parameters: {total_params:,} total, {trainable_params:,} trainable")
            print(f"Model Size: {model_size:.2f} MB")
            print(f"Input Dimension: {input_dim}")
            print(f"Encoding Dimension: {encoding_dim}")
            
            enhanced_features = model_info.get('enhanced_features', {})
            if enhanced_features:
                features = []
                if enhanced_features.get('attention', False):
                    features.append("Attention")
                if enhanced_features.get('residual_blocks', False):
                    features.append("Residual Blocks")
                if enhanced_features.get('skip_connections', False):
                    features.append("Skip Connections")
                if features:
                    print(f"Enhanced Features: {', '.join(features)}")
            
            ensemble_info = results.get('ensemble_info', {})
            if ensemble_info:
                num_models = ensemble_info.get('num_models', 'N/A')
                diversity_factor = ensemble_info.get('diversity_factor', 'N/A')
                actual_models = ensemble_info.get('actual_models', 'N/A')
                model_types = ensemble_info.get('model_types', {})
                
                print(f"Ensemble Size: {num_models}")
                print(f"Actual Models: {actual_models}")
                print(f"Diversity Factor: {diversity_factor}")
                if model_types:
                    type_distribution = ", ".join([f"{k}: {v}" for k, v in model_types.items()])
                    print(f"Model Distribution: {type_distribution}")
        
        data_info = results.get('data_info', {})
        if data_info:
            print(f"\nDATA INFORMATION")
            print(f"-" * 40)
            
            source = data_info.get('source', 'N/A').title()
            train_samples = data_info.get('train_samples', 0)
            val_samples = data_info.get('val_samples', 0)
            test_samples = data_info.get('test_samples', 0)
            features = data_info.get('features', 0)
            
            print(f"Source: {source}")
            print(f"Training Samples: {train_samples:,}")
            print(f"Validation Samples: {val_samples:,}")
            print(f"Test Samples: {test_samples:,}")
            print(f"Features: {features}")
        
        system_info = results.get('system_info', {})
        if system_info:
            print(f"\nSYSTEM INFORMATION")
            print(f"-" * 40)
            
            device = system_info.get('device', 'N/A')
            device_type = system_info.get('device_type', 'N/A')
            mixed_precision = system_info.get('mixed_precision', False)
            pytorch_version = system_info.get('pytorch_version', 'N/A')
            cuda_available = system_info.get('cuda_available', False)
            cuda_version = system_info.get('cuda_version', 'N/A')
            gpu_count = system_info.get('gpu_count', 0)
            gpu_name = system_info.get('gpu_name', 'N/A')
            
            print(f"Device: {device}")
            print(f"Device Type: {device_type}")
            print(f"Mixed Precision: {'Enabled' if mixed_precision else 'Disabled'}")
            print(f"PyTorch Version: {pytorch_version}")
            
            if cuda_available:
                print(f"CUDA Available: Yes")
                print(f"CUDA Version: {cuda_version}")
                print(f"GPU Count: {gpu_count}")
                if gpu_name != 'N/A':
                    print(f"GPU Name: {gpu_name}")
            else:
                print(f"CUDA Available: No")
        
        training_stats = results.get('training_stats', {})
        if training_stats:
            total_training_time = training_stats.get('total_training_time', 0)
            final_epoch = training_stats.get('final_epoch', 0)
            early_stopped = training_stats.get('early_stopped', False)
            model_compiled = training_stats.get('model_compiled', False)
            
            if total_training_time > 0 or final_epoch > 0:
                print(f"\nTRAINING STATISTICS")
                print(f"-" * 40)
                
                if total_training_time > 0:
                    print(f"Total Training Time: {total_training_time/60:.2f} minutes")
                    if final_epoch > 0:
                        print(f"Time per Epoch: {total_training_time/final_epoch:.2f} seconds")
                
                if final_epoch > 0:
                    print(f"Epochs Completed: {final_epoch}")
                
                print(f"Early Stopping: {'Yes' if early_stopped else 'No'}")
                print(f"Model Compiled: {'Yes' if model_compiled else 'No'}")
            
            training_history = training_stats.get('training_history', {})
            if training_history:
                train_losses = training_history.get('train_loss', [])
                val_losses = training_history.get('val_loss', [])
                learning_rates = training_history.get('learning_rate', [])
                epoch_times = training_history.get('epoch_times', [])
                
                if train_losses and val_losses:
                    print(f"\nTRAINING PROGRESSION")
                    print(f"-" * 40)
                    print(f"Initial Train Loss: {train_losses[0]:.6f}")
                    print(f"Final Train Loss: {train_losses[-1]:.6f}")
                    print(f"Initial Val Loss: {val_losses[0]:.6f}")
                    print(f"Final Val Loss: {val_losses[-1]:.6f}")
                    
                    best_val_idx = val_losses.index(min(val_losses))
                    print(f"Best Val Loss: {val_losses[best_val_idx]:.6f} (epoch {best_val_idx + 1})")
                
                if learning_rates:
                    print(f"Initial Learning Rate: {learning_rates[0]:.2e}")
                    print(f"Final Learning Rate: {learning_rates[-1]:.2e}")
                
                if epoch_times:
                    avg_epoch_time = sum(epoch_times) / len(epoch_times)
                    print(f"Average Epoch Time: {avg_epoch_time:.2f} seconds")
        
        config = results.get('configuration', {})
        if config:
            model_config = config.get('model_architecture', config.get('model', {}))
            training_config = config.get('training_config', config.get('training', {}))
            
            if model_config or training_config:
                print(f"\nCONFIGURATION SUMMARY")
                print(f"-" * 40)
                
                if model_config:
                    activation = model_config.get('activation', 'N/A')
                    normalization = model_config.get('normalization', 'N/A')
                    hidden_dims = model_config.get('hidden_dims', [])
                    dropout_rates = model_config.get('dropout_rates', [])
                    
                    print(f"Activation: {activation}")
                    print(f"Normalization: {normalization}")
                    if hidden_dims:
                        print(f"Hidden Dimensions: {hidden_dims}")
                    if dropout_rates:
                        print(f"Dropout Rates: {dropout_rates}")
                
                if training_config:
                    batch_size = training_config.get('batch_size', 'N/A')
                    learning_rate = training_config.get('learning_rate', 'N/A')
                    optimizer = training_config.get('optimizer_type', training_config.get('optimizer', 'N/A'))
                    scheduler = training_config.get('scheduler_type', training_config.get('scheduler', 'N/A'))
                    
                    print(f"Batch Size: {batch_size}")
                    print(f"Learning Rate: {learning_rate}")
                    print(f"Optimizer: {optimizer}")
                    print(f"Scheduler: {scheduler}")
        
        artifacts = results.get('artifacts', {})
        if artifacts:
            print(f"\nSAVED ARTIFACTS")
            print(f"-" * 40)
            
            for artifact_type, path in artifacts.items():
                if path:
                    artifact_name = artifact_type.replace('_', ' ').title()
                    print(f"{artifact_name}: {path}")
        
        threshold_data = results.get('training_stats', {}).get('threshold_data', {})
        if threshold_data:
            threshold = threshold_data.get('threshold', 'N/A')
            method = threshold_data.get('method', 'N/A')
            metadata = threshold_data.get('metadata', {})
            
            print(f"\nTHRESHOLD INFORMATION")
            print(f"-" * 40)
            print(f"Threshold Value: {threshold:.6f}" if isinstance(threshold, (int, float)) else f"Threshold Value: {threshold}")
            print(f"Calculation Method: {method}")
            
            if isinstance(metadata, dict) and metadata:
                percentile = metadata.get('percentile', None)
                adaptive = metadata.get('adaptive', None)
                
                if percentile is not None:
                    print(f"Percentile Used: {percentile}")
                if adaptive is not None:
                    print(f"Adaptive: {'Yes' if adaptive else 'No'}")
        
        final_evaluation = results.get('training_stats', {}).get('final_evaluation', {})
        if final_evaluation:
            test_metrics = final_evaluation.get('test_metrics', {})
            mse_stats = final_evaluation.get('mse_statistics', {})
            
            if test_metrics or mse_stats:
                print(f"\nDETAILED EVALUATION")
                print(f"-" * 40)
                
                if test_metrics:
                    for metric_name, metric_value in test_metrics.items():
                        if isinstance(metric_value, (int, float)):
                            if metric_name.lower().endswith('loss') or metric_name.lower().endswith('error'):
                                print(f"{metric_name.replace('_', ' ').title()}: {metric_value:.6f}")
                            else:
                                print(f"{metric_name.replace('_', ' ').title()}: {metric_value:.4f}")
                        else:
                            print(f"{metric_name.replace('_', ' ').title()}: {metric_value}")
                
                if mse_stats:
                    print(f"MSE Statistics:")
                    for stat_name, stat_value in mse_stats.items():
                        if isinstance(stat_value, (int, float)):
                            print(f"  {stat_name.upper()}: {stat_value:.6f}")
        
        error_info = results.get('error_details', {})
        if error_info:
            partial_results = error_info.get('partial_results', {})
            if partial_results:
                print(f"\nPARTIAL RESULTS")
                print(f"-" * 40)
                
                epochs_completed = partial_results.get('epochs_completed', 0)
                best_val_loss = partial_results.get('best_val_loss', 'N/A')
                
                print(f"Epochs Completed: {epochs_completed}")
                print(f"Best Validation Loss: {best_val_loss}")
        
        performance_metrics = results.get('training_stats', {}).get('training_history', {}).get('detailed_metrics', {})
        if performance_metrics:
            print(f"\nPERFORMANCE METRICS")
            print(f"-" * 40)
            
            for metric_name, metric_values in performance_metrics.items():
                if isinstance(metric_values, list) and metric_values:
                    final_value = metric_values[-1]
                    if isinstance(final_value, (int, float)):
                        metric_display = metric_name.replace('_', ' ').title()
                        if 'loss' in metric_name.lower() or 'error' in metric_name.lower():
                            print(f"{metric_display}: {final_value:.6f}")
                        else:
                            print(f"{metric_display}: {final_value:.4f}")
        
        memory_usage = results.get('training_stats', {}).get('training_history', {}).get('memory_usage', [])
        if memory_usage:
            print(f"\nRESOURCE USAGE")
            print(f"-" * 40)
            
            avg_memory = sum(memory_usage) / len(memory_usage)
            max_memory = max(memory_usage)
            print(f"Average GPU Memory: {avg_memory:.2f} GB")
            print(f"Peak GPU Memory: {max_memory:.2f} GB")
        
        experiment_dir = results.get('training_stats', {}).get('experiment_dir')
        if experiment_dir:
            print(f"\nEXPERIMENT TRACKING")
            print(f"-" * 40)
            print(f"TensorBoard Logs: {experiment_dir}")
        
        print("="*80)
        
        additional_info = []
        
        if success:
            additional_info.append("Training completed successfully!")
            
            if training_time > 0:
                if training_time < 1:
                    additional_info.append("Very fast training (< 1 minute)")
                elif training_time < 5:
                    additional_info.append("Quick training (< 5 minutes)")
                elif training_time < 30:
                    additional_info.append("Normal training duration")
                else:
                    additional_info.append("Extended training duration")
            
            if final_metrics:
                test_loss = final_metrics.get('test_loss', float('inf'))
                val_loss = final_metrics.get('best_validation_loss', float('inf'))
                
                if test_loss != float('inf') and val_loss != float('inf'):
                    if abs(test_loss - val_loss) / val_loss < 0.1:
                        additional_info.append("Good generalization (test/val loss similar)")
                    elif test_loss > val_loss * 1.5:
                        additional_info.append("Possible overfitting detected")
                
                detection_rate = final_metrics.get('anomaly_detection_rate', 0)
                if detection_rate > 0:
                    if detection_rate < 0.05:
                        additional_info.append("Low anomaly detection rate")
                    elif detection_rate > 0.3:
                        additional_info.append("High anomaly detection rate")
                    else:
                        additional_info.append("Normal anomaly detection rate")
            
            if model_info:
                total_params = model_info.get('parameters', 0)
                if total_params > 0:
                    if total_params < 10000:
                        additional_info.append("Lightweight model")
                    elif total_params > 1000000:
                        additional_info.append("Large model")
                
                model_size = model_info.get('size_mb', 0)
                if model_size > 0:
                    if model_size < 1:
                        additional_info.append("Very compact model")
                    elif model_size > 100:
                        additional_info.append("Large model file")
        
        if additional_info:
            print("SUMMARY")
            print("-" * 40)
            for info in additional_info:
                print(f"- {info}")
            print("="*80)
        
    except Exception as e:
        print(f"\nError displaying training results: {str(e)}")
        print("Raw results structure:")
        try:
            print(f"Success: {results.get('success', 'Unknown')}")
            print(f"Error: {results.get('error', 'None')}")
            print(f"Available keys: {list(results.keys())}")
        except Exception:
            print("Failed to extract basic result information")
        print("="*80)

def train_model_quick(
    # Quick Test Parameters
    quick_epochs: Optional[int] = None,
    quick_batch_size: Optional[int] = None,
    quick_learning_rate: Optional[float] = None,
    quick_model_type: Optional[str] = None,
    quick_encoding_dim: Optional[int] = None,
    quick_normal_samples: Optional[int] = None,
    quick_attack_samples: Optional[int] = None,
    quick_features: Optional[int] = None,
    
    # Override Parameters
    use_real_data: Optional[bool] = None,
    device: Optional[str] = None,
    verbose: Optional[bool] = None,
    save_results: Optional[bool] = None,
    export_onnx: Optional[bool] = None,
    tensorboard_logging: Optional[bool] = None,
    
    # System Parameters
    model_dir: Optional[Union[str, Path]] = None,
    random_seed: Optional[int] = None,
    non_interactive: Optional[bool] = None,
    
    # Advanced Quick Options
    minimal_logging: Optional[bool] = None,
    skip_validation: Optional[bool] = None,
    fast_mode: Optional[bool] = None,
    benchmark_mode: Optional[bool] = None,
    
    # Interactive Parameters (new)
    interactive: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    quick_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """Quick model training with enhanced context display and error handling."""
    try:
        # Clear screen and show banner with config
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Extract configuration context with error handling
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu header with context
        #print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.MAGENTA + Style.BRIGHT + "QUICK MODEL TRAINING - FAST TEST MODE")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"\nActive Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        
        # Start timing
        start_time = datetime.now()
        quick_start_time = time.time()
        
        # Initialize configuration with quick defaults
        if config is None:
            try:
                config = get_current_config() if 'get_current_config' in globals() else {}
            except Exception as e:
                logger.warning(f"Failed to load current config: {e}")
                config = {}
        
        # Apply quick-specific configuration
        if quick_config:
            config.setdefault('quick_training', {}).update(quick_config)
        
        # Apply all parameters to configuration
        final_config = config.copy()
        
        # Apply individual parameters
        params = locals().copy()
        params.update(kwargs)
        
        # Remove non-parameter items
        params_to_remove = {
            'config', 'quick_config', 'kwargs', 'start_time', 'quick_start_time',
            'datetime', 'traceback', 'time', 'gc', 'warnings', 'Path'
        }
        
        cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
        
        # Set up quick training defaults (optimized for speed)
        quick_training_config = final_config.setdefault('quick_training', {})
        
        # Core quick parameters with speed-optimized defaults
        quick_epochs = quick_training_config.setdefault('quick_epochs', cleaned_params.get('quick_epochs', 10))
        quick_batch_size = quick_training_config.setdefault('quick_batch_size', cleaned_params.get('quick_batch_size', 128))
        quick_learning_rate = quick_training_config.setdefault('quick_learning_rate', cleaned_params.get('quick_learning_rate', 0.01))
        quick_model_type = quick_training_config.setdefault('quick_model_type', cleaned_params.get('quick_model_type', 'SimpleAutoencoder'))
        quick_encoding_dim = quick_training_config.setdefault('quick_encoding_dim', cleaned_params.get('quick_encoding_dim', 8))
        quick_normal_samples = quick_training_config.setdefault('quick_normal_samples', cleaned_params.get('quick_normal_samples', 1000))
        quick_attack_samples = quick_training_config.setdefault('quick_attack_samples', cleaned_params.get('quick_attack_samples', 200))
        quick_features = quick_training_config.setdefault('quick_features', cleaned_params.get('quick_features', 20))
        
        # System parameters
        use_real_data = quick_training_config.setdefault('use_real_data', cleaned_params.get('use_real_data', False))
        device = quick_training_config.setdefault('device', cleaned_params.get('device', 'auto'))
        verbose = quick_training_config.setdefault('verbose', cleaned_params.get('verbose', True))
        save_results = quick_training_config.setdefault('save_results', cleaned_params.get('save_results', True))
        export_onnx = quick_training_config.setdefault('export_onnx', cleaned_params.get('export_onnx', False))
        tensorboard_logging = quick_training_config.setdefault('tensorboard_logging', cleaned_params.get('tensorboard_logging', False))
        model_dir = quick_training_config.setdefault('model_dir', cleaned_params.get('model_dir', DEFAULT_MODEL_DIR / "quick_test"))
        random_seed = quick_training_config.setdefault('random_seed', cleaned_params.get('random_seed', 42))
        non_interactive = quick_training_config.setdefault('non_interactive', cleaned_params.get('non_interactive', False))
        interactive = quick_training_config.setdefault('interactive', cleaned_params.get('interactive', not cleaned_params.get('non_interactive', False)))
        
        # Speed optimization parameters
        minimal_logging = quick_training_config.setdefault('minimal_logging', cleaned_params.get('minimal_logging', True))
        skip_validation = quick_training_config.setdefault('skip_validation', cleaned_params.get('skip_validation', False))
        fast_mode = quick_training_config.setdefault('fast_mode', cleaned_params.get('fast_mode', True))
        benchmark_mode = quick_training_config.setdefault('benchmark_mode', cleaned_params.get('benchmark_mode', False))
        
        # Interactive prompt if enabled
        if interactive:
            print(Fore.YELLOW + Style.BRIGHT + "\nQuick Training Features:")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Optimized parameters for fast training")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Lightweight model for rapid validation")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Performance metrics and system validation")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Basic functionality and compatibility testing")
            print(Fore.GREEN + Style.BRIGHT + "  └─ Immediate feedback with training results")
            
            # Display current configuration with enhanced styling
            print(Fore.YELLOW + Style.BRIGHT + "\nQuick Training Configurations:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + Style.BRIGHT + f"{quick_model_type}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.GREEN + Style.BRIGHT + f"{quick_epochs}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + Style.BRIGHT + f"{quick_batch_size}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + Style.BRIGHT + f"{quick_learning_rate}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + Style.BRIGHT + f"{'Real Data' if use_real_data else f'{quick_normal_samples + quick_attack_samples} synthetic samples'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Features: " + Fore.GREEN + Style.BRIGHT + f"{quick_features}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Encoding Dimension: " + Fore.GREEN + Style.BRIGHT + f"{quick_encoding_dim}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Device: " + Fore.GREEN + Style.BRIGHT + f"{device}")
            
            estimated_time = (quick_epochs * 0.5) if torch.cuda.is_available() else (quick_epochs * 2.0)
            print(Fore.YELLOW + Style.BRIGHT + "\nPerformance Settings:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Fast Mode: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if fast_mode else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Save Results: " + Fore.GREEN + Style.BRIGHT + f"{'Yes' if save_results else 'No'}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + Style.BRIGHT + f"~{estimated_time:.1f} minutes")
            
            print(Fore.YELLOW + Style.BRIGHT + "\nOptimizations:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Minimal Logging: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if minimal_logging else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Skip Validation: " + Fore.GREEN + Style.BRIGHT + f"{'Yes' if skip_validation else 'No'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ TensorBoard Logging: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if tensorboard_logging else 'Disabled'}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Benchmark Mode: " + Fore.GREEN + Style.BRIGHT + f"{'Enabled' if benchmark_mode else 'Disabled'}")
            
            # Configuration options
            print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration Options:")
            print(Fore.WHITE + Style.BRIGHT + "1. Run with current settings")
            print(Fore.WHITE + Style.BRIGHT + "2. Modify training parameters")
            print(Fore.WHITE + Style.BRIGHT + "3. Change model type")
            print(Fore.WHITE + Style.BRIGHT + "4. Adjust data settings")
            print(Fore.WHITE + Style.BRIGHT + "5. Toggle optimizations")
            print(Fore.RED + Style.BRIGHT + "0. Cancel and return to main menu")
            
            while True:
                try:
                    choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-5): " + Style.RESET_ALL).strip()
                    if choice in ['0', '1', '2', '3', '4', '5']:
                        break
                    print(Fore.RED + Style.BRIGHT + "Invalid choice. Please select 0-5.")
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                    return {'success': False, 'cancelled': True, 'message': 'Quick training cancelled by user'}
            
            if choice == '0':
                print(Fore.RED + Style.BRIGHT + "Quick training cancelled by user")
                return {'success': False, 'cancelled': True, 'message': 'Quick training cancelled by user'}
            
            elif choice == '2':
                print(Fore.YELLOW + Style.BRIGHT + "\nTRAINING PARAMETERS")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                # Epochs
                new_epochs = input(Fore.YELLOW + Style.BRIGHT + f"Epochs ({quick_epochs}): " + Style.RESET_ALL).strip()
                if new_epochs:
                    try:
                        quick_epochs = int(new_epochs)
                        quick_training_config['quick_epochs'] = quick_epochs
                        print(Fore.GREEN + Style.BRIGHT + f"Updated epochs to {quick_epochs}")
                    except ValueError:
                        print(Fore.RED + Style.BRIGHT + "Invalid input, keeping current value")
                
                # Batch size
                new_batch_size = input(Fore.YELLOW + Style.BRIGHT + f"Batch size ({quick_batch_size}): " + Style.RESET_ALL).strip()
                if new_batch_size:
                    try:
                        quick_batch_size = int(new_batch_size)
                        quick_training_config['quick_batch_size'] = quick_batch_size
                        print(Fore.GREEN + Style.BRIGHT + f"Updated batch size to {quick_batch_size}")
                    except ValueError:
                        print(Fore.RED + Style.BRIGHT + "Invalid input, keeping current value")
                
                # Learning rate
                new_lr = input(Fore.YELLOW + Style.BRIGHT + f"Learning rate ({quick_learning_rate}): " + Style.RESET_ALL).strip()
                if new_lr:
                    try:
                        quick_learning_rate = float(new_lr)
                        quick_training_config['quick_learning_rate'] = quick_learning_rate
                        print(Fore.GREEN + Style.BRIGHT + f"Updated learning rate to {quick_learning_rate}")
                    except ValueError:
                        print(Fore.RED + Style.BRIGHT + "Invalid input, keeping current value")
                
                # Encoding dimension
                new_encoding = input(Fore.YELLOW + Style.BRIGHT + f"Encoding dimension ({quick_encoding_dim}): " + Style.RESET_ALL).strip()
                if new_encoding:
                    try:
                        quick_encoding_dim = int(new_encoding)
                        quick_training_config['quick_encoding_dim'] = quick_encoding_dim
                        print(Fore.GREEN + Style.BRIGHT + f"Updated encoding dimension to {quick_encoding_dim}")
                    except ValueError:
                        print(Fore.RED + Style.BRIGHT + "Invalid input, keeping current value")
            
            elif choice == '3':
                print(Fore.YELLOW + Style.BRIGHT + "\nMODEL TYPE SELECTION")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                for i, mt in enumerate(model_types, 1):
                    current = Fore.GREEN + " (current)" if mt == quick_model_type else ""
                    if mt == 'SimpleAutoencoder':
                        desc = Fore.CYAN + " - Fastest, lightweight"
                    elif mt == 'EnhancedAutoencoder':
                        desc = Fore.CYAN + " - Balanced performance"
                    else:
                        desc = Fore.CYAN + " - Best accuracy, slower"
                    print(Fore.WHITE + Style.BRIGHT + f"{i}. {mt}{current}{desc}")
                
                model_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select model type (1-{len(model_types)}, Enter to keep current): " + Style.RESET_ALL).strip()
                if model_choice and model_choice.isdigit():
                    idx = int(model_choice) - 1
                    if 0 <= idx < len(model_types):
                        quick_model_type = model_types[idx]
                        quick_training_config['quick_model_type'] = quick_model_type
                        
                        # Adjust defaults for model type
                        if quick_model_type == 'SimpleAutoencoder':
                            quick_training_config['quick_encoding_dim'] = 8
                            quick_encoding_dim = 8
                        elif quick_model_type == 'EnhancedAutoencoder':
                            quick_training_config['quick_encoding_dim'] = 16
                            quick_encoding_dim = 16
                        else:  # AutoencoderEnsemble
                            quick_training_config['quick_encoding_dim'] = 12
                            quick_encoding_dim = 12
                        
                        print(Fore.GREEN + Style.BRIGHT + f"Updated model type to {quick_model_type}")
                        print(Fore.GREEN + Style.BRIGHT + f"Adjusted encoding dimension to {quick_encoding_dim}")
            
            elif choice == '4':
                print(Fore.YELLOW + Style.BRIGHT + "\nDATA SETTINGS")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                # Real vs synthetic data
                use_real_choice = input(Fore.YELLOW + Style.BRIGHT + f"Use real data? (y/N, current: {'Yes' if use_real_data else 'No'}): " + Style.RESET_ALL).strip().lower()
                if use_real_choice in ['y', 'yes']:
                    use_real_data = True
                    quick_training_config['use_real_data'] = True
                    print(Fore.GREEN + Style.BRIGHT + "Switched to real data mode")
                elif use_real_choice in ['n', 'no']:
                    use_real_data = False
                    quick_training_config['use_real_data'] = False
                    print(Fore.GREEN + Style.BRIGHT + "Using synthetic data")
                
                if not use_real_data:
                    # Synthetic data parameters
                    new_normal = input(Fore.YELLOW + Style.BRIGHT + f"Normal samples ({quick_normal_samples}): " + Style.RESET_ALL).strip()
                    if new_normal:
                        try:
                            quick_normal_samples = int(new_normal)
                            quick_training_config['quick_normal_samples'] = quick_normal_samples
                            print(Fore.GREEN + Style.BRIGHT + f"Updated normal samples to {quick_normal_samples}")
                        except ValueError:
                            print(Fore.RED + Style.BRIGHT + "Invalid input, keeping current value")
                    
                    new_attack = input(Fore.YELLOW + Style.BRIGHT + f"Attack samples ({quick_attack_samples}): " + Style.RESET_ALL).strip()
                    if new_attack:
                        try:
                            quick_attack_samples = int(new_attack)
                            quick_training_config['quick_attack_samples'] = quick_attack_samples
                            print(Fore.GREEN + Style.BRIGHT + f"Updated attack samples to {quick_attack_samples}")
                        except ValueError:
                            print(Fore.RED + Style.BRIGHT + "Invalid input, keeping current value")
                    
                    new_features = input(Fore.YELLOW + Style.BRIGHT + f"Features ({quick_features}): " + Style.RESET_ALL).strip()
                    if new_features:
                        try:
                            quick_features = int(new_features)
                            quick_training_config['quick_features'] = quick_features
                            print(Fore.GREEN + Style.BRIGHT + f"Updated features to {quick_features}")
                        except ValueError:
                            print(Fore.RED + Style.BRIGHT + "Invalid input, keeping current value")
            
            elif choice == '5':
                print(Fore.YELLOW + Style.BRIGHT + "\nOPTIMIZATION SETTINGS")
                print(Fore.CYAN + Style.BRIGHT + "-" * 40)
                
                # Fast mode
                fast_choice = input(Fore.YELLOW + Style.BRIGHT + f"Fast mode? (Y/n, current: {'Yes' if fast_mode else 'No'}): " + Style.RESET_ALL).strip().lower()
                if fast_choice in ['', 'y', 'yes']:
                    fast_mode = True
                    quick_training_config['fast_mode'] = True
                elif fast_choice in ['n', 'no']:
                    fast_mode = False
                    quick_training_config['fast_mode'] = False
                print(Fore.GREEN + Style.BRIGHT + f"Fast mode: {'Enabled' if fast_mode else 'Disabled'}")
                
                # Minimal logging
                log_choice = input(Fore.YELLOW + Style.BRIGHT + f"Minimal logging? (Y/n, current: {'Yes' if minimal_logging else 'No'}): " + Style.RESET_ALL).strip().lower()
                if log_choice in ['', 'y', 'yes']:
                    minimal_logging = True
                    quick_training_config['minimal_logging'] = True
                elif log_choice in ['n', 'no']:
                    minimal_logging = False
                    quick_training_config['minimal_logging'] = False
                print(Fore.GREEN + Style.BRIGHT + f"Minimal logging: {'Enabled' if minimal_logging else 'Disabled'}")
                
                # TensorBoard logging
                tb_choice = input(Fore.YELLOW + Style.BRIGHT + f"TensorBoard logging? (y/N, current: {'Yes' if tensorboard_logging else 'No'}): " + Style.RESET_ALL).strip().lower()
                if tb_choice in ['y', 'yes']:
                    tensorboard_logging = True
                    quick_training_config['tensorboard_logging'] = True
                elif tb_choice in ['', 'n', 'no']:
                    tensorboard_logging = False
                    quick_training_config['tensorboard_logging'] = False
                print(Fore.GREEN + Style.BRIGHT + f"TensorBoard logging: {'Enabled' if tensorboard_logging else 'Disabled'}")
                
                # Save results
                save_choice = input(Fore.YELLOW + Style.BRIGHT + f"Save results? (Y/n, current: {'Yes' if save_results else 'No'}): " + Style.RESET_ALL).strip().lower()
                if save_choice in ['', 'y', 'yes']:
                    save_results = True
                    quick_training_config['save_results'] = True
                elif save_choice in ['n', 'no']:
                    save_results = False
                    quick_training_config['save_results'] = False
                print(Fore.GREEN + Style.BRIGHT + f"Save results: {'Enabled' if save_results else 'Disabled'}")
                
                # Skip validation
                skip_choice = input(Fore.YELLOW + Style.BRIGHT + f"Skip validation? (y/N, current: {'Yes' if skip_validation else 'No'}): " + Style.RESET_ALL).strip().lower()
                if skip_choice in ['y', 'yes']:
                    skip_validation = True
                    quick_training_config['skip_validation'] = True
                elif skip_choice in ['', 'n', 'no']:
                    skip_validation = False
                    quick_training_config['skip_validation'] = False
                print(Fore.GREEN + Style.BRIGHT + f"Skip validation: {'Enabled' if skip_validation else 'Disabled'}")
            
            # Final confirmation
            print(Fore.YELLOW + Style.BRIGHT + f"\nUpdated Quick Training Configuration:")
            print(Fore.CYAN + Style.BRIGHT + "-" * 40)
            
            print(Fore.YELLOW + Style.BRIGHT + "Core Settings:")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model Type: " + Fore.GREEN + Style.BRIGHT + f"{quick_model_type}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.GREEN + Style.BRIGHT + f"{quick_epochs}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + Style.BRIGHT + f"{quick_batch_size}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + Style.BRIGHT + f"{quick_learning_rate}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + Style.BRIGHT + f"{'Real Data' if use_real_data else f'{quick_normal_samples + quick_attack_samples} synthetic samples'}")
            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Features: " + Fore.GREEN + Style.BRIGHT + f"{quick_features}")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Encoding Dimension: " + Fore.GREEN + Style.BRIGHT + f"{quick_encoding_dim}")
            
            estimated_time = (quick_epochs * 0.5) if torch.cuda.is_available() else (quick_epochs * 2.0)
            print(Fore.YELLOW + Style.BRIGHT + "\nPerformance:")
            print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + Style.BRIGHT + f"~{estimated_time:.1f} minutes")
            
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nProceed with quick training? (Y/n): " + Style.RESET_ALL).lower().strip()
            if confirm not in ('', 'y', 'yes'):
                print(Fore.RED + Style.BRIGHT + "Quick training cancelled by user")
                return {'success': False, 'cancelled': True, 'message': 'Quick training cancelled by user'}
        
        # Set up logging level
        # if verbose and not minimal_logging:
        #     original_level = logger.level
        #     logger.setLevel(logging.INFO)
        # elif minimal_logging:
        #     original_level = logger.level
        #     logger.setLevel(logging.WARNING)
        
        # Set up logging
        if verbose:
            handlers_to_suppress = []
            for handler in logger.handlers:
                if isinstance(handler, logging.StreamHandler) and handler.stream.name in ['<stdout>', '<stderr>']:
                    handlers_to_suppress.append(handler)
                    handler.setLevel(logging.CRITICAL)  # Temporarily suppress console output
        
        try:
            # Display quick training header with enhanced styling
            if verbose:
                
                print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration:")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{quick_model_type}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{quick_epochs}")
                print(Fore.GREEN + Style.BRIGHT + f"  ├─ Data: " + Fore.YELLOW + Style.BRIGHT + f"{quick_normal_samples + quick_attack_samples} samples, {quick_features} features")
                print(Fore.GREEN + Style.BRIGHT + f"  └─ Mode: " + Fore.YELLOW + Style.BRIGHT + f"{'Real data' if use_real_data else 'Synthetic data'}")
                print(Fore.CYAN + Style.BRIGHT + "-"*40)
            
            # Prepare comprehensive training configuration optimized for speed
            comprehensive_config = {
                # Model architecture - simplified for speed
                #'model_architecture': {
                'model': {
                    'model_type': quick_model_type,
                    'input_dim': quick_features,
                    'encoding_dim': quick_encoding_dim,
                    # Simplified architecture for quick training
                    'hidden_dims': [max(16, quick_features // 2)] if quick_model_type == 'SimpleAutoencoder' else [64, 32],
                    'dropout_rates': [0.1] if quick_model_type == 'SimpleAutoencoder' else [0.1, 0.1],
                    'activation': 'leaky_relu',
                    'normalization': None if quick_model_type == 'SimpleAutoencoder' else 'batch',
                    'skip_connection': False if quick_model_type == 'SimpleAutoencoder' else True,
                    'residual_blocks': False if quick_model_type == 'SimpleAutoencoder' else False,  # Disabled for speed
                    'use_attention': False,  # Disabled for speed in quick mode
                    'legacy_mode': False,
                    # Ensemble parameters (minimal for speed)
                    'num_models': 2 if quick_model_type == 'AutoencoderEnsemble' else None,
                    'diversity_factor': 0.1 if quick_model_type == 'AutoencoderEnsemble' else None
                },
                
                # Training configuration - optimized for speed
                #'training_config': {
                'training': {
                    'batch_size': quick_batch_size,
                    'epochs': quick_epochs,
                    'learning_rate': quick_learning_rate,
                    'patience': max(3, quick_epochs // 3),  # Early stopping for speed
                    'weight_decay': 1e-4,
                    'gradient_clip': 1.0,
                    'gradient_accumulation_steps': 1,  # No accumulation for speed
                    'mixed_precision': torch.cuda.is_available() and not benchmark_mode,  # Enable if CUDA available
                    'optimizer_type': 'AdamW',
                    'scheduler_type': 'ReduceLROnPlateau' if quick_epochs > 5 else None,  # Skip scheduler for very quick training
                    'scheduler_params': {
                        'patience': 2,
                        'factor': 0.5,
                        'min_lr': 1e-6
                    } if quick_epochs > 5 else {},
                    'early_stopping': True,
                    'validation_split': 0.2
                },
                
                # Data configuration - synthetic for speed
                #'data_config': {
                'data': {
                    'normal_samples': quick_normal_samples,
                    'attack_samples': quick_attack_samples,
                    'features': quick_features,
                    'use_real_data': use_real_data,
                    'data_preprocessing': not fast_mode,  # Skip preprocessing in fast mode
                    'normalization_method': 'standard' if not fast_mode else 'none',
                    'synthetic_config': {
                        'generation_method': 'mixed',
                        'noise_level': 0.01,
                        'anomaly_factor': 1.5,  # Moderate anomaly factor for quick convergence
                        'validation_split': 0.2,
                        'test_split': 1.0,  # Use all attack data for testing
                        'shuffle': True
                    }
                },
                
                # Security configuration - simplified for speed
                #'security_config': {
                'security': {
                    'percentile': 95.0,  # Standard percentile
                    'attack_threshold': None,  # Will be calculated
                    'false_negative_cost': 1.0,
                    'enable_security_metrics': True,
                    'threshold_method': 'percentile',  # Fast threshold method
                    'adaptive_threshold': False  # Disabled for speed
                },
                
                # System configuration - optimized for speed
                #'system_config': {
                'system': {
                    'model_dir': Path(model_dir),
                    'log_dir': Path(model_dir) / "logs",
                    'tensorboard_dir': Path(model_dir) / "tensorboard",
                    'tb_dir': Path(model_dir) / "tensorboard",
                    'config_dir': Path(model_dir) / "config",
                    'checkpoint_dir': Path(model_dir) / "checkpoints",
                    'device': device,
                    'random_seed': random_seed,
                    'reproducible': not benchmark_mode  # Disable for benchmark mode
                },
                
                # Monitoring configuration - minimal for speed
                #'monitoring_config': {
                'monitoring': {
                    'verbose': verbose and not minimal_logging,
                    'debug_mode': False,
                    'tensorboard_dir': Path(model_dir) / "tensorboard",
                    'tb_dir': Path(model_dir) / "tensorboard",
                    'tensorboard_logging': tensorboard_logging,
                    'save_checkpoints': False if fast_mode else True,
                    'checkpoint_frequency': max(5, quick_epochs // 2),
                    'log_frequency': 1 if quick_epochs <= 10 else 2,
                    'metrics_frequency': 5,
                    'progress_bar': verbose and not minimal_logging
                },
                
                # Export configuration - minimal for speed
                #'export_config': {
                'export': {
                    'export_onnx': export_onnx,
                    'save_model': save_results,
                    'save_metadata': save_results,
                    'save_training_history': save_results and not fast_mode
                },
                
                # Advanced training - speed optimized
                'advanced_training': {
                    'num_workers': 0 if fast_mode else min(2, NUM_WORKERS),  # Minimal workers for speed
                    'pin_memory': False if fast_mode else torch.cuda.is_available(),
                    'persistent_workers': False,  # Disabled for speed
                    'compile_model': False,  # Disabled for compatibility and speed
                    'benchmark_mode': benchmark_mode,
                    'memory_efficient': True,
                    'gradient_checkpointing': False  # Disabled for speed
                },
                
                # Validation configuration - minimal for speed
                #'validation_config': {
                'validation': {
                    'calculate_detailed_metrics': not fast_mode,
                    'cross_validation': False,  # Disabled for speed
                    'cv_folds': 3
                },
                
                # Error handling - permissive for quick testing
                'error_handling': {
                    'error_handling': 'continue',  # Continue on errors for quick testing
                    'continue_on_error': True,
                    'graceful_degradation': True,
                    'fallback_mode': True
                },
                
                # Experimental features - disabled for stability and speed
                'experimental': {
                    'experimental_features': False,
                    'auto_optimize': False
                }
            }
            
            # Apply any additional configuration from parameters
            for section_name, section_config in final_config.items():
                if section_name in comprehensive_config:
                    if isinstance(section_config, dict) and isinstance(comprehensive_config[section_name], dict):
                        comprehensive_config[section_name].update(section_config)
                    else:
                        comprehensive_config[section_name] = section_config
                else:
                    comprehensive_config[section_name] = section_config
            
            # Initialize quick training statistics
            quick_stats = {
                'start_time': start_time.isoformat(),
                'mode': 'quick_training',
                'configuration': comprehensive_config,
                'quick_parameters': {
                    'epochs': quick_epochs,
                    'batch_size': quick_batch_size,
                    'learning_rate': quick_learning_rate,
                    'model_type': quick_model_type,
                    'encoding_dim': quick_encoding_dim,
                    'samples': quick_normal_samples + quick_attack_samples,
                    'features': quick_features
                },
                'optimizations': {
                    'fast_mode': fast_mode,
                    'minimal_logging': minimal_logging,
                    'skip_validation': skip_validation,
                    'benchmark_mode': benchmark_mode
                }
            }
            
            if verbose:
                logger.info("Starting quick training with optimized configuration")
                logger.info(f"Target: {quick_epochs} epochs, {quick_batch_size} batch size, {quick_learning_rate} learning rate")
            
            # Execute the comprehensive training pipeline
            training_results = train_model(config=comprehensive_config)
            
            # Calculate quick training duration
            quick_duration = time.time() - quick_start_time
            
            # Process and enhance results for quick training context
            if training_results and training_results.get('success', False):
                # Extract key metrics
                final_metrics = training_results.get('final_metrics', {})
                model_info = training_results.get('model_info', {})
                training_time = training_results.get('training_time_minutes', 0)
                
                # Prepare quick training results
                quick_results = {
                    'success': True,
                    'mode': 'quick_training',
                    'quick_training_time_minutes': quick_duration / 60,
                    'quick_stats': quick_stats,
                    
                    # Core results from comprehensive training
                    'run_id': training_results.get('run_id'),
                    'timestamp': training_results.get('timestamp'),
                    'model_type': quick_model_type,
                    'training_time_minutes': training_time,
                    
                    # Key metrics
                    'final_metrics': {
                        'best_validation_loss': final_metrics.get('best_validation_loss', float('inf')),
                        'test_loss': final_metrics.get('test_loss', float('inf')),
                        'anomaly_detection_rate': final_metrics.get('anomaly_detection_rate', 0.0),
                        'threshold': final_metrics.get('threshold', 0.0),
                        'final_epoch': final_metrics.get('final_epoch', 0),
                        'epochs_target': quick_epochs,
                        'training_completed': final_metrics.get('final_epoch', 0) > 0
                    },
                    
                    # Model information
                    'model_info': {
                        'type': quick_model_type,
                        'class_name': model_info.get('class_name', 'Unknown'),
                        'parameters': model_info.get('parameters', 0),
                        'trainable_parameters': model_info.get('trainable_parameters', 0),
                        'size_mb': model_info.get('size_mb', 0),
                        'input_dim': quick_features,
                        'encoding_dim': quick_encoding_dim
                    },
                    
                    # Data information
                    'data_info': training_results.get('data_info', {}),
                    
                    # System information
                    'system_info': training_results.get('system_info', {}),
                    
                    # Artifacts and paths
                    'artifacts': training_results.get('artifacts', {}),
                    
                    # Quick training specific metrics
                    'quick_metrics': {
                        'epochs_per_minute': final_metrics.get('final_epoch', 0) / (training_time + 0.001),
                        'samples_per_second': (quick_normal_samples + quick_attack_samples) * final_metrics.get('final_epoch', 0) / (quick_duration + 0.001),
                        'convergence_rate': 'fast' if final_metrics.get('best_validation_loss', 1.0) < 0.1 else 'moderate' if final_metrics.get('best_validation_loss', 1.0) < 0.5 else 'slow',
                        'efficiency_score': min(1.0, (quick_epochs - final_metrics.get('final_epoch', quick_epochs)) / quick_epochs + (0.5 - min(0.5, final_metrics.get('best_validation_loss', 1.0))))
                    },
                    
                    # Performance analysis
                    'performance_analysis': {
                        'training_speed': 'excellent' if training_time < 2 else 'good' if training_time < 5 else 'acceptable' if training_time < 10 else 'slow',
                        'convergence_quality': 'excellent' if final_metrics.get('best_validation_loss', 1.0) < 0.05 else 'good' if final_metrics.get('best_validation_loss', 1.0) < 0.1 else 'acceptable',
                        'resource_efficiency': 'optimal' if quick_duration < 300 else 'good' if quick_duration < 600 else 'acceptable',
                        'overall_rating': 'excellent'  # Will be calculated below
                    },
                    
                    # Recommendations for production use
                    'recommendations': [],
                    
                    # Original comprehensive results for reference
                    'comprehensive_results': training_results
                }
                
                # Calculate overall rating
                speed_score = 1.0 if training_time < 2 else 0.8 if training_time < 5 else 0.6 if training_time < 10 else 0.4
                quality_score = 1.0 if final_metrics.get('best_validation_loss', 1.0) < 0.05 else 0.8 if final_metrics.get('best_validation_loss', 1.0) < 0.1 else 0.6
                efficiency_score = quick_results['quick_metrics']['efficiency_score']
                
                overall_score = (speed_score + quality_score + efficiency_score) / 3
                
                if overall_score >= 0.9:
                    quick_results['performance_analysis']['overall_rating'] = 'excellent'
                elif overall_score >= 0.7:
                    quick_results['performance_analysis']['overall_rating'] = 'good'
                elif overall_score >= 0.5:
                    quick_results['performance_analysis']['overall_rating'] = 'acceptable'
                else:
                    quick_results['performance_analysis']['overall_rating'] = 'needs_improvement'
                
                # Generate recommendations
                recommendations = []
                
                if final_metrics.get('best_validation_loss', 1.0) > 0.1:
                    recommendations.append("Consider increasing epochs or adjusting learning rate for better convergence")
                
                if training_time > 10:
                    recommendations.append("Training time is high - consider using GPU acceleration or reducing model complexity")
                
                if final_metrics.get('final_epoch', 0) < quick_epochs * 0.5:
                    recommendations.append("Training converged early - model may be too simple or learning rate too high")
                
                if final_metrics.get('anomaly_detection_rate', 0) < 0.05:
                    recommendations.append("Low anomaly detection rate - consider adjusting threshold or model parameters")
                elif final_metrics.get('anomaly_detection_rate', 0) > 0.3:
                    recommendations.append("High anomaly detection rate - threshold may be too sensitive")
                
                if model_info.get('parameters', 0) > 100000:
                    recommendations.append("Large model detected - consider simplifying architecture for production deployment")
                
                if not torch.cuda.is_available():
                    recommendations.append("GPU acceleration not available - consider enabling CUDA for better performance")
                
                quick_results['recommendations'] = recommendations
                
                # Display quick training results with enhanced styling
                if verbose:
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nTraining Summary:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{quick_model_type}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('final_epoch', 0)}/{quick_epochs}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Duration: " + Fore.YELLOW + Style.BRIGHT + f"{training_time:.1f} minutes")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Status: " + Fore.GREEN + Style.BRIGHT + f"COMPLETED")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nKey Metrics:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Best Validation Loss: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('best_validation_loss', 0):.6f}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Test Loss: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('test_loss', 0):.6f}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Anomaly Threshold: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('threshold', 0):.6f}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Detection Rate: " + Fore.YELLOW + Style.BRIGHT + f"{final_metrics.get('anomaly_detection_rate', 0)*100:.1f}%")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nPerformance Analysis:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Training Speed: " + Fore.YELLOW + Style.BRIGHT + f"{quick_results['performance_analysis']['training_speed'].title()}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Convergence Quality: " + Fore.YELLOW + Style.BRIGHT + f"{quick_results['performance_analysis']['convergence_quality'].title()}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Overall Rating: " + Fore.YELLOW + Style.BRIGHT + f"{quick_results['performance_analysis']['overall_rating'].title()}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nModel Information:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Parameters: " + Fore.YELLOW + Style.BRIGHT + f"{model_info.get('parameters', 0):,}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model Size: " + Fore.YELLOW + Style.BRIGHT + f"{model_info.get('size_mb', 0):.1f} MB")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Architecture: " + Fore.YELLOW + Style.BRIGHT + f"{quick_features} → {quick_encoding_dim}")
                    
                    # Artifacts
                    artifacts = training_results.get('artifacts', {})
                    if artifacts:
                        print(Fore.YELLOW + Style.BRIGHT + "\nSaved Artifacts:")
                        for artifact_type, path in artifacts.items():
                            if artifact_type in ['model_path', 'threshold_path', 'metadata_path']:
                                print(Fore.GREEN + Style.BRIGHT + f"  ├─ {artifact_type.replace('_', ' ').title()}: " + Fore.YELLOW + Style.BRIGHT + f"{path}")
                    
                    # Recommendations
                    if recommendations:
                        print(Fore.YELLOW + Style.BRIGHT + "\nRecommendations for Improvement:")
                        for i, rec in enumerate(recommendations, 1):
                            print(Fore.CYAN + Style.BRIGHT + f"{i}. {rec}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\n" + "="*40)
                    print(Fore.GREEN + Style.BRIGHT + "Quick training completed!")
                    print(Fore.GREEN + Style.BRIGHT + "Ready for production or further testing.")
                    print(Fore.YELLOW + Style.BRIGHT + "="*40)
            
            else:
                # Training failed or returned error
                error_info = training_results.get('error', 'Unknown training error') if training_results else 'No training results returned'
                
                quick_results = {
                    'success': False,
                    'mode': 'quick_training',
                    'error': error_info,
                    'error_type': training_results.get('error_type', 'Unknown') if training_results else 'Unknown',
                    'quick_training_time_minutes': quick_duration / 60,
                    'quick_stats': quick_stats,
                    'timestamp': start_time.isoformat(),
                    'model_type': quick_model_type,
                    'configuration': comprehensive_config,
                    'partial_results': training_results.get('partial_results', {}) if training_results else {},
                    'recommendations': [
                        "Check system compatibility and dependencies",
                        "Try running stability test first: run_stability_test()",
                        "Consider using different model parameters",
                        "Verify data generation is working correctly"
                    ]
                }
                
                if verbose:
                    
                    print(Fore.RED + Style.BRIGHT + "\nError Details:")
                    print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Error: " + Fore.RED + Style.BRIGHT + f"{error_info}")
                    print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Duration: " + Fore.RED + Style.BRIGHT + f"{quick_duration/60:.1f} minutes")
                    print(Fore.YELLOW + Style.BRIGHT + f"  └─ Model Type: " + Fore.RED + Style.BRIGHT + f"{quick_model_type}")
                    
                    if training_results and training_results.get('partial_results'):
                        partial = training_results['partial_results']
                        print(Fore.RED + Style.BRIGHT + "\nPartial Results:")
                        if partial.get('epochs_completed', 0) > 0:
                            print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Epochs Completed: " + Fore.RED + Style.BRIGHT + f"{partial.get('epochs_completed', 0)}")
                            print(Fore.YELLOW + Style.BRIGHT + f"  └─ Last Training Loss: " + Fore.RED + Style.BRIGHT + f"{partial.get('last_train_loss', 'N/A')}")
                    
                    for i, rec in enumerate(quick_results['recommendations'], 1):
                        print(Fore.RED + Style.BRIGHT + f"{i}. {rec}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "="*40)
            
            # Save quick training results if requested
            if save_results:
                try:
                    results_dir = Path(model_dir)
                    results_dir.mkdir(parents=True, exist_ok=True)
                    
                    quick_results_path = results_dir / f"quick_training_results_{start_time.strftime('%Y%m%d_%H%M%S')}.json"
                    with open(quick_results_path, 'w') as f:
                        json.dump(quick_results, f, indent=2, default=str)
                    
                    quick_results['quick_results_path'] = str(quick_results_path)
                    
                    if verbose:
                        print(Fore.GREEN + Style.BRIGHT + f"\nQuick training results saved to: {quick_results_path}")
                
                except Exception as e:
                    logger.warning(f"Failed to save quick training results: {e}")
                    quick_results.setdefault('warnings', []).append(f"Failed to save results: {e}")
            
            return quick_results
            
        except Exception as e:
            # Handle unexpected errors in quick training
            error_msg = f"Quick training failed: {str(e)}"
            logger.error(error_msg)
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            quick_duration = time.time() - quick_start_time
            
            error_results = {
                'success': False,
                'mode': 'quick_training',
                'error': error_msg,
                'error_type': type(e).__name__,
                'traceback': traceback.format_exc(),
                'quick_training_time_minutes': quick_duration / 60,
                'timestamp': start_time.isoformat(),
                'model_type': quick_model_type,
                'configuration': locals().get('comprehensive_config', {}),
                'quick_parameters': {
                    'epochs': quick_epochs,
                    'batch_size': quick_batch_size,
                    'learning_rate': quick_learning_rate,
                    'model_type': quick_model_type,
                    'encoding_dim': quick_encoding_dim,
                    'samples': quick_normal_samples + quick_attack_samples,
                    'features': quick_features
                },
                'system_info': {
                    'platform': sys.platform,
                    'python_version': sys.version.split()[0],
                    'pytorch_version': torch.__version__,
                    'cuda_available': torch.cuda.is_available()
                },
                'recommendations': [
                    "Run stability test to check system compatibility: run_stability_test()",
                    "Try with simpler parameters: fewer epochs, smaller model",
                    "Check error details in traceback for specific issues",
                    "Verify all dependencies are properly installed",
                    "Try running train_model() with explicit configuration"
                ]
            }
            
            if verbose:
                
                print(Fore.RED + Style.BRIGHT + "Error Details:")
                print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Error: " + Fore.RED + Style.BRIGHT + f"{str(e)}")
                print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Duration: " + Fore.RED + Style.BRIGHT + f"{quick_duration/60:.1f} minutes")
                print(Fore.YELLOW + Style.BRIGHT + f"  └─ Error Type: " + Fore.RED + Style.BRIGHT + f"{type(e).__name__}")
                
                for i, rec in enumerate(error_results['recommendations'], 1):
                    print(Fore.RED + Style.BRIGHT + f"{i}. {rec}")
                
                print(Fore.YELLOW + Style.BRIGHT + "\nFor detailed error information,")
                print(Fore.YELLOW + Style.BRIGHT + "check the returned error_results.")
                print(Fore.RED + Style.BRIGHT + "="*40)
            
            # Save error results if possible
            if save_results:
                try:
                    results_dir = Path(model_dir)
                    results_dir.mkdir(parents=True, exist_ok=True)
                    
                    error_path = results_dir / f"quick_training_error_{start_time.strftime('%Y%m%d_%H%M%S')}.json"
                    with open(error_path, 'w') as f:
                        json.dump(error_results, f, indent=2, default=str)
                    
                    error_results['error_log_path'] = str(error_path)
                
                except Exception as save_error:
                    logger.warning(f"Failed to save error results: {save_error}")
            
            return error_results
        
        finally:
            # Restore logging level
            # if (verbose or minimal_logging) and 'original_level' in locals():
            #     try:
            #         logger.setLevel(original_level)
            #     except Exception:
            #         pass
            
            # Restore logging level
            if (verbose or minimal_logging) and 'original_level' in locals():
                try:
                    for handler in handlers_to_suppress:
                        handler.setLevel(logging.ERROR)
                except Exception:
                    pass
            
            # Final cleanup
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
            except Exception:
                pass
    
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\n\nQuick training interrupted by user!")
        return {'success': False, 'cancelled': True, 'message': 'Quick training interrupted by user'}
    except Exception as e:
        logger.error(f"Quick training setup failed: {e}", exc_info=True)
        message = (
            f"Error encountered during quick training setup: {str(e)}\n"
            f"Context:\n"
            f"- Quick Epochs: {quick_epochs}\n"
            f"- Model Type: {quick_model_type}\n"
            f"- Interactive Mode: {interactive}\n\n"
            f"This could be due to:\n"
            f"- Configuration file corruption\n"
            f"- Missing dependencies\n"
            f"- System resource issues\n"
            f"- Invalid parameter combinations"
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="QUICK TRAINING SETUP ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        return {'success': False, 'error': str(e), 'message': 'Quick training setup failed'}

def train_model_custom(config: Optional[Dict[str, Any]] = None):
    """Custom model training configuration with context display and error handling."""
    try:
        # Clear screen and show banner with config
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Extract configuration context with error handling
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu header with context
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "CUSTOM TRAINING CONFIGURATION")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + f"Active Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}")
        
        base_config = {}
        try:
            base_config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception as e:
            logger.warning(f"Failed to load current config: {e}")
            console.print(
                Panel.fit(
                    #f"Warning: Failed to load current config, using defaults: {str(e)}",
                    f"Failed to load current config, using defaults: {str(e)}",
                    title="WARNING",
                    style="bold yellow",
                    border_style="yellow",
                    padding=(1, 1),
                    box=box.ROUNDED
                )
            )
            base_config = {}
        
        print(Fore.WHITE + Style.BRIGHT + "Configuration includes:")
        print(Fore.CYAN + Style.BRIGHT + "  ├─ Model architecture parameters")
        print(Fore.CYAN + Style.BRIGHT + "  ├─ Training hyperparameters")
        print(Fore.CYAN + Style.BRIGHT + "  ├─ Data configuration")
        print(Fore.CYAN + Style.BRIGHT + "  ├─ System and hardware settings")
        print(Fore.CYAN + Style.BRIGHT + "  └─ Security and monitoring options")
        
        print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration Sections:")
        print(Fore.WHITE + Style.BRIGHT + "1. Model Architecture " + Fore.GREEN + Style.BRIGHT + "(Layers, Activation, Normalization)")
        print(Fore.WHITE + Style.BRIGHT + "2. Training Parameters " + Fore.GREEN + Style.BRIGHT + "(Optimizer, Scheduler, Regularization)")
        print(Fore.WHITE + Style.BRIGHT + "3. Data Configuration " + Fore.GREEN + Style.BRIGHT + "(Source, Preprocessing, Splits)")
        print(Fore.WHITE + Style.BRIGHT + "4. System & Hardware " + Fore.GREEN + Style.BRIGHT + "(Device, Memory, Performance)")
        print(Fore.WHITE + Style.BRIGHT + "5. Security & Anomaly Detection " + Fore.GREEN + Style.BRIGHT + "(Thresholds, Metrics)")
        print(Fore.WHITE + Style.BRIGHT + "6. Monitoring & Logging " + Fore.GREEN + Style.BRIGHT + "(TensorBoard, Checkpoints)")
        print(Fore.WHITE + Style.BRIGHT + "7. Advanced Options " + Fore.GREEN + Style.BRIGHT + "(Experimental Features)")
        print(Fore.WHITE + Style.BRIGHT + "8. Complete Configuration Review " + Fore.GREEN + Style.BRIGHT + "(Summary & Validation)")
        print(Fore.WHITE + Style.BRIGHT + "9. Start Training " + Fore.GREEN + Style.BRIGHT + "(Launch with Current Settings)")
        print(Fore.RED + Style.BRIGHT + "0. Exit " + Fore.GREEN + Style.BRIGHT + "(Return to Previous Menu)")
        
        final_config = base_config.copy()
        
        while True:
            try:
                section_choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect section to configure (0-9): " + Style.RESET_ALL).strip()
                
                if section_choice == '0':
                    print(Fore.RED + Style.BRIGHT + "Custom training cancelled")
                    return None
                
                elif section_choice == '1':
                    print(Fore.YELLOW + Style.BRIGHT + "\nMODEL ARCHITECTURE CONFIGURATION")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    
                    model_config = final_config.setdefault('model', {})
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nModel Type:")
                    model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                    for i, mtype in enumerate(model_types, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if mtype == model_config.get('model_type', 'EnhancedAutoencoder') else ""
                        if mtype == 'SimpleAutoencoder':
                            desc = Fore.BLUE + Style.BRIGHT + " - Fastest, lightweight"
                        elif mtype == 'EnhancedAutoencoder':
                            desc = Fore.CYAN + Style.BRIGHT + " - Balanced performance"
                        else:
                            desc = Fore.MAGENTA + Style.BRIGHT + " - Best accuracy, slower"
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {mtype}{current}{desc}")
                    
                    while True:
                        try:
                            model_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select model type (1-{len(model_types)}, default=2): " + Style.RESET_ALL).strip()
                            if model_choice in ['', '1', '2', '3']:
                                break
                            print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(model_types)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    model_type = model_types[int(model_choice)-1] if model_choice else 'EnhancedAutoencoder'
                    model_config['model_type'] = model_type
                    
                    print(Fore.CYAN + Style.BRIGHT + f"\nArchitecture for {model_type}:")
                    
                    encoding_dim = input(Fore.YELLOW + Style.BRIGHT + "Encoding dimension (16/32/24 for Simple/Enhanced/Ensemble): " + Style.RESET_ALL).strip()
                    if encoding_dim:
                        model_config['encoding_dim'] = int(encoding_dim)
                    elif model_type == 'SimpleAutoencoder':
                        model_config['encoding_dim'] = 16
                    elif model_type == 'EnhancedAutoencoder':
                        model_config['encoding_dim'] = 32
                    elif model_type == 'AutoencoderEnsemble':
                        model_config['encoding_dim'] = 24
                    
                    hidden_dims_input = input(Fore.YELLOW + Style.BRIGHT + "Hidden layers (comma-separated, e.g., 256,128,64): " + Style.RESET_ALL).strip()
                    if hidden_dims_input:
                        model_config['hidden_dims'] = [int(x.strip()) for x in hidden_dims_input.split(',')]
                    elif model_type == 'SimpleAutoencoder':
                        model_config['hidden_dims'] = [128, 64]
                    elif model_type == 'EnhancedAutoencoder':
                        model_config['hidden_dims'] = [256, 128, 64]
                    elif model_type == 'AutoencoderEnsemble':
                        model_config['hidden_dims'] = [192, 96, 48]
                    
                    dropout_input = input(Fore.YELLOW + Style.BRIGHT + "Dropout rates (comma-separated, e.g., 0.2,0.15,0.1): " + Style.RESET_ALL).strip()
                    if dropout_input:
                        model_config['dropout_rates'] = [float(x.strip()) for x in dropout_input.split(',')]
                    else:
                        hidden_len = len(model_config['hidden_dims'])
                        if hidden_len == 2:
                            model_config['dropout_rates'] = [0.2, 0.15]
                        elif hidden_len == 3:
                            model_config['dropout_rates'] = [0.2, 0.15, 0.1]
                        else:
                            model_config['dropout_rates'] = [0.2 - i*0.05 for i in range(hidden_len)]
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nActivation Functions:")
                    activations = ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid', 'swish', 'elu']
                    for i, act in enumerate(activations, 1):
                        current = Fore.GREEN + " (current)" if act == model_config.get('activation', 'leaky_relu') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {act}{current}")
                    
                    while True:
                        try:
                            act_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select activation (1-{len(activations)}, default=2): " + Style.RESET_ALL).strip()
                            if act_choice in [''] + [str(i) for i in range(1, len(activations)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(activations)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    activation = activations[int(act_choice)-1] if act_choice else 'leaky_relu'
                    model_config['activation'] = activation
                    
                    if activation == 'leaky_relu':
                        activation_param = input(Fore.YELLOW + Style.BRIGHT + "Negative slope for LeakyReLU (0.2): " + Style.RESET_ALL).strip()
                        model_config['activation_param'] = float(activation_param) if activation_param else 0.2
                    elif activation == 'elu':
                        activation_param = input(Fore.YELLOW + Style.BRIGHT + "Alpha for ELU (1.0): " + Style.RESET_ALL).strip()
                        model_config['activation_param'] = float(activation_param) if activation_param else 1.0
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nNormalization:")
                    norm_options = ['none', 'batch', 'layer', 'instance', 'group']
                    for i, norm in enumerate(norm_options, 1):
                        current = Fore.GREEN + " (current)" if norm == model_config.get('normalization', 'batch') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {norm}{current}")
                    
                    while True:
                        try:
                            norm_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select normalization (1-{len(norm_options)}, default=2): " + Style.RESET_ALL).strip()
                            if norm_choice in [''] + [str(i) for i in range(1, len(norm_options)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(norm_options)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    normalization = norm_options[int(norm_choice)-1] if norm_choice else 'batch'
                    model_config['normalization'] = normalization
                    
                    if normalization == 'batch':
                        model_config['use_batch_norm'] = True
                        model_config['use_layer_norm'] = False
                    elif normalization == 'layer':
                        model_config['use_batch_norm'] = False
                        model_config['use_layer_norm'] = True
                    else:
                        model_config['use_batch_norm'] = False
                        model_config['use_layer_norm'] = False
                    
                    bias = input(Fore.YELLOW + Style.BRIGHT + "Use bias in layers? (Y/n): " + Style.RESET_ALL).strip().lower()
                    model_config['bias'] = bias in ('', 'y', 'yes')
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nWeight Initialization:")
                    init_methods = ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'orthogonal']
                    for i, init in enumerate(init_methods, 1):
                        current = Fore.GREEN + " (current)" if init == model_config.get('weight_init', 'xavier_uniform') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {init}{current}")
                    
                    while True:
                        try:
                            init_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select initialization (1-{len(init_methods)}, default=1): " + Style.RESET_ALL).strip()
                            if init_choice in [''] + [str(i) for i in range(1, len(init_methods)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(init_methods)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    weight_init = init_methods[int(init_choice)-1] if init_choice else 'xavier_uniform'
                    model_config['weight_init'] = weight_init
                    
                    if model_type in ['EnhancedAutoencoder', 'AutoencoderEnsemble']:
                        print(Fore.CYAN + Style.BRIGHT + "\nEnhanced Features:")
                        attention = input(Fore.YELLOW + Style.BRIGHT + "Use attention mechanism? (Y/n): " + Style.RESET_ALL).strip().lower()
                        model_config['use_attention'] = attention in ('', 'y', 'yes')
                        
                        residual = input(Fore.YELLOW + Style.BRIGHT + "Use residual blocks? (Y/n): " + Style.RESET_ALL).strip().lower()
                        model_config['residual_blocks'] = residual in ('', 'y', 'yes')
                        
                        skip_conn = input(Fore.YELLOW + Style.BRIGHT + "Use skip connections? (Y/n): " + Style.RESET_ALL).strip().lower()
                        model_config['skip_connection'] = skip_conn in ('', 'y', 'yes')
                        
                        if model_type == 'EnhancedAutoencoder':
                            legacy = input(Fore.YELLOW + Style.BRIGHT + "Use legacy mode? (y/N): " + Style.RESET_ALL).strip().lower()
                            model_config['legacy_mode'] = legacy in ('y', 'yes')
                    
                    if model_type == 'AutoencoderEnsemble':
                        print(Fore.CYAN + Style.BRIGHT + "\nEnsemble Configuration:")
                        num_models = input(Fore.YELLOW + Style.BRIGHT + "Number of ensemble models (3): " + Style.RESET_ALL).strip()
                        model_config['num_models'] = int(num_models) if num_models else 3
                        
                        diversity = input(Fore.YELLOW + Style.BRIGHT + "Diversity factor (0.3): " + Style.RESET_ALL).strip()
                        model_config['diversity_factor'] = float(diversity) if diversity else 0.3
                    
                    min_features = input(Fore.YELLOW + Style.BRIGHT + "Minimum features validation (5): " + Style.RESET_ALL).strip()
                    model_config['min_features'] = int(min_features) if min_features else 5
                    
                    print(Fore.GREEN + Style.BRIGHT + f"\nModel configuration updated for {model_type}")
                
                elif section_choice == '2':
                    print(Fore.YELLOW + Style.BRIGHT + "\nTRAINING PARAMETERS CONFIGURATION")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    
                    training_config = final_config.setdefault('training', {})
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nBasic Training Parameters:")
                    epochs = input(Fore.YELLOW + Style.BRIGHT + "Number of epochs (50): " + Style.RESET_ALL).strip()
                    training_config['epochs'] = int(epochs) if epochs else 50
                    
                    batch_size = input(Fore.YELLOW + Style.BRIGHT + "Batch size (64): " + Style.RESET_ALL).strip()
                    training_config['batch_size'] = int(batch_size) if batch_size else 64
                    
                    lr = input(Fore.YELLOW + Style.BRIGHT + "Learning rate (0.001): " + Style.RESET_ALL).strip()
                    training_config['learning_rate'] = float(lr) if lr else 0.001
                    
                    patience = input(Fore.YELLOW + Style.BRIGHT + "Early stopping patience (15): " + Style.RESET_ALL).strip()
                    training_config['patience'] = int(patience) if patience else 15
                    
                    validation_split = input(Fore.YELLOW + Style.BRIGHT + "Validation split (0.2): " + Style.RESET_ALL).strip()
                    training_config['validation_split'] = float(validation_split) if validation_split else 0.2
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nRegularization:")
                    weight_decay = input(Fore.YELLOW + Style.BRIGHT + "Weight decay (1e-4): " + Style.RESET_ALL).strip()
                    training_config['weight_decay'] = float(weight_decay) if weight_decay else 1e-4
                    
                    gradient_clip = input(Fore.YELLOW + Style.BRIGHT + "Gradient clipping threshold (1.0): " + Style.RESET_ALL).strip()
                    training_config['gradient_clip'] = float(gradient_clip) if gradient_clip else 1.0
                    
                    grad_accum = input(Fore.YELLOW + Style.BRIGHT + "Gradient accumulation steps (1): " + Style.RESET_ALL).strip()
                    training_config['gradient_accumulation_steps'] = int(grad_accum) if grad_accum else 1
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nOptimizer Configuration:")
                    optimizers = ['AdamW', 'Adam', 'SGD', 'RMSprop', 'Adagrad']
                    for i, opt in enumerate(optimizers, 1):
                        current = Fore.GREEN + " (current)" if opt == training_config.get('optimizer', 'AdamW') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {opt}{current}")
                    
                    while True:
                        try:
                            opt_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select optimizer (1-{len(optimizers)}, default=1): " + Style.RESET_ALL).strip()
                            if opt_choice in [''] + [str(i) for i in range(1, len(optimizers)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(optimizers)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    optimizer = optimizers[int(opt_choice)-1] if opt_choice else 'AdamW'
                    training_config['optimizer'] = optimizer
                    
                    if optimizer in ['AdamW', 'Adam']:
                        adam_betas_input = input(Fore.YELLOW + Style.BRIGHT + "Adam betas (0.9,0.999): " + Style.RESET_ALL).strip()
                        if adam_betas_input:
                            betas = [float(x.strip()) for x in adam_betas_input.split(',')]
                            training_config['adam_betas'] = tuple(betas)
                        else:
                            training_config['adam_betas'] = (0.9, 0.999)
                        
                        adam_eps = input(Fore.YELLOW + Style.BRIGHT + "Adam epsilon (1e-8): " + Style.RESET_ALL).strip()
                        training_config['adam_eps'] = float(adam_eps) if adam_eps else 1e-8
                    elif optimizer == 'SGD':
                        momentum = input(Fore.YELLOW + Style.BRIGHT + "SGD momentum (0.9): " + Style.RESET_ALL).strip()
                        training_config['momentum'] = float(momentum) if momentum else 0.9
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nLearning Rate Scheduler:")
                    scheduler_options = ['None', 'ReduceLROnPlateau', 'StepLR', 'CosineAnnealingLR', 'ExponentialLR', 'OneCycleLR']
                    for i, sched in enumerate(scheduler_options, 1):
                        current = Fore.GREEN + " (current)" if sched == training_config.get('scheduler', 'ReduceLROnPlateau') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {sched}{current}")
                    
                    while True:
                        try:
                            sched_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select scheduler (1-{len(scheduler_options)}, default=2): " + Style.RESET_ALL).strip()
                            if sched_choice in [''] + [str(i) for i in range(1, len(scheduler_options)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(scheduler_options)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    if sched_choice == '1':
                        training_config['scheduler'] = None
                    else:
                        scheduler_type = scheduler_options[int(sched_choice)-1] if sched_choice else 'ReduceLROnPlateau'
                        training_config['scheduler'] = scheduler_type
                        
                        scheduler_params = {}
                        if scheduler_type == 'ReduceLROnPlateau':
                            lr_patience = input(Fore.YELLOW + Style.BRIGHT + "LR scheduler patience (5): " + Style.RESET_ALL).strip()
                            lr_factor = input(Fore.YELLOW + Style.BRIGHT + "LR reduction factor (0.5): " + Style.RESET_ALL).strip()
                            min_lr = input(Fore.YELLOW + Style.BRIGHT + "Minimum learning rate (1e-6): " + Style.RESET_ALL).strip()
                            
                            scheduler_params = {
                                'patience': int(lr_patience) if lr_patience else 5,
                                'factor': float(lr_factor) if lr_factor else 0.5,
                                'min_lr': float(min_lr) if min_lr else 1e-6
                            }
                        elif scheduler_type == 'StepLR':
                            step_size = input(Fore.YELLOW + Style.BRIGHT + "Step size (30): " + Style.RESET_ALL).strip()
                            gamma = input(Fore.YELLOW + Style.BRIGHT + "Gamma (0.1): " + Style.RESET_ALL).strip()
                            
                            scheduler_params = {
                                'step_size': int(step_size) if step_size else 30,
                                'gamma': float(gamma) if gamma else 0.1
                            }
                        elif scheduler_type == 'CosineAnnealingLR':
                            t_max = input(Fore.YELLOW + Style.BRIGHT + f"T_max ({training_config['epochs']}): " + Style.RESET_ALL).strip()
                            eta_min = input(Fore.YELLOW + Style.BRIGHT + "Eta min (1e-7): " + Style.RESET_ALL).strip()
                            
                            scheduler_params = {
                                'T_max': int(t_max) if t_max else training_config['epochs'],
                                'eta_min': float(eta_min) if eta_min else 1e-7
                            }
                        elif scheduler_type == 'ExponentialLR':
                            gamma = input(Fore.YELLOW + Style.BRIGHT + "Gamma (0.95): " + Style.RESET_ALL).strip()
                            scheduler_params = {'gamma': float(gamma) if gamma else 0.95}
                        elif scheduler_type == 'OneCycleLR':
                            max_lr = input(Fore.YELLOW + Style.BRIGHT + f"Max LR ({training_config['learning_rate'] * 10}): " + Style.RESET_ALL).strip()
                            pct_start = input(Fore.YELLOW + Style.BRIGHT + "Percent start (0.3): " + Style.RESET_ALL).strip()
                            
                            scheduler_params = {
                                'max_lr': float(max_lr) if max_lr else training_config['learning_rate'] * 10,
                                'total_steps': training_config['epochs'],
                                'pct_start': float(pct_start) if pct_start else 0.3
                            }
                        
                        training_config['scheduler_params'] = scheduler_params
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nTraining Options:")
                    early_stopping = input(Fore.YELLOW + Style.BRIGHT + "Enable early stopping? (Y/n): " + Style.RESET_ALL).strip().lower()
                    training_config['early_stopping'] = early_stopping in ('', 'y', 'yes')
                    
                    shuffle = input(Fore.YELLOW + Style.BRIGHT + "Shuffle training data? (Y/n): " + Style.RESET_ALL).strip().lower()
                    training_config['shuffle'] = shuffle in ('', 'y', 'yes')
                    
                    if torch.cuda.is_available():
                        mixed_prec = input(Fore.YELLOW + Style.BRIGHT + "Use mixed precision training? (Y/n): " + Style.RESET_ALL).strip().lower()
                        training_config['mixed_precision'] = mixed_prec in ('', 'y', 'yes')
                    else:
                        training_config['mixed_precision'] = False
                    
                    print(Fore.GREEN + Style.BRIGHT + "Training configuration updated")
                
                elif section_choice == '3':
                    print(Fore.YELLOW + Style.BRIGHT + "\nDATA CONFIGURATION")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    
                    data_config = final_config.setdefault('data', {})
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nData Source:")
                    use_real_data = input(Fore.YELLOW + Style.BRIGHT + "Use real network data? (y/N): " + Style.RESET_ALL).strip().lower()
                    data_config['use_real_data'] = use_real_data in ('y', 'yes')
                    
                    if data_config['use_real_data']:
                        print(Fore.CYAN + Style.BRIGHT + "\nReal Data Configuration:")
                        data_path = input(Fore.YELLOW + Style.BRIGHT + "Data file path (optional): " + Style.RESET_ALL).strip()
                        if data_path:
                            data_config['data_path'] = data_path
                        
                        artifacts_path = input(Fore.YELLOW + Style.BRIGHT + "Artifacts path (optional): " + Style.RESET_ALL).strip()
                        if artifacts_path:
                            data_config['artifacts_path'] = artifacts_path
                    else:
                        print(Fore.CYAN + Style.BRIGHT + "\nSynthetic Data Configuration:")
                        normal_samples = input(Fore.YELLOW + Style.BRIGHT + "Normal samples (8000): " + Style.RESET_ALL).strip()
                        data_config['normal_samples'] = int(normal_samples) if normal_samples else 8000
                        
                        attack_samples = input(Fore.YELLOW + Style.BRIGHT + "Attack samples (2000): " + Style.RESET_ALL).strip()
                        data_config['attack_samples'] = int(attack_samples) if attack_samples else 2000
                        
                        features = input(Fore.YELLOW + Style.BRIGHT + "Number of features (20): " + Style.RESET_ALL).strip()
                        data_config['features'] = int(features) if features else 20
                        
                        anomaly_factor = input(Fore.YELLOW + Style.BRIGHT + "Anomaly factor (0.1): " + Style.RESET_ALL).strip()
                        data_config['anomaly_factor'] = float(anomaly_factor) if anomaly_factor else 0.1
                    
                    random_state = input(Fore.YELLOW + Style.BRIGHT + "Random state (42): " + Style.RESET_ALL).strip()
                    data_config['random_state'] = int(random_state) if random_state else 42
                    
                    test_split = input(Fore.YELLOW + Style.BRIGHT + "Test split ratio (0.2): " + Style.RESET_ALL).strip()
                    data_config['test_split'] = float(test_split) if test_split else 0.2
                    
                    stratified = input(Fore.YELLOW + Style.BRIGHT + "Use stratified split? (Y/n): " + Style.RESET_ALL).strip().lower()
                    data_config['stratified_split'] = stratified in ('', 'y', 'yes')
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nData Preprocessing:")
                    norm_options = ['standard', 'minmax', 'robust', 'none']
                    for i, norm in enumerate(norm_options, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if norm == data_config.get('data_normalization', 'standard') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {norm}{current}")
                    
                    while True:
                        try:
                            norm_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select normalization (1-{len(norm_options)}, default=1): " + Style.RESET_ALL).strip()
                            if norm_choice in [''] + [str(i) for i in range(1, len(norm_options)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(norm_options)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    normalization = norm_options[int(norm_choice)-1] if norm_choice else 'standard'
                    data_config['data_normalization'] = normalization
                    
                    preprocessing = input(Fore.YELLOW + Style.BRIGHT + "Enable data preprocessing? (Y/n): " + Style.RESET_ALL).strip().lower()
                    data_config['data_preprocessing'] = preprocessing in ('', 'y', 'yes')
                    
                    print(Fore.GREEN + Style.BRIGHT + "Data configuration updated")
                
                elif section_choice == '4':
                    print(Fore.YELLOW + Style.BRIGHT + "\nSYSTEM & HARDWARE CONFIGURATION")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    
                    hardware_config = final_config.setdefault('hardware', {})
                    system_config = final_config.setdefault('system', {})
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nDevice Configuration:")
                    device_options = ['auto', 'cpu', 'cuda']
                    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        device_options.append('mps')
                    
                    for i, dev in enumerate(device_options, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if dev == hardware_config.get('device', system_config.get('device', 'auto')) else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {dev}{current}")
                    
                    while True:
                        try:
                            dev_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select device (1-{len(device_options)}, default=1): " + Style.RESET_ALL).strip()
                            if dev_choice in [''] + [str(i) for i in range(1, len(device_options)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(device_options)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    device = device_options[int(dev_choice)-1] if dev_choice else 'auto'
                    hardware_config['device'] = device
                    
                    if torch.cuda.is_available():
                        cuda_opt = input(Fore.YELLOW + Style.BRIGHT + "Enable CUDA optimizations? (Y/n): " + Style.RESET_ALL).strip().lower()
                        hardware_config['cuda_optimizations'] = cuda_opt in ('', 'y', 'yes')
                    
                    memory_mgmt = input(Fore.YELLOW + Style.BRIGHT + "Enable memory management? (Y/n): " + Style.RESET_ALL).strip().lower()
                    hardware_config['memory_management'] = {'enable_memory_efficient': memory_mgmt in ('', 'y', 'yes')}
                    
                    gpu_memory = input(Fore.YELLOW + Style.BRIGHT + "Recommended GPU memory GB (4.0): " + Style.RESET_ALL).strip()
                    hardware_config['recommended_gpu_memory'] = float(gpu_memory) if gpu_memory else 4.0
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nPerformance Settings:")
                    num_workers = input(Fore.YELLOW + Style.BRIGHT + "Data loader workers (4): " + Style.RESET_ALL).strip()
                    system_config['num_workers'] = int(num_workers) if num_workers else 4
                    
                    pin_memory = input(Fore.YELLOW + Style.BRIGHT + "Pin memory for GPU? (Y/n): " + Style.RESET_ALL).strip().lower()
                    system_config['pin_memory'] = pin_memory in ('', 'y', 'yes') and torch.cuda.is_available()
                    
                    persistent_workers = input(Fore.YELLOW + Style.BRIGHT + "Use persistent workers? (Y/n): " + Style.RESET_ALL).strip().lower()
                    system_config['persistent_workers'] = persistent_workers in ('', 'y', 'yes')
                    
                    compile_model = input(Fore.YELLOW + Style.BRIGHT + "Compile model with torch.compile? (y/N): " + Style.RESET_ALL).strip().lower()
                    system_config['compile_model'] = compile_model in ('y', 'yes')
                    
                    benchmark_mode = input(Fore.YELLOW + Style.BRIGHT + "Enable benchmark mode? (y/N): " + Style.RESET_ALL).strip().lower()
                    system_config['benchmark_mode'] = benchmark_mode in ('y', 'yes')
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nReproducibility:")
                    reproducible = input(Fore.YELLOW + Style.BRIGHT + "Enable reproducible training? (Y/n): " + Style.RESET_ALL).strip().lower()
                    system_config['reproducible'] = reproducible in ('', 'y', 'yes')
                    
                    if system_config['reproducible']:
                        random_seed = input(Fore.YELLOW + Style.BRIGHT + "Random seed (42): " + Style.RESET_ALL).strip()
                        system_config['random_seed'] = int(random_seed) if random_seed else 42
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nDirectories:")
                    model_dir = input(Fore.YELLOW + Style.BRIGHT + f"Model directory ({DEFAULT_MODEL_DIR}): " + Style.RESET_ALL).strip()
                    system_config['model_dir'] = model_dir if model_dir else DEFAULT_MODEL_DIR
                    
                    log_dir = input(Fore.YELLOW + Style.BRIGHT + f"Log directory ({LOG_DIR}): " + Style.RESET_ALL).strip()
                    system_config['log_dir'] = log_dir if log_dir else LOG_DIR
                    
                    tb_dir = input(Fore.YELLOW + Style.BRIGHT + f"TensorBoard directory ({TB_DIR}): " + Style.RESET_ALL).strip()
                    system_config['tensorboard_dir'] = tb_dir if tb_dir else TB_DIR
                    
                    print(Fore.GREEN + Style.BRIGHT + "System & hardware configuration updated")
                
                elif section_choice == '5':
                    print(Fore.YELLOW + Style.BRIGHT + "\nSECURITY & ANOMALY DETECTION CONFIGURATION")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    
                    security_config = final_config.setdefault('security', {})
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nAnomaly Detection:")
                    percentile = input(Fore.YELLOW + Style.BRIGHT + "Anomaly threshold percentile (95.0): " + Style.RESET_ALL).strip()
                    security_config['percentile'] = float(percentile) if percentile else 95.0
                    
                    threshold_methods = ['percentile', 'adaptive', 'statistical', 'robust']
                    for i, method in enumerate(threshold_methods, 1):
                        current = Fore.GREEN + " (current)" if method == security_config.get('anomaly_threshold_strategy', 'percentile') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {method}{current}")
                    
                    while True:
                        try:
                            method_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select threshold method (1-{len(threshold_methods)}, default=1): " + Style.RESET_ALL).strip()
                            if method_choice in [''] + [str(i) for i in range(1, len(threshold_methods)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(threshold_methods)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    threshold_method = threshold_methods[int(method_choice)-1] if method_choice else 'percentile'
                    security_config['anomaly_threshold_strategy'] = threshold_method
                    
                    enable_security = input(Fore.YELLOW + Style.BRIGHT + "Enable security metrics? (Y/n): " + Style.RESET_ALL).strip().lower()
                    security_config['enable_security_metrics'] = enable_security in ('', 'y', 'yes')
                    
                    adaptive_thresh = input(Fore.YELLOW + Style.BRIGHT + "Use adaptive thresholding? (Y/n): " + Style.RESET_ALL).strip().lower()
                    security_config['adaptive_threshold'] = adaptive_thresh in ('', 'y', 'yes')
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nAdvanced Security Options:")
                    attack_thresh = input(Fore.YELLOW + Style.BRIGHT + "Attack threshold (optional): " + Style.RESET_ALL).strip()
                    if attack_thresh:
                        security_config['attack_threshold'] = float(attack_thresh)
                    
                    false_neg_cost = input(Fore.YELLOW + Style.BRIGHT + "False negative cost (optional): " + Style.RESET_ALL).strip()
                    if false_neg_cost:
                        security_config['false_negative_cost'] = float(false_neg_cost)
                    
                    early_warning = input(Fore.YELLOW + Style.BRIGHT + "Early warning threshold (optional): " + Style.RESET_ALL).strip()
                    if early_warning:
                        security_config['early_warning_threshold'] = float(early_warning)
                    
                    confidence_interval = input(Fore.YELLOW + Style.BRIGHT + "Confidence interval (optional): " + Style.RESET_ALL).strip()
                    if confidence_interval:
                        security_config['confidence_interval'] = float(confidence_interval)
                    
                    threshold_validation = input(Fore.YELLOW + Style.BRIGHT + "Enable threshold validation? (Y/n): " + Style.RESET_ALL).strip().lower()
                    security_config['threshold_validation'] = threshold_validation in ('', 'y', 'yes')
                    
                    robust_detection = input(Fore.YELLOW + Style.BRIGHT + "Enable robust detection? (Y/n): " + Style.RESET_ALL).strip().lower()
                    security_config['robust_detection'] = robust_detection in ('', 'y', 'yes')
                    
                    fp_tolerance = input(Fore.YELLOW + Style.BRIGHT + "False positive tolerance (optional): " + Style.RESET_ALL).strip()
                    if fp_tolerance:
                        security_config['false_positive_tolerance'] = float(fp_tolerance)
                    
                    perf_optimized = input(Fore.YELLOW + Style.BRIGHT + "Performance optimized detection? (Y/n): " + Style.RESET_ALL).strip().lower()
                    security_config['performance_optimized_detection'] = perf_optimized in ('', 'y', 'yes')
                    
                    real_time = input(Fore.YELLOW + Style.BRIGHT + "Real-time monitoring? (y/N): " + Style.RESET_ALL).strip().lower()
                    security_config['real_time_monitoring'] = real_time in ('y', 'yes')
                    
                    model_type = final_config.get('model', {}).get('model_type', 'EnhancedAutoencoder')
                    if model_type == 'AutoencoderEnsemble':
                        print(Fore.CYAN + Style.BRIGHT + "\nEnsemble Security:")
                        voting_methods = ['average', 'majority', 'weighted', 'max', 'min']
                        for i, voting in enumerate(voting_methods, 1):
                            current = Fore.GREEN + Style.BRIGHT + " (current)" if voting == security_config.get('ensemble_voting', 'average') else ""
                            print(Fore.WHITE + Style.BRIGHT + f"{i}. {voting}{current}")
                        
                        while True:
                            try:
                                voting_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select ensemble voting (1-{len(voting_methods)}, default=1): " + Style.RESET_ALL).strip()
                                if voting_choice in [''] + [str(i) for i in range(1, len(voting_methods)+1)]:
                                    break
                                print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(voting_methods)}")
                            except (EOFError, KeyboardInterrupt):
                                print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                                continue
                        
                        voting = voting_methods[int(voting_choice)-1] if voting_choice else 'average'
                        security_config['ensemble_voting'] = voting
                        
                        uncertainty = input(Fore.YELLOW + Style.BRIGHT + "Uncertainty threshold (optional): " + Style.RESET_ALL).strip()
                        if uncertainty:
                            security_config['uncertainty_threshold'] = float(uncertainty)
                    
                    print(Fore.GREEN + Style.BRIGHT + "Security configuration updated")
                
                elif section_choice == '6':
                    print(Fore.YELLOW + Style.BRIGHT + "\nMONITORING & LOGGING CONFIGURATION")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    
                    monitoring_config = final_config.setdefault('monitoring', {})
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nLogging Options:")
                    verbose = input(Fore.YELLOW + Style.BRIGHT + "Verbose output? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['verbose'] = verbose in ('', 'y', 'yes')
                    
                    debug = input(Fore.YELLOW + Style.BRIGHT + "Enable debug mode? (y/N): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['debug_mode'] = debug in ('y', 'yes')
                    
                    console_levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR']
                    for i, level in enumerate(console_levels, 1):
                        current = Fore.GREEN + Style.BRIGHT + " (current)" if level == monitoring_config.get('console_logging_level', 'INFO') else ""
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. {level}{current}")
                    
                    while True:
                        try:
                            level_choice = input(Fore.YELLOW + Style.BRIGHT + f"Select console logging level (1-{len(console_levels)}, default=2): " + Style.RESET_ALL).strip()
                            if level_choice in [''] + [str(i) for i in range(1, len(console_levels)+1)]:
                                break
                            print(Fore.RED + Style.BRIGHT + f"Please select 1-{len(console_levels)}")
                        except (EOFError, KeyboardInterrupt):
                            print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                            continue
                    
                    console_level = console_levels[int(level_choice)-1] if level_choice else 'INFO'
                    monitoring_config['console_logging_level'] = console_level
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nProgress Tracking:")
                    tensorboard = input(Fore.YELLOW + Style.BRIGHT + "Enable TensorBoard logging? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['tensorboard_logging'] = tensorboard in ('', 'y', 'yes')
                    
                    progress_bar = input(Fore.YELLOW + Style.BRIGHT + "Show progress bars? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['progress_bar'] = progress_bar in ('', 'y', 'yes')
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nCheckpoints and Saving:")
                    checkpoints = input(Fore.YELLOW + Style.BRIGHT + "Save training checkpoints? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['save_checkpoints'] = checkpoints in ('', 'y', 'yes')
                    
                    if monitoring_config['save_checkpoints']:
                        checkpoint_freq = input(Fore.YELLOW + Style.BRIGHT + "Checkpoint frequency in epochs (10): " + Style.RESET_ALL).strip()
                        monitoring_config['checkpoint_frequency'] = int(checkpoint_freq) if checkpoint_freq else 10
                    
                    save_best = input(Fore.YELLOW + Style.BRIGHT + "Save best model? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['save_best_model'] = save_best in ('', 'y', 'yes')
                    
                    save_history = input(Fore.YELLOW + Style.BRIGHT + "Save model history? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['save_model_history'] = save_history in ('', 'y', 'yes')
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nMetrics and Frequencies:")
                    log_freq = input(Fore.YELLOW + Style.BRIGHT + "Log frequency (1): " + Style.RESET_ALL).strip()
                    monitoring_config['log_frequency'] = int(log_freq) if log_freq else 1
                    
                    metrics_freq = input(Fore.YELLOW + Style.BRIGHT + "Metrics frequency (1): " + Style.RESET_ALL).strip()
                    monitoring_config['metrics_frequency'] = int(metrics_freq) if metrics_freq else 1
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nMetrics to track (space-separated):")
                    print(Fore.CYAN + Style.BRIGHT + "Available: loss, reconstruction_error, anomaly_detection_rate, mse_statistics, memory_usage")
                    metrics_input = input(Fore.YELLOW + Style.BRIGHT + "Metrics (default: loss reconstruction_error): " + Style.RESET_ALL).strip()
                    if metrics_input:
                        monitoring_config['metrics_to_track'] = metrics_input.split()
                    else:
                        monitoring_config['metrics_to_track'] = ['loss', 'reconstruction_error']
                    
                    early_metric = input(Fore.YELLOW + Style.BRIGHT + "Early stopping metric (val_loss): " + Style.RESET_ALL).strip()
                    monitoring_config['early_stopping_metric'] = early_metric if early_metric else 'val_loss'
                    
                    log_summary = input(Fore.YELLOW + Style.BRIGHT + "Log model summary? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['log_model_summary'] = log_summary in ('', 'y', 'yes')
                    
                    stability_metrics = input(Fore.YELLOW + Style.BRIGHT + "Track stability metrics? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['stability_metrics'] = stability_metrics in ('', 'y', 'yes')
                    
                    performance_metrics = input(Fore.YELLOW + Style.BRIGHT + "Track performance metrics? (Y/n): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['performance_metrics'] = performance_metrics in ('', 'y', 'yes')
                    
                    profiling = input(Fore.YELLOW + Style.BRIGHT + "Enable profiling? (y/N): " + Style.RESET_ALL).strip().lower()
                    monitoring_config['profiling_enabled'] = profiling in ('y', 'yes')
                    
                    print(Fore.GREEN + Style.BRIGHT + "Monitoring configuration updated")
                
                elif section_choice == '7':
                    print(Fore.YELLOW + Style.BRIGHT + "\nADVANCED OPTIONS CONFIGURATION")
                    print(Fore.GREEN + Style.BRIGHT + "-" * 40)
                    
                    validation_config = final_config.setdefault('validation', {})
                    experimental_config = final_config.setdefault('experimental', {})
                    export_config = final_config.setdefault('export', {})
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nValidation and Testing:")
                    detailed_metrics = input(Fore.YELLOW + Style.BRIGHT + "Calculate detailed metrics? (Y/n): " + Style.RESET_ALL).strip().lower()
                    validation_config['detailed_metrics'] = detailed_metrics in ('', 'y', 'yes')
                    
                    cross_validation = input(Fore.YELLOW + Style.BRIGHT + "Enable cross-validation? (y/N): " + Style.RESET_ALL).strip().lower()
                    if cross_validation in ('y', 'yes'):
                        cv_folds = input(Fore.YELLOW + Style.BRIGHT + "Number of CV folds (5): " + Style.RESET_ALL).strip()
                        validation_config['cross_validation'] = {
                            'enabled': True,
                            'folds': int(cv_folds) if cv_folds else 5
                        }
                    else:
                        validation_config['cross_validation'] = {'enabled': False}
                    
                    val_freq = input(Fore.YELLOW + Style.BRIGHT + "Validation frequency (1): " + Style.RESET_ALL).strip()
                    validation_config['validation_frequency'] = int(val_freq) if val_freq else 1
                    
                    save_val_results = input(Fore.YELLOW + Style.BRIGHT + "Save validation results? (Y/n): " + Style.RESET_ALL).strip().lower()
                    validation_config['save_validation_results'] = save_val_results in ('', 'y', 'yes')
                    
                    robustness = input(Fore.YELLOW + Style.BRIGHT + "Enable robustness testing? (y/N): " + Style.RESET_ALL).strip().lower()
                    validation_config['robustness_testing'] = robustness in ('y', 'yes')
                    
                    benchmarking = input(Fore.YELLOW + Style.BRIGHT + "Enable performance benchmarking? (y/N): " + Style.RESET_ALL).strip().lower()
                    validation_config['performance_benchmarking'] = benchmarking in ('y', 'yes')
                    
                    confidence_intervals = input(Fore.YELLOW + Style.BRIGHT + "Calculate confidence intervals? (y/N): " + Style.RESET_ALL).strip().lower()
                    validation_config['confidence_intervals'] = confidence_intervals in ('y', 'yes')
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nExport Options:")
                    save_model = input(Fore.YELLOW + Style.BRIGHT + "Save trained model? (Y/n): " + Style.RESET_ALL).strip().lower()
                    export_config['save_model'] = save_model in ('', 'y', 'yes')
                    
                    save_metadata = input(Fore.YELLOW + Style.BRIGHT + "Save training metadata? (Y/n): " + Style.RESET_ALL).strip().lower()
                    export_config['save_metadata'] = save_metadata in ('', 'y', 'yes')
                    
                    save_training_history = input(Fore.YELLOW + Style.BRIGHT + "Save training history? (Y/n): " + Style.RESET_ALL).strip().lower()
                    export_config['save_training_history'] = save_training_history in ('', 'y', 'yes')
                    
                    export_onnx = input(Fore.YELLOW + Style.BRIGHT + "Export to ONNX format? (y/N): " + Style.RESET_ALL).strip().lower()
                    export_config['export_onnx'] = export_onnx in ('y', 'yes')
                    
                    print(Fore.CYAN + Style.BRIGHT + "\nExperimental Features:")
                    experimental_features = input(Fore.YELLOW + Style.BRIGHT + "Enable experimental features? (y/N): " + Style.RESET_ALL).strip().lower()
                    if experimental_features in ('y', 'yes'):
                        print(Fore.YELLOW + Style.BRIGHT + "Available experimental features:")
                        print(Fore.GREEN + Style.BRIGHT + "1. Advanced attention mechanisms")
                        print(Fore.GREEN + Style.BRIGHT + "2. Dynamic architecture adjustment")
                        print(Fore.GREEN + Style.BRIGHT + "3. Adaptive learning rates")
                        print(Fore.GREEN + Style.BRIGHT + "4. Novel regularization techniques")
                        print(Fore.GREEN + Style.BRIGHT + "5. Memory optimization")
                        
                        exp_features_input = input(Fore.YELLOW + Style.BRIGHT + "Select features (space-separated numbers, optional): " + Style.RESET_ALL).strip()
                        if exp_features_input:
                            feature_map = {
                                '1': 'advanced_attention',
                                '2': 'dynamic_architecture',
                                '3': 'adaptive_lr',
                                '4': 'novel_regularization',
                                '5': 'memory_optimization'
                            }
                            selected_features = {}
                            for num in exp_features_input.split():
                                if num in feature_map:
                                    selected_features[feature_map[num]] = True
                            experimental_config['experimental_features'] = selected_features
                    
                    auto_optimize = input(Fore.YELLOW + Style.BRIGHT + "Enable automatic optimization? (y/N): " + Style.RESET_ALL).strip().lower()
                    experimental_config['auto_optimize'] = auto_optimize in ('y', 'yes')
                    
                    print(Fore.GREEN + Style.BRIGHT + "Advanced options configuration updated")
                
                elif section_choice == '8':
                    print(Fore.YELLOW + Style.BRIGHT + "\nCOMPLETE CONFIGURATION REVIEW")
                    print(Fore.GREEN + Style.BRIGHT + "=" * 40)
                    
                    model_config = final_config.get('model', {})
                    training_config = final_config.get('training', {})
                    data_config = final_config.get('data', {})
                    security_config = final_config.get('security', {})
                    system_config = final_config.get('system', {})
                    monitoring_config = final_config.get('monitoring', {})
                    hardware_config = final_config.get('hardware', {})
                    validation_config = final_config.get('validation', {})
                    experimental_config = final_config.get('experimental', {})
                    export_config = final_config.get('export', {})
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nModel Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Type: " + Fore.GREEN + f"{model_config.get('model_type', 'EnhancedAutoencoder')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Encoding Dim: " + Fore.GREEN + f"{model_config.get('encoding_dim', 32)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Hidden Dims: " + Fore.GREEN + f"{model_config.get('hidden_dims', [256, 128, 64])}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Dropout Rates: " + Fore.GREEN + f"{model_config.get('dropout_rates', [0.2, 0.15, 0.1])}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Activation: " + Fore.GREEN + f"{model_config.get('activation', 'leaky_relu')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Normalization: " + Fore.GREEN + f"{model_config.get('normalization', 'batch')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Weight Init: " + Fore.GREEN + f"{model_config.get('weight_init', 'xavier_uniform')}")
                    
                    if model_config.get('model_type') == 'AutoencoderEnsemble':
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Ensemble Size: " + Fore.GREEN + f"{model_config.get('num_models', 3)}")
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Diversity Factor: " + Fore.GREEN + f"{model_config.get('diversity_factor', 0.3)}")
                    elif model_config.get('model_type') == 'EnhancedAutoencoder':
                        features = []
                        if model_config.get('use_attention', True):
                            features.append("Attention")
                        if model_config.get('residual_blocks', True):
                            features.append("Residual Blocks")
                        if model_config.get('skip_connection', True):
                            features.append("Skip Connections")
                        if features:
                            print(Fore.CYAN + Style.BRIGHT + f"  ├─ Enhanced Features: " + Fore.GREEN + f"{', '.join(features)}")
                        print(Fore.CYAN + Style.BRIGHT + f"  └─ Legacy Mode: " + Fore.GREEN + f"{model_config.get('legacy_mode', False)}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nTraining Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Epochs: " + Fore.GREEN + f"{training_config.get('epochs', 50)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.GREEN + f"{training_config.get('batch_size', 64)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Learning Rate: " + Fore.GREEN + f"{training_config.get('learning_rate', 0.001)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Weight Decay: " + Fore.GREEN + f"{training_config.get('weight_decay', 1e-4)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Patience: " + Fore.GREEN + f"{training_config.get('patience', 15)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Optimizer: " + Fore.GREEN + f"{training_config.get('optimizer', 'AdamW')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Scheduler: " + Fore.GREEN + f"{training_config.get('scheduler', 'ReduceLROnPlateau')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Mixed Precision: " + Fore.GREEN + f"{training_config.get('mixed_precision', torch.cuda.is_available())}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Early Stopping: " + Fore.GREEN + f"{training_config.get('early_stopping', True)}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nData Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Use Real Data: " + Fore.GREEN + f"{data_config.get('use_real_data', False)}")
                    if not data_config.get('use_real_data', False):
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Normal Samples: " + Fore.GREEN + f"{data_config.get('normal_samples', 8000)}")
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Attack Samples: " + Fore.GREEN + f"{data_config.get('attack_samples', 2000)}")
                        print(Fore.CYAN + Style.BRIGHT + f"  ├─ Features: " + Fore.GREEN + f"{data_config.get('features', 20)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Validation Split: " + Fore.GREEN + f"{training_config.get('validation_split', 0.2)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Test Split: " + Fore.GREEN + f"{data_config.get('test_split', 0.2)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Data Normalization: " + Fore.GREEN + f"{data_config.get('data_normalization', 'standard')}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nSecurity Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Anomaly Threshold: " + Fore.GREEN + f"{security_config.get('percentile', 95.0)}th percentile")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Threshold Method: " + Fore.GREEN + f"{security_config.get('anomaly_threshold_strategy', 'percentile')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Adaptive Threshold: " + Fore.GREEN + f"{security_config.get('adaptive_threshold', True)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Security Metrics: " + Fore.GREEN + f"{security_config.get('enable_security_metrics', True)}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nSystem Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Device: " + Fore.GREEN + f"{hardware_config.get('device', system_config.get('device', 'auto'))}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Reproducible: " + Fore.GREEN + f"{system_config.get('reproducible', True)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Workers: " + Fore.GREEN + f"{system_config.get('num_workers', 4)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Model Directory: " + Fore.GREEN + f"{system_config.get('model_dir', DEFAULT_MODEL_DIR)}")
                    
                    print(Fore.YELLOW + Style.BRIGHT + "\nMonitoring Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ TensorBoard: " + Fore.GREEN + f"{monitoring_config.get('tensorboard_logging', True)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Checkpoints: " + Fore.GREEN + f"{monitoring_config.get('save_checkpoints', True)}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Metrics: " + Fore.GREEN + f"{monitoring_config.get('metrics_to_track', ['loss', 'reconstruction_error'])}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Verbose: " + Fore.GREEN + f"{monitoring_config.get('verbose', True)}")
                    
                    estimated_time = _estimate_training_time(
                        training_config.get('epochs', 50),
                        model_config.get('model_type', 'EnhancedAutoencoder'),
                        training_config.get('batch_size', 64)
                    )
                    print(Fore.YELLOW + Style.BRIGHT + "\nPerformance Estimate:")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Training Time: " + Fore.GREEN + f"~{estimated_time} minutes")
                    
                    
                    modify_more = input(Fore.YELLOW + Style.BRIGHT + "Continue modifying configuration? (y/N): " + Style.RESET_ALL).strip().lower()
                    if modify_more not in ('y', 'yes'):
                        continue
                
                elif section_choice == '9':
                    print(Fore.GREEN + Style.BRIGHT + "\nFINAL CONFIGURATION SUMMARY")
                    print(Fore.YELLOW + Style.BRIGHT + "=" * 40)
                    
                    model_config = final_config.get('model', {})
                    training_config = final_config.get('training', {})
                    data_config = final_config.get('data', {})
                    hardware_config = final_config.get('hardware', {})
                    system_config = final_config.get('system', {})
                    
                    console.print(
                        Panel.fit(
                            "READY TO START CUSTOM TRAINING",
                            style="bold green",
                            border_style="green",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
                    print(Fore.YELLOW + Style.BRIGHT + "Final Configuration:")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Model: " + Fore.GREEN + f"{model_config.get('model_type', 'EnhancedAutoencoder')}")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Training: " + Fore.GREEN + f"{training_config.get('epochs', 50)} epochs")
                    print(Fore.CYAN + Style.BRIGHT + f"  ├─ Data: " + Fore.GREEN + f"{'Real Data' if data_config.get('use_real_data', False) else 'Synthetic Data'}")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Device: " + Fore.GREEN + f"{final_config.get('hardware', {}).get('device', final_config.get('system', {}).get('device', 'auto'))}")
                    
                    estimated_time = _estimate_training_time(
                        training_config.get('epochs', 50),
                        model_config.get('model_type', 'EnhancedAutoencoder'),
                        training_config.get('batch_size', 64)
                    )
                    print(Fore.YELLOW + Style.BRIGHT + "\nPerformance:")
                    print(Fore.CYAN + Style.BRIGHT + f"  └─ Estimated Time: " + Fore.GREEN + f"~{estimated_time} minutes")
                    
                    confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart training with this configuration? (Y/n): " + Style.RESET_ALL).strip().lower()
                    if confirm in ('', 'y', 'yes'):
                        print(Fore.GREEN + Style.BRIGHT + "\nLaunching custom training...")
                        return _launch_training_with_config(final_config)
                    else:
                        save_config = input(Fore.YELLOW + Style.BRIGHT + "Save this configuration for later? (y/N): " + Style.RESET_ALL).strip().lower()
                        if save_config in ('y', 'yes'):
                            config_name = input(Fore.YELLOW + Style.BRIGHT + "Configuration name: " + Style.RESET_ALL).strip()
                            if config_name:
                                try:
                                    config_path = Path(final_config.get('system', {}).get('model_dir', DEFAULT_MODEL_DIR)) / f"custom_config_{config_name}.json"
                                    config_path.parent.mkdir(parents=True, exist_ok=True)
                                    with open(config_path, 'w') as f:
                                        import json
                                        json.dump(final_config, f, indent=2, default=str)
                                    print(Fore.GREEN + Style.BRIGHT + f"Configuration saved to: {config_path}")
                                except Exception as e:
                                    print(Fore.RED + Style.BRIGHT + f"Failed to save configuration: {e}")
                        
                        print(Fore.RED + Style.BRIGHT + "Training cancelled")
                        return None
                
                else:
                    print(Fore.RED + Style.BRIGHT + "Invalid choice. Please select 0-9.")
            
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nOperation cancelled")
                continue
    
    except KeyboardInterrupt:
        print(Fore.RED + Style.BRIGHT + "\n\nCustom training configuration interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"Custom training configuration failed: {e}", exc_info=True)
        message = (
            f"Error encountered during custom training configuration: {str(e)}\n"
            f"Context:\n"
            f"- Current Preset: {preset_name}\n"
            f"- Model Type: {model_type}\n"
            f"- Config Source: {config_source}\n\n"
            f"This could be due to:\n"
            f"- Configuration file corruption\n"
            f"- Invalid parameter combinations\n"
            f"- System resource issues\n"
            f"- Interactive input handling problems"
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="CUSTOM TRAINING CONFIGURATION ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        return None

def run_stability_test(
    # Test Configuration Parameters
    test_epochs: Optional[int] = None,
    test_batch_size: Optional[int] = None,
    test_learning_rate: Optional[float] = None,
    test_model_type: Optional[str] = None,
    test_encoding_dim: Optional[int] = None,
    test_normal_samples: Optional[int] = None,
    test_attack_samples: Optional[int] = None,
    test_features: Optional[int] = None,
    
    # Test Mode Parameters
    quick_test: Optional[bool] = None,
    comprehensive_test: Optional[bool] = None,
    stress_test: Optional[bool] = None,
    minimal_test: Optional[bool] = None,
    
    # System Parameters
    test_device: Optional[str] = None,
    test_mixed_precision: Optional[bool] = None,
    test_num_workers: Optional[int] = None,
    test_memory_efficient: Optional[bool] = None,
    
    # Output Parameters
    verbose: Optional[bool] = None,
    save_test_results: Optional[bool] = None,
    test_results_dir: Optional[Union[str, Path]] = None,
    interactive: Optional[bool] = None,
    
    # Error Handling Parameters
    continue_on_error: Optional[bool] = None,
    graceful_degradation: Optional[bool] = None,
    
    # Advanced Test Parameters
    test_all_models: Optional[bool] = None,
    test_data_sources: Optional[bool] = None,
    test_hardware_configs: Optional[bool] = None,
    performance_benchmarking: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    test_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """Comprehensive system stability test with context display and error handling."""
    try:
        # Clear screen and show banner with config
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Extract configuration context with error handling
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Extract preset name with multiple fallbacks
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu header with context
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + "SYSTEM STABILITY TEST")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Active Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")

        # Start timing
        start_time = datetime.now()
        test_start_time = time.time()
        
        # Initialize configuration
        if config is None:
            try:
                config = get_current_config() if 'get_current_config' in globals() else {}
            except Exception as e:
                logger.warning(f"Failed to load current config: {e}")
                console.print(
                    Panel.fit(
                        f"Failed to load current config, using defaults: {str(e)}",
                        title="WARNING",
                        style="bold red",
                        border_style="red",
                        padding=(1, 1),
                        box=box.ROUNDED
                    )
                )
                config = {}
        
        # Apply test-specific configuration
        if test_config:
            config.setdefault('stability_test', {}).update(test_config)
        
        # Apply all parameters to configuration
        final_config = config.copy()
        
        # Apply individual parameters
        params = locals().copy()
        params.update(kwargs)
        
        # Remove non-parameter items
        params_to_remove = {
            'config', 'test_config', 'kwargs', 'start_time', 'test_start_time',
            'datetime', 'traceback', 'time', 'gc', 'warnings', 'Path'
        }
        
        cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
        
        # Set up defaults
        stability_config = final_config.setdefault('stability_test', {})
        
        # Test configuration defaults
        test_epochs = stability_config.setdefault('test_epochs', cleaned_params.get('test_epochs', 10))
        test_batch_size = stability_config.setdefault('test_batch_size', cleaned_params.get('test_batch_size', 32))
        test_learning_rate = stability_config.setdefault('test_learning_rate', cleaned_params.get('test_learning_rate', 0.01))
        test_model_type = stability_config.setdefault('test_model_type', cleaned_params.get('test_model_type', 'SimpleAutoencoder'))
        test_encoding_dim = stability_config.setdefault('test_encoding_dim', cleaned_params.get('test_encoding_dim', 8))
        test_normal_samples = stability_config.setdefault('test_normal_samples', cleaned_params.get('test_normal_samples', 1000))
        test_attack_samples = stability_config.setdefault('test_attack_samples', cleaned_params.get('test_attack_samples', 200))
        test_features = stability_config.setdefault('test_features', cleaned_params.get('test_features', 20))
        
        # Test mode defaults
        quick_test = stability_config.setdefault('quick_test', cleaned_params.get('quick_test', True))
        comprehensive_test = stability_config.setdefault('comprehensive_test', cleaned_params.get('comprehensive_test', False))
        stress_test = stability_config.setdefault('stress_test', cleaned_params.get('stress_test', False))
        minimal_test = stability_config.setdefault('minimal_test', cleaned_params.get('minimal_test', False))
        
        # System defaults
        test_device = stability_config.setdefault('test_device', cleaned_params.get('test_device', 'auto'))
        test_mixed_precision = stability_config.setdefault('test_mixed_precision', cleaned_params.get('test_mixed_precision', False))
        test_num_workers = stability_config.setdefault('test_num_workers', cleaned_params.get('test_num_workers', 0))
        test_memory_efficient = stability_config.setdefault('test_memory_efficient', cleaned_params.get('test_memory_efficient', True))
        
        # Output defaults
        verbose = stability_config.setdefault('verbose', cleaned_params.get('verbose', True))
        save_test_results = stability_config.setdefault('save_test_results', cleaned_params.get('save_test_results', True))
        test_results_dir = stability_config.setdefault('test_results_dir', cleaned_params.get('test_results_dir', DEFAULT_MODEL_DIR / "stability_tests"))
        interactive = stability_config.setdefault('interactive', cleaned_params.get('interactive', not cleaned_params.get('non_interactive', False)))
        
        # Error handling defaults
        continue_on_error = stability_config.setdefault('continue_on_error', cleaned_params.get('continue_on_error', True))
        graceful_degradation = stability_config.setdefault('graceful_degradation', cleaned_params.get('graceful_degradation', True))
        
        # Advanced test defaults
        test_all_models = stability_config.setdefault('test_all_models', cleaned_params.get('test_all_models', False))
        test_data_sources = stability_config.setdefault('test_data_sources', cleaned_params.get('test_data_sources', False))
        test_hardware_configs = stability_config.setdefault('test_hardware_configs', cleaned_params.get('test_hardware_configs', False))
        performance_benchmarking = stability_config.setdefault('performance_benchmarking', cleaned_params.get('performance_benchmarking', False))

        # Set up logging
        if verbose:
            handlers_to_suppress = []
            for handler in logger.handlers:
                if isinstance(handler, logging.StreamHandler) and handler.stream.name in ['<stdout>', '<stderr>']:
                    handlers_to_suppress.append(handler)
                    handler.setLevel(logging.CRITICAL)  # Temporarily suppress console output
        
        # Initialize test results
        test_results = {
            'start_time': start_time.isoformat(),
            'test_configuration': stability_config,
            'system_info': {},
            'tests_run': [],
            'test_results': {},
            'overall_status': 'UNKNOWN',
            'recommendations': [],
            'warnings': [],
            'errors': [],
            'performance_metrics': {},
            'hardware_compatibility': {},
            'summary': {}
        }
        
        # Interactive prompt if enabled
        if interactive:
            print(Fore.YELLOW + Style.BRIGHT + "\nStability Test Validates:")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ System hardware compatibility")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Training pipeline functionality")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Data processing integrity")
            print(Fore.GREEN + Style.BRIGHT + "  ├─ Model training stability")
            print(Fore.GREEN + Style.BRIGHT + "  └─ Memory and performance characteristics")
            
            # Display test configuration
            print(Fore.YELLOW + Style.BRIGHT + "\nTest Mode Configuration:")
            print(Fore.WHITE + Style.BRIGHT + f"  1. Mode: " + Fore.GREEN + Style.BRIGHT + f"{'Comprehensive' if comprehensive_test else 'Stress' if stress_test else 'Minimal' if minimal_test else 'Quick'}")
            print(Fore.WHITE + Style.BRIGHT + f"  2. Model Type: " + Fore.GREEN + Style.BRIGHT + f"{test_model_type}")
            print(Fore.WHITE + Style.BRIGHT + f"  3. Epochs: " + Fore.GREEN + Style.BRIGHT + f"{test_epochs}")
            print(Fore.WHITE + Style.BRIGHT + f"  4. Batch Size: " + Fore.GREEN + Style.BRIGHT + f"{test_batch_size}")
            print(Fore.WHITE + Style.BRIGHT + f"  5. Learning Rate: " + Fore.GREEN + Style.BRIGHT + f"{test_learning_rate}")
            print(Fore.WHITE + Style.BRIGHT + f"  6. Samples: " + Fore.GREEN + Style.BRIGHT + f"{test_normal_samples} normal, {test_attack_samples} attack")
            print(Fore.WHITE + Style.BRIGHT + f"  7. Features: " + Fore.GREEN + Style.BRIGHT + f"{test_features}")
            print(Fore.WHITE + Style.BRIGHT + f"  8. Device: " + Fore.GREEN + Style.BRIGHT + f"{test_device}")
            print(Fore.WHITE + Style.BRIGHT + f"  9. Mixed Precision: " + Fore.GREEN + Style.BRIGHT + f"{test_mixed_precision}")
            
            confirm = input(Fore.YELLOW + Style.BRIGHT + "\nProceed with stability test? (Y/n): " + Style.RESET_ALL).lower().strip()
            if confirm not in ('', 'y', 'yes'):
                print(Fore.RED + Style.BRIGHT + "Stability test cancelled by user")
                test_results['overall_status'] = 'CANCELLED'
                return test_results
        
        try:
            # Create test results directory
            test_results_dir = Path(test_results_dir)
            test_results_dir.mkdir(parents=True, exist_ok=True)
            
            # Test timestamp for unique identification
            test_timestamp = start_time.strftime("%Y%m%d_%H%M%S")
            test_session_id = f"stability_test_{test_timestamp}"
            
            test_results['test_session_id'] = test_session_id
            test_results['test_results_dir'] = str(test_results_dir)
            
            # System Information Collection
            try:
                system_info = {
                    'python_version': sys.version,
                    'pytorch_version': torch.__version__,
                    'platform': sys.platform,
                    'architecture': platform.architecture(),
                    'processor': platform.processor(),
                    'machine': platform.machine(),
                    'node': platform.node(),
                    'system': platform.system(),
                    'timestamp': test_timestamp
                }
                
                # Hardware information
                try:
                    hw_info = check_hardware()
                    system_info.update(hw_info)
                except Exception as e:
                    logger.warning(f"Failed to get hardware info: {e}")
                    system_info['hardware_error'] = str(e)
                
                # Memory information
                try:
                    memory = psutil.virtual_memory()
                    system_info.update({
                        'total_memory_gb': memory.total / (1024**3),
                        'available_memory_gb': memory.available / (1024**3),
                        'memory_percent_used': memory.percent
                    })
                except ImportError:
                    logger.warning("psutil not available for memory monitoring")
                except Exception as e:
                    logger.warning(f"Memory info collection failed: {e}")
                
                # CUDA information
                if torch.cuda.is_available():
                    system_info.update({
                        'cuda_version': torch.version.cuda,
                        'cudnn_version': torch.backends.cudnn.version(),
                        'gpu_count': torch.cuda.device_count(),
                        'gpu_names': [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],
                        'gpu_memory': [torch.cuda.get_device_properties(i).total_memory / (1024**3) 
                                      for i in range(torch.cuda.device_count())]
                    })
                
                test_results['system_info'] = system_info
                
                if verbose:
                    print(Fore.YELLOW + Style.BRIGHT + "System Information:")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ System: " + Fore.GREEN + Style.BRIGHT + f"{system_info.get('system', 'Unknown')} {system_info.get('machine', '')}")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ Python: " + Fore.GREEN + Style.BRIGHT + f"{system_info.get('python_version', '').split()[0]}")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ PyTorch: " + Fore.GREEN + Style.BRIGHT + f"{system_info.get('pytorch_version', 'Unknown')}")
                    print(Fore.WHITE + Style.BRIGHT + f"  ├─ Device: " + Fore.GREEN + Style.BRIGHT + f"{system_info.get('device', 'Unknown')}")
                    if torch.cuda.is_available():
                        print(Fore.WHITE + Style.BRIGHT + f"  ├─ CUDA: " + Fore.GREEN + Style.BRIGHT + f"{system_info.get('cuda_version', 'Unknown')}")
                        print(Fore.WHITE + Style.BRIGHT + f"  └─ GPUs: " + Fore.GREEN + Style.BRIGHT + f"{system_info.get('gpu_count', 0)}")
                    print()
                
            except Exception as e:
                error_msg = f"System information collection failed: {e}"
                logger.error(error_msg)
                test_results['errors'].append(error_msg)
                test_results['system_info'] = {'error': str(e)}
            
            # Test 1: Basic Hardware Compatibility
            logger.info("Test 1: Hardware Compatibility")
            test_results['tests_run'].append('hardware_compatibility')
            
            try:
                hw_test_results = {
                    'test_name': 'hardware_compatibility',
                    'status': 'UNKNOWN',
                    'details': {},
                    'duration_seconds': 0,
                    'error': None
                }
                
                hw_start_time = time.time()
                
                # Device detection and validation
                if test_device == 'auto':
                    if torch.cuda.is_available():
                        device = torch.device('cuda')
                        hw_test_results['details']['cuda_available'] = True
                    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        device = torch.device('mps')
                        hw_test_results['details']['mps_available'] = True
                    else:
                        device = torch.device('cpu')
                    hw_test_results['details']['auto_device'] = str(device)
                else:
                    device = torch.device(test_device)
                    hw_test_results['details']['specified_device'] = str(device)
                
                # Test basic tensor operations
                test_tensor = torch.randn(100, test_features, device=device)
                test_output = torch.relu(test_tensor @ test_tensor.T)
                
                hw_test_results['details'].update({
                    'tensor_operations': 'SUCCESS',
                    'device_used': str(device),
                    'tensor_shape_test': test_tensor.shape,
                    'computation_result_shape': test_output.shape
                })
                
                # Test mixed precision if enabled
                if test_mixed_precision and device.type == 'cuda':
                    try:
                        autocast_context = get_autocast_context(device, True, True)
                        with autocast_context:
                            mp_test = torch.matmul(test_tensor, test_tensor.T)
                        hw_test_results['details']['mixed_precision'] = 'SUCCESS'
                    except Exception as mp_e:
                        hw_test_results['details']['mixed_precision'] = f'FAILED: {str(mp_e)}'
                
                hw_test_results['duration_seconds'] = time.time() - hw_start_time
                hw_test_results['status'] = 'PASSED'
                
                if verbose:
                    print(Fore.GREEN + Style.BRIGHT + f"Hardware compatibility test passed:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Device: {device}")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Tensor operations: OK")
                    if test_mixed_precision and device.type == 'cuda':
                        print(Fore.GREEN + Style.BRIGHT + f"  └─ Mixed precision: {hw_test_results['details'].get('mixed_precision', 'N/A')}")
                
            except Exception as e:
                hw_test_results['status'] = 'FAILED'
                hw_test_results['error'] = str(e)
                hw_test_results['duration_seconds'] = time.time() - hw_start_time
                test_results['errors'].append(f"Hardware compatibility test failed: {e}")
                
                if verbose:
                    print(Fore.RED + Style.BRIGHT + f"Hardware compatibility test failed: {e}")
                
                if not continue_on_error:
                    test_results['test_results']['hardware_compatibility'] = hw_test_results
                    test_results['overall_status'] = 'FAILED'
                    return test_results
            
            test_results['test_results']['hardware_compatibility'] = hw_test_results
            test_results['hardware_compatibility'] = hw_test_results['details']
            
            # Test 2: Data Pipeline Integrity
            logger.info("Test 2: Data Pipeline Integrity")
            test_results['tests_run'].append('data_pipeline')
            
            try:
                data_test_results = {
                    'test_name': 'data_pipeline',
                    'status': 'UNKNOWN',
                    'details': {},
                    'duration_seconds': 0,
                    'error': None
                }
                
                data_start_time = time.time()
                
                # Test synthetic data generation
                synthetic_data = generate_synthetic_data(
                    normal_samples=test_normal_samples,
                    attack_samples=test_attack_samples,
                    features=test_features,
                    random_state=42,
                    output_format='dict',
                    verbose=False,
                    config=final_config
                )
                
                data_test_results['details'].update({
                    'synthetic_data_generation': 'SUCCESS',
                    'train_samples': len(synthetic_data['X_train']),
                    'val_samples': len(synthetic_data['X_val']),
                    'test_samples': len(synthetic_data['X_test']),
                    'features': len(synthetic_data['feature_names']),
                    'data_shapes': {
                        'X_train': synthetic_data['X_train'].shape,
                        'X_val': synthetic_data['X_val'].shape,
                        'X_test': synthetic_data['X_test'].shape
                    }
                })
                
                # Test data validation
                validation_result = validate_data_integrity(synthetic_data, final_config)
                data_test_results['details']['data_validation'] = {
                    'passed': validation_result['passed'],
                    'quality_score': validation_result['quality_score'],
                    'warnings': len(validation_result['warnings']),
                    'errors': len(validation_result['errors'])
                }
                
                # Test DataLoader creation
                train_loader, val_loader, test_loader = create_dataloaders(
                    data=synthetic_data,
                    batch_size=test_batch_size,
                    num_workers=test_num_workers,
                    pin_memory=False,  # Disable for stability test
                    config=final_config
                )
                
                data_test_results['details']['dataloader_creation'] = {
                    'status': 'SUCCESS',
                    'train_batches': len(train_loader),
                    'val_batches': len(val_loader),
                    'test_batches': len(test_loader)
                }
                
                # Test batch iteration
                test_batch = next(iter(train_loader))
                data_test_results['details']['batch_iteration'] = {
                    'status': 'SUCCESS',
                    'batch_shape': test_batch.shape if isinstance(test_batch, torch.Tensor) else test_batch[0].shape,
                    'batch_dtype': str(test_batch.dtype if isinstance(test_batch, torch.Tensor) else test_batch[0].dtype)
                }
                
                data_test_results['duration_seconds'] = time.time() - data_start_time
                data_test_results['status'] = 'PASSED'
                
                if verbose:
                    print(Fore.GREEN + Style.BRIGHT + f"Data pipeline test passed:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Synthetic data: {test_normal_samples + test_attack_samples} samples, {test_features} features")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ DataLoaders: {len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test batches")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Data quality score: {validation_result['quality_score']:.2f}")
            
            except Exception as e:
                data_test_results['status'] = 'FAILED'
                data_test_results['error'] = str(e)
                data_test_results['duration_seconds'] = time.time() - data_start_time
                test_results['errors'].append(f"Data pipeline test failed: {e}")
                
                if verbose:
                    print(Fore.RED + Style.BRIGHT + f"Data pipeline test failed: {e}")
                
                if not continue_on_error:
                    test_results['test_results']['data_pipeline'] = data_test_results
                    test_results['overall_status'] = 'FAILED'
                    return test_results
            
            test_results['test_results']['data_pipeline'] = data_test_results
            
            # Test 3: Model Initialization
            logger.info("Test 3: Model Initialization")
            test_results['tests_run'].append('model_initialization')
            
            try:
                model_test_results = {
                    'test_name': 'model_initialization',
                    'status': 'UNKNOWN',
                    'details': {},
                    'duration_seconds': 0,
                    'error': None
                }
                
                model_start_time = time.time()
                
                # Prepare model configuration
                model_config = {
                    'model': {
                        'model_type': test_model_type,
                        'input_dim': test_features,
                        'encoding_dim': test_encoding_dim,
                        'hidden_dims': [max(32, test_features // 2)],
                        'dropout_rates': [0.2],
                        'activation': 'leaky_relu',
                        'normalization': None if test_model_type == 'SimpleAutoencoder' else 'batch',
                        'skip_connection': False if test_model_type == 'SimpleAutoencoder' else True,
                        'residual_blocks': False if test_model_type == 'SimpleAutoencoder' else True,
                        'use_attention': False if test_model_type == 'SimpleAutoencoder' else True
                    },
                    'training': {
                        'mixed_precision': test_mixed_precision,
                        'batch_size': test_batch_size
                    },
                    'hardware': {
                        'device': str(device)
                    }
                }
                
                # Create model instance
                if test_model_type == 'SimpleAutoencoder':
                    model = SimpleAutoencoder(
                        input_dim=test_features,
                        encoding_dim=test_encoding_dim,
                        config=model_config
                    )
                elif test_model_type == 'EnhancedAutoencoder':
                    model = EnhancedAutoencoder(
                        input_dim=test_features,
                        encoding_dim=test_encoding_dim,
                        config=model_config
                    )
                elif test_model_type == 'AutoencoderEnsemble':
                    model = AutoencoderEnsemble(
                        input_dim=test_features,
                        encoding_dim=test_encoding_dim,
                        num_models=2,  # Reduced for stability test
                        config=model_config
                    )
                else:
                    raise ValueError(f"Unknown model type: {test_model_type}")
                
                model = model.to(device)
                
                # Test model forward pass
                test_input = torch.randn(test_batch_size, test_features, device=device)
                
                autocast_context = get_autocast_context(device, test_mixed_precision, True)
                with autocast_context:
                    test_output = model(test_input)
                
                # Model information
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                
                model_test_results['details'].update({
                    'model_creation': 'SUCCESS',
                    'model_type': test_model_type,
                    'model_class': type(model).__name__,
                    'total_parameters': total_params,
                    'trainable_parameters': trainable_params,
                    'model_size_mb': total_params * 4 / (1024 * 1024),
                    'forward_pass': 'SUCCESS',
                    'input_shape': test_input.shape,
                    'output_shape': test_output.shape,
                    'output_dtype': str(test_output.dtype),
                    'device_placement': str(next(model.parameters()).device)
                })
                
                model_test_results['duration_seconds'] = time.time() - model_start_time
                model_test_results['status'] = 'PASSED'
                
                if verbose:
                    print(Fore.GREEN + Style.BRIGHT + f"Model initialization test passed:")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: {test_model_type} ({type(model).__name__})")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Parameters: {total_params:,} total, {trainable_params:,} trainable")
                    print(Fore.GREEN + Style.BRIGHT + f"  ├─ Forward pass: {test_input.shape} → {test_output.shape}")
                    print(Fore.GREEN + Style.BRIGHT + f"  └─ Device: {next(model.parameters()).device}")
            
            except Exception as e:
                model_test_results['status'] = 'FAILED'
                model_test_results['error'] = str(e)
                model_test_results['duration_seconds'] = time.time() - model_start_time
                test_results['errors'].append(f"Model initialization test failed: {e}")
                
                if verbose:
                    print(Fore.RED + Style.BRIGHT + f"Model initialization test failed: {e}")
                
                if not continue_on_error:
                    test_results['test_results']['model_initialization'] = model_test_results
                    test_results['overall_status'] = 'FAILED'
                    return test_results
            
            test_results['test_results']['model_initialization'] = model_test_results
            
            # Test 4: Training Stability
            logger.info("Test 4: Training Stability")
            test_results['tests_run'].append('training_stability')
            
            try:
                training_test_results = {
                    'test_name': 'training_stability',
                    'status': 'UNKNOWN',
                    'details': {},
                    'duration_seconds': 0,
                    'error': None
                }
                
                training_start_time = time.time()
                
                # Prepare training configuration
                stability_training_config = {
                    #'model_architecture': {
                    'model': {
                        'model_type': test_model_type,
                        'input_dim': test_features,
                        'encoding_dim': test_encoding_dim,
                        'hidden_dims': [max(32, test_features // 2)] if test_model_type == 'SimpleAutoencoder' else [128, 64],
                        'dropout_rates': [0.2] if test_model_type == 'SimpleAutoencoder' else [0.2, 0.15],
                        'activation': 'leaky_relu',
                        'normalization': None if test_model_type == 'SimpleAutoencoder' else 'batch'
                    },
                    #'training_config': {
                    'training': {
                        'batch_size': test_batch_size,
                        'epochs': test_epochs,
                        'learning_rate': test_learning_rate,
                        'patience': max(3, test_epochs // 3),
                        'weight_decay': 1e-4,
                        'gradient_clip': 1.0,
                        'mixed_precision': test_mixed_precision,
                        'optimizer_type': 'AdamW',
                        'scheduler_type': 'ReduceLROnPlateau',
                        'early_stopping': True,
                        'validation_split': 0.2
                    },
                    #'data_config': {
                    'data': {
                        'normal_samples': test_normal_samples,
                        'attack_samples': test_attack_samples,
                        'features': test_features,
                        'use_real_data': False,
                        'data_preprocessing': True
                    },
                    #'system_config': {
                    'system': {
                        'model_dir': test_results_dir / "training_test",
                        'device': str(device),
                        'random_seed': 42,
                        'reproducible': True
                    },
                    #'monitoring_config': {
                    'monitoring': {
                        'verbose': False,
                        'debug_mode': False,
                        'tensorboard_logging': False,
                        'save_checkpoints': False,
                        'progress_bar': False
                    },
                    #'export_config': {
                    'export': {
                        'export_onnx': False,
                        'save_model': False,
                        'save_metadata': False,
                        'save_training_history': False
                    },
                    'advanced_training': {
                        'num_workers': test_num_workers,
                        'pin_memory': False,
                        'memory_efficient': test_memory_efficient
                    },
                    'error_handling': {
                        'error_handling': 'strict',
                        'graceful_degradation': graceful_degradation
                    }
                }
                
                # Run training
                training_results = train_model(config=stability_training_config)
                
                # Analyze training results
                if training_results and training_results.get('success', False):
                    final_metrics = training_results.get('final_metrics', {})
                    model_info = training_results.get('model_info', {})
                    training_time = training_results.get('training_time_minutes', 0)
                    
                    training_test_results['details'].update({
                        'training_completion': 'SUCCESS',
                        'epochs_completed': final_metrics.get('final_epoch', 0),
                        'best_val_loss': final_metrics.get('best_validation_loss', float('inf')),
                        'test_loss': final_metrics.get('test_loss', float('inf')),
                        'training_time_minutes': training_time,
                        'model_parameters': model_info.get('parameters', 0),
                        'threshold_calculated': final_metrics.get('threshold', 0),
                        'anomaly_rate': final_metrics.get('anomaly_detection_rate', 0)
                    })
                    
                    # Determine training quality
                    best_val_loss = final_metrics.get('best_validation_loss', float('inf'))
                    if best_val_loss < 0.05:
                        training_quality = 'EXCELLENT'
                    elif best_val_loss < 0.1:
                        training_quality = 'GOOD'
                    elif best_val_loss < 0.5:
                        training_quality = 'ACCEPTABLE'
                    else:
                        training_quality = 'POOR'
                    
                    training_test_results['details']['training_quality'] = training_quality
                    training_test_results['status'] = 'PASSED'
                    
                    if verbose:
                        print(Fore.GREEN + Style.BRIGHT + f"Training stability test passed:")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Epochs: {final_metrics.get('final_epoch', 0)}/{test_epochs}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Best validation loss: {best_val_loss:.6f}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Training quality: {training_quality}")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Training time: {training_time:.1f} minutes")
                        print(Fore.GREEN + Style.BRIGHT + f"  └─ Threshold: {final_metrics.get('threshold', 0):.6f}")
                
                else:
                    # Training failed or returned error
                    error_info = training_results.get('error', 'Unknown training error') if training_results else 'No training results returned'
                    training_test_results['details']['training_completion'] = 'FAILED'
                    training_test_results['details']['error_info'] = error_info
                    training_test_results['status'] = 'FAILED'
                    test_results['errors'].append(f"Training failed: {error_info}")
                    
                    if verbose:
                        print(Fore.RED + Style.BRIGHT + f"Training stability test failed: {error_info}")
                
                training_test_results['duration_seconds'] = time.time() - training_start_time
            
            except Exception as e:
                training_test_results['status'] = 'FAILED'
                training_test_results['error'] = str(e)
                training_test_results['duration_seconds'] = time.time() - training_start_time
                test_results['errors'].append(f"Training stability test failed: {e}")
                
                if verbose:
                    print(Fore.RED + Style.BRIGHT + f"Training stability test failed: {e}")
            
            test_results['test_results']['training_stability'] = training_test_results
            
            # Test 5: Performance Benchmarking (if enabled)
            if performance_benchmarking or comprehensive_test:
                logger.info("Test 5: Performance Benchmarking")
                test_results['tests_run'].append('performance_benchmarking')
                
                try:
                    perf_test_results = {
                        'test_name': 'performance_benchmarking',
                        'status': 'UNKNOWN',
                        'details': {},
                        'duration_seconds': 0,
                        'error': None
                    }
                    
                    perf_start_time = time.time()
                    
                    # Memory usage monitoring
                    if torch.cuda.is_available():
                        torch.cuda.reset_peak_memory_stats(device)
                        initial_memory = torch.cuda.memory_allocated(device)
                    
                    # Throughput testing
                    model.eval()
                    throughput_samples = 1000
                    throughput_data = torch.randn(throughput_samples, test_features, device=device)
                    
                    # Warmup
                    with torch.no_grad():
                        for _ in range(10):
                            _ = model(throughput_data[:32])
                    
                    # Actual throughput test
                    torch.cuda.synchronize() if torch.cuda.is_available() else None
                    throughput_start = time.time()
                    
                    with torch.no_grad():
                        throughput_output = model(throughput_data)
                    
                    torch.cuda.synchronize() if torch.cuda.is_available() else None
                    throughput_time = time.time() - throughput_start
                    
                    samples_per_second = throughput_samples / throughput_time
                    
                    perf_test_results['details'].update({
                        'throughput_test': 'SUCCESS',
                        'samples_per_second': samples_per_second,
                        'inference_time_ms': throughput_time * 1000,
                        'throughput_samples': throughput_samples
                    })
                    
                    # Memory usage
                    if torch.cuda.is_available():
                        peak_memory = torch.cuda.max_memory_allocated(device)
                        memory_usage_mb = (peak_memory - initial_memory) / (1024 * 1024)
                        perf_test_results['details'].update({
                            'memory_usage_mb': memory_usage_mb,
                            'peak_memory_mb': peak_memory / (1024 * 1024)
                        })
                    
                    perf_test_results['duration_seconds'] = time.time() - perf_start_time
                    perf_test_results['status'] = 'PASSED'
                    
                    # Store performance metrics
                    test_results['performance_metrics'] = {
                        'samples_per_second': samples_per_second,
                        'inference_time_ms': throughput_time * 1000,
                        'memory_usage_mb': perf_test_results['details'].get('memory_usage_mb', 0)
                    }
                    
                    if verbose:
                        print(Fore.GREEN + Style.BRIGHT + f"Performance benchmarking passed:")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Throughput: {samples_per_second:.1f} samples/sec")
                        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Inference time: {throughput_time * 1000:.2f} ms")
                        if torch.cuda.is_available():
                            print(Fore.GREEN + Style.BRIGHT + f"  └─ Memory usage: {memory_usage_mb:.1f} MB")
                
                except Exception as e:
                    perf_test_results['status'] = 'FAILED'
                    perf_test_results['error'] = str(e)
                    perf_test_results['duration_seconds'] = time.time() - perf_start_time
                    test_results['warnings'].append(f"Performance benchmarking failed: {e}")
                    
                    if verbose:
                        print(Fore.RED + Style.BRIGHT + f"Performance benchmarking failed: {e}")
                
                test_results['test_results']['performance_benchmarking'] = perf_test_results
            
            # Additional tests for comprehensive mode
            if comprehensive_test:
                # Test different model types
                if test_all_models:
                    logger.info("Test 6: Multiple Model Types")
                    test_results['tests_run'].append('multiple_models')
                    
                    model_types_to_test = ['SimpleAutoencoder', 'EnhancedAutoencoder']
                    multi_model_results = {}
                    
                    for mt in model_types_to_test:
                        if mt != test_model_type:  # Skip already tested model
                            try:
                                # Quick test for each model type
                                quick_config = stability_training_config.copy()
                                quick_config['model_architecture']['model_type'] = mt
                                quick_config['training_config']['epochs'] = 3
                                quick_config['training_config']['verbose'] = False
                                
                                result = train_model(config=quick_config)
                                success = result and result.get('success', False)
                                
                                multi_model_results[mt] = {
                                    'status': 'PASSED' if success else 'FAILED',
                                    'final_loss': result.get('final_metrics', {}).get('best_validation_loss', float('inf')) if result else float('inf')
                                }
                                
                            except Exception as e:
                                multi_model_results[mt] = {
                                    'status': 'FAILED',
                                    'error': str(e)
                                }
                    
                    test_results['test_results']['multiple_models'] = {
                        'test_name': 'multiple_models',
                        'status': 'PASSED' if all(r.get('status') == 'PASSED' for r in multi_model_results.values()) else 'PARTIAL',
                        'details': multi_model_results
                    }
                    
                    if verbose:
                        for mt, result in multi_model_results.items():
                            status_symbol = "✓" if result['status'] == 'PASSED' else "✗"
                            print(Fore.GREEN + Style.BRIGHT + f"  {status_symbol} {mt}: {result['status']}")
            
            # Stress test
            if stress_test:
                logger.info("Test 7: Stress Testing")
                test_results['tests_run'].append('stress_test')
                
                try:
                    stress_test_results = {
                        'test_name': 'stress_test',
                        'status': 'UNKNOWN',
                        'details': {},
                        'duration_seconds': 0,
                        'error': None
                    }
                    
                    stress_start_time = time.time()
                    
                    # Memory stress test
                    large_batch_size = min(512, test_batch_size * 8)
                    large_data = torch.randn(large_batch_size, test_features, device=device)
                    
                    # Repeated inference
                    stress_iterations = 100
                    successful_iterations = 0
                    
                    model.eval()
                    for i in range(stress_iterations):
                        try:
                            with torch.no_grad():
                                _ = model(large_data)
                            successful_iterations += 1
                        except Exception as e:
                            if i == 0:  # Fail immediately if first iteration fails
                                raise e
                            break
                    
                    stress_success_rate = successful_iterations / stress_iterations
                    
                    stress_test_results['details'].update({
                        'stress_iterations': stress_iterations,
                        'successful_iterations': successful_iterations,
                        'success_rate': stress_success_rate,
                        'large_batch_size': large_batch_size
                    })
                    
                    stress_test_results['duration_seconds'] = time.time() - stress_start_time
                    stress_test_results['status'] = 'PASSED' if stress_success_rate > 0.95 else 'PARTIAL' if stress_success_rate > 0.8 else 'FAILED'
                    
                    if verbose:
                        status_symbol = "✓" if stress_test_results['status'] == 'PASSED' else "⚠" if stress_test_results['status'] == 'PARTIAL' else "✗"
                        status_color = Fore.GREEN if stress_test_results['status'] == 'PASSED' else Fore.YELLOW if stress_test_results['status'] == 'PARTIAL' else Fore.RED
                        print(status_color + Style.BRIGHT + f"{status_symbol} Stress test: {successful_iterations}/{stress_iterations} iterations successful")
                
                except Exception as e:
                    stress_test_results['status'] = 'FAILED'
                    stress_test_results['error'] = str(e)
                    stress_test_results['duration_seconds'] = time.time() - stress_start_time
                    test_results['warnings'].append(f"Stress test failed: {e}")
                    
                    if verbose:
                        print(Fore.RED + Style.BRIGHT + f"Stress test failed: {e}")
                
                test_results['test_results']['stress_test'] = stress_test_results
            
            # Calculate overall test duration
            total_test_time = time.time() - test_start_time
            test_results['total_duration_seconds'] = total_test_time
            
            # Determine overall status
            test_statuses = [result.get('status', 'UNKNOWN') for result in test_results['test_results'].values()]
            
            if all(status == 'PASSED' for status in test_statuses):
                overall_status = 'PASSED'
                status_description = "All tests passed successfully"
            elif any(status == 'FAILED' for status in test_statuses):
                if any(status == 'PASSED' for status in test_statuses):
                    overall_status = 'PARTIAL'
                    status_description = "Some tests failed"
                else:
                    overall_status = 'FAILED'
                    status_description = "Multiple tests failed"
            elif any(status == 'PARTIAL' for status in test_statuses):
                overall_status = 'PARTIAL'
                status_description = "Tests completed with warnings"
            else:
                overall_status = 'UNKNOWN'
                status_description = "Test status unclear"
            
            test_results['overall_status'] = overall_status
            test_results['status_description'] = status_description
            
            # Generate recommendations
            recommendations = []
            
            # Performance recommendations
            if test_results.get('performance_metrics', {}).get('samples_per_second', 0) < 100:
                recommendations.append("Consider using GPU acceleration for better performance")
            
            # Memory recommendations
            if test_results.get('performance_metrics', {}).get('memory_usage_mb', 0) > 1000:
                recommendations.append("High memory usage detected - consider reducing batch size")
            
            # Training quality recommendations
            training_result = test_results['test_results'].get('training_stability', {})
            if training_result.get('status') == 'PASSED':
                best_loss = training_result.get('details', {}).get('best_val_loss', float('inf'))
                if best_loss > 0.1:
                    recommendations.append("Training loss is high - consider adjusting learning rate or model architecture")
                elif best_loss > 0.05:
                    recommendations.append("Training performance is acceptable but could be improved")
            
            # Hardware recommendations
            if not torch.cuda.is_available():
                recommendations.append("CUDA not available - GPU acceleration would improve performance")
            
            # Error-based recommendations
            if test_results['errors']:
                recommendations.append("Errors detected - check system compatibility and dependencies")
            
            test_results['recommendations'] = recommendations
            
            # Summary statistics
            test_results['summary'] = {
                'total_tests_run': len(test_results['tests_run']),
                'tests_passed': sum(1 for result in test_results['test_results'].values() if result.get('status') == 'PASSED'),
                'tests_failed': sum(1 for result in test_results['test_results'].values() if result.get('status') == 'FAILED'),
                'tests_partial': sum(1 for result in test_results['test_results'].values() if result.get('status') == 'PARTIAL'),
                'total_errors': len(test_results['errors']),
                'total_warnings': len(test_results['warnings']),
                'total_duration_minutes': total_test_time / 60,
                'overall_status': overall_status,
                'system_stable': overall_status in ['PASSED', 'PARTIAL'] and len(test_results['errors']) == 0
            }
            
            # Save test results if requested
            if save_test_results:
                try:
                    results_file = test_results_dir / f"stability_test_results_{test_timestamp}.json"
                    with open(results_file, 'w') as f:
                        json.dump(test_results, f, indent=2, default=str)
                    test_results['results_file'] = str(results_file)
                    
                    if verbose:
                        print(Fore.GREEN + Style.BRIGHT + f"\nTest results saved to: {results_file}")
                
                except Exception as e:
                    logger.warning(f"Failed to save test results: {e}")
                    test_results['warnings'].append(f"Failed to save results: {e}")
            
            # Display comprehensive results
            if verbose:
                print(Fore.CYAN + Style.BRIGHT + "\n" + "="*40)
                print(Fore.YELLOW + Style.BRIGHT + "STABILITY TEST RESULTS")
                print(Fore.CYAN + Style.BRIGHT + "="*40)
                
                # Overall status
                status_symbol = "✓" if overall_status == 'PASSED' else "⚠" if overall_status == 'PARTIAL' else "✗"
                status_color = Fore.GREEN if overall_status == 'PASSED' else Fore.YELLOW if overall_status == 'PARTIAL' else Fore.RED
                print(Fore.WHITE + Style.BRIGHT + f"\nOverall Status: " + status_color + Style.BRIGHT + f"{status_symbol} {overall_status} - {status_description}")
                
                # Summary
                summary = test_results['summary']
                print(Fore.YELLOW + Style.BRIGHT + f"\nTest Summary:")
                print(Fore.CYAN + Style.BRIGHT + f"- Total Tests: " + Fore.GREEN + Style.BRIGHT + f"{summary['total_tests_run']}")
                print(Fore.CYAN + Style.BRIGHT + f"- Passed: " + Fore.GREEN + Style.BRIGHT + f"{summary['tests_passed']}")
                print(Fore.CYAN + Style.BRIGHT + f"- Failed: " + Fore.GREEN + Style.BRIGHT + f"{summary['tests_failed']}")
                print(Fore.CYAN + Style.BRIGHT + f"- Partial: " + Fore.GREEN + Style.BRIGHT + f"{summary['tests_partial']}")
                print(Fore.CYAN + Style.BRIGHT + f"- Duration: " + Fore.GREEN + Style.BRIGHT + f"{summary['total_duration_minutes']:.1f} minutes")
                
                # System information
                if test_results['system_info']:
                    si = test_results['system_info']
                    print(Fore.YELLOW + Style.BRIGHT + f"\nSystem Information:")
                    print(Fore.CYAN + Style.BRIGHT + f"- Platform: " + Fore.GREEN + Style.BRIGHT + f"{si.get('system', 'Unknown')} {si.get('machine', '')}")
                    print(Fore.CYAN + Style.BRIGHT + f"- Python: " + Fore.GREEN + Style.BRIGHT + f"{si.get('python_version', '').split()[0]}")
                    print(Fore.CYAN + Style.BRIGHT + f"- PyTorch: " + Fore.GREEN + Style.BRIGHT + f"{si.get('pytorch_version', 'Unknown')}")
                    print(Fore.CYAN + Style.BRIGHT + f"- Device: " + Fore.GREEN + Style.BRIGHT + f"{si.get('device', 'Unknown')}")
                    if si.get('gpu_count', 0) > 0:
                        print(Fore.CYAN + Style.BRIGHT + f"- GPUs: " + Fore.GREEN + Style.BRIGHT + f"{si.get('gpu_count', 0)}")
                
                # Performance metrics
                if test_results.get('performance_metrics'):
                    pm = test_results['performance_metrics']
                    print(Fore.YELLOW + Style.BRIGHT + f"\nPerformance Metrics:")
                    print(Fore.CYAN + Style.BRIGHT + f"- Throughput: " + Fore.GREEN + Style.BRIGHT + f"{pm.get('samples_per_second', 0):.1f} samples/sec")
                    print(Fore.CYAN + Style.BRIGHT + f"- Inference Time: " + Fore.GREEN + Style.BRIGHT + f"{pm.get('inference_time_ms', 0):.2f} ms")
                    if pm.get('memory_usage_mb'):
                        print(Fore.CYAN + Style.BRIGHT + f"- Memory Usage: " + Fore.GREEN + Style.BRIGHT + f"{pm.get('memory_usage_mb', 0):.1f} MB")
                
                # Recommendations
                if test_results['recommendations']:
                    print(Fore.YELLOW + Style.BRIGHT + f"\nRecommendations:")
                    for i, rec in enumerate(test_results['recommendations'], 1):
                        print(Fore.CYAN + Style.BRIGHT + f"{i}. " + Fore.GREEN + Style.BRIGHT + f"{rec}")
                
                # Errors and warnings
                if test_results['errors']:
                    print(Fore.YELLOW + Style.BRIGHT + f"\nErrors ({len(test_results['errors'])}):")
                    for i, error in enumerate(test_results['errors'], 1):
                        print(Fore.YELLOW + Style.BRIGHT + f"{i}. " + Fore.RED + Style.BRIGHT + f"{error}")
                
                if test_results['warnings']:
                    print(Fore.YELLOW + Style.BRIGHT + f"\nWarnings ({len(test_results['warnings'])}):")
                    for i, warning in enumerate(test_results['warnings'], 1):
                        print(Fore.WHITE + Style.BRIGHT + f"{i}. " + Fore.YELLOW + Style.BRIGHT + f"{warning}")
                
                print(Fore.CYAN + Style.BRIGHT + "\n" + "="*40)
                
                # Final recommendation
                if overall_status == 'PASSED':
                    final_msg = "System is stable and ready for production use!"
                    final_color = Fore.GREEN
                elif overall_status == 'PARTIAL':
                    final_msg = "System is functional but may need attention for optimal performance."
                    final_color = Fore.YELLOW
                else:
                    final_msg = "System has stability issues that should be addressed before use."
                    final_color = Fore.RED
                
                print(final_color + Style.BRIGHT + final_msg)
                print(Fore.CYAN + Style.BRIGHT + "="*40)
            
            # Cleanup
            try:
                if 'model' in locals():
                    del model
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
            except Exception as cleanup_error:
                logger.warning(f"Cleanup failed: {cleanup_error}")
            
            return test_results
            
        except Exception as e:
            # Handle unexpected errors
            test_results['overall_status'] = 'ERROR'
            test_results['error'] = str(e)
            test_results['traceback'] = traceback.format_exc()
            test_results['total_duration_seconds'] = time.time() - test_start_time
            
            logger.error(f"Stability test encountered unexpected error: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            if verbose:
                print(Fore.RED + Style.BRIGHT + f"\nSTABILITY TEST ERROR: {e}")
                print(Fore.YELLOW + Style.BRIGHT + "\nPlease check system configuration and dependencies.")
            
            # Save error results if possible
            if save_test_results:
                try:
                    error_file = test_results_dir / f"stability_test_error_{test_timestamp}.json"
                    with open(error_file, 'w') as f:
                        json.dump(test_results, f, indent=2, default=str)
                    test_results['error_file'] = str(error_file)
                except Exception as save_error:
                    logger.error(f"Failed to save error results: {save_error}")
            
            return test_results
        
        finally:
            # Restore logging level
            if verbose and 'original_level' in locals():
                try:
                    for handler in handlers_to_suppress:
                        handler.setLevel(logging.ERROR)
                except Exception:
                    pass
            
            # Final cleanup
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
            except Exception:
                pass
            
    except Exception as e:
        # Top-level error handling
        error_msg = f"Stability test initialization failed: {e}"
        logger.error(error_msg, exc_info=True)
        
        console.print(
            Panel.fit(
                f"{error_msg}",
                title="STABILITY TEST INITIALIZATION ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        return {
            'overall_status': 'FAILED',
            'errors': [error_msg],
            'tests_run': [],
            'test_results': {},
            'summary': {'total_tests': 0, 'passed_tests': 0, 'failed_tests': 0}
        }

def hyperparameter_search(
    # Data Parameters
    X_train: Optional[np.ndarray] = None,
    X_val: Optional[np.ndarray] = None,
    X_test: Optional[np.ndarray] = None,
    data: Optional[Dict[str, Any]] = None,
    use_real_data: Optional[bool] = None,
    data_path: Optional[Union[str, Path]] = None,
    artifacts_path: Optional[Union[str, Path]] = None,
    
    # Search Configuration Parameters
    n_trials: Optional[int] = None,
    timeout_seconds: Optional[float] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    
    # Model Type Selection
    model_types: Optional[List[str]] = None,
    search_all_models: Optional[bool] = None,
    
    # Cross-Validation Parameters
    cv_folds: Optional[int] = None,
    cv_shuffle: Optional[bool] = None,
    cv_random_state: Optional[int] = None,
    
    # Search Space Configuration
    search_space: Optional[Dict[str, Any]] = None,
    parameter_ranges: Optional[Dict[str, Any]] = None,
    fixed_params: Optional[Dict[str, Any]] = None,
    
    # Training Configuration
    trial_epochs: Optional[int] = None,
    trial_patience: Optional[int] = None,
    trial_batch_size: Optional[int] = None,
    
    # System Parameters
    device: Optional[str] = None,
    random_seed: Optional[int] = None,
    num_workers: Optional[int] = None,
    
    # Output Parameters
    verbose: Optional[bool] = None,
    save_study: Optional[bool] = None,
    study_dir: Optional[Union[str, Path]] = None,
    generate_plots: Optional[bool] = None,
    
    # Storage Parameters
    storage_url: Optional[str] = None,
    load_if_exists: Optional[bool] = None,
    
    # Optimization Callbacks
    early_stopping_patience: Optional[int] = None,
    early_stopping_min_trials: Optional[int] = None,
    
    # Advanced Parameters
    parallel_jobs: Optional[int] = None,
    memory_limit: Optional[str] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    hpo_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """
    Comprehensive hyperparameter search that integrates with the current train_model() implementation.
    
    Returns:
        Dictionary containing optimization results, best parameters, and study information.
    """
    
    # Start timing
    start_time = datetime.now()
    hpo_start_time = time.time()
    
    # Initialize configuration
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply HPO-specific configuration
    if hpo_config:
        config.setdefault('hyperparameter_optimization', {}).update(hpo_config)
    
    # Apply all parameters to configuration
    final_config = config.copy()
    
    # Apply individual parameters
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'hpo_config', 'kwargs', 'start_time', 'hpo_start_time',
        'datetime', 'traceback', 'time', 'gc', 'warnings', 'Path'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Set up HPO configuration with intelligent defaults
    hpo_config_section = final_config.setdefault('hyperparameter_optimization', {})
    
    # Core HPO parameters
    n_trials = hpo_config_section.setdefault('n_trials', cleaned_params.get('n_trials', 100))
    timeout_seconds = hpo_config_section.setdefault('timeout_seconds', cleaned_params.get('timeout_seconds', 0))
    study_name = hpo_config_section.setdefault('study_name', cleaned_params.get('study_name', f"autoencoder_hpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"))
    direction = hpo_config_section.setdefault('direction', cleaned_params.get('direction', 'minimize'))
    sampler_type = hpo_config_section.setdefault('sampler_type', cleaned_params.get('sampler_type', 'TPE'))
    pruner_type = hpo_config_section.setdefault('pruner_type', cleaned_params.get('pruner_type', 'MedianPruner'))
    
    # Model selection
    model_types = hpo_config_section.setdefault('model_types', cleaned_params.get('model_types', ['SimpleAutoencoder', 'EnhancedAutoencoder']))
    search_all_models = hpo_config_section.setdefault('search_all_models', cleaned_params.get('search_all_models', False))
    
    if search_all_models:
        model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
    
    # Cross-validation parameters
    cv_folds = hpo_config_section.setdefault('cv_folds', cleaned_params.get('cv_folds', 3))
    cv_shuffle = hpo_config_section.setdefault('cv_shuffle', cleaned_params.get('cv_shuffle', True))
    cv_random_state = hpo_config_section.setdefault('cv_random_state', cleaned_params.get('cv_random_state', 42))
    
    # Training parameters for trials
    trial_epochs = hpo_config_section.setdefault('trial_epochs', cleaned_params.get('trial_epochs', 20))
    trial_patience = hpo_config_section.setdefault('trial_patience', cleaned_params.get('trial_patience', 5))
    trial_batch_size = hpo_config_section.setdefault('trial_batch_size', cleaned_params.get('trial_batch_size', 64))
    
    # System parameters
    device = hpo_config_section.setdefault('device', cleaned_params.get('device', 'auto'))
    random_seed = hpo_config_section.setdefault('random_seed', cleaned_params.get('random_seed', 42))
    num_workers = hpo_config_section.setdefault('num_workers', cleaned_params.get('num_workers', 0))
    
    # Output parameters
    verbose = hpo_config_section.setdefault('verbose', cleaned_params.get('verbose', True))
    save_study = hpo_config_section.setdefault('save_study', cleaned_params.get('save_study', True))
    study_dir = hpo_config_section.setdefault('study_dir', cleaned_params.get('study_dir', DEFAULT_MODEL_DIR / "hpo_studies"))
    generate_plots = hpo_config_section.setdefault('generate_plots', cleaned_params.get('generate_plots', True))
    
    # Storage parameters
    storage_url = hpo_config_section.setdefault('storage_url', cleaned_params.get('storage_url', None))
    load_if_exists = hpo_config_section.setdefault('load_if_exists', cleaned_params.get('load_if_exists', False))
    
    # Early stopping
    early_stopping_patience = hpo_config_section.setdefault('early_stopping_patience', cleaned_params.get('early_stopping_patience', 10))
    early_stopping_min_trials = hpo_config_section.setdefault('early_stopping_min_trials', cleaned_params.get('early_stopping_min_trials', 20))
    
    # Advanced parameters
    parallel_jobs = hpo_config_section.setdefault('parallel_jobs', cleaned_params.get('parallel_jobs', 1))
    memory_limit = hpo_config_section.setdefault('memory_limit', cleaned_params.get('memory_limit', None))
    
    # Set up logging
    # if verbose:
    #     original_level = logger.level
    #     logger.setLevel(logging.INFO)
    
    try:
        # Display HPO header
        if verbose:
            print("\n" + "="*80)
            print("HYPERPARAMETER OPTIMIZATION")
            print("="*80)
            print(f"Study: {study_name}")
            print(f"Trials: {n_trials}")
            print(f"Timeout: {timeout_seconds}s" if timeout_seconds > 0 else "Timeout: None")
            print(f"Model Types: {', '.join(model_types)}")
            print(f"CV Folds: {cv_folds}")
            print(f"Sampler: {sampler_type}")
            print(f"Pruner: {pruner_type}")
            print("-"*80)
        
        # Prepare data
        if data is None:
            if X_train is not None:
                # Create data dictionary from provided arrays
                data = {
                    'X_train': X_train,
                    'X_val': X_val if X_val is not None else None,
                    'X_test': X_test if X_test is not None else None,
                    'feature_names': [f'feature_{i}' for i in range(X_train.shape[1])]
                }
            else:
                # Generate or load data based on configuration
                if use_real_data:
                    try:
                        data = load_and_validate_data(
                            data_path=data_path,
                            artifacts_path=artifacts_path,
                            config=final_config
                        )
                        logger.info("Loaded real data for HPO")
                    except Exception as e:
                        logger.warning(f"Failed to load real data: {e}, falling back to synthetic data")
                        data = generate_synthetic_data(
                            normal_samples=10000,
                            attack_samples=2000,
                            features=78,
                            validation_split=0.2,
                            random_state=random_seed,
                            config=final_config
                        )
                else:
                    data = generate_synthetic_data(
                        normal_samples=10000,
                        attack_samples=2000,
                        features=78,
                        validation_split=0.2,
                        random_state=random_seed,
                        config=final_config
                    )
        
        # Validate data
        if not isinstance(data, dict) or 'X_train' not in data:
            raise ValueError("Invalid data format. Expected dictionary with 'X_train' key.")
        
        X_train = data['X_train']
        X_val = data.get('X_val')
        X_test = data.get('X_test')
        feature_names = data.get('feature_names', [f'feature_{i}' for i in range(X_train.shape[1])])
        input_dim = X_train.shape[1]
        
        if verbose:
            print(f"Data prepared: {X_train.shape[0]} train samples, {input_dim} features")
            if X_val is not None:
                print(f"Validation samples: {X_val.shape[0]}")
            if X_test is not None:
                print(f"Test samples: {X_test.shape[0]}")
        
        # Set up cross-validation
        if X_val is None:
            kf = KFold(n_splits=cv_folds, shuffle=cv_shuffle, random_state=cv_random_state)
            cv_splits = list(kf.split(X_train))
        else:
            # Use fixed train/validation split
            cv_splits = [(np.arange(len(X_train)), np.arange(len(X_val)))]
            cv_folds = 1
        
        # Create study directory
        study_dir = Path(study_dir)
        study_dir.mkdir(parents=True, exist_ok=True)
        
        # Set up Optuna sampler
        if sampler_type == 'TPE':
            sampler = TPESampler(
                seed=random_seed,
                consider_prior=True,
                consider_magic_clip=True,
                consider_endpoints=False,
                n_startup_trials=min(10, n_trials // 10),
                multivariate=True
            )
        elif sampler_type == 'Random':
            sampler = RandomSampler(seed=random_seed)
        elif sampler_type == 'CmaEs':
            sampler = CmaEsSampler(seed=random_seed)
        else:
            logger.warning(f"Unknown sampler type '{sampler_type}', using TPE")
            sampler = TPESampler(seed=random_seed)
        
        # Set up Optuna pruner
        if pruner_type == 'MedianPruner':
            pruner = MedianPruner(
                n_startup_trials=5,
                n_warmup_steps=5,
                interval_steps=1
            )
        elif pruner_type == 'HyperbandPruner':
            pruner = HyperbandPruner(
                min_resource=1,
                max_resource=trial_epochs,
                reduction_factor=3
            )
        elif pruner_type == 'NopPruner' or pruner_type == 'None':
            pruner = NopPruner()
        else:
            logger.warning(f"Unknown pruner type '{pruner_type}', using MedianPruner")
            pruner = MedianPruner()
        
        # Set up storage if specified
        storage = None
        if storage_url:
            try:
                storage = optuna.storages.RDBStorage(
                    url=storage_url,
                    heartbeat_interval=60,
                    grace_period=120
                )
            except Exception as e:
                logger.warning(f"Failed to setup storage: {e}")
        
        # Create study
        study = optuna.create_study(
            direction=direction,
            sampler=sampler,
            pruner=pruner,
            study_name=study_name,
            storage=storage,
            load_if_exists=load_if_exists
        )
        
        # Initialize results tracking
        hpo_results = {
            'start_time': start_time.isoformat(),
            'study_name': study_name,
            'configuration': hpo_config_section,
            'data_info': {
                'train_samples': len(X_train),
                'val_samples': len(X_val) if X_val is not None else 0,
                'test_samples': len(X_test) if X_test is not None else 0,
                'features': input_dim,
                'cv_folds': cv_folds
            },
            'trials_completed': 0,
            'best_value': float('inf'),
            'best_params': {},
            'best_config': {},
            'optimization_history': [],
            'model_performance': {},
            'errors': [],
            'warnings': []
        }
        
        # Define search space based on model types
        def get_search_space_for_model(trial, model_type, input_dim):
            """Define search space for specific model type"""
            
            if model_type == 'SimpleAutoencoder':
                # SimpleAutoencoder search space
                #encoding_dim = trial.suggest_int('encoding_dim', 8, min(64, input_dim // 2), step=8)
                min_encoding_dim = min(8, max(4, input_dim // 4))  # Adjust min based on input_dim
                max_encoding_dim = min(64, input_dim // 2)
                if max_encoding_dim < min_encoding_dim:
                    max_encoding_dim = min_encoding_dim
                encoding_dim = trial.suggest_int('encoding_dim', min_encoding_dim, max_encoding_dim)
                
                # Hidden layer configurations
                hidden_choice = trial.suggest_categorical('hidden_choice', [
                    'single_small', 'single_medium', 'single_large', 'double_small', 'double_medium'
                ])
                
                if hidden_choice == 'single_small':
                    hidden_dims = [max(16, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif hidden_choice == 'single_medium':
                    hidden_dims = [max(32, encoding_dim * 4)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif hidden_choice == 'single_large':
                    hidden_dims = [max(64, encoding_dim * 8)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif hidden_choice == 'double_small':
                    hidden_dims = [max(32, encoding_dim * 4), max(16, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.1, 0.4),
                        trial.suggest_float('dropout_1', 0.05, 0.3)
                    ]
                else:  # double_medium
                    hidden_dims = [max(64, encoding_dim * 8), max(32, encoding_dim * 4)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.15, 0.4),
                        trial.suggest_float('dropout_1', 0.1, 0.3)
                    ]
                
                return {
                    'model_type': 'SimpleAutoencoder',
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'activation': trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'gelu', 'tanh']),
                    'normalization': trial.suggest_categorical('normalization', [None, 'batch']),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'weight_init': trial.suggest_categorical('weight_init', ['xavier_uniform', 'xavier_normal', 'kaiming_uniform']),
                    'bias': trial.suggest_categorical('bias', [True, False]),
                    'mixed_precision': trial.suggest_categorical('mixed_precision', [False, True])
                }
            
            elif model_type == 'EnhancedAutoencoder':
                # EnhancedAutoencoder search space
                #encoding_dim = trial.suggest_int('encoding_dim', 16, min(64, input_dim // 2), step=8)
                min_encoding_dim = min(8, max(4, input_dim // 4))  # Adjust min based on input_dim
                max_encoding_dim = min(64, input_dim // 2)
                if max_encoding_dim < min_encoding_dim:
                    max_encoding_dim = min_encoding_dim
                encoding_dim = trial.suggest_int('encoding_dim', min_encoding_dim, max_encoding_dim)
                
                # Architecture choices
                arch_choice = trial.suggest_categorical('arch_choice', [
                    'compact', 'balanced', 'deep', 'wide'
                ])
                
                if arch_choice == 'compact':
                    hidden_dims = [max(32, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.3)]
                elif arch_choice == 'balanced':
                    hidden_dims = [max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.15, 0.35),
                        trial.suggest_float('dropout_1', 0.1, 0.25)
                    ]
                elif arch_choice == 'deep':
                    hidden_dims = [max(128, encoding_dim * 8), max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.2, 0.4),
                        trial.suggest_float('dropout_1', 0.15, 0.35),
                        trial.suggest_float('dropout_2', 0.1, 0.3)
                    ]
                else:  # wide
                    hidden_dims = [max(256, encoding_dim * 16), max(128, encoding_dim * 8)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.25, 0.45),
                        trial.suggest_float('dropout_1', 0.2, 0.4)
                    ]
                
                return {
                    'model_type': 'EnhancedAutoencoder',
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'activation': trial.suggest_categorical('activation', ['leaky_relu', 'gelu', 'relu', 'swish']),
                    'normalization': trial.suggest_categorical('normalization', ['batch', 'layer', 'instance']),
                    'use_attention': trial.suggest_categorical('use_attention', [False, True]),
                    'residual_blocks': trial.suggest_categorical('residual_blocks', [False, True]),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'weight_init': trial.suggest_categorical('weight_init', ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal']),
                    'bias': trial.suggest_categorical('bias', [True, False]),
                    'legacy_mode': trial.suggest_categorical('legacy_mode', [False, True]),
                    'mixed_precision': trial.suggest_categorical('mixed_precision', [False, True])
                }
            
            elif model_type == 'AutoencoderEnsemble':
                # AutoencoderEnsemble search space
                encoding_dim = trial.suggest_int('encoding_dim', 16, min(48, input_dim // 3), step=8)
                num_models = trial.suggest_int('num_models', 3, 7, step=2)
                diversity_factor = trial.suggest_float('diversity_factor', 0.1, 0.5)
                
                # Ensemble architecture
                ensemble_arch = trial.suggest_categorical('ensemble_arch', ['small', 'medium', 'large'])
                
                if ensemble_arch == 'small':
                    hidden_dims = [max(32, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.15, 0.35)]
                elif ensemble_arch == 'medium':
                    hidden_dims = [max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.2, 0.4),
                        trial.suggest_float('dropout_1', 0.15, 0.35)
                    ]
                else:  # large
                    hidden_dims = [max(128, encoding_dim * 8), max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.25, 0.45),
                        trial.suggest_float('dropout_1', 0.2, 0.4),
                        trial.suggest_float('dropout_2', 0.15, 0.35)
                    ]
                
                return {
                    'model_type': 'AutoencoderEnsemble',
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'num_models': num_models,
                    'diversity_factor': diversity_factor,
                    'activation': trial.suggest_categorical('activation', ['leaky_relu', 'gelu', 'relu']),
                    'normalization': trial.suggest_categorical('normalization', ['batch', 'layer']),
                    'use_attention': trial.suggest_categorical('use_attention', [False, True]),
                    'residual_blocks': trial.suggest_categorical('residual_blocks', [False, True]),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'weight_init': trial.suggest_categorical('weight_init', ['xavier_uniform', 'kaiming_uniform']),
                    'bias': trial.suggest_categorical('bias', [True, False]),
                    'min_features': trial.suggest_int('min_features', 3, 10),
                    'mixed_precision': trial.suggest_categorical('mixed_precision', [False, True])
                }
            
            else:
                raise ValueError(f"Unknown model type: {model_type}")
        
        # Define objective function
        def objective(trial):
            try:
                # Select model type
                model_type = trial.suggest_categorical('model_type', model_types)
                
                # Get model-specific parameters
                #model_params = get_search_space_for_model(trial, model_type)
                model_params = get_search_space_for_model(trial, model_type, input_dim)
                
                # Training parameters
                learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
                batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])
                weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)
                optimizer_type = trial.suggest_categorical('optimizer_type', ['Adam', 'AdamW', 'RMSprop'])
                scheduler_type = trial.suggest_categorical('scheduler_type', [None, 'ReduceLROnPlateau', 'StepLR', 'CosineAnnealingLR'])
                
                # Advanced training parameters
                gradient_clip = trial.suggest_categorical('gradient_clip', [None, 0.5, 1.0, 2.0])
                gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])
                
                # Scheduler parameters
                scheduler_params = {}
                if scheduler_type == 'ReduceLROnPlateau':
                    scheduler_params = {
                        'patience': trial.suggest_int('lr_patience', 3, 8),
                        'factor': trial.suggest_float('lr_factor', 0.3, 0.7),
                        'min_lr': trial.suggest_float('min_lr', 1e-8, 1e-5, log=True)
                    }
                elif scheduler_type == 'StepLR':
                    scheduler_params = {
                        'step_size': trial.suggest_int('step_size', 10, 30),
                        'gamma': trial.suggest_float('gamma', 0.1, 0.5)
                    }
                elif scheduler_type == 'CosineAnnealingLR':
                    scheduler_params = {
                        'T_max': trial_epochs,
                        'eta_min': trial.suggest_float('eta_min', 1e-8, 1e-5, log=True)
                    }
                
                # Build comprehensive configuration for train_model
                trial_config = {
                    # Model architecture configuration
                    #'model_architecture': {
                    'model': {
                        'model_type': model_params['model_type'],
                        'input_dim': input_dim,
                        'encoding_dim': model_params['encoding_dim'],
                        'hidden_dims': model_params['hidden_dims'],
                        'dropout_rates': model_params['dropout_rates'],
                        'activation': model_params['activation'],
                        'normalization': model_params['normalization'],
                        'skip_connection': model_params.get('skip_connection', True),
                        'residual_blocks': model_params.get('residual_blocks', True),
                        'use_attention': model_params.get('use_attention', True),
                        'legacy_mode': model_params.get('legacy_mode', False),
                        'num_models': model_params.get('num_models'),
                        'diversity_factor': model_params.get('diversity_factor'),
                        'weight_init': model_params.get('weight_init', 'xavier_uniform'),
                        'bias': model_params.get('bias', True),
                        'min_features': model_params.get('min_features', 5)
                    },
                    
                    # Training configuration
                    #'training_config': {
                    'training': {
                        'batch_size': batch_size,
                        'epochs': trial_epochs,
                        'learning_rate': learning_rate,
                        'patience': trial_patience,
                        'weight_decay': weight_decay,
                        'gradient_clip': gradient_clip,
                        'gradient_accumulation_steps': gradient_accumulation_steps,
                        'mixed_precision': model_params.get('mixed_precision', False),
                        'optimizer_type': optimizer_type,
                        'scheduler_type': scheduler_type,
                        'scheduler_params': scheduler_params,
                        'early_stopping': True,
                        'validation_split': 0.2
                    },
                    
                    # Data configuration
                    #'data_config': {
                    'data': {
                        'use_real_data': True,  # We're using provided data
                        'features': input_dim,
                        'data_preprocessing': True,
                        'normalization_method': 'standard'
                    },
                    
                    # System configuration
                    #'system_config': {
                    'system': {
                        'device': device,
                        'random_seed': random_seed,
                        'reproducible': True,
                        'model_dir': study_dir / f"trial_{trial.number}",
                        'log_dir': study_dir / f"trial_{trial.number}" / "logs"
                    },
                    
                    # Monitoring configuration (minimal for HPO)
                    #'monitoring_config': {
                    'monitoring': {
                        'verbose': False,
                        'debug_mode': False,
                        'tensorboard_logging': False,
                        'save_checkpoints': False,
                        'progress_bar': False,
                        'log_frequency': 999999
                    },
                    
                    # Export configuration (minimal for HPO)
                    #'export_config': {
                    'export': {
                        'export_onnx': False,
                        'save_model': False,
                        'save_metadata': False,
                        'save_training_history': False
                    },
                    
                    # Advanced training configuration
                    'advanced_training': {
                        'num_workers': num_workers,
                        'pin_memory': False,  # Disabled for HPO stability
                        'persistent_workers': False,
                        'memory_efficient': True
                    },
                    
                    # Error handling for HPO
                    'error_handling': {
                        'error_handling': 'continue',
                        'graceful_degradation': True,
                        'continue_on_error': True
                    }
                }
                
                # Perform cross-validation
                fold_scores = []
                
                for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
                    try:
                        # Prepare fold data
                        if X_val is None:
                            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
                        else:
                            X_fold_train, X_fold_val = X_train, X_val
                        
                        # Create fold data dictionary
                        fold_data = {
                            'X_train': X_fold_train,
                            'X_val': X_fold_val,
                            'X_test': X_fold_val,  # Use validation as test for HPO
                            'y_train': np.zeros(len(X_fold_train)),  # Dummy labels
                            'y_val': np.zeros(len(X_fold_val)),
                            'y_test': np.zeros(len(X_fold_val)),
                            'feature_names': feature_names
                        }
                        
                        # Use the comprehensive train_model function
                        result = train_model(
                            # Pass data directly via config
                            config={
                                **trial_config,
                                'hpo_data': fold_data,  # Special key for HPO data
                                'hpo_fold': fold_idx
                            }
                        )
                        
                        # Extract validation loss
                        if result and result.get('success', False):
                            final_metrics = result.get('final_metrics', {})
                            val_loss = final_metrics.get('best_validation_loss', float('inf'))
                            
                            # Additional metrics for analysis
                            test_loss = final_metrics.get('test_loss', float('inf'))
                            training_time = result.get('training_time_minutes', 0)
                            
                            # Use validation loss as primary metric
                            fold_scores.append(val_loss)
                            
                            # Store additional info in trial attributes
                            trial.set_user_attr(f'fold_{fold_idx}_val_loss', val_loss)
                            trial.set_user_attr(f'fold_{fold_idx}_test_loss', test_loss)
                            trial.set_user_attr(f'fold_{fold_idx}_training_time', training_time)
                            
                        else:
                            # Training failed
                            error_msg = result.get('error', 'Unknown error') if result else 'No result returned'
                            logger.warning(f"Trial {trial.number}, Fold {fold_idx} failed: {error_msg}")
                            fold_scores.append(float('inf'))
                        
                        # Report intermediate value for pruning
                        if fold_idx == 0:  # Report after first fold
                            trial.report(fold_scores[0], fold_idx)
                            if trial.should_prune():
                                raise optuna.TrialPruned()
                    
                    except optuna.TrialPruned:
                        raise
                    except Exception as e:
                        logger.warning(f"Trial {trial.number}, Fold {fold_idx} error: {str(e)}")
                        fold_scores.append(float('inf'))
                
                # Calculate final score
                if fold_scores:
                    mean_score = np.mean(fold_scores)
                    std_score = np.std(fold_scores)
                    
                    # Store trial results
                    trial.set_user_attr('mean_cv_score', mean_score)
                    trial.set_user_attr('std_cv_score', std_score)
                    trial.set_user_attr('individual_fold_scores', fold_scores)
                    trial.set_user_attr('model_config', model_params)
                    trial.set_user_attr('training_config', {
                        'learning_rate': learning_rate,
                        'batch_size': batch_size,
                        'weight_decay': weight_decay,
                        'optimizer_type': optimizer_type,
                        'scheduler_type': scheduler_type
                    })
                    trial.set_user_attr('complete_config', trial_config)
                    
                    return mean_score
                else:
                    return float('inf')
            
            except optuna.TrialPruned:
                raise
            except Exception as e:
                logger.error(f"Trial {trial.number} failed with error: {str(e)}")
                trial.set_user_attr('error', str(e))
                trial.set_user_attr('failed', True)
                return float('inf')
        
        # Set up callbacks
        callbacks = []
        
        # Progress callback
        def progress_callback(study, trial):
            if trial.state == optuna.trial.TrialState.COMPLETE:
                if verbose:
                    print(f"Trial {trial.number:3d} complete | Value: {trial.value:.5f} | Best: {study.best_value:.5f}")
                
                # Update results
                hpo_results['trials_completed'] = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])
                hpo_results['optimization_history'].append({
                    'trial': trial.number,
                    'value': trial.value,
                    'best_value': study.best_value,
                    'params': trial.params
                })
                
                if trial.value < hpo_results['best_value']:
                    hpo_results['best_value'] = trial.value
                    hpo_results['best_params'] = trial.params.copy()
                    if 'complete_config' in trial.user_attrs:
                        hpo_results['best_config'] = trial.user_attrs['complete_config'].copy()
            
            elif trial.state == optuna.trial.TrialState.PRUNED:
                if verbose:
                    print(f"Trial {trial.number:3d} pruned")
            elif trial.state == optuna.trial.TrialState.FAIL:
                if verbose:
                    print(f"Trial {trial.number:3d} failed")
                hpo_results['errors'].append(f"Trial {trial.number} failed")
        
        callbacks.append(progress_callback)
        
        # Early stopping callback
        def early_stopping_callback(study, trial):
            if len(study.trials) >= early_stopping_min_trials:
                recent_trials = study.trials[-early_stopping_patience:]
                recent_values = [t.value for t in recent_trials if t.state == optuna.trial.TrialState.COMPLETE]
                
                if len(recent_values) >= early_stopping_patience:
                    if min(recent_values) >= study.best_value:
                        if verbose:
                            print(f"Early stopping triggered after {len(study.trials)} trials")
                        study.stop()
        
        callbacks.append(early_stopping_callback)
        
        # Run optimization
        if verbose:
            print("Starting optimization...")
        
        study.optimize(
            objective,
            n_trials=n_trials,
            timeout=timeout_seconds if timeout_seconds > 0 else None,
            callbacks=callbacks,
            gc_after_trial=True,
            show_progress_bar=False  # We have custom progress
        )
        
        # Calculate total time
        total_time = time.time() - hpo_start_time
        
        # Finalize results
        hpo_results.update({
            'end_time': datetime.now().isoformat(),
            'total_time_seconds': total_time,
            'total_time_minutes': total_time / 60,
            'study': study,
            'n_trials_completed': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
            'n_trials_pruned': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),
            'n_trials_failed': len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])
        })
        
        if study.trials:
            best_trial = study.best_trial
            hpo_results.update({
                'best_trial_number': best_trial.number,
                'best_value': best_trial.value,
                'best_params': best_trial.params,
                'best_user_attrs': best_trial.user_attrs
            })
            
            if 'complete_config' in best_trial.user_attrs:
                hpo_results['best_config'] = best_trial.user_attrs['complete_config']
        
        # Display results
        if verbose:
            print("\n" + "="*80)
            print("HYPERPARAMETER OPTIMIZATION COMPLETED")
            print("="*80)
            print(f"Total time: {total_time/60:.1f} minutes")
            print(f"Trials completed: {hpo_results['n_trials_completed']}")
            print(f"Trials pruned: {hpo_results['n_trials_pruned']}")
            print(f"Trials failed: {hpo_results['n_trials_failed']}")
            
            if study.trials:
                print(f"\nBest trial (#{best_trial.number}):")
                print(f"  Objective value: {best_trial.value:.6f}")
                print(f"  Model type: {best_trial.params.get('model_type', 'Unknown')}")
                print(f"  Learning rate: {best_trial.params.get('learning_rate', 'Unknown')}")
                print(f"  Batch size: {best_trial.params.get('batch_size', 'Unknown')}")
                print(f"  Encoding dim: {best_trial.params.get('encoding_dim', 'Unknown')}")
                
                if 'mean_cv_score' in best_trial.user_attrs:
                    mean_cv = best_trial.user_attrs['mean_cv_score']
                    std_cv = best_trial.user_attrs.get('std_cv_score', 0)
                    print(f"  CV Score: {mean_cv:.6f} ± {std_cv:.6f}")
        
        # Save study and results
        if save_study:
            try:
                # Save study object
                study_path = study_dir / f"{study_name}_study.pkl"
                joblib.dump(study, study_path)
                
                # Save results
                results_path = study_dir / f"{study_name}_results.json"
                with open(results_path, 'w') as f:
                    # Prepare JSON-serializable results
                    json_results = hpo_results.copy()
                    json_results.pop('study', None)  # Remove non-serializable study object
                    json.dump(json_results, f, indent=2, default=str)
                
                hpo_results['study_path'] = str(study_path)
                hpo_results['results_path'] = str(results_path)
                
                if verbose:
                    print(f"\nStudy saved to: {study_path}")
                    print(f"Results saved to: {results_path}")
            
            except Exception as e:
                logger.warning(f"Failed to save study: {e}")
                hpo_results['warnings'].append(f"Failed to save study: {e}")
        
        # Generate plots
        if generate_plots and study.trials:
            try:
                plot_dir = study_dir / "plots"
                plot_dir.mkdir(exist_ok=True)
                
                # Optimization history
                fig = vis.plot_optimization_history(study)
                fig.write_html(plot_dir / "optimization_history.html")
                
                # Parameter importances
                if len(study.trials) > 10:
                    fig = vis.plot_param_importances(study)
                    fig.write_html(plot_dir / "param_importances.html")
                
                # Parallel coordinate plot
                if len(study.trials) > 5:
                    fig = vis.plot_parallel_coordinate(study)
                    fig.write_html(plot_dir / "parallel_coordinate.html")
                
                hpo_results['plots_dir'] = str(plot_dir)
                
                if verbose:
                    print(f"Plots saved to: {plot_dir}")
            
            except Exception as e:
                logger.warning(f"Failed to generate plots: {e}")
                hpo_results['warnings'].append(f"Failed to generate plots: {e}")
        
        # Summary statistics
        if study.trials:
            completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
            if completed_trials:
                values = [t.value for t in completed_trials]
                hpo_results['summary_statistics'] = {
                    'best_value': min(values),
                    'worst_value': max(values),
                    'mean_value': np.mean(values),
                    'median_value': np.median(values),
                    'std_value': np.std(values),
                    'improvement_ratio': (max(values) - min(values)) / max(values) if max(values) > 0 else 0
                }
        
        return hpo_results
    
    except Exception as e:
        error_msg = f"Hyperparameter optimization failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Create error results
        error_results = {
            'success': False,
            'error': error_msg,
            'error_type': type(e).__name__,
            'start_time': start_time.isoformat(),
            'end_time': datetime.now().isoformat(),
            'total_time_seconds': time.time() - hpo_start_time,
            'study_name': study_name,
            'configuration': hpo_config_section,
            'trials_completed': locals().get('hpo_results', {}).get('trials_completed', 0),
            'traceback': traceback.format_exc()
        }
        
        # Save error information
        if save_study:
            try:
                error_path = study_dir / f"{study_name}_error.json"
                with open(error_path, 'w') as f:
                    json.dump(error_results, f, indent=2, default=str)
                error_results['error_log_path'] = str(error_path)
            except Exception as save_error:
                logger.warning(f"Failed to save error log: {save_error}")
        
        return error_results
    
    finally:
        # Restore logging level
        # if verbose and 'original_level' in locals():
        #     try:
        #         logger.setLevel(original_level)
        #     except Exception:
        #         pass
        
        # Final cleanup
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        except Exception:
            pass

def setup_hyperparameter_optimization(
    # Core HPO Parameters
    n_trials: Optional[int] = None,
    timeout_seconds: Optional[float] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    
    # Data Parameters
    use_real_data: Optional[bool] = None,
    data_path: Optional[Union[str, Path]] = None,
    artifacts_path: Optional[Union[str, Path]] = None,
    normal_samples: Optional[int] = None,
    attack_samples: Optional[int] = None,
    features: Optional[int] = None,
    
    # Model Selection
    model_types: Optional[List[str]] = None,
    search_all_models: Optional[bool] = None,
    
    # Cross-Validation
    cv_folds: Optional[int] = None,
    cv_shuffle: Optional[bool] = None,
    cv_random_state: Optional[int] = None,
    
    # Search Space Configuration
    search_space: Optional[Dict[str, Any]] = None,
    parameter_ranges: Optional[Dict[str, Any]] = None,
    fixed_params: Optional[Dict[str, Any]] = None,
    
    # Trial Configuration
    trial_epochs: Optional[int] = None,
    trial_patience: Optional[int] = None,
    trial_batch_size: Optional[int] = None,
    
    # System Parameters
    device: Optional[str] = None,
    random_seed: Optional[int] = None,
    num_workers: Optional[int] = None,
    
    # Storage and Output
    storage_url: Optional[str] = None,
    load_if_exists: Optional[bool] = None,
    save_study: Optional[bool] = None,
    study_dir: Optional[Union[str, Path]] = None,
    generate_plots: Optional[bool] = None,
    
    # Early Stopping
    early_stopping_patience: Optional[int] = None,
    early_stopping_min_trials: Optional[int] = None,
    
    # Monitoring
    verbose: Optional[bool] = None,
    show_progress: Optional[bool] = None,
    log_level: Optional[str] = None,
    
    # Advanced Parameters
    parallel_jobs: Optional[int] = None,
    memory_limit: Optional[str] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    hpo_config: Optional[Dict[str, Any]] = None,
    args: Optional[argparse.Namespace] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """
    Set up hyperparameter optimization that integrates seamlessly with the current train_model() implementation.
    
    This function creates a comprehensive HPO configuration that leverages the train_model() function's
    extensive parameter system and configuration management.
    
    Returns:
        Dictionary containing the configured study, optimization functions, and metadata.
    """
    
    # Start timing
    start_time = datetime.now()
    setup_start_time = time.time()
    
    # Initialize configuration
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply HPO-specific configuration
    if hpo_config:
        config.setdefault('hyperparameter_optimization', {}).update(hpo_config)
    
    # Handle legacy args parameter
    if args is not None:
        # Extract parameters from args object
        legacy_params = {}
        for attr_name in dir(args):
            if not attr_name.startswith('_'):
                value = getattr(args, attr_name)
                if value is not None:
                    legacy_params[attr_name] = value
        
        # Map args parameters to function parameters
        legacy_mapping = {
            'hpo_trials': 'n_trials',
            'hpo_timeout': 'timeout_seconds',
            'hpo_study_name': 'study_name',
            'hpo_sampler': 'sampler_type',
            'hpo_pruner': 'pruner_type',
            'hpo_cv_folds': 'cv_folds',
            'hpo_trial_epochs': 'trial_epochs',
            'hpo_generate_plots': 'generate_plots',
            'hpo_save_study': 'save_study'
        }
        
        for old_key, new_key in legacy_mapping.items():
            if old_key in legacy_params:
                kwargs.setdefault(new_key, legacy_params[old_key])
        
        # Apply other relevant args
        kwargs.update(legacy_params)
    
    # Apply all parameters to configuration
    final_config = config.copy()
    
    # Apply individual parameters
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'hpo_config', 'kwargs', 'start_time', 'setup_start_time',
        'datetime', 'traceback', 'time', 'gc', 'warnings', 'Path', 'args'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Set up HPO configuration with intelligent defaults
    hpo_section = final_config.setdefault('hyperparameter_optimization', {})
    
    # Core HPO parameters
    n_trials = hpo_section.setdefault('n_trials', cleaned_params.get('n_trials', 100))
    timeout_seconds = hpo_section.setdefault('timeout_seconds', cleaned_params.get('timeout_seconds', 0))
    study_name = hpo_section.setdefault('study_name', cleaned_params.get('study_name', f"autoencoder_hpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"))
    direction = hpo_section.setdefault('direction', cleaned_params.get('direction', 'minimize'))
    sampler_type = hpo_section.setdefault('sampler_type', cleaned_params.get('sampler_type', 'TPE'))
    pruner_type = hpo_section.setdefault('pruner_type', cleaned_params.get('pruner_type', 'MedianPruner'))
    
    # Data parameters
    use_real_data = hpo_section.setdefault('use_real_data', cleaned_params.get('use_real_data', False))
    data_path = hpo_section.setdefault('data_path', cleaned_params.get('data_path', None))
    artifacts_path = hpo_section.setdefault('artifacts_path', cleaned_params.get('artifacts_path', None))
    normal_samples = hpo_section.setdefault('normal_samples', cleaned_params.get('normal_samples', 10000))
    attack_samples = hpo_section.setdefault('attack_samples', cleaned_params.get('attack_samples', 2000))
    features = hpo_section.setdefault('features', cleaned_params.get('features', 78))
    
    # Model selection
    model_types = hpo_section.setdefault('model_types', cleaned_params.get('model_types', ['SimpleAutoencoder', 'EnhancedAutoencoder']))
    search_all_models = hpo_section.setdefault('search_all_models', cleaned_params.get('search_all_models', False))
    
    if search_all_models:
        model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
    
    # Cross-validation parameters
    cv_folds = hpo_section.setdefault('cv_folds', cleaned_params.get('cv_folds', 3))
    cv_shuffle = hpo_section.setdefault('cv_shuffle', cleaned_params.get('cv_shuffle', True))
    cv_random_state = hpo_section.setdefault('cv_random_state', cleaned_params.get('cv_random_state', 42))
    
    # Trial configuration
    trial_epochs = hpo_section.setdefault('trial_epochs', cleaned_params.get('trial_epochs', 20))
    trial_patience = hpo_section.setdefault('trial_patience', cleaned_params.get('trial_patience', 5))
    trial_batch_size = hpo_section.setdefault('trial_batch_size', cleaned_params.get('trial_batch_size', 64))
    
    # System parameters
    device = hpo_section.setdefault('device', cleaned_params.get('device', 'auto'))
    random_seed = hpo_section.setdefault('random_seed', cleaned_params.get('random_seed', 42))
    num_workers = hpo_section.setdefault('num_workers', cleaned_params.get('num_workers', 0))
    
    # Storage and output
    storage_url = hpo_section.setdefault('storage_url', cleaned_params.get('storage_url', None))
    load_if_exists = hpo_section.setdefault('load_if_exists', cleaned_params.get('load_if_exists', False))
    save_study = hpo_section.setdefault('save_study', cleaned_params.get('save_study', True))
    study_dir = hpo_section.setdefault('study_dir', cleaned_params.get('study_dir', DEFAULT_MODEL_DIR / "hpo_studies"))
    generate_plots = hpo_section.setdefault('generate_plots', cleaned_params.get('generate_plots', True))
    
    # Early stopping
    early_stopping_patience = hpo_section.setdefault('early_stopping_patience', cleaned_params.get('early_stopping_patience', 10))
    early_stopping_min_trials = hpo_section.setdefault('early_stopping_min_trials', cleaned_params.get('early_stopping_min_trials', 20))
    
    # Monitoring
    verbose = hpo_section.setdefault('verbose', cleaned_params.get('verbose', True))
    show_progress = hpo_section.setdefault('show_progress', cleaned_params.get('show_progress', True))
    log_level = hpo_section.setdefault('log_level', cleaned_params.get('log_level', 'INFO'))
    
    # Advanced parameters
    parallel_jobs = hpo_section.setdefault('parallel_jobs', cleaned_params.get('parallel_jobs', 1))
    memory_limit = hpo_section.setdefault('memory_limit', cleaned_params.get('memory_limit', None))
    
    # Set up logging
    # if verbose:
    #     original_level = logger.level
    #     logger.setLevel(getattr(logging, log_level))
    
    try:
        # Display setup header
        if verbose:
            print("\n" + "="*80)
            print("HYPERPARAMETER OPTIMIZATION SETUP")
            print("="*80)
            print(f"Study Name: {study_name}")
            print(f"Trials: {n_trials}")
            print(f"Timeout: {timeout_seconds}s" if timeout_seconds > 0 else "Timeout: None")
            print(f"Model Types: {', '.join(model_types)}")
            print(f"CV Folds: {cv_folds}")
            print(f"Sampler: {sampler_type}")
            print(f"Pruner: {pruner_type}")
            print(f"Device: {device}")
            print("-"*80)
        
        # Create study directory
        study_dir = Path(study_dir)
        study_dir.mkdir(parents=True, exist_ok=True)
        
        # Set up Optuna sampler
        sampler_config = {
            'seed': random_seed,
            'n_startup_trials': min(10, n_trials // 10)
        }

        # Fix sampler configuration parsing
        sampler_raw = hpo_section.get('sampler_type', 'TPE')
        if isinstance(sampler_raw, dict):
            # Extract type from dictionary configuration
            sampler_type = sampler_raw.get('type', 'TPE')
            # Merge additional sampler configuration if present
            additional_config = {k: v for k, v in sampler_raw.items() if k != 'type'}
            sampler_config.update(additional_config)
        else:
            sampler_type = sampler_raw

        # Map common sampler variants to standard names
        sampler_mapping = {
            'Random': 'TPE',  # Use the base name for the if-elif chain
            'TPE': 'TPE',
            'CmaEs': 'CmaEs'
        }
        sampler_type = sampler_mapping.get(sampler_type, 'TPE')

        if sampler_type == 'TPE':
            sampler = TPESampler(
                seed=sampler_config['seed'],
                consider_prior=sampler_config.get('consider_prior', True),
                consider_magic_clip=sampler_config.get('consider_magic_clip', True),
                consider_endpoints=sampler_config.get('consider_endpoints', False),
                n_startup_trials=sampler_config['n_startup_trials'],
                multivariate=sampler_config.get('multivariate', True),
                warn_independent_sampling=False
            )
        elif sampler_type == 'Random':
            sampler = RandomSampler(seed=sampler_config['seed'])
        elif sampler_type == 'CmaEs':
            sampler = CmaEsSampler(seed=sampler_config['seed'])
        else:
            logger.warning(f"Unknown sampler type '{sampler_type}', using TPE")
            sampler = TPESampler(seed=sampler_config['seed'])

        # Set up Optuna pruner with proper configuration parsing
        pruner_config = {
            'n_startup_trials': 5,
            'n_warmup_steps': 5
        }

        # Fix pruner configuration parsing
        pruner_raw = hpo_section.get('pruner_type', 'MedianPruner')
        if isinstance(pruner_raw, dict):
            # Extract type from dictionary configuration
            pruner_type = pruner_raw.get('type', 'MedianPruner')
            # Merge additional pruner configuration if present
            additional_config = {k: v for k, v in pruner_raw.items() if k != 'type'}
            pruner_config.update(additional_config)
        else:
            pruner_type = pruner_raw

        # Map common pruner variants to standard names
        pruner_mapping = {
            'Nop': 'NopPruner',
            'Median': 'MedianPruner', 
            'Hyperband': 'HyperbandPruner'
        }
        pruner_type = pruner_mapping.get(pruner_type, 'MedianPruner')

        if pruner_type == 'MedianPruner':
            pruner = MedianPruner(
                n_startup_trials=pruner_config['n_startup_trials'],
                n_warmup_steps=pruner_config['n_warmup_steps'],
                interval_steps=pruner_config.get('interval_steps', 1)
            )
        elif pruner_type == 'HyperbandPruner':
            pruner = HyperbandPruner(
                min_resource=pruner_config.get('min_resource', 1),
                max_resource=pruner_config.get('max_resource', trial_epochs),
                reduction_factor=pruner_config.get('reduction_factor', 3)
            )
        elif pruner_type == 'NopPruner' or pruner_type == 'None':
            pruner = NopPruner()
        else:
            logger.warning(f"Unknown pruner type '{pruner_type}', using MedianPruner")
            pruner = MedianPruner()
        
        # Set up storage if specified
        storage = None
        if storage_url:
            try:
                storage = optuna.storages.RDBStorage(
                    url=storage_url,
                    heartbeat_interval=60,
                    grace_period=120
                )
                if verbose:
                    print(f"Using persistent storage: {storage_url}")
            except Exception as e:
                logger.warning(f"Failed to setup storage: {e}")
        
        # Create study
        study = optuna.create_study(
            direction=direction,
            sampler=sampler,
            pruner=pruner,
            study_name=study_name,
            storage=storage,
            load_if_exists=load_if_exists
        )
        
        # Define search space that integrates with train_model parameters
        def create_search_space(trial):
            """Create search space that maps directly to train_model parameters"""
            
            # Model type selection
            model_type = trial.suggest_categorical('model_type', model_types)
            input_dim = features
            
            # Base model parameters that apply to all model types
            learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
            batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])
            weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)
            optimizer_type = trial.suggest_categorical('optimizer_type', ['Adam', 'AdamW', 'RMSprop'])
            
            # Scheduler configuration
            scheduler_type = trial.suggest_categorical('scheduler_type', [None, 'ReduceLROnPlateau', 'StepLR', 'CosineAnnealingLR'])
            scheduler_params = {}
            
            if scheduler_type == 'ReduceLROnPlateau':
                scheduler_params = {
                    'patience': trial.suggest_int('lr_patience', 3, 8),
                    'factor': trial.suggest_float('lr_factor', 0.3, 0.7),
                    'min_lr': trial.suggest_float('min_lr', 1e-8, 1e-5, log=True)
                }
            elif scheduler_type == 'StepLR':
                scheduler_params = {
                    'step_size': trial.suggest_int('step_size', 10, 30),
                    'gamma': trial.suggest_float('gamma', 0.1, 0.5)
                }
            elif scheduler_type == 'CosineAnnealingLR':
                scheduler_params = {
                    'T_max': trial_epochs,
                    'eta_min': trial.suggest_float('eta_min', 1e-8, 1e-5, log=True)
                }
            
            # Advanced training parameters
            gradient_clip = trial.suggest_categorical('gradient_clip', [None, 0.5, 1.0, 2.0])
            gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])
            mixed_precision = trial.suggest_categorical('mixed_precision', [False, True])
            
            # Model-specific parameters
            if model_type == 'SimpleAutoencoder':
                #encoding_dim = trial.suggest_int('encoding_dim', 8, min(64, features // 2), step=8)
                min_encoding_dim = min(8, max(4, input_dim // 4))  # Adjust min based on input_dim
                max_encoding_dim = min(64, input_dim // 2)
                if max_encoding_dim < min_encoding_dim:
                    max_encoding_dim = min_encoding_dim
                encoding_dim = trial.suggest_int('encoding_dim', min_encoding_dim, max_encoding_dim)
                
                # Architecture choices
                arch_choice = trial.suggest_categorical('hidden_arch', [
                    'single_small', 'single_medium', 'single_large', 
                    'double_small', 'double_medium', 'double_large'
                ])
                
                if arch_choice == 'single_small':
                    hidden_dims = [max(16, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif arch_choice == 'single_medium':
                    hidden_dims = [max(32, encoding_dim * 4)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif arch_choice == 'single_large':
                    hidden_dims = [max(64, encoding_dim * 8)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.4)]
                elif arch_choice == 'double_small':
                    hidden_dims = [max(32, encoding_dim * 4), max(16, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.1, 0.4),
                        trial.suggest_float('dropout_1', 0.05, 0.3)
                    ]
                elif arch_choice == 'double_medium':
                    hidden_dims = [max(64, encoding_dim * 8), max(32, encoding_dim * 4)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.15, 0.4),
                        trial.suggest_float('dropout_1', 0.1, 0.3)
                    ]
                else:  # double_large
                    hidden_dims = [max(128, encoding_dim * 16), max(64, encoding_dim * 8)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.2, 0.45),
                        trial.suggest_float('dropout_1', 0.15, 0.35)
                    ]
                
                model_params = {
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'activation': trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'gelu', 'tanh']),
                    'normalization': trial.suggest_categorical('normalization', [None, 'batch']),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'residual_blocks': False,
                    'use_attention': False,
                    'legacy_mode': False
                }
            
            elif model_type == 'EnhancedAutoencoder':
                encoding_dim = trial.suggest_int('encoding_dim', 16, min(64, features // 2), step=8)
                
                # Architecture choices
                arch_choice = trial.suggest_categorical('enhanced_arch', [
                    'compact', 'balanced', 'deep', 'wide', 'ultra_deep'
                ])
                
                if arch_choice == 'compact':
                    hidden_dims = [max(32, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.1, 0.3)]
                elif arch_choice == 'balanced':
                    hidden_dims = [max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.15, 0.35),
                        trial.suggest_float('dropout_1', 0.1, 0.25)
                    ]
                elif arch_choice == 'deep':
                    hidden_dims = [max(128, encoding_dim * 8), max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.2, 0.4),
                        trial.suggest_float('dropout_1', 0.15, 0.35),
                        trial.suggest_float('dropout_2', 0.1, 0.3)
                    ]
                elif arch_choice == 'wide':
                    hidden_dims = [max(256, encoding_dim * 16), max(128, encoding_dim * 8)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.25, 0.45),
                        trial.suggest_float('dropout_1', 0.2, 0.4)
                    ]
                else:  # ultra_deep
                    hidden_dims = [
                        max(256, encoding_dim * 16), 
                        max(128, encoding_dim * 8), 
                        max(64, encoding_dim * 4), 
                        max(32, encoding_dim * 2)
                    ]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.25, 0.45),
                        trial.suggest_float('dropout_1', 0.2, 0.4),
                        trial.suggest_float('dropout_2', 0.15, 0.35),
                        trial.suggest_float('dropout_3', 0.1, 0.3)
                    ]
                
                model_params = {
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'activation': trial.suggest_categorical('activation', ['leaky_relu', 'gelu', 'relu', 'swish']),
                    'normalization': trial.suggest_categorical('normalization', ['batch', 'layer', 'instance']),
                    'use_attention': trial.suggest_categorical('use_attention', [False, True]),
                    'residual_blocks': trial.suggest_categorical('residual_blocks', [False, True]),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'legacy_mode': trial.suggest_categorical('legacy_mode', [False, True])
                }
            
            elif model_type == 'AutoencoderEnsemble':
                encoding_dim = trial.suggest_int('encoding_dim', 16, min(48, features // 3), step=8)
                num_models = trial.suggest_int('num_models', 3, 7, step=2)
                diversity_factor = trial.suggest_float('diversity_factor', 0.1, 0.5)
                
                # Ensemble architecture
                ensemble_arch = trial.suggest_categorical('ensemble_arch', ['small', 'medium', 'large'])
                
                if ensemble_arch == 'small':
                    hidden_dims = [max(32, encoding_dim * 2)]
                    dropout_rates = [trial.suggest_float('dropout_0', 0.15, 0.35)]
                elif ensemble_arch == 'medium':
                    hidden_dims = [max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.2, 0.4),
                        trial.suggest_float('dropout_1', 0.15, 0.35)
                    ]
                else:  # large
                    hidden_dims = [max(128, encoding_dim * 8), max(64, encoding_dim * 4), max(32, encoding_dim * 2)]
                    dropout_rates = [
                        trial.suggest_float('dropout_0', 0.25, 0.45),
                        trial.suggest_float('dropout_1', 0.2, 0.4),
                        trial.suggest_float('dropout_2', 0.15, 0.35)
                    ]
                
                model_params = {
                    'encoding_dim': encoding_dim,
                    'hidden_dims': hidden_dims,
                    'dropout_rates': dropout_rates,
                    'num_models': num_models,
                    'diversity_factor': diversity_factor,
                    'activation': trial.suggest_categorical('activation', ['leaky_relu', 'gelu', 'relu']),
                    'normalization': trial.suggest_categorical('normalization', ['batch', 'layer']),
                    'use_attention': trial.suggest_categorical('use_attention', [False, True]),
                    'residual_blocks': trial.suggest_categorical('residual_blocks', [False, True]),
                    'skip_connection': trial.suggest_categorical('skip_connection', [False, True]),
                    'min_features': trial.suggest_int('min_features', 3, 10)
                }
            
            # Security parameters
            percentile = trial.suggest_float('percentile', 90.0, 99.0)
            threshold_method = trial.suggest_categorical('threshold_method', ['percentile', 'adaptive'])
            
            # Return parameters in the format expected by train_model
            return {
                # Model architecture parameters
                'model_type': model_type,
                'input_dim': features,
                'encoding_dim': model_params['encoding_dim'],
                'hidden_dims': model_params['hidden_dims'],
                'dropout_rates': model_params['dropout_rates'],
                'activation': model_params['activation'],
                'normalization': model_params['normalization'],
                'skip_connection': model_params.get('skip_connection', True),
                'residual_blocks': model_params.get('residual_blocks', True),
                'use_attention': model_params.get('use_attention', True),
                'legacy_mode': model_params.get('legacy_mode', False),
                'num_models': model_params.get('num_models'),
                'diversity_factor': model_params.get('diversity_factor'),
                
                # Training configuration parameters
                'batch_size': batch_size,
                'epochs': trial_epochs,
                'learning_rate': learning_rate,
                'patience': trial_patience,
                'weight_decay': weight_decay,
                'gradient_clip': gradient_clip,
                'gradient_accumulation_steps': gradient_accumulation_steps,
                'mixed_precision': mixed_precision,
                'optimizer_type': optimizer_type,
                'scheduler_type': scheduler_type,
                'scheduler_params': scheduler_params,
                'early_stopping': True,
                'validation_split': 0.2,
                
                # Data configuration parameters
                'normal_samples': normal_samples,
                'attack_samples': attack_samples,
                'features': features,
                'use_real_data': use_real_data,
                'data_path': data_path,
                'artifacts_path': artifacts_path,
                'data_preprocessing': True,
                
                # Security configuration parameters
                'percentile': percentile,
                'threshold_method': threshold_method,
                'enable_security_metrics': True,
                
                # System configuration parameters
                'device': device,
                'random_seed': random_seed,
                'reproducible': True,
                
                # Monitoring configuration (minimal for HPO)
                'verbose': False,
                'debug_mode': False,
                'tensorboard_logging': False,
                'save_checkpoints': False,
                'progress_bar': False,
                
                # Export configuration (minimal for HPO)
                'export_onnx': False,
                'save_model': False,
                'save_metadata': False,
                'save_training_history': False,
                
                # Advanced training parameters
                'num_workers': num_workers,
                'pin_memory': False,  # Disabled for HPO stability
                'persistent_workers': False,
                'memory_efficient': True,
                
                # Error handling for HPO
                'error_handling': 'continue',
                'graceful_degradation': True,
                'continue_on_error': True
            }
        
        # Define objective function that integrates with train_model
        def objective(trial):
            """Objective function that uses train_model for evaluation"""
            try:
                # Get trial parameters
                trial_params = create_search_space(trial)
                
                use_real_data_local = use_real_data
                input_dim = features
                
                # Set up cross-validation
                if use_real_data_local and data_path:
                    # Load real data once for all folds
                    try:
                        data = load_and_validate_data(
                            data_path=data_path,
                            artifacts_path=artifacts_path,
                            config=final_config
                        )
                        X_data = data['X_train']
                        features_actual = len(data['feature_names'])
                        input_dim = features_actual
                        
                        # Update input dimension if different
                        if features_actual != features:
                            trial_params['input_dim'] = features_actual
                            trial_params['features'] = features_actual
                    except Exception as e:
                        logger.warning(f"Failed to load real data: {e}, using synthetic data")
                        use_real_data_local = False
                        trial_params['use_real_data'] = False
                
                if not use_real_data_local:
                    # Generate synthetic data for cross-validation
                    # For autoencoders, we only need normal data, but generate_synthetic_data
                    # requires both normal and attack samples to be positive
                    data = generate_synthetic_data(
                        normal_samples=normal_samples,
                        attack_samples=attack_samples,  # FIXED: Use original attack_samples value
                        features=features,
                        validation_split=0.1,  # Use small split for data generation
                        random_state=cv_random_state,
                        config=final_config
                    )
                    # For CV, we only need the normal training data
                    X_train_orig = data['X_train']
                    X_val_orig = data.get('X_val', np.array([]))
                    
                    # Combine train and validation data, but only keep normal samples
                    if len(X_val_orig) > 0:
                        X_combined = np.vstack([X_train_orig, X_val_orig])
                    else:
                        X_combined = X_train_orig
                    
                    # For autoencoders, we only use normal data (label 0)
                    y_combined = data.get('y_train', np.zeros(len(X_train_orig)))
                    if len(X_val_orig) > 0:
                        y_val = data.get('y_val', np.zeros(len(X_val_orig)))
                        y_combined = np.concatenate([y_combined, y_val])
                    
                    # Filter to only normal samples (label 0) for autoencoder training
                    normal_mask = y_combined == 0
                    X_data = X_combined[normal_mask]
                    
                    # Ensure we have enough data
                    if len(X_data) < cv_folds * 50:  # Increased minimum requirement
                        logger.warning(f"Not enough normal samples ({len(X_data)}) for CV, generating more")
                        # Generate more data with proper parameters
                        data = generate_synthetic_data(
                            normal_samples=max(normal_samples, cv_folds * 100),  # Generate more samples
                            attack_samples=attack_samples,  # FIXED: Keep original attack_samples value
                            features=features,
                            validation_split=0.1,
                            random_state=cv_random_state,
                            config=final_config
                        )
                        X_train_orig = data['X_train']
                        X_val_orig = data.get('X_val', np.array([]))
                        y_train_orig = data.get('y_train', np.zeros(len(X_train_orig)))
                        y_val_orig = data.get('y_val', np.zeros(len(X_val_orig))) if len(X_val_orig) > 0 else np.array([])
                        
                        # Combine and filter normal samples
                        if len(X_val_orig) > 0:
                            X_combined = np.vstack([X_train_orig, X_val_orig])
                            y_combined = np.concatenate([y_train_orig, y_val_orig])
                        else:
                            X_combined = X_train_orig
                            y_combined = y_train_orig
                        
                        normal_mask = y_combined == 0
                        X_data = X_combined[normal_mask]
                
                # Perform cross-validation
                kf = KFold(n_splits=cv_folds, shuffle=cv_shuffle, random_state=cv_random_state)
                fold_scores = []
                
                for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_data)):
                    try:
                        # Create fold-specific data
                        X_fold_train = X_data[train_idx]
                        X_fold_val = X_data[val_idx]
                        
                        # Ensure minimum fold size
                        if len(X_fold_train) < 10 or len(X_fold_val) < 5:
                            logger.warning(f"Fold {fold_idx} too small: train={len(X_fold_train)}, val={len(X_fold_val)}")
                            fold_scores.append(float('inf'))
                            continue
                        
                        # Create temporary directory for this fold
                        fold_dir = study_dir / f"trial_{trial.number}" / f"fold_{fold_idx}"
                        fold_dir.mkdir(parents=True, exist_ok=True)
                        
                        # Update trial parameters for this fold
                        fold_params = trial_params.copy()
                        fold_params.update({
                            'model_dir': fold_dir,
                            'log_dir': fold_dir / "logs",
                            'tensorboard_dir': fold_dir / "tensorboard",
                            'tb_dir': fold_dir / "tensorboard",
                            
                            # Override data parameters - use actual fold sizes
                            'normal_samples': len(X_fold_train),
                            'attack_samples': 0,  # No attack samples for autoencoder
                            'use_real_data': False,  # We're providing the data directly
                            
                            # Use minimal settings for speed
                            'save_checkpoints': False,
                            'checkpoint_frequency': 999999,
                            'log_frequency': 999999,
                            'metrics_frequency': 999999
                        })
                        
                        # Create fold data in the format expected by train_model
                        fold_data = {
                            'X_train': X_fold_train,
                            'X_val': X_fold_val,
                            'X_test': X_fold_val,  # Use validation as test for HPO
                            'y_train': np.zeros(len(X_fold_train)),  # All normal samples
                            'y_val': np.zeros(len(X_fold_val)),      # All normal samples  
                            'y_test': np.zeros(len(X_fold_val)),     # All normal samples
                            'feature_names': data.get('feature_names', [f'feature_{i}' for i in range(X_fold_train.shape[1])])
                        }
                        
                        # Create comprehensive config for train_model
                        fold_config = {
                            # Model architecture configuration
                            #'model_architecture': {
                            'model': {
                                'model_type': fold_params['model_type'],
                                'input_dim': fold_params['input_dim'],
                                'encoding_dim': fold_params['encoding_dim'],
                                'hidden_dims': fold_params['hidden_dims'],
                                'dropout_rates': fold_params['dropout_rates'],
                                'activation': fold_params['activation'],
                                'normalization': fold_params['normalization'],
                                'skip_connection': fold_params.get('skip_connection', True),
                                'residual_blocks': fold_params.get('residual_blocks', True),
                                'use_attention': fold_params.get('use_attention', True),
                                'legacy_mode': fold_params.get('legacy_mode', False),
                                'num_models': fold_params.get('num_models'),
                                'diversity_factor': fold_params.get('diversity_factor'),
                                'weight_init': 'xavier_uniform',
                                'bias': True,
                                'min_features': fold_params.get('min_features', 5)
                            },
                            
                            # Training configuration
                            #'training_config': {
                            'training': {
                                'batch_size': fold_params['batch_size'],
                                'epochs': fold_params['epochs'],
                                'learning_rate': fold_params['learning_rate'],
                                'patience': fold_params['patience'],
                                'weight_decay': fold_params['weight_decay'],
                                'gradient_clip': fold_params['gradient_clip'],
                                'gradient_accumulation_steps': fold_params['gradient_accumulation_steps'],
                                'mixed_precision': fold_params['mixed_precision'],
                                'optimizer_type': fold_params['optimizer_type'],
                                'scheduler_type': fold_params['scheduler_type'],
                                'scheduler_params': fold_params['scheduler_params'],
                                'early_stopping': True,
                                'validation_split': 0.0  # We provide validation data directly
                            },
                            
                            # Data configuration
                            #'data_config': {
                            'data': {
                                'use_real_data': False,
                                'features': fold_params['features'],
                                'normal_samples': len(X_fold_train),
                                'attack_samples': attack_samples,
                                'data_preprocessing': True,
                                'normalization_method': 'standard'
                            },
                            
                            # System configuration
                            #'system_config': {
                            'system': {
                                'device': fold_params['device'],
                                'random_seed': fold_params['random_seed'],
                                'reproducible': True,
                                'model_dir': fold_dir,
                                'log_dir': fold_dir / "logs"
                            },
                            
                            # Monitoring configuration (minimal for HPO)
                            #'monitoring_config': {
                            'monitoring': {
                                'verbose': False,
                                'debug_mode': False,
                                'tensorboard_logging': False,
                                'save_checkpoints': False,
                                'progress_bar': False,
                                'log_frequency': 999999
                            },
                            
                            # Export configuration (minimal for HPO)
                            #'export_config': {
                            'export': {
                                'export_onnx': False,
                                'save_model': False,
                                'save_metadata': False,
                                'save_training_history': False
                            },
                            
                            # Advanced training configuration
                            'advanced_training': {
                                'num_workers': fold_params['num_workers'],
                                'pin_memory': False,  # Disabled for HPO stability
                                'persistent_workers': False,
                                'memory_efficient': True
                            },
                            
                            # Error handling for HPO
                            'error_handling': {
                                'error_handling': 'continue',
                                'graceful_degradation': True,
                                'continue_on_error': True
                            },
                            
                            # HPO-specific data - provide data directly
                            'hpo_fold_data': fold_data,
                            'hpo_trial_number': trial.number,
                            'hpo_fold_number': fold_idx,
                            'hpo_direct_data': True  # Flag to indicate direct data provision
                        }
                        
                        # Train model using the comprehensive train_model function
                        # Pass fold_data directly to avoid data generation issues
                        result = train_model(
                            config=fold_config,
                            X_train=X_fold_train,
                            X_val=X_fold_val,
                            X_test=X_fold_val,
                            y_train=np.zeros(len(X_fold_train)),
                            y_val=np.zeros(len(X_fold_val)),
                            y_test=np.zeros(len(X_fold_val)),
                            **{k: v for k, v in fold_params.items() 
                               if k not in ['model_dir', 'log_dir', 'tensorboard_dir', 'tb_dir', 'normal_samples', 'attack_samples', 'use_real_data']}
                        )
                        
                        # Extract validation loss
                        if result and result.get('success', False):
                            final_metrics = result.get('final_metrics', {})
                            val_loss = final_metrics.get('best_validation_loss', float('inf'))
                            
                            # Fallback to other loss metrics if validation loss not available
                            if val_loss == float('inf') or val_loss is None:
                                val_loss = final_metrics.get('validation_loss', float('inf'))
                            if val_loss == float('inf') or val_loss is None:
                                val_loss = final_metrics.get('final_validation_loss', float('inf'))
                            if val_loss == float('inf') or val_loss is None:
                                val_loss = final_metrics.get('test_loss', float('inf'))
                            
                            # Store additional metrics
                            test_loss = final_metrics.get('test_loss', float('inf'))
                            training_time = result.get('training_time_minutes', 0)
                            
                            fold_scores.append(val_loss)
                            
                            # Store fold-specific attributes
                            trial.set_user_attr(f'fold_{fold_idx}_val_loss', val_loss)
                            trial.set_user_attr(f'fold_{fold_idx}_test_loss', test_loss)
                            trial.set_user_attr(f'fold_{fold_idx}_training_time', training_time)
                            
                        else:
                            # Training failed
                            error_msg = result.get('error', 'Unknown error') if result else 'No result returned'
                            logger.warning(f"Trial {trial.number}, Fold {fold_idx} failed: {error_msg}")
                            fold_scores.append(float('inf'))
                        
                        # Report intermediate value for pruning (after first fold)
                        if fold_idx == 0 and fold_scores[-1] != float('inf'):
                            trial.report(fold_scores[0], fold_idx)
                            if trial.should_prune():
                                raise optuna.TrialPruned()
                        
                        # Cleanup fold directory
                        try:
                            shutil.rmtree(fold_dir, ignore_errors=True)
                        except Exception:
                            pass
                    
                    except optuna.TrialPruned:
                        raise
                    except Exception as e:
                        logger.warning(f"Trial {trial.number}, Fold {fold_idx} error: {str(e)}")
                        fold_scores.append(float('inf'))
                
                # Calculate final score
                if fold_scores and any(score != float('inf') for score in fold_scores):
                    # Filter out infinite scores for statistics
                    valid_scores = [score for score in fold_scores if score != float('inf')]
                    if valid_scores:
                        mean_score = np.mean(valid_scores)
                        std_score = np.std(valid_scores)
                    else:
                        mean_score = float('inf')
                        std_score = 0.0
                else:
                    mean_score = float('inf')
                    std_score = 0.0
                
                # Store trial results
                trial.set_user_attr('mean_cv_score', mean_score)
                trial.set_user_attr('std_cv_score', std_score)
                trial.set_user_attr('individual_fold_scores', fold_scores)
                trial.set_user_attr('valid_folds', len([s for s in fold_scores if s != float('inf')]))
                trial.set_user_attr('trial_parameters', trial_params)
                trial.set_user_attr('complete_config', fold_config)
                
                return mean_score
            
            except optuna.TrialPruned:
                raise
            except Exception as e:
                logger.error(f"Trial {trial.number} failed with error: {str(e)}")
                trial.set_user_attr('error', str(e))
                trial.set_user_attr('failed', True)
                return float('inf')
        
        # Set up callbacks for monitoring
        callbacks = []
        
        # Progress callback
        def progress_callback(study, trial):
            if trial.state == optuna.trial.TrialState.COMPLETE:
                if verbose and show_progress:
                    print(f"Trial {trial.number:3d} complete | Value: {trial.value:.5f} | Best: {study.best_value:.5f}")
            elif trial.state == optuna.trial.TrialState.PRUNED:
                if verbose and show_progress:
                    print(f"Trial {trial.number:3d} pruned")
            elif trial.state == optuna.trial.TrialState.FAIL:
                if verbose:
                    print(f"Trial {trial.number:3d} failed")
        
        callbacks.append(progress_callback)
        
        # Early stopping callback
        def early_stopping_callback(study, trial):
            if len(study.trials) >= early_stopping_min_trials:
                recent_trials = study.trials[-early_stopping_patience:]
                recent_values = [t.value for t in recent_trials if t.state == optuna.trial.TrialState.COMPLETE]
                
                if len(recent_values) >= early_stopping_patience:
                    if min(recent_values) >= study.best_value:
                        if verbose:
                            print(f"Early stopping triggered after {len(study.trials)} trials")
                        study.stop()
        
        callbacks.append(early_stopping_callback)
        
        # Save study configuration
        study_config = {
            "study_name": study_name,
            "configuration": hpo_section,
            "sampler": {
                "type": type(sampler).__name__,
                "config": sampler_config
            },
            "pruner": {
                "type": type(pruner).__name__,
                "config": pruner_config
            },
            "search_space": {
                "model_types": model_types,
                "trial_epochs": trial_epochs,
                "cv_folds": cv_folds
            },
            "data_config": {
                "use_real_data": use_real_data,
                "normal_samples": normal_samples,
                "attack_samples": attack_samples,
                "features": features
            },
            "optimization": {
                "n_trials": n_trials,
                "timeout_seconds": timeout_seconds,
                "direction": direction
            },
            "timestamp": datetime.now().isoformat()
        }
        
        config_path = study_dir / "study_config.json"
        with open(config_path, "w") as f:
            json.dump(study_config, f, indent=2, default=str)
        
        # Optimization function
        def run_optimization():
            """Run the optimization process"""
            try:
                if verbose:
                    print("Starting optimization...")
                
                study.optimize(
                    objective,
                    n_trials=n_trials,
                    timeout=timeout_seconds if timeout_seconds > 0 else None,
                    callbacks=callbacks,
                    gc_after_trial=True,
                    show_progress_bar=False  # We have custom progress
                )
                
                return {
                    'success': True,
                    'study': study,
                    'n_trials_completed': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
                    'best_value': study.best_value if study.trials else float('inf'),
                    'best_params': study.best_params if study.trials else {},
                    'best_trial': study.best_trial if study.trials else None
                }
            
            except Exception as e:
                logger.error(f"Optimization failed: {str(e)}")
                return {
                    'success': False,
                    'error': str(e),
                    'study': study,
                    'n_trials_completed': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])
                }
        
        # Results analysis function
        def analyze_results():
            """Analyze optimization results and generate reports"""
            if not study.trials:
                return {'error': 'No trials completed'}
            
            try:
                best_trial = study.best_trial
                
                # Generate comprehensive analysis
                analysis = {
                    'study_summary': {
                        'study_name': study_name,
                        'n_trials': len(study.trials),
                        'n_complete_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
                        'n_pruned_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),
                        'n_failed_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]),
                        'best_value': study.best_value,
                        'best_trial_number': best_trial.number,
                        'optimization_direction': direction
                    },
                    'best_trial': {
                        'number': best_trial.number,
                        'value': best_trial.value,
                        'params': best_trial.params,
                        'user_attrs': best_trial.user_attrs
                    },
                    'parameter_importance': {},
                    'configuration': study_config,
                    'timestamp': datetime.now().isoformat()
                }
                
                # Calculate parameter importance if enough trials
                if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 10:
                    try:
                        importance = optuna.importance.get_param_importances(study)
                        analysis['parameter_importance'] = importance
                    except Exception as e:
                        logger.warning(f"Failed to calculate parameter importance: {e}")
                
                # Save analysis
                if save_study:
                    analysis_path = study_dir / f"{study_name}_analysis.json"
                    with open(analysis_path, "w") as f:
                        json.dump(analysis, f, indent=2, default=str)
                    analysis['analysis_path'] = str(analysis_path)
                
                return analysis
            
            except Exception as e:
                logger.error(f"Results analysis failed: {str(e)}")
                return {'error': str(e)}
        
        # Plot generation function
        def generate_plots():
            """Generate optimization plots"""
            if not study.trials or not generate_plots:
                return {}
            
            try:
                plot_dir = study_dir / "plots"
                plot_dir.mkdir(exist_ok=True)
                
                plots = {}
                
                # Optimization history
                try:
                    fig = vis.plot_optimization_history(study)
                    plot_path = plot_dir / "optimization_history.html"
                    fig.write_html(plot_path)
                    plots['optimization_history'] = str(plot_path)
                except Exception as e:
                    logger.warning(f"Failed to generate optimization history plot: {e}")
                
                # Parameter importances
                if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 10:
                    try:
                        fig = vis.plot_param_importances(study)
                        plot_path = plot_dir / "param_importances.html"
                        fig.write_html(plot_path)
                        plots['param_importances'] = str(plot_path)
                    except Exception as e:
                        logger.warning(f"Failed to generate parameter importance plot: {e}")
                
                # Parallel coordinate plot
                if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 5:
                    try:
                        fig = vis.plot_parallel_coordinate(study)
                        plot_path = plot_dir / "parallel_coordinate.html"
                        fig.write_html(plot_path)
                        plots['parallel_coordinate'] = str(plot_path)
                    except Exception as e:
                        logger.warning(f"Failed to generate parallel coordinate plot: {e}")
                
                # Slice plot
                if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 5:
                    try:
                        fig = vis.plot_slice(study)
                        plot_path = plot_dir / "slice_plot.html"
                        fig.write_html(plot_path)
                        plots['slice_plot'] = str(plot_path)
                    except Exception as e:
                        logger.warning(f"Failed to generate slice plot: {e}")
                
                if verbose and plots:
                    print(f"Plots saved to: {plot_dir}")
                
                return plots
            
            except Exception as e:
                logger.error(f"Plot generation failed: {str(e)}")
                return {'error': str(e)}
        
        # Save study function
        def save_study_data():
            """Save study data and metadata"""
            if not save_study:
                return {}
            
            try:
                saved_files = {}
                
                # Save study object
                study_path = study_dir / f"{study_name}_study.pkl"
                joblib.dump(study, study_path)
                saved_files['study_path'] = str(study_path)
                
                # Save study trials data
                trials_data = []
                for trial in study.trials:
                    trial_data = {
                        'number': trial.number,
                        'state': trial.state.name,
                        'value': trial.value,
                        'params': trial.params,
                        'user_attrs': trial.user_attrs,
                        'datetime_start': trial.datetime_start.isoformat() if trial.datetime_start else None,
                        'datetime_complete': trial.datetime_complete.isoformat() if trial.datetime_complete else None
                    }
                    trials_data.append(trial_data)
                
                trials_path = study_dir / f"{study_name}_trials.json"
                with open(trials_path, "w") as f:
                    json.dump(trials_data, f, indent=2, default=str)
                saved_files['trials_path'] = str(trials_path)
                
                if verbose:
                    print(f"Study data saved to: {study_dir}")
                
                return saved_files
            
            except Exception as e:
                logger.error(f"Failed to save study data: {str(e)}")
                return {'error': str(e)}
        
        # Calculate setup time
        setup_time = time.time() - setup_start_time
        
        # Return comprehensive setup results
        setup_results = {
            'success': True,
            'study': study,
            'study_name': study_name,
            'study_dir': str(study_dir),
            'configuration': study_config,
            'setup_time_seconds': setup_time,
            
            # Functions for running optimization
            'run_optimization': run_optimization,
            'analyze_results': analyze_results,
            'generate_plots': generate_plots,
            'save_study_data': save_study_data,
            
            # Configuration details
            'optimization_config': {
                'n_trials': n_trials,
                'timeout_seconds': timeout_seconds,
                'direction': direction,
                'sampler_type': sampler_type,
                'pruner_type': pruner_type,
                'cv_folds': cv_folds,
                'trial_epochs': trial_epochs,
                'model_types': model_types
            },
            
            # Data configuration
            'data_config': {
                'use_real_data': use_real_data,
                'data_path': data_path,
                'artifacts_path': artifacts_path,
                'normal_samples': normal_samples,
                'attack_samples': attack_samples,
                'features': features
            },
            
            # System configuration
            'system_config': {
                'device': device,
                'random_seed': random_seed,
                'num_workers': num_workers,
                'parallel_jobs': parallel_jobs
            },
            
            # Callbacks and monitoring
            'callbacks': callbacks,
            'verbose': verbose,
            'show_progress': show_progress
        }
        
        if verbose:
            print(f"Setup completed in {setup_time:.1f} seconds")
            print("-"*80)
        
        return setup_results
    
    except Exception as e:
        error_msg = f"HPO setup failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Create error results
        error_results = {
            'success': False,
            'error': error_msg,
            'error_type': type(e).__name__,
            'setup_time_seconds': time.time() - setup_start_time,
            'study_name': study_name,
            'configuration': hpo_section,
            'traceback': traceback.format_exc()
        }
        
        # Save error information
        if save_study:
            try:
                error_path = study_dir / f"{study_name}_setup_error.json"
                study_dir.mkdir(parents=True, exist_ok=True)
                with open(error_path, 'w') as f:
                    json.dump(error_results, f, indent=2, default=str)
                error_results['error_log_path'] = str(error_path)
            except Exception as save_error:
                logger.warning(f"Failed to save error log: {save_error}")
        
        return error_results
    
    finally:
        # Restore logging level
        # if verbose and 'original_level' in locals():
        #     try:
        #         logger.setLevel(original_level)
        #     except Exception:
        #         pass
        
        # Final cleanup
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        except Exception:
            pass

def run_hyperparameter_optimization(
    # Core HPO Parameters
    n_trials: Optional[int] = None,
    timeout_minutes: Optional[float] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    
    # Data Parameters
    use_real_data: Optional[bool] = None,
    data_path: Optional[Union[str, Path]] = None,
    artifacts_path: Optional[Union[str, Path]] = None,
    normal_samples: Optional[int] = None,
    attack_samples: Optional[int] = None,
    features: Optional[int] = None,
    
    # Model Selection
    model_types: Optional[List[str]] = None,
    search_all_models: Optional[bool] = None,
    
    # Optimization Configuration
    sampler_type: Optional[str] = None,
    pruner_type: Optional[str] = None,
    cv_folds: Optional[int] = None,
    trial_epochs: Optional[int] = None,
    
    # System Parameters
    device: Optional[str] = None,
    random_seed: Optional[int] = None,
    num_workers: Optional[int] = None,
    
    # Output Parameters
    verbose: Optional[bool] = None,
    interactive: Optional[bool] = None,
    save_study: Optional[bool] = None,
    study_dir: Optional[Union[str, Path]] = None,
    generate_plots: Optional[bool] = None,
    
    # Training Parameters
    train_best_model: Optional[bool] = None,
    save_best_config: Optional[bool] = None,
    
    # Direct Configuration Override
    config: Optional[Dict[str, Any]] = None,
    hpo_config: Optional[Dict[str, Any]] = None,
    
    **kwargs
) -> Optional[Dict[str, Any]]:
    """
    Run comprehensive hyperparameter optimization with interactive configuration and seamless train_model() integration.
    
    This function provides both interactive and programmatic interfaces for hyperparameter optimization,
    integrating with the current train_model() implementation and configuration system.
    
    Returns:
        Dictionary containing optimization results, best parameters, and study information, or None if cancelled.
    """
    
    # Start timing
    start_time = datetime.now()
    hpo_start_time = time.time()
    
    # Initialize configuration
    if config is None:
        try:
            config = get_current_config() if 'get_current_config' in globals() else {}
        except Exception:
            config = {}
    
    # Apply HPO-specific configuration
    if hpo_config:
        config.setdefault('hyperparameter_optimization', {}).update(hpo_config)
    
    # Apply all parameters to configuration
    final_config = config.copy()
    
    # Apply individual parameters
    params = locals().copy()
    params.update(kwargs)
    
    # Remove non-parameter items
    params_to_remove = {
        'config', 'hpo_config', 'kwargs', 'start_time', 'hpo_start_time',
        'datetime', 'traceback', 'time', 'gc', 'warnings', 'Path'
    }
    
    cleaned_params = {k: v for k, v in params.items() if k not in params_to_remove and v is not None}
    
    # Set up defaults
    hpo_section = final_config.setdefault('hyperparameter_optimization', {})
    
    # Core parameters with intelligent defaults
    n_trials = hpo_section.setdefault('n_trials', cleaned_params.get('n_trials', 50))
    timeout_minutes = hpo_section.setdefault('timeout_minutes', cleaned_params.get('timeout_minutes', 0))
    study_name = hpo_section.setdefault('study_name', cleaned_params.get('study_name', f"autoencoder_hpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"))
    direction = hpo_section.setdefault('direction', cleaned_params.get('direction', 'minimize'))
    sampler_type = hpo_section.setdefault('sampler_type', cleaned_params.get('sampler_type', 'TPE'))
    pruner_type = hpo_section.setdefault('pruner_type', cleaned_params.get('pruner_type', 'MedianPruner'))
    
    # Data parameters
    use_real_data = hpo_section.setdefault('use_real_data', cleaned_params.get('use_real_data', None))
    data_path = hpo_section.setdefault('data_path', cleaned_params.get('data_path', None))
    artifacts_path = hpo_section.setdefault('artifacts_path', cleaned_params.get('artifacts_path', None))
    normal_samples = hpo_section.setdefault('normal_samples', cleaned_params.get('normal_samples', 10000))
    attack_samples = hpo_section.setdefault('attack_samples', cleaned_params.get('attack_samples', 2000))
    features = hpo_section.setdefault('features', cleaned_params.get('features', 78))
    
    # Model selection
    model_types = hpo_section.setdefault('model_types', cleaned_params.get('model_types', ['SimpleAutoencoder', 'EnhancedAutoencoder']))
    search_all_models = hpo_section.setdefault('search_all_models', cleaned_params.get('search_all_models', False))
    
    # Optimization configuration
    cv_folds = hpo_section.setdefault('cv_folds', cleaned_params.get('cv_folds', 3))
    trial_epochs = hpo_section.setdefault('trial_epochs', cleaned_params.get('trial_epochs', 20))
    
    # System parameters
    device = hpo_section.setdefault('device', cleaned_params.get('device', 'auto'))
    random_seed = hpo_section.setdefault('random_seed', cleaned_params.get('random_seed', 42))
    num_workers = hpo_section.setdefault('num_workers', cleaned_params.get('num_workers', 0))
    
    # Output parameters
    verbose = hpo_section.setdefault('verbose', cleaned_params.get('verbose', True))
    interactive = hpo_section.setdefault('interactive', cleaned_params.get('interactive', True))
    save_study = hpo_section.setdefault('save_study', cleaned_params.get('save_study', True))
    study_dir = hpo_section.setdefault('study_dir', cleaned_params.get('study_dir', DEFAULT_MODEL_DIR / "hpo_studies"))
    generate_plots = hpo_section.setdefault('generate_plots', cleaned_params.get('generate_plots', True))
    
    # Training parameters
    train_best_model = hpo_section.setdefault('train_best_model', cleaned_params.get('train_best_model', True))
    save_best_config = hpo_section.setdefault('save_best_config', cleaned_params.get('save_best_config', True))
    
    try:
        # Interactive configuration if enabled
        if interactive:
            print("\n" + "="*80)
            print("HYPERPARAMETER OPTIMIZATION SETUP")
            print("="*80)
            print("This will optimize model parameters for best performance using Optuna.")
            print("Configure the optimization parameters or use defaults.")
            print("-"*80)
            
            # Core optimization parameters
            print("\nCORE OPTIMIZATION PARAMETERS")
            print("-" * 40)
            
            user_trials = input(f"Number of trials ({n_trials}): ").strip()
            if user_trials:
                try:
                    n_trials = int(user_trials)
                    hpo_section['n_trials'] = n_trials
                except ValueError:
                    print(f"Invalid input, using default: {n_trials}")
            
            user_timeout = input(f"Timeout in minutes (0 for no timeout, current: {timeout_minutes}): ").strip()
            if user_timeout:
                try:
                    timeout_minutes = float(user_timeout)
                    hpo_section['timeout_minutes'] = timeout_minutes
                except ValueError:
                    print(f"Invalid input, using default: {timeout_minutes}")
            
            user_study_name = input(f"Study name ({study_name}): ").strip()
            if user_study_name:
                study_name = user_study_name
                hpo_section['study_name'] = study_name
            
            # Model type selection
            print("\nMODEL TYPE SELECTION")
            print("-" * 40)
            print("Select which model types to optimize:")
            print("1. All model types (SimpleAutoencoder, EnhancedAutoencoder, AutoencoderEnsemble)")
            print("2. SimpleAutoencoder only")
            print("3. EnhancedAutoencoder only")
            print("4. AutoencoderEnsemble only")
            print("5. SimpleAutoencoder + EnhancedAutoencoder")
            print("6. EnhancedAutoencoder + AutoencoderEnsemble")
            print("7. Custom selection")
            
            model_choice = input("Select option (1-7): ").strip()
            
            if model_choice == "1":
                model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                search_all_models = True
            elif model_choice == "2":
                model_types = ['SimpleAutoencoder']
                search_all_models = False
            elif model_choice == "3":
                model_types = ['EnhancedAutoencoder']
                search_all_models = False
            elif model_choice == "4":
                model_types = ['AutoencoderEnsemble']
                search_all_models = False
            elif model_choice == "5":
                model_types = ['SimpleAutoencoder', 'EnhancedAutoencoder']
                search_all_models = False
            elif model_choice == "6":
                model_types = ['EnhancedAutoencoder', 'AutoencoderEnsemble']
                search_all_models = False
            elif model_choice == "7":
                print("\nAvailable model types:")
                available_types = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
                for i, mt in enumerate(available_types, 1):
                    print(f"{i}. {mt}")
                
                selection = input("Enter numbers separated by commas (e.g., 1,2): ").strip()
                try:
                    indices = [int(x.strip()) - 1 for x in selection.split(',')]
                    model_types = [available_types[i] for i in indices if 0 <= i < len(available_types)]
                    search_all_models = len(model_types) >= 3
                except (ValueError, IndexError):
                    print("Invalid selection, using default model types")
            
            hpo_section['model_types'] = model_types
            hpo_section['search_all_models'] = search_all_models
            
            # Data configuration
            print("\nDATA CONFIGURATION")
            print("-" * 40)
            
            if use_real_data is None:
                data_choice = input("Use real network data? (y/N): ").lower().strip()
                use_real_data = data_choice in ('y', 'yes')
                hpo_section['use_real_data'] = use_real_data
            
            if use_real_data:
                print("Real data configuration:")
                user_data_path = input(f"Data file path ({data_path or 'default'}): ").strip()
                if user_data_path:
                    data_path = user_data_path
                    hpo_section['data_path'] = data_path
                
                user_artifacts_path = input(f"Artifacts path ({artifacts_path or 'default'}): ").strip()
                if user_artifacts_path:
                    artifacts_path = user_artifacts_path
                    hpo_section['artifacts_path'] = artifacts_path
            else:
                print("Synthetic data configuration:")
                user_normal = input(f"Normal samples ({normal_samples}): ").strip()
                if user_normal:
                    try:
                        normal_samples = int(user_normal)
                        hpo_section['normal_samples'] = normal_samples
                    except ValueError:
                        print(f"Invalid input, using default: {normal_samples}")
                
                user_attack = input(f"Attack samples ({attack_samples}): ").strip()
                if user_attack:
                    try:
                        attack_samples = int(user_attack)
                        hpo_section['attack_samples'] = attack_samples
                    except ValueError:
                        print(f"Invalid input, using default: {attack_samples}")
                
                user_features = input(f"Number of features ({features}): ").strip()
                if user_features:
                    try:
                        features = int(user_features)
                        hpo_section['features'] = features
                    except ValueError:
                        print(f"Invalid input, using default: {features}")
            
            # Advanced configuration
            advanced_config = input("\nConfigure advanced options? (y/N): ").lower().strip() == 'y'
            
            if advanced_config:
                print("\nADVANCED CONFIGURATION")
                print("-" * 40)
                
                # Sampler configuration
                print("Sampler options:")
                samplers = ['TPE', 'Random', 'CmaEs']
                for i, sampler in enumerate(samplers, 1):
                    print(f"{i}. {sampler}")
                
                sampler_choice = input(f"Select sampler (1-{len(samplers)}): ").strip()
                if sampler_choice and sampler_choice.isdigit():
                    idx = int(sampler_choice) - 1
                    if 0 <= idx < len(samplers):
                        sampler_type = samplers[idx]
                        hpo_section['sampler_type'] = sampler_type
                
                # Pruner configuration
                print("\nPruner options:")
                pruners = ['MedianPruner', 'HyperbandPruner', 'NopPruner']
                for i, pruner in enumerate(pruners, 1):
                    print(f"{i}. {pruner}")
                
                pruner_choice = input(f"Select pruner (1-{len(pruners)}): ").strip()
                if pruner_choice and pruner_choice.isdigit():
                    idx = int(pruner_choice) - 1
                    if 0 <= idx < len(pruners):
                        pruner_type = pruners[idx]
                        hpo_section['pruner_type'] = pruner_type
                
                # Cross-validation configuration
                user_cv_folds = input(f"Cross-validation folds ({cv_folds}): ").strip()
                if user_cv_folds:
                    try:
                        cv_folds = int(user_cv_folds)
                        hpo_section['cv_folds'] = cv_folds
                    except ValueError:
                        print(f"Invalid input, using default: {cv_folds}")
                
                # Trial configuration
                user_trial_epochs = input(f"Epochs per trial ({trial_epochs}): ").strip()
                if user_trial_epochs:
                    try:
                        trial_epochs = int(user_trial_epochs)
                        hpo_section['trial_epochs'] = trial_epochs
                    except ValueError:
                        print(f"Invalid input, using default: {trial_epochs}")
                
                # System configuration
                user_device = input(f"Device ({device}): ").strip()
                if user_device:
                    device = user_device
                    hpo_section['device'] = device
                
                # Output configuration
                user_generate_plots = input("Generate optimization plots? (Y/n): ").lower().strip()
                generate_plots = user_generate_plots not in ('n', 'no')
                hpo_section['generate_plots'] = generate_plots
                
                user_study_dir = input(f"Study directory ({study_dir}): ").strip()
                if user_study_dir:
                    study_dir = Path(user_study_dir)
                    hpo_section['study_dir'] = str(study_dir)
            
            # Final confirmation
            print("\n" + "="*60)
            print("OPTIMIZATION CONFIGURATION SUMMARY")
            print("="*60)
            print(f"Study Name: {study_name}")
            print(f"Trials: {n_trials}")
            print(f"Timeout: {timeout_minutes} minutes" if timeout_minutes > 0 else "Timeout: No limit")
            print(f"Model Types: {', '.join(model_types)}")
            print(f"Data Source: {'Real Data' if use_real_data else 'Synthetic Data'}")
            if not use_real_data:
                print(f"Samples: {normal_samples} normal, {attack_samples} attack")
                print(f"Features: {features}")
            print(f"Cross-Validation: {cv_folds} folds")
            print(f"Trial Epochs: {trial_epochs}")
            print(f"Sampler: {sampler_type}")
            print(f"Pruner: {pruner_type}")
            print(f"Device: {device}")
            print("="*60)
            
            confirm = input("\nStart hyperparameter optimization? (Y/n): ").lower().strip()
            if confirm in ('n', 'no'):
                print("Hyperparameter optimization cancelled.")
                return None
        
        # Update final configuration
        final_config['hyperparameter_optimization'] = hpo_section
        
        if verbose:
            print("\nSetting up hyperparameter optimization...")
        
        # Set up hyperparameter optimization using the setup function
        hpo_setup = setup_hyperparameter_optimization(
            n_trials=n_trials,
            timeout_seconds=timeout_minutes * 60 if timeout_minutes > 0 else 0,
            study_name=study_name,
            direction=direction,
            sampler_type=sampler_type,
            pruner_type=pruner_type,
            use_real_data=use_real_data,
            data_path=data_path,
            artifacts_path=artifacts_path,
            normal_samples=normal_samples,
            attack_samples=attack_samples,
            features=features,
            model_types=model_types,
            search_all_models=search_all_models,
            cv_folds=cv_folds,
            trial_epochs=trial_epochs,
            device=device,
            random_seed=random_seed,
            num_workers=num_workers,
            verbose=verbose,
            save_study=save_study,
            study_dir=study_dir,
            generate_plots=generate_plots,
            config=final_config
        )
        
        if not hpo_setup.get('success', False):
            error_msg = hpo_setup.get('error', 'Setup failed')
            print(f"HPO setup failed: {error_msg}")
            return hpo_setup
        
        if verbose:
            print("Setup completed successfully. Starting optimization...")
        
        # Run optimization
        optimization_results = hpo_setup['run_optimization']()
        
        if not optimization_results.get('success', False):
            error_msg = optimization_results.get('error', 'Optimization failed')
            print(f"Optimization failed: {error_msg}")
            return optimization_results
        
        # Get study and results
        study = optimization_results['study']
        n_trials_completed = optimization_results['n_trials_completed']
        best_value = optimization_results['best_value']
        best_params = optimization_results['best_params']
        best_trial = optimization_results['best_trial']
        
        # Calculate total time
        total_time = time.time() - hpo_start_time
        
        # Analyze results
        if verbose:
            print("Analyzing optimization results...")
        
        analysis = hpo_setup['analyze_results']()
        
        # Generate plots
        plots = {}
        if generate_plots:
            if verbose:
                print("Generating optimization plots...")
            plots = hpo_setup['generate_plots']()
        
        # Save study data
        saved_files = {}
        if save_study:
            if verbose:
                print("Saving study data...")
            saved_files = hpo_setup['save_study_data']()
        
        # Prepare comprehensive results
        hpo_results = {
            'success': True,
            'start_time': start_time.isoformat(),
            'end_time': datetime.now().isoformat(),
            'total_time_seconds': total_time,
            'total_time_minutes': total_time / 60,
            
            # Study information
            'study_name': study_name,
            'study_dir': str(study_dir),
            'study': study,
            
            # Optimization results
            'n_trials_total': len(study.trials),
            'n_trials_completed': n_trials_completed,
            'n_trials_pruned': len([t for t in study.trials if t.state.name == 'PRUNED']),
            'n_trials_failed': len([t for t in study.trials if t.state.name == 'FAIL']),
            'best_value': best_value,
            'best_params': best_params,
            'best_trial_number': best_trial.number if best_trial else None,
            
            # Configuration
            'optimization_config': hpo_section,
            'model_types_optimized': model_types,
            'data_config': {
                'use_real_data': use_real_data,
                'normal_samples': normal_samples,
                'attack_samples': attack_samples,
                'features': features,
                'cv_folds': cv_folds
            },
            
            # Analysis and artifacts
            'analysis': analysis,
            'plots': plots,
            'saved_files': saved_files,
            
            # Best configuration for train_model
            'best_config_for_training': None,
            'recommendations': []
        }
        
        # Extract best configuration for train_model integration
        if best_trial and hasattr(best_trial, 'user_attrs'):
            best_config_raw = best_trial.user_attrs.get('complete_config', {})
            if best_config_raw:
                hpo_results['best_config_for_training'] = best_config_raw
        
        # Display comprehensive results
        if verbose:
            print("\n" + "="*80)
            print("HYPERPARAMETER OPTIMIZATION COMPLETED")
            print("="*80)
            print(f"Total Time: {total_time/60:.1f} minutes")
            print(f"Study Name: {study_name}")
            print(f"Trials: {n_trials_completed}/{len(study.trials)} completed")
            
            if n_trials_completed > 0:
                print(f"\nBest Results:")
                print(f"  Objective Value: {best_value:.6f}")
                print(f"  Trial Number: {best_trial.number if best_trial else 'N/A'}")
                
                print(f"\nBest Parameters:")
                for param, value in best_params.items():
                    print(f"  {param}: {value}")
                
                # Show model-specific best parameters
                if best_trial and hasattr(best_trial, 'user_attrs'):
                    model_config = best_trial.user_attrs.get('model_config', {})
                    training_config = best_trial.user_attrs.get('training_config', {})
                    
                    if model_config:
                        print(f"\nBest Model Configuration:")
                        print(f"  Model Type: {model_config.get('model_type', 'N/A')}")
                        print(f"  Encoding Dim: {model_config.get('encoding_dim', 'N/A')}")
                        print(f"  Hidden Dims: {model_config.get('hidden_dims', 'N/A')}")
                        print(f"  Activation: {model_config.get('activation', 'N/A')}")
                        print(f"  Normalization: {model_config.get('normalization', 'N/A')}")
                        
                        if model_config.get('model_type') == 'AutoencoderEnsemble':
                            print(f"  Ensemble Size: {model_config.get('num_models', 'N/A')}")
                            print(f"  Diversity Factor: {model_config.get('diversity_factor', 'N/A')}")
                        
                        if model_config.get('model_type') in ['EnhancedAutoencoder', 'AutoencoderEnsemble']:
                            print(f"  Use Attention: {model_config.get('use_attention', 'N/A')}")
                            print(f"  Residual Blocks: {model_config.get('residual_blocks', 'N/A')}")
                            print(f"  Skip Connections: {model_config.get('skip_connection', 'N/A')}")
                    
                    if training_config:
                        print(f"\nBest Training Configuration:")
                        print(f"  Learning Rate: {training_config.get('learning_rate', 'N/A')}")
                        print(f"  Batch Size: {training_config.get('batch_size', 'N/A')}")
                        print(f"  Weight Decay: {training_config.get('weight_decay', 'N/A')}")
                        print(f"  Optimizer: {training_config.get('optimizer_type', 'N/A')}")
                        print(f"  Scheduler: {training_config.get('scheduler_type', 'N/A')}")
                
                # Show cross-validation statistics
                if best_trial and hasattr(best_trial, 'user_attrs'):
                    cv_scores = best_trial.user_attrs.get('individual_fold_scores', [])
                    if cv_scores:
                        valid_scores = [s for s in cv_scores if s != float('inf')]
                        if valid_scores:
                            print(f"\nCross-Validation Results:")
                            print(f"  Mean CV Score: {np.mean(valid_scores):.6f}")
                            print(f"  Std CV Score: {np.std(valid_scores):.6f}")
                            print(f"  Valid Folds: {len(valid_scores)}/{len(cv_scores)}")
            
            # Show study statistics
            if analysis.get('study_summary'):
                summary = analysis['study_summary']
                print(f"\nStudy Statistics:")
                print(f"  Completed Trials: {summary.get('n_complete_trials', 0)}")
                print(f"  Pruned Trials: {summary.get('n_pruned_trials', 0)}")
                print(f"  Failed Trials: {summary.get('n_failed_trials', 0)}")
            
            # Show artifacts
            if saved_files:
                print(f"\nSaved Artifacts:")
                for file_type, file_path in saved_files.items():
                    print(f"  {file_type.replace('_', ' ').title()}: {file_path}")
            
            if plots:
                print(f"\nGenerated Plots:")
                for plot_type, plot_path in plots.items():
                    print(f"  {plot_type.replace('_', ' ').title()}: {plot_path}")
        
        # Generate recommendations
        recommendations = []
        
        if n_trials_completed == 0:
            recommendations.append("No trials completed successfully - check configuration and data")
        elif n_trials_completed < n_trials * 0.5:
            recommendations.append("Many trials failed - consider simplifying search space or checking system resources")
        
        if best_value != float('inf'):
            if best_value < 0.01:
                recommendations.append("Excellent optimization results - ready for production use")
            elif best_value < 0.05:
                recommendations.append("Good optimization results - consider additional fine-tuning")
            elif best_value < 0.1:
                recommendations.append("Acceptable results - may benefit from longer optimization or different search space")
            else:
                recommendations.append("High objective value - consider adjusting search space or model architecture")
        
        if len(model_types) == 1:
            recommendations.append("Consider optimizing multiple model types for comparison")
        
        if cv_folds < 3:
            recommendations.append("Consider using more cross-validation folds for more robust results")
        
        if trial_epochs < 20:
            recommendations.append("Consider increasing trial epochs for more stable convergence")
        
        hpo_results['recommendations'] = recommendations
        
        if verbose and recommendations:
            print(f"\nRecommendations:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        
        # Save best configuration to global config if requested
        if save_best_config and hpo_results.get('best_config_for_training'):
            try:
                best_config = hpo_results['best_config_for_training']
                current_config = get_current_config()
                
                # Merge best configuration
                for section, values in best_config.items():
                    if isinstance(values, dict):
                        current_config.setdefault(section, {}).update(values)
                    else:
                        current_config[section] = values
                
                # Add HPO metadata
                current_config.setdefault('metadata', {}).update({
                    'optimized_with_hpo': True,
                    'hpo_study_name': study_name,
                    'hpo_best_value': best_value,
                    'hpo_optimization_date': datetime.now().isoformat(),
                    'hpo_trial_number': best_trial.number if best_trial else None
                })
                
                update_global_config(current_config)
                
                if verbose:
                    print(f"\nBest configuration saved to global config")
                
            except Exception as e:
                logger.warning(f"Failed to save best configuration: {e}")
                recommendations.append("Failed to save best configuration - manually apply best parameters")
        
        # Offer to train final model with best parameters
        if train_best_model and n_trials_completed > 0 and hpo_results.get('best_config_for_training'):
            if interactive:
                train_final = input("\nTrain final model with optimized parameters? (Y/n): ").lower().strip()
                train_final = train_final not in ('n', 'no')
            else:
                train_final = True
            
            if train_final:
                try:
                    if verbose:
                        print("\nTraining final model with optimized parameters...")
                    
                    # Use the best configuration for training
                    best_config = hpo_results['best_config_for_training'].copy()
                    
                    # Update with full training parameters (not trial parameters)
                    training_config = best_config.setdefault('training_config', {})
                    training_config.update({
                        'epochs': 100,  # Full training epochs
                        'patience': 20,  # Longer patience for final training
                        'verbose': True,
                        'tensorboard_logging': True,
                        'save_checkpoints': True,
                        'progress_bar': True
                    })
                    
                    # Update monitoring for full training
                    monitoring_config = best_config.setdefault('monitoring_config', {})
                    monitoring_config.update({
                        'verbose': True,
                        'tensorboard_logging': True,
                        'save_checkpoints': True,
                        'save_best_model': True,
                        'progress_bar': True
                    })
                    
                    # Update export for full training
                    export_config = best_config.setdefault('export_config', {})
                    export_config.update({
                        'save_model': True,
                        'save_metadata': True,
                        'save_training_history': True
                    })
                    
                    # Update system config for final training
                    system_config = best_config.setdefault('system_config', {})
                    system_config.update({
                        'model_dir': Path(study_dir) / "final_model",
                        'log_dir': Path(study_dir) / "final_model" / "logs"
                    })
                    
                    # Train final model
                    final_training_results = train_model(config=best_config)
                    
                    if final_training_results and final_training_results.get('success', False):
                        hpo_results['final_model_training'] = final_training_results
                        
                        if verbose:
                            print("\nFinal model training completed successfully!")
                            final_metrics = final_training_results.get('final_metrics', {})
                            print(f"Final Validation Loss: {final_metrics.get('best_validation_loss', 'N/A')}")
                            print(f"Test Loss: {final_metrics.get('test_loss', 'N/A')}")
                            print(f"Training Time: {final_training_results.get('training_time_minutes', 0):.1f} minutes")
                            
                            artifacts = final_training_results.get('artifacts', {})
                            if artifacts:
                                print(f"Final Model Artifacts:")
                                for artifact_type, path in artifacts.items():
                                    print(f"  {artifact_type.replace('_', ' ').title()}: {path}")
                    else:
                        error_msg = final_training_results.get('error', 'Unknown error') if final_training_results else 'No results'
                        print(f"Final model training failed: {error_msg}")
                        hpo_results['final_model_training'] = {'success': False, 'error': error_msg}
                        recommendations.append("Final model training failed - manually train with best parameters")
                
                except Exception as e:
                    error_msg = f"Final model training failed: {str(e)}"
                    print(error_msg)
                    hpo_results['final_model_training'] = {'success': False, 'error': error_msg}
                    recommendations.append("Final model training failed - manually train with best parameters")
        
        if verbose:
            print("\n" + "="*80)
            print("HYPERPARAMETER OPTIMIZATION SUMMARY")
            print("="*80)
            
            if n_trials_completed > 0:
                print("Optimization completed successfully!")
                print(f"Best objective value: {best_value:.6f}")
                print(f"Optimization time: {total_time/60:.1f} minutes")
                
                if hpo_results.get('final_model_training', {}).get('success', False):
                    print("Final model trained successfully!")
                
                if save_best_config:
                    print("Best configuration saved to global config")
                
                print(f"\nNext steps:")
                print(f"1. Review optimization results and plots")
                print(f"2. Use best parameters for production training")
                print(f"3. Consider further optimization if needed")
                
            else:
                print("Optimization completed but no trials succeeded")
                print("Check configuration and system requirements")
            
            print("="*80)
        
        return hpo_results
    
    except KeyboardInterrupt:
        print("\nHyperparameter optimization interrupted by user!")
        return {
            'success': False,
            'error': 'Interrupted by user',
            'error_type': 'KeyboardInterrupt',
            'start_time': start_time.isoformat(),
            'end_time': datetime.now().isoformat(),
            'total_time_seconds': time.time() - hpo_start_time,
            'study_name': study_name,
            'configuration': hpo_section
        }
    
    except Exception as e:
        error_msg = f"Hyperparameter optimization failed: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        error_results = {
            'success': False,
            'error': error_msg,
            'error_type': type(e).__name__,
            'start_time': start_time.isoformat(),
            'end_time': datetime.now().isoformat(),
            'total_time_seconds': time.time() - hpo_start_time,
            'study_name': study_name,
            'configuration': hpo_section,
            'traceback': traceback.format_exc()
        }
        
        # Save error information
        if save_study:
            try:
                error_dir = Path(study_dir)
                error_dir.mkdir(parents=True, exist_ok=True)
                error_path = error_dir / f"hpo_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                
                with open(error_path, 'w') as f:
                    json.dump(error_results, f, indent=2, default=str)
                
                error_results['error_log_path'] = str(error_path)
                
                if verbose:
                    print(f"Error information saved to: {error_path}")
            
            except Exception as save_error:
                logger.warning(f"Failed to save error log: {save_error}")
        
        if verbose:
            print(f"\nHyperparameter optimization failed: {error_msg}")
            print("Check the error log for detailed information.")
        
        return error_results
    
    finally:
        # Final cleanup
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        except Exception:
            pass

def run_hyperparameter_optimization_interactive(
    use_real_data: Optional[bool] = None,
    use_current_config: bool = False,
    preset: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None,
    non_interactive: bool = False,
    **kwargs
) -> Optional[Dict[str, Any]]:
    """Interactive HPO setup with express, preset, and custom configuration options."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        show_banner()
        
        print("\n" + "="*80)
        print("INTERACTIVE HYPERPARAMETER OPTIMIZATION SETUP")
        print("="*80)
        
        if config:
            base_config = config.copy()
            print("Using provided configuration as base")
        elif use_current_config:
            try:
                base_config = get_current_config() if 'get_current_config' in globals() else {}
                print("Using current system configuration")
            except Exception as e:
                logger.warning(f"Failed to load current config: {e}")
                base_config = {}
        else:
            base_config = {}
        
        if preset:
            try:
                # Use existing PRESET_CONFIGS instead of non-existent HPO_PRESET_CONFIGS
                if preset in PRESET_CONFIGS:
                    preset_config = PRESET_CONFIGS[preset].copy()
                    
                    # Deep merge the preset configuration into base_config
                    for section, values in preset_config.items():
                        if section not in base_config:
                            base_config[section] = values
                        elif isinstance(values, dict) and isinstance(base_config[section], dict):
                            base_config[section] = deep_update(base_config[section], values)
                        else:
                            base_config[section] = values
                    
                    print(f"Applied preset configuration: {preset}")
                    
                    # Extract and display HPO settings from the preset
                    hpo_settings = preset_config.get('hyperparameter_optimization', {})
                    if hpo_settings:
                        print(f"  - HPO enabled: {hpo_settings.get('enabled', False)}")
                        print(f"  - Strategy: {hpo_settings.get('strategy', 'optuna')}")
                        print(f"  - Trials: {hpo_settings.get('n_trials', 50)}")
                        print(f"  - Sampler: {hpo_settings.get('sampler', 'TPESampler')}")
                        print(f"  - Pruner: {hpo_settings.get('pruner', 'MedianPruner')}")
                    
                else:
                    print(f"Preset '{preset}' not found in PRESET_CONFIGS, using base configuration")
                    available_presets = list(PRESET_CONFIGS.keys())
                    print(f"Available presets: {', '.join(available_presets)}")
                    
            except Exception as e:
                logger.warning(f"Failed to apply preset '{preset}': {e}")
        
        if non_interactive or use_current_config:
            print("\nNon-interactive mode - using configured defaults")
            return _launch_hpo_with_config(base_config, **kwargs)
        
        print("\nThis interactive setup will guide you through:")
        print("   - HPO strategy and optimization goals")
        print("   - Study configuration (trials, timeout, sampling)")
        print("   - Model types and search space")
        print("   - Data source and cross-validation")
        print("   - System resources and performance")
        print("   - Result analysis and visualization")
        
        print("\nQuick Start Options:")
        print("1. Express HPO (recommended for quick optimization)")
        print("2. Custom HPO Configuration (full control)")
        print("3. Use HPO Preset Configuration")
        print("4. Launch existing study (continue optimization)")
        print("0. Cancel and return to previous menu")
        
        while True:
            choice = input("\nSelect option (0-4): ").strip()
            if choice in ['1', '2', '3', '4', '0']:
                break
            print("Invalid choice. Please select 0-4.")
        
        if choice == '1':
            print("\nEXPRESS HPO SETUP")
            print("-" * 40)
            return _interactive_hpo_express_setup(base_config, use_real_data, **kwargs)
        elif choice == '2':
            print("\nCUSTOM HPO CONFIGURATION")
            print("-" * 40)
            return _interactive_hpo_custom_setup(base_config, use_real_data, **kwargs)
        elif choice == '3':
            print("\nHPO PRESET SELECTION")
            print("-" * 40)
            return _interactive_hpo_preset_setup(base_config, use_real_data, **kwargs)
        elif choice == '4':
            print("\nCONTINUE EXISTING STUDY")
            print("-" * 40)
            return _interactive_hpo_continue_setup(base_config, **kwargs)
        elif choice == '0':
            print("HPO setup cancelled by user")
            return None
            
    except KeyboardInterrupt:
        print("\n\nHPO setup interrupted by user!")
        return None
    except Exception as e:
        logger.error(f"Interactive HPO setup failed: {e}")
        print(f"\nHPO setup failed: {str(e)}")
        return None

def _interactive_hpo_express_setup(
    base_config: Dict[str, Any], 
    use_real_data: Optional[bool],
    **kwargs
) -> Optional[Dict[str, Any]]:
    """Express HPO setup with smart defaults and minimal user input."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        show_banner()
        
        print("Setting up hyperparameter optimization with smart defaults...")
        
        # Data source selection
        if use_real_data is None:
            print("\nDATA SOURCE")
            print("1. Real network data (recommended for production)")
            print("2. Synthetic data (good for testing and development)")
            print("0. Cancel and return to previous menu")
            
            while True:
                data_choice = input("Select data source (0-2): ").strip()
                if data_choice in ['1', '2', '0']:
                    break
                print("Invalid choice. Please select 0-2.")
            
            if data_choice == '0':
                print("Data selection cancelled")
                return None
                
            use_real_data = data_choice == '1'
        
        # Optimization goal
        print("\nOPTIMIZATION GOAL")
        print("1. Quick exploration (50 trials, ~30 minutes)")
        print("2. Balanced optimization (100 trials, ~1 hour)")
        print("3. Thorough search (200 trials, ~2-3 hours)")
        print("0. Cancel and return to previous menu")
        
        while True:
            goal_choice = input("Select optimization goal (0-3): ").strip()
            if goal_choice in ['1', '2', '3', '0']:
                break
            print("Invalid choice. Please select 0-3.")
        
        if goal_choice == '0':
            print("Optimization goal selection cancelled")
            return None
        
        # Set parameters based on goal
        goal_configs = {
            '1': {'n_trials': 50, 'trial_epochs': 15, 'timeout_minutes': 30},
            '2': {'n_trials': 100, 'trial_epochs': 20, 'timeout_minutes': 60},
            '3': {'n_trials': 200, 'trial_epochs': 25, 'timeout_minutes': 180}
        }
        
        goal_config = goal_configs[goal_choice]
        n_trials = goal_config['n_trials']
        trial_epochs = goal_config['trial_epochs']
        timeout_minutes = goal_config['timeout_minutes']
        
        # Model types selection
        print("\nMODEL TYPES TO OPTIMIZE")
        print("1. All models (comprehensive search)")
        print("2. Enhanced + Ensemble (best performance)")
        print("3. Simple + Enhanced (balanced)")
        print("4. Enhanced only (recommended)")
        print("0. Cancel and return to previous menu")
        
        while True:
            model_choice = input("Select model types (0-4): ").strip()
            if model_choice in ['1', '2', '3', '4', '0']:
                break
            print("Invalid choice. Please select 0-4.")
        
        if model_choice == '0':
            print("Model selection cancelled")
            return None
        
        model_type_configs = {
            '1': ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble'],
            '2': ['EnhancedAutoencoder', 'AutoencoderEnsemble'],
            '3': ['SimpleAutoencoder', 'EnhancedAutoencoder'],
            '4': ['EnhancedAutoencoder']
        }
        
        model_types = model_type_configs[model_choice]
        
        # Cross-validation setup
        print("\nVALIDATION STRATEGY")
        print("1. 3-fold CV (faster)")
        print("2. 5-fold CV (more robust)")
        print("3. Single split (fastest)")
        print("0. Cancel and return to previous menu")
        
        while True:
            cv_choice = input("Select validation strategy (0-3): ").strip()
            if cv_choice in ['1', '2', '3', '0']:
                break
            print("Invalid choice. Please select 0-3.")
        
        if cv_choice == '0':
            print("Validation strategy selection cancelled")
            return None
        
        cv_configs = {
            '1': {'cv_folds': 3},
            '2': {'cv_folds': 5},
            '3': {'cv_folds': 1}  # Single split
        }
        
        cv_config = cv_configs[cv_choice]
        cv_folds = cv_config['cv_folds']
        
        # Build comprehensive configuration using existing PRESET_CONFIGS
        final_config = base_config.copy()
        
        # If no base_config was provided, use default preset as foundation
        if not base_config:
            if 'default' in PRESET_CONFIGS:
                final_config = deepcopy(PRESET_CONFIGS['default'])
                print("Using default preset as foundation for express HPO")
            else:
                logger.warning("No default preset available, creating minimal config")
                final_config = {}
        
        # HPO configuration - merge with existing preset HPO settings
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        
        # Update with express settings while preserving preset structure
        hpo_config.update({
            'enabled': True,
            'strategy': hpo_config.get('strategy', 'optuna'),
            'n_trials': n_trials,
            'timeout': timeout_minutes * 60,
            'timeout_seconds': timeout_minutes * 60,
            'study_name': f"express_hpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'direction': hpo_config.get('direction', 'minimize'),
            'sampler': hpo_config.get('sampler', 'TPESampler'),
            'pruner': hpo_config.get('pruner', 'MedianPruner'),
            'objective_metric': hpo_config.get('objective_metric', 'validation_loss'),
            
            # Express-specific settings
            'trial_epochs': trial_epochs,
            'trial_patience': max(5, trial_epochs // 4),
            'early_stopping': {
                'enabled': True,
                'patience': max(10, n_trials // 10),
                'min_improvement': 1e-4
            },
            'cleanup_trials': True,
            'generate_plots': True,
            
            # Model search configuration
            'model_search': {
                'enabled': len(model_types) > 1,
                'model_types': model_types,
                'search_all_models': len(model_types) >= 3
            },
            
            # Cross-validation settings
            'cross_validation': {
                'enabled': cv_folds > 1,
                'folds': cv_folds,
                'shuffle': True,
                'random_state': 42
            },
            
            # Preserve existing optimization space or use defaults
            'optimization_space': hpo_config.get('optimization_space', {
                'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 1e-2, 'log': True},
                'batch_size': {'type': 'categorical', 'choices': [32, 64, 128]},
                'encoding_dim': {'type': 'int', 'low': 8, 'high': 24},
                'dropout_rate': {'type': 'float', 'low': 0.1, 'high': 0.4}
            }),
            
            # Storage settings
            'storage': {
                'enabled': True,
                'url': f"sqlite:///{DEFAULT_MODEL_DIR}/hpo_studies/express_study_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db",
                'load_if_exists': False,
                'heartbeat_interval': 60,
                'grace_period': 120
            }
        })
        
        # Data configuration - merge with existing preset data settings
        data_config = final_config.setdefault('data', {})
        data_config.update({
            'use_real_data': use_real_data,
            'normal_samples': data_config.get('normal_samples', 8000) if not use_real_data else None,
            'attack_samples': data_config.get('attack_samples', 2000) if not use_real_data else None,
            'features': data_config.get('features', 20) if not use_real_data else None,
            'validation_split': data_config.get('validation_split', 0.2),
            'test_split': data_config.get('test_split', 0.2),
            'normalization': data_config.get('normalization', 'standard'),
            'random_state': data_config.get('random_state', 42)
        })
        
        # Training configuration adjustments for HPO
        training_config = final_config.setdefault('training', {})
        training_config.update({
            'num_workers': 0,  # Safer for HPO
            'pin_memory': False,  # Disabled for HPO stability
            'persistent_workers': False
        })
        
        # System configuration
        system_config = final_config.setdefault('system', {})
        system_config.update({
            'non_interactive': True,  # Express mode is non-interactive for trials
            'verbose': True,
            'random_seed': 42,
            'reproducible': True
        })
        
        # Update metadata to reflect express setup
        metadata = final_config.setdefault('metadata', {})
        metadata.update({
            'last_modified': datetime.now().isoformat(),
            'setup_method': 'express_hpo',
            'express_configuration': {
                'goal': ['quick', 'balanced', 'thorough'][int(goal_choice)-1],
                'data_source': 'real' if use_real_data else 'synthetic',
                'model_types': model_types,
                'validation_strategy': f"{cv_folds}_fold_cv" if cv_folds > 1 else "single_split",
                'estimated_duration_minutes': timeout_minutes
            }
        })
        
        # Display configuration summary
        print("\nEXPRESS HPO CONFIGURATION SUMMARY")
        print("-" * 50)
        print(f"Data Source: {'Real Data' if use_real_data else 'Synthetic Data'}")
        print(f"Optimization Goal: {['Quick', 'Balanced', 'Thorough'][int(goal_choice)-1]}")
        print(f"Trials: {n_trials}")
        print(f"Timeout: {timeout_minutes} minutes")
        print(f"Trial Epochs: {trial_epochs}")
        print(f"Model Types: {', '.join(model_types)}")
        print(f"Cross-Validation: {cv_folds} folds" if cv_folds > 1 else "Single split validation")
        print(f"Sampler: {hpo_config.get('sampler', 'TPESampler')}")
        print(f"Pruner: {hpo_config.get('pruner', 'MedianPruner')}")
        print(f"Expected Duration: ~{_estimate_hpo_time(n_trials, trial_epochs, len(model_types), cv_folds) if '_estimate_hpo_time' in globals() else 'Unknown'}")
        
        # Confirm configuration
        confirm = input("\nStart hyperparameter optimization with these settings? (Y/n/c to cancel): ").strip().lower()
        if confirm in ('', 'y', 'yes'):
            print("\nLaunching hyperparameter optimization...")
            return _launch_hpo_with_config(final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print("HPO cancelled")
            return None
        else:
            print("Would you like to:")
            print("1. Try again with different settings")
            print("0. Return to previous menu")
            
            while True:
                retry_choice = input("Select option (0-1): ").strip()
                if retry_choice in ['1', '0']:
                    break
                print("Invalid choice. Please select 0-1.")
            
            if retry_choice == '1':
                return _interactive_hpo_express_setup(base_config, use_real_data, **kwargs)
            else:
                print("Returning to previous menu")
                return None
                
    except KeyboardInterrupt:
        print("\nExpress HPO setup interrupted")
        return None
    except Exception as e:
        logger.error(f"Express HPO setup failed: {e}")
        print(f"Express HPO setup failed: {str(e)}")
        return None

def _interactive_hpo_preset_setup(
    base_config: Dict[str, Any],
    use_real_data: Optional[bool], 
    **kwargs
) -> Optional[Dict[str, Any]]:
    """HPO preset selection using existing PRESET_CONFIGS with their hyperparameter_optimization sections."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        show_banner()
        
        print("Select a preset configuration optimized for hyperparameter optimization...")
        
        # Get available presets and their HPO configurations from PRESET_CONFIGS
        available_presets = []
        for preset_name, preset_config in PRESET_CONFIGS.items():
            hpo_config = preset_config.get('hyperparameter_optimization', {})
            metadata = preset_config.get('metadata', {})
            
            # Include all presets, whether HPO is enabled by default or not
            preset_info = {
                'name': preset_name,
                'description': metadata.get('description', f'{preset_name.title()} preset'),
                'enabled': hpo_config.get('enabled', False),
                'n_trials': hpo_config.get('n_trials', 50),
                'timeout_minutes': hpo_config.get('timeout', 3600) // 60 if hpo_config.get('timeout') else 60,
                'sampler': hpo_config.get('sampler', 'TPESampler'),
                'pruner': hpo_config.get('pruner', 'MedianPruner'),
                'recommended_hardware': metadata.get('recommended_hardware', {}),
                'compatibility': metadata.get('compatibility', []),
                'preset_data': preset_config
            }
            
            available_presets.append(preset_info)
        
        if not available_presets:
            print("No presets available! Using express setup instead.")
            return _interactive_hpo_express_setup(base_config, use_real_data, **kwargs)
        
        # Display preset options with enhanced information
        print("\nAvailable HPO Presets:")
        print("-" * 75)
        
        for i, preset in enumerate(available_presets, 1):
            print(f"{i}. {preset['name'].upper()}")
            print(f"   Description: {preset['description']}")
            print(f"   HPO Default: {'Enabled' if preset['enabled'] else 'Disabled (can be enabled)'}")
            print(f"   Trials: {preset['n_trials']}")
            print(f"   Timeout: {preset['timeout_minutes']} minutes")
            print(f"   Sampler: {preset['sampler']}")
            print(f"   Pruner: {preset['pruner']}")
            
            # Hardware requirements
            hw_req = preset['recommended_hardware']
            if hw_req:
                print(f"   Hardware: {hw_req.get('cpu_cores', 'N/A')} cores, "
                      f"{hw_req.get('ram_gb', 'N/A')}GB RAM, "
                      f"{hw_req.get('gpu_memory_gb', 'N/A')}GB GPU")
            
            compatible_models = ', '.join(preset['compatibility']) if preset['compatibility'] else 'All models'
            print(f"   Compatible Models: {compatible_models}")
            print()
        
        print("0. Cancel and return to previous menu")
        
        # Get user selection
        while True:
            try:
                choice = int(input("Select preset (0-{}): ".format(len(available_presets))))
                if 0 <= choice <= len(available_presets):
                    break
                print(f"Invalid choice. Please select 0-{len(available_presets)}.")
            except ValueError:
                print("Please enter a valid number.")
        
        if choice == 0:
            print("Preset selection cancelled")
            return None
        
        selected_preset = available_presets[choice - 1]
        preset_name = selected_preset['name']
        
        print(f"\nSelected preset: {preset_name.upper()}")
        
        # Load the full preset configuration from PRESET_CONFIGS
        final_config = deepcopy(PRESET_CONFIGS[preset_name])
        
        # Deep merge with base_config if provided
        if base_config:
            final_config = deep_update(final_config, base_config)
        
        # Data source selection if not specified
        if use_real_data is None:
            print("\nDATA SOURCE")
            print("1. Real network data")
            print("2. Synthetic data")
            print("0. Cancel")
            
            while True:
                data_choice = input("Select data source (0-2): ").strip()
                if data_choice in ['1', '2', '0']:
                    break
                print("Invalid choice. Please select 0-2.")
            
            if data_choice == '0':
                return None
            
            use_real_data = data_choice == '1'
        
        # Update data configuration
        data_config = final_config.setdefault('data', {})
        data_config['use_real_data'] = use_real_data
        
        # Ensure HPO is enabled and configure it properly
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        hpo_config['enabled'] = True
        
        # Update study name to include preset and timestamp
        original_study_name = hpo_config.get('study_name', 'autoencoder_hpo')
        hpo_config['study_name'] = f"{preset_name}_{original_study_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Ensure timeout is in both formats for compatibility
        if 'timeout' in hpo_config and 'timeout_seconds' not in hpo_config:
            hpo_config['timeout_seconds'] = hpo_config['timeout']
        
        # Update storage configuration with preset-specific path
        if 'storage' in hpo_config:
            storage_config = hpo_config['storage']
            if 'url' in storage_config:
                # Update storage URL to include preset name
                original_url = storage_config['url']
                storage_config['url'] = original_url.replace('study.db', f'{preset_name}_study.db')
        
        # Update metadata to reflect preset usage and HPO setup
        metadata = final_config.setdefault('metadata', {})
        metadata.update({
            'last_modified': datetime.now().isoformat(),
            'setup_method': 'preset_hpo',
            'hpo_preset_used': preset_name,
            'hpo_enabled_at': datetime.now().isoformat(),
            'original_hpo_enabled': selected_preset['enabled']
        })
        
        # Display final configuration summary
        print(f"\nHPO PRESET CONFIGURATION SUMMARY")
        print("-" * 55)
        print(f"Preset: {preset_name.upper()}")
        print(f"Description: {selected_preset['description']}")
        print(f"Data Source: {'Real Data' if use_real_data else 'Synthetic Data'}")
        print(f"HPO Enabled: {'Yes (default)' if selected_preset['enabled'] else 'Yes (enabled for this session)'}")
        print(f"Trials: {hpo_config.get('n_trials', 50)}")
        print(f"Timeout: {hpo_config.get('timeout', 3600) // 60 if hpo_config.get('timeout') else 60} minutes")
        print(f"Sampler: {hpo_config.get('sampler', 'TPESampler')}")
        print(f"Pruner: {hpo_config.get('pruner', 'MedianPruner')}")
        print(f"Compatible Models: {', '.join(final_config.get('metadata', {}).get('compatibility', ['Unknown']))}")
        print(f"Study Name: {hpo_config['study_name']}")
        
        # Show hardware recommendations
        hw_req = selected_preset['recommended_hardware']
        if hw_req:
            print(f"Recommended Hardware:")
            print(f"  - CPU Cores: {hw_req.get('cpu_cores', 'N/A')}")
            print(f"  - RAM: {hw_req.get('ram_gb', 'N/A')}GB")
            print(f"  - GPU Memory: {hw_req.get('gpu_memory_gb', 'N/A')}GB")
        
        # Optional customization
        print(f"\nCustomization Options:")
        print("1. Use preset as-is (recommended)")
        print("2. Adjust trial count")
        print("3. Adjust timeout")
        print("4. Modify sampler/pruner")
        print("5. Advanced settings")
        print("0. Cancel")
        
        while True:
            custom_choice = input("Select option (0-5): ").strip()
            if custom_choice in ['1', '2', '3', '4', '5', '0']:
                break
            print("Invalid choice. Please select 0-5.")
        
        if custom_choice == '0':
            return None
        elif custom_choice == '2':
            while True:
                try:
                    new_trials = int(input(f"Enter number of trials (current: {hpo_config.get('n_trials', 50)}): "))
                    if new_trials > 0:
                        hpo_config['n_trials'] = new_trials
                        print(f"Updated trials to {new_trials}")
                        break
                    print("Please enter a positive number.")
                except ValueError:
                    print("Please enter a valid number.")
        elif custom_choice == '3':
            while True:
                try:
                    new_timeout_minutes = int(input(f"Enter timeout in minutes (current: {hpo_config.get('timeout', 3600) // 60 if hpo_config.get('timeout') else 60}): "))
                    if new_timeout_minutes > 0:
                        hpo_config['timeout'] = new_timeout_minutes * 60
                        hpo_config['timeout_seconds'] = new_timeout_minutes * 60
                        print(f"Updated timeout to {new_timeout_minutes} minutes")
                        break
                    print("Please enter a positive number.")
                except ValueError:
                    print("Please enter a valid number.")
        elif custom_choice == '4':
            # Sampler selection
            print("\nAvailable Samplers:")
            samplers = ['TPESampler', 'RandomSampler', 'CmaEsSampler']
            for i, sampler in enumerate(samplers, 1):
                print(f"{i}. {sampler}")
            
            sampler_choice = input(f"Select sampler (1-{len(samplers)}, Enter for current): ").strip()
            if sampler_choice and sampler_choice.isdigit() and 1 <= int(sampler_choice) <= len(samplers):
                hpo_config['sampler'] = samplers[int(sampler_choice) - 1]
                print(f"Updated sampler to {hpo_config['sampler']}")
            
            # Pruner selection
            print("\nAvailable Pruners:")
            pruners = ['MedianPruner', 'HyperbandPruner', 'NopPruner']
            for i, pruner in enumerate(pruners, 1):
                print(f"{i}. {pruner}")
            
            pruner_choice = input(f"Select pruner (1-{len(pruners)}, Enter for current): ").strip()
            if pruner_choice and pruner_choice.isdigit() and 1 <= int(pruner_choice) <= len(pruners):
                hpo_config['pruner'] = pruners[int(pruner_choice) - 1]
                print(f"Updated pruner to {hpo_config['pruner']}")
        elif custom_choice == '5':
            print("\nSwitching to advanced custom setup...")
            return _interactive_hpo_custom_setup(final_config, use_real_data, **kwargs)
        
        # Final confirmation
        confirm = input("\nLaunch hyperparameter optimization with this preset? (Y/n): ").strip().lower()
        if confirm in ('', 'y', 'yes'):
            print(f"\nLaunching HPO with {preset_name} preset...")
            
            # Ensure system configuration is HPO-friendly
            system_config = final_config.setdefault('system', {})
            system_config.update({
                'non_interactive': True,
                'verbose': True
            })
            
            # Ensure training configuration is optimized for HPO
            training_config = final_config.setdefault('training', {})
            training_config.update({
                'num_workers': 0,  # Safer for HPO
                'pin_memory': False,  # Disabled for HPO stability
                'persistent_workers': False
            })
            
            return _launch_hpo_with_config(final_config, **kwargs)
        else:
            print("HPO launch cancelled")
            return None
            
    except KeyboardInterrupt:
        print("\nHPO preset setup interrupted")
        return None
    except Exception as e:
        logger.error(f"HPO preset setup failed: {e}")
        print(f"HPO preset setup failed: {str(e)}")
        return None

def _interactive_hpo_custom_setup(
    base_config: Dict[str, Any],
    use_real_data: Optional[bool],
    **kwargs
) -> Optional[Dict[str, Any]]:
    """Full custom HPO configuration with all available options and preset integration."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        show_banner()
        
        print("Custom hyperparameter optimization configuration")
        print("Press Enter for defaults shown in parentheses")
        print("Enter 'c' at any time to cancel and return to previous menu")
        
        # Deep copy base config to avoid mutations
        final_config = deepcopy(base_config) if base_config else {}
        
        # If no base config, use default preset as foundation like other functions
        if not final_config:
            if 'default' in PRESET_CONFIGS:
                final_config = deepcopy(PRESET_CONFIGS['default'])
                print("Using default preset as foundation for custom HPO")
            else:
                logger.warning("No default preset available for custom HPO")
                final_config = {}
        
        print("\n" + "="*60)
        print("OPTIMIZATION STRATEGY")
        print("="*60)
        
        # Study configuration
        study_name = input("Study name (auto-generated): ").strip()
        if study_name.lower() == 'c':
            print("Study configuration cancelled")
            return None
        if not study_name:
            study_name = f"custom_hpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print("\nOptimization direction:")
        print("1. Minimize (lower values are better)")
        print("2. Maximize (higher values are better)")
        print("0. Cancel and return to previous menu")
        
        while True:
            direction_choice = input("Select direction (0-2, default=1): ").strip()
            if direction_choice in ['1', '2', '0', '']:
                break
            print("Invalid choice. Please select 0-2.")
        
        if direction_choice == '0':
            print("Direction selection cancelled")
            return None
        
        direction = 'minimize' if direction_choice in ['1', ''] else 'maximize'
        
        n_trials = input("Number of trials (100): ").strip()
        if n_trials.lower() == 'c':
            print("Trial configuration cancelled")
            return None
        n_trials = int(n_trials) if n_trials else 100
        
        timeout_minutes = input("Timeout in minutes (0 for no timeout): ").strip()
        if timeout_minutes.lower() == 'c':
            print("Timeout configuration cancelled")
            return None
        timeout_minutes = int(timeout_minutes) if timeout_minutes else 0
        
        print("\n" + "="*60)
        print("SAMPLING AND PRUNING")
        print("="*60)
        
        print("\nSampling strategy:")
        samplers = ['TPESampler', 'RandomSampler', 'CmaEsSampler', 'GridSampler']
        for i, sampler in enumerate(samplers, 1):
            descriptions = {
                'TPESampler': 'Tree-structured Parzen Estimator (recommended)',
                'RandomSampler': 'Random sampling (baseline)',
                'CmaEsSampler': 'Covariance Matrix Adaptation',
                'GridSampler': 'Grid search (systematic)'
            }
            print(f"{i}. {sampler} - {descriptions[sampler]}")
        print("0. Cancel and return to previous menu")
        
        while True:
            sampler_choice = input(f"Select sampler (0-{len(samplers)}, default=1): ").strip()
            if sampler_choice in [str(i) for i in range(len(samplers)+1)] or sampler_choice == '':
                break
            print(f"Invalid choice. Please select 0-{len(samplers)}.")
        
        if sampler_choice == '0':
            print("Sampler selection cancelled")
            return None
        
        sampler_idx = int(sampler_choice) - 1 if sampler_choice else 0
        sampler_type = samplers[sampler_idx]
        
        print("\nPruning strategy:")
        pruners = ['MedianPruner', 'HyperbandPruner', 'NopPruner']
        for i, pruner in enumerate(pruners, 1):
            descriptions = {
                'MedianPruner': 'Prune trials below median performance',
                'HyperbandPruner': 'Successive halving algorithm',
                'NopPruner': 'No pruning (complete all trials)'
            }
            print(f"{i}. {pruner} - {descriptions[pruner]}")
        print("0. Cancel and return to previous menu")
        
        while True:
            pruner_choice = input(f"Select pruner (0-{len(pruners)}, default=1): ").strip()
            if pruner_choice in [str(i) for i in range(len(pruners)+1)] or pruner_choice == '':
                break
            print(f"Invalid choice. Please select 0-{len(pruners)}.")
        
        if pruner_choice == '0':
            print("Pruner selection cancelled")
            return None
        
        pruner_idx = int(pruner_choice) - 1 if pruner_choice else 0
        pruner_type = pruners[pruner_idx]
        
        print("\n" + "="*60)
        print("MODEL SELECTION")
        print("="*60)
        
        print("\nModel types to optimize:")
        available_models = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
        selected_models = []
        
        for i, model in enumerate(available_models, 1):
            descriptions = {
                'SimpleAutoencoder': 'Fast, lightweight model',
                'EnhancedAutoencoder': 'Advanced features, good balance',
                'AutoencoderEnsemble': 'Best performance, more complex'
            }
            include = input(f"{i}. Include {model}? ({descriptions[model]}) (Y/n): ").strip().lower()
            if include == 'c':
                print("Model selection cancelled")
                return None
            if include in ('', 'y', 'yes'):
                selected_models.append(model)
        
        if not selected_models:
            print("No models selected, using EnhancedAutoencoder")
            selected_models = ['EnhancedAutoencoder']
        
        search_all_models = len(selected_models) >= 3
        
        print("\n" + "="*60)
        print("DATA CONFIGURATION")
        print("="*60)
        
        if use_real_data is None:
            print("\nData source:")
            print("1. Real network data")
            print("2. Synthetic data")
            print("0. Cancel and return to previous menu")
            
            while True:
                data_choice = input("Select data source (0-2): ").strip()
                if data_choice in ['1', '2', '0']:
                    break
                print("Invalid choice. Please select 0-2.")
            
            if data_choice == '0':
                print("Data selection cancelled")
                return None
                
            use_real_data = data_choice == '1'
        
        data_path = None
        artifacts_path = None
        normal_samples = 10000
        attack_samples = 2000
        features = 78
        
        if use_real_data:
            print("\nReal data configuration:")
            data_path = input("Data file path (optional): ").strip()
            if data_path.lower() == 'c':
                print("Data configuration cancelled")
                return None
            if not data_path:
                data_path = None
            
            artifacts_path = input("Artifacts path (optional): ").strip()
            if artifacts_path.lower() == 'c':
                print("Data configuration cancelled")
                return None
            if not artifacts_path:
                artifacts_path = None
        else:
            print("\nSynthetic data configuration:")
            normal_input = input("Normal samples (10000): ").strip()
            if normal_input.lower() == 'c':
                print("Data configuration cancelled")
                return None
            normal_samples = int(normal_input) if normal_input else 10000
            
            attack_input = input("Attack samples (2000): ").strip()
            if attack_input.lower() == 'c':
                print("Data configuration cancelled")
                return None
            attack_samples = int(attack_input) if attack_input else 2000
            
            features_input = input("Number of features (78): ").strip()
            if features_input.lower() == 'c':
                print("Data configuration cancelled")
                return None
            features = int(features_input) if features_input else 78
        
        print("\n" + "="*60)
        print("VALIDATION STRATEGY")
        print("="*60)
        
        cv_folds = input("Cross-validation folds (3): ").strip()
        if cv_folds.lower() == 'c':
            print("Validation configuration cancelled")
            return None
        cv_folds = int(cv_folds) if cv_folds else 3
        
        cv_shuffle = input("Shuffle CV splits? (Y/n): ").strip().lower()
        if cv_shuffle == 'c':
            print("Validation configuration cancelled")
            return None
        cv_shuffle = cv_shuffle in ('', 'y', 'yes')
        
        cv_random_state = input("CV random state (42): ").strip()
        if cv_random_state.lower() == 'c':
            print("Validation configuration cancelled")
            return None
        cv_random_state = int(cv_random_state) if cv_random_state else 42
        
        print("\n" + "="*60)
        print("TRIAL CONFIGURATION")
        print("="*60)
        
        trial_epochs = input("Epochs per trial (20): ").strip()
        if trial_epochs.lower() == 'c':
            print("Trial configuration cancelled")
            return None
        trial_epochs = int(trial_epochs) if trial_epochs else 20
        
        trial_patience = input("Early stopping patience per trial (5): ").strip()
        if trial_patience.lower() == 'c':
            print("Trial configuration cancelled")
            return None
        trial_patience = int(trial_patience) if trial_patience else 5
        
        trial_batch_size = input("Batch size for trials (64): ").strip()
        if trial_batch_size.lower() == 'c':
            print("Trial configuration cancelled")
            return None
        trial_batch_size = int(trial_batch_size) if trial_batch_size else 64
        
        print("\n" + "="*60)
        print("SEARCH SPACE CONFIGURATION")
        print("="*60)
        
        print("\nDefine hyperparameter search ranges:")
        
        # Learning rate range
        lr_min_input = input("Learning rate minimum (1e-5): ").strip()
        if lr_min_input.lower() == 'c':
            print("Search space configuration cancelled")
            return None
        lr_min = float(lr_min_input) if lr_min_input else 1e-5
        
        lr_max_input = input("Learning rate maximum (1e-2): ").strip()
        if lr_max_input.lower() == 'c':
            print("Search space configuration cancelled")
            return None
        lr_max = float(lr_max_input) if lr_max_input else 1e-2
        
        # Batch size options
        batch_sizes_input = input("Batch sizes (comma-separated, e.g., 32,64,128): ").strip()
        if batch_sizes_input.lower() == 'c':
            print("Search space configuration cancelled")
            return None
        if batch_sizes_input:
            batch_sizes = [int(x.strip()) for x in batch_sizes_input.split(',')]
        else:
            batch_sizes = [32, 64, 128]
        
        # Encoding dimension range
        encoding_dim_min_input = input("Encoding dimension minimum (4): ").strip()
        if encoding_dim_min_input.lower() == 'c':
            print("Search space configuration cancelled")
            return None
        encoding_dim_min = int(encoding_dim_min_input) if encoding_dim_min_input else 4
        
        encoding_dim_max_input = input("Encoding dimension maximum (32): ").strip()
        if encoding_dim_max_input.lower() == 'c':
            print("Search space configuration cancelled")
            return None
        encoding_dim_max = int(encoding_dim_max_input) if encoding_dim_max_input else 32
        
        # Dropout rate range
        dropout_min_input = input("Dropout rate minimum (0.0): ").strip()
        if dropout_min_input.lower() == 'c':
            print("Search space configuration cancelled")
            return None
        dropout_min = float(dropout_min_input) if dropout_min_input else 0.0
        
        dropout_max_input = input("Dropout rate maximum (0.5): ").strip()
        if dropout_max_input.lower() == 'c':
            print("Search space configuration cancelled")
            return None
        dropout_max = float(dropout_max_input) if dropout_max_input else 0.5
        
        # Weight decay range
        weight_decay_min_input = input("Weight decay minimum (1e-6): ").strip()
        if weight_decay_min_input.lower() == 'c':
            print("Search space configuration cancelled")
            return None
        weight_decay_min = float(weight_decay_min_input) if weight_decay_min_input else 1e-6
        
        weight_decay_max_input = input("Weight decay maximum (1e-2): ").strip()
        if weight_decay_max_input.lower() == 'c':
            print("Search space configuration cancelled")
            return None
        weight_decay_max = float(weight_decay_max_input) if weight_decay_max_input else 1e-2
        
        print("\n" + "="*60)
        print("SYSTEM CONFIGURATION")
        print("="*60)
        
        device = input("Device (auto/cpu/cuda): ").strip()
        if device.lower() == 'c':
            print("System configuration cancelled")
            return None
        device = device if device else 'auto'
        
        random_seed = input("Random seed (42): ").strip()
        if random_seed.lower() == 'c':
            print("System configuration cancelled")
            return None
        random_seed = int(random_seed) if random_seed else 42
        
        num_workers = input("Number of workers (0 recommended for HPO): ").strip()
        if num_workers.lower() == 'c':
            print("System configuration cancelled")
            return None
        num_workers = int(num_workers) if num_workers else 0
        
        print("\n" + "="*60)
        print("EARLY STOPPING AND PRUNING")
        print("="*60)
        
        early_stopping_patience = input("HPO early stopping patience (10): ").strip()
        if early_stopping_patience.lower() == 'c':
            print("Early stopping configuration cancelled")
            return None
        early_stopping_patience = int(early_stopping_patience) if early_stopping_patience else 10
        
        early_stopping_min_trials = input("Minimum trials before early stopping (20): ").strip()
        if early_stopping_min_trials.lower() == 'c':
            print("Early stopping configuration cancelled")
            return None
        early_stopping_min_trials = int(early_stopping_min_trials) if early_stopping_min_trials else 20
        
        print("\n" + "="*60)
        print("OUTPUT AND STORAGE")
        print("="*60)
        
        verbose = input("Verbose output? (Y/n): ").strip().lower()
        if verbose == 'c':
            print("Output configuration cancelled")
            return None
        verbose = verbose in ('', 'y', 'yes')
        
        save_study = input("Save study results? (Y/n): ").strip().lower()
        if save_study == 'c':
            print("Storage configuration cancelled")
            return None
        save_study = save_study in ('', 'y', 'yes')
        
        generate_plots = input("Generate optimization plots? (Y/n): ").strip().lower()
        if generate_plots == 'c':
            print("Plotting configuration cancelled")
            return None
        generate_plots = generate_plots in ('', 'y', 'yes')
        
        study_dir = input(f"Study directory ({DEFAULT_MODEL_DIR / 'hpo_studies'}): ").strip()
        if study_dir.lower() == 'c':
            print("Directory configuration cancelled")
            return None
        study_dir = Path(study_dir) if study_dir else DEFAULT_MODEL_DIR / "hpo_studies"
        
        print("\n" + "="*60)
        print("ADVANCED OPTIONS")
        print("="*60)
        
        storage_url = input("Database storage URL (optional): ").strip()
        if storage_url.lower() == 'c':
            print("Storage configuration cancelled")
            return None
        if not storage_url:
            storage_url = None
        
        load_if_exists = input("Load existing study if found? (y/N): ").strip().lower()
        if load_if_exists == 'c':
            print("Study loading configuration cancelled")
            return None
        load_if_exists = load_if_exists in ('y', 'yes')
        
        parallel_jobs = input("Parallel jobs (1): ").strip()
        if parallel_jobs.lower() == 'c':
            print("Parallel configuration cancelled")
            return None
        parallel_jobs = int(parallel_jobs) if parallel_jobs else 1
        
        memory_limit = input("Memory limit (optional, e.g., '8GB'): ").strip()
        if memory_limit.lower() == 'c':
            print("Memory configuration cancelled")
            return None
        if not memory_limit:
            memory_limit = None
        
        # Build comprehensive HPO configuration using preset structure
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        
        # Update with custom settings while preserving preset structure
        hpo_config.update({
            'enabled': True,
            'strategy': hpo_config.get('strategy', 'optuna'),
            'n_trials': n_trials,
            'timeout': timeout_minutes * 60 if timeout_minutes > 0 else 0,
            'timeout_seconds': timeout_minutes * 60 if timeout_minutes > 0 else 0,
            'study_name': study_name,
            'direction': direction,
            'sampler': sampler_type,
            'pruner': pruner_type,
            'objective_metric': hpo_config.get('objective_metric', 'validation_loss'),
            
            # Custom trial configuration
            'trial_epochs': trial_epochs,
            'trial_patience': trial_patience,
            'trial_batch_size': trial_batch_size,
            
            # Model search configuration
            'model_search': {
                'enabled': len(selected_models) > 1,
                'model_types': selected_models,
                'search_all_models': search_all_models
            },
            
            # Cross-validation settings
            'cross_validation': {
                'enabled': cv_folds > 1,
                'folds': cv_folds,
                'shuffle': cv_shuffle,
                'random_state': cv_random_state
            },
            
            # Comprehensive optimization space
            'optimization_space': {
                'learning_rate': {'type': 'float', 'low': lr_min, 'high': lr_max, 'log': True},
                'batch_size': {'type': 'categorical', 'choices': batch_sizes},
                'encoding_dim': {'type': 'int', 'low': encoding_dim_min, 'high': encoding_dim_max},
                'dropout_rate': {'type': 'float', 'low': dropout_min, 'high': dropout_max},
                'weight_decay': {'type': 'float', 'low': weight_decay_min, 'high': weight_decay_max, 'log': True}
            },
            
            # Early stopping configuration
            'early_stopping': {
                'enabled': True,
                'patience': early_stopping_patience,
                'min_improvement': 1e-4
            },
            
            # Advanced settings
            'cleanup_trials': True,
            'generate_plots': generate_plots,
            
            # Detailed search space for compatibility
            'search_space': {
                'encoding_dim_min': encoding_dim_min,
                'encoding_dim_max': encoding_dim_max,
                'hidden_layers_min': 1,
                'hidden_layers_max': 3,
                'lr_min': lr_min,
                'lr_max': lr_max,
                'batch_sizes': batch_sizes,
                'weight_decay_min': weight_decay_min,
                'weight_decay_max': weight_decay_max,
                'dropout_min': dropout_min,
                'dropout_max': dropout_max,
                'activations': ["relu", "leaky_relu", "gelu", "tanh"],
                'normalizations': [None, "batch", "layer"],
                'percentile_min': 90,
                'percentile_max': 99
            },
            
            # Sampler configuration
            'sampler_config': {
                'type': sampler_type,
                'seed': random_seed,
                'consider_prior': sampler_type == 'TPESampler',
                'prior_weight': 1.0,
                'consider_magic_clip': sampler_type == 'TPESampler',
                'consider_endpoints': False,
                'n_startup_trials': 10 if sampler_type == 'TPESampler' else 5,
                'n_ei_candidates': 24 if sampler_type == 'TPESampler' else 10,
                'multivariate': sampler_type == 'TPESampler'
            },
            
            # Pruner configuration
            'pruner_config': {
                'type': pruner_type,
                'n_startup_trials': 5 if pruner_type != 'NopPruner' else 0,
                'n_warmup_steps': 10 if pruner_type == 'MedianPruner' else 5,
                'interval_steps': 1,
                'min_resource': 1 if pruner_type == 'HyperbandPruner' else None,
                'max_resource': 'auto' if pruner_type == 'HyperbandPruner' else None,
                'reduction_factor': 3 if pruner_type == 'HyperbandPruner' else None
            },
            
            # Scoring configuration
            'scoring': {
                'use_composite_score': False,
                'validation_weight': 0.7,
                'test_weight': 0.2,
                'complexity_weight': 0.1,
                'max_params_penalty': 100000
            },
            
            # Storage configuration
            'storage': {
                'enabled': save_study,
                'url': storage_url or f"sqlite:///{study_dir}/custom_study_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db",
                'load_if_exists': load_if_exists,
                'heartbeat_interval': 60,
                'grace_period': 120
            }
        })
        
        # Data configuration - merge with existing preset data settings
        data_config = final_config.setdefault('data', {})
        data_config.update({
            'use_real_data': use_real_data,
            'data_path': data_path,
            'artifacts_path': artifacts_path,
            'normal_samples': normal_samples if not use_real_data else data_config.get('normal_samples'),
            'attack_samples': attack_samples if not use_real_data else data_config.get('attack_samples'),
            'features': features if not use_real_data else data_config.get('features'),
            'validation_split': data_config.get('validation_split', 0.2),
            'test_split': data_config.get('test_split', 0.2),
            'normalization': data_config.get('normalization', 'standard'),
            'random_state': cv_random_state
        })
        
        # Training configuration adjustments for HPO
        training_config = final_config.setdefault('training', {})
        training_config.update({
            'num_workers': num_workers,
            'pin_memory': False,  # Disabled for HPO stability
            'persistent_workers': False
        })
        
        # System configuration
        system_config = final_config.setdefault('system', {})
        system_config.update({
            'device': device,
            'random_seed': random_seed,
            'reproducible': True,
            'non_interactive': True,
            'verbose': verbose,
            'parallel_processing': parallel_jobs > 1,
            'max_workers': parallel_jobs
        })
        
        # Update metadata to reflect custom setup
        metadata = final_config.setdefault('metadata', {})
        metadata.update({
            'last_modified': datetime.now().isoformat(),
            'setup_method': 'custom_hpo',
            'custom_configuration': {
                'optimization_goal': 'custom',
                'data_source': 'real' if use_real_data else 'synthetic',
                'model_types': selected_models,
                'validation_strategy': f"{cv_folds}_fold_cv" if cv_folds > 1 else "single_split",
                'sampler': sampler_type,
                'pruner': pruner_type,
                'search_space_customized': True,
                'estimated_duration_minutes': timeout_minutes if timeout_minutes > 0 else 'unlimited'
            }
        })
        
        # Display comprehensive configuration review
        print("\n" + "="*70)
        print("CUSTOM HPO CONFIGURATION REVIEW")
        print("="*70)
        
        print(f"Study: {study_name}")
        print(f"Optimization: {direction} over {n_trials} trials")
        if timeout_minutes > 0:
            print(f"Timeout: {timeout_minutes} minutes")
        else:
            print("Timeout: No limit")
        
        print(f"Sampling: {sampler_type}")
        print(f"Pruning: {pruner_type}")
        
        print(f"Models: {', '.join(selected_models)}")
        print(f"Data: {'Real Data' if use_real_data else 'Synthetic Data'}")
        if not use_real_data:
            print(f"   Samples: {normal_samples} normal, {attack_samples} attack")
            print(f"   Features: {features}")
        
        print(f"Validation: {cv_folds} folds cross-validation")
        print(f"Trial Configuration: {trial_epochs} epochs, patience {trial_patience}")
        print(f"System: Device {device}, {num_workers} workers")
        
        print(f"Search Space:")
        print(f"   Learning Rate: {lr_min:.2e} to {lr_max:.2e}")
        print(f"   Batch Sizes: {batch_sizes}")
        print(f"   Encoding Dim: {encoding_dim_min} to {encoding_dim_max}")
        print(f"   Dropout: {dropout_min} to {dropout_max}")
        print(f"   Weight Decay: {weight_decay_min:.2e} to {weight_decay_max:.2e}")
        
        estimated_time = _estimate_hpo_time(n_trials, trial_epochs, len(selected_models), cv_folds)
        print(f"Estimated Duration: ~{estimated_time}")
        
        if storage_url:
            print(f"Database Storage: {storage_url}")
        if load_if_exists:
            print("Will load existing study if found")
        if parallel_jobs > 1:
            print(f"Parallel Jobs: {parallel_jobs}")
        if memory_limit:
            print(f"Memory Limit: {memory_limit}")
        
        print("="*70)
        
        confirm = input("\nStart hyperparameter optimization with this configuration? (Y/n/c to cancel): ").strip().lower()
        if confirm in ('', 'y', 'yes'):
            print("\nLaunching comprehensive hyperparameter optimization...")
            
            # Ensure HPO-friendly system settings
            system_config.update({
                'non_interactive': True,
                'verbose': verbose
            })
            
            return _launch_hpo_with_config(final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print("HPO cancelled")
            return None
        else:
            print("Would you like to:")
            print("1. Try again with different settings")
            print("2. Switch to preset configuration")
            print("3. Switch to express setup")
            print("0. Return to previous menu")
            
            while True:
                retry_choice = input("Select option (0-3): ").strip()
                if retry_choice in ['1', '2', '3', '0']:
                    break
                print("Invalid choice. Please select 0-3.")
            
            if retry_choice == '1':
                return _interactive_hpo_custom_setup(base_config, use_real_data, **kwargs)
            elif retry_choice == '2':
                return _interactive_hpo_preset_setup(base_config, use_real_data, **kwargs)
            elif retry_choice == '3':
                return _interactive_hpo_express_setup(base_config, use_real_data, **kwargs)
            else:
                print("Returning to previous menu")
                return None
                
    except KeyboardInterrupt:
        print("\nCustom HPO setup interrupted")
        return None
    except Exception as e:
        logger.error(f"Custom HPO setup failed: {e}")
        print(f"Custom HPO setup failed: {str(e)}")
        return None

def _interactive_hpo_continue_setup(
    base_config: Dict[str, Any],
    **kwargs
) -> Optional[Dict[str, Any]]:
    """Setup to continue an existing HPO study with enhanced integration and error handling."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        show_banner()
        
        print("Continue existing hyperparameter optimization study")
        
        # Look for existing studies in multiple locations
        study_base_dir = Path(DEFAULT_MODEL_DIR) / "hpo_studies"
        alternative_dirs = [
            Path("studies"),
            Path("optuna_studies"),
            Path("hpo"),
            CONFIG_DIR / "studies"
        ]
        
        # Check primary directory
        if not study_base_dir.exists():
            print(f"Primary study directory not found at {study_base_dir}")
            
            # Check alternative directories
            found_dir = None
            for alt_dir in alternative_dirs:
                if alt_dir.exists() and list(alt_dir.glob("*.db")) or list(alt_dir.glob("*.pkl")):
                    found_dir = alt_dir
                    print(f"Found studies in alternative directory: {found_dir}")
                    study_base_dir = found_dir
                    break
            
            if not found_dir:
                print("No study directories found. Switching to express setup...")
                return _interactive_hpo_express_setup(base_config, None, **kwargs)
        
        # Find study files (both database and pickle formats)
        study_db_files = list(study_base_dir.glob("*.db"))
        study_pkl_files = list(study_base_dir.glob("*_study.pkl"))
        
        # Combine and prioritize database files
        all_study_files = study_db_files + study_pkl_files
        
        if not all_study_files:
            print("No existing studies found")
            print("Switching to express setup...")
            return _interactive_hpo_express_setup(base_config, None, **kwargs)
        
        print(f"\nFound {len(all_study_files)} existing study files:")
        
        studies_info = []
        
        for i, study_file in enumerate(all_study_files, 1):
            try:
                study_info = {
                    'index': i,
                    'file': study_file,
                    'study': None,
                    'name': study_file.stem,
                    'file_type': 'database' if study_file.suffix == '.db' else 'pickle',
                    'trials': 0,
                    'complete': 0,
                    'pruned': 0,
                    'failed': 0,
                    'best_value': "No trials",
                    'error': None,
                    'accessible': False
                }
                
                # Try to load the study based on file type
                if study_file.suffix == '.db':
                    # Database format - try Optuna direct loading
                    try:
                        import optuna
                        storage_url = f"sqlite:///{study_file}"
                        
                        # Try to get study names from this database
                        try:
                            storage = optuna.storages.get_storage(storage_url)
                            study_names = storage.get_all_study_names()
                            
                            if study_names:
                                # Use the first study name (most common case)
                                study_name = study_names[0]
                                study = optuna.load_study(study_name=study_name, storage=storage_url)
                                
                                study_info.update({
                                    'study': study,
                                    'name': study_name,
                                    'trials': len(study.trials),
                                    'complete': len([t for t in study.trials if t.state.name == 'COMPLETE']),
                                    'pruned': len([t for t in study.trials if t.state.name == 'PRUNED']),
                                    'failed': len([t for t in study.trials if t.state.name == 'FAIL']),
                                    'best_value': study.best_value if study.trials and study_info['complete'] > 0 else "No completed trials",
                                    'accessible': True
                                })
                                
                                if len(study_names) > 1:
                                    study_info['multiple_studies'] = study_names
                                    
                            else:
                                study_info['error'] = "Database contains no studies"
                                
                        except Exception as load_error:
                            study_info['error'] = f"Could not load from database: {str(load_error)}"
                            
                    except ImportError:
                        study_info['error'] = "Optuna not available for database loading"
                    except Exception as e:
                        study_info['error'] = f"Database access failed: {str(e)}"
                        
                else:
                    # Pickle format - legacy support
                    try:
                        study = joblib.load(study_file)
                        study_info.update({
                            'study': study,
                            'name': getattr(study, 'study_name', study_file.stem),
                            'trials': len(study.trials),
                            'complete': len([t for t in study.trials if t.state.name == 'COMPLETE']),
                            'pruned': len([t for t in study.trials if t.state.name == 'PRUNED']),
                            'failed': len([t for t in study.trials if t.state.name == 'FAIL']),
                            'best_value': study.best_value if hasattr(study, 'best_value') and study.trials and study_info['complete'] > 0 else "No completed trials",
                            'accessible': True
                        })
                    except Exception as e:
                        study_info['error'] = f"Pickle loading failed: {str(e)}"
                
                # Add file metadata
                try:
                    file_stat = study_file.stat()
                    study_info.update({
                        'file_size_mb': file_stat.st_size / (1024 * 1024),
                        'last_modified': datetime.fromtimestamp(file_stat.st_mtime),
                        'creation_time': datetime.fromtimestamp(file_stat.st_ctime) if hasattr(file_stat, 'st_ctime') else None
                    })
                except Exception:
                    pass
                
                studies_info.append(study_info)
                
                # Display study information
                print(f"{i}. {study_info['name']} ({study_info['file_type'].upper()})")
                if study_info['accessible']:
                    print(f"   Trials: {study_info['complete']}/{study_info['trials']} complete, {study_info['pruned']} pruned, {study_info['failed']} failed")
                    print(f"   Best value: {study_info['best_value']}")
                    
                    if 'multiple_studies' in study_info:
                        print(f"   Multiple studies in database: {len(study_info['multiple_studies'])}")
                    
                    print(f"   Last modified: {study_info['last_modified'].strftime('%Y-%m-%d %H:%M:%S')}")
                    if study_info['file_size_mb'] > 0:
                        print(f"   File size: {study_info['file_size_mb']:.1f} MB")
                else:
                    print(f"   Error: {study_info['error']}")
                
                print()
                
            except Exception as e:
                study_info = {
                    'index': i,
                    'file': study_file,
                    'name': study_file.name,
                    'error': str(e),
                    'accessible': False
                }
                studies_info.append(study_info)
                print(f"{i}. {study_file.name} (Error: {e})")
                print()
        
        # Filter accessible studies
        accessible_studies = [s for s in studies_info if s['accessible']]
        
        if not accessible_studies:
            print("No accessible studies found. All studies have errors.")
            print("Available options:")
            print("1. Try to repair database files")
            print("2. Switch to express setup")
            print("0. Cancel")
            
            while True:
                choice = input("Select option (0-2): ").strip()
                if choice in ['1', '2', '0']:
                    break
                print("Invalid choice. Please select 0-2.")
            
            if choice == '0':
                return None
            elif choice == '1':
                print("Database repair not implemented. Switching to express setup...")
            
            return _interactive_hpo_express_setup(base_config, None, **kwargs)
        
        print(f"{len(studies_info)+1}. Back to express setup")
        print("0. Cancel")
        
        # Get user selection
        while True:
            try:
                choice = input(f"\nSelect study to continue (1-{len(studies_info)}, {len(studies_info)+1} for express setup, 0 to cancel): ").strip()
                choice_num = int(choice)
                
                if choice_num == 0:
                    print("Continue study cancelled")
                    return None
                elif choice_num == len(studies_info) + 1:
                    return _interactive_hpo_express_setup(base_config, None, **kwargs)
                elif 1 <= choice_num <= len(studies_info):
                    selected_study_info = studies_info[choice_num-1]
                    break
                else:
                    print(f"Invalid choice. Please select 0-{len(studies_info)+1}.")
            except ValueError:
                print(f"Invalid input. Please enter a number 0-{len(studies_info)+1}.")
        
        if not selected_study_info['accessible']:
            print(f"Cannot continue study due to error: {selected_study_info['error']}")
            return None
        
        study = selected_study_info['study']
        study_name = selected_study_info['name']
        file_type = selected_study_info['file_type']
        
        print(f"\nSelected study: {study_name} ({file_type})")
        print(f"Current status: {selected_study_info['complete']} completed trials out of {selected_study_info['trials']} total")
        
        # Display study configuration if available
        try:
            if hasattr(study, 'user_attrs'):
                config_info = study.user_attrs.get('configuration', {})
                if config_info:
                    print(f"\nOriginal study configuration:")
                    if 'sampler_type' in config_info:
                        print(f"   Sampler: {config_info['sampler_type']}")
                    if 'pruner_type' in config_info:
                        print(f"   Pruner: {config_info['pruner_type']}")
                    if 'model_types' in config_info:
                        print(f"   Models: {', '.join(config_info['model_types'])}")
        except Exception:
            pass
        
        # Enhanced configuration for continuing
        print(f"\n" + "="*50)
        print("CONTINUATION CONFIGURATION")
        print("="*50)
        
        additional_trials = input("Additional trials to run (50): ").strip()
        if additional_trials.lower() == 'c':
            print("Study continuation cancelled")
            return None
        additional_trials = int(additional_trials) if additional_trials else 50
        
        additional_timeout = input("Additional timeout in minutes (60, 0 for no limit): ").strip()
        if additional_timeout.lower() == 'c':
            print("Study continuation cancelled")
            return None
        additional_timeout = int(additional_timeout) if additional_timeout else 60
        
        # Advanced options
        print("\nAdvanced continuation options:")
        print("1. Continue with same configuration")
        print("2. Modify search space")
        print("3. Change pruning strategy")
        print("4. Add new model types")
        
        advanced_choice = input("Select option (1-4, default=1): ").strip()
        if advanced_choice.lower() == 'c':
            print("Study continuation cancelled")
            return None
        advanced_choice = int(advanced_choice) if advanced_choice in ['2', '3', '4'] else 1
        
        # Build comprehensive configuration for continuation
        final_config = deepcopy(base_config) if base_config else {}
        
        # If no base config, try to extract from study or use default
        if not final_config:
            if 'default' in PRESET_CONFIGS:
                final_config = deepcopy(PRESET_CONFIGS['default'])
                print("Using default preset as base for continuation")
            else:
                logger.warning("No base config available for continuation")
                final_config = {}
        
        # HPO configuration for continuation
        hpo_config = final_config.setdefault('hyperparameter_optimization', {})
        hpo_config.update({
            'enabled': True,
            'strategy': 'optuna',
            'n_trials': additional_trials,
            'timeout': additional_timeout * 60 if additional_timeout > 0 else 0,
            'timeout_seconds': additional_timeout * 60 if additional_timeout > 0 else 0,
            'study_name': study_name,
            'direction': 'minimize',  # Will be overridden by existing study
            'load_if_exists': True,
            'verbose': True,
            'cleanup_trials': True,
            'generate_plots': True,
            
            # Storage configuration
            'storage': {
                'enabled': True,
                'url': f"sqlite:///{selected_study_info['file']}" if file_type == 'database' else None,
                'load_if_exists': True,
                'heartbeat_interval': 60,
                'grace_period': 120
            }
        })
        
        # Apply advanced modifications based on user choice
        if advanced_choice == 2:  # Modify search space
            print("\nModifying search space...")
            print("Current optimization space will be expanded")
            
            # Get existing optimization space from study if available
            existing_space = {}
            try:
                if hasattr(study, 'user_attrs') and 'optimization_space' in study.user_attrs:
                    existing_space = study.user_attrs['optimization_space']
            except Exception:
                pass
            
            # Expand the search space
            expanded_space = {
                'learning_rate': {'type': 'float', 'low': 1e-6, 'high': 1e-1, 'log': True},
                'batch_size': {'type': 'categorical', 'choices': [16, 32, 64, 128, 256]},
                'encoding_dim': {'type': 'int', 'low': 4, 'high': 64},
                'dropout_rate': {'type': 'float', 'low': 0.0, 'high': 0.6},
                'weight_decay': {'type': 'float', 'low': 1e-7, 'high': 1e-2, 'log': True}
            }
            
            # Merge with existing space
            for key, value in existing_space.items():
                if key not in expanded_space:
                    expanded_space[key] = value
            
            hpo_config['optimization_space'] = expanded_space
            print("   Search space expanded with broader ranges")
            
        elif advanced_choice == 3:  # Change pruning strategy
            print("\nChanging pruning strategy...")
            print("1. MedianPruner (aggressive)")
            print("2. HyperbandPruner (successive halving)")
            print("3. NopPruner (no pruning)")
            
            pruner_choice = input("Select pruner (1-3): ").strip()
            if pruner_choice == '1':
                hpo_config['pruner'] = 'MedianPruner'
            elif pruner_choice == '2':
                hpo_config['pruner'] = 'HyperbandPruner'
            elif pruner_choice == '3':
                hpo_config['pruner'] = 'NopPruner'
            else:
                hpo_config['pruner'] = 'MedianPruner'
            
            print(f"   Pruner set to {hpo_config['pruner']}")
            
        elif advanced_choice == 4:  # Add new model types
            print("\nAdding new model types...")
            available_models = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
            
            # Try to get existing models from study
            existing_models = []
            try:
                if hasattr(study, 'user_attrs') and 'model_types' in study.user_attrs:
                    existing_models = study.user_attrs['model_types']
            except Exception:
                pass
            
            if existing_models:
                print(f"Current models: {', '.join(existing_models)}")
            
            new_models = []
            for model in available_models:
                if model not in existing_models:
                    include = input(f"Add {model}? (y/N): ").strip().lower()
                    if include == 'y':
                        new_models.append(model)
            
            if new_models:
                all_models = list(set(existing_models + new_models))
                hpo_config['model_search'] = {
                    'enabled': len(all_models) > 1,
                    'model_types': all_models,
                    'search_all_models': len(all_models) >= 3
                }
                print(f"   Added models: {', '.join(new_models)}")
            else:
                print("   No new models added")
        
        # Try to extract and preserve original study configuration
        try:
            if hasattr(study, 'user_attrs') and 'configuration' in study.user_attrs:
                original_config = study.user_attrs['configuration']
                
                # Preserve important original settings
                for section, values in original_config.items():
                    if section in ['data', 'model', 'training'] and isinstance(values, dict):
                        section_config = final_config.setdefault(section, {})
                        for key, value in values.items():
                            if key not in section_config:
                                section_config[key] = value
                
                print("   Preserved original study configuration")
        except Exception as e:
            print(f"   Could not extract original study configuration: {e}")
            print("   Using default configuration for missing settings")
        
        # System configuration for continuation
        system_config = final_config.setdefault('system', {})
        system_config.update({
            'non_interactive': True,
            'verbose': True,
            'random_seed': 42,
            'reproducible': True
        })
        
        # Update metadata to reflect continuation
        metadata = final_config.setdefault('metadata', {})
        metadata.update({
            'last_modified': datetime.now().isoformat(),
            'setup_method': 'continue_hpo',
            'continuation_info': {
                'original_study': study_name,
                'original_trials': selected_study_info['trials'],
                'completed_trials': selected_study_info['complete'],
                'additional_trials': additional_trials,
                'additional_timeout_minutes': additional_timeout if additional_timeout > 0 else 'unlimited',
                'file_type': file_type,
                'advanced_modifications': advanced_choice > 1,
                'continuation_timestamp': datetime.now().isoformat()
            }
        })
        
        # Display continuation summary
        print(f"\n" + "="*60)
        print("CONTINUATION SUMMARY")
        print("="*60)
        print(f"Study: {study_name}")
        print(f"File Type: {file_type.upper()}")
        print(f"Current Progress: {selected_study_info['complete']} completed trials")
        print(f"Additional Trials: {additional_trials}")
        print(f"Additional Timeout: {additional_timeout} minutes" if additional_timeout > 0 else "Additional Timeout: No limit")
        print(f"Advanced Modifications: {'Yes' if advanced_choice > 1 else 'No'}")
        
        if selected_study_info['best_value'] != "No completed trials" and selected_study_info['best_value'] != "No trials":
            print(f"Current Best Value: {selected_study_info['best_value']}")
        
        # Estimate additional time
        try:
            model_count = len(hpo_config.get('model_search', {}).get('model_types', ['EnhancedAutoencoder']))
            trial_epochs = hpo_config.get('trial_epochs', 20)
            cv_folds = hpo_config.get('cross_validation', {}).get('folds', 3)
            
            estimated_time = _estimate_hpo_time(additional_trials, trial_epochs, model_count, cv_folds)
            print(f"Estimated Additional Time: ~{estimated_time}")
        except Exception:
            pass
        
        print("="*60)
        
        # Final confirmation
        confirm = input("\nContinue hyperparameter optimization with these settings? (Y/n/c to cancel): ").strip().lower()
        if confirm in ('', 'y', 'yes'):
            print(f"\nContinuing hyperparameter optimization study: {study_name}...")
            
            # Additional setup for continuation
            hpo_config.update({
                'continuation_mode': True,
                'original_trials': selected_study_info['trials'],
                'original_completed': selected_study_info['complete']
            })
            
            return _launch_hpo_with_config(final_config, **kwargs)
        elif confirm in ('c', 'cancel'):
            print("Study continuation cancelled")
            return None
        else:
            print("Would you like to:")
            print("1. Try with different settings")
            print("2. Switch to express setup")
            print("0. Return to previous menu")
            
            while True:
                retry_choice = input("Select option (0-2): ").strip()
                if retry_choice in ['1', '2', '0']:
                    break
                print("Invalid choice. Please select 0-2.")
            
            if retry_choice == '1':
                return _interactive_hpo_continue_setup(base_config, **kwargs)
            elif retry_choice == '2':
                return _interactive_hpo_express_setup(base_config, None, **kwargs)
            else:
                print("Returning to previous menu")
                return None
                
    except KeyboardInterrupt:
        print("\nStudy continuation setup interrupted")
        return None
    except Exception as e:
        logger.error(f"Study continuation setup failed: {e}")
        print(f"Study continuation setup failed: {str(e)}")
        
        # Fallback to express setup
        print("Falling back to express setup...")
        try:
            return _interactive_hpo_express_setup(base_config, None, **kwargs)
        except Exception:
            return None

def _launch_hpo_with_config(config: Dict[str, Any], **kwargs) -> Optional[Dict[str, Any]]:
    """Launch hyperparameter optimization with comprehensive configuration support and error handling."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        show_banner()
        
        print("\nLAUNCHING HYPERPARAMETER OPTIMIZATION")
        print("="*60)
        
        # Validate configuration structure
        if not config:
            raise ValueError("Configuration is empty or None")
        
        # Extract and validate HPO parameters from configuration
        hpo_config = config.get('hyperparameter_optimization', {})
        if not hpo_config:
            raise ValueError("No hyperparameter_optimization section in configuration")
        
        if not hpo_config.get('enabled', False):
            raise ValueError("Hyperparameter optimization is not enabled in configuration")
        
        # Extract configuration sections with fallbacks
        data_config = config.get('data', {})
        system_config = config.get('system', {})
        training_config = config.get('training', {})
        model_config = config.get('model', {})
        security_config = config.get('security', {})
        hardware_config = config.get('hardware', {})
        monitoring_config = config.get('monitoring', {})
        metadata = config.get('metadata', {})
        
        # Prepare comprehensive parameters for run_hyperparameter_optimization
        hpo_params = {}
        
        # Core HPO parameters with validation
        hpo_params['n_trials'] = hpo_config.get('n_trials', 50)
        hpo_params['timeout'] = hpo_config.get('timeout', 3600)  # seconds
        hpo_params['timeout_minutes'] = hpo_config.get('timeout', 3600) // 60
        hpo_params['study_name'] = hpo_config.get('study_name', f"hpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        hpo_params['direction'] = hpo_config.get('direction', 'minimize')
        hpo_params['objective_metric'] = hpo_config.get('objective_metric', 'validation_loss')
        
        # Sampling and pruning configuration
        hpo_params['sampler_type'] = hpo_config.get('sampler', 'TPESampler')
        hpo_params['pruner_type'] = hpo_config.get('pruner', 'MedianPruner')
        
        # Model search configuration
        model_search = hpo_config.get('model_search', {})
        if model_search.get('enabled', False):
            hpo_params['model_types'] = model_search.get('model_types', ['EnhancedAutoencoder'])
            hpo_params['search_all_models'] = model_search.get('search_all_models', len(hpo_params['model_types']) > 1)
        else:
            hpo_params['model_types'] = [model_config.get('model_type', 'EnhancedAutoencoder')]
            hpo_params['search_all_models'] = False
        
        # Cross-validation configuration
        cv_config = hpo_config.get('cross_validation', {})
        hpo_params['cv_folds'] = cv_config.get('folds', 3)
        hpo_params['cv_shuffle'] = cv_config.get('shuffle', True)
        hpo_params['cv_random_state'] = cv_config.get('random_state', 42)
        
        # Trial configuration
        hpo_params['trial_epochs'] = hpo_config.get('trial_epochs', 20)
        hpo_params['trial_patience'] = hpo_config.get('trial_patience', 5)
        hpo_params['trial_batch_size'] = hpo_config.get('trial_batch_size', training_config.get('batch_size', 64))
        
        # Early stopping configuration
        early_stopping = hpo_config.get('early_stopping', {})
        hpo_params['early_stopping_patience'] = early_stopping.get('patience', 10)
        hpo_params['early_stopping_min_trials'] = early_stopping.get('min_trials', 20)
        hpo_params['early_stopping_min_improvement'] = early_stopping.get('min_improvement', 1e-4)
        
        # Storage configuration
        storage_config = hpo_config.get('storage', {})
        if storage_config.get('enabled', False):
            hpo_params['storage_url'] = storage_config.get('url')
            hpo_params['load_if_exists'] = storage_config.get('load_if_exists', False)
        
        # System and performance parameters
        hpo_params['verbose'] = hpo_config.get('verbose', system_config.get('verbose', True))
        hpo_params['save_study'] = hpo_config.get('cleanup_trials', True)
        hpo_params['generate_plots'] = hpo_config.get('generate_plots', True)
        hpo_params['cleanup_trials'] = hpo_config.get('cleanup_trials', True)
        hpo_params['interactive'] = kwargs.get('interactive', False)
        
        # Data parameters
        hpo_params['use_real_data'] = data_config.get('use_real_data', False)
        hpo_params['data_path'] = data_config.get('data_path')
        hpo_params['artifacts_path'] = data_config.get('artifacts_path')
        
        if not hpo_params['use_real_data']:
            hpo_params['normal_samples'] = data_config.get('normal_samples', 8000)
            hpo_params['attack_samples'] = data_config.get('attack_samples', 2000)
            hpo_params['features'] = data_config.get('features', 20)
        
        hpo_params['normalization_method'] = data_config.get('normalization', 'standard')
        hpo_params['validation_split'] = data_config.get('validation_split', 0.2)
        hpo_params['test_split'] = data_config.get('test_split', 0.2)
        hpo_params['random_state'] = data_config.get('random_state', 42)
        
        # System parameters with hardware awareness
        hpo_params['device'] = hardware_config.get('device', 'auto')
        hpo_params['random_seed'] = system_config.get('random_seed', 42)
        hpo_params['reproducible'] = system_config.get('reproducible', True)
        hpo_params['num_workers'] = training_config.get('num_workers', 0)  # Safe default for HPO
        
        # Study directory configuration
        hpo_params['study_dir'] = Path(system_config.get('model_dir', DEFAULT_MODEL_DIR)) / "hpo_studies"
        hpo_params['study_dir'].mkdir(parents=True, exist_ok=True)
        
        # Optimization space configuration
        optimization_space = hpo_config.get('optimization_space', {})
        if optimization_space:
            hpo_params['optimization_space'] = optimization_space
        
        # Advanced HPO configuration
        sampler_config = hpo_config.get('sampler_config', {})
        if sampler_config:
            hpo_params['sampler_config'] = sampler_config

        # Fix sampler configuration parsing
        sampler_raw = hpo_config.get('sampler', 'TPESampler')
        if isinstance(sampler_raw, dict):
            # Extract type from dictionary configuration
            sampler_type = sampler_raw.get('type', 'TPESampler')
            # Store the full sampler configuration for later use
            if not sampler_config:
                hpo_params['sampler_config'] = sampler_raw
        else:
            sampler_type = sampler_raw

        # Map common sampler variants to standard names
        sampler_mapping = {
            'Random': 'RandomSampler',
            'TPE': 'TPESampler',
            'CmaEs': 'CmaEsSampler',
            'Grid': 'GridSampler'
        }
        hpo_params['sampler_type'] = sampler_mapping.get(sampler_type, sampler_type)

        pruner_config = hpo_config.get('pruner_config', {})
        if pruner_config:
            hpo_params['pruner_config'] = pruner_config

        # Fix pruner configuration parsing
        pruner_raw = hpo_config.get('pruner', 'MedianPruner')
        if isinstance(pruner_raw, dict):
            # Extract type from dictionary configuration
            pruner_type = pruner_raw.get('type', 'MedianPruner')
            # Store the full pruner configuration for later use
            if not pruner_config:
                hpo_params['pruner_config'] = pruner_raw
        else:
            pruner_type = pruner_raw

        # Map common pruner variants to standard names
        pruner_mapping = {
            'Nop': 'NopPruner',
            'Median': 'MedianPruner',
            'Hyperband': 'HyperbandPruner',
            'SuccessiveHalving': 'HyperbandPruner'
        }
        hpo_params['pruner_type'] = pruner_mapping.get(pruner_type, pruner_type)

        scoring_config = hpo_config.get('scoring', {})
        if scoring_config:
            hpo_params['scoring_config'] = scoring_config
        
        # Performance and resource management
        hpo_params['parallel_jobs'] = system_config.get('max_workers', 1)
        hpo_params['memory_limit'] = hardware_config.get('memory_management', {}).get('memory_limit')
        
        # Error handling configuration
        error_config = config.get('error_handling', {})
        hpo_params['error_handling'] = error_config.get('enabled', True)
        hpo_params['graceful_degradation'] = error_config.get('graceful_degradation', True)
        hpo_params['continue_on_error'] = error_config.get('continue_on_error', False)
        
        # Continuation mode handling
        if hpo_config.get('continuation_mode', False):
            hpo_params['continuation_mode'] = True
            hpo_params['original_trials'] = hpo_config.get('original_trials', 0)
            hpo_params['original_completed'] = hpo_config.get('original_completed', 0)
            print(f"Continuation mode: Adding {hpo_params['n_trials']} trials to existing study")
        
        # Add the complete configuration for reference
        hpo_params['config'] = config
        
        # Add any additional kwargs passed to the function
        hpo_params.update({k: v for k, v in kwargs.items() if k not in hpo_params})
        
        # Log launch parameters for debugging
        logger.info(f"Launching HPO with configuration:")
        logger.info(f"  - Study: {hpo_params['study_name']}")
        logger.info(f"  - Trials: {hpo_params['n_trials']}")
        logger.info(f"  - Timeout: {hpo_params['timeout_minutes']} minutes")
        logger.info(f"  - Models: {', '.join(hpo_params['model_types'])}")
        logger.info(f"  - Sampler: {hpo_params['sampler_type']}")
        logger.info(f"  - Pruner: {hpo_params['pruner_type']}")
        logger.info(f"  - Data: {'Real' if hpo_params['use_real_data'] else 'Synthetic'}")
        logger.info(f"  - CV Folds: {hpo_params['cv_folds']}")
        
        print("\nHPO Parameters Summary:")
        print(f"  Study Name: {hpo_params['study_name']}")
        print(f"  Trials: {hpo_params['n_trials']}")
        print(f"  Timeout: {hpo_params['timeout_minutes']} minutes")
        print(f"  Model Types: {', '.join(hpo_params['model_types'])}")
        print(f"  Cross-Validation: {hpo_params['cv_folds']} folds")
        print(f"  Data Source: {'Real Data' if hpo_params['use_real_data'] else 'Synthetic Data'}")
        
        # Estimate completion time
        try:
            estimated_time = _estimate_hpo_time(
                hpo_params['n_trials'], 
                hpo_params['trial_epochs'], 
                len(hpo_params['model_types']), 
                hpo_params['cv_folds']
            )
            print(f"  Estimated Time: ~{estimated_time}")
        except Exception as e:
            logger.debug(f"Time estimation failed: {e}")
        
        print("\nCalling hyperparameter optimization function...")
        
        # Call the main HPO function with comprehensive parameters
        try:
            results = run_hyperparameter_optimization(**hpo_params)
        except Exception as hpo_error:
            logger.error(f"HPO execution failed: {hpo_error}")
            
            # Return structured error result
            return {
                'success': False,
                'error': str(hpo_error),
                'error_type': type(hpo_error).__name__,
                'study_name': hpo_params['study_name'],
                'n_trials_total': hpo_params['n_trials'],
                'n_trials_completed': 0,
                'configuration': config,
                'start_time': datetime.now().isoformat(),
                'end_time': datetime.now().isoformat(),
                'recommendations': [
                    'Check system resources and configuration',
                    'Verify data availability and format',
                    'Consider reducing trial count or complexity',
                    'Review log files for detailed error information'
                ]
            }
        
        # Process and enhance results
        if results:
            # Ensure basic success flag
            if 'success' not in results:
                results['success'] = results.get('n_trials_completed', 0) > 0
            
            # Add configuration metadata to results
            results['launch_config'] = {
                'launch_method': 'interactive_hpo',
                'setup_method': metadata.get('setup_method', 'unknown'),
                'config_source': metadata.get('config_source', 'unknown'),
                'launch_timestamp': datetime.now().isoformat(),
                'parameters_used': {
                    'n_trials': hpo_params['n_trials'],
                    'timeout_minutes': hpo_params['timeout_minutes'],
                    'model_types': hpo_params['model_types'],
                    'sampler_type': hpo_params['sampler_type'],
                    'pruner_type': hpo_params['pruner_type']
                }
            }
            
            # Enhanced success handling
            if results.get('success', False):
                print("\n" + "="*70)
                print("HYPERPARAMETER OPTIMIZATION COMPLETED SUCCESSFULLY!")
                print("="*70)
                
                # Display results using the comprehensive display function
                _display_hpo_results(results)
                
            else:
                print("\n" + "="*70)
                print("HYPERPARAMETER OPTIMIZATION COMPLETED WITH ISSUES")
                print("="*70)
                
                error_msg = results.get('error', 'Unknown error occurred')
                print(f"Error: {error_msg}")
                
                # Show partial results if available
                n_completed = results.get('n_trials_completed', 0)
                if n_completed > 0:
                    print(f"\nPartial optimization completed:")
                    print(f"   Trials completed: {n_completed}")
                    
                    best_value = results.get('best_value')
                    if best_value and best_value != float('inf'):
                        print(f"   Best value found: {best_value:.6f}")
                        
                        best_params = results.get('best_params', {})
                        if best_params:
                            print(f"   Best parameters preview:")
                            # Show top 5 parameters
                            for i, (param, value) in enumerate(list(best_params.items())[:5]):
                                print(f"      {param}: {value}")
                            if len(best_params) > 5:
                                print(f"      ... and {len(best_params) - 5} more parameters")
                
                # Display recommendations for recovery
                recommendations = results.get('recommendations', [])
                if recommendations:
                    print(f"\nRecommendations:")
                    for i, rec in enumerate(recommendations, 1):
                        print(f"   {i}. {rec}")
                
                # Show error recovery information
                error_log_path = results.get('error_log_path')
                if error_log_path:
                    print(f"\nDetailed error log: {error_log_path}")
                
                print("="*70)
        
        else:
            # Handle case where no results are returned
            print("\n" + "="*70)
            print("HYPERPARAMETER OPTIMIZATION FAILED TO START")
            print("="*70)
            print("No results returned from optimization function.")
            print("This may indicate a configuration or system issue.")
            
            results = {
                'success': False,
                'error': 'No results returned from HPO function',
                'error_type': 'ExecutionError',
                'study_name': hpo_params['study_name'],
                'n_trials_total': hpo_params['n_trials'],
                'n_trials_completed': 0,
                'start_time': datetime.now().isoformat(),
                'end_time': datetime.now().isoformat(),
                'recommendations': [
                    'Check HPO function implementation',
                    'Verify all required dependencies are available',
                    'Review system logs for more information'
                ]
            }
        
        return results
        
    except KeyboardInterrupt:
        print("HPO LAUNCH INTERRUPTED BY USER")
        
        # Get basic parameters for the interrupt result
        study_name = "unknown"
        n_trials = 0
        try:
            if 'hpo_params' in locals():
                study_name = hpo_params.get('study_name', 'unknown')
                n_trials = hpo_params.get('n_trials', 0)
        except:
            pass
        
        print(f"Study: {study_name}")
        print(f"Planned trials: {n_trials}")
        print("HPO was interrupted before starting optimization")
        print("No trials were completed or data was lost")
        
        # Return interrupt result
        interrupt_result = {
            'success': False,
            'error': 'HPO launch interrupted by user',
            'error_type': 'KeyboardInterrupt',
            'interrupted': True,
            'study_name': study_name,
            'n_trials_total': n_trials,
            'n_trials_completed': 0,
            'start_time': datetime.now().isoformat(),
            'end_time': datetime.now().isoformat(),
            'configuration': config,
            'interrupt_stage': 'launch',
            'recovery_info': {
                'can_resume': False,
                'data_lost': False,
                'message': 'HPO was cancelled before starting - no data to recover'
            },
            'recommendations': [
                'HPO can be restarted safely from the beginning',
                'Consider reducing trial count if time is limited',
                'All configuration has been preserved for restart'
            ]
        }
        
        print("\nInterrupt handled cleanly - no data was lost")
        
        return interrupt_result
        
    except ValueError as ve:
        logger.error(f"HPO configuration validation failed: {ve}")
        print(f"\nConfiguration Error: {str(ve)}")
        return {
            'success': False,
            'error': str(ve),
            'error_type': 'ConfigurationError',
            'recommendations': ['Fix configuration issues and try again']
        }
        
    except Exception as e:
        logger.error(f"HPO launch failed with unexpected error: {e}", exc_info=True)
        print(f"\nUnexpected error during HPO launch: {str(e)}")
        
        return {
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__,
            'recommendations': [
                'Check system resources and environment',
                'Verify configuration file integrity', 
                'Review error logs for more details',
                'Consider restarting the system'
            ]
        }

def _estimate_hpo_time(n_trials: int, trial_epochs: int, n_model_types: int, cv_folds: int, 
                      hardware_info: Optional[Dict[str, Any]] = None, 
                      system_class: str = 'unknown') -> str:
    """
    Enhanced HPO completion time estimation with hardware awareness and detailed analysis.
    
    Args:
        n_trials: Number of optimization trials
        trial_epochs: Epochs per trial
        n_model_types: Number of model types to optimize
        cv_folds: Cross-validation folds
        hardware_info: Optional hardware information from system analysis
        system_class: System performance class (e.g., 'high-end', 'standard', 'limited')
    
    Returns:
        Human-readable time estimate string
    """
    try:
        # Base time per trial epoch in seconds (conservative estimate)
        base_epoch_time = 15  # seconds per epoch on average hardware
        
        # Get hardware information if not provided
        if hardware_info is None:
            try:
                hardware_info = check_hardware(min_disk_gb=1.0, include_memory_usage=False)
            except Exception:
                hardware_info = {}
        
        # Hardware performance factors
        cpu_factor = 1.0
        memory_factor = 1.0
        gpu_factor = 1.0
        storage_factor = 1.0
        
        # CPU analysis
        cpu_info = hardware_info.get('cpu_cores', {})
        if cpu_info.get('available', False):
            logical_cores = cpu_info.get('logical_cores', 1)
            
            if logical_cores >= 16:
                cpu_factor = 0.4  # High-end CPU
            elif logical_cores >= 8:
                cpu_factor = 0.6  # Good CPU
            elif logical_cores >= 4:
                cpu_factor = 0.8  # Standard CPU
            elif logical_cores >= 2:
                cpu_factor = 1.0  # Basic CPU
            else:
                cpu_factor = 1.5  # Limited CPU
            
            # Consider CPU frequency if available
            cpu_capacity = cpu_info.get('capacity', {})
            frequency_ghz = cpu_capacity.get('frequency_ghz', 0)
            if frequency_ghz > 3.5:
                cpu_factor *= 0.85  # High frequency bonus
            elif frequency_ghz < 2.0 and frequency_ghz > 0:
                cpu_factor *= 1.15  # Low frequency penalty
        
        # Memory analysis
        ram_info = hardware_info.get('system_ram', {})
        if ram_info.get('available', False):
            ram_gb = ram_info.get('ram_total_gb', 4.0)
            ram_available_gb = ram_info.get('ram_available_gb', ram_gb * 0.7)
            
            if ram_gb >= 32:
                memory_factor = 0.7  # Plenty of memory
            elif ram_gb >= 16:
                memory_factor = 0.8  # Good memory
            elif ram_gb >= 8:
                memory_factor = 0.9  # Standard memory
            elif ram_gb >= 4:
                memory_factor = 1.0  # Basic memory
            else:
                memory_factor = 1.3  # Limited memory
            
            # Consider available memory ratio
            if ram_available_gb / ram_gb < 0.3:  # Less than 30% available
                memory_factor *= 1.2  # Memory pressure penalty
        
        # GPU analysis
        cuda_info = hardware_info.get('cuda', {})
        if cuda_info.get('available', False):
            gpu_count = cuda_info.get('gpu_count', 0)
            gpus = cuda_info.get('gpus', [])
            
            if gpus:
                # Analyze the best GPU
                best_gpu = max(gpus, key=lambda g: g.get('memory_gb', 0))
                gpu_memory_gb = best_gpu.get('memory_gb', 0)
                compute_capability = float(best_gpu.get('compute_capability', '0.0'))
                
                if gpu_memory_gb >= 16 and compute_capability >= 8.0:
                    gpu_factor = 0.15  # High-end modern GPU (RTX 40xx series)
                elif gpu_memory_gb >= 12 and compute_capability >= 7.5:
                    gpu_factor = 0.2   # High-end GPU (RTX 30xx series)
                elif gpu_memory_gb >= 8 and compute_capability >= 7.0:
                    gpu_factor = 0.25  # Good GPU (RTX 20xx series)
                elif gpu_memory_gb >= 6 and compute_capability >= 6.1:
                    gpu_factor = 0.35  # Mid-range GPU (GTX 16xx series)
                elif gpu_memory_gb >= 4:
                    gpu_factor = 0.45  # Entry-level GPU
                else:
                    gpu_factor = 0.6   # Low-end GPU
                
                # Multi-GPU bonus (limited for HPO)
                if gpu_count > 1:
                    gpu_factor *= 0.9  # Slight improvement for multi-GPU
            else:
                gpu_factor = 1.0  # CUDA available but no GPU info
        else:
            gpu_factor = 1.0  # CPU-only execution
        
        # Storage analysis (affects data loading)
        disk_info = hardware_info.get('disk_space', {})
        if disk_info.get('available', False):
            # This is basic - in real implementation, would check SSD vs HDD
            free_gb = disk_info.get('free_gb', 10)
            if free_gb < 5:
                storage_factor = 1.2  # Low disk space penalty
            else:
                storage_factor = 1.0
        
        # System class adjustment
        system_class_factors = {
            'high-end': 0.6,
            'performance': 0.7, 
            'standard': 1.0,
            'baseline': 1.1,
            'limited': 1.4,
            'debug': 1.6,
            'lightweight': 1.3,
            'unknown': 1.0
        }
        system_class_factor = system_class_factors.get(system_class.lower(), 1.0)
        
        # Model complexity factor
        model_complexity_factor = 1.0
        if n_model_types == 1:
            model_complexity_factor = 1.0
        elif n_model_types == 2:
            model_complexity_factor = 1.6  # Not quite 2x due to shared overhead
        elif n_model_types == 3:
            model_complexity_factor = 2.2
        else:
            model_complexity_factor = n_model_types * 0.8  # Diminishing returns
        
        # Cross-validation factor
        cv_factor = max(1, cv_folds)
        if cv_folds > 5:
            # Diminishing returns for very high CV
            cv_factor = 5 + (cv_folds - 5) * 0.5
        
        # HPO overhead factor (trial setup, parameter sampling, etc.)
        hpo_overhead_factor = 1.25
        if n_trials > 200:
            hpo_overhead_factor = 1.3  # More overhead for large studies
        elif n_trials < 20:
            hpo_overhead_factor = 1.4  # Proportionally more overhead for small studies
        
        # Progressive trial factor (later trials may be faster due to pruning)
        if n_trials > 50:
            progressive_factor = 0.9  # Some trials will be pruned early
        else:
            progressive_factor = 1.0
        
        # Calculate total time
        base_trial_time = trial_epochs * base_epoch_time
        
        # Apply all factors
        hardware_adjusted_time = (base_trial_time * cpu_factor * memory_factor * 
                                min(gpu_factor, 1.0) * storage_factor * system_class_factor)
        
        total_seconds = (n_trials * hardware_adjusted_time * model_complexity_factor * 
                        cv_factor * hpo_overhead_factor * progressive_factor)
        
        # Add setup and teardown time
        setup_time = 60  # 1 minute setup
        teardown_time = 30  # 30 seconds teardown per trial average
        total_seconds += setup_time + (n_trials * teardown_time * 0.1)  # Reduced teardown per trial
        
        # Convert to minutes
        total_minutes = total_seconds / 60.0
        
        # Generate human-readable estimate with confidence interval
        if total_minutes < 1:
            base_estimate = "< 1 minute"
            range_estimate = "< 2 minutes"
        elif total_minutes < 60:
            minutes = int(total_minutes)
            base_estimate = f"{minutes} minute{'s' if minutes != 1 else ''}"
            range_estimate = f"{int(minutes * 0.7)}-{int(minutes * 1.3)} minutes"
        elif total_minutes < 1440:  # Less than 24 hours
            hours = total_minutes / 60
            if hours < 2:
                base_hours = int(hours)
                remaining_minutes = int((hours - base_hours) * 60)
                base_estimate = f"{base_hours}h {remaining_minutes}m"
                range_estimate = f"{int(hours * 0.7 * 60)}m - {int(hours * 1.3 * 60)}m"
            else:
                base_hours = int(hours)
                base_estimate = f"{base_hours} hour{'s' if base_hours != 1 else ''}"
                range_estimate = f"{int(base_hours * 0.7)}-{int(base_hours * 1.3)} hours"
        else:
            days = total_minutes / 1440
            base_days = int(days)
            remaining_hours = int((days - base_days) * 24)
            if remaining_hours == 0:
                base_estimate = f"{base_days} day{'s' if base_days != 1 else ''}"
            else:
                base_estimate = f"{base_days}d {remaining_hours}h"
            range_estimate = f"{int(days * 0.7)}-{int(days * 1.3)} days"
        
        # Add hardware context to estimate
        hardware_context = []
        if cuda_info.get('available', False):
            best_gpu = cuda_info.get('gpus', [{}])[0] if cuda_info.get('gpus') else {}
            gpu_name = best_gpu.get('name', 'GPU')
            hardware_context.append(f"GPU: {gpu_name}")
        else:
            hardware_context.append("CPU-only")
        
        cpu_cores = hardware_info.get('cpu_cores', {}).get('logical_cores', 'unknown')
        if cpu_cores != 'unknown':
            hardware_context.append(f"{cpu_cores} cores")
        
        ram_gb = hardware_info.get('system_ram', {}).get('ram_total_gb', 0)
        if ram_gb > 0:
            hardware_context.append(f"{ram_gb:.0f}GB RAM")
        
        # Build final estimate string
        if hardware_context:
            context_str = f" ({', '.join(hardware_context)})"
        else:
            context_str = ""
        
        # Return estimate with context
        final_estimate = f"{base_estimate}{context_str}"
        
        # Add confidence and range information for debugging
        logger.debug(f"HPO time estimation details:")
        logger.debug(f"  Base time per trial: {base_trial_time:.1f}s")
        logger.debug(f"  Hardware factors: CPU={cpu_factor:.2f}, Memory={memory_factor:.2f}, GPU={gpu_factor:.2f}")
        logger.debug(f"  Model complexity factor: {model_complexity_factor:.2f}")
        logger.debug(f"  CV factor: {cv_factor:.1f}")
        logger.debug(f"  Total estimated minutes: {total_minutes:.1f}")
        logger.debug(f"  Confidence range: {range_estimate}")
        
        return final_estimate
        
    except Exception as e:
        logger.debug(f"Time estimation failed: {e}")
        
        # Fallback estimation based on simple heuristics
        try:
            simple_minutes = n_trials * trial_epochs * n_model_types * cv_folds * 0.5  # 30 seconds per epoch average
            
            if torch.cuda.is_available():
                simple_minutes *= 0.3  # GPU acceleration
            
            if simple_minutes < 60:
                return f"~{int(simple_minutes)} minutes"
            elif simple_minutes < 1440:
                hours = int(simple_minutes // 60)
                return f"~{hours} hour{'s' if hours != 1 else ''}"
            else:
                days = int(simple_minutes // 1440)
                return f"~{days} day{'s' if days != 1 else ''}"
                
        except Exception:
            # Ultimate fallback
            return "Several hours (estimation failed)"

def _display_hpo_results(results: Dict[str, Any]) -> None:
    """
    Display comprehensive HPO results in a user-friendly format with complete integrated functionality.
    Enhanced to handle all result types from updated HPO functions with rich formatting and detailed analysis.
    """
    try:
        print("\n" + "="*80)
        print("HYPERPARAMETER OPTIMIZATION RESULTS SUMMARY")
        print("="*80)
        
        # Extract core result information with enhanced fallback handling
        success = results.get('success', False)
        start_time = results.get('start_time', 'Unknown')
        end_time = results.get('end_time', 'Unknown')
        study_name = results.get('study_name', 'Unknown')
        setup_method = results.get('launch_config', {}).get('setup_method', 'unknown')
        
        # Display basic status information
        status_icon = "✓" if success else "✗"
        print(f"Status: {status_icon} {'SUCCESS' if success else 'FAILED'}")
        print(f"Study Name: {study_name}")
        print(f"Setup Method: {setup_method.replace('_', ' ').title()}")
        print(f"Started: {start_time}")
        print(f"Completed: {end_time}")
        
        # Calculate and display duration if available
        try:
            if isinstance(start_time, str) and isinstance(end_time, str) and start_time != 'Unknown' and end_time != 'Unknown':
                start_dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                end_dt = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
                duration = end_dt - start_dt
                duration_minutes = duration.total_seconds() / 60
                print(f"Duration: {duration_minutes:.1f} minutes")
            else:
                total_time_seconds = results.get('total_time_seconds', 0)
                total_time_minutes = results.get('total_time_minutes', total_time_seconds / 60)
                if total_time_minutes > 0:
                    print(f"Duration: {total_time_minutes:.1f} minutes ({total_time_seconds:.1f} seconds)")
        except Exception:
            total_time_minutes = results.get('total_time_minutes', 0)
            if total_time_minutes > 0:
                print(f"Duration: {total_time_minutes:.1f} minutes")
        
        # Handle failure cases with comprehensive error information
        if not success:
            error_msg = results.get('error', 'Unknown error occurred')
            error_type = results.get('error_type', 'UnknownError')
            
            print(f"\nError Details:")
            print(f"  Type: {error_type}")
            print(f"  Message: {error_msg}")
            
            # Show continuation information if this was a continuation attempt
            continuation_info = results.get('launch_config', {}).get('continuation_info')
            if continuation_info:
                print(f"  Original Study: {continuation_info.get('original_study', 'Unknown')}")
                print(f"  Previous Trials: {continuation_info.get('completed_trials', 0)}")
            
            # Show partial results if any trials completed
            n_completed = results.get('n_trials_completed', 0)
            n_total = results.get('n_trials_total', 0)
            
            if n_completed > 0:
                print(f"\n" + "="*60)
                print("PARTIAL OPTIMIZATION RESULTS")
                print("="*60)
                print(f"Trials Completed: {n_completed}/{n_total}")
                
                best_value = results.get('best_value')
                if best_value is not None and best_value != float('inf'):
                    print(f"Best Value Found: {best_value:.6f}")
                    
                    best_params = results.get('best_params', {})
                    if best_params:
                        print(f"Best Parameters (Top 10):")
                        # Display top 10 parameters in organized format
                        param_items = list(best_params.items())[:10]
                        for param, value in param_items:
                            if isinstance(value, float):
                                if abs(value) < 0.001:
                                    print(f"  {param}: {value:.2e}")
                                else:
                                    print(f"  {param}: {value:.4f}")
                            elif isinstance(value, (list, tuple)):
                                print(f"  {param}: {list(value) if isinstance(value, tuple) else value}")
                            else:
                                print(f"  {param}: {value}")
                        
                        if len(best_params) > 10:
                            print(f"  ... and {len(best_params) - 10} more parameters")
                
                # Show trial statistics for partial results
                n_pruned = results.get('n_trials_pruned', 0)
                n_failed = results.get('n_trials_failed', 0)
                
                if n_pruned > 0 or n_failed > 0:
                    print(f"\nTrial Breakdown:")
                    print(f"  Completed: {n_completed}")
                    if n_pruned > 0:
                        print(f"  Pruned: {n_pruned}")
                    if n_failed > 0:
                        print(f"  Failed: {n_failed}")
            
            # Display recovery recommendations
            recommendations = results.get('recommendations', [])
            if recommendations:
                print(f"\n" + "="*60)
                print("RECOVERY RECOMMENDATIONS")
                print("="*60)
                for i, rec in enumerate(recommendations, 1):
                    print(f"{i}. {rec}")
            else:
                # Provide default recommendations based on error type and context
                default_recommendations = []
                if error_type in ['ConfigurationError', 'ValueError']:
                    default_recommendations.extend([
                        'Review and validate configuration parameters',
                        'Check data paths and file availability',
                        'Verify model architecture compatibility'
                    ])
                elif error_type in ['RuntimeError', 'CUDA_ERROR']:
                    default_recommendations.extend([
                        'Check GPU memory availability and reduce batch size if needed',
                        'Verify CUDA installation and compatibility',
                        'Consider switching to CPU device for testing'
                    ])
                elif error_type in ['MemoryError', 'OutOfMemoryError']:
                    default_recommendations.extend([
                        'Reduce number of trials or model complexity',
                        'Decrease batch size and number of workers',
                        'Enable memory optimization settings'
                    ])
                else:
                    default_recommendations.extend([
                        'Check system resources (CPU, memory, disk space)',
                        'Review error logs for detailed information',
                        'Consider using a simpler configuration or preset',
                        'Verify all required dependencies are installed'
                    ])
                
                if default_recommendations:
                    print(f"\n" + "="*60)
                    print("GENERAL RECOMMENDATIONS")
                    print("="*60)
                    for i, rec in enumerate(default_recommendations, 1):
                        print(f"{i}. {rec}")
            
            # Show error log path if available
            error_log_path = results.get('error_log_path')
            if error_log_path:
                print(f"\nDetailed error information saved to: {error_log_path}")
            
            print("="*80)
            return
        
        # SUCCESS CASE - Display comprehensive results
        print(f"\n" + "="*60)
        print("TRIAL STATISTICS")
        print("="*60)
        
        n_trials_total = results.get('n_trials_total', 0)
        n_trials_completed = results.get('n_trials_completed', 0)
        n_trials_pruned = results.get('n_trials_pruned', 0)
        n_trials_failed = results.get('n_trials_failed', 0)
        
        # Calculate completion rate and efficiency metrics
        completion_rate = (n_trials_completed / n_trials_total * 100) if n_trials_total > 0 else 0
        pruning_rate = (n_trials_pruned / n_trials_total * 100) if n_trials_total > 0 else 0
        failure_rate = (n_trials_failed / n_trials_total * 100) if n_trials_total > 0 else 0
        
        print(f"Total Trials: {n_trials_total}")
        print(f"Completed: {n_trials_completed} ({completion_rate:.1f}%)")
        if n_trials_pruned > 0:
            print(f"Pruned: {n_trials_pruned} ({pruning_rate:.1f}%)")
        if n_trials_failed > 0:
            print(f"Failed: {n_trials_failed} ({failure_rate:.1f}%)")
        
        # Display efficiency assessment
        if completion_rate >= 80:
            efficiency = "Excellent"
        elif completion_rate >= 60:
            efficiency = "Good"
        elif completion_rate >= 40:
            efficiency = "Fair"
        else:
            efficiency = "Poor"
        print(f"Optimization Efficiency: {efficiency}")
        
        if n_trials_completed == 0:
            print("\nNo trials completed successfully!")
            print("Please check your configuration and system resources.")
            print("="*80)
            return
        
        # BEST TRIAL RESULTS
        best_value = results.get('best_value')
        best_params = results.get('best_params', {})
        best_trial_number = results.get('best_trial_number')
        
        if best_value is not None and best_value != float('inf'):
            print(f"\n" + "="*60)
            print("BEST TRIAL RESULTS")
            print("="*60)
            print(f"Best Objective Value: {best_value:.6f}")
            if best_trial_number is not None:
                print(f"Best Trial Number: {best_trial_number}")
            
            # Performance assessment based on objective value
            if hasattr(results.get('study'), 'direction') and results.get('study').direction.name == 'MAXIMIZE':
                if best_value > 0.95:
                    performance = "Excellent"
                elif best_value > 0.85:
                    performance = "Very Good"
                elif best_value > 0.75:
                    performance = "Good"
                else:
                    performance = "Needs Improvement"
            else:  # Minimize direction
                if best_value < 0.01:
                    performance = "Excellent"
                elif best_value < 0.05:
                    performance = "Very Good"
                elif best_value < 0.1:
                    performance = "Good"
                else:
                    performance = "Needs Improvement"
            
            print(f"Performance Assessment: {performance}")
            
            # Display best parameters with intelligent categorization
            if best_params:
                print(f"\nOptimal Hyperparameters:")
                
                # Define parameter categories for organized display
                param_categories = {
                    'Model Architecture': {
                        'params': ['model_type', 'encoding_dim', 'hidden_arch', 'enhanced_arch', 'ensemble_arch', 
                                 'activation', 'normalization', 'num_models', 'diversity_factor'],
                        'icon': '[+]'
                    },
                    'Training Configuration': {
                        'params': ['learning_rate', 'batch_size', 'weight_decay', 'optimizer_type', 'scheduler_type', 
                                 'gradient_clip', 'gradient_accumulation_steps'],
                        'icon': '[+]'
                    },
                    'Regularization': {
                        'params': ['dropout_0', 'dropout_1', 'dropout_2', 'dropout_3', 'dropout_rate'],
                        'icon': '[+]'
                    },
                    'Advanced Features': {
                        'params': ['use_attention', 'residual_blocks', 'skip_connection', 'legacy_mode', 
                                 'mixed_precision', 'use_batch_norm', 'use_layer_norm'],
                        'icon': '[+]'
                    },
                    'Scheduler Parameters': {
                        'params': ['lr_patience', 'lr_factor', 'min_lr', 'step_size', 'gamma', 'eta_min'],
                        'icon': '[+]'
                    },
                    'Detection Thresholds': {
                        'params': ['percentile', 'threshold_method', 'attack_threshold'],
                        'icon': '[+]'
                    }
                }
                
                displayed_params = set()
                
                # Display categorized parameters
                for category_name, category_info in param_categories.items():
                    category_params = {}
                    for param in category_info['params']:
                        if param in best_params:
                            category_params[param] = best_params[param]
                            displayed_params.add(param)
                    
                    if category_params:
                        print(f"\n  {category_info.get('icon', '-')} {category_name}:")
                        for param, value in category_params.items():
                            # Format value based on type and magnitude
                            if isinstance(value, float):
                                if abs(value) < 0.001:
                                    formatted_value = f"{value:.2e}"
                                elif abs(value) < 0.01:
                                    formatted_value = f"{value:.4f}"
                                else:
                                    formatted_value = f"{value:.3f}"
                            elif isinstance(value, (list, tuple)):
                                formatted_value = str(list(value) if isinstance(value, tuple) else value)
                            elif isinstance(value, bool):
                                formatted_value = "✓" if value else "✗"
                            else:
                                formatted_value = str(value)
                            
                            print(f"    {param}: {formatted_value}")
                
                # Display any remaining uncategorized parameters
                remaining_params = {k: v for k, v in best_params.items() if k not in displayed_params}
                if remaining_params:
                    print(f"\n    Other Parameters:")
                    for param, value in remaining_params.items():
                        if isinstance(value, float):
                            if abs(value) < 0.001:
                                formatted_value = f"{value:.2e}"
                            else:
                                formatted_value = f"{value:.4f}"
                        else:
                            formatted_value = str(value)
                        print(f"    {param}: {formatted_value}")
        
        # OPTIMIZATION CONFIGURATION
        optimization_config = results.get('optimization_config', {})
        model_types_optimized = results.get('model_types_optimized', [])
        launch_config = results.get('launch_config', {})
        
        if optimization_config or model_types_optimized or launch_config:
            print(f"\n" + "="*60)
            print("OPTIMIZATION CONFIGURATION")
            print("="*60)
            
            if model_types_optimized:
                print(f"Model Types: {', '.join(model_types_optimized)}")
            
            # Display configuration details from various sources
            config_details = {}
            config_details.update(optimization_config)
            if 'parameters_used' in launch_config:
                config_details.update(launch_config['parameters_used'])
            
            for key, value in config_details.items():
                if key not in ['model_types', 'model_types_optimized']:  # Avoid duplication
                    formatted_key = key.replace('_', ' ').title()
                    print(f"{formatted_key}: {value}")
        
        # CROSS-VALIDATION RESULTS
        study = results.get('study')
        cv_results_displayed = False
        
        if study and hasattr(study, 'best_trial') and study.best_trial:
            best_trial = study.best_trial
            if hasattr(best_trial, 'user_attrs') and best_trial.user_attrs:
                user_attrs = best_trial.user_attrs
                
                # Cross-validation scores
                mean_cv_score = user_attrs.get('mean_cv_score')
                std_cv_score = user_attrs.get('std_cv_score')
                individual_scores = user_attrs.get('individual_fold_scores', [])
                valid_folds = user_attrs.get('valid_folds')
                
                if mean_cv_score is not None:
                    print(f"\n" + "="*60)
                    print("CROSS-VALIDATION RESULTS")
                    print("="*60)
                    cv_results_displayed = True
                    
                    print(f"Mean CV Score: {mean_cv_score:.6f}")
                    if std_cv_score is not None:
                        print(f"Standard Deviation: ±{std_cv_score:.6f}")
                        # Calculate confidence interval
                        confidence_lower = mean_cv_score - 1.96 * std_cv_score
                        confidence_upper = mean_cv_score + 1.96 * std_cv_score
                        print(f"95% Confidence Interval: [{confidence_lower:.6f}, {confidence_upper:.6f}]")
                    
                    if valid_folds is not None and individual_scores:
                        print(f"Valid Folds: {valid_folds}/{len(individual_scores)}")
                        
                        if len(individual_scores) > 1:
                            valid_scores = [s for s in individual_scores if s != float('inf') and s is not None]
                            if valid_scores:
                                print(f"Individual Fold Scores:")
                                for i, score in enumerate(individual_scores):
                                    if score != float('inf') and score is not None:
                                        print(f"  Fold {i+1}: {score:.6f}")
                                    else:
                                        print(f"  Fold {i+1}: Failed")
                                
                                # Calculate fold consistency metrics
                                if len(valid_scores) > 1:
                                    score_range = max(valid_scores) - min(valid_scores)
                                    relative_std = (std_cv_score / abs(mean_cv_score)) * 100 if mean_cv_score != 0 and std_cv_score else 0
                                    print(f"\nFold Consistency:")
                                    print(f"  Score Range: {score_range:.6f}")
                                    print(f"  Relative Std Dev: {relative_std:.1f}%")
                                    
                                    if relative_std < 5:
                                        consistency = "Excellent"
                                    elif relative_std < 10:
                                        consistency = "Good"
                                    elif relative_std < 20:
                                        consistency = "Fair"
                                    else:
                                        consistency = "Poor"
                                    print(f"  Consistency Rating: {consistency}")
                
                # Training time information
                fold_times = []
                for i in range(10):  # Check up to 10 folds
                    time_key = f'fold_{i}_training_time'
                    if time_key in user_attrs:
                        fold_times.append(user_attrs[time_key])
                
                if fold_times and cv_results_displayed:
                    avg_fold_time = sum(fold_times) / len(fold_times)
                    total_fold_time = sum(fold_times)
                    print(f"\nTraining Time Analysis:")
                    print(f"  Average per Fold: {avg_fold_time:.1f} minutes")
                    print(f"  Total Training Time: {total_fold_time:.1f} minutes")
                    
                    # Estimate time for different fold counts
                    if avg_fold_time > 0:
                        print(f"  Estimated Time for 3-fold: {avg_fold_time * 3:.1f} minutes")
                        print(f"  Estimated Time for 5-fold: {avg_fold_time * 5:.1f} minutes")
                        print(f"  Estimated Time for 10-fold: {avg_fold_time * 10:.1f} minutes")
        
        # DATA CONFIGURATION
        data_config = results.get('data_config', {})
        if data_config:
            print(f"\n" + "="*60)
            print("DATA CONFIGURATION")
            print("="*60)
            
            use_real_data = data_config.get('use_real_data', False)
            print(f"Data Source: {'Real Network Data' if use_real_data else 'Synthetic Data'}")
            
            if not use_real_data:
                normal_samples = data_config.get('normal_samples', 'N/A')
                attack_samples = data_config.get('attack_samples', 'N/A')
                if normal_samples != 'N/A' and attack_samples != 'N/A':
                    total_samples = normal_samples + attack_samples
                    attack_ratio = (attack_samples / total_samples * 100) if total_samples > 0 else 0
                    print(f"Dataset Size: {total_samples:,} samples")
                    print(f"  Normal: {normal_samples:,} samples")
                    print(f"  Attack: {attack_samples:,} samples ({attack_ratio:.1f}%)")
            
            features = data_config.get('features', 'N/A')
            if features != 'N/A':
                print(f"Features: {features}")
            
            normalization = data_config.get('normalization', 'N/A')
            if normalization != 'N/A':
                print(f"Normalization: {normalization}")
            
            cv_folds = data_config.get('cv_folds', 'N/A')
            if cv_folds != 'N/A':
                print(f"Cross-Validation Folds: {cv_folds}")
        
        # PERFORMANCE ANALYSIS
        analysis = results.get('analysis', {})
        if analysis:
            print(f"\n" + "="*60)
            print("PERFORMANCE ANALYSIS")
            print("="*60)
            
            # Display study summary metrics
            study_summary = analysis.get('study_summary', {})
            if study_summary:
                print("Study Summary:")
                for metric, value in study_summary.items():
                    formatted_metric = metric.replace('_', ' ').title()
                    if isinstance(value, float):
                        print(f"  {formatted_metric}: {value:.4f}")
                    else:
                        print(f"  {formatted_metric}: {value}")
            
            # Parameter importance analysis
            param_importance = analysis.get('parameter_importance', {})
            if param_importance:
                print(f"\nParameter Importance Analysis:")
                sorted_importance = sorted(param_importance.items(), key=lambda x: x[1], reverse=True)
                
                print(f"  Top 10 Most Important Parameters:")
                for i, (param, importance) in enumerate(sorted_importance[:10], 1):
                    # Create importance bar visualization
                    bar_length = int(importance * 20)  # Scale to 20 characters
                    bar = "█" * bar_length + "░" * (20 - bar_length)
                    print(f"    {i:2d}. {param:<25} │{bar}│ {importance:.4f}")
                
                if len(sorted_importance) > 10:
                    print(f"    ... and {len(sorted_importance) - 10} more parameters")
            
            # Optimization efficiency metrics
            if n_trials_total > 0:
                efficiency_metrics = {
                    'Trial Completion Rate': (n_trials_completed / n_trials_total) * 100,
                    'Trial Success Rate': ((n_trials_completed + n_trials_pruned) / n_trials_total) * 100 if n_trials_total > 0 else 0
                }
                
                if n_trials_pruned > 0:
                    efficiency_metrics['Pruning Effectiveness'] = (n_trials_pruned / n_trials_total) * 100
                
                print(f"\nOptimization Efficiency Metrics:")
                for metric, value in efficiency_metrics.items():
                    print(f"  {metric}: {value:.1f}%")
        
        # CONVERGENCE ANALYSIS
        optimization_history = results.get('optimization_history', [])
        if optimization_history and len(optimization_history) > 5:
            print(f"\n" + "="*60)
            print("CONVERGENCE ANALYSIS")
            print("="*60)
            
            # Extract valid values from optimization history
            valid_entries = [(i, entry) for i, entry in enumerate(optimization_history) 
                           if entry.get('value') is not None and entry.get('value') != float('inf')]
            
            if valid_entries:
                values = [entry[1]['value'] for entry in valid_entries]
                indices = [entry[0] for entry in valid_entries]
                
                if len(values) > 1:
                    initial_best = values[0]
                    final_best = min(values)
                    
                    # Calculate improvement
                    if initial_best != 0:
                        improvement = abs((final_best - initial_best) / initial_best) * 100
                        improvement_direction = "improvement" if final_best < initial_best else "degradation"
                    else:
                        improvement = 0
                        improvement_direction = "stable"
                    
                    print(f"Initial Best Value: {initial_best:.6f}")
                    print(f"Final Best Value: {final_best:.6f}")
                    print(f"Total Change: {improvement:.1f}% {improvement_direction}")
                    
                    # Find convergence point
                    best_value_final = min(values)
                    best_trial_idx = None
                    for i, value in enumerate(values):
                        if value == best_value_final:
                            best_trial_idx = indices[i]
                            break
                    
                    if best_trial_idx is not None:
                        convergence_point = (best_trial_idx + 1) / len(optimization_history) * 100
                        print(f"Best Value Found: Trial {best_trial_idx + 1} ({convergence_point:.1f}% through optimization)")
                        
                        # Assess convergence quality
                        if convergence_point < 25:
                            convergence_quality = "Early (Excellent search efficiency)"
                        elif convergence_point < 50:
                            convergence_quality = "Mid-stage (Good exploration)"
                        elif convergence_point < 75:
                            convergence_quality = "Late-stage (Thorough search)"
                        else:
                            convergence_quality = "Very late (May need more trials)"
                        
                        print(f"Convergence Assessment: {convergence_quality}")
                    
                    # Calculate convergence stability
                    if len(values) > 10:
                        last_10_values = values[-10:]
                        recent_variance = np.var(last_10_values) if len(last_10_values) > 1 else 0
                        print(f"Recent Optimization Stability: {recent_variance:.6f} (lower is better)")
        
        # SYSTEM CONFIGURATION
        system_config = results.get('system_config', {})
        if system_config:
            print(f"\n" + "="*60)
            print("SYSTEM CONFIGURATION")
            print("="*60)
            
            device = system_config.get('device', 'N/A')
            print(f"Compute Device: {device}")
            
            if system_config.get('random_seed') is not None:
                print(f"Random Seed: {system_config['random_seed']}")
            
            if system_config.get('num_workers') is not None:
                print(f"Data Loading Workers: {system_config['num_workers']}")
            
            if system_config.get('parallel_jobs', 1) > 1:
                print(f"Parallel Jobs: {system_config['parallel_jobs']}")
            
            # Display hardware utilization if available
            hardware_info = system_config.get('hardware_info', {})
            if hardware_info:
                print(f"Hardware Utilization:")
                for component, info in hardware_info.items():
                    if isinstance(info, dict) and 'utilization' in info:
                        print(f"  {component.title()}: {info['utilization']}")
        
        # SAVED ARTIFACTS AND FILES
        saved_files = results.get('saved_files', {})
        plots = results.get('plots', {})
        
        if saved_files or plots:
            print(f"\n" + "="*60)
            print("GENERATED ARTIFACTS")
            print("="*60)
            
            if saved_files:
                print("Study Files:")
                for file_type, file_path in saved_files.items():
                    file_name = file_type.replace('_', ' ').title()
                    print(f"  {file_name}: {file_path}")
                    
                    # Display file size if available
                    try:
                        if isinstance(file_path, (str, Path)):
                            file_size = Path(file_path).stat().st_size
                            if file_size > 1024 * 1024:
                                size_str = f"({file_size / (1024 * 1024):.1f} MB)"
                            elif file_size > 1024:
                                size_str = f"({file_size / 1024:.1f} KB)"
                            else:
                                size_str = f"({file_size} bytes)"
                            print(f"    Size: {size_str}")
                    except Exception:
                        pass
            
            if plots:
                print("\nVisualization Plots:")
                for plot_type, plot_path in plots.items():
                    plot_name = plot_type.replace('_', ' ').title()
                    print(f"  {plot_name}: {plot_path}")
        
        # RECOMMENDATIONS
        recommendations = results.get('recommendations', [])
        
        # Generate intelligent recommendations based on results
        auto_recommendations = []
        
        if best_value is not None and best_value != float('inf'):
            if hasattr(results.get('study'), 'direction') and results.get('study').direction.name == 'MAXIMIZE':
                if best_value < 0.7:
                    auto_recommendations.append("Consider expanding search space or increasing trial count for better performance")
            else:  # Minimize direction
                if best_value > 0.1:
                    auto_recommendations.append("High objective value suggests need for search space refinement or more trials")
                elif best_value < 0.01:
                    auto_recommendations.append("Excellent results achieved - ready for production deployment")
        
        if completion_rate < 60:
            auto_recommendations.append("Low completion rate - consider simplifying model or increasing resources")
        
        if n_trials_failed > n_trials_completed // 2:
            auto_recommendations.append("High failure rate detected - review error logs and system resources")
        
        # Combine user recommendations with auto-generated ones
        all_recommendations = list(recommendations) + auto_recommendations
        
        if all_recommendations:
            print(f"\n" + "="*60)
            print("RECOMMENDATIONS")
            print("="*60)
            
            for i, rec in enumerate(all_recommendations, 1):
                print(f"{i}. {rec}")
        
        # FINAL MODEL TRAINING RESULTS
        final_model_training = results.get('final_model_training', {})
        if final_model_training:
            print(f"\n" + "="*60)
            print("FINAL MODEL TRAINING")
            print("="*60)
            
            if final_model_training.get('success', False):
                print("Status: SUCCESS")
                
                training_time = final_model_training.get('training_time_minutes', 0)
                if training_time > 0:
                    print(f"Training Time: {training_time:.1f} minutes")
                
                final_metrics = final_model_training.get('final_metrics', {})
                if final_metrics:
                    print(f"Final Model Performance:")
                    for metric, value in final_metrics.items():
                        metric_name = metric.replace('_', ' ').title()
                        if isinstance(value, float):
                            print(f"  {metric_name}: {value:.6f}")
                        else:
                            print(f"  {metric_name}: {value}")
                
                artifacts = final_model_training.get('artifacts', {})
                if artifacts:
                    print(f"Model Artifacts:")
                    for artifact_type, path in artifacts.items():
                        artifact_name = artifact_type.replace('_', ' ').title()
                        print(f"  {artifact_name}: {path}")
            else:
                print("Status: FAILED")
                error = final_model_training.get('error', 'Unknown error')
                print(f"Error: {error}")
        
        # OPTIMIZATION SUMMARY AND NEXT STEPS
        print(f"\n" + "="*80)
        print("OPTIMIZATION SUMMARY & NEXT STEPS")
        print("="*80)
        
        if n_trials_completed > 0 and best_value is not None and best_value != float('inf'):
            print("Hyperparameter optimization completed successfully!")
            print(f"Found optimal configuration with objective value: {best_value:.6f}")
            print(f"Completed {n_trials_completed} trials in {total_time_minutes:.1f} minutes" if 'total_time_minutes' in locals() else f"Completed {n_trials_completed} trials")
            
            # Performance assessment and recommendations
            objective_quality = "Unknown"
            if hasattr(results.get('study'), 'direction') and results.get('study').direction.name == 'MAXIMIZE':
                if best_value > 0.9:
                    objective_quality = "Outstanding"
                elif best_value > 0.8:
                    objective_quality = "Excellent"
                elif best_value > 0.7:
                    objective_quality = "Good"
                else:
                    objective_quality = "Needs Improvement"
            else:  # Minimize direction
                if best_value < 0.01:
                    objective_quality = "Outstanding"
                elif best_value < 0.05:
                    objective_quality = "Excellent"
                elif best_value < 0.1:
                    objective_quality = "Good"
                else:
                    objective_quality = "Needs Improvement"
            
            print(f"Performance Quality: {objective_quality}")
            
            print(f"\nRecommended Next Steps:")
            if objective_quality in ["Outstanding", "Excellent"]:
                print(f"  1. Configuration is production-ready")
                print(f"  2. Deploy model with optimized parameters")
                print(f"  3. Monitor performance on real-world data")
                print(f"  4. Set up automated retraining pipeline")
            elif objective_quality == "Good":
                print(f"  1. Review optimization plots for insights")
                print(f"  2. Consider extended optimization with more trials")
                print(f"  3. Fine-tune search space based on parameter importance")
                print(f"  4. Test configuration on validation dataset")
            else:
                print(f"  1. Analyze failed and pruned trials for patterns")
                print(f"  2. Expand search space or adjust ranges")
                print(f"  3. Check system resources and configuration")
                print(f"  4. Consider different optimization strategy")
            
            if final_model_training and final_model_training.get('success'):
                print(f"  5. Final trained model is ready for deployment")
            elif not final_model_training:
                print(f"  5. Train final model using optimized parameters")
            
            if plots:
                print(f"  6. Review generated optimization visualizations")
        else:
            print("Hyperparameter optimization completed with limited success:")
            if n_trials_completed == 0:
                print("   - No trials completed successfully")
                print("   - Check configuration, data paths, and system resources")
            else:
                print(f"   - Only {n_trials_completed} out of {n_trials_total} trials completed")
                print("   - Consider adjusting search space or increasing timeout")
            
            print(f"\nTroubleshooting Steps:")
            print(f"  1. Review error logs and failed trial information")
            print(f"  2. Simplify search space or reduce model complexity")
            print(f"  3. Check data availability and preprocessing")
            print(f"  4. Verify system resources (CPU, memory, GPU)")
            print(f"  5. Try using preset configurations for stable baseline")
        
        # Study continuation information
        study_dir = results.get('study_dir')
        if study_dir and saved_files:
            print(f"\nStudy Continuation:")
            print(f"   Study data saved to: {study_dir}")
            print(f"   Use 'Continue Existing Study' option to resume with additional trials")
            print(f"   Saved configurations can be loaded for similar optimizations")
        
        # Final summary statistics
        if 'total_time_minutes' in locals() and total_time_minutes > 0:
            trials_per_minute = n_trials_completed / total_time_minutes if total_time_minutes > 0 else 0
            print(f"\nPerformance Statistics:")
            print(f"   Throughput: {trials_per_minute:.2f} trials per minute")
            
            if best_value is not None and best_value != float('inf') and n_trials_completed > 0:
                value_improvement_rate = abs(best_value) / total_time_minutes if total_time_minutes > 0 else 0
                print(f"   Optimization Rate: {value_improvement_rate:.6f} improvement per minute")
        
        print("="*80)
        
    except Exception as e:
        logger.error(f"Error displaying HPO results: {e}", exc_info=True)
        print(f"\nError displaying results: {str(e)}")
        
        # Fallback display for critical information
        try:
            print(f"\nBasic Results Summary:")
            print(f"   Success: {results.get('success', 'Unknown')}")
            print(f"   Study Name: {results.get('study_name', 'Unknown')}")
            
            n_completed = results.get('n_trials_completed', 0)
            if n_completed > 0:
                print(f"   Trials Completed: {n_completed}")
                best_value = results.get('best_value')
                if best_value and best_value != float('inf'):
                    print(f"   Best Value: {best_value:.6f}")
            
            error = results.get('error')
            if error:
                print(f"   Error: {error}")
            
            # Show saved files if available
            saved_files = results.get('saved_files', {})
            if saved_files:
                print(f"   Saved Files:")
                for file_type, path in saved_files.items():
                    print(f"     {file_type}: {path}")
                    
        except Exception as fallback_error:
            logger.error(f"Fallback display also failed: {fallback_error}")
            print("Unable to display results due to formatting errors")
            print("Check the raw results dictionary for detailed information")
            print("Review log files for error details and debugging information")

def export_to_onnx(
    model: nn.Module,
    input_dim: int,
    device: torch.device,
    model_dir: Path = None,
    opset_version: int = None,
    config: Optional[Dict] = None
) -> Optional[Path]:
    """
    Export model to ONNX format with memory protection and comprehensive error handling.
    
    Args:
        model: PyTorch model to export
        input_dim: Input dimension 
        device: Device to use
        model_dir: Directory to save ONNX model
        opset_version: ONNX opset version
        config: Configuration dict with export settings
    
    Returns:
        Path to saved ONNX model or None if export fails
    """
    # Initialize memory tracking
    memory_stats = {
        'initial_ram': psutil.virtual_memory().used / (1024**3),
        'initial_vram': torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0
    }
    
    def log_memory_usage(stage: str):
        """Log current memory usage at different stages"""
        mem = psutil.virtual_memory()
        memory_stats[f'{stage}_ram'] = mem.used / (1024**3)
        if torch.cuda.is_available():
            memory_stats[f'{stage}_vram'] = torch.cuda.memory_allocated() / (1024**3)
        logger.debug(f"Memory at {stage}: RAM {memory_stats[f'{stage}_ram']:.2f}GB | "
                    f"VRAM {memory_stats.get(f'{stage}_vram', 0):.2f}GB")

    try:
        # Load configuration with memory limits
        if config is None:
            try:
                config = get_current_config()
            except Exception:
                config = {}
        
        system_config = config.get('system', {})
        export_config = system_config.get('onnx_export', {})
        
        # Apply memory-aware configuration 
        max_ram_usage = export_config.get('max_ram_gb', 8)
        max_vram_usage = export_config.get('max_vram_gb', 2) if torch.cuda.is_available() else 0
        chunk_size = export_config.get('chunk_size', min(128, input_dim))

        # Verify system resources before starting
        current_ram = psutil.virtual_memory().used / (1024**3)
        current_vram = torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0
        
        if current_ram > max_ram_usage * 0.7:
            raise MemoryError(f"High RAM usage detected: {current_ram:.1f}/{max_ram_usage}GB")
        if current_vram > max_vram_usage * 0.8:
            raise MemoryError(f"High VRAM usage detected: {current_vram:.1f}/{max_vram_usage}GB")

        # Apply configuration with parameter precedence
        if model_dir is None:
            model_dir = Path(system_config.get('model_dir', DEFAULT_MODEL_DIR))
        if opset_version is None:
            opset_version = export_config.get('opset_version', 14)
        
        model.eval()
        onnx_path = model_dir / "autoencoder_ids.onnx"
        
        logger.info(f"Exporting model to ONNX format at {onnx_path}")
        log_memory_usage('pre_export')

        # Memory protection context
        with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):
            # Prepare directory and inputs with memory cleanup
            try:
                model_dir.mkdir(parents=True, exist_ok=True)
                
                # Create dummy input in chunks to reduce memory spikes
                dummy_input = None
                try:
                    chunks = []
                    for i in range(0, input_dim, chunk_size):
                        chunk = torch.randn(1, min(chunk_size, input_dim - i), device='cpu')
                        chunks.append(chunk)
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None
                    
                    dummy_input = torch.cat(chunks, dim=1).to(device)
                    del chunks
                except Exception as e:
                    if dummy_input is not None:
                        del dummy_input
                    raise MemoryError(f"Input creation failed: {str(e)}")

                log_memory_usage('post_input_creation')

                # Export with memory monitoring
                try:
                    torch.onnx.export(
                        model,
                        dummy_input,
                        onnx_path,
                        opset_version=opset_version,
                        input_names=["input"],
                        output_names=["output"],
                        dynamic_axes=export_config.get('dynamic_axes', {'input': {0: 'batch_size'}}),
                        do_constant_folding=export_config.get('constant_folding', True),
                        export_params=True,
                        verbose=export_config.get('verbose', False),
                        training=torch.onnx.TrainingMode.EVAL,
                        operator_export_type=torch.onnx.OperatorExportTypes.ONNX
                    )
                finally:
                    del dummy_input
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None

                log_memory_usage('post_export')

                # Validation with memory protection
                validation_result = None
                if export_config.get('runtime_validation', True) and ONNXRUNTIME_AVAILABLE:
                    validation_result = validate_onnx_model(
                        model, onnx_path, device, 
                        tolerance=export_config.get('validation_tolerance', 1e-5),
                        strict=export_config.get('strict_validation', False)
                    )
                
                # Generate metadata with memory info
                metadata = create_export_metadata(
                    model, onnx_path, config, validation_result, memory_stats
                )
                
                save_metadata(metadata, model_dir / "onnx_export_metadata.json")
                logger.info(f"Export completed: {onnx_path}")
                return onnx_path

            except Exception as e:
                # Cleanup any partial files
                if onnx_path.exists():
                    try:
                        onnx_path.unlink()
                    except:
                        pass
                raise

    except MemoryError as e:
        logger.error(f"Memory error during export: {str(e)}")
        log_memory_usage('error')
        if export_config.get('fail_silently', False):
            return None
        raise
    except Exception as e:
        logger.error(f"Export failed: {str(e)}")
        if export_config.get('fail_silently', False):
            return None
        raise RuntimeError(f"ONNX export failed: {str(e)}") from e
    finally:
        # Force cleanup
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

def validate_onnx_model(
    model: nn.Module, 
    onnx_path: Path, 
    device: torch.device,
    tolerance: float = 1e-5,
    strict: bool = False
) -> Dict:
    """
    Memory-safe ONNX model validation with comprehensive error handling.
    
    Args:
        model: PyTorch model to validate against
        onnx_path: Path to ONNX model file
        device: Device to use for validation
        tolerance: Maximum allowed difference between outputs
        strict: Whether to raise error on validation failure
    
    Returns:
        Dict containing validation results and status
    """
    validation_result = {
        'status': 'skipped',
        'max_difference': None,
        'error': None
    }
    
    try:
        # Load in memory-safe way
        with warnings.catch_warnings(), torch.no_grad():
            # 1. Verify ONNX model structure
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)
            
            # 2. Runtime validation
            ort_session = ort.InferenceSession(
                str(onnx_path),
                providers=['CPUExecutionProvider']
            )
            
            # Create test input in chunks to reduce memory usage
            input_shape = onnx_model.graph.input[0].type.tensor_type.shape.dim[1].dim_value
            test_input = torch.randn(1, input_shape, device='cpu').numpy()
            
            # Run comparison with memory safety
            ort_output = ort_session.run(None, {'input': test_input})[0]
            
            # Move model output to CPU and detach gradients 
            with torch.no_grad():
                torch_output = model(torch.from_numpy(test_input).to(device))
                torch_output = torch_output.cpu().numpy()
            
            # Validate shapes before numerical comparison
            if ort_output.shape != torch_output.shape:
                error_msg = f"Shape mismatch: ONNX {ort_output.shape} vs PyTorch {torch_output.shape}"
                if strict:
                    raise RuntimeError(error_msg)
                validation_result.update({
                    'status': 'failed',
                    'error': error_msg
                })
            else:
                # Convert to numpy arrays of same dtype for comparison
                ort_output = ort_output.astype(np.float32)
                torch_output = torch_output.astype(np.float32)
                
                # Calculate max difference safely
                max_diff = np.abs(ort_output - torch_output).max()
                validation_result['max_difference'] = float(max_diff)
                
                if max_diff > tolerance:
                    error_msg = f"Numerical difference {max_diff:.2e} > tolerance {tolerance:.1e}"
                    if strict:
                        raise RuntimeError(error_msg)
                    validation_result.update({
                        'status': 'warning',
                        'error': error_msg
                    })
                else:
                    validation_result['status'] = 'passed'
    
    except Exception as e:
        validation_result.update({
            'status': 'failed',
            'error': str(e)
        })
        if strict:
            raise
    
    return validation_result

def create_export_metadata(
    model: nn.Module,
    onnx_path: Path,
    config: Dict,
    validation_result: Optional[Dict],
    memory_stats: Dict
) -> Dict:
    """Generate comprehensive export metadata"""
    return {
        "export_timestamp": datetime.now().isoformat(),
        "model_type": type(model).__name__,
        "input_dim": getattr(model, 'input_dim', 'unknown'),
        "opset_version": config.get('system', {}).get('onnx_export', {}).get('opset_version', 14),
        "file_size_mb": onnx_path.stat().st_size / (1024**2),
        "memory_usage": memory_stats,
        "validation": validation_result or {'status': 'not_performed'},
        "system": {
            "python_version": sys.version,
            "pytorch_version": torch.__version__,
            "onnx_version": onnx.__version__,
            "onnxruntime_available": ONNXRUNTIME_AVAILABLE,
            "device": str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
        },
        "config": config.get('system', {}).get('onnx_export', {})
    }

def save_metadata(metadata: Dict, path: Path) -> None:
    """Safely save metadata with error handling"""
    try:
        with open(path, 'w') as f:
            json.dump(metadata, f, indent=2)
        logger.info(f"Metadata saved to {path}")
    except Exception as e:
        logger.error(f"Failed to save metadata: {str(e)}")

def save_tensorboard_data(
    writer: SummaryWriter,
    save_dir: Path = None,
    filename_suffix: str = "",
    config: Optional[Dict] = None
) -> Dict[str, Path]:
    """Save TensorBoard event data in multiple readable formats with configuration integration."""
    # Load configuration
    if config is None:
        try:
            config = get_current_config()
        except Exception:
            config = {}
    
    monitoring_config = config.get('monitoring', {})
    tensorboard_config = monitoring_config.get('tensorboard', {})
    
    # Apply configuration with parameter precedence
    if save_dir is None:
        save_dir = Path(monitoring_config.get('tensorboard_dir', TB_DIR))
    
    export_formats = tensorboard_config.get('export_formats', ['json', 'csv'])
    include_histograms = tensorboard_config.get('include_histograms', False)
    include_images = tensorboard_config.get('include_images', False)
    
    logger.info(f"Saving TensorBoard data to: {save_dir}")
    logger.info(f"Export formats: {export_formats}")
    
    saved_files = {}
    
    try:
        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
    except ImportError:
        logger.warning("Could not import EventAccumulator - skipping TensorBoard data export")
        return saved_files

    try:
        # Ensure save directory exists
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # Get the event file path from the SummaryWriter
        event_files = list(Path(writer.log_dir).glob('events.out.tfevents.*'))
        if not event_files:
            logger.warning("No TensorBoard event files found")
            return saved_files
        
        # Use the most recent event file
        event_file = max(event_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"Processing event file: {event_file}")
        
        # Load the event data with size guidance
        size_guidance = {
            'scalars': tensorboard_config.get('max_scalars', 1000),
            'histograms': tensorboard_config.get('max_histograms', 100) if include_histograms else 0,
            'images': tensorboard_config.get('max_images', 10) if include_images else 0,
        }
        
        event_acc = EventAccumulator(str(event_file), size_guidance=size_guidance)
        event_acc.Reload()
        
        # Extract scalar data
        tb_data = {
            'metadata': {
                'export_timestamp': datetime.now().isoformat(),
                'source_file': str(event_file),
                'tensorflow_version': getattr(event_acc, '_tensorflow_version', 'unknown'),
                'export_config': tensorboard_config
            },
            'scalars': {},
            'tags_metadata': {}
        }
        
        # Process scalar data
        scalar_tags = event_acc.Tags().get('scalars', [])
        logger.info(f"Found {len(scalar_tags)} scalar tags")
        
        for tag in scalar_tags:
            try:
                events = event_acc.Scalars(tag)
                tb_data['scalars'][tag] = {
                    'steps': [e.step for e in events],
                    'values': [e.value for e in events],
                    'wall_times': [e.wall_time for e in events],
                    'count': len(events)
                }
                tb_data['tags_metadata'][tag] = {
                    'type': 'scalar',
                    'first_step': events[0].step if events else 0,
                    'last_step': events[-1].step if events else 0,
                    'min_value': min(e.value for e in events) if events else 0,
                    'max_value': max(e.value for e in events) if events else 0
                }
            except Exception as e:
                logger.warning(f"Failed to process scalar tag '{tag}': {str(e)}")
        
        # Process histograms if requested
        if include_histograms:
            histogram_tags = event_acc.Tags().get('histograms', [])
            if histogram_tags:
                tb_data['histograms'] = {}
                logger.info(f"Processing {len(histogram_tags)} histogram tags")
                
                for tag in histogram_tags:
                    try:
                        events = event_acc.Histograms(tag)
                        tb_data['histograms'][tag] = [
                            {
                                'step': e.step,
                                'wall_time': e.wall_time,
                                'bucket_limits': e.histogram_value.bucket_limit,
                                'bucket_counts': e.histogram_value.bucket,
                                'min': e.histogram_value.min,
                                'max': e.histogram_value.max,
                                'sum': e.histogram_value.sum,
                                'count': e.histogram_value.num
                            }
                            for e in events
                        ]
                    except Exception as e:
                        logger.warning(f"Failed to process histogram tag '{tag}': {str(e)}")
        
        # Save in requested formats
        base_filename = f"tensorboard_data{filename_suffix}"
        
        # JSON format
        if 'json' in export_formats:
            json_path = save_dir / f"{base_filename}.json"
            with open(json_path, 'w') as f:
                json.dump(tb_data, f, indent=2, default=str)
            saved_files['json'] = json_path
            logger.info(f"[INFO] Saved JSON data: {json_path}")
        
        # CSV format for scalars
        if 'csv' in export_formats and tb_data['scalars']:
            csv_path = save_dir / f"{base_filename}.csv"
            try:
                # Create a flattened DataFrame
                csv_data = []
                for tag, values in tb_data['scalars'].items():
                    for step, value, wall_time in zip(values['steps'], values['values'], values['wall_times']):
                        csv_data.append({
                            'tag': tag,
                            'step': step,
                            'value': value,
                            'wall_time': wall_time,
                            'timestamp': datetime.fromtimestamp(wall_time).isoformat()
                        })
                
                df = pd.DataFrame(csv_data)
                df.to_csv(csv_path, index=False)
                saved_files['csv'] = csv_path
                logger.info(f"[INFO] Saved CSV data: {csv_path}")
            except Exception as e:
                logger.warning(f"Could not save CSV format: {str(e)}")
        
        # Parquet format for efficient storage
        if 'parquet' in export_formats and tb_data['scalars']:
            try:
                parquet_path = save_dir / f"{base_filename}.parquet"
                # Similar to CSV but save as Parquet
                parquet_data = []
                for tag, values in tb_data['scalars'].items():
                    for step, value, wall_time in zip(values['steps'], values['values'], values['wall_times']):
                        parquet_data.append({
                            'tag': tag,
                            'step': step,
                            'value': value,
                            'wall_time': wall_time
                        })
                
                df = pd.DataFrame(parquet_data)
                df.to_parquet(parquet_path, index=False)
                saved_files['parquet'] = parquet_path
                logger.info(f"[INFO] Saved Parquet data: {parquet_path}")
            except Exception as e:
                logger.warning(f"Could not save Parquet format: {str(e)}")
        
        # Save summary statistics
        if tensorboard_config.get('save_summary', True):
            summary = {
                'export_summary': {
                    'timestamp': datetime.now().isoformat(),
                    'total_scalar_tags': len(tb_data['scalars']),
                    'total_data_points': sum(len(v['values']) for v in tb_data['scalars'].values()),
                    'file_size_mb': event_file.stat().st_size / 1024 / 1024,
                    'time_range': {
                        'first_event': min(
                            min(v['wall_times']) for v in tb_data['scalars'].values() 
                            if v['wall_times']
                        ) if tb_data['scalars'] else 0,
                        'last_event': max(
                            max(v['wall_times']) for v in tb_data['scalars'].values() 
                            if v['wall_times']
                        ) if tb_data['scalars'] else 0
                    },
                    'exported_formats': list(saved_files.keys()),
                    'tags_summary': tb_data['tags_metadata']
                }
            }
            
            summary_path = save_dir / f"{base_filename}_summary.json"
            with open(summary_path, 'w') as f:
                json.dump(summary, f, indent=2, default=str)
            saved_files['summary'] = summary_path
            logger.info(f"[INFO] Saved summary: {summary_path}")
        
        logger.info(f"[INFO] TensorBoard data export complete ({len(saved_files)} files)")
        return saved_files
        
    except Exception as e:
        logger.error(f"Failed to save TensorBoard data: {str(e)}")
        return saved_files

def prompt_user(prompt: str, default: bool = True) -> bool:
    """Interactive user prompt with default handling."""
    while True:
        response = input(f"{prompt} [{'Y/n' if default else 'y/N'}]: ").strip().lower()
        if not response:
            return default
        if response in ('y', 'yes'):
            return True
        if response in ('n', 'no'):
            return False
        print("Please answer yes/y or no/n")

def show_banner(return_config=False) -> None:
    """Display the application banner"""
    # ASCII art banner
    console.print("\n" , Panel.fit(
        """
                            
⠀⠀⠀⠀⠀⠀⠀⢀⣠⣤⣠⣶⠚⠛⠿⠷⠶⣤⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⢀⣴⠟⠉⠀⠀⢠⡄⠀⠀⠀⠀⠀⠉⠙⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⢀⡴⠛⠁⠀⠀⠀⠀⠘⣷⣴⠏⠀⠀⣠⡄⠀⠀⢨⡇⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠺⣇⠀⠀⠀⠀⠀⠀⠀⠘⣿⠀⠀⠘⣻⣻⡆⠀⠀⠙⠦⣄⣀⠀⠀⠀⠀
⠀⠀⠀⢰⡟⢷⡄⠀⠀⠀⠀⠀⠀⢸⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⢻⠶⢤⡀
⠀⠀⠀⣾⣇⠀⠻⣄⠀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣀⣴⣿
⠀⠀⢸⡟⠻⣆⠀⠈⠳⢄⡀⠀⠀⡼⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠶⠶⢤⣬⡿⠁
⠀⢀⣿⠃⠀⠹⣆⠀⠀⠀⠙⠓⠿⢧⡀⠀⢠⡴⣶⣶⣒⣋⣀⣀⣤⣶⣶⠟⠁⠀
⠀⣼⡏⠀⠀⠀⠙⠀⠀⠀⠀⠀⠀⠀⠙⠳⠶⠤⠵⣶⠒⠚⠻⠿⠋⠁⠀⠀⠀⠀
⢰⣿⡇⠀⠀⠀⠀⠀⠀⠀⣆⠀⠀⠀⠀⠀⠀⠀⢠⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⢿⡿⠁⠀⠀⠀⠀⠀⠀⠀⠘⣦⡀⠀⠀⠀⠀⠀⢸⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣷⡄⠀⠀⠀⠀⣿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢷⡀⠀⠀⠀⢸⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣿⠇⠀⠀⠀⠀⠀⠀⠀

    """,
        style="bold cyan", 
        title="[bold yellow]GreyChamp | IDS[/]", 
        subtitle="[magenta]DEEP LEARNING SUITE[/]",
        border_style="bold blue",
        box=box.DOUBLE,
        padding=(1, 2)
    ))
    
    print(Fore.CYAN + Style.BRIGHT + "\n" + "=" *40)
    print(Fore.GREEN + Style.BRIGHT + "  - Interactive Mode -  ".center(40))
    print(Fore.CYAN + Style.BRIGHT + "="*40 + Style.RESET_ALL)
    
    try:
        # Get configuration status with error handling
        config = get_current_config()
        metadata_config = config.get('metadata', {})
        training_config = config.get('training', {})
        model_config = config.get('model', {})
        security_config = config.get('security', {})
        data_config = config.get('data', {})
        monitoring_config = config.get('monitoring', {})
        hardware_config = config.get('hardware', {})
        system_config = config.get('system', {})
        presets_config = config.get('presets', {})
        hpo_config = config.get('hyperparameter_optimization', {})
        validation_config = config.get('validation', {})
        experimental_config = config.get('experimental', {})
        
        # Multiple fallback approaches for determining preset name
        preset_name = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Unknown")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Unknown", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Unknown")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Unknown", None, ""]:
            preset_name = config.get("_preset_name", "Unknown")
        
        # Method 4: Check runtime information
        if preset_name in ["Unknown", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Unknown")
        
        # Determine configuration source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Clean up preset name display
        if preset_name in ["Unknown", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Get additional configuration info
        model_type = "Unknown"
        total_sections = len(config) if isinstance(config, dict) else 0
        
        try:
            model_section = config.get("model", {})
            if isinstance(model_section, dict):
                model_type = model_section.get("model_type", "Unknown")
        except Exception:
            model_type = "Unknown"
        
        # Get configuration health if available
        if "runtime" in config and isinstance(config["runtime"], dict):
            runtime = config["runtime"]
            if "configuration_health" in runtime:
                health = runtime["configuration_health"]
                health_status = health.get("status", "unknown")
                
                # Color code health status
                if health_status in ["healthy", "passed"]:
                    health_color = Fore.GREEN
                elif health_status in ["needs_attention", "warning"]:
                    health_color = Fore.YELLOW
                else:
                    health_color = Fore.RED
        
        if return_config:
            return config
        
    except Exception as e:
        logger.warning(f"Error getting configuration status: {e}")
        print(Fore.RED + Style.BRIGHT + f"Error getting configuration status: {e}\n")
        if return_config:
            return {}

def print_main_menu(config: Optional[Dict[str, Any]] = None):
    """Print the main menu options with context display."""
    
    # Get configuration context if available
    preset_name = "Custom/Default"
    model_type = "Unknown"
    config_source = "Unknown"
    
    if config:
        # Extract preset name
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Extract model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
    
    # menu header with context
    print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
    print(Fore.CYAN + Style.BRIGHT + "MAIN APPLICATION MENU")
    print(Fore.YELLOW + Style.BRIGHT + "-"*40)
    print(Fore.GREEN + Style.BRIGHT + f"Active Context:")
    print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
    print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
    print(Fore.WHITE + Style.BRIGHT + f"  └─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}")
    
    # Print menu options with descriptions
    print(Fore.YELLOW + Style.BRIGHT + "\nCore Functions:")
    print(Fore.WHITE + Style.BRIGHT + "1. Model Training " + Fore.GREEN + Style.BRIGHT + "(Train & Evaluate Models)")
    print(Fore.WHITE + Style.BRIGHT + "2. Hyperparameter Optimization " + Fore.GREEN + Style.BRIGHT + "(Auto-tune Parameters)")
    print(Fore.WHITE + Style.BRIGHT + "3. Model Architecture Comparison " + Fore.GREEN + Style.BRIGHT + "(Compare Performance)")
    print(Fore.WHITE + Style.BRIGHT + "4. Configuration Management " + Fore.GREEN + Style.BRIGHT + "(Manage Settings)")
    print(Fore.WHITE + Style.BRIGHT + "5. System Information " + Fore.GREEN + Style.BRIGHT + "(Hardware & Resources)")
    print(Fore.WHITE + Style.BRIGHT + "6. Performance Benchmark " + Fore.GREEN + Style.BRIGHT + "(Speed & Memory Tests)")
    print(Fore.WHITE + Style.BRIGHT + "7. Model Analysis & Visualization " + Fore.GREEN + Style.BRIGHT + "(Metrics & Charts)")
    print(Fore.WHITE + Style.BRIGHT + "8. Advanced Tools " + Fore.GREEN + Style.BRIGHT + "(Expert Features)")
    print(Fore.WHITE + Style.BRIGHT + "9. View Initialization Reports " + Fore.GREEN + Style.BRIGHT + "(System Status)")
    print(Fore.RED + Style.BRIGHT + "0. Exit Application")

def interactive_main():
    """Main interactive interface with banner integration."""
    # Clear any residual input buffer from system initialization
    if hasattr(sys.stdin, 'flush'):
        try:
            sys.stdin.flush()
        except:
            pass
    
    # Small delay to ensure all output is complete
    time.sleep(0.2)
    
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Print main menu with context
        print_main_menu(config)
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-9): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nExiting...")
                print(Fore.YELLOW + Style.BRIGHT + "Goodbye!")
                return
        
        try:
            if choice == "1":
                try:
                    model_training_menu(config)
                except Exception as e:
                    message = (
                        f"Error encountered while showing model training menu: {str(e)}\n"
                        f"Context:\n"
                        f"- Current Preset: {config.get('presets', {}).get('current_preset', 'Custom/Default')}\n"
                        f"- Model Type: {config.get('model', {}).get('model_type', 'Unknown')}\n\n"
                        f"This could be due to:\n"
                        f"- Configuration file corruption\n"
                        f"- Missing model dependencies\n"
                        f"- System resource constraints\n"
                        f"- Data availability issues"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="TRAINING MENU ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
            elif choice == "2":
                try:
                    console.clear()
                    
                    current_preset = config.get('presets', {}).get('current_preset', 'Custom/Default')
                    model_type = config.get('model', {}).get('model_type', 'Unknown')
                    
                    message = (
                        f"Hyperparameter Optimization (HPO) is a powerful tool\n"
                        f"that allows automatic tuning of the model's parameters\n"
                        f"to achieve better performance.\n\n"
                        f"Current Context:\n"
                        f"- Active Preset: {current_preset}\n"
                        f"- Model Type: {model_type}\n\n"
                        f"The HPO menu provides various optimization strategies:\n"
                        f"- Express setup for quick optimization\n"
                        f"- Custom configuration for advanced users\n"
                        f"- Preset configurations for different scenarios\n"
                        f"- Model comparison and analysis tools\n\n"
                        f"HPO can significantly improve model accuracy and efficiency\n"
                        f"by finding optimal settings for your specific dataset and task."
                    )
                    console.print(
                        Panel.fit(
                            f"[bold white]{message}[/bold white]",
                            title="HYPERPARAMETER OPTIMIZATION",
                            style="bold yellow",
                            border_style="yellow",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    # Small delay to ensure panel is rendered before proceeding
                    time.sleep(3)
                    
                    console.clear()
                    
                    hpo_training_menu(config)
                except Exception as e:
                    message = (
                        f"Error encountered while showing HPO training menu: {str(e)}\n"
                        f"Context:\n"
                        f"- Current Configuration: {config.get('presets', {}).get('current_preset', 'Custom/Default')}\n\n"
                        f"Please check:\n"
                        f"- HPO dependencies are installed\n"
                        f"- Configuration files are valid\n"
                        f"- System has sufficient resources"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="HPO MENU ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "3":
                try:
                    display_model_comparison()
                except Exception as e:
                    message = (
                        f"Error encountered while displaying model comparison: {str(e)}\n"
                        "Please check configuration and try again.\n\n"
                        "Ensure:\n"
                        "- Multiple models are configured\n"
                        "- Comparison data is available\n"
                        "- Visualization dependencies are installed"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="MODEL COMPARISON ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "4":
                try:
                    configuration_menu()
                except Exception as e:
                    message = (
                        f"Error encountered while showing configuration menu: {str(e)}\n"
                        "Please check configuration files and permissions."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="CONFIGURATION ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "5":
                try:
                    show_system_info()
                except Exception as e:
                    message = (
                        f"Error encountered while showing system information: {str(e)}\n"
                        "Please check system dependencies and permissions."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="SYSTEM INFO ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "6":
                try:
                    run_performance_benchmark_interactive()
                except Exception as e:
                    message = (
                        f"Error encountered while running performance benchmark: {str(e)}\n"
                        "Please check configuration and system resources."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="BENCHMARK ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "7":
                try:
                    model_analysis_menu()
                except Exception as e:
                    message = (
                        f"Error encountered while showing model analysis menu: {str(e)}\n"
                        "Please check configuration and visualization dependencies."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="ANALYSIS ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "8":
                try:
                    advanced_tools_menu()
                except Exception as e:
                    message = (
                        f"Error encountered while showing advanced tools menu: {str(e)}\n"
                        "Please check configuration and expert feature dependencies."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="ADVANCED TOOLS ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "9":
                try:
                    show_init_report()
                except Exception as e:
                    message = (
                        f"Error encountered while showing initialization reports: {str(e)}\n"
                        "Please check if reports exist and try again."
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            style="bold red",
                            title="REPORTS ERROR",
                            border_style="bold red",
                            box=box.ROUNDED,
                            padding=(1, 2)
                        )
                    )
            elif choice == "0":
                console.clear()
                # Exit message
                console.print(
                    Panel.fit(
                        "Thank you for using GreyChamp IDS Deep Learning Suite!",
                        title="EXIT APPLICATION",
                        style="bold yellow",
                        border_style="yellow",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                print(Fore.RED + Style.BRIGHT + "\nExiting...")
                print(Fore.YELLOW + Style.BRIGHT + "Goodbye!")
                break
            else:
                print(Fore.RED + Style.BRIGHT + f"\nInvalid selection '{choice}'. Please enter a number from 0-9.")
            
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nOperation interrupted by user")
        except Exception as e:
            logger.error(f"Main menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in main menu: {str(e)}\n"
                f"Context:\n"
                f"- Selected Option: {choice}\n"
                f"- Current Preset: {config.get('presets', {}).get('current_preset', 'Custom/Default')}\n"
                f"- Model Type: {config.get('model', {}).get('model_type', 'Unknown')}\n\n"
                f"This could indicate:\n"
                f"- System resource issues\n"
                f"- Configuration problems\n"
                f"- Dependency conflicts\n"
                f"- Data corruption\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="MAIN MENU ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nExiting...")
                print(Fore.YELLOW + Style.BRIGHT + "Goodbye!")
                break

def model_training_menu(config: Optional[Dict[str, Any]] = None):
    """Menu for model training options with context display and error handling."""
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        model_config = config.get('model', {})
        system_config = config.get('system', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        preset_count = len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        epoch_count = training_config.get('epochs', 'Default')
        batch_count = training_config.get('batch_size', 'Default')
        normal_samples_count = data_config.get('normal_samples', 10000)
        data_path_config = data_config.get('data_path', 'Default')
        
        # Menu display with context
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "MODEL TRAINING MENU")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + f"Active Training Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Epochs: " + Fore.CYAN + Style.BRIGHT + f"{epoch_count}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Batch Size: " + Fore.CYAN + Style.BRIGHT + f"{batch_count}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Data Path: " + Fore.CYAN + Style.BRIGHT + f"{data_path_config}")
        
        # Menu options with context-aware descriptions
        print(Fore.YELLOW + Style.BRIGHT + "\nCore Training Operations:")
        print(Fore.WHITE + Style.BRIGHT + "1. Train with Current Configuration " + Fore.GREEN + Style.BRIGHT + f"(Epochs: {epoch_count}, Batch Size: {batch_count})")
        print(Fore.WHITE + Style.BRIGHT + "2. Train with Synthetic Data " + Fore.GREEN + Style.BRIGHT + f"(Samples: {normal_samples_count})")
        print(Fore.WHITE + Style.BRIGHT + "3. Train with Real Data " + Fore.GREEN + Style.BRIGHT + f"(Source: {data_path_config})")
        print(Fore.WHITE + Style.BRIGHT + "4. Quick Training (Fast Test) " + Fore.GREEN + Style.BRIGHT + f"(Model: {model_type})")
        print(Fore.WHITE + Style.BRIGHT + "5. Custom Training Parameters " + Fore.GREEN + Style.BRIGHT + "(Interactive Setup)")
        print(Fore.WHITE + Style.BRIGHT + "6. Select Preset Configuration " + Fore.GREEN + Style.BRIGHT + f"(Available: {preset_count})")
        print(Fore.WHITE + Style.BRIGHT + "7. Stability Test " + Fore.GREEN + Style.BRIGHT + "(10 Epochs Quick Test)")
        print(Fore.RED + Style.BRIGHT + "0. Back to Main Menu")
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-7): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                return
        
        try:
            if choice == "1":
                try:
                    train_model_interactive(use_current_config=True, config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during training with current configuration: {str(e)}\n"
                        f"Context:\n"
                        f"- Preset: {preset_name}\n"
                        f"- Model: {model_type}\n"
                        f"- Epochs: {epoch_count}\n\n"
                        f"This could be due to:\n"
                        f"- Invalid configuration parameters\n"
                        f"- Missing or corrupted data\n"
                        f"- Insufficient system resources\n"
                        f"- Model architecture issues"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="TRAINING ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "2":
                try:
                    train_model_interactive(use_real_data=False, config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during synthetic data training: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Synthetic Samples: {normal_samples_count}\n\n"
                        f"Please check:\n"
                        f"- Data generation configuration\n"
                        f"- Model compatibility with synthetic data\n"
                        f"- System memory availability"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="SYNTHETIC TRAINING ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "3":
                try:
                    train_model_interactive(use_real_data=True, config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during real data training: {str(e)}\n"
                        f"Context:\n"
                        f"- Data Path: {data_path_config}\n"
                        f"- Model: {model_type}\n\n"
                        f"Please verify:\n"
                        f"- Data file exists and is accessible\n"
                        f"- Data format is compatible\n"
                        f"- Sufficient disk space\n"
                        f"- Data preprocessing requirements"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="REAL DATA TRAINING ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "4":
                try:
                    train_model_quick(config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during quick training: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Mode: Fast test\n\n"
                        f"This could indicate:\n"
                        f"- Model initialization issues\n"
                        f"- Basic configuration problems\n"
                        f"- Critical dependency missing"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="QUICK TRAINING ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "5":
                try:
                    train_model_custom(config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during custom training setup: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Preset: {preset_name}\n\n"
                        f"Please check:\n"
                        f"- Parameter validation rules\n"
                        f"- Configuration file permissions\n"
                        f"- Interactive input handling"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CUSTOM TRAINING ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "6":
                try:
                    select_preset_config()
                except Exception as e:
                    message = (
                        f"Error encountered during preset selection: {str(e)}\n"
                        f"Context:\n"
                        f"- Available Presets: {preset_count}\n"
                        f"- Current Preset: {preset_name}\n\n"
                        f"Please verify:\n"
                        f"- Preset configuration files\n"
                        f"- Preset validation logic\n"
                        f"- Configuration loading mechanisms"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="PRESET SELECTION ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "7":
                try:
                    run_stability_test(config=config)
                except Exception as e:
                    message = (
                        f"Error encountered during stability test: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Test Type: 10-epoch quick test\n\n"
                        f"This may indicate:\n"
                        f"- Fundamental model issues\n"
                        f"- Training loop problems\n"
                        f"- Data loading failures"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="STABILITY TEST ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "0":
                return
            else:
                print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-7.")
        
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nTraining operation interrupted by user")
        except Exception as e:
            logger.error(f"Training menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in training menu: {str(e)}\n"
                f"Context:\n"
                f"- Selected Option: {choice}\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could indicate:\n"
                f"- System resource exhaustion\n"
                f"- Configuration corruption\n"
                f"- Dependency conflicts\n"
                f"- Data access issues\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="TRAINING MENU ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                break

def hpo_training_menu(config: Optional[Dict[str, Any]] = None):
    """Menu for hyperparameter optimization options with context display and error handling."""
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        banner_config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if banner_config is not None:
            config = banner_config
        elif config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        hpo_config = config.get('hyperparameter_optimization', {})
        data_config = config.get('data', {})
        system_config = config.get('system', {})
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        hpo_data_path = data_config.get('data_path', 'Default')
        preset_count = len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        hpo_preset_count = len([p for p in PRESET_CONFIGS.values() if p.get('hyperparameter_optimization', {}).get('enabled', False)]) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        hpo_epoch_count = training_config.get('epochs', 'Default')
        hpo_batch_count = training_config.get('batch_size', 'Default')
        hpo_normal_samples_count = data_config.get('normal_samples', 10000)
        hpo_trials_count = hpo_config.get('n_trials', 50)
        hpo_strategy = hpo_config.get('strategy', 'optuna')
        
        # Menu display with context
        print(Fore.CYAN + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + "HYPERPARAMETER OPTIMIZATION MENU")
        print(Fore.CYAN + Style.BRIGHT + "-"*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Active HPO Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Preset: " + Fore.YELLOW + Style.BRIGHT + f"{preset_name}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Model: " + Fore.YELLOW + Style.BRIGHT + f"{model_type}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Source: " + Fore.YELLOW + Style.BRIGHT + f"{config_source}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Strategy: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_strategy}")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Trials: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_trials_count}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Data: " + Fore.YELLOW + Style.BRIGHT + f"{hpo_data_path}")
        
        # Menu options with context-aware display
        print(Fore.YELLOW + Style.BRIGHT + "\nOptimization Options:")
        print(Fore.WHITE + Style.BRIGHT + "1. Express HPO (Quick Setup) " + Fore.GREEN + Style.BRIGHT + f"(Trials: {hpo_trials_count})")
        print(Fore.WHITE + Style.BRIGHT + "2. HPO with Current Configuration " + Fore.GREEN + Style.BRIGHT + f"(Preset: {preset_name})")
        print(Fore.WHITE + Style.BRIGHT + "3. HPO with Synthetic Data " + Fore.GREEN + Style.BRIGHT + f"(Samples: {hpo_normal_samples_count})")
        print(Fore.WHITE + Style.BRIGHT + "4. HPO with Real Data " + Fore.GREEN + Style.BRIGHT + f"(Source: {hpo_data_path})")
        print(Fore.WHITE + Style.BRIGHT + "5. Custom HPO Configuration " + Fore.GREEN + Style.BRIGHT + "(Full Interactive Setup)")
        print(Fore.WHITE + Style.BRIGHT + "6. Select HPO Preset " + Fore.GREEN + Style.BRIGHT + f"(Available: {hpo_preset_count})")
        print(Fore.WHITE + Style.BRIGHT + "7. Continue Existing Study " + Fore.GREEN + Style.BRIGHT + "(Resume Previous HPO)")
        print(Fore.WHITE + Style.BRIGHT + "8. Quick HPO Test " + Fore.GREEN + Style.BRIGHT + "(10 Trials, 5 Epochs)")
        print(Fore.WHITE + Style.BRIGHT + "9. HPO Model Comparison " + Fore.GREEN + Style.BRIGHT + "(Compare All Model Types)")
        print(Fore.RED + Style.BRIGHT + "0. Back to Main Menu")
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-9): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                return
        
        try:
            if choice == "1":
                try:
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=False,
                        preset="express_hpo" if "express_hpo" in PRESET_CONFIGS else None,
                        config=config
                    )
                    _handle_hpo_result(result, "Express HPO")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during Express HPO: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Preset: {preset_name}\n"
                        f"- Trials: {hpo_trials_count}\n\n"
                        f"This could be due to:\n"
                        f"- Invalid HPO configuration\n"
                        f"- Missing optimization dependencies\n"
                        f"- System resource constraints\n"
                        f"- Data loading issues"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="EXPRESS HPO ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "2":
                try:
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=True,
                        config=config
                    )
                    _handle_hpo_result(result, "Current Configuration HPO")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during Current Configuration HPO: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Preset: {preset_name}\n"
                        f"- Config Source: {config_source}\n\n"
                        f"Please check:\n"
                        f"- Current configuration validity\n"
                        f"- Model compatibility with HPO\n"
                        f"- Configuration file integrity"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CURRENT CONFIG HPO ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "3":
                try:
                    result = run_hyperparameter_optimization_interactive(
                        use_real_data=False,
                        config=config
                    )
                    _handle_hpo_result(result, "Synthetic Data HPO")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during Synthetic Data HPO: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Synthetic Samples: {hpo_normal_samples_count}\n\n"
                        f"Please verify:\n"
                        f"- Data generation configuration\n"
                        f"- Model compatibility with synthetic data\n"
                        f"- System memory availability"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="SYNTHETIC DATA HPO ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "4":
                try:
                    result = run_hyperparameter_optimization_interactive(
                        use_real_data=True,
                        config=config
                    )
                    _handle_hpo_result(result, "Real Data HPO")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during Real Data HPO: {str(e)}\n"
                        f"Context:\n"
                        f"- Data Path: {hpo_data_path}\n"
                        f"- Model: {model_type}\n\n"
                        f"Please verify:\n"
                        f"- Data file exists and is accessible\n"
                        f"- Data format is compatible\n"
                        f"- Sufficient disk space\n"
                        f"- Data preprocessing requirements"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="REAL DATA HPO ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "5":
                try:
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=False,
                        config=config
                    )
                    _handle_hpo_result(result, "Custom HPO")
                    
                except Exception as e:
                    message = (
                        f"Error encountered during Custom HPO setup: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Preset: {preset_name}\n\n"
                        f"Please check:\n"
                        f"- Parameter validation rules\n"
                        f"- Configuration file permissions\n"
                        f"- Interactive input handling"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CUSTOM HPO ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "6":
                try:
                    _hpo_preset_selection_menu(config)
                except Exception as e:
                    message = (
                        f"Error encountered during HPO preset selection: {str(e)}\n"
                        f"Context:\n"
                        f"- Available Presets: {hpo_preset_count}\n"
                        f"- Current Preset: {preset_name}\n\n"
                        f"Please verify:\n"
                        f"- HPO preset configuration files\n"
                        f"- Preset validation logic\n"
                        f"- Configuration loading mechanisms"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="HPO PRESET SELECTION ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "7":
                try:
                    result = run_hyperparameter_optimization_interactive(
                        use_current_config=False,
                        config=config,
                        non_interactive=False
                    )
                    _handle_hpo_result(result, "Continued Study HPO")
                    
                except Exception as e:
                    message = (
                        f"Error encountered while continuing existing study: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Strategy: {hpo_strategy}\n\n"
                        f"Please check:\n"
                        f"- Existing study files exist\n"
                        f"- Study database accessibility\n"
                        f"- Study compatibility with current configuration"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CONTINUE STUDY ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "8":
                try:
                    _run_quick_hpo_test(config)
                except Exception as e:
                    message = (
                        f"Error encountered during quick HPO test: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Test Type: 10 trials, 5 epochs\n\n"
                        f"This may indicate:\n"
                        f"- Basic HPO functionality issues\n"
                        f"- Model initialization problems\n"
                        f"- Resource allocation failures"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="QUICK HPO TEST ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "9":
                try:
                    _run_hpo_model_comparison(config)
                except Exception as e:
                    message = (
                        f"Error encountered during HPO model comparison: {str(e)}\n"
                        f"Context:\n"
                        f"- Model: {model_type}\n"
                        f"- Comparison Type: Multi-model HPO\n\n"
                        f"Please ensure:\n"
                        f"- Multiple models are configured\n"
                        f"- Comparison data is available\n"
                        f"- Sufficient system resources"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="HPO MODEL COMPARISON ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "0":
                return
            else:
                print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-9.")
        
        except KeyboardInterrupt:
            print(Fore.YELLOW + Style.BRIGHT + "\nHPO operation interrupted by user")
        except Exception as e:
            logger.error(f"HPO menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in HPO menu: {str(e)}\n"
                f"Context:\n"
                f"- Selected Option: {choice}\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could indicate:\n"
                f"- System resource exhaustion\n"
                f"- HPO configuration corruption\n"
                f"- Dependency conflicts\n"
                f"- Optimization algorithm issues\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="HPO MENU ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                break

def configuration_menu():
    """Menu for configuration management options with context display and error handling."""
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        metadata_config = config.get('metadata', {})
        training_config = config.get('training', {})
        model_config = config.get('model', {})
        data_config = config.get('data', {})
        system_config = config.get('system', {})
        presets_config = config.get('presets', {})
        hpo_config = config.get('hyperparameter_optimization', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        config_status = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
            config_status = config["runtime"].get("config_status", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
            config_status = config["metadata"].get("config_status", "Unknown")
        
        # Get configuration health if available
        health_status = "unknown"
        health_color = Fore.WHITE
        if "runtime" in config and isinstance(config["runtime"], dict):
            runtime = config["runtime"]
            if "configuration_health" in runtime:
                health = runtime["configuration_health"]
                health_status = health.get("status", "unknown")
                if health_status in ["healthy", "passed"]:
                    health_color = Fore.GREEN
                elif health_status in ["needs_attention", "warning"]:
                    health_color = Fore.YELLOW
                else:
                    health_color = Fore.RED
        
        preset_count = len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        
        # Menu display with context
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "CONFIGURATION MANAGEMENT MENU")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + f"Active Configuration Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Status: " + health_color + Style.BRIGHT + f"{config_status}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Health: " + health_color + Style.BRIGHT + f"{health_status}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Sections: " + Fore.CYAN + Style.BRIGHT + f"{len(config) if isinstance(config, dict) else 'Unknown'}")
        
        # Context-aware Menu options
        print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration Operations:")
        print(Fore.WHITE + Style.BRIGHT + "1. Show Current Configuration " + Fore.GREEN + Style.BRIGHT + f"(Preset: {preset_name})")
        print(Fore.WHITE + Style.BRIGHT + "2. Save Current Configuration " + Fore.GREEN + Style.BRIGHT + f"(Status: {config_status})")
        print(Fore.WHITE + Style.BRIGHT + "3. Select Preset Configuration " + Fore.GREEN + Style.BRIGHT + f"(Available: {preset_count})")
        print(Fore.WHITE + Style.BRIGHT + "4. Load Saved Configuration " + Fore.GREEN + Style.BRIGHT + "(From File)")
        print(Fore.WHITE + Style.BRIGHT + "5. Reset to Default Configuration " + Fore.GREEN + Style.BRIGHT + "(Factory Reset)")
        print(Fore.WHITE + Style.BRIGHT + "6. Validate Current Configuration " + Fore.GREEN + Style.BRIGHT + f"(Health: {health_status})")
        print(Fore.WHITE + Style.BRIGHT + "7. Edit Configuration Interactively " + Fore.GREEN + Style.BRIGHT + "(Live Editor)")
        print(Fore.WHITE + Style.BRIGHT + "8. Compare Configurations " + Fore.GREEN + Style.BRIGHT + "(Side-by-Side)")
        print(Fore.WHITE + Style.BRIGHT + "9. Configuration Health Report " + Fore.GREEN + Style.BRIGHT + "(Detailed Analysis)")
        print(Fore.RED + Style.BRIGHT + "0. Back to Main Menu")
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-9): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                return
        
        try:
            if choice == "1":
                try:
                    show_current_config()
                except Exception as e:
                    message = (
                        f"Error encountered while displaying current configuration: {str(e)}\n"
                        f"Context:\n"
                        f"- Preset: {preset_name}\n"
                        f"- Model: {model_type}\n"
                        f"- Config Source: {config_source}\n\n"
                        f"This could be due to:\n"
                        f"- Configuration file corruption\n"
                        f"- Invalid configuration structure\n"
                        f"- Display formatting issues\n"
                        f"- System resource constraints"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CONFIGURATION DISPLAY ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "2":
                try:
                    save_config_interactive()
                except Exception as e:
                    message = (
                        f"Error encountered while saving configuration: {str(e)}\n"
                        f"Context:\n"
                        f"- Preset: {preset_name}\n"
                        f"- Config Status: {config_status}\n\n"
                        f"Please check:\n"
                        f"- File system permissions\n"
                        f"- Available disk space\n"
                        f"- Configuration validity\n"
                        f"- File path accessibility"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CONFIGURATION SAVE ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "3":
                try:
                    select_preset_config()
                except Exception as e:
                    message = (
                        f"Error encountered during preset selection: {str(e)}\n"
                        f"Context:\n"
                        f"- Available Presets: {preset_count}\n"
                        f"- Current Preset: {preset_name}\n\n"
                        f"Please verify:\n"
                        f"- Preset configuration files exist\n"
                        f"- Preset validation logic\n"
                        f"- Configuration loading mechanisms"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="PRESET SELECTION ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "4":
                try:
                    load_saved_config_interactive()
                except Exception as e:
                    message = (
                        f"Error encountered while loading configuration: {str(e)}\n"
                        f"Context:\n"
                        f"- Current Preset: {preset_name}\n"
                        f"- Config Status: {config_status}\n\n"
                        f"Please check:\n"
                        f"- Configuration file exists and is accessible\n"
                        f"- File format is supported\n"
                        f"- Configuration is compatible with current system\n"
                        f"- File is not corrupted"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CONFIGURATION LOAD ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "5":
                try:
                    reset_config_interactive()
                except Exception as e:
                    message = (
                        f"Error encountered while resetting configuration: {str(e)}\n"
                        f"Context:\n"
                        f"- Current Preset: {preset_name}\n\n"
                        f"This could indicate:\n"
                        f"- Default configuration files missing\n"
                        f"- System file permissions issues\n"
                        f"- Configuration restoration problems"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CONFIGURATION RESET ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "6":
                try:
                    validate_config_interactive()
                except Exception as e:
                    message = (
                        f"Error encountered during configuration validation: {str(e)}\n"
                        f"Context:\n"
                        f"- Current Health: {health_status}\n"
                        f"- Preset: {preset_name}\n\n"
                        f"Please check:\n"
                        f"- Configuration file integrity\n"
                        f"- Validation rule definitions\n"
                        f"- System state and dependencies"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CONFIGURATION VALIDATION ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "7":
                try:
                    edit_config_interactive()
                except Exception as e:
                    message = (
                        f"Error encountered in interactive configuration editor: {str(e)}\n"
                        f"Context:\n"
                        f"- Preset: {preset_name}\n"
                        f"- Model: {model_type}\n\n"
                        f"Please check:\n"
                        f"- Editor dependencies are installed\n"
                        f"- Configuration file permissions\n"
                        f"- Interactive input handling"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CONFIGURATION EDITOR ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "8":
                try:
                    compare_configs_interactive()
                except Exception as e:
                    message = (
                        f"Error encountered during configuration comparison: {str(e)}\n"
                        f"Context:\n"
                        f"- Active Preset: {preset_name}\n"
                        f"- Config Source: {config_source}\n\n"
                        f"Please verify:\n"
                        f"- Comparison configurations exist\n"
                        f"- Configuration formats are compatible\n"
                        f"- Sufficient system resources available"
                    )
                    console.print(
                        Panel.fit(
                            f"{message}",
                            title="CONFIGURATION COMPARISON ERROR",
                            style="bold red",
                            border_style="red",
                            padding=(1, 2),
                            box=box.ROUNDED
                        )
                    )
                    
            elif choice == "0":
                return
            else:
                print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-8.")
        
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nConfiguration operation interrupted by user")
        except Exception as e:
            logger.error(f"Configuration menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in configuration menu: {str(e)}\n"
                f"Context:\n"
                f"- Selected Option: {choice}\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could indicate:\n"
                f"- System resource issues\n"
                f"- Configuration file corruption\n"
                f"- Dependency conflicts\n"
                f"- File system problems\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="CONFIGURATION MENU ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                break

def model_analysis_menu():
    """Menu for model analysis and visualization with context display and error handling."""
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        data_config = config.get('data', {})
        validation_config = config.get('validation', {})
        monitoring_config = config.get('monitoring', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        preset_count = len(PRESET_CONFIGS) if 'PRESET_CONFIGS' in globals() else 'Unknown'
        epoch_count = training_config.get('epochs', 'Default')
        batch_count = training_config.get('batch_size', 'Default')
        normal_samples_count = data_config.get('normal_samples', 10000)
        data_path_config = data_config.get('data_path', 'Default')
        
        # Menu display with context
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "MODEL ANALYSIS & VISUALIZATION MENU")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + f"Active Analysis Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Epochs: " + Fore.CYAN + Style.BRIGHT + f"{epoch_count}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Data: " + Fore.CYAN + Style.BRIGHT + f"{data_path_config}")
        
        # Menu options with context-aware display
        print(Fore.YELLOW + Style.BRIGHT + "\nAnalysis & Visualization Options:")
        print(Fore.WHITE + Style.BRIGHT + "1. Training Performance Analysis " + Fore.GREEN + Style.BRIGHT + f"(Model: {model_type})")
        print(Fore.WHITE + Style.BRIGHT + "2. Model Architecture Visualization " + Fore.GREEN + Style.BRIGHT + "(Layer Details)")
        print(Fore.WHITE + Style.BRIGHT + "3. Anomaly Detection Results " + Fore.GREEN + Style.BRIGHT + "(Detection Metrics)")
        print(Fore.WHITE + Style.BRIGHT + "4. Feature Importance Analysis " + Fore.GREEN + Style.BRIGHT + "(Input Analysis)")
        print(Fore.WHITE + Style.BRIGHT + "5. Confusion Matrix & Metrics " + Fore.GREEN + Style.BRIGHT + "(Performance Visualization)")
        print(Fore.WHITE + Style.BRIGHT + "6. ROC Curve Analysis " + Fore.GREEN + Style.BRIGHT + "(Classification Performance)")
        print(Fore.WHITE + Style.BRIGHT + "7. Training History Visualization " + Fore.GREEN + Style.BRIGHT + "(Learning Curves)")
        print(Fore.WHITE + Style.BRIGHT + "8. Model Comparison Dashboard " + Fore.GREEN + Style.BRIGHT + "(Multi-Model Analysis)")
        print(Fore.WHITE + Style.BRIGHT + "9. Export Analysis Reports " + Fore.GREEN + Style.BRIGHT + "(PDF/HTML Reports)")
        print(Fore.RED + Style.BRIGHT + "0. Back to Main Menu")
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-9): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                return
        
        try:
            # Show "Coming Soon" message for all analysis options (1-9)
            if choice in ["1", "2", "3", "4", "5", "6", "7", "8", "9"]:
                analysis_type = {
                    "1": "Training Performance Analysis",
                    "2": "Model Architecture Visualization",
                    "3": "Anomaly Detection Results",
                    "4": "Feature Importance Analysis",
                    "5": "Confusion Matrix & Metrics",
                    "6": "ROC Curve Analysis",
                    "7": "Training History Visualization", 
                    "8": "Model Comparison Dashboard",
                    "9": "Export Analysis Reports"
                }.get(choice, "Advanced Analysis")
                
                # Context-specific details for each analysis type
                analysis_context = {
                    "1": f"- Model: {model_type}\n- Epochs: {epoch_count}",
                    "2": f"- Model: {model_type}\n- Source: {config_source}",
                    "3": f"- Model: {model_type}\n- Data: {data_path_config}",
                    "4": f"- Model: {model_type}\n- Data Features: Various input dimensions",
                    "5": f"- Model: {model_type}\n- Validation: {validation_config.get('validation_split', 'Default')}",
                    "6": f"- Model: {model_type}\n- Validation: {validation_config.get('validation_split', 'Default')}",
                    "7": f"- Model: {model_type}\n- Epochs: {training_config.get('epochs', 'Default')}",
                    "8": f"- Model: {model_type}\n- Available Models: Multiple comparison",
                    "9": f"- Model: {model_type}\n- Export Formats: PDF/HTML/CSV"
                }.get(choice, f"- Model: {model_type}\n- Preset: {preset_name}")
                
                message = (
                    f"{analysis_type}\n\n"
                    f"Current Context:\n"
                    f"{analysis_context}\n"
                    f"- Preset: {preset_name}\n"
                    f"- Status: Under Development\n\n"
                    f"This advanced analysis feature is currently in development\n"
                    f"and will be available in the next release.\n\n"
                    f"Planned features include:\n"
                    f"- Interactive visualization dashboards\n"
                    f"- Advanced statistical analysis\n"
                    f"- Export capabilities for reports\n"
                    f"- Comparative analysis tools\n"
                    f"- Real-time performance monitoring\n"
                    f"- Automated insights generation\n\n"
                    f"Check back soon for updates!\n"
                )
                
                # Different colors for different analysis types
                panel_styles = {
                    "1": ("bold cyan", "cyan"),
                    "2": ("bold green", "green"),
                    "3": ("bold magenta", "magenta"),
                    "4": ("bold yellow", "yellow"),
                    "5": ("bold blue", "blue"),
                    "6": ("bold cyan", "cyan"),
                    "7": ("bold green", "green"),
                    "8": ("bold magenta", "magenta"),
                    "9": ("bold yellow", "yellow")
                }
                
                style, border_style = panel_styles.get(choice, ("bold yellow", "yellow"))
                
                console.print(
                    Panel.fit(
                        f"[bold white]{message}[/bold white]",
                        title=f"{analysis_type} - COMING SOON",
                        style=style,
                        border_style=border_style,
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                time.sleep(3)
                
            elif choice == "0":
                return
            else:
                print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-9.")
        
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nAnalysis operation interrupted by user")
        except Exception as e:
            logger.error(f"Model analysis menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in model analysis menu: {str(e)}\n\n"
                f"Context:\n"
                f"- Selected Option: {choice}\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could indicate:\n"
                f"- Analysis dependency issues\n"
                f"- System resource constraints\n"
                f"- Data accessibility problems\n"
                f"- Visualization library conflicts\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"[bold red]{message}[/bold red]",
                    title="MODEL ANALYSIS MENU ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                break

def advanced_tools_menu():
    """Menu for advanced tools and utilities with context display and error handling."""
    while True:
        # Clear screen and show banner
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        model_config = config.get('model', {})
        system_config = config.get('system', {})
        experimental_config = config.get('experimental', {})
        hardware_config = config.get('hardware', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu display with context
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "ADVANCED TOOLS MENU")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + f"Advanced Tools Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Hardware: " + Fore.CYAN + Style.BRIGHT + f"{hardware_config.get('device', 'Default')}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Experimental: " + Fore.CYAN + Style.BRIGHT + f"{'Enabled' if experimental_config.get('enabled', False) else 'Disabled'}")
        
        # Menu options with context-aware display
        print(Fore.YELLOW + Style.BRIGHT + "\nAdvanced Tools & Utilities:")
        print(Fore.WHITE + Style.BRIGHT + "1. Model Export Utilities " + Fore.GREEN + Style.BRIGHT + f"(Model: {model_type})")
        print(Fore.WHITE + Style.BRIGHT + "2. Custom Data Preprocessing " + Fore.GREEN + Style.BRIGHT + "(Advanced Pipelines)")
        print(Fore.WHITE + Style.BRIGHT + "3. Batch Processing Tools " + Fore.GREEN + Style.BRIGHT + "(Large-scale Operations)")
        print(Fore.WHITE + Style.BRIGHT + "4. Performance Profiling " + Fore.GREEN + Style.BRIGHT + "(System Optimization)")
        print(Fore.WHITE + Style.BRIGHT + "5. Model Conversion Tools " + Fore.GREEN + Style.BRIGHT + "(Format Conversion)")
        print(Fore.WHITE + Style.BRIGHT + "6. Custom Training Loops " + Fore.GREEN + Style.BRIGHT + "(Expert Configuration)")
        print(Fore.WHITE + Style.BRIGHT + "7. Distributed Training Setup " + Fore.GREEN + Style.BRIGHT + "(Multi-GPU/Node)")
        print(Fore.WHITE + Style.BRIGHT + "8. Model Serving Deployment " + Fore.GREEN + Style.BRIGHT + "(Production Ready)")
        print(Fore.WHITE + Style.BRIGHT + "9. Experimental Features " + Fore.GREEN + Style.BRIGHT + "(Cutting-edge Tools)")
        print(Fore.RED + Style.BRIGHT + "0. Back to Main Menu")
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect option (0-9): ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                return
        
        try:
            if choice in ["1", "2", "3", "4", "5", "6", "7", "8", "9"]:
                # Coming soon message with context for all tools
                tool_type = {
                    "1": "Model Export Utilities",
                    "2": "Custom Data Preprocessing",
                    "3": "Batch Processing Tools",
                    "4": "Performance Profiling",
                    "5": "Model Conversion Tools", 
                    "6": "Custom Training Loops",
                    "7": "Distributed Training Setup",
                    "8": "Model Serving Deployment",
                    "9": "Experimental Features"
                }.get(choice, "Advanced Tool")
                
                message = (
                    f"{tool_type}\n\n"
                    f"Current Context:\n"
                    f"- Model: {model_type}\n"
                    f"- Preset: {preset_name}\n"
                    f"- Hardware: {hardware_config.get('device', 'Default')}\n"
                    f"- Status: Under Development\n\n"
                    f"This advanced tool is currently in development\n"
                    f"and will be available in the next release.\n\n"
                    f"Planned features include:\n"
                    f"- Professional-grade utilities and tools\n"
                    f"- Enterprise-level functionality\n"
                    f"- Production deployment capabilities\n"
                    f"- Advanced optimization features\n\n"
                    f"These tools are designed for:\n"
                    f"- Expert users and developers\n"
                    f"- Production system deployment\n"
                    f"- Research and development\n"
                    f"- Large-scale implementations\n\n"
                    f"Check back soon for updates!\n"
                )
                console.print(
                    Panel.fit(
                        f"[bold white]{message}[/bold white]",
                        title=f"{tool_type} - COMING SOON",
                        style="bold magenta",
                        border_style="magenta",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                time.sleep(3)
                
            elif choice == "0":
                return
            else:
                print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-9.")
        
        except KeyboardInterrupt:
            print(Fore.RED + Style.BRIGHT + "\nAdvanced tools operation interrupted by user")
        except Exception as e:
            logger.error(f"Advanced tools menu error: {e}", exc_info=True)
            message = (
                f"Unexpected error in advanced tools menu: {str(e)}\n\n"
                f"Context:\n"
                f"- Selected Option: {choice}\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could indicate:\n"
                f"- System compatibility issues\n"
                f"- Advanced feature dependencies\n"
                f"- Experimental tool conflicts\n"
                f"- Hardware configuration problems\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"[bold red]{message}[/bold red]",
                    title="ADVANCED TOOLS MENU ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if not exiting
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                break

def show_init_report():
    """
    Display available initialization reports and provide options to view them.
    Supports multiple report formats: HTML dashboard, JSON, TXT summary, and diagnostics.
    """
    from datetime import datetime
    
    try:
        # Clear screen and show banner with configuration
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        system_config = config.get('system', {})
        hardware_config = config.get('hardware', {})
        monitoring_config = config.get('monitoring', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        model_config = config.get('model', {})
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Determine report directory
        report_dir = Path(__file__).resolve().parent / "reports"
        
        if not report_dir.exists():
            message = (
                f"No reports directory found!\n"
                f"System Context:\n"
                f"- Preset: {preset_name}\n"
                f"- Model: {model_type}\n"
                f"- Source: {config_source}\n"
                f"Initialize the system first to generate reports.\n"
                f"Reports will be saved to: {report_dir}\n"
                f"Run system initialization from the main menu to create reports."
            )
            
            console.print(
                Panel.fit(
                    f"{message}",
                    title="NO REPORTS DIRECTORY",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
            # Input handling
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
            return
        
        # Scan for available reports
        report_files = {
            'html_dashboards': list(report_dir.glob("deep_system_dashboard_*.html")),
            'json_reports': list(report_dir.glob("deep_init_report_*.json")),
            'txt_summaries': list(report_dir.glob("deep_init_summary_*.txt")),
            'status_files': list(report_dir.glob("deep_init_status_*.json")),
            'diagnostics': list(report_dir.glob("deep_init_diagnostics_*.json")),
            'dashboard_data': list(report_dir.glob("deep_system_dashboard_data_*.json")),
            'latest_html': list(report_dir.glob("deep_latest_system_dashboard.html")),
            'latest_summary': list(report_dir.glob("deep_latest_init_summary.txt")),
            'index_file': list(report_dir.glob("deep_initialization_reports.txt"))
        }
        
        # Count total reports
        total_files = sum(len(files) for files in report_files.values())
        
        if total_files == 0:
            message = (
                f"No initialization reports found!\n"
                f"System Context:\n"
                f"- Preset: {preset_name}\n"
                f"- Model: {model_type}\n"
                f"- Source: {config_source}\n"
                f"Run system initialization to generate reports.\n"
                f"Reports will include:\n"
                f"- System configuration analysis\n"
                f"- Hardware capability assessment\n"
                f"- Dependency verification\n"
                f"- Performance benchmarks\n"
                f"- Health status evaluation\n"
                f"Check the reports directory: {report_dir}"
            )
            
            console.print(
                Panel.fit(
                    f"{message}",
                    title="NO REPORTS FOUND",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
            return
        
        # Menu display with context
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "INITIALIZATION REPORTS MENU")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + f"System Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Reports: " + Fore.CYAN + Style.BRIGHT + f"{total_files} files")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Location: " + Fore.CYAN + Style.BRIGHT + f"{report_dir}")
        
        # Prepare menu categories
        categories = []
        if report_files['html_dashboards'] or report_files['latest_html']:
            categories.append("1. HTML Dashboards " + Fore.GREEN + Style.BRIGHT + "(Interactive Visualization)")
        if report_files['txt_summaries'] or report_files['latest_summary']:
            categories.append("2. Text Summaries " + Fore.GREEN + Style.BRIGHT + "(Human-readable)")
        if report_files['json_reports']:
            categories.append("3. Full JSON Reports " + Fore.GREEN + Style.BRIGHT + "(Machine-readable)")
        if report_files['status_files']:
            categories.append("4. Status Reports " + Fore.GREEN + Style.BRIGHT + "(Compact Overview)")
        if report_files['diagnostics']:
            categories.append("5. Diagnostic Reports " + Fore.GREEN + Style.BRIGHT + "(Technical Details)")
        if report_files['dashboard_data']:
            categories.append("6. Dashboard Data " + Fore.GREEN + Style.BRIGHT + "(Raw Data Files)")
        if report_files['index_file']:
            categories.append("7. Report Index " + Fore.GREEN + Style.BRIGHT + "(Quick Overview)")
        
        categories.append("8. Show All Available Files " + Fore.GREEN + Style.BRIGHT + "(Complete Listing)")
        categories.append(Fore.RED + Style.BRIGHT + "0. Return to Main Menu")
        
        # def display_menu():
        #     """Display the report menu categories with enhanced styling."""
        #     print(Fore.YELLOW + Style.BRIGHT + "\nAvailable Report Categories:")
        #     for category in categories:
        #         if category.startswith("0."):
        #             print(f"  {category}")
        #         else:
        #             print(f"  {Fore.WHITE + Style.BRIGHT}{category.split(' (')[0]}{Style.RESET_ALL}{' (' + category.split(' (')[1] if '(' in category else ''}")
        
        def display_menu():
            """Display the report menu categories with enhanced styling."""
            print(Fore.YELLOW + Style.BRIGHT + "\nAvailable Report Categories:")
            for category in categories:
                if category.startswith("0."):
                    print(f"  {category}")
                else:
                    # Use partition which always returns 3 parts (before, separator, after)
                    main_part, separator, desc_part = category.partition(' (')
                    if separator:  # If ' (' was found in the string
                        print(f"  {Fore.WHITE + Style.BRIGHT}{main_part}{Style.RESET_ALL}{separator}{desc_part}")
                    else:
                        # Handle categories without parentheses
                        print(f"  {Fore.WHITE + Style.BRIGHT}{category}{Style.RESET_ALL}")
        
        # Display menu initially
        display_menu()
        
        while True:
            # Input handling with retry logic
            choice = None
            while not choice:
                try:
                    choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect report type (0-8): ").strip()
                    
                    # If empty input, retry
                    if not choice:
                        continue
                        
                except (EOFError, KeyboardInterrupt):
                    print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")
                    return
            
            try:
                # HTML Dashboards
                if choice == "1":
                    try:
                        html_files = report_files['html_dashboards'] + report_files['latest_html']
                        if not html_files:
                            message = (
                                f"No HTML dashboard files found.\n"
                                f"HTML dashboards provide interactive visualizations of:\n"
                                f"- System performance metrics\n"
                                f"- Hardware utilization charts\n"
                                f"- Configuration health status\n"
                                f"- Training progress analytics\n"
                                f"Run system initialization to generate HTML dashboards."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO HTML DASHBOARDS",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nHTML Dashboard Reports:")
                        for i, file_path in enumerate(html_files, 1):
                            file_size = file_path.stat().st_size / 1024  # KB
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        # File selection with error handling
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(html_files):
                                    selected_file = html_files[file_choice - 1]
                                    message = (
                                        f"Opening HTML dashboard in browser...\n"
                                        f"File: {selected_file.name}\n"
                                        f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                                        f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                                        f"The dashboard will open in your default web browser.\n"
                                        f"This may take a few moments..."
                                    )
                                    console.print(
                                        Panel.fit(
                                            f"{message}",
                                            title="OPENING DASHBOARD",
                                            style="bold green",
                                            border_style="green",
                                            padding=(1, 2),
                                            box=box.ROUNDED
                                        )
                                    )
                                    webbrowser.open(f"file://{selected_file.absolute()}")
                                    print(Fore.GREEN + Style.BRIGHT + f"Dashboard opened: {selected_file.name}")
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection. Please choose a valid file number.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing HTML dashboards: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Model: {model_type}\n"
                            f"This could be due to:\n"
                            f"- File permission issues\n"
                            f"- Corrupted report files\n"
                            f"- Browser compatibility problems"
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="HTML DASHBOARD ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Text Summaries
                elif choice == "2":
                    try:
                        txt_files = report_files['txt_summaries'] + report_files['latest_summary']
                        if not txt_files:
                            print(Fore.RED + Style.BRIGHT + "No text summary files found.")
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nText Summary Reports:")
                        for i, file_path in enumerate(txt_files, 1):
                            file_size = file_path.stat().st_size / 1024
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(txt_files):
                                    selected_file = txt_files[file_choice - 1]
                                    _display_text_report(selected_file)
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing text summaries: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Total Files: {len(txt_files) if 'txt_files' in locals() else 0}\n"
                            f"Please check file permissions and integrity."
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="TEXT SUMMARY ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Full JSON Reports
                elif choice == "3":
                    try:
                        json_files = report_files['json_reports']
                        if not json_files:
                            message = (
                                f"No JSON report files found.\n"
                                f"JSON reports provide comprehensive machine-readable data including:\n"
                                f"- Complete system configuration details\n"
                                f"- Hardware specifications and capabilities\n"
                                f"- Dependency verification results\n"
                                f"- Performance benchmark data\n"
                                f"- Health assessment metrics\n"
                                f"Run system initialization to generate JSON reports."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO JSON REPORTS",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nFull JSON Reports:")
                        for i, file_path in enumerate(json_files, 1):
                            file_size = file_path.stat().st_size / 1024  # KB
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            # Try to get entry count from JSON
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    data = json.load(f)
                                    entry_count = len(data.get('reports', []))
                                    print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {entry_count} entries, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                            except:
                                print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(json_files):
                                    selected_file = json_files[file_choice - 1]
                                    message = (
                                        f"Displaying JSON Report\n"
                                        f"File: {selected_file.name}\n"
                                        f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                                        f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                                        f"JSON reports contain structured data suitable for:\n"
                                        f"- Programmatic analysis\n"
                                        f"- Data processing pipelines\n"
                                        f"- Integration with other tools\n"
                                        f"- Automated reporting systems"
                                    )
                                    console.print(
                                        Panel.fit(
                                            f"{message}",
                                            title="JSON REPORT VIEWER",
                                            style="bold green",
                                            border_style="green",
                                            padding=(1, 2),
                                            box=box.ROUNDED
                                        )
                                    )
                                    _display_json_report(selected_file)
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection. Please choose a valid file number.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing JSON reports: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Total JSON Files: {len(json_files) if 'json_files' in locals() else 0}\n"
                            f"This could be due to:\n"
                            f"- JSON file corruption\n"
                            f"- Encoding issues\n"
                            f"- File permission problems\n"
                            f"- Memory constraints for large files"
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="JSON REPORT ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Status Reports
                elif choice == "4":
                    try:
                        status_files = report_files['status_files']
                        if not status_files:
                            message = (
                                f"No status report files found.\n"
                                f"Status reports provide compact overviews including:\n"
                                f"- System health status summary\n"
                                f"- Key performance indicators\n"
                                f"- Critical configuration settings\n"
                                f"- Quick reference metrics\n"
                                f"- Alert and warning summaries\n"
                                f"Run system initialization to generate status reports."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO STATUS REPORTS",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nStatus Reports:")
                        for i, file_path in enumerate(status_files, 1):
                            file_size = file_path.stat().st_size / 1024  # KB
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            # Try to get entry count from JSON
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    data = json.load(f)
                                    entry_count = len(data.get('entries', []))
                                    print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {entry_count} entries, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                            except:
                                print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(status_files):
                                    selected_file = status_files[file_choice - 1]
                                    message = (
                                        f"Displaying Status Report\n"
                                        f"File: {selected_file.name}\n"
                                        f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                                        f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                                        f"Status reports provide quick overviews of:\n"
                                        f"- System operational status\n"
                                        f"- Critical metrics at a glance\n"
                                        f"- Health indicators summary\n"
                                        f"- Performance snapshots"
                                    )
                                    console.print(
                                        Panel.fit(
                                            f"{message}",
                                            title="STATUS REPORT VIEWER",
                                            style="bold green",
                                            border_style="green",
                                            padding=(1, 2),
                                            box=box.ROUNDED
                                        )
                                    )
                                    _display_status_report(selected_file)
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection. Please choose a valid file number.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing status reports: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Total Status Files: {len(status_files) if 'status_files' in locals() else 0}\n"
                            f"Please check file integrity and permissions."
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="STATUS REPORT ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Diagnostic Reports
                elif choice == "5":
                    try:
                        diag_files = report_files['diagnostics']
                        if not diag_files:
                            message = (
                                f"No diagnostic report files found.\n"
                                f"Diagnostic reports provide technical details including:\n"
                                f"- Detailed system diagnostics\n"
                                f"- Performance profiling data\n"
                                f"- Resource utilization analysis\n"
                                f"- Error and warning logs\n"
                                f"- Troubleshooting information\n"
                                f"Run system initialization to generate diagnostic reports."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO DIAGNOSTIC REPORTS",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nDiagnostic Reports:")
                        for i, file_path in enumerate(diag_files, 1):
                            file_size = file_path.stat().st_size / 1024  # KB
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(diag_files):
                                    selected_file = diag_files[file_choice - 1]
                                    message = (
                                        f"Displaying Diagnostic Report\n"
                                        f"File: {selected_file.name}\n"
                                        f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                                        f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                                        f"Diagnostic reports contain technical information for:\n"
                                        f"- System troubleshooting\n"
                                        f"- Performance optimization\n"
                                        f"- Resource analysis\n"
                                        f"- Technical support scenarios"
                                    )
                                    console.print(
                                        Panel.fit(
                                            f"{message}",
                                            title="DIAGNOSTIC REPORT VIEWER",
                                            style="bold green",
                                            border_style="green",
                                            padding=(1, 2),
                                            box=box.ROUNDED
                                        )
                                    )
                                    _display_diagnostic_report(selected_file)
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection. Please choose a valid file number.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing diagnostic reports: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Total Diagnostic Files: {len(diag_files) if 'diag_files' in locals() else 0}\n"
                            f"This could indicate:\n"
                            f"- Diagnostic data corruption\n"
                            f"- Technical analysis issues\n"
                            f"- File format problems"
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="DIAGNOSTIC REPORT ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Dashboard Data
                elif choice == "6":
                    try:
                        data_files = report_files['dashboard_data']
                        if not data_files:
                            message = (
                                f"No dashboard data files found.\n"
                                f"Dashboard data files contain raw data for:\n"
                                f"- Interactive visualization generation\n"
                                f"- Custom chart creation\n"
                                f"- Data analysis and processing\n"
                                f"- External tool integration\n"
                                f"- Historical data comparison\n"
                                f"Run system initialization to generate dashboard data files."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO DASHBOARD DATA",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        print(Fore.YELLOW + Style.BRIGHT + "\nDashboard Data Files:")
                        for i, file_path in enumerate(data_files, 1):
                            file_size = file_path.stat().st_size / 1024  # KB
                            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            print(Fore.WHITE + Style.BRIGHT + f"  {i}. " + Fore.GREEN + Style.BRIGHT + f"{file_path.name}, ({file_size:.1f}KB, {mod_time.strftime('%Y-%m-%d %H:%M')})")
                        
                        file_choice = None
                        while file_choice is None:
                            try:
                                file_input = input(Fore.YELLOW + Style.BRIGHT + "\nSelect file number (or 0 to go back): ").strip()
                                if not file_input:
                                    continue
                                file_choice = int(file_input)
                                
                                if file_choice == 0:
                                    display_menu()
                                    break
                                elif 1 <= file_choice <= len(data_files):
                                    selected_file = data_files[file_choice - 1]
                                    message = (
                                        f"Displaying Dashboard Data\n"
                                        f"File: {selected_file.name}\n"
                                        f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                                        f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                                        f"Dashboard data files contain structured data for:\n"
                                        f"- Visualization backend processing\n"
                                        f"- Custom analytics development\n"
                                        f"- Data export and integration\n"
                                        f"- Performance trend analysis"
                                    )
                                    console.print(
                                        Panel.fit(
                                            f"{message}",
                                            title="DASHBOARD DATA VIEWER",
                                            style="bold green",
                                            border_style="green",
                                            padding=(1, 2),
                                            box=box.ROUNDED
                                        )
                                    )
                                    _display_dashboard_data(selected_file)
                                    display_menu()
                                    break
                                else:
                                    print(Fore.RED + Style.BRIGHT + "Invalid selection. Please choose a valid file number.")
                                    file_choice = None
                            except ValueError:
                                print(Fore.RED + Style.BRIGHT + "Invalid input. Please enter a number.")
                                file_choice = None
                                
                    except Exception as e:
                        message = (
                            f"Error accessing dashboard data: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Total Data Files: {len(data_files) if 'data_files' in locals() else 0}\n"
                            f"Please check data file integrity and format."
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="DASHBOARD DATA ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Report Index
                elif choice == "7":
                    try:
                        index_files = report_files['index_file']
                        if not index_files:
                            message = (
                                f"No index file found.\n"
                                f"The report index provides:\n"
                                f"- Quick overview of all available reports\n"
                                f"- File locations and timestamps\n"
                                f"- Report categories and types\n"
                                f"- Summary statistics\n"
                                f"- Navigation guidance\n"
                                f"Run system initialization to generate the report index."
                            )
                            console.print(
                                Panel.fit(
                                    f"{message}",
                                    title="NO REPORT INDEX",
                                    style="bold red",
                                    border_style="red",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                            )
                            display_menu()
                            continue
                        
                        selected_file = index_files[0]
                        message = (
                            f"Displaying Report Index\n"
                            f"File: {selected_file.name}\n"
                            f"Size: {selected_file.stat().st_size / 1024:.1f}KB\n"
                            f"Modified: {datetime.fromtimestamp(selected_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M')}\n"
                            f"The report index provides a comprehensive overview of:\n"
                            f"- All available report files and formats\n"
                            f"- File locations and access information\n"
                            f"- Summary statistics and metadata\n"
                            f"- Quick navigation references"
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="REPORT INDEX VIEWER",
                                style="bold green",
                                border_style="green",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        _display_text_report(selected_file, title="Report Index")
                        display_menu()
                        
                    except Exception as e:
                        message = (
                            f"Error accessing report index: {str(e)}\n"
                            f"Context:\n"
                            f"- Preset: {preset_name}\n"
                            f"- Index File: {index_files[0].name if index_files else 'None'}\n"
                            f"Please check index file accessibility and format."
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="REPORT INDEX ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Show All Report Files
                elif choice == "8":
                    try:
                        _show_all_report_files(report_dir, report_files)
                        display_menu()
                    except Exception as e:
                        message = (
                            f"Error displaying all files: {str(e)}\n"
                            f"Context:\n"
                            f"- Report Directory: {report_dir}\n"
                            f"- Total Files: {total_files}\n"
                            f"Please check directory accessibility."
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="FILE LISTING ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        display_menu()
                
                # Exit to Main Menu
                elif choice == "0":
                    return
                
                else:
                    print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-8.")
                    display_menu()
                    
            except KeyboardInterrupt:
                print(Fore.RED + Style.BRIGHT + "\nReport operation interrupted by user")
                display_menu()
            except Exception as e:
                logger.error(f"Init report menu error: {e}", exc_info=True)
                message = (
                    f"Unexpected error in reports menu: {str(e)}\n"
                    f"Context:\n"
                    f"- Selected Option: {choice}\n"
                    f"- Current Preset: {preset_name}\n"
                    f"- Model Type: {model_type}\n"
                    f"- Config Source: {config_source}\n\n"
                    f"This could indicate:\n"
                    f"- File system issues\n"
                    f"- Report corruption\n"
                    f"- Permission problems\n"
                    f"- Resource constraints\n"
                    f"Please check the logs for detailed information."
                )
                console.print(
                    Panel.fit(
                        f"{message}",
                        title="REPORTS MENU ERROR",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                display_menu()
    
    except Exception as e:
        logger.error(f"Show init report error: {e}", exc_info=True)
        message = (
            f"Error accessing initialization reports: {str(e)}\n"
            f"This could be due to:\n"
            f"- Missing reports directory\n"
            f"- File system permissions\n"
            f"- Configuration issues\n"
            f"- System resource problems\n"
            f"Please check system initialization and try again."
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="INIT REPORTS ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        # Input handling on fatal error
        try:
            input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
        except (EOFError, KeyboardInterrupt):
            print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")

def run_performance_benchmark_interactive():
    """Interactive performance benchmark with context display and error handling."""
    try:
        # Clear screen and show banner with configuration
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration sections with error handling
        system_config = config.get('system', {})
        hardware_config = config.get('hardware', {})
        training_config = config.get('training', {})
        model_config = config.get('model', {})
        monitoring_config = config.get('monitoring', {})
        
        # Context extraction using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        if isinstance(model_config, dict):
            model_type = model_config.get('model_type', 'Unknown')
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Context-aware Menu options
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "="*40)
        print(Fore.CYAN + Style.BRIGHT + "PERFORMANCE BENCHMARK")
        print(Fore.YELLOW + Style.BRIGHT + "="*40)
        print(Fore.GREEN + Style.BRIGHT + f"Benchmark Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Device: " + Fore.CYAN + Style.BRIGHT + f"{hardware_config.get('device', 'Default')}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Epochs: " + Fore.CYAN + Style.BRIGHT + f"{training_config.get('epochs', 'Default')}")
        
        # Display benchmark overview panel
        message = (
            f"Performance Benchmark Analysis\n\n"
            f"This benchmark will test multiple model configurations to evaluate:\n"
            f"- Training speed and efficiency\n"
            f"- Memory usage and optimization\n"
            f"- Model scalability across sizes\n"
            f"- System resource utilization\n\n"
            f"Benchmark configurations include:\n"
            f"- Small model (8 encoding dimensions)\n"
            f"- Medium model (16 encoding dimensions)  \n"
            f"- Large model (32 encoding dimensions)\n\n"
            f"Each configuration will be trained for 10 epochs with performance metrics.\n"
            f"Results will be saved for comparison and analysis."
        )
        console.print(
            Panel.fit(
                f"[bold white]{message}[/bold white]",
                title="PERFORMANCE BENCHMARK OVERVIEW",
                style="bold cyan",
                border_style="cyan",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        # Input handling with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + "\nRun performance benchmark? This will train multiple models. (Y/n): ").strip().lower()
                
                # If empty input, default to yes
                if not choice:
                    choice = 'y'
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nBenchmark cancelled by user")
                return
        
        if choice in ('', 'y', 'yes'):
            try:
                # Create benchmark arguments with configuration
                args = argparse.Namespace()
                args.model_dir = DEFAULT_MODEL_DIR / "benchmarks"
                args.epochs = 10
                args.non_interactive = True
                args.export_onnx = False
                
                # Show starting message
                message = (
                    f"Starting Performance Benchmark\n\n"
                    f"Configuration:\n"
                    f"- Preset: {preset_name}\n"
                    f"- Model Type: {model_type}\n"
                    f"- Benchmark Directory: {args.model_dir}\n"
                    f"- Epochs per Configuration: {args.epochs}\n"
                    f"- Test Configurations: Small, Medium, Large\n\n"
                    f"This may take several minutes depending on your hardware.\n"
                    f"Progress will be displayed below..."
                )
                console.print(
                    Panel.fit(
                        f"[bold green]{message}[/bold green]",
                        title="STARTING BENCHMARK",
                        style="bold green",
                        border_style="green",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                
                # Small delay to ensure user sees the message
                time.sleep(3)
                
                # Run the benchmark
                run_performance_benchmark(args)
                
            except Exception as e:
                message = (
                    f"Error encountered during benchmark execution: {str(e)}\n\n"
                    f"Context:\n"
                    f"- Preset: {preset_name}\n"
                    f"- Model: {model_type}\n"
                    f"- Benchmark Directory: {args.model_dir if 'args' in locals() else 'Unknown'}\n\n"
                    f"This could be due to:\n"
                    f"- Insufficient system resources\n"
                    f"- Model configuration issues\n"
                    f"- File system permissions\n"
                    f"- Dependency conflicts\n"
                )
                console.print(
                    Panel.fit(
                        f"[bold red]{message}[/bold red]",
                        title="BENCHMARK EXECUTION ERROR",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
        else:
            print(Fore.RED + Style.BRIGHT + "\nBenchmark cancelled")
            
    except Exception as e:
        logger.error(f"Performance benchmark interactive error: {e}", exc_info=True)
        message = (
            f"Unexpected error in performance benchmark setup: {str(e)}\n\n"
            f"This could indicate:\n"
            f"- System configuration issues\n"
            f"- Resource allocation problems\n"
            f"- Benchmark dependency conflicts\n"
            f"- File system access restrictions\n\n"
            f"Please check the logs for detailed information."
        )
        console.print(
            Panel.fit(
                f"[bold red]{message}[/bold red]",
                title="BENCHMARK SETUP ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
    
    # Input handling on fatal error
    try:
        input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
    except (EOFError, KeyboardInterrupt):
        print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")

def run_performance_benchmark(args: argparse.Namespace) -> None:
    """Run performance benchmark across different configurations with error handling and reporting."""
    try:
        logger.info("Running performance benchmark...")
        
        # Benchmark configurations with descriptions
        benchmark_configs = [
            ('small', {
                'description': 'Small model for quick testing',
                'config': {'model': {'encoding_dim': 8}, 'training': {'batch_size': 32, 'epochs': 10}}
            }),
            ('medium', {
                'description': 'Medium model for balanced performance', 
                'config': {'model': {'encoding_dim': 16}, 'training': {'batch_size': 64, 'epochs': 10}}
            }),
            ('large', {
                'description': 'Large model for maximum accuracy',
                'config': {'model': {'encoding_dim': 32}, 'training': {'batch_size': 128, 'epochs': 10}}
            })
        ]
        
        results = {}
        total_configs = len(benchmark_configs)
        
        # Display benchmark progress header
        console.print(
            Panel.fit(
                f"[bold cyan]Running {total_configs} benchmark configurations...[/bold cyan]",
                title="BENCHMARK PROGRESS",
                style="bold cyan",
                border_style="cyan",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        for i, (name, benchmark_info) in enumerate(benchmark_configs, 1):
            config_override = benchmark_info['config']
            description = benchmark_info['description']
            
            # Show current benchmark progress
            progress_message = (
                f"Configuration {i}/{total_configs}: {name.upper()}\n"
                f"Description: {description}\n"
                f"Encoding Dimensions: {config_override['model']['encoding_dim']}\n"
                f"Batch Size: {config_override['training']['batch_size']}\n"
                f"Epochs: {config_override['training']['epochs']}\n\n"
                f"Starting benchmark..."
            )
            console.print(
                Panel.fit(
                    f"[bold white]{progress_message}[/bold white]",
                    title=f"BENCHMARKING {name.upper()}",
                    style="bold yellow",
                    border_style="yellow",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
            logger.info(f"Benchmarking {name} configuration: {description}")
            
            # Create benchmark args
            benchmark_args = argparse.Namespace(**vars(args))
            benchmark_args.model_dir = args.model_dir / f"benchmark_{name}"
            benchmark_args.epochs = 10
            benchmark_args.non_interactive = True
            benchmark_args.export_onnx = False
            
            # Apply config override
            current_config = get_current_config()
            benchmark_config = deep_update(current_config, config_override)
            update_global_config(benchmark_config)
            
            start_time = time.time()
            try:
                training_results = train_model(benchmark_args)
                duration = time.time() - start_time
                
                # Results collection
                results[name] = {
                    'duration': duration,
                    'final_loss': training_results.get('evaluation', {}).get('test_loss', float('inf')),
                    'parameters': training_results.get('model', {}).get('total_parameters', 0),
                    'training_accuracy': training_results.get('evaluation', {}).get('training_accuracy', 0),
                    'validation_accuracy': training_results.get('evaluation', {}).get('validation_accuracy', 0),
                    'memory_usage': training_results.get('system', {}).get('peak_memory_mb', 0),
                    'description': description,
                    'success': True
                }
                
                # Show success message
                success_message = (
                    f"✓ {name.upper()} benchmark completed successfully!\n\n"
                    f"Results:\n"
                    f"- Duration: {duration:.1f}s\n"
                    f"- Final Loss: {results[name]['final_loss']:.4f}\n"
                    f"- Parameters: {results[name]['parameters']:,}\n"
                    f"- Training Accuracy: {results[name]['training_accuracy']:.2%}\n"
                    f"- Peak Memory: {results[name]['memory_usage']:.1f} MB"
                )
                console.print(
                    Panel.fit(
                        f"[bold green]{success_message}[/bold green]",
                        title=f"BENCHMARK COMPLETE - {name.upper()}",
                        style="bold green",
                        border_style="green",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                
            except Exception as e:
                duration = time.time() - start_time
                results[name] = {
                    'duration': duration,
                    'error': str(e),
                    'description': description,
                    'success': False
                }
                
                # Show error message
                error_message = (
                    f"✗ {name.upper()} benchmark failed!\n\n"
                    f"Error: {str(e)}\n"
                    f"Duration: {duration:.1f}s\n\n"
                    f"The benchmark will continue with remaining configurations."
                )
                console.print(
                    Panel.fit(
                        f"[bold red]{error_message}[/bold red]",
                        title=f"BENCHMARK FAILED - {name.upper()}",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                logger.error(f"Benchmark {name} failed: {e}")
        
        # Save benchmark results with metadata
        benchmark_path = args.model_dir / "benchmark_results.json"
        benchmark_metadata = {
            'timestamp': datetime.now().isoformat(),
            'total_configurations': total_configs,
            'successful_runs': sum(1 for r in results.values() if r['success']),
            'failed_runs': sum(1 for r in results.values() if not r['success']),
            'system_info': {
                'preset': get_current_config().get('presets', {}).get('current_preset', 'Unknown'),
                'model_type': get_current_config().get('model', {}).get('model_type', 'Unknown'),
                'device': get_current_config().get('hardware', {}).get('device', 'Unknown')
            },
            'results': results
        }
        
        with open(benchmark_path, 'w', encoding='utf-8') as f:
            json.dump(benchmark_metadata, f, indent=2, default=str)
        
        # Results display
        console.print(
            Panel.fit(
                f"[bold cyan]Benchmark Complete![/bold cyan]\n\n"
                f"Results saved to: {benchmark_path}\n"
                f"Total configurations: {total_configs}\n"
                f"Successful: {benchmark_metadata['successful_runs']}\n"
                f"Failed: {benchmark_metadata['failed_runs']}",
                title="BENCHMARK SUMMARY",
                style="bold cyan",
                border_style="cyan",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        # Display results table
        logger.info("Benchmark Results:")
        logger.info("=" * 70)
        for name, result in results.items():
            if result['success']:
                logger.info(f"{name:>10}: {result['duration']:6.1f}s | Loss: {result['final_loss']:.4f} | "
                           f"Params: {result['parameters']:,} | Acc: {result['training_accuracy']:.2%} | "
                           f"Memory: {result['memory_usage']:.1f}MB")
            else:
                logger.info(f"{name:>10}: {result['duration']:6.1f}s | FAILED: {result['error']}")
        
    except Exception as e:
        logger.error(f"Performance benchmark execution error: {e}", exc_info=True)
        message = (
            f"Critical error during benchmark execution: {str(e)}\n\n"
            f"The benchmark could not complete successfully.\n"
            f"Please check system resources and configuration.\n\n"
            f"Detailed error information has been logged."
        )
        console.print(
            Panel.fit(
                f"[bold red]{message}[/bold red]",
                title="BENCHMARK CRITICAL ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        raise

def display_model_comparison() -> None:
    """
    Display available initialization reports and provide options to view them.
    Supports multiple report formats: HTML dashboard, JSON, TXT summary, and diagnostics.
    """
    try:
        # Clear screen and show banner with configuration
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Configuration context using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu display with context
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "="*40)
        print(Fore.CYAN + Style.BRIGHT + "MODEL ARCHITECTURE COMPARISON")
        print(Fore.YELLOW + Style.BRIGHT + "="*40)
        print(Fore.GREEN + Style.BRIGHT + f"Active Comparison Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}\n")
        
        # Get comparison results with error handling
        try:
            results = compare_model_architectures()
        except Exception as e:
            logger.error(f"Failed to generate model comparison: {e}")
            
            # Error display with context
            message = (
                f"Failed to generate model architecture comparison: {str(e)}\n"
                f"Context:\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could be due to:\n"
                f"- Model variants not properly initialized\n"
                f"- Configuration parameter issues\n"
                f"- System resource constraints\n"
                f"- Missing dependencies or compatibility problems\n\n"
                f"Troubleshooting Steps:\n"
                f"1. Run 'initialize_model_variants()' to refresh model registry\n"
                f"2. Check configuration with 'get_current_config()'\n"
                f"3. Validate models with 'validate_model_variants(logger)'\n"
                f"4. Check system resources and dependencies"
            )
            console.print(
                Panel.fit(
                    f"[bold red]{message}[/bold red]",
                    title="COMPARISON FAILED",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            return
        
        # Handle critical errors with error analysis
        if isinstance(results, dict):
            if 'error' in results or 'critical_validation_failure' in results:
                error_msg = results.get('error') or results.get('critical_validation_failure', 'Unknown error')
                
                message = (
                    f"Analysis Error: {error_msg}\n"
                    f"Context:\n"
                    f"- Current Preset: {preset_name}\n"
                    f"- Model Type: {model_type}\n\n"
                )
                
                # Error-specific guidance based on current implementation
                if 'initialization_failed' in error_msg.lower() or 'MODEL_VARIANTS' in error_msg:
                    message += (
                        f"Model Initialization Issues:\n"
                        f"1. Check if model classes are properly imported\n"
                        f"2. Verify PyTorch installation and dependencies\n"
                        f"3. Run: initialize_model_variants(silent=False)\n"
                        f"4. Check for configuration compatibility issues\n"
                    )
                elif 'memory' in error_msg.lower() or 'resource' in error_msg.lower():
                    message += (
                        f"Resource Issues:\n"
                        f"1. Close other applications to free memory\n"
                        f"2. Reduce batch size or model complexity\n"
                        f"3. Enable memory optimization: enhanced_clear_memory()\n"
                        f"4. Check available system and GPU memory\n"
                    )
                elif 'validation' in error_msg.lower():
                    message += (
                        f"Model Validation Issues:\n"
                        f"1. Run: validate_model_variants(logger, silent=False)\n"
                        f"2. Check individual model instantiation\n"
                        f"3. Verify configuration parameters\n"
                    )
                else:
                    message += (
                        f"General Analysis Issues:\n"
                        f"1. Check system resources and dependencies\n"
                        f"2. Verify configuration file integrity\n"
                        f"3. Ensure all required packages are installed\n"
                    )
                
                # Display partial results if available
                partial_results = results.get('partial_results', {})
                if partial_results and isinstance(partial_results, dict):
                    message += f"\nPartial Results Available:\n"
                    for key, value in partial_results.items():
                        if not key.startswith('_'):
                            status = value.get('analysis_status', 'unknown') if isinstance(value, dict) else 'unknown'
                            message += f"  - {key}: {status}\n"
                
                console.print(
                    Panel.fit(
                        f"[bold red]{message}[/bold red]",
                        title="ANALYSIS ERROR",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                return
            
            if 'initialization_error' in results:
                message = (
                    f"Initialization Error: {results['initialization_error']}\n"
                    f"Context:\n"
                    f"- Current Preset: {preset_name}\n"
                    f"- Model Type: {model_type}\n\n"
                    f"This indicates:\n"
                    f"1. Model variants could not be initialized properly\n"
                    f"2. Configuration parameters are invalid or incompatible\n"
                    f"3. Required dependencies may be missing\n"
                    f"4. System resources may be insufficient"
                )
                console.print(
                    Panel.fit(
                        f"[bold red]{message}[/bold red]",
                        title="INITIALIZATION ERROR",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                return
        
        # Extract metadata and summary with validation
        metadata = results.get('_metadata', {})
        summary = results.get('_summary', {})
        analysis_results = results.get('_analysis_results', {})
        
        # Validate metadata
        if not metadata:
            logger.warning("No metadata found in comparison results")
            metadata = {
                'comparison_timestamp': datetime.now().isoformat(),
                'comparison_version': '3.2',
                'available_variants': 0,
                'successful_comparisons': 0,
                'failed_comparisons': 0,
                'hardware_context': {},
                'input_dimension': 'unknown',
                'config_source': 'unknown',
                'validation_checks_performed': [],
                'memory_optimization_summary': {'optimizations_performed': 0},
                'helper_functions_utilized': []
            }
        
        # Filter model results (exclude metadata and analysis results)
        model_results = {}
        for key, value in results.items():
            if not key.startswith('_') and isinstance(value, dict):
                model_results[key] = value
        
        if not model_results:
            message = (
                f"No Model Results Available\n"
                f"Context:\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n\n"
                f"This could indicate:\n"
                f"1. No model variants are initialized in MODEL_VARIANTS\n"
                f"2. All model analyses failed during execution\n"
                f"3. Configuration issues prevent proper analysis\n"
                f"4. System resource constraints blocked analysis\n\n"
                f"Try running: [bold cyan]initialize_model_variants(silent=False)[/bold cyan]"
            )
            console.print(
                Panel.fit(
                    f"[bold yellow]{message}[/bold yellow]",
                    title="NO RESULTS AVAILABLE",
                    style="bold yellow",
                    border_style="yellow",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            return
        
        # ENHANCED MAIN HEADER SECTION
        comparison_version = metadata.get('comparison_version', '3.2')
        hardware_ctx = metadata.get('hardware_context', {})
        gpu_available = hardware_ctx.get('gpu_available', False)
        system_class = hardware_ctx.get('system_class', 'unknown')
        memory_optimizations = metadata.get('memory_optimization_summary', {}).get('optimizations_performed', 0)
        helper_functions_used = len(metadata.get('helper_functions_utilized', []))
        comparison_checks = len(metadata.get('validation_checks_performed', []))
        
        header_panel = Panel.fit(
            f"[bold yellow]MODEL ARCHITECTURE COMPARISON REPORT v{comparison_version}[/bold yellow]\n"
            f"Generated: {metadata.get('comparison_timestamp', 'Unknown')[:19]} | "
            f"Duration: {metadata.get('comparison_duration_seconds', 0):.2f}s\n"
            f"Input Dimension: {metadata.get('input_dimension', 'Unknown')} | "
            f"Config Source: {metadata.get('config_source', 'Unknown')}\n"
            f"Models Available: {metadata.get('available_variants', 0)} | "
            f"Successfully Analyzed: {metadata.get('successful_comparisons', 0)} | "
            f"Failed: {metadata.get('failed_comparisons', 0)}\n"
            f"System Class: {system_class.title()} | "
            f"Hardware: {'GPU Available' if gpu_available else 'CPU Only'} | "
            f"Memory Optimizations: {memory_optimizations}\n"
            f"Helper Functions: {helper_functions_used} | "
            f"Comparison Checks: {comparison_checks} | "
            f"Analysis Completeness: {(metadata.get('successful_comparisons', 0) / max(metadata.get('available_variants', 1), 1) * 100):.1f}%",
            title="[bold yellow]COMPREHENSIVE ANALYSIS OVERVIEW[/bold yellow]",
            border_style="green",
            title_align="left",
            style="bold green",
            padding=(1, 2)
        )
        console.print(header_panel)
        
        # ENHANCED MAIN COMPARISON TABLE
        main_table = Table(
            title="\nPERFORMANCE & RESOURCE COMPARISON",
            box=box.ROUNDED,
            header_style="bold cyan",
            border_style="white",
            title_style="bold yellow",
            title_justify="left",
            show_lines=True,
            expand=True,
            width=min(180, console.width - 2)
        )
        
        # Configure main table columns
        main_table.add_column("Model", style="bold cyan", width=15, no_wrap=True)
        main_table.add_column("Parameters", width=10, justify="left")
        main_table.add_column("Size (MB)", width=8, justify="left")
        main_table.add_column("Complexity", width=8, justify="left")
        main_table.add_column("Inference (ms)", width=10, justify="left")
        main_table.add_column("Throughput", width=12, justify="left")
        main_table.add_column("Memory (MB)", width=10, justify="left")
        main_table.add_column("Efficiency", width=10, justify="left")
        main_table.add_column("Status", width=10, justify="left")
        
        # Prepare and sort model data
        model_data_list = []
        successful_models = []
        failed_models = []
        
        for model_name, model_data in model_results.items():
            analysis_status = model_data.get('analysis_status', 'unknown')
            
            if analysis_status in ['failed', 'analysis_failed', 'instantiation_failed', 'validation_error'] or 'error' in model_data:
                failed_models.append((model_name, model_data))
            elif analysis_status == 'completed':
                param_count = model_data.get('architecture', {}).get('total_params', 0)
                model_data_list.append((param_count, model_name, model_data))
                successful_models.append((model_name, model_data))
            else:
                # Handle partial success cases
                param_count = model_data.get('architecture', {}).get('total_params', 0)
                if param_count > 0 and 'architecture' in model_data:
                    model_data_list.append((param_count, model_name, model_data))
                    successful_models.append((model_name, model_data))
                else:
                    failed_models.append((model_name, model_data))
        
        # Sort by parameter count for logical ordering
        model_data_list.sort(key=lambda x: x[0])
        
        # Add successful models to main table
        for param_count, model_name, model_data in model_data_list:
            arch = model_data.get('architecture', {})
            perf = model_data.get('performance', {})
            memory_analysis = model_data.get('memory_analysis', {})
            resource_req = model_data.get('resource_requirements', {})
            
            # Performance metrics extraction
            inference_time_ms = None
            throughput_sps = None
            
            # Extract from scenario_summary
            scenario_summary = perf.get('scenario_summary', {})
            if scenario_summary:
                # Try standard_batch first, then single_sample
                for scenario_name in ['standard_batch', 'single_sample', 'small_batch']:
                    scenario_data = scenario_summary.get(scenario_name, {})
                    if scenario_data and 'latency_ms' in scenario_data:
                        inference_time_ms = scenario_data.get('latency_ms', 0)
                        throughput_sps = scenario_data.get('throughput', 0)
                        break
            
            # Fallback to direct performance metrics
            if inference_time_ms is None:
                inference_time_ms = (
                    perf.get('standard_batch_inference_time_ms') or
                    perf.get('single_sample_inference_time_ms') or
                    perf.get('avg_inference_time_ms', 0)
                )
            
            if throughput_sps is None:
                throughput_sps = (
                    perf.get('standard_batch_throughput_samples_per_second') or
                    perf.get('single_sample_throughput_samples_per_second') or
                    perf.get('throughput_samples_per_second', 0)
                )
            
            # Memory usage from multiple sources
            memory_mb = 0
            
            # Try GPU memory from detailed analysis
            gpu_memory_detailed = memory_analysis.get('gpu_memory_detailed', {})
            if gpu_memory_detailed:
                memory_mb = gpu_memory_detailed.get('allocated_mb', 0)
            
            # Fallback to other memory sources
            if memory_mb == 0:
                memory_mb = (
                    memory_analysis.get('gpu_memory_mb', 0) or
                    memory_analysis.get('cpu_memory_analysis', {}).get('estimated_total_cpu_memory_mb', 0) or
                    arch.get('model_size_mb', 0) * 4  # Rough estimate
                )
            
            # Calculate efficiency score
            efficiency_score = arch.get('architecture_efficiency', 0)
            
            # Status determination based on current metrics
            if inference_time_ms and throughput_sps:
                if inference_time_ms < 5 and throughput_sps > 1000:
                    status_text = "EXCELLENT"
                    status_style = "bold green"
                elif inference_time_ms < 20 and throughput_sps > 500:
                    status_text = "VERY GOOD"
                    status_style = "bold cyan"
                elif inference_time_ms < 50 and throughput_sps > 100:
                    status_text = "GOOD"
                    status_style = "bold blue"
                elif inference_time_ms < 200:
                    status_text = "ACCEPTABLE"
                    status_style = "bold yellow"
                else:
                    status_text = "SLOW"
                    status_style = "bold red"
            else:
                status_text = "PARTIAL"
                status_style = "bold magenta"
            
            # Memory formatting
            if memory_mb < 50:
                memory_text = f"{memory_mb:.3f}"
                memory_style = "bold green"
            elif memory_mb < 200:
                memory_text = f"{memory_mb:.3f}"
                memory_style = "bold cyan"
            elif memory_mb < 1000:
                memory_text = f"{memory_mb:.3f}"
                memory_style = "yellow"
            else:
                memory_text = f"{memory_mb:.3f}"
                memory_style = "red"
            
            # Format efficiency score
            if efficiency_score >= 80:
                eff_text = f"{efficiency_score:.0f}%"
                eff_style = "bold green"
            elif efficiency_score >= 60:
                eff_text = f"{efficiency_score:.0f}%"
                eff_style = "bold cyan"
            elif efficiency_score >= 40:
                eff_text = f"{efficiency_score:.0f}%"
                eff_style = "bold yellow"
            else:
                eff_text = f"{efficiency_score:.0f}%" if efficiency_score > 0 else "N/A"
                eff_style = "bold red" if efficiency_score > 0 else "bold magenta"
            
            main_table.add_row(
                Text(model_name, style="bold"),
                f"{param_count:,}",
                f"{arch.get('model_size_mb', 0):.3f}",
                arch.get('complexity_level', 'Unknown').title(),
                f"{inference_time_ms:.1f}" if inference_time_ms and inference_time_ms > 0 else "N/A",
                f"{throughput_sps:.0f}/s" if throughput_sps and throughput_sps > 0 else "N/A",
                Text(memory_text, style=memory_style),
                Text(eff_text, style=eff_style),
                Text(status_text, style=status_style)
            )
        
        # Add failed models to table with error information
        for model_name, model_data in failed_models:
            errors = model_data.get('errors', [])
            if errors:
                error_msg = errors[0] if isinstance(errors, list) else str(errors)
            else:
                error_msg = model_data.get('error', 'Unknown error')
            
            # Categorize error types based on current implementation
            if 'memory' in error_msg.lower():
                error_type = "MEMORY"
                error_style = "bold red"
            elif 'parameter' in error_msg.lower() or 'config' in error_msg.lower():
                error_type = "CONFIG"
                error_style = "bold yellow"
            elif 'instantiation' in error_msg.lower():
                error_type = "INIT"
                error_style = "bold red"
            elif 'validation' in error_msg.lower():
                error_type = "VALID"
                error_style = "bold red"
            else:
                error_type = "ERROR"
                error_style = "bold red"
            
            main_table.add_row(
                Text(model_name, style="bold"),
                Text("--", style="bold"),
                Text("--", style="bold"),
                Text(error_type, style=error_style),
                Text("--", style="bold"),
                Text("--", style="bold"),
                Text("--", style="bold"),
                Text("--", style="bold"),
                Text("FAILED", style="bold red")
            )
        
        console.print(main_table)
        
        # ENHANCED DETAILED ARCHITECTURE ANALYSIS
        if successful_models:
            detail_table = Table(
                title="DETAILED ARCHITECTURE & FEATURE ANALYSIS",
                box=box.ROUNDED,
                header_style="bold cyan",
                border_style="cyan",
                title_style="bold magenta",
                title_justify="left",
                show_lines=True,
                expand=True,
                width=min(200, console.width - 2)
            )
            
            detail_table.add_column("Model", style="bold cyan", width=15)
            detail_table.add_column("Description", width=32, justify="left")
            detail_table.add_column("Layers", width=5, justify="left")
            detail_table.add_column("Features", width=22, justify="left")
            detail_table.add_column("FLOPs", width=7, justify="left")
            detail_table.add_column("Complexity", width=8, justify="left")
            detail_table.add_column("Use Cases", width=30, justify="left")
            
            for model_name, model_data in successful_models:
                arch = model_data.get('architecture', {})
                flop_analysis = model_data.get('computational_complexity', {})
                feature_analysis = model_data.get('feature_analysis', {})
                use_cases = model_data.get('use_cases', [])
                
                # Feature detection
                features = []
                if feature_analysis.get('supports_attention', False):
                    features.append("Attention")
                if feature_analysis.get('supports_residual_blocks', False):
                    features.append("Residual")
                if feature_analysis.get('supports_skip_connections', False):
                    features.append("Skip")
                if feature_analysis.get('has_batch_norm', False):
                    features.append("BatchNorm")
                if feature_analysis.get('has_layer_norm', False):
                    features.append("LayerNorm")
                if feature_analysis.get('mixed_precision_compatible', False):
                    features.append("FP16")
                
                # Check normalization type
                norm_type = feature_analysis.get('normalization_type', 'none')
                if norm_type and norm_type != 'none' and norm_type not in [f.lower() for f in features]:
                    features.append(norm_type.title())
                
                # Ensemble information
                ensemble_size = feature_analysis.get('ensemble_size', 1)
                if ensemble_size > 1:
                    features.append(f"Ens({ensemble_size})")
                
                features_text = ", ".join(features) if features else "Basic"
                
                # FLOP formatting
                flops = flop_analysis.get('forward_pass', {}).get('total_flops', 0)
                if flops > 1e12:
                    flops_text = f"{flops/1e12:.1f}T"
                elif flops > 1e9:
                    flops_text = f"{flops/1e9:.1f}G"
                elif flops > 1e6:
                    flops_text = f"{flops/1e6:.1f}M"
                elif flops > 1e3:
                    flops_text = f"{flops/1e3:.1f}K"
                else:
                    flops_text = str(int(flops)) if flops > 0 else "N/A"
                
                # Use cases formatting
                use_cases_text = ', '.join(use_cases[:2]) + ('...' if len(use_cases) > 2 else '')
                if not use_cases_text:
                    use_cases_text = "General purpose"
                
                # Get complexity class from current implementation
                complexity_class = (
                    flop_analysis.get('complexity_metrics', {}).get('complexity_class') or
                    arch.get('computational_class', 'Unknown')
                )
                
                detail_table.add_row(
                    model_name,
                    arch.get('description', 'No description available')[:32],
                    str(arch.get('layer_count', 0)),
                    features_text,
                    flops_text,
                    complexity_class.replace('_', ' ').title() if complexity_class else "Unknown",
                    use_cases_text
                )
            
            console.print(detail_table)
        
        # ENHANCED HARDWARE CONTEXT & SYSTEM INFO
        gpu_memory_gb = hardware_ctx.get('gpu_memory_gb', 0)
        cpu_count = hardware_ctx.get('cpu_count', os.cpu_count() or 1)
        system_memory_gb = hardware_ctx.get('system_memory_gb', 8)
        
        # Hardware status with GPU info
        if gpu_available:
            cuda_info = hardware_ctx.get('cuda', {})
            if cuda_info.get('device_name'):
                gpu_name = cuda_info['device_name']
                gpu_status = f"[bold green]-OK- {gpu_name}[/bold green] ({gpu_memory_gb:.1f}GB VRAM)"
            elif 'devices' in cuda_info and isinstance(cuda_info['devices'], list) and cuda_info['devices']:
                gpu_name = cuda_info['devices'][0].get('name', 'Unknown GPU')
                gpu_status = f"[bold green]-OK- {gpu_name}[/bold green] ({gpu_memory_gb:.1f}GB VRAM)"
            else:
                gpu_status = f"[bold green]-OK- GPU Available[/bold green] ({gpu_memory_gb:.1f}GB VRAM)"
        else:
            gpu_status = "[bold yellow]-WARN- CPU Only[/bold yellow] - Consider GPU for better performance"
        
        # CPU status with performance class info
        cpu_perf_class = hardware_ctx.get('cpu_performance_class', 'unknown')
        if cpu_count >= 16:
            cpu_status = f"[bold green]-OK- {cpu_count} CPU Cores[/bold green] ({cpu_perf_class})"
        elif cpu_count >= 8:
            cpu_status = f"[bold green]-OK- {cpu_count} CPU Cores[/bold green] ({cpu_perf_class})"
        elif cpu_count >= 4:
            cpu_status = f"[bold yellow]-WARN- {cpu_count} CPU Cores[/bold yellow] ({cpu_perf_class})"
        else:
            cpu_status = f"[bold red]-WARN- {cpu_count} CPU Cores[/bold red] (Limited)"
        
        # System memory status with performance class
        memory_perf_class = hardware_ctx.get('memory_performance_class', 'unknown')
        if system_memory_gb >= 32:
            mem_status = f"[bold green]-OK- {system_memory_gb:.1f}GB RAM[/bold green] ({memory_perf_class})"
        elif system_memory_gb >= 16:
            mem_status = f"[bold green]-OK- {system_memory_gb:.1f}GB RAM[/bold green] ({memory_perf_class})"
        elif system_memory_gb >= 8:
            mem_status = f"[bold yellow]-WARN- {system_memory_gb:.1f}GB RAM[/bold yellow] ({memory_perf_class})"
        else:
            mem_status = f"[bold red]-WARN- {system_memory_gb:.1f}GB RAM[/bold red] (Limited)"
        
        hardware_text = f"{gpu_status}\n{cpu_status}\n{mem_status}"
        
        # Add system class
        if system_class != 'unknown':
            hardware_text += f"\n[bold cyan]-INFO- System Class: {system_class.title()}[/bold cyan]"
        
        # Add memory optimization
        if memory_optimizations > 0:
            hardware_text += f"\n[bold green]-INFO- Memory optimizations performed: {memory_optimizations}[/bold green]"
        
        # Add helper function utilization
        if helper_functions_used > 0:
            hardware_text += f"\n[bold cyan]-INFO- Helper functions utilized: {helper_functions_used}[/bold cyan]"
        
        # Add hardware analysis method if available
        collection_method = hardware_ctx.get('collection_method', '')
        if collection_method:
            hardware_text += f"\n[bold]-INFO- Detection method: {collection_method}[/bold]"
        
        hardware_panel = Panel.fit(
            hardware_text,
            title="[bold]SYSTEM CAPABILITIES & ANALYSIS CONTEXT[/bold]",
            border_style="bright_blue",
            title_align="left",
            padding=(1, 2)
        )
        console.print(hardware_panel)
        
        # ENHANCED RECOMMENDATIONS & OPTIMAL CHOICES
        if summary:
            # Overall recommendations
            overall_recs = summary.get('recommendations', [])
            optimization_suggestions = summary.get('optimization_suggestions', [])
            
            all_recommendations = overall_recs + optimization_suggestions
            
            if all_recommendations:
                # Categorize recommendations based on current implementation patterns
                performance_recs = [rec for rec in all_recommendations if any(word in rec.lower() for word in 
                                   ['performance', 'speed', 'throughput', 'optimization', 'gpu', 'batch', 'precision'])]
                memory_recs = [rec for rec in all_recommendations if any(word in rec.lower() for word in 
                              ['memory', 'ram', 'vram', 'checkpointing', 'accumulation', 'clear'])]
                config_recs = [rec for rec in all_recommendations if any(word in rec.lower() for word in 
                              ['config', 'setting', 'parameter', 'dimension', 'encoding'])]
                general_recs = [rec for rec in all_recommendations if rec not in performance_recs + memory_recs + config_recs]
                
                rec_sections = [
                    ("Performance Optimization", performance_recs, "bold green"),
                    ("Memory Management", memory_recs, "bold yellow"),
                    ("Configuration Tuning", config_recs, "bold cyan"),
                    ("General Recommendations", general_recs, "bold white")
                ]
                
                for section_title, recs, color in rec_sections:
                    if recs:
                        # Limit to 5 per section and format consistently
                        rec_text = "\n".join([f"[{color}]+[/{color}] {rec}" for rec in recs[:5]])
                        
                        rec_panel = Panel.fit(
                            rec_text,
                            title=f"[bold {color}]{section_title.upper()}[/bold {color}]",
                            border_style=color,
                            title_align="left",
                            padding=(1, 2)
                        )
                        console.print(rec_panel)
            
            # Optimal choices table
            optimal = summary.get('optimal_choices', {})
            if optimal:
                opt_table = Table(
                    title="[bold yellow]OPTIMAL MODEL SELECTION GUIDE[/bold yellow]",
                    box=box.ROUNDED,
                    header_style="bold white",
                    title_style="bold green",
                    title_justify="left",
                    border_style="green",
                    show_lines=True,
                    expand=True
                )
                
                opt_table.add_column("Scenario", style="bold cyan", width=15)
                opt_table.add_column("Recommended Model", style="bold green", width=15)
                opt_table.add_column("Rationale", width=35)
                opt_table.add_column("Performance", width=10, justify="left")
                
                # Rationale and performance metrics
                for scenario, choice in optimal.items():
                    scenario_display = scenario.replace('_', ' ').title()
                    
                    # Get performance data for the chosen model
                    choice_data = model_results.get(choice, {})
                    perf_data = choice_data.get('performance', {})
                    arch_data = choice_data.get('architecture', {})
                    memory_data = choice_data.get('memory_analysis', {})
                    
                    # Rationale based on actual metrics from current implementation
                    if scenario == 'fastest_inference':
                        rationale = "Minimizes latency for real-time applications"
                        # Get inference time from scenario summary
                        scenario_summary = perf_data.get('scenario_summary', {})
                        best_time = float('inf')
                        for sc_name, sc_data in scenario_summary.items():
                            if 'latency_ms' in sc_data:
                                best_time = min(best_time, sc_data['latency_ms'])
                        perf_metric = f"{best_time:.1f}ms" if best_time < float('inf') else "N/A"
                        
                    elif scenario == 'most_efficient':
                        rationale = "Best performance per parameter ratio"
                        eff_score = arch_data.get('architecture_efficiency', 0)
                        perf_metric = f"{eff_score:.0f}%" if eff_score > 0 else "N/A"
                        
                    elif scenario == 'lowest_memory':
                        rationale = "Suitable for memory-constrained environments"
                        # Get memory from detailed analysis
                        gpu_mem = memory_data.get('gpu_memory_detailed', {}).get('allocated_mb', 0)
                        cpu_mem = memory_data.get('cpu_memory_analysis', {}).get('estimated_total_cpu_memory_mb', 0)
                        mem_mb = gpu_mem or cpu_mem
                        perf_metric = f"{mem_mb:.3f}MB" if mem_mb > 0 else "N/A"
                        
                    elif scenario == 'smallest_model':
                        rationale = "Minimal resource footprint and fastest loading"
                        params = arch_data.get('total_params', 0)
                        perf_metric = f"{params:,}" if params > 0 else "N/A"
                        
                    elif scenario == 'best_balanced':
                        rationale = "Optimal trade-off across all performance metrics"
                        perf_metric = "Balanced"
                        
                    else:
                        rationale = "Best choice for this specific use case"
                        perf_metric = "Optimized"
                    
                    opt_table.add_row(
                        scenario_display,
                        Text(choice, style="bold"),
                        rationale,
                        perf_metric
                    )
                
                console.print(opt_table)
            
            # Performance rankings
            rankings = summary.get('performance_ranking', {})
            if rankings:
                rank_table = Table(
                    title="[bold yellow]PERFORMANCE RANKINGS BY CATEGORY[/bold yellow]",
                    box=box.ROUNDED,
                    header_style="bold white",
                    title_style="bold blue",
                    title_justify="left",
                    border_style="blue",
                    show_lines=True,
                    expand=True
                )
                
                rank_table.add_column("Category", style="bold cyan", width=15)
                rank_table.add_column("First Place", style="bold green", width=15)
                rank_table.add_column("Second Place", style="bold yellow", width=15)
                rank_table.add_column("Third Place", style="bold white", width=15)
                rank_table.add_column("Metric", width=10, justify="left")
                
                ranking_labels = {
                    'speed': 'Fastest Inference',
                    'efficiency': 'Most Efficient',
                    'memory_efficiency': 'Memory Efficient',
                    'size': 'Smallest Size'
                }
                
                for metric, models in rankings.items():
                    if models and metric in ranking_labels:
                        first = models[0] if len(models) > 0 else "N/A"
                        second = models[1] if len(models) > 1 else "N/A"
                        third = models[2] if len(models) > 2 else "N/A"
                        
                        # Get metric value for first place from current implementation data
                        if first != "N/A" and first in model_results:
                            first_data = model_results[first]
                            if metric == 'speed':
                                # Get best throughput from scenario summary
                                perf = first_data.get('performance', {})
                                scenario_summary = perf.get('scenario_summary', {})
                                best_throughput = 0
                                for sc_data in scenario_summary.values():
                                    if 'throughput' in sc_data:
                                        best_throughput = max(best_throughput, sc_data['throughput'])
                                metric_val = f"{best_throughput:.0f}/s" if best_throughput > 0 else "N/A"
                                
                            elif metric == 'efficiency':
                                arch = first_data.get('architecture', {})
                                metric_val = f"{arch.get('architecture_efficiency', 0):.0f}%"
                                
                            elif metric == 'memory_efficiency':
                                mem = first_data.get('memory_analysis', {})
                                gpu_mem = mem.get('gpu_memory_detailed', {}).get('allocated_mb', 0)
                                cpu_mem = mem.get('cpu_memory_analysis', {}).get('estimated_total_cpu_memory_mb', 0)
                                mem_mb = gpu_mem or cpu_mem
                                metric_val = f"{mem_mb:.3f}MB" if mem_mb > 0 else "N/A"
                                
                            elif metric == 'size':
                                arch = first_data.get('architecture', {})
                                metric_val = f"{arch.get('total_params', 0):,}"
                            else:
                                metric_val = "N/A"
                        else:
                            metric_val = "N/A"
                        
                        rank_table.add_row(
                            ranking_labels[metric],
                            first,
                            second,
                            third,
                            metric_val
                        )
                
                console.print(rank_table)
        
        # ENHANCED WARNINGS & ISSUES
        warnings = summary.get('warnings', []) if summary else []
        analysis_warnings = []
        for model_name, model_data in model_results.items():
            model_warnings = model_data.get('warnings', [])
            if model_warnings:
                analysis_warnings.extend([f"{model_name}: {w}" for w in model_warnings])
        
        all_warnings = warnings + analysis_warnings
        
        if all_warnings or failed_models:
            warn_items = []
            
            # Categorize warnings by type based on current implementation
            critical_warnings = []
            performance_warnings = []
            config_warnings = []
            memory_warnings = []
            
            for warning in all_warnings:
                if any(word in warning.lower() for word in ['critical', 'error', 'fail', 'crash']):
                    critical_warnings.append(f"[bold red]-CRITICAL-[/bold red] {warning}")
                elif any(word in warning.lower() for word in ['memory', 'ram', 'vram', 'oom', 'allocation']):
                    memory_warnings.append(f"[bold yellow]-MEMORY-[/bold yellow] {warning}")
                elif any(word in warning.lower() for word in ['performance', 'slow', 'degradation', 'throughput']):
                    performance_warnings.append(f"[bold cyan]-PERF-[/bold cyan] {warning}")
                else:
                    config_warnings.append(f"[bold magenta]-CONFIG-[/bold magenta] {warning}")
            
            # Add failed model warnings with error analysis from current implementation
            for model_name, model_data in failed_models:
                errors = model_data.get('errors', [])
                analysis_status = model_data.get('analysis_status', 'unknown')
                
                if errors:
                    error_msg = errors[0] if isinstance(errors, list) else str(errors)
                else:
                    error_msg = model_data.get('error', 'Unknown error')
                
                analysis_time = model_data.get('analysis_metadata', {}).get('analysis_time_seconds', 0)
                error_type = model_data.get('analysis_metadata', {}).get('error_type', 'Unknown')
                
                detailed_error = f"{model_name}: {error_msg[:60]}{'...' if len(error_msg) > 60 else ''}"
                if analysis_time > 0:
                    detailed_error += f" (failed after {analysis_time:.2f}s)"
                if error_type != 'Unknown':
                    detailed_error += f" [{error_type}]"
                
                critical_warnings.append(f"[bold red]-FAIL-[/boldred] {detailed_error}")
            
            # Display warnings by category with current implementation context
            all_warn_items = critical_warnings + memory_warnings + performance_warnings + config_warnings
            
            if all_warn_items:
                warn_text = "\n".join(all_warn_items[:12])  # Limit to 12 total warnings
                if len(all_warn_items) > 12:
                    warn_text += f"\n[bold]... and {len(all_warn_items) - 12} more issues[/bold]"
                
                warn_panel = Panel.fit(
                    warn_text,
                    title="[bold yellow]WARNINGS & ANALYSIS ISSUES[/bold yellow]",
                    border_style="yellow",
                    title_align="left",
                    padding=(1, 2)
                )
                console.print(warn_panel)
        
        # ENHANCED USE CASE RECOMMENDATIONS
        use_case_recs = summary.get('use_case_recommendations', {}) if summary else {}
        if use_case_recs:
            use_case_table = Table(
                title="[bold yellow]USE CASE SPECIFIC RECOMMENDATIONS[/bold yellow]",
                box=box.ROUNDED,
                header_style="bold white",
                title_style="bold magenta",
                title_justify="left",
                border_style="magenta",
                show_lines=True,
                expand=True
            )
            
            use_case_table.add_column("Use Case", style="bold magenta", width=15)
            use_case_table.add_column("Primary Choice", style="bold green", width=15)
            use_case_table.add_column("Alternative Options", width=25)
            use_case_table.add_column("Key Benefits", width=25)
            
            use_case_labels = {
                'prototyping_development': 'Prototyping & Development',
                'production_deployment': 'Production Deployment',
                'resource_constrained': 'Resource-Constrained',
                'high_performance': 'High Performance',
                'research_experimentation': 'Research & Experimentation'
            }
            
            use_case_benefits = {
                'prototyping_development': 'Fast iteration, low resource use',
                'production_deployment': 'Balanced performance & reliability',
                'resource_constrained': 'Minimal memory & compute needs',
                'high_performance': 'Maximum accuracy & capability',
                'research_experimentation': 'Advanced features & flexibility'
            }
            
            for use_case, models in use_case_recs.items():
                if models and use_case in use_case_labels:
                    primary = models[0] if models else "None"
                    alternatives = ", ".join(models[1:3]) if len(models) > 1 else "None"
                    if len(models) > 3:
                        alternatives += f" (+{len(models) - 3} more)"
                    
                    benefits = use_case_benefits.get(use_case, "Optimized for this scenario")
                    
                    use_case_table.add_row(
                        use_case_labels[use_case],
                        Text(primary, style="bold"),
                        alternatives,
                        benefits
                    )
            
            console.print(use_case_table)
        
        # ENHANCED CONFIGURATION GUIDANCE
        config_guidance = []
        
        # Hardware-specific guidance with recommendations from current system analysis
        if gpu_available:
            gpu_perf_class = hardware_ctx.get('gpu_performance_class', 'unknown')
            if gpu_memory_gb >= 16:
                config_guidance.extend([
                    f"[bold green]OK[/bold green] High-end GPU detected ({gpu_perf_class}) - All models fully supported",
                    "[bold green]OK[/bold green] Enable mixed precision (FP16) for 40-50% memory savings",
                    "[bold green]OK[/bold green] Use large batch sizes (64-128) for optimal throughput",
                    "[bold green]OK[/bold green] AutoencoderEnsemble recommended for maximum accuracy"
                ])
            elif gpu_memory_gb >= 8:
                config_guidance.extend([
                    f"[bold cyan]OK[/bold cyan] Mid-range GPU ({gpu_perf_class}) - Good for most models",
                    "[bold cyan]WARN[/bold cyan] Use moderate batch sizes (32-64) to avoid memory issues",
                    "[bold cyan]OK[/bold cyan] EnhancedAutoencoder recommended for balanced performance"
                ])
            elif gpu_memory_gb >= 4:
                config_guidance.extend([
                    f"[bold yellow]WARN[/bold yellow] Entry-level GPU ({gpu_perf_class}) - Reduce batch sizes to 16-32",
                    "[bold yellow]WARN[/bold yellow] SimpleAutoencoder or small EnhancedAutoencoder recommended",
                    "[bold yellow]WARN[/bold yellow] Monitor memory usage during training with nvidia-smi"
                ])
            else:
                config_guidance.extend([
                    "[bold red]WARN[/bold red] Very limited GPU memory - Use smallest models only",
                    "[bold red]WARN[/bold red] Batch size should be 8-16 maximum",
                    "[bold red]WARN[/bold red] Consider gradient accumulation instead of large batches"
                ])
        else:
            config_guidance.extend([
                "[bold yellow]WARN[/bold yellow] CPU-only training - Use SimpleAutoencoder for reasonable performance",
                "[bold yellow]WARN[/bold yellow] Reduce batch_size to 16-32 for CPU efficiency",
                "[bold yellow]WARN[/bold yellow] Set num_workers to match CPU core count for data loading",
                "[bold yellow]INFO[/bold yellow] Consider cloud GPU instances for significantly faster training"
            ])
        
        # System class specific guidance
        if system_class == 'high_performance':
            config_guidance.append("[bold green]-OK-[/bold green] High-performance system - Can handle complex models and large batches")
        elif system_class == 'limited':
            config_guidance.append("[bold yellow]-WARN-[/bold yellow] Limited system resources - Use conservative settings")
        elif system_class == 'standard':
            config_guidance.append("[bold cyan]-INFO-[/bold cyan] Standard system capabilities - Balanced configuration recommended")
        
        # Memory-specific guidance from current system analysis
        if system_memory_gb < 8:
            config_guidance.append("[bold red]-WARN-[/bold red] Low system RAM - Close other applications during training")
        elif system_memory_gb >= 32:
            config_guidance.append("[bold green]-OK-[/bold green] Excellent system RAM - Can handle large datasets and models")
        
        # Memory optimization guidance
        if memory_optimizations > 0:
            config_guidance.append(f"[bold cyan]-INFO-[/bold cyan] Memory optimizations already applied ({memory_optimizations} operations)")
        
        # Helper function integration guidance
        if helper_functions_used > 0:
            config_guidance.append(f"[bold cyan]-INFO-[/bold cyan] Comprehensive analysis using {helper_functions_used} helper functions")
        
        # Configuration optimization tips
        config_guidance.extend([
            "[bold cyan]-INFO-[/bold cyan] Use presets: select_preset_config('lightweight') or select_preset_config('performance')",
            "[bold cyan]-INFO-[/bold cyan] Enable tensorboard logging: config['training']['use_tensorboard'] = True",
            "[bold cyan]-INFO-[/bold cyan] Set up early stopping: config['training']['early_stopping'] = True",
            "[bold cyan]-INFO-[/bold cyan] Configure gradient clipping: config['training']['gradient_clip'] = 1.0",
            "[bold cyan]-INFO-[/bold cyan] Use enhanced_clear_memory() for memory optimization"
        ])
        
        # Model-specific configuration guidance based on successful analyses
        if successful_models:
            best_model = max(successful_models, key=lambda x: x[1].get('architecture', {}).get('architecture_efficiency', 0))
            config_guidance.append(f"[bold green]-RECOMMENDED-[/bold green] Start with {best_model[0]} for optimal balance")
        
        config_text = "\n".join(config_guidance)
        config_panel = Panel.fit(
            config_text,
            title="[bold yellow]CONFIGURATION GUIDANCE & OPTIMIZATION[/bold yellow]",
            border_style="bold cyan",
            title_align="left",
            padding=(1, 2)
        )
        console.print(config_panel)
        
        # ENHANCED RESOURCE REQUIREMENTS SUMMARY
        if successful_models:
            resource_table = Table(
                title="[bold yellow]RESOURCE REQUIREMENTS SUMMARY[/bold yellow]",
                box=box.ROUNDED,
                header_style="bold cyan",
                border_style="cyan",
                title_style="bold green",
                title_justify="left",
                show_lines=True,
                expand=True
            )
            
            resource_table.add_column("Model", style="bold cyan", width=15)
            resource_table.add_column("Training Memory", width=10, justify="left")
            resource_table.add_column("Training Time", width=10, justify="left")
            resource_table.add_column("Min Hardware", width=10)
            resource_table.add_column("Recommended", width=10)
            resource_table.add_column("Optimization", width=15)
            
            for model_name, model_data in successful_models[:5]:  # Limit to top 5
                resources_data = model_data.get('resource_requirements', {})
                
                # Extract resource information
                training_mem = resources_data.get('memory', {}).get('training_memory', {})
                time_estimates = resources_data.get('time_estimates', {})
                hw_reqs = resources_data.get('hardware_requirements', {})
                
                # Format memory requirement
                mem_with_overhead = training_mem.get('total_with_overhead', {})
                if isinstance(mem_with_overhead, dict):
                    mem_gb = mem_with_overhead.get('gb', 0)
                else:
                    mem_gb = training_mem.get('total_training', {}).get('gb', 0)
                mem_text = f"{mem_gb:.3f}GB" if mem_gb > 0 else "N/A"
                
                # Format training time
                convergence_estimates = time_estimates.get('convergence_estimates', {})
                if convergence_estimates:
                    epoch_time = convergence_estimates.get('total_training_time_hours', 0) / max(convergence_estimates.get('estimated_epochs', 100), 1)
                else:
                    epoch_time = time_estimates.get('training_time', {}).get('time_per_epoch_hours', 0)
                
                if epoch_time > 1:
                    time_text = f"{epoch_time:.3f}h/epoch"
                elif epoch_time > 0:
                    time_text = f"{epoch_time*60:.3f}m/epoch"
                else:
                    time_text = "N/A"
                
                # Hardware recommendations
                min_hw = hw_reqs.get('minimum_requirements', {})
                rec_hw = hw_reqs.get('recommended_requirements', {})
                
                min_gpu = min_hw.get('gpu_memory_gb', 0) or 0
                rec_gpu = rec_hw.get('gpu_memory_gb', 0) or 0
                
                min_text = f"{min_gpu:.3f}GB GPU" if min_gpu > 0 else "CPU OK"
                rec_text = f"{rec_gpu:.3f}GB GPU" if rec_gpu > 0 else "CPU"
                
                # Optimization suggestions
                opt_strategies = resources_data.get('optimization_strategies', {})
                mem_opts = opt_strategies.get('memory_optimization', [])
                comp_opts = opt_strategies.get('compute_optimization', [])
                
                if mem_opts and 'mixed precision' in str(mem_opts[0]).lower():
                    opt_text = "Mixed Precision"
                elif mem_opts and 'batch' in str(mem_opts[0]).lower():
                    opt_text = "Smaller Batches"
                elif mem_opts and 'checkpointing' in str(mem_opts[0]).lower():
                    opt_text = "Grad Checkpoint"
                elif comp_opts and 'gpu' in str(comp_opts[0]).lower():
                    opt_text = "GPU Required"
                else:
                    opt_text = "Standard"
                
                resource_table.add_row(
                    model_name,
                    mem_text,
                    time_text,
                    min_text,
                    rec_text,
                    opt_text
                )
            
            console.print(resource_table)
        
        # ENHANCED SCALING ANALYSIS SUMMARY
        if successful_models and any('scaling' in model_data for _, model_data in successful_models):
            scaling_table = Table(
                title="[bold yellow]MODEL SCALING ANALYSIS[/bold yellow]",
                box=box.ROUNDED,
                header_style="bold cyan",
                border_style="cyan",
                title_style="bold blue",
                title_justify="left",
                show_lines=True,
                expand=True
            )
            
            scaling_table.add_column("Model", style="bold cyan", width=15)
            scaling_table.add_column("Parameter Scaling", width=10)
            scaling_table.add_column("FLOP Scaling", width=10)
            scaling_table.add_column("Memory Scaling", width=10)
            scaling_table.add_column("Optimal Batch", width=10, justify="left")
            scaling_table.add_column("Complexity Growth", width=15)
            
            for model_name, model_data in successful_models:
                scaling_data = model_data.get('scaling', {})
                if scaling_data:
                    # Input dimension scaling
                    input_scaling = scaling_data.get('input_dimension_scaling', {})
                    scaling_coeffs = input_scaling.get('scaling_coefficients', {})
                    
                    param_exp = scaling_coeffs.get('parameter_scaling_exponent', 0)
                    flop_exp = scaling_coeffs.get('flop_scaling_exponent', 0)
                    
                    param_scaling_text = f"O(n^{param_exp:.3f})" if param_exp > 0 else "N/A"
                    flop_scaling_text = f"O(n^{flop_exp:.3f})" if flop_exp > 0 else "N/A"
                    
                    # Memory scaling
                    memory_curves = input_scaling.get('memory_curves', {})
                    mem_exp = memory_curves.get('memory_scaling_exponent', 0)
                    mem_scaling_text = f"O(n^{mem_exp:.3f})" if mem_exp > 0 else "Linear"
                    
                    # Batch size scaling
                    batch_scaling = scaling_data.get('batch_size_scaling', {})
                    optimal_batches = batch_scaling.get('optimal_batch_sizes', {})
                    opt_batch = optimal_batches.get('recommended', 'N/A')
                    
                    # Parameter scaling complexity growth
                    param_scaling = scaling_data.get('parameter_scaling', {})
                    complexity_growth = param_scaling.get('complexity_growth', 'unknown').replace('_', ' ').title()
                    
                    scaling_table.add_row(
                        model_name,
                        param_scaling_text,
                        flop_scaling_text,
                        mem_scaling_text,
                        str(opt_batch),
                        complexity_growth
                    )
            
            if scaling_table.rows:  # Only show if we have scaling data
                console.print(scaling_table)
        
        # TROUBLESHOOTING SECTION
        troubleshoot_sections = {
            "Essential Commands": [
                "[bold cyan]initialize_model_variants(silent=False)[/bold cyan] - Refresh model registry with detailed logs",
                "[bold cyan]validate_model_variants(logger, silent=False)[/bold cyan] - Test model functionality thoroughly",
                "[bold cyan]get_current_config()[/bold cyan] - View current configuration parameters",
                "[bold cyan]select_preset_config('performance')[/bold cyan] - Load optimized preset configuration",
                "[bold cyan]enhanced_clear_memory(aggressive=True)[/bold cyan] - Optimize memory usage"
            ],
            "Performance Optimization": [
                "Adjust batch_size based on available GPU memory and system class",
                "Use mixed precision (FP16) for 40-50% memory savings on modern GPUs",
                "Enable gradient checkpointing: config['training']['gradient_checkpointing'] = True",
                "Use learning rate scheduling for better convergence",
                "Enable early stopping to prevent overfitting and reduce training time"
            ],
            "Memory Management": [
                "Call enhanced_clear_memory() before training to optimize memory",
                "Reduce batch_size if encountering OOM errors",
                "Enable gradient accumulation for large effective batch sizes",
                "Use gradient checkpointing for memory-intensive models",
                "Monitor GPU memory usage with nvidia-smi during training"
            ],
            "Model Selection": [
                f"Start with {optimal.get('best_balanced', 'EnhancedAutoencoder')} for balanced performance" if optimal else "Use EnhancedAutoencoder for balanced performance",
                "Use SimpleAutoencoder for prototyping and debugging",
                "Use AutoencoderEnsemble only when maximum accuracy is required",
                f"Consider system class ({system_class}) when selecting models",
                "Test with default presets before custom configurations"
            ]
        }
        
        troubleshoot_text = ""
        for section, items in troubleshoot_sections.items():
            troubleshoot_text += f"[bold cyan]{section}:[/bold cyan]\n"
            # Limit to 4 items per section
            for item in items[:4]:
                troubleshoot_text += f"- {item}\n"
            troubleshoot_text += "\n"
        
        troubleshoot_panel = Panel.fit(
            troubleshoot_text.strip(),
            title="[bold yellow]TROUBLESHOOTING & OPTIMIZATION GUIDE[/bold yellow]",
            border_style="bold cyan",
            title_align="left",
            style="bold",
            padding=(1, 2)
        )
        console.print(troubleshoot_panel)
        
        # FOOTER WITH STATISTICS
        total_models = len(model_results)
        success_count = len(successful_models)
        fail_count = len(failed_models)
        success_rate = (success_count / total_models * 100) if total_models > 0 else 0
        
        # Analysis completeness metrics
        total_validation_checks = sum(
            len(model_data.get('analysis_metadata', {}).get('validation_checks_passed', []))
            for _, model_data in successful_models
        )
        avg_checks = total_validation_checks / max(success_count, 1)
        
        # Helper function utilization metrics
        total_helper_functions = sum(
            len(model_data.get('analysis_metadata', {}).get('helper_functions_utilized', []))
            for _, model_data in successful_models
        )
        avg_helper_functions = total_helper_functions / max(success_count, 1)
        
        # Performance summary
        if successful_models:
            avg_params = sum(model_data.get('architecture', {}).get('total_params', 0) 
                           for _, model_data in successful_models) / success_count
            avg_efficiency = sum(model_data.get('architecture', {}).get('architecture_efficiency', 0) 
                               for _, model_data in successful_models) / success_count
            
            perf_summary = f"Avg Parameters: {avg_params:,.0f} | Avg Efficiency: {avg_efficiency:.1f}%"
        else:
            perf_summary = "No performance data available"
        
        # Additional metrics
        total_memory_opts = metadata.get('memory_optimization_summary', {}).get('optimizations_performed', 0)
        total_cleanup_actions = metadata.get('memory_optimization_summary', {}).get('total_cleanup_actions', 0)
        
        footer_text = (
            f"[bold cyan]Analysis Completed Successfully[/bold cyan]\n"
            f"Models analyzed: {total_models} | Successful: {success_count} | Failed: {fail_count} | Success rate: {success_rate:.1f}%\n"
            f"System: {system_class.title()} | Hardware: {'GPU available' if gpu_available else 'CPU only'} | {perf_summary}\n"
            f"Analysis depth: {avg_checks:.1f} avg checks/model | Helper functions: {avg_helper_functions:.1f}/model\n"
            f"Memory optimizations: {total_memory_opts} operations, {total_cleanup_actions} cleanup actions\n"
            f"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Version: {comparison_version}"
        )
        
        footer_panel = Panel.fit(
            footer_text,
            title="[bold yellow]COMPREHENSIVE ANALYSIS SUMMARY[/bold yellow]",
            border_style="bold green",
            title_align="left",
            style="bold",
            padding=(0, 2)
        )
        console.print(footer_panel)
        
        # Logging with metrics
        logger.debug(f"Model comparison display completed successfully:")
        logger.debug(f"  - Total models analyzed: {total_models}")
        logger.debug(f"  - Successful comprehensive analyses: {success_count}")
        logger.debug(f"  - Failed analyses: {fail_count}")
        logger.debug(f"  - Overall success rate: {success_rate:.1f}%")
        logger.debug(f"  - Average validation checks per model: {avg_checks:.1f}")
        logger.debug(f"  - Average helper functions per model: {avg_helper_functions:.1f}")
        logger.debug(f"  - System class: {system_class}")
        logger.debug(f"  - Hardware context: {'GPU available' if gpu_available else 'CPU only'}")
        logger.debug(f"  - Memory optimizations: {total_memory_opts} operations")
        logger.debug(f"  - Comparison version: {comparison_version}")
        
        # Log individual model performance for debugging
        for model_name, model_data in successful_models:
            arch = model_data.get('architecture', {})
            analysis_status = model_data.get('analysis_status', 'unknown')
            validation_checks = len(model_data.get('analysis_metadata', {}).get('validation_checks_passed', []))
            helper_functions = len(model_data.get('analysis_metadata', {}).get('helper_functions_utilized', []))
            
            logger.debug(
                f"Model {model_name} [{analysis_status}]: "
                f"{arch.get('total_params', 0):,} params, "
                f"efficiency: {arch.get('architecture_efficiency', 0):.1f}%, "
                f"complexity: {arch.get('complexity_level', 'unknown')}, "
                f"checks: {validation_checks}, "
                f"helpers: {helper_functions}"
            )
        
        # Log error information for failed models
        for model_name, model_data in failed_models:
            errors = model_data.get('errors', [])
            analysis_status = model_data.get('analysis_status', 'unknown')
            error_type = model_data.get('analysis_metadata', {}).get('error_type', 'Unknown')
            
            if errors:
                error_msg = errors[0] if isinstance(errors, list) else str(errors)
            else:
                error_msg = model_data.get('error', 'Unknown error')
            
            logger.error(f"Model {model_name} analysis failed [{analysis_status}] [{error_type}]: {error_msg}")
        
    except Exception as e:
        error_msg = f"Critical failure in enhanced display_model_comparison: {str(e)}"
        logger.critical(error_msg, exc_info=True)
        
        # Display errors with context
        message = (
            f"Critical failure in model comparison display: {str(e)}\n"
            f"Context:\n"
            f"- Current Preset: [bold yellow]{preset_name if 'preset_name' in locals() else 'Unknown'}[/bold yellow]\n"
            f"- Model Type: [bold yellow]{model_type if 'model_type' in locals() else 'Unknown'}[/bold yellow]\n"
            f"- Config Source: [bold yellow]{config_source if 'config_source' in locals() else 'Unknown'}[/bold yellow]\n\n"
            f"Enhanced Recovery Actions:\n"
            f"1. [bold yellow]initialize_model_variants()[/bold yellow] - Reinitialize model registry\n"
            f"2. [bold yellow]check_hardware()[/bold yellow] - Verify system capabilities and resources\n"
            f"3. [bold yellow]get_current_config()[/bold yellow] - Validate configuration parameters\n"
            f"4. [bold yellow]validate_model_variants(logger)[/bold yellow] - Test model functionality\n"
            f"5. [bold yellow]select_preset_config('lightweight')[/bold yellow] - Use simplified configuration\n"
            f"6. Restart Python session if persistent issues occur"
        )
        console.print(
            Panel.fit(
                f"[bold red]{message}[/bold red]",
                title="CRITICAL DISPLAY ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )

def show_system_info():
    """Enhanced system information display leveraging comprehensive get_system_info() analysis.
    
    This function provides a rich, detailed system overview using Rich tables
    for improved readability and organization. Now fully integrated with the
    enhanced get_system_info() capabilities including performance metrics,
    memory optimization, and detailed analysis.
    """
    try:
        # Clear screen and show banner with configuration
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract configuration context using multiple fallbacks
        preset_name = "Custom/Default"
        model_type = "Unknown"
        config_source = "Unknown"
        
        # Method 1: Check presets section
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            preset_name = presets_section.get("current_preset", "Custom/Default")
        
        # Method 2: Check metadata for preset_used
        if preset_name in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                preset_name = metadata.get("preset_used", "Custom/Default")
        
        # Method 3: Check legacy _preset_name field
        if preset_name in ["Custom/Default", None, ""]:
            preset_name = config.get("_preset_name", "Custom/Default")
        
        # Method 4: Check runtime information
        if preset_name in ["Custom/Default", None, ""]:
            runtime = config.get("runtime", {})
            if isinstance(runtime, dict):
                preset_name = runtime.get("active_preset", "Custom/Default")
        
        # Clean up preset name display
        if preset_name in ["Custom/Default", None, "", "none"]:
            preset_name = "Custom/Default"
        elif isinstance(preset_name, str):
            preset_name = preset_name.title()
        
        # Extract model type with error handling
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            model_type = model_section.get("model_type", "Unknown")
        
        # Extract config source with fallbacks
        if "runtime" in config and isinstance(config["runtime"], dict):
            config_source = config["runtime"].get("config_source", "Unknown")
        elif "metadata" in config and isinstance(config["metadata"], dict):
            config_source = config["metadata"].get("config_source", "Unknown")
        
        # Menu display with context
        print(Fore.YELLOW + Style.BRIGHT + "\n" + "-"*40)
        print(Fore.CYAN + Style.BRIGHT + "SYSTEM INFORMATION & ANALYSIS")
        print(Fore.YELLOW + Style.BRIGHT + "-"*40)
        print(Fore.GREEN + Style.BRIGHT + f"Active System Context:")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Preset: " + Fore.CYAN + Style.BRIGHT + f"{preset_name}")
        print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
        print(Fore.WHITE + Style.BRIGHT + f"  └─ Source: " + Fore.CYAN + Style.BRIGHT + f"{config_source}")
        
        # Get system information
        try:
            system_info = get_system_info(
                include_versions=True,
                include_hardware=True,
                include_memory_usage=True,
                include_detailed_analysis=True,
                include_performance_baseline=False,
                include_memory_optimization=False
            )
            analysis_available = True
            collection_successful = system_info.get('collection_metadata', {}).get('success', True)
            
        except Exception as e:
            logger.error(f"Failed to generate comprehensive system analysis: {e}")
            
            # Display errors with context
            message = (
                f"Failed to generate comprehensive system analysis: {str(e)}\n"
                f"Context:\n"
                f"- Current Preset: {preset_name}\n"
                f"- Model Type: {model_type}\n"
                f"- Config Source: {config_source}\n\n"
                f"This could be due to:\n"
                f"- System resource constraints\n"
                f"- Hardware detection issues\n"
                f"- Missing system dependencies\n"
                f"- Permission or access problems\n\n"
                f"Attempting fallback analysis..."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="ANALYSIS WARNING",
                    style="bold yellow",
                    border_style="yellow",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
            # Fallback to basic hardware check
            try:
                hw = check_hardware(include_memory_usage=True)
                system_info = {
                    'hardware': hw,
                    'collection_metadata': {
                        'success': False,
                        'data_quality': 'fallback',
                        'errors': [str(e)]
                    }
                }
                analysis_available = False
                collection_successful = False
            except Exception as fallback_error:
                logger.error(f"Fallback hardware check also failed: {fallback_error}")
                message = (
                    f"Critical: Comprehensive system analysis failed\n"
                    f"Primary Error: {str(e)}\n"
                    f"Fallback Error: {str(fallback_error)}\n\n"
                    f"Context:\n"
                    f"- Current Preset: {preset_name}\n"
                    f"- Model Type: {model_type}\n\n"
                    f"This indicates serious system issues that may affect\n"
                    f"application functionality. Please check:\n"
                    f"1. System resource availability\n"
                    f"2. Hardware detection permissions\n"
                    f"3. Required system dependencies\n"
                    f"4. System logs for detailed errors"
                )
                console.print(
                    Panel.fit(
                        f"{message}",
                        title="CRITICAL ANALYSIS FAILURE",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                system_info = {
                    'collection_metadata': {
                        'success': False,
                        'data_quality': 'failed',
                        'errors': [str(e), str(fallback_error)]
                    }
                }
                analysis_available = False
                collection_successful = False
        
        # Create main system info table
        main_table = Table(
            title="\n[bold yellow]COMPREHENSIVE SYSTEM INFORMATION & ANALYSIS[/bold yellow]",
            box=box.ROUNDED,
            header_style="bold yellow",
            border_style="magenta",
            title_style="bold yellow",
            title_justify="left",
            show_lines=True,
            expand=True,
            width=min(120, console.width - 4)
        )
        
        # Configure columns
        main_table.add_column("Category", style="bold cyan", width=28, no_wrap=True)
        main_table.add_column("Status", width=12, justify="center")
        main_table.add_column("Details", style="bold", min_width=50, max_width=75)
        
        # COLLECTION METADATA & PERFORMANCE
        if analysis_available and 'collection_metadata' in system_info:
            metadata = system_info['collection_metadata']
            collection_quality = metadata.get('data_quality', 'unknown')
            collection_time = metadata.get('collection_duration_ms', 0)
            data_sources = len(metadata.get('data_sources', []))
            errors = len(metadata.get('errors', []))
            warnings = len(metadata.get('warnings', []))
            
            main_table.add_row(
                Text("COLLECTION METADATA", style="bold white on blue"),
                "",
                ""
            )
            
            # Data quality
            quality_colors = {
                'excellent': "bold green",
                'good': "bold green", 
                'acceptable': "bold yellow",
                'degraded': "bold yellow",
                'poor': "bold red",
                'failed': "bold red"
            }
            quality_style = quality_colors.get(collection_quality, "bold yellow")
            
            main_table.add_row(
                "Data Quality",
                Text(collection_quality.upper(), style=quality_style),
                f"Collection: {collection_time:.1f}ms | Sources: {data_sources} | Errors: {errors} | Warnings: {warnings}"
            )
            
            # Performance metrics summary if available
            perf_metrics = metadata.get('performance_metrics', {})
            if perf_metrics:
                ops_completed = len(perf_metrics)
                perf_summary = metadata.get('performance_summary', {})
                efficiency_score = perf_summary.get('efficiency_score', 0)
                efficiency_style = "bold green" if efficiency_score > 80 else "bold yellow" if efficiency_score > 50 else "bold red"
                main_table.add_row(
                    "Performance Metrics",
                    Text(f"{efficiency_score:.0f}%", style=efficiency_style),
                    f"Operations: {ops_completed} | Efficiency Score: {efficiency_score:.1f}%"
                )
            
            # Memory optimization results if available
            mem_opt_results = metadata.get('memory_optimization_results', {})
            if mem_opt_results:
                pre_opt = mem_opt_results.get('pre_collection', {})
                post_opt = mem_opt_results.get('post_collection', {})
                
                if pre_opt.get('success') or post_opt.get('success'):
                    actions_count = len(pre_opt.get('actions_taken', [])) + len(post_opt.get('actions_taken', []))
                    main_table.add_row(
                        "Memory Optimization",
                        Text("ACTIVE", style="bold green"),
                        f"Optimization actions: {actions_count} | Memory management active"
                    )
            
            main_table.add_row(
                "Timestamp",
                Text("INFO", style="bold blue"),
                system_info.get('timestamp', 'unknown')
            )
        
        # PLATFORM & SYSTEM INFORMATION
        main_table.add_row(
            Text("PLATFORM & SYSTEM", style="bold white on blue"),
            "",
            ""
        )
        
        if 'platform' in system_info:
            platform_info = system_info['platform']
            
            # System with boot time if available
            system_detail = f"{platform_info.get('system', 'Unknown')} {platform_info.get('release', '')}"
            if platform_info.get('boot_time'):
                boot_time = platform_info['boot_time'][:19].replace('T', ' ')
                system_detail += f" | Boot: {boot_time}"
                
            main_table.add_row(
                "Operating System",
                Text("INFO", style="bold blue"),
                system_detail
            )
            
            # Platform architecture info
            arch_info = platform_info.get('architecture', ['Unknown', ''])
            platform_detail = platform_info.get('platform', 'Unknown')
            if len(arch_info) > 1 and arch_info[1]:
                platform_detail += f" ({arch_info[0]} - {arch_info[1]})"
            else:
                platform_detail += f" ({arch_info[0] if arch_info else 'Unknown'})"
                
            main_table.add_row(
                "Platform Architecture",
                Text("INFO", style="bold blue"),
                platform_detail
            )
            
            # Processor details
            processor = platform_info.get('processor', 'Unknown')
            processor_detail = processor[:60] + "..." if len(processor) > 60 else processor
            machine = platform_info.get('machine', '')
            if machine and machine not in processor_detail:
                processor_detail += f" ({machine})"
                
            main_table.add_row(
                "Processor",
                Text("INFO", style="bold blue"),
                processor_detail
            )
            
            main_table.add_row(
                "Hostname",
                Text("INFO", style="bold blue"),
                platform_info.get('node', 'Unknown')
            )
        else:
            # Fallback to basic platform info
            main_table.add_row(
                "Operating System",
                Text("INFO", style="bold blue"),
                f"{platform.system()} {platform.release()}"
            )
            
            main_table.add_row(
                "Platform Architecture", 
                Text("INFO", style="bold blue"),
                f"{platform.platform()} ({platform.machine()})"
            )
            
            main_table.add_row(
                "Hostname",
                Text("INFO", style="bold blue"),
                platform.node()
            )
        
        # PYTHON ENVIRONMENT
        main_table.add_row(
            Text("PYTHON ENVIRONMENT", style="bold white on blue"),
            "",
            ""
        )
        
        if 'python' in system_info:
            python_info = system_info['python']
            version_info = python_info.get('version_info', {})
            
            # Version string with implementation
            version_str = f"Python {version_info.get('major', '?')}.{version_info.get('minor', '?')}.{version_info.get('micro', '?')}"
            implementation = python_info.get('implementation', 'Unknown')
            if implementation != 'CPython':
                version_str += f" ({implementation})"
                
            main_table.add_row(
                "Python Version",
                Text("INFO", style="bold blue"),
                version_str
            )
            
            # Build info with compiler
            build_info = python_info.get('build', ['Unknown', ''])
            compiler = python_info.get('compiler', 'Unknown')
            build_detail = f"{build_info[0] if build_info else 'Unknown'}"
            if compiler != 'Unknown' and compiler not in build_detail:
                build_detail += f" | {compiler}"
                
            main_table.add_row(
                "Build & Compiler",
                Text("INFO", style="bold blue"),
                build_detail
            )
            
            # Executable path (shortened for display)
            executable = python_info.get('executable', 'Unknown')
            if len(executable) > 50:
                executable = "..." + executable[-47:]
            main_table.add_row(
                "Executable Path",
                Text("INFO", style="bold blue"),
                executable
            )
            
            # Encoding information
            encoding = python_info.get('encoding', {})
            encoding_detail = f"Default: {encoding.get('default', 'Unknown')}"
            fs_encoding = encoding.get('filesystem', '')
            if fs_encoding and fs_encoding != encoding.get('default'):
                encoding_detail += f" | Filesystem: {fs_encoding}"
                
            main_table.add_row(
                "Character Encoding",
                Text("INFO", style="bold blue"),
                encoding_detail
            )
            
            # Module and path information
            modules_count = python_info.get('modules_count', 0)
            path_count = len(python_info.get('path', []))
            main_table.add_row(
                "Environment Info",
                Text("INFO", style="bold blue"),
                f"Loaded modules: {modules_count} | Path entries: {path_count}"
            )
        else:
            # Fallback Python info
            version_parts = sys.version.split()
            main_table.add_row(
                "Python Version",
                Text("INFO", style="bold blue"),
                version_parts[0] if version_parts else "Unknown"
            )
            
            executable = sys.executable
            if len(executable) > 50:
                executable = "..." + executable[-47:]
            main_table.add_row(
                "Executable Path",
                Text("INFO", style="bold blue"),
                executable
            )
        
        # PACKAGE ENVIRONMENT
        if 'package_versions' in system_info and 'package_analysis' in system_info:
            pkg_analysis = system_info['package_analysis']
            env_health = pkg_analysis.get('environment_health', {})
            
            main_table.add_row(
                Text("PACKAGE ENVIRONMENT", style="bold white on blue"),
                "",
                ""
            )
            
            # Health status
            health_status = env_health.get('overall_status', 'unknown')
            health_colors = {
                'healthy': "bold green",
                'degraded': "bold yellow", 
                'critical': "bold red"
            }
            health_style = health_colors.get(health_status, "bold yellow")
            compat_score = env_health.get('compatibility_score', 0)
            complete_score = env_health.get('completeness_score', 0)
            health_detail = f"Compatibility: {compat_score:.1f}% | Completeness: {complete_score:.1f}%"
            critical_issues = env_health.get('critical_issues', 0)
            warnings_count = env_health.get('warnings', 0)
            if critical_issues > 0 or warnings_count > 0:
                health_detail += f" | Issues: {critical_issues} critical, {warnings_count} warnings"
            
            main_table.add_row(
                "Environment Health",
                Text(health_status.upper(), style=health_style),
                health_detail
            )
            
            # Package status summary
            status_summary = pkg_analysis.get('status_summary', {})
            total_packages = pkg_analysis.get('total_packages', 0)
            available_packages = pkg_analysis.get('available_packages', 0)
            status_details = []
            for status, count in status_summary.items():
                if count > 0:
                    status_color = "green" if status == "OK" else "yellow" if status == "WARNING" else "red"
                    status_details.append(f"[{status_color}]{status}: {count}[/{status_color}]")
                    
            main_table.add_row(
                "Package Status",
                Text("INFO", style="bold blue"),
                f"Total: {total_packages} | Available: {available_packages} | " + " | ".join(status_details)
            )
            
            # Key packages with version details
            packages = system_info['package_versions']
            key_packages = ['PyTorch', 'NumPy', 'Pandas', 'Scikit-learn', 'Rich', 'Optuna']
            key_pkg_details = []
            
            for pkg_name in key_packages:
                # Try different possible keys for the package
                pkg_info = None
                for possible_key in [pkg_name, pkg_name.lower(), pkg_name.replace('-', '_')]:
                    if possible_key in packages:
                        pkg_info = packages[possible_key]
                        break
                if pkg_info:
                    status = pkg_info.get('status', 'UNKNOWN')
                    version = pkg_info.get('version', 'Unknown')
                    
                    # Truncate long version strings
                    if len(version) > 15:
                        version = version[:12] + "..."
                    status_color = "bold green" if status == "OK" else "bold yellow" if status == "WARNING" else "bold red"
                    key_pkg_details.append(f"[{status_color}]{pkg_name}: {version}[/{status_color}]")
                else:
                    key_pkg_details.append(f"[bold red]{pkg_name}: Missing[/bold red]")
            
            main_table.add_row(
                "Key Packages",
                Text("INFO", style="bold blue"),
                " | ".join(key_pkg_details[:3]) + (" | ..." if len(key_pkg_details) > 3 else "")
            )
            
            # Missing required packages
            missing_required = pkg_analysis.get('missing_required', [])
            if missing_required:
                missing_display = ", ".join(missing_required[:3])
                if len(missing_required) > 3:
                    missing_display += f" (and {len(missing_required) - 3} more)"
                main_table.add_row(
                    "Missing Critical",
                    Text("ERROR", style="bold red"),
                    missing_display
                )
            
        else:
            # Fallback package environment check
            main_table.add_row(
                Text("PACKAGE ENVIRONMENT", style="bold white on blue"),
                "",
                ""
            )
            
            fallback_packages = []
            package_checks = [
                ('PyTorch', 'torch', torch if 'torch' in globals() else None),
                ('NumPy', 'numpy', np if 'np' in globals() else None),
                ('Rich', 'rich', None),
                ('Pandas', 'pandas', None)
            ]
            
            for display_name, module_name, module_obj in package_checks:
                try:
                    if module_obj and hasattr(module_obj, '__version__'):
                        version = module_obj.__version__
                        fallback_packages.append(f"[bold green]{display_name}: {version}[/bold green]")
                    else:
                        # Try to import and get version
                        try:
                            imported_module = __import__(module_name)
                            version = getattr(imported_module, '__version__', 'Available')
                            fallback_packages.append(f"[bold green]{display_name}: {version}[/bold green]")
                        except ImportError:
                            fallback_packages.append(f"[bold red]{display_name}: Missing[/bold red]")
                except Exception:
                    fallback_packages.append(f"[bold yellow]{display_name}: Unknown[/bold yellow]")
            
            if fallback_packages:
                main_table.add_row(
                    "Package Status",
                    Text("BASIC", style="bold yellow"),
                    " | ".join(fallback_packages)
                )
        
        # HARDWARE ANALYSIS
        if analysis_available and 'hardware_analysis' in system_info:
            hw_analysis = system_info['hardware_analysis']
            capabilities = hw_analysis.get('capabilities', {})
            
            main_table.add_row(
                Text("HARDWARE ANALYSIS", style="bold white on blue"),
                "",
                ""
            )
            
            # System performance overview
            system_class = hw_analysis.get('system_class', 'unknown')
            performance_score = hw_analysis.get('performance_score', 0)
            overall_health = hw_analysis.get('overall_health', 'unknown')
            
            # Performance class styling
            class_colors = {
                'high_performance': "bold green",
                'standard': "bold blue",
                'limited': "bold yellow",
                'critical': "bold red"
            }
            
            class_style = class_colors.get(system_class, "bold yellow")
            health_colors = {
                'healthy': "bold green",
                'degraded': "bold yellow",
                'critical': "bold red"
            }
            
            health_style = health_colors.get(overall_health, "bold yellow")
            
            # Performance details
            perf_detail = f"Performance Score: {performance_score}/100"
            
            # Add component summary
            components_info = []
            
            if 'components_healthy' in hw_analysis and 'components_detected' in hw_analysis:
                healthy = hw_analysis['components_healthy']
                total = hw_analysis['components_detected']
                warning = hw_analysis.get('components_warning', 0)
                failed = hw_analysis.get('components_failed', 0)
                if total > 0:
                    components_info.append(f"Components: {healthy}/{total} healthy")
                    if warning > 0:
                        components_info.append(f"{warning} warnings")
                    if failed > 0:
                        components_info.append(f"{failed} failed")
            
            if components_info:
                perf_detail += f" | {' | '.join(components_info)}"
            
            main_table.add_row(
                "System Classification",
                Text(system_class.replace('_', ' ').title(), style=class_style),
                perf_detail
            )
            
            main_table.add_row(
                "Overall Health",
                Text(overall_health.upper(), style=health_style),
                ""
            )
            
            # CPU Information with performance metrics
            cpu_info = capabilities.get('cpu', {})
            
            if cpu_info:
                logical_cores = cpu_info.get('logical_cores', 'Unknown')
                physical_cores = cpu_info.get('physical_cores', 'Unknown')
                perf_class = cpu_info.get('performance_class', 'unknown')
                cpu_detail = f"Logical: {logical_cores}, Physical: {physical_cores}"
                
                # Add frequency information
                freq_ghz = cpu_info.get('frequency_ghz')
                max_freq_ghz = cpu_info.get('max_frequency_ghz')
                
                if freq_ghz:
                    cpu_detail += f" | Base: {freq_ghz:.2f}GHz"
                
                if max_freq_ghz and max_freq_ghz != freq_ghz:
                    cpu_detail += f" | Max: {max_freq_ghz:.2f}GHz"
                
                main_table.add_row(
                    "CPU Cores",
                    Text("INFO", style="bold blue"),
                    cpu_detail
                )
                
                # Hyperthreading and performance
                hyperthreading = cpu_info.get('hyperthreading', False)
                ht_style = "bold green" if hyperthreading else "bold yellow"
                perf_detail = f"Performance Class: {perf_class.replace('_', ' ').title()}"
                
                # Add current load if available
                current_load = cpu_info.get('current_load')
                
                if current_load is not None:
                    load_color = "bold green" if current_load < 50 else "bold yellow" if current_load < 80 else "bold red"
                    perf_detail += f" | Load: [{load_color}]{current_load:.1f}%[/{load_color}]"
                
                main_table.add_row(
                    "CPU Performance",
                    Text("HT-ON" if hyperthreading else "HT-OFF", style=ht_style),
                    perf_detail
                )
            
            # Memory Information with usage metrics
            memory_info = capabilities.get('memory', {})
            
            if memory_info:
                total_ram = memory_info.get('total_gb', 'Unknown')
                perf_class = memory_info.get('performance_class', 'unknown')
                mem_detail = f"{total_ram}GB Total | Performance: {perf_class.replace('_', ' ').title()}"
                
                # Add swap information
                swap_gb = memory_info.get('swap_gb', 0)
                has_swap = memory_info.get('has_swap', False)
                
                if has_swap and swap_gb > 0:
                    mem_detail += f" | Swap: {swap_gb:.1f}GB"
                
                main_table.add_row(
                    "System Memory",
                    Text("INFO", style="bold blue"),
                    mem_detail
                )
                
                # Memory usage if available
                usage_percent = memory_info.get('usage_percent')
                
                if usage_percent is not None:
                    used_gb = memory_info.get('used_gb', 0)
                    available_gb = memory_info.get('available_gb', 0)
                    usage_color = "green" if usage_percent < 70 else "yellow" if usage_percent < 85 else "red"
                    usage_detail = f"Used: {used_gb:.1f}GB | Available: {available_gb:.1f}GB"
                    
                    # Add swap usage if available
                    swap_usage = memory_info.get('swap_usage_percent', 0)
                    
                    if swap_usage > 0:
                        swap_color = "yellow" if swap_usage < 50 else "red"
                        usage_detail += f" | Swap: [{swap_color}]{swap_usage:.1f}%[/{swap_color}]"
                    
                    main_table.add_row(
                        "Memory Usage",
                        Text(f"{usage_percent:.1f}%", style=f"bold {usage_color}"),
                        usage_detail
                    )
            
            # GPU Information with metrics
            gpu_info = capabilities.get('gpu', {})
            
            if gpu_info:
                gpu_available = gpu_info.get('available', False)
                gpu_count = gpu_info.get('count', 0)
                total_memory = gpu_info.get('total_memory_gb', 0)
                perf_class = gpu_info.get('performance_class', 'none')
                gpu_style = "bold green" if gpu_available else "bold red"
                gpu_detail = ""
                
                if gpu_available:
                    gpu_detail = f"Count: {gpu_count} | Memory: {total_memory:.1f}GB | Class: {perf_class.replace('_', ' ').title()}"
                    # Add version info
                    cuda_ver = gpu_info.get('cuda_version')
                    cudnn_ver = gpu_info.get('cudnn_version')
                    
                    if cuda_ver:
                        gpu_detail += f" | CUDA: {cuda_ver}"
                    
                    if cudnn_ver:
                        gpu_detail += f" | cuDNN: {cudnn_ver}"
                
                else:
                    gpu_detail = "No CUDA-capable devices detected"
                
                main_table.add_row(
                    "CUDA/GPU Support",
                    Text("AVAILABLE" if gpu_available else "UNAVAILABLE", style=gpu_style),
                    gpu_detail
                )
                
                # Individual GPU details if available
                devices = gpu_info.get('devices', [])
                
                if devices and len(devices) <= 3:  # Show details for up to 3 GPUs
                    for i, device in enumerate(devices):
                        gpu_name = device.get('name', 'Unknown')
                        gpu_memory = device.get('memory_gb', 0)
                        compute_cap = device.get('compute_capability', 'Unknown')
                        device_detail = f"{gpu_name} | {gpu_memory:.1f}GB | Compute: {compute_cap}"
                        
                        # Add utilization if available
                        utilization = device.get('utilization_percent', None)
                        
                        if utilization is not None:
                            util_color = "bold green" if utilization < 50 else "bold yellow" if utilization < 80 else "bold red"
                            device_detail += f" | Usage: [{util_color}]{utilization:.1f}%[/{util_color}]"
                        
                        main_table.add_row(
                            f"GPU {device.get('index', i)}",
                            Text("INFO", style="bold blue"),
                            device_detail
                        )
            
            # Storage Information with performance metrics
            storage_info = capabilities.get('storage', {})
            
            if storage_info:
                total_gb = storage_info.get('total_gb', 0)
                free_gb = storage_info.get('free_gb', 0)
                used_gb = storage_info.get('used_gb', 0)
                usage_percent = storage_info.get('usage_percent', 0)
                perf_class = storage_info.get('performance_class', 'unknown')
                storage_detail = f"Total: {total_gb:.1f}GB | Free: {free_gb:.1f}GB"
                storage_detail += f" | Performance: {perf_class.replace('_', ' ').title()}"
                usage_color = "green" if usage_percent < 70 else "yellow" if usage_percent < 85 else "red"
                
                main_table.add_row(
                    "Storage Capacity",
                    Text("INFO", style="bold blue"),
                    storage_detail
                )
                
                main_table.add_row(
                    "Storage Usage",
                    Text(f"{usage_percent:.1f}%", style=f"bold {usage_color}"),
                    f"Used: {used_gb:.1f}GB | Available: {free_gb:.1f}GB"
                )
        else:
            # Fallback hardware information
            main_table.add_row(
                Text("HARDWARE ANALYSIS", style="bold white on blue"),
                "",
                ""
            )
            
            if 'hardware' in system_info:
                hw = system_info['hardware']
                
                # Device information
                device = hw.get('device', 'Unknown')
                
                main_table.add_row(
                    "Primary Device",
                    Text("INFO", style="bold blue"),
                    device.upper()
                )
                
                # CPU information
                cpu_info = hw.get('cpu_cores', {})
                
                if cpu_info:
                    logical_cores = cpu_info.get('logical_cores', os.cpu_count() or 'Unknown')
                    physical_cores = cpu_info.get('physical_cores', 'Unknown')
                    cpu_detail = f"Logical: {logical_cores}"
                    if physical_cores != 'Unknown' and physical_cores != logical_cores:
                        cpu_detail += f" | Physical: {physical_cores}"
                    
                    main_table.add_row(
                        "CPU Cores",
                        Text("INFO", style="bold blue"),
                        cpu_detail
                    )
                
                # CUDA information
                if 'cuda' in hw:
                    cuda_info = hw['cuda']
                    cuda_available = cuda_info.get('available', False)
                    cuda_style = "bold green" if cuda_available else "bold red"
                    cuda_detail = ""
                    
                    if cuda_available:
                        gpu_count = cuda_info.get('gpu_count', 0)
                        cuda_detail = f"GPU Count: {gpu_count}"
                        # Add basic GPU info if available
                        gpus = cuda_info.get('gpus', [])
                        
                        if gpus and len(gpus) > 0:
                            total_memory = sum(gpu.get('memory_gb', 0) for gpu in gpus)
                            cuda_detail += f" | Total Memory: {total_memory:.1f}GB"
                    
                    else:
                        cuda_detail = "No CUDA support detected"
                    
                    main_table.add_row(
                        "CUDA Support",
                        Text("YES" if cuda_available else "NO", style=cuda_style),
                        cuda_detail
                    )
                
                # Memory information
                ram_info = hw.get('system_ram', {})
                
                if ram_info and ram_info.get('available'):
                    total_ram = ram_info.get('ram_total_gb', 0)
                    main_table.add_row(
                        "System Memory",
                        Text("INFO", style="bold blue"),
                        f"{total_ram:.1f}GB Total"
                    )
            
            else:
                # Basic fallback using system calls
                cpu_cores = os.cpu_count() or 'Unknown'
                
                main_table.add_row(
                    "CPU Cores",
                    Text("INFO", style="bold blue"),
                    str(cpu_cores)
                )
                
                # Basic CUDA check
                cuda_available = 'torch' in globals() and torch.cuda.is_available()
                cuda_style = "bold green" if cuda_available else "bold red"
                cuda_detail = ""
                
                if cuda_available:
                    gpu_count = torch.cuda.device_count()
                    cuda_detail = f"GPU Count: {gpu_count}"
                
                main_table.add_row(
                    "CUDA Support",
                    Text("YES" if cuda_available else "NO", style=cuda_style),
                    cuda_detail
                )
        
        # CURRENT CONFIGURATION
        if config:
            main_table.add_row(
                Text("CURRENT CONFIGURATION", style="bold white on blue"),
                "",
                ""
            )
            
            # Configuration metadata
            metadata = config.get('metadata', {})
            
            if metadata:
                config_version = metadata.get('config_version', 'Unknown')
                config_type = metadata.get('config_type', 'Unknown')
                
                main_table.add_row(
                    "Config Version",
                    Text("INFO", style="bold blue"),
                    f"v{config_version} ({config_type})"
                )
                
                # Preset information
                if preset_name:
                    preset_style = "bold green" if preset_name in ['Default', 'Performance', 'Stability'] else "bold yellow"
                    
                    # Add preset compatibility info
                    compatibility = metadata.get('compatibility', [])
                    preset_detail = preset_name
                    
                    if compatibility:
                        preset_detail += f" | Compatible: {', '.join(compatibility[:2])}"
                        if len(compatibility) > 2:
                            preset_detail += "..."
                    
                    main_table.add_row(
                        "Active Preset",
                        Text(preset_name.upper(), style=preset_style),
                        preset_detail
                    )
                
                # Creation/modification time
                created = metadata.get('created')
                last_modified = metadata.get('last_modified') or metadata.get('modified')
                
                if created:
                    created_display = created[:19].replace('T', ' ')
                    time_detail = f"Created: {created_display}"
                    
                    if last_modified and last_modified != created:
                        modified_display = last_modified[:19].replace('T', ' ')
                        time_detail += f" | Modified: {modified_display}"
                    
                    main_table.add_row(
                        "Timestamps",
                        Text("INFO", style="bold blue"),
                        time_detail
                    )
            
            # Training configuration
            training_config = config.get('training', {})
            
            if training_config:
                # Core training parameters
                epochs = training_config.get('epochs', 'Unknown')
                batch_size = training_config.get('batch_size', 'Unknown')
                learning_rate = training_config.get('learning_rate', 'Unknown')
                optimizer = training_config.get('optimizer', 'Unknown')
                train_detail = f"Epochs: {epochs} | Batch: {batch_size} | LR: {learning_rate} | Optimizer: {optimizer}"
                
                main_table.add_row(
                    "Training Config",
                    Text("INFO", style="bold blue"),
                    train_detail
                )
                
                # Advanced training features
                features = []
                
                if training_config.get('mixed_precision', False):
                    features.append("[green]Mixed Precision[/green]")
                
                if training_config.get('gradient_accumulation_steps', 1) > 1:
                    steps = training_config.get('gradient_accumulation_steps')
                    features.append(f"[blue]Grad Accum: {steps}[/blue]")
                
                if training_config.get('scheduler'):
                    scheduler = training_config.get('scheduler')
                    features.append(f"[yellow]Scheduler: {scheduler}[/yellow]")
                
                if features:
                    main_table.add_row(
                        "Training Features",
                        Text("INFO", style="bold blue"),
                        " | ".join(features)
                    )
            
            # Model configuration
            model_config = config.get('model', {})
            
            if model_config:
                model_type = model_config.get('model_type', 'Unknown')
                encoding_dim = model_config.get('encoding_dim', 'Unknown')
                
                # Model type styling
                model_colors = {
                    'SimpleAutoencoder': "bold blue",
                    'EnhancedAutoencoder': "bold green", 
                    'AutoencoderEnsemble': "bold magenta"
                }
                
                model_style = model_colors.get(model_type, "bold blue")
                model_detail = f"Encoding Dim: {encoding_dim}"
                
                # Add architecture info
                hidden_dims = model_config.get('hidden_dims', [])
                
                if isinstance(hidden_dims, list) and hidden_dims:
                    arch_summary = f"Architecture: {len(hidden_dims)} layers"
                    
                    if len(hidden_dims) <= 4:
                        arch_summary += f" [{', '.join(map(str, hidden_dims))}]"
                    
                    else:
                        arch_summary += f" [{', '.join(map(str, hidden_dims[:2]))}...{hidden_dims[-1]}]"
                    
                    model_detail += f" | {arch_summary}"
                
                main_table.add_row(
                    "Model Architecture",
                    Text(model_type, style=model_style),
                    model_detail
                )
                
                # Model features
                features = []
                
                if model_config.get('use_batch_norm', False):
                    features.append("[green]Batch Norm[/green]")
                
                if model_config.get('use_layer_norm', False):
                    features.append("[green]Layer Norm[/green]")
                
                if model_config.get('skip_connection', False):
                    features.append("[blue]Skip Connections[/blue]")
                
                if model_config.get('residual_blocks', False):
                    features.append("[blue]Residual Blocks[/blue]")
                
                if model_config.get('use_attention', False):
                    features.append("[magenta]Attention[/magenta]")
                
                # Ensemble-specific info
                if model_type == 'AutoencoderEnsemble':
                    num_models = model_config.get('num_models', 'Unknown')
                    diversity_factor = model_config.get('diversity_factor', 'Unknown')
                    features.insert(0, f"[magenta]Ensemble: {num_models} models (diversity: {diversity_factor})[/magenta]")
                
                if features:
                    main_table.add_row(
                        "Model Features",
                        Text("INFO", style="bold blue"),
                        " | ".join(features)
                    )
            
            # Security and data configuration summary
            security_config = config.get('security', {})
            data_config = config.get('data', {})
            
            if security_config or data_config:
                summary_parts = []
                
                if security_config:
                    percentile = security_config.get('percentile', 'Unknown')
                    threshold = security_config.get('attack_threshold', 'Unknown')
                    summary_parts.append(f"Security: {percentile}th percentile, threshold {threshold}")
                
                if data_config:
                    normal_samples = data_config.get('normal_samples', 'Unknown')
                    attack_samples = data_config.get('attack_samples', 'Unknown')
                    features = data_config.get('features', 'Unknown')
                    summary_parts.append(f"Data: {normal_samples}+{attack_samples} samples, {features} features")
                
                if summary_parts:
                    main_table.add_row(
                        "Security & Data",
                        Text("INFO", style="bold blue"),
                        " | ".join(summary_parts)
                    )
        
        # Print the main table
        console.print(main_table)
        console.print()
        
        # SYSTEM STATUS & HEALTH
        status_table = Table(
            title="[bold yellow]SYSTEM STATUS & HEALTH[/bold yellow]",
            box=box.ROUNDED,
            header_style="bold yellow",
            border_style="white",
            title_style="bold yellow",
            title_justify="left",
            show_lines=True,
            expand=True,
            width=min(120, console.width - 4)
        )
        
        status_table.add_column("Component", style="bold cyan", width=25)
        status_table.add_column("Status", width=12, justify="center")
        status_table.add_column("Details", style="bold", min_width=50, max_width=80)
        
        # Directory check
        try:
            required_dirs = [DEFAULT_MODEL_DIR, LOG_DIR, TB_DIR, CONFIG_DIR]
            dirs_status = [(d, d.exists(), os.access(d, os.W_OK) if d.exists() else False) for d in required_dirs]
            dirs_ok = all(exists for _, exists, _ in dirs_status)
            dirs_writable = all(writable for _, exists, writable in dirs_status if exists)
            
            if dirs_ok and dirs_writable:
                status_style = "bold green"
                status_text = "OK"
                details = "All directories present and writable"
            
            elif dirs_ok:
                status_style = "bold yellow"
                status_text = "WARN"
                details = "Directories exist but some may not be writable"
            
            else:
                status_style = "bold red"
                status_text = "FAIL"
                missing_dirs = [str(d.name) for d, exists, _ in dirs_status if not exists]
                details = f"Missing: {', '.join(missing_dirs[:3])}"
                
                if len(missing_dirs) > 3:
                    details += f" (+{len(missing_dirs)-3} more)"
            
            status_table.add_row(
                "[bold cyan]Directory Structure[/bold cyan]",
                Text(status_text, style=status_style),
                details, style=status_style
            )
        
        except Exception as e:
            status_style = "bold red"
            status_text = "ERROR"
            details = f"Check failed: {str(e)[:50]}..."
            details_style = "bold red"
            
            status_table.add_row(
                "[bold cyan]Directory Structure[/bold cyan]",
                Text(status_text, style=status_style),
                details, style=details_style
            )
        
        # Model variants check
        try:
            if 'MODEL_VARIANTS' in globals() and MODEL_VARIANTS:
                variants_count = len(MODEL_VARIANTS)
                variants_names = list(MODEL_VARIANTS.keys())
                variants_style = "bold green" if variants_count > 0 else "bold yellow"
                status_text = "OK" if variants_count > 0 else "WARN"
                details_style = "bold green" if status_text == "OK" else "bold yellow"
                details = f"{variants_count} variants: {', '.join(variants_names[:3])}"
                
                if len(variants_names) > 3:
                    details += "..."
                
                status_table.add_row(
                    "[bold cyan]Model Variants[/bold cyan]",
                    Text(status_text, style=variants_style),
                    details, style=details_style
                )
            
            else:
                status_table.add_row(
                    "[bold cyan]Model Variants[/bold cyan]", 
                    Text("INIT", style="bold yellow"),
                    "Not initialized or empty", style="bold yellow"
                )
        
        except Exception as e:
            status_style = "bold red"
            status_text = "ERROR"
            details = f"Check failed: {str(e)[:50]}..."
            details_style = "bold red"
            
            status_table.add_row(
                "Model Variants",
                Text(status_text, style=status_style),
                details, style=details_style
            )
        
        # Configuration validation using the config from show_banner
        if config:
            try:
                # Use the validate_config function
                is_valid, errors, warnings = validate_config(config, strict=False)
                
                if is_valid:
                    status_style = "bold green"
                    status_text = "VALID"
                    details = f"Configuration validated successfully"
                    
                    if warnings:
                        details += f" ({len(warnings)} warnings)"
                
                elif errors:
                    status_style = "bold red" 
                    status_text = "INVALID"
                    details = f"{len(errors)} errors found"
                    
                    if warnings:
                        details += f", {len(warnings)} warnings"
                
                else:
                    status_style = "bold yellow"
                    status_text = "WARN"
                    details = f"{len(warnings)} warnings found"
                
                status_table.add_row(
                    "[bold cyan]Configuration[/bold cyan]",
                    Text(status_text, style=status_style),
                    details, style=status_style
                )
            
            except Exception as e:
                status_style = "bold red"
                status_text = "ERROR"
                details = f"Validation failed: {str(e)[:50]}..."
                details_style = "bold red"
                
                status_table.add_row(
                    "[bold cyan]Configuration[/bold cyan]",
                    Text(status_text, style=status_style),
                    details, style=details_style
                )
        
        else:
            status_table.add_row(
                "[bold cyan]Configuration[/bold cyan]",
                Text("MISSING", style="bold yellow"),
                "No configuration loaded", style="bold yellow"
            )
        
        # Memory status if available
        if analysis_available and 'collection_metadata' in system_info:
            metadata = system_info['collection_metadata']
            memory_impact = metadata.get('memory_impact', {})
            
            if memory_impact:
                rss_delta = memory_impact.get('rss_delta_mb', 0)
                system_delta = memory_impact.get('system_delta_gb', 0)
                
                if abs(rss_delta) < 10 and abs(system_delta) < 0.1:
                    mem_style = "bold green"
                    mem_status = "STABLE"
                    mem_details = f"Memory stable (±{abs(rss_delta):.1f}MB process, ±{abs(system_delta*1024):.0f}MB system)"
                
                elif rss_delta > 50 or system_delta > 0.5:
                    mem_style = "bold yellow"
                    mem_status = "HIGH"
                    mem_details = f"Memory usage increased ({rss_delta:+.1f}MB process, {system_delta*1024:+.0f}MB system)"
                
                else:
                    mem_style = "bold blue"
                    mem_status = "NORMAL"
                    mem_details = f"Memory usage: {rss_delta:+.1f}MB process, {system_delta*1024:+.0f}MB system"
                
                status_table.add_row(
                    "[bold cyan] Impact[/bold cyan]",
                    Text(mem_status, style=mem_style),
                    mem_details, style=mem_style
                )
        
        # Optional dependencies status
        if 'optional_dependencies' in system_info:
            opt_deps = system_info['optional_dependencies']
            enabled_count = opt_deps.get('enabled_features', 0)
            total_count = opt_deps.get('feature_count', 0)
            availability_score = opt_deps.get('feature_availability_score', 0)
            
            if availability_score > 80:
                opt_style = "bold green"
                opt_status = "FULL"
            
            elif availability_score > 60:
                opt_style = "bold blue"
                opt_status = "GOOD"  
            
            elif availability_score > 40:
                opt_style = "bold yellow"
                opt_status = "LIMITED"
            
            else:
                opt_style = "bold red"
                opt_status = "MINIMAL"
            
            opt_details = f"{enabled_count}/{total_count} features available ({availability_score:.0f}%)"
            
            status_table.add_row(
                "[bold cyan]Optional Features[/bold cyan]",
                Text(opt_status, style=opt_style),
                opt_details, style=opt_style
            )
        
        # Overall system status
        overall_status = "HEALTHY"
        overall_style = "bold green"
        overall_details = ""
        
        # Collect status information for overall assessment
        status_factors = []
        
        if analysis_available and 'hardware_analysis' in system_info:
            hw_health = system_info['hardware_analysis'].get('overall_health', 'unknown')
            perf_score = system_info['hardware_analysis'].get('performance_score', 0)
            status_factors.append(('hardware', hw_health, perf_score))
            
            if hw_health == 'degraded':
                if overall_status == "HEALTHY":
                    overall_status = "DEGRADED"
                    overall_style = "bold yellow"
            
            elif hw_health == 'critical':
                overall_status = "CRITICAL"
                overall_style = "bold red"
        
        if analysis_available and 'package_analysis' in system_info:
            env_health = system_info['package_analysis'].get('environment_health', {}).get('overall_status', 'unknown')
            compat_score = system_info['package_analysis'].get('environment_health', {}).get('compatibility_score', 0)
            status_factors.append(('packages', env_health, compat_score))
            
            if env_health == 'degraded' and overall_status == "HEALTHY":
                overall_status = "NEEDS ATTENTION"
                overall_style = "bold yellow"
        
        if not analysis_available or not config:
            
            if overall_status in ["HEALTHY", "DEGRADED"]:
                overall_status = "ISSUES DETECTED"
                overall_style = "bold red"
        
        if not collection_successful:
            overall_status = "DATA COLLECTION ISSUES"
            overall_style = "bold red"
        
        # Build overall details
        if status_factors:
            factor_details = []
            for factor_type, health, score in status_factors:
                
                if score > 0:
                    factor_details.append(f"{factor_type}: {health} ({score:.0f}%)")
                
                else:
                    factor_details.append(f"{factor_type}: {health}")
            
            overall_details = " | ".join(factor_details)
        
        status_table.add_row(
            Text("OVERALL STATUS", style="bold white on blue"),
            Text(overall_status, style=overall_style),
            overall_details, style=overall_style
        )
        
        # Print the status table
        console.print(status_table)
        console.print()
        
        # WARNINGS & RECOMMENDATIONS
        if analysis_available and 'detailed_analysis' in system_info:
            analysis = system_info['detailed_analysis']
            
            # Collect all types of recommendations
            all_warnings = []
            
            all_warnings.extend((item, 'System Recommendation', 'HIGH') for item in analysis.get('system_recommendations', []))
            all_warnings.extend((item, 'Compatibility Issue', 'HIGH') for item in analysis.get('compatibility_issues', []))
            all_warnings.extend((item, 'Resource Warning', 'MEDIUM') for item in analysis.get('resource_warnings', []))
            all_warnings.extend((item, 'Performance Optimization', 'MEDIUM') for item in analysis.get('performance_optimizations', []))
            all_warnings.extend((item, 'Configuration Suggestion', 'LOW') for item in analysis.get('configuration_suggestions', []))
            
            if all_warnings:
                recommendations_table = Table(
                    title="[bold yellow]SYSTEM ANALYSIS & RECOMMENDATIONS[/bold yellow]",
                    #box=box.ROUNDED,
                    box=box.DOUBLE_EDGE,
                    header_style="bold cyan",
                    border_style="cyan", 
                    title_style="bold yellow",
                    title_justify="left",
                    show_lines=True,
                    expand=True,
                    width=min(120, console.width - 4)
                )
                
                recommendations_table.add_column("Type", style="bold cyan", width=18)
                recommendations_table.add_column("Priority", width=10, justify="center")
                recommendations_table.add_column("Recommendation", style="bold", min_width=65, max_width=85)
                
                # Sort by priority (HIGH -> MEDIUM -> LOW)
                priority_order = {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}
                all_warnings.sort(key=lambda x: priority_order.get(x[2], 3))
                
                # Add recommendations with styling
                for message, rec_type, priority in all_warnings[:10]:  # Limit to top 10
                    priority_colors = {
                        'HIGH': "bold red",
                        'MEDIUM': "bold yellow", 
                        'LOW': "bold blue"
                    }
                    
                    priority_style = priority_colors.get(priority, "bold blue")
                    message_style = "bold red" if priority == 'HIGH' else "bold yellow" if priority == 'MEDIUM' else "bold blue"
                    
                    # Truncate very long messages
                    display_message = message
                    
                    if len(display_message) > 85:
                        display_message = display_message[:82] + "..."
                    
                    recommendations_table.add_row(
                        rec_type,
                        Text(priority, style=priority_style),
                        display_message, style=message_style
                    )
                
                # Show total count if there are more recommendations
                if len(all_warnings) > 10:
                    recommendations_table.add_row(
                        Text("...", style="bold"),
                        Text("...", style="bold"),
                        Text(f"... and {len(all_warnings) - 10} more recommendations", style="bold italic")
                    )
                
                console.print(recommendations_table)
        
        # Collection errors and warnings if any
        if analysis_available and 'collection_metadata' in system_info:
            metadata = system_info['collection_metadata']
            errors = metadata.get('errors', [])
            warnings = metadata.get('warnings', [])
            
            if errors or warnings:
                issues_table = Table(
                    title="[bold red]COLLECTION ISSUES[/bold red]" if errors else "[bold yellow]COLLECTION WARNINGS[/bold yellow]",
                    box=box.ROUNDED,
                    header_style="bold white",
                    border_style="red" if errors else "yellow",
                    title_style="bold red" if errors else "bold yellow",
                    title_justify="left", 
                    show_lines=True,
                    expand=True,
                    width=min(120, console.width - 4)
                )
                
                issues_table.add_column("Type", style="bold cyan", width=12)
                issues_table.add_column("Issue", style="bold", min_width=80, max_width=100)
                
                for error in errors[:5]:  # Show up to 5 errors
                    display_error = error
                    
                    if len(display_error) > 100:
                        display_error = display_error[:97] + "..."
                    
                    issues_table.add_row(
                        Text("ERROR", style="bold red"),
                        display_error, style="bold red"
                    )
                
                for warning in warnings[:5]:  # Show up to 5 warnings  
                    display_warning = warning
                    
                    if len(display_warning) > 100:
                        display_warning = display_warning[:97] + "..."
                    
                    issues_table.add_row(
                        Text("WARNING", style="bold yellow"),
                        display_warning, style="bold yellow"
                    )
                
                if len(errors) > 5 or len(warnings) > 5:
                    total_remaining = (len(errors) - 5 if len(errors) > 5 else 0) + (len(warnings) - 5 if len(warnings) > 5 else 0)
                    
                    issues_table.add_row(
                        Text("...", style="bold"),
                        Text(f"... and {total_remaining} more issues", style="bold italic")
                    )
                
                console.print(issues_table)
    
    except Exception as e:
        error_msg = f"CRITICAL ERROR in show_system_info(): {e}"
        console.print(f"[bold red]{error_msg}[/bold red]")
        console.print("This indicates a serious system issue that should be investigated.")
        logger.error(error_msg, exc_info=True)
        
        # Last resort basic info with error handling
        try:
            basic_table = Table(
                title="[bold red]BASIC SYSTEM INFO (EMERGENCY FALLBACK)[/bold red]",
                box=box.SIMPLE,
                show_header=False,
                width=min(80, console.width - 4),
                title_style="bold red"
            )
            
            basic_table.add_column("Property", style="bold yellow")
            basic_table.add_column("Value", style="white")
            
            # Basic system information
            try:
                basic_table.add_row("Operating System", f"{platform.system()} {platform.release()}")
            except:
                basic_table.add_row("Operating System", "Unknown")
            
            try:
                basic_table.add_row("Python Version", platform.python_version())
            except:
                basic_table.add_row("Python Version", "Unknown")
            
            try:
                basic_table.add_row("CPU Cores", str(os.cpu_count() or 'Unknown'))
            except:
                basic_table.add_row("CPU Cores", "Unknown")
            
            try:
                basic_table.add_row("Architecture", platform.machine())
            except:
                basic_table.add_row("Architecture", "Unknown")
            
            # PyTorch information if available
            if 'torch' in globals():
                try:
                    pytorch_version = torch.__version__ if hasattr(torch, '__version__') else 'Unknown'
                    basic_table.add_row("PyTorch Version", pytorch_version)
                except:
                    basic_table.add_row("PyTorch Version", "Unknown")
                
                try:
                    cuda_status = "Available" if torch.cuda.is_available() else "Not Available"
                    if torch.cuda.is_available():
                        cuda_status += f" ({torch.cuda.device_count()} devices)"
                    basic_table.add_row("CUDA Support", cuda_status)
                except:
                    basic_table.add_row("CUDA Support", "Unknown")
                
            basic_table.add_row("Error Context", str(e)[:50] + "..." if len(str(e)) > 50 else str(e))
            
            console.print(basic_table)
            
        except Exception as basic_error:
            console.print(f"[bold red]Even basic system info failed: {basic_error}[/bold red]")
            console.print(f"[bold red]Original error: {e}[/bold red]")
            console.print("[bold red]System is in critical failure state[/bold red]")

def _handle_hpo_result(result: Optional[Dict[str, Any]], hpo_type: str) -> None:
    """Handle and display HPO results with appropriate formatting"""
    try:
        if result is None:
            console.print(
                Panel.fit(
                    "[bold yellow]HPO operation was cancelled by user.[/bold yellow]",
                    title="HPO CANCELLED",
                    style="bold yellow",
                    border_style="yellow",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            return
        
        success = result.get('success', False)
        
        if success:
            n_completed = result.get('n_trials_completed', 0)
            best_value = result.get('best_value', 'N/A')
            total_time = result.get('total_time_minutes', 0)
            
            # Format best_value properly
            if isinstance(best_value, (int, float)) and best_value != float('inf'):
                best_value_str = f"{best_value:.6f}"
            else:
                best_value_str = str(best_value)
            
            message = (
                f"{hpo_type} completed successfully!\n\n"
                f"Results Summary:\n"
                f"- Trials Completed: {n_completed}\n"
                f"- Best Objective Value: {best_value_str}\n"
                f"- Total Time: {total_time:.1f} minutes\n\n"
                "Detailed results and visualizations have been generated.\n"
                "Check the study directory for complete analysis."
            )
            
            console.print(
                Panel.fit(
                    f"[bold green]{message}[/bold green]",
                    title="HPO SUCCESS",
                    style="bold green",
                    border_style="green",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
            # Show artifacts if available
            artifacts = []
            if result.get('saved_files'):
                artifacts.extend([f"Study Files: {len(result['saved_files'])} files"])
            if result.get('plots'):
                artifacts.extend([f"Visualizations: {len(result['plots'])} plots"])
            if result.get('final_model_training', {}).get('success'):
                artifacts.append("Final Model: Trained successfully")
            
            if artifacts:
                print(Fore.CYAN + Style.BRIGHT + "\nGenerated Artifacts:")
                for artifact in artifacts:
                    print(Fore.WHITE + f"  - {artifact}")
        else:
            error = result.get('error', 'Unknown error')
            error_type = result.get('error_type', 'UnknownError')
            n_completed = result.get('n_trials_completed', 0)
            
            message = (
                f"{hpo_type} encountered issues.\n\n"
                f"Error: {error}\n"
                f"Type: {error_type}\n"
            )
            
            if n_completed > 0:
                message += f"\nPartial Results:\n• {n_completed} trials completed\n"
                best_value = result.get('best_value')
                if best_value and best_value != float('inf'):
                    # Format best_value properly for partial results too
                    if isinstance(best_value, (int, float)):
                        best_value_str = f"{best_value:.6f}"
                    else:
                        best_value_str = str(best_value)
                    message += f"- Best value found: {best_value_str}\n"
                message += "\nSome optimization progress was made despite the error."
            
            console.print(
                Panel.fit(
                    f"[bold red]{message}[/bold red]",
                    title="HPO ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
            # Show recommendations if available
            recommendations = result.get('recommendations', [])
            if recommendations:
                print(Fore.YELLOW + Style.BRIGHT + "\nRecommendations:")
                for i, rec in enumerate(recommendations, 1):
                    print(Fore.WHITE + f"  {i}. {rec}")
    
    except Exception as e:
        logger.error(f"Error handling HPO result: {e}", exc_info=True)
        console.print(
            Panel.fit(
                f"[bold red]Error processing HPO results: {str(e)}[/bold red]",
                title="RESULT PROCESSING ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )

def _hpo_preset_selection_menu(config: Dict[str, Any]) -> None:
    """Interactive menu for selecting HPO presets"""
    while True:
        try:
            # Clear screen and show enhanced banner with configuration
            print("\033c", end="")
            config = show_banner(return_config=True)
            
            # Use the config returned from show_banner or fallback
            if config is None:
                config = get_current_config()
            
            # Get current configuration sections for enhanced display
            hpo_config = config.get('hyperparameter_optimization', {})
            presets_config = config.get('presets', {})
            model_config = config.get('model', {})
            
            # Enhanced context display using configuration from banner
            model_type = model_config.get('model_type', 'Unknown')
            current_preset = "Custom/Default"
            if 'presets' in config and isinstance(config['presets'], dict):
                current_preset = config['presets'].get('current_preset', 'Custom/Default')
            
            # Get available HPO-enabled presets
            hpo_presets = {}
            for name, preset_config in PRESET_CONFIGS.items():
                hpo_config = preset_config.get('hyperparameter_optimization', {})
                if hpo_config.get('enabled', False) or 'hyperparameter_optimization' in preset_config:
                    hpo_presets[name] = {
                        'config': preset_config,
                        'hpo_config': hpo_config,
                        'description': preset_config.get('metadata', {}).get('description', f'{name.title()} preset'),
                        'trials': hpo_config.get('n_trials', 50),
                        'strategy': hpo_config.get('strategy', 'optuna'),
                        'model_type': preset_config.get('model', {}).get('model_type', 'Unknown')
                    }
            
            # Enhanced header display
            print(Fore.YELLOW + Style.BRIGHT + "\n" + "="*40)
            print(Fore.CYAN + Style.BRIGHT + "HPO PRESET SELECTION MENU")
            print(Fore.YELLOW + Style.BRIGHT + "="*40)
            print(Fore.GREEN + Style.BRIGHT + f"Active Context:")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Current Preset: " + Fore.CYAN + Style.BRIGHT + f"{current_preset}")
            print(Fore.WHITE + Style.BRIGHT + f"  ├─ Model Type: " + Fore.CYAN + Style.BRIGHT + f"{model_type}")
            print(Fore.WHITE + Style.BRIGHT + f"  └─ Available HPO Presets: " + Fore.CYAN + Style.BRIGHT + f"{len(hpo_presets)}")
            print(Fore.YELLOW + Style.BRIGHT + "-"*40)
            
            if not hpo_presets:
                console.print(
                    Panel.fit(
                        "[bold yellow]No HPO-enabled presets found.\nUsing express setup instead.[/bold yellow]",
                        title="NO HPO PRESETS",
                        style="bold yellow",
                        border_style="yellow",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                
                confirm = input(Fore.YELLOW + Style.BRIGHT + "\nProceed with express setup instead? (Y/n): ").strip().lower()
                if confirm not in ('', 'y', 'yes'):
                    print(Fore.RED + Style.BRIGHT + "Express setup cancelled.")
                    continue
                
                result = run_hyperparameter_optimization_interactive(
                    use_current_config=False,
                    config=config
                )
                _handle_hpo_result(result, "Express HPO")
                return
            
            # Enhanced preset listing with context-aware display
            print(Fore.YELLOW + Style.BRIGHT + "\nAvailable HPO Presets:")
            print(Fore.YELLOW + Style.BRIGHT + "-"*40)
            
            preset_list = list(hpo_presets.items())
            for i, (name, info) in enumerate(preset_list, 1):
                # Color code based on strategy
                strategy_color = Fore.GREEN if info['strategy'] == 'optuna' else Fore.CYAN
                
                print(Fore.CYAN + Style.BRIGHT + f"{i}. {name.upper()}")
                print(Fore.WHITE + Style.BRIGHT + f"   Description: " + Fore.GREEN + Style.BRIGHT + f"{info['description']}")
                print(Fore.WHITE + Style.BRIGHT + f"   Trials: " + Fore.CYAN + Style.BRIGHT + f"{info['trials']}")
                print(Fore.WHITE + Style.BRIGHT + f"   Strategy: " + strategy_color + Style.BRIGHT + f"{info['strategy'].upper()}")
                print(Fore.WHITE + Style.BRIGHT + f"   Model: " + Fore.YELLOW + Style.BRIGHT + f"{info['model_type']}")
                
                # Show additional preset info if available
                metadata = info['config'].get('metadata', {})
                if 'recommended_for' in metadata:
                    print(Fore.WHITE + Style.BRIGHT + f"   Recommended for: " + Fore.MAGENTA + Style.BRIGHT + f"{metadata['recommended_for']}")
                
                # Show optimization focus if available
                hpo_config = info['hpo_config']
                if 'optimization_focus' in hpo_config:
                    focus = hpo_config['optimization_focus']
                    focus_color = Fore.GREEN if focus == 'accuracy' else Fore.CYAN
                    print(Fore.WHITE + Style.BRIGHT + f"   Focus: " + focus_color + Style.BRIGHT + f"{focus.title()}")
                
                print()
            
            print(Fore.RED + Style.BRIGHT + "0. Back to HPO Menu")
            
            choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect preset (0-{len(preset_list)}): ").strip()
            
            try:
                choice_num = int(choice)
                
                if choice_num == 0:
                    return
                elif 1 <= choice_num <= len(preset_list):
                    selected_name, selected_info = preset_list[choice_num - 1]
                    
                    # Enhanced confirmation with preset details
                    print(Fore.GREEN + Style.BRIGHT + f"\nSelected preset: {selected_name.upper()}")
                    print(Fore.GREEN + Style.BRIGHT + f"Description: {selected_info['description']}")
                    print(Fore.GREEN + Style.BRIGHT + f"Trials: {selected_info['trials']}")
                    print(Fore.GREEN + Style.BRIGHT + f"Strategy: {selected_info['strategy']}")
                    print(Fore.GREEN + Style.BRIGHT + f"Model: {selected_info['model_type']}")
                    
                    confirm = input(Fore.YELLOW + Style.BRIGHT + "\nProceed with this preset? (Y/n): ").strip().lower()
                    if confirm not in ('', 'y', 'yes'):
                        print(Fore.RED + Style.BRIGHT + "Preset selection cancelled.")
                        continue
                    
                    result = run_hyperparameter_optimization_interactive(
                        preset=selected_name,
                        config=selected_info['config']
                    )
                    _handle_hpo_result(result, f"{selected_name.title()} Preset HPO")
                    
                else:
                    print(Fore.RED + Style.BRIGHT + f"Invalid selection '{choice}'. Please enter a number from 0-{len(preset_list)}.")
            
            except ValueError:
                print(Fore.RED + Style.BRIGHT + f"Invalid input '{choice}'. Please enter a valid number.")
        
        except KeyboardInterrupt:
            print(Fore.YELLOW + Style.BRIGHT + "\nHPO preset selection interrupted by user")
            break
        except Exception as e:
            logger.error(f"HPO preset selection error: {e}", exc_info=True)
            # Enhanced error message with context
            message = (
                f"Error encountered during HPO preset selection: {str(e)}\n\n"
                f"Context:\n"
                f"- Current Preset: {current_preset}\n"
                f"- Model Type: {model_type}\n"
                f"- Available Presets: {len(hpo_presets)}\n\n"
                f"This could be due to:\n"
                f"- Configuration file issues\n"
                f"- Preset definition problems\n"
                f"- Missing dependencies\n"
                f"- System resource constraints\n\n"
                f"Please check the logs for detailed information."
            )
            console.print(
                Panel.fit(
                    f"[bold red]{message}[/bold red]",
                    title="HPO PRESET SELECTION ERROR",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
        
        # Only continue if we're not returning to previous menu
        if choice != "0":
            try:
                input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nReturning to HPO menu...")
                break

def _run_quick_hpo_test(config: Dict[str, Any]) -> None:
    """Run a quick HPO test with minimal parameters"""
    try:
        # clear screen and show banner
        console.clear()
        show_banner()
        
        message = (
            "Quick HPO Test runs a fast hyperparameter\n"
            "optimization for testing purposes.\n\n"
            "Configuration:\n"
            "- 10 optimization trials\n"
            "- 5 epochs per trial\n"
            "- Single model type\n"
            "- 3-fold cross-validation\n"
            "- Synthetic data\n"
            "- Estimated time: 5-15 minutes\n\n"
            "Perfect for: System testing, configuration\n"
            "validation, quick experiments\n"
        )
        console.print(
            Panel.fit(
                f"[bold blue]{message}[/bold blue]",
                title="QUICK HPO TEST",
                subtitle="Fast optimization for testing",
                style="bold blue",
                border_style="blue",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        confirm = input(Fore.YELLOW + Style.BRIGHT + "Start quick HPO test? (Y/n): ").strip().lower()
        if confirm not in ('', 'y', 'yes'):
            print(Fore.RED + Style.BRIGHT + "Quick test cancelled.")
            return
        
        console.print(
            Panel.fit(
                "[bold green]Starting Quick HPO Test...\nThis should complete in 5-15 minutes.[/bold green]",
                title="QUICK HPO TEST IN PROGRESS",
                style="bold green",
                border_style="green",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        # Create quick test configuration
        quick_config = deepcopy(config) if config else {}
        
        hpo_config = quick_config.setdefault('hyperparameter_optimization', {})
        hpo_config.update({
            'enabled': True,
            'n_trials': 10,
            'timeout': 900,  # 15 minutes max
            'strategy': 'optuna',
            'trial_epochs': 5,
            'trial_patience': 3,
            'model_search': {
                'enabled': False,
                'model_types': ['EnhancedAutoencoder']
            },
            'cross_validation': {
                'enabled': True,
                'folds': 3,
                'shuffle': True
            },
            'early_stopping': {
                'enabled': True,
                'patience': 5,
                'min_trials': 5
            },
            'generate_plots': True,
            'study_name': f"quick_hpo_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        })
        
        # Ensure synthetic data
        data_config = quick_config.setdefault('data', {})
        data_config.update({
            'use_real_data': False,
            'normal_samples': 5000,
            'attack_samples': 1000,
            'features': 20
        })
        
        result = run_hyperparameter_optimization_interactive(
            use_real_data=False,
            config=quick_config,
            non_interactive=True
        )
        
        _handle_hpo_result(result, "Quick HPO Test")
    
    except Exception as e:
        logger.error(f"Quick HPO test error: {e}", exc_info=True)
        console.print(
            Panel.fit(
                f"[bold red]Quick HPO test failed: {str(e)}[/bold red]",
                title="QUICK TEST ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )

def _run_hpo_model_comparison(config: Dict[str, Any]) -> None:
    """Run HPO comparing all available model types"""
    try:
        # clear screen
        console.clear()
        
        available_models = ['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble']
        
        message = (
            f"HPO Model Comparison optimizes all {len(available_models)} model architectures\n"
            "and provides detailed performance analysis.\n\n"
            "Models to compare:\n"
            f"{chr(10).join('  - ' + model for model in available_models)}\n\n"
            "This comprehensive comparison will:\n"
            "- Optimize each model type independently\n"
            "- Compare performance across architectures\n"
            "- Identify best model for your data\n"
            "- Generate comparative visualizations\n\n"
            "Estimated time: 1-3 hours depending on configuration\n"
        )
        console.print(
            Panel.fit(
                f"[bold purple]{message}[/bold purple]",
                title="HPO MODEL COMPARISON",
                subtitle="Compare all model architectures",
                style="bold purple",
                border_style="purple",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        # small delay to ensure panel is rendered before proceeding
        time.sleep(3)
        
        # clear screen and show banner
        console.clear()
        show_banner()
        
        # Configuration options
        print(Fore.YELLOW + Style.BRIGHT + "\nComparison Configuration:")
        print(Fore.WHITE + Style.BRIGHT + "1. Quick Comparison (10 trials per model)")
        print(Fore.WHITE + Style.BRIGHT + "2. Standard Comparison (50 trials per model)")
        print(Fore.WHITE + Style.BRIGHT + "3. Thorough Comparison (100 trials per model)")
        print(Fore.WHITE + Style.BRIGHT + "4. Custom Configuration")
        print(Fore.RED + Style.BRIGHT + "0. Cancel")
        
        choice = input(Fore.YELLOW + Style.BRIGHT + "\nSelect configuration (0-4): ").strip()
        
        if choice == "0":
            print(Fore.RED + Style.BRIGHT + "Model comparison cancelled.")
            return
        
        # Set parameters based on choice
        trial_configs = {
            '1': {'trials': 10, 'epochs': 5, 'timeout': 60, 'name': 'Quick'},
            '2': {'trials': 50, 'epochs': 20, 'timeout': 120, 'name': 'Standard'}, 
            '3': {'trials': 100, 'epochs': 25, 'timeout': 180, 'name': 'Thorough'},
            '4': None  # Custom configuration
        }
        
        if choice in trial_configs and trial_configs[choice] is not None:
            trial_config = trial_configs[choice]
            n_trials = trial_config['trials']
            trial_epochs = trial_config['epochs']
            timeout_minutes = trial_config['timeout']
            config_name = trial_config['name']
        elif choice == '4':
            # Custom configuration
            try:
                n_trials = int(input(Fore.YELLOW + Style.BRIGHT + "Trials per model (50): ") or "50")
                trial_epochs = int(input(Fore.YELLOW + Style.BRIGHT + "Epochs per trial (20): ") or "20")
                timeout_minutes = int(input(Fore.YELLOW + Style.BRIGHT + "Timeout in minutes (120): ") or "120")
                config_name = "Custom"
            except ValueError:
                print(Fore.RED + Style.BRIGHT + "Invalid input. Using default values.")
                n_trials, trial_epochs, timeout_minutes, config_name = 50, 20, 120, "Default"
        else:
            print(Fore.RED + Style.BRIGHT + "Invalid choice. Using standard configuration.")
            n_trials, trial_epochs, timeout_minutes, config_name = 50, 20, 120, "Standard"
        
        # Confirm configuration
        total_trials = n_trials * len(available_models)
        estimated_hours = (total_trials * trial_epochs * 0.5) / 60  # Rough estimate
        
        print(Fore.YELLOW + Style.BRIGHT + f"\n{config_name} Model Comparison Configuration:")
        print(Fore.CYAN + Style.BRIGHT + f"  - Models: {len(available_models)} architectures")
        print(Fore.CYAN + Style.BRIGHT + f"  - Trials per model: {n_trials}")
        print(Fore.CYAN + Style.BRIGHT + f"  - Total trials: {total_trials}")
        print(Fore.CYAN + Style.BRIGHT + f"  - Epochs per trial: {trial_epochs}")
        print(Fore.CYAN + Style.BRIGHT + f"  - Timeout: {timeout_minutes} minutes")
        print(Fore.CYAN + Style.BRIGHT + f"  - Estimated time: {estimated_hours:.1f} hours")
        
        confirm = input(Fore.YELLOW + Style.BRIGHT + "\nStart model comparison? (Y/n): ").strip().lower()
        if confirm not in ('', 'y', 'yes'):
            print(Fore.RED + Style.BRIGHT + "Model comparison cancelled.")
            return
        
        print("\033c", end="")
        show_banner()
        
        console.print(
            Panel.fit(
                f"[bold green]Starting {config_name} HPO Model Comparison...\n"
                f"Comparing {len(available_models)} model architectures\n"
                f"Total trials: {total_trials}\n"
                f"Estimated completion: {estimated_hours:.1f} hours[/bold green]",
                title="HPO MODEL COMPARISON IN PROGRESS",
                style="bold green",
                border_style="green",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        
        # Create comparison configuration
        comparison_config = deepcopy(config) if config else {}
        
        hpo_config = comparison_config.setdefault('hyperparameter_optimization', {})
        hpo_config.update({
            'enabled': True,
            'n_trials': n_trials,
            'timeout': timeout_minutes * 60,
            'strategy': 'optuna',
            'trial_epochs': trial_epochs,
            'trial_patience': max(5, trial_epochs // 4),
            'model_search': {
                'enabled': True,
                'model_types': available_models,
                'search_all_models': True
            },
            'cross_validation': {
                'enabled': True,
                'folds': 5,  # More folds for better comparison
                'shuffle': True
            },
            'early_stopping': {
                'enabled': True,
                'patience': max(10, n_trials // 10),
                'min_trials': 20
            },
            'generate_plots': True,
            'study_name': f"model_comparison_{config_name.lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        })
        
        result = run_hyperparameter_optimization_interactive(
            config=comparison_config,
            non_interactive=True
        )
        
        _handle_hpo_result(result, f"{config_name} Model Comparison HPO")
        
        # Additional comparison analysis if successful
        if result and result.get('success', False):
            _display_model_comparison_summary(result, available_models)
    
    except Exception as e:
        logger.error(f"HPO model comparison error: {e}", exc_info=True)
        console.print(
            Panel.fit(
                f"[bold red]Model comparison failed: {str(e)}[/bold red]",
                title="MODEL COMPARISON ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )

def _display_model_comparison_summary(result: Dict[str, Any], models: List[str]) -> None:
    """Display summary of model comparison results"""
    try:
        study = result.get('study')
        if not study or not hasattr(study, 'trials'):
            print(Fore.YELLOW + Style.BRIGHT + "No detailed comparison data available.")
            return
        
        print(Fore.CYAN + Style.BRIGHT + "\n" + "="*40)
        print("MODEL COMPARISON SUMMARY")
        print("="*40)
        
        # Analyze trials by model type
        model_results = {}
        
        for trial in study.trials:
            if trial.state.name == 'COMPLETE' and trial.params:
                model_type = trial.params.get('model_type', 'Unknown')
                if model_type not in model_results:
                    model_results[model_type] = {
                        'trials': [],
                        'best_value': float('inf'),
                        'worst_value': float('-inf'),
                        'avg_value': 0
                    }
                
                model_results[model_type]['trials'].append(trial.value)
                if trial.value < model_results[model_type]['best_value']:
                    model_results[model_type]['best_value'] = trial.value
                if trial.value > model_results[model_type]['worst_value']:
                    model_results[model_type]['worst_value'] = trial.value
        
        # Calculate averages and display results
        for model_type, data in model_results.items():
            if data['trials']:
                data['avg_value'] = sum(data['trials']) / len(data['trials'])
                data['std_value'] = np.std(data['trials']) if len(data['trials']) > 1 else 0
        
        # Sort models by performance (best average)
        sorted_models = sorted(model_results.items(), key=lambda x: x[1]['avg_value'])
        
        print(f"Model Performance Ranking:")
        for i, (model_type, data) in enumerate(sorted_models, 1):
            trials_count = len(data['trials'])
            print(f"\n{i}. {model_type}")
            print(f"   Completed Trials: {trials_count}")
            print(f"   Best Value: {data['best_value']:.6f}")
            print(f"   Average Value: {data['avg_value']:.6f}")
            if data['std_value'] > 0:
                print(f"   Std Deviation: ±{data['std_value']:.6f}")
            
            # Performance assessment
            if i == 1:
                print(Fore.GREEN + "   Status: BEST PERFORMER" + Style.RESET_ALL)
            elif i == len(sorted_models):
                print(Fore.RED + "   Status: Needs Improvement" + Style.RESET_ALL)
            else:
                print(Fore.YELLOW + "   Status: Good Performance" + Style.RESET_ALL)
        
        # Overall recommendations
        if sorted_models:
            best_model = sorted_models[0][0]
            best_value = sorted_models[0][1]['best_value']
            
            print(f"\n" + Fore.GREEN + Style.BRIGHT + "RECOMMENDATION:")
            print(f"Use {best_model} with best objective value: {best_value:.6f}")
            print(Style.RESET_ALL)
            
            # Performance gap analysis
            if len(sorted_models) > 1:
                second_best = sorted_models[1][1]['avg_value']
                performance_gap = ((second_best - sorted_models[0][1]['avg_value']) / second_best * 100)
                print(f"Performance advantage: {performance_gap:.1f}% better than second-best")
    
    except Exception as e:
        logger.error(f"Error displaying comparison summary: {e}", exc_info=True)
        print(Fore.RED + Style.BRIGHT + f"Could not display detailed comparison: {str(e)}")

# Helper functions for configuration management
def select_preset_config():
    """Preset configuration selection with error handling and context integration."""
    try:
        # Clear screen and show banner with config retrieval
        print("\033c", end="")
        config = show_banner(return_config=True)
        
        # Use the config returned from show_banner or fallback
        if config is None:
            config = get_current_config()
        
        # Extract current context for display
        current_preset = "Custom/Default"
        current_model = "Unknown"
        
        # Extract current preset with multiple fallbacks
        presets_section = config.get("presets", {})
        if isinstance(presets_section, dict):
            current_preset = presets_section.get("current_preset", "Custom/Default")
        
        if current_preset in ["Custom/Default", None, ""]:
            metadata = config.get("metadata", {})
            if isinstance(metadata, dict):
                current_preset = metadata.get("preset_used", "Custom/Default")
        
        if current_preset in ["Custom/Default", None, ""]:
            current_preset = config.get("_preset_name", "Custom/Default")
        
        # Extract current model type
        model_section = config.get("model", {})
        if isinstance(model_section, dict):
            current_model = model_section.get("model_type", "Unknown")
        
        # Clean up preset name display
        if current_preset in ["Custom/Default", None, "", "none"]:
            current_preset = "Custom/Default"
        elif isinstance(current_preset, str):
            current_preset = current_preset.title()
        
        # Menu header with current context
        print(Fore.CYAN + Style.BRIGHT + "\n" + "="*40)
        print(Fore.YELLOW + Style.BRIGHT + "PRESET CONFIGURATION SELECTION")
        print(Fore.CYAN + Style.BRIGHT + "="*40)
        print(Fore.YELLOW + Style.BRIGHT + f"Current Context:")
        print(Fore.GREEN + Style.BRIGHT + f"  ├─ Active Preset: " + Fore.YELLOW + Style.BRIGHT + f"{current_preset}")
        print(Fore.GREEN + Style.BRIGHT + f"  └─ Current Model: " + Fore.YELLOW + Style.BRIGHT + f"{current_model}")
        
        # Check if presets are available
        if not PRESET_CONFIGS:
            message = (
                f"No preset configurations available.\n"
                f"This could be due to:\n"
                f"- Missing preset configuration files\n"
                f"- Corrupted preset definitions\n"
                f"- Installation issues\n\n"
                f"Please check the configuration directory and logs."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="NO PRESETS AVAILABLE",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            return
        
        # Display available presets table
        print(Fore.YELLOW + Style.BRIGHT + "\nAvailable Preset Configurations:")
        
        # Create main table for preset selection
        preset_table = Table(
            box=box.ROUNDED,
            header_style="bold white",
            border_style="white",
            show_header=True,
            show_lines=True,
            width=min(140, console.width - 4)
        )
        
        # Define columns
        preset_table.add_column("#", style="bold cyan", width=5, justify="center")
        preset_table.add_column("Preset Name", style="bold green", width=20)
        preset_table.add_column("Description", style="bold", width=40)
        preset_table.add_column("Model", style="bold blue", width=25)
        preset_table.add_column("Training", style="bold yellow", width=30)
        
        presets = list(PRESET_CONFIGS.items())
        for i, (name, preset) in enumerate(presets, 1):
            training_config = preset.get('training', {})
            model_config = preset.get('model', {})
            metadata_config = preset.get('metadata', {})
            
            # Highlight current preset
            name_style = "bold green" if name == current_preset else "bold white"
            
            preset_table.add_row(
                str(i),
                Text(name.title(), style=name_style),
                metadata_config.get('description', 'No description')[:50] + "..." if len(metadata_config.get('description', '')) > 50 else metadata_config.get('description', 'No description'),
                model_config.get('model_type', 'N/A'),
                f"Epochs:{training_config.get('epochs', 'N/A')} Batch:{training_config.get('batch_size', 'N/A')}"
            )
        
        console.print(preset_table)
        
        # Add detailed information panels for each preset
        print(Fore.YELLOW + Style.BRIGHT + "\nPreset Details:")
        for i, (name, preset) in enumerate(presets, 1):
            training_config = preset.get('training', {})
            model_config = preset.get('model', {})
            security_config = preset.get('security', {})
            metadata_config = preset.get('metadata', {})
            data_config = preset.get('data', {})
            
            description = metadata_config.get('description', 'No description')
            if len(description) > 50:
                description = description[:50] + "..."
            
            # Highlight current preset in details
            border_style = "green" if name == current_preset else "cyan"
            title_style = "bold green" if name == current_preset else "bold white"
            
            detail_panel = Panel.fit(
                f"[bold]Description:[/bold] {description}\n"
                f"[bold]Training:[/bold] {training_config.get('epochs', 'N/A')} epochs, "
                f"Batch: {training_config.get('batch_size', 'N/A')}, "
                f"LR: {training_config.get('learning_rate', 'N/A')}\n"
                f"[bold]Model:[/bold] {model_config.get('model_type', 'N/A')}, "
                f"Encoding: {model_config.get('encoding_dim', 'N/A')}, "
                f"Hidden: {len(model_config.get('hidden_dims', []))} layers\n"
                f"[bold]Data:[/bold] Samples: {data_config.get('normal_samples', 'N/A')}, "
                f"Path: {data_config.get('data_path', 'N/A')}\n"
                f"[bold]Security:[/bold] Threshold: {security_config.get('attack_threshold', 'N/A')}, "
                f"Percentile: {security_config.get('percentile', 'N/A')}%",
                title=f"[{i}] {name.title()}",
                style=title_style,
                border_style=border_style,
                padding=(1, 2)
            )
            console.print(detail_panel)
        
        # Selection options
        max_choice = len(PRESET_CONFIGS)
        print(Fore.YELLOW + Style.BRIGHT + f"\nSelection Options:")
        print(Fore.YELLOW + Style.BRIGHT + f"  ├─ Choose preset (1-{max_choice})")
        print(Fore.YELLOW + Style.BRIGHT + f"  └─ " + Fore.RED + Style.BRIGHT + "0 to cancel")
        
        # Get user input with retry logic
        choice = None
        while not choice:
            try:
                choice = input(Fore.YELLOW + Style.BRIGHT + f"\nSelect preset (1-{max_choice}) or 0 to cancel: ").strip()
                
                # If empty input, retry
                if not choice:
                    continue
                    
            except (EOFError, KeyboardInterrupt):
                print(Fore.RED + Style.BRIGHT + "\nSelection cancelled...")
                return
        
        if choice == "0":
            print(Fore.RED + Style.BRIGHT + "Preset selection cancelled.")
            return
        
        if choice.isdigit() and 1 <= int(choice) <= max_choice:
            preset_name = presets[int(choice)-1][0]
            
            try:
                # Use deepcopy to avoid modifying original
                preset_config = deepcopy(PRESET_CONFIGS[preset_name])
                preset_training_config = preset_config.get('training', {})
                preset_model_config = preset_config.get('model', {})
                preset_data_config = preset_config.get('data', {})
                preset_metadata_config = preset_config.get('metadata', {})
                
                description = preset_metadata_config.get('description', 'No description')
                if len(description) > 50:
                    description = description[:50] + "..."
                
                # Show confirmation pane
                confirm_panel = Panel.fit(
                    f"[bold]Selected Preset:[/bold] [green]{preset_name.title()}[/green]\n"
                    f"[bold]Description:[/bold] {description}\n"
                    f"[bold]Model Type:[/bold] {preset_model_config.get('model_type', 'N/A')}\n"
                    f"[bold]Training:[/bold] {preset_training_config.get('epochs', 'N/A')} epochs, "
                    f"Batch: {preset_training_config.get('batch_size', 'N/A')}, "
                    f"LR: {preset_training_config.get('learning_rate', 'N/A')}\n"
                    f"[bold]Data:[/bold] Samples: {preset_data_config.get('normal_samples', 'N/A')}, "
                    f"Path: {preset_data_config.get('data_path', 'N/A')}",
                    title="[bold]PRESET CONFIRMATION[/bold]",
                    style="bold",
                    border_style="green",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
                console.print(confirm_panel)
                
                # Confirmation prompt
                confirm = None
                while not confirm:
                    try:
                        confirm_input = input(Fore.YELLOW + Style.BRIGHT + "\nApply this configuration? (Y/n): ").lower().strip()
                        
                        if not confirm_input:
                            confirm = 'y'  # Default to yes
                        elif confirm_input in ('y', 'yes', 'n', 'no'):
                            confirm = confirm_input
                        else:
                            print(Fore.RED + Style.BRIGHT + "Please enter Y or N")
                            confirm = None
                            
                    except (EOFError, KeyboardInterrupt):
                        print(Fore.RED + Style.BRIGHT + "\nConfirmation cancelled...")
                        return
                
                if confirm in ('y', 'yes'):
                    try:
                        # Get current configuration
                        current_config = get_current_config()
                        
                        # Create a deep copy to avoid modifying the original
                        merged_config = deepcopy(current_config)
                        
                        # Update the preset information BEFORE merging (matching your pattern)
                        if 'presets' not in merged_config:
                            merged_config['presets'] = {}
                        
                        # Set the current preset name
                        merged_config['presets']['current_preset'] = preset_name
                        merged_config['presets']['last_applied'] = datetime.now().isoformat()
                        
                        # Update metadata to reflect preset usage
                        if 'metadata' not in merged_config:
                            merged_config['metadata'] = {}
                        
                        merged_config['metadata']['preset_used'] = preset_name
                        merged_config['metadata']['modified'] = datetime.now().isoformat()
                        merged_config['metadata']['last_preset_change'] = datetime.now().isoformat()
                        
                        # Apply the preset configuration using deep_update
                        merged_config = deep_update(merged_config, preset_config)
                        
                        # Ensure preset information is preserved after merge
                        merged_config['presets']['current_preset'] = preset_name
                        merged_config['metadata']['preset_used'] = preset_name
                        
                        # Update global configuration variables
                        update_global_config(merged_config)
                        
                        # Save the updated configuration
                        try:
                            save_config(merged_config)
                            logger.info(f"Successfully applied and saved preset: {preset_name}")
                        except Exception as save_error:
                            logger.warning(f"Preset applied but save failed: {save_error}")
                            # Continue even if save fails - config is still updated in memory
                        
                        # Invalidate cache to ensure fresh config loading
                        invalidate_config_cache()
                        
                        # Verify the preset was actually applied
                        verification_config = get_current_config()
                        verification_preset = verification_config.get('presets', {}).get('current_preset')
                        
                        if verification_preset == preset_name:
                            success_panel = Panel.fit(
                                f"[bold green]✓ Successfully applied preset: [bold yellow]{preset_name.title()}[/bold yellow][/bold green]\n"
                                f"[bold green]Configuration updated with preset settings[/bold green]\n"
                                f"[bold green]Active preset: {verification_preset}[/bold green]",
                                title="[bold]SUCCESS[/bold]",
                                border_style="green",
                                style="bold green",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                            console.print(success_panel)
                            
                            # Show key changes for verification
                            preset_training = preset_config.get('training', {})
                            preset_model = preset_config.get('model', {})
                            preset_data = preset_config.get('data', {})
                            
                            if preset_training or preset_model:
                                changes_panel = Panel.fit(
                                    f"[bold]Key Configuration Changes:[/bold]\n"
                                    f"Model Type: {preset_model.get('model_type', 'N/A')}\n"
                                    f"Epochs: {preset_training.get('epochs', 'N/A')}\n"
                                    f"Batch Size: {preset_training.get('batch_size', 'N/A')}\n"
                                    f"Learning Rate: {preset_training.get('learning_rate', 'N/A')}\n"
                                    f"Encoding Dim: {preset_model.get('encoding_dim', 'N/A')}\n"
                                    f"Data Samples: {preset_data.get('normal_samples', 'N/A')}",
                                    title="[bold]APPLIED SETTINGS[/bold]",
                                    border_style="blue",
                                    style="bold blue",
                                    padding=(1, 2),
                                    box=box.ROUNDED
                                )
                                console.print(changes_panel)
                        else:
                            error_panel = Panel.fit(
                                f"[bold red]Warning: Preset application may not have completed successfully[/bold red]\n"
                                f"Expected: {preset_name}, Current: {verification_preset}\n\n"
                                f"Please try again or check configuration files.",
                                title="[bold]WARNING[/bold]",
                                border_style="yellow",
                                style="bold yellow",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                            console.print(error_panel)
                            
                    except Exception as apply_error:
                        message = (
                            f"Failed to apply preset: {str(apply_error)}\n"
                            f"Context:\n"
                            f"- Selected Preset: {preset_name}\n"
                            f"- Current Model: {current_model}\n\n"
                            f"This could be due to:\n"
                            f"- Configuration file corruption\n"
                            f"- Permission issues\n"
                            f"- Invalid preset structure\n"
                            f"- System resource constraints"
                        )
                        console.print(
                            Panel.fit(
                                f"{message}",
                                title="PRESET APPLICATION ERROR",
                                style="bold red",
                                border_style="red",
                                padding=(1, 2),
                                box=box.ROUNDED
                            )
                        )
                        logger.error(f"Preset application failed: {apply_error}")
                else:
                    print(Fore.RED + Style.BRIGHT + "Configuration not applied.")
                    
            except Exception as selection_error:
                message = (
                    f"Error during preset selection: {str(selection_error)}\n"
                    f"Context:\n"
                    f"- Selected Option: {choice}\n"
                    f"- Preset Name: {preset_name if 'preset_name' in locals() else 'Unknown'}\n\n"
                    f"Please try selecting a different preset or check preset configuration."
                )
                console.print(
                    Panel.fit(
                        f"{message}",
                        title="SELECTION PROCESSING ERROR",
                        style="bold red",
                        border_style="red",
                        padding=(1, 2),
                        box=box.ROUNDED
                    )
                )
                logger.error(f"Preset selection processing failed: {selection_error}")
        else:
            message = (
                f"Invalid selection: '{choice}'\n"
                f"Please enter a number between 1 and {max_choice}\n"
                f"or 0 to cancel the selection."
            )
            console.print(
                Panel.fit(
                    f"{message}",
                    title="INVALID SELECTION",
                    style="bold red",
                    border_style="red",
                    padding=(1, 2),
                    box=box.ROUNDED
                )
            )
            
    except Exception as e:
        message = (
            f"Unexpected error in preset selection: {str(e)}\n"
            f"This could indicate:\n"
            f"- Preset configuration corruption\n"
            f"- System resource issues\n"
            f"- Dependency problems\n"
            f"- Configuration loading failures\n\n"
            f"Please check the logs for detailed information."
        )
        console.print(
            Panel.fit(
                f"{message}",
                title="PRESET SELECTION ERROR",
                style="bold red",
                border_style="red",
                padding=(1, 2),
                box=box.ROUNDED
            )
        )
        logger.error(f"Preset selection failed: {e}", exc_info=True)
    
    # Only continue if not exiting
    try:
        input(Fore.YELLOW + Style.BRIGHT + "\nPress Enter to continue..." + Style.RESET_ALL)
    except (EOFError, KeyboardInterrupt):
        print(Fore.RED + Style.BRIGHT + "\nReturning to main menu...")

def show_current_config():
    """Display current configuration in formatted rich tables for each section."""
    config = None
    try:
        # clear screen and show banner
        console.clear()
        
        # Get configuration from show_banner function
        config = show_banner(return_config=True)
        
        console.print("\n[bold yellow]CURRENT CONFIGURATION[/bold yellow]\n")
        
        metadata = config.get('metadata', {})
        if metadata:
            meta_table = Table(
                title="[bold yellow]Metadata[/bold yellow]",
                title_justify="left",
                box=box.ROUNDED,
                header_style="bold cyan",
                border_style="blue",
                show_header=False,
                show_lines=True,
                width=min(85, console.width - 4)
            )
            
            meta_table.add_column("Property", style="bold green", width=25)
            meta_table.add_column("Value", style="bold", width=60)
            
            for key, value in metadata.items():
                if key != 'system':
                    if isinstance(value, dict):
                        meta_table.add_row(key, f"{len(value)} nested parameters")
                    elif isinstance(value, list):
                        meta_table.add_row(key, f"{len(value)} items")
                    else:
                        meta_table.add_row(key, str(value))
            
            console.print(meta_table)
            console.print()

        system_info = config.get('metadata', {}).get('system', {})
        if system_info:
            sys_table = Table(
                title="[bold yellow]System Information[/bold yellow]",
                title_justify="left",
                box=box.ROUNDED,
                header_style="bold cyan",
                border_style="green",
                show_header=False,
                show_lines=True,
                width=min(85, console.width - 4)
            )
            
            sys_table.add_column("Component", style="bold green", width=25)
            sys_table.add_column("Value", style="bold", width=60)
            
            for key, value in system_info.items():
                if not isinstance(value, (dict, list)):
                    sys_table.add_row(key.replace('_', ' ').title(), str(value))
            
            console.print(sys_table)
            console.print()

        sections = [
            ('training', 'Training Configuration', 'yellow'),
            ('model', 'Model Architecture', 'magenta'),
            ('security', 'Security Settings', 'red'),
            ('data', 'Data Configuration', 'blue'),
            ('monitoring', 'Monitoring Settings', 'cyan'),
            ('hardware', 'Hardware Requirements', 'green'),
            ('presets', 'Preset Information', 'bright_blue'),
            ('hyperparameter_optimization', 'HPO Settings', 'bright_magenta'),
            ('validation', 'Validation Settings', 'bright_yellow'),
            ('experimental', 'Experimental Features', 'bright_red')
        ]
        
        for section_key, section_title, color in sections:
            if section_key in config:
                section_data = config[section_key]
                
                section_table = Table(
                    title=f"[bold yellow]{section_title}[/bold yellow]",
                    title_justify="left",
                    box=box.ROUNDED,
                    header_style=f"bold {color}",
                    border_style=color,
                    show_header=True,
                    show_lines=True,
                    width=min(85, console.width - 4)
                )
                
                section_table.add_column("Parameter", style="bold green", width=25)
                section_table.add_column("Value", style="bold", width=60)
                
                if isinstance(section_data, dict):
                    for key, value in section_data.items():
                        formatted_value = str(value)
                        
                        if isinstance(value, dict):
                            if key == 'synthetic_generation':
                                nested_values = []
                                for k, v in value.items():
                                    nested_values.append(f"{k}={v}")
                                formatted_value = ", ".join(nested_values) if nested_values else "empty"
                            elif key == 'preprocessing':
                                preproc_values = []
                                for k, v in value.items():
                                    preproc_values.append(f"{k}={v}")
                                formatted_value = ", ".join(preproc_values) if preproc_values else "none"
                            elif key == 'performance_optimization':
                                perf_values = []
                                for k, v in value.items():
                                    perf_values.append(f"{k}={v}")
                                formatted_value = ", ".join(perf_values) if perf_values else "none"
                            elif key == 'minimum_system_requirements' or key == 'optimal_system_requirements':
                                req_values = []
                                for k, v in value.items():
                                    req_values.append(f"{k}={v}")
                                formatted_value = ", ".join(req_values) if req_values else "none"
                            elif key == 'tensorboard':
                                tb_values = []
                                for k, v in value.items():
                                    tb_values.append(f"{k}={v}")
                                formatted_value = ", ".join(tb_values) if tb_values else "disabled"
                            elif key == 'scheduler_params':
                                if value:
                                    sched_values = []
                                    for k, v in value.items():
                                        sched_values.append(f"{k}={v}")
                                    formatted_value = ", ".join(sched_values)
                                else:
                                    formatted_value = "none"
                            else:
                                formatted_value = f"{len(value)} parameters"
                                
                        elif isinstance(value, list):
                            if key == 'hidden_dims':
                                formatted_value = f"[{', '.join(map(str, value))}] ({len(value)} layers)"
                            elif key == 'dropout_rates':
                                formatted_value = f"[{', '.join(map(str, value))}]"
                            elif key == 'available_presets':
                                formatted_value = f"{len(value)} presets: " + ", ".join(value[:3]) + ("..." if len(value) > 3 else "")
                            elif key == 'custom_presets_available':
                                formatted_value = f"{len(value)} custom presets" + (f": {', '.join(value[:3])}" if value else "")
                            elif key == 'compatibility':
                                formatted_value = ", ".join(value) if value else "none"
                            elif key == 'metrics_to_track':
                                formatted_value = ", ".join(value) if value else "none"
                            elif key == 'detection_methods':
                                formatted_value = ", ".join(value) if value else "none"
                            elif key == 'alert_levels':
                                formatted_value = ", ".join(value) if value else "none"
                            else:
                                formatted_value = f"[{len(value)} items]"
                        
                        elif isinstance(value, bool):
                            formatted_value = "enabled" if value else "disabled"
                        
                        elif isinstance(value, float):
                            if key == 'learning_rate' or key == 'weight_decay' or key == 'adam_eps':
                                formatted_value = f"{value:.6f}"
                            else:
                                formatted_value = f"{value:.3f}"
                        
                        elif value is None:
                            formatted_value = "none"
                        
                        section_table.add_row(key.replace('_', ' ').title(), formatted_value)
                else:
                    section_table.add_row("Configuration", str(section_data))
                
                console.print(section_table)
                console.print()

        hardware = config.get('hardware', {})
        system_info = config.get('metadata', {}).get('system', {})
        
        if hardware and system_info:
            compat_table = Table(
                title="[bold yellow]Hardware Compatibility Check[/bold yellow]",
                title_justify="left",
                box=box.ROUNDED,
                header_style="bold yellow",
                border_style="bright_white",
                show_header=True,
                width=min(80, console.width - 4)
            )
            
            compat_table.add_column("Component", style="bold cyan", width=15)
            compat_table.add_column("Available", style="bold green", width=15)
            compat_table.add_column("Recommended", style="bold blue", width=20)
            compat_table.add_column("Status", style="bold", width=10, justify="center")
            
            cuda_available = system_info.get('cuda_available', False)
            cuda_devices = system_info.get('cuda_devices', 0)
            recommended_gpu = hardware.get('recommended_gpu_memory', 0)
            
            gpu_status = "OK" if cuda_available and cuda_devices > 0 else "FAIL"
            gpu_style = "bold green" if gpu_status == "OK" else "bold red"
            compat_table.add_row(
                "GPU",
                f"{cuda_devices} devices" if cuda_available else "None",
                f"{recommended_gpu}GB",
                Text(gpu_status, style=gpu_style)
            )
            
            cpu_count = system_info.get('cpu_count', 1)
            min_cpu = hardware.get('minimum_system_requirements', {}).get('cpu_cores', 2)
            recommended_cpu = hardware.get('optimal_system_requirements', {}).get('cpu_cores', 4)
            
            cpu_status = "OK" if cpu_count >= min_cpu else "WARN"
            cpu_style = "bold green" if cpu_status == "OK" else "bold yellow"
            compat_table.add_row(
                "CPU Cores",
                str(cpu_count),
                f"{min_cpu} (min) / {recommended_cpu} (opt)",
                Text(cpu_status, style=cpu_style)
            )
            
            min_ram = hardware.get('minimum_system_requirements', {}).get('ram_gb', 4)
            recommended_ram = hardware.get('optimal_system_requirements', {}).get('ram_gb', 8)
            ram_status = "OK"
            compat_table.add_row(
                "RAM",
                "Unknown",
                f"{min_ram}GB (min) / {recommended_ram}GB (opt)",
                Text(ram_status, style="bold green")
            )
            
            console.print(compat_table)
            console.print()

        presets = config.get('presets', {})
        runtime = config.get('runtime', {})
        current_preset = presets.get('current_preset')
        
        # Gather preset information
        preset_info_content = []
        if current_preset:
            preset_configs = presets.get('preset_configs', {})
            
            if isinstance(preset_configs, dict) and current_preset in preset_configs:
                preset_info = preset_configs[current_preset]
                if isinstance(preset_info, str):
                    description = preset_info
                elif isinstance(preset_info, dict):
                    description = preset_info.get('description', 'No description available')
                else:
                    description = 'No description available'
            else:
                description = 'No description available'
            
            available_count = len(presets.get('available_presets', []))
            custom_count = len(presets.get('custom_presets_available', []))
            last_applied = presets.get('last_applied', 'Unknown')
            # Handle various timestamp formats
            if last_applied != 'Unknown':
                try:
                    if last_applied.endswith('Z'):
                        applied_dt = datetime.fromisoformat(last_applied.replace('Z', '+00:00'))
                    elif 'T' in last_applied:
                        applied_dt = datetime.fromisoformat(last_applied)
                    else:
                        applied_dt = datetime.fromisoformat(last_applied)
                    last_applied = applied_dt.strftime('%Y-%m-%d %H:%M:%S')
                except (ValueError, TypeError):
                    # truncate if invalid
                    last_applied = str(last_applied)[:19]
            else:
                last_applied = 'Unknown'
            
            preset_info_content.extend([
                f"Active Preset: [bold green]{current_preset}[/bold green]",
                f"Description: [dim]{description}[/dim]",
                f"Available Presets: [bold]{available_count}[/bold]",
                f"Custom Presets: [bold]{custom_count}[/bold]",
                f"Last Applied: [dim]{last_applied}[/dim]"
            ])
        else:
            preset_info_content.append("Active Preset: [bold yellow]None (custom configuration)[/bold yellow]")
        
        # Gather runtime information
        config_source = (
            runtime.get('config_source') or 
            config.get('metadata', {}).get('config_source') or
            (presets.get('current_preset', 'unknown') + '_preset' if presets.get('current_preset') else 'Unknown') or
            'Unknown'
        )
        
        config_loaded_at = (
            runtime.get('config_loaded_at') or
            runtime.get('config_generated_at') or
            config.get('metadata', {}).get('last_accessed') or
            config.get('metadata', {}).get('created') or
            'Unknown'
        )
        
        if config_loaded_at != 'Unknown':
            try:
                # Handle various timestamp formats
                if config_loaded_at.endswith('Z'):
                    loaded_dt = datetime.fromisoformat(config_loaded_at.replace('Z', '+00:00'))
                elif 'T' in config_loaded_at:
                    loaded_dt = datetime.fromisoformat(config_loaded_at)
                else:
                    loaded_dt = datetime.fromisoformat(config_loaded_at)
                loaded_str = loaded_dt.strftime('%Y-%m-%d %H:%M:%S')
            except (ValueError, TypeError):
                loaded_str = str(config_loaded_at)[:19]  # Truncate if invalid
        else:
            loaded_str = 'Unknown'
        
        health_info = runtime.get('configuration_health', {})
        
        # Determine health status with intelligent fallback logic
        health_status = health_info.get('status')
        if not health_status:
            # Analyze configuration to determine health
            try:
                is_valid, errors, warnings = validate_config(config, strict=False)
                if is_valid and not warnings:
                    health_status = 'healthy'
                elif is_valid and len(warnings) <= 3:
                    health_status = 'needs_attention' 
                elif is_valid:
                    health_status = 'degraded'
                else:
                    health_status = 'critical'
            except Exception:
                # Final fallback based on config completeness
                required_sections = ['training', 'model', 'security', 'data']
                missing_sections = [s for s in required_sections if s not in config]
                health_status = 'healthy' if not missing_sections else 'needs_attention'
        
        # Get warning and recommendation counts with fallbacks
        warning_count = (
            health_info.get('warning_count') or
            len(runtime.get('system_warnings', [])) or
            len(config.get('metadata', {}).get('validation_warnings', [])) or
            0
        )
        
        recommendation_count = (
            health_info.get('recommendation_count') or
            len(runtime.get('recommendations', [])) or
            0
        )
        
        health_color = "green" if health_status == "healthy" else "yellow" if health_status == "needs_attention" else "red"
        
        # Add runtime information to the content
        preset_info_content.extend([
            "",
            f"Config Source: [bold blue]{config_source}[/bold blue]",
            f"Loaded At: [dim]{loaded_str}[/dim]",
            f"Health Status: [bold {health_color}]{health_status.upper()}[/bold {health_color}]",
            f"Warnings: [bold yellow]{warning_count}[/bold yellow]",
            f"Recommendations: [bold cyan]{recommendation_count}[/bold cyan]"
        ])
        
        # Create combined panel
        combined_panel = Panel.fit(
            "\n".join(preset_info_content),
            title="[bold]Configuration Status & Runtime Information[/bold]",
            border_style="cyan",
            style="bold",
            padding=(1, 2)
        )
        console.print(combined_panel)

    except Exception as e:
        console.print(f"[bold red]Failed to display configuration: {e}[/bold red]")
        try:
            if config is not None and isinstance(config, dict):
                console.print("\n[bold yellow]Fallback: Configuration as JSON:[/bold yellow]")
                console.print(json.dumps(config, indent=2, default=str))
            else:
                try:
                    fallback_config = get_current_config()
                    console.print("\n[bold yellow]Fallback: Fresh configuration as JSON:[/bold yellow]")
                    console.print(json.dumps(fallback_config, indent=2, default=str))
                except Exception as fallback_error:
                    console.print(f"[bold red]Could not retrieve configuration for fallback: {fallback_error}[/bold red]")
                    console.print("[bold yellow]Minimal configuration information:[/bold yellow]")
                    console.print("[bold yellow]Training: batch_size=32, epochs=100[/bold yellow]")
                    console.print("[bold yellow]Model: SimpleAutoencoder with encoding_dim=8[/bold yellow]")
                    console.print("[bold yellow]Status: Emergency fallback active[/bold yellow]")
        except Exception as fallback_error:
            console.print(f"[bold red]Fallback display also failed: {fallback_error}[/bold red]")
            console.print("[bold yellow]Configuration system requires attention[/bold yellow]")

def load_config_from_file():
    """Load configuration from file."""
    file_path = input(Fore.YELLOW + Style.BRIGHT + "\nConfiguration file path: ")
    if file_path:
        try:
            with open(file_path, 'r') as f:
                config = json.load(f)
            update_global_config(config)
            print(Fore.GREEN + Style.BRIGHT + f"Configuration loaded from '{file_path}'")
        except Exception as e:
            print(Fore.RED + Style.BRIGHT + f"Failed to load configuration: {e}")

def validate_config_interactive():
    """Interactive configuration validation."""
    try:
        # clear screen and show banner
        print("\033c", end="")
        #show_banner()
        
        # Get configuration from show_banner function
        config = show_banner(return_config=True)
        
        #config = get_current_config()
        validate_config(config)
        print("\nConfiguration is valid")
    except ValueError as e:
        print(f"\nConfiguration validation failed: {e}")
        
        if input("\nShow detailed validation report? (y/N): ").lower().strip() == 'y':
            print("\nValidation Details:")
            # Additional validation logic could go here

def edit_config_interactive():
    """Interactive configuration editor."""
    # clear screen and show banner
    print("\033c", end="")
    show_banner()
    
    print(Fore.YELLOW + Style.BRIGHT + "\nInteractive configuration editing not yet implemented")
    print(Fore.YELLOW + Style.BRIGHT + "Please use the preset configurations or manual file editing for now.")

def compare_configs_interactive():
    """Interactive configuration comparison."""
    # clear screen and show banner
    print("\033c", end="")
    show_banner()
    
    print(Fore.YELLOW + Style.BRIGHT + "\nConfiguration comparison not yet implemented")
    print(Fore.YELLOW + Style.BRIGHT + "This feature will allow side-by-side comparison of different configurations.")



def main(logger: logging.Logger):
    """Main entry point with comprehensive argument parsing and system orchestration."""
    # Initialize basic logger first (fallback)
    #logger = logging.getLogger(__name__)
    
    # Initialize system to set up logging and configuration
    try:
        system_status, config, logger = initialize_system()
        #validate_config(config)
    except Exception as e:
        # Use fallback logger since initialize_system failed
        logger.warning(f"Configuration initialization failed, using defaults: {e}")
        config = get_current_config()
    
    # If no arguments provided, launch interactive mode
    if len(sys.argv) == 1:
        interactive_main()
        return

    # Initialize configuration system early
    
    # Extract configuration sections for defaults
    training_config = config.get('training', {})
    model_config = config.get('model', {})
    data_config = config.get('data', {})
    security_config = config.get('security', {})
    system_config = config.get('system', {})
    hpo_config = config.get('hyperparameter_optimization', {})
    monitoring_config = config.get('monitoring', {})
    
    # Create argument parser with comprehensive configuration integration
    parser = argparse.ArgumentParser(
        description="Enhanced Anomaly Detection Model Training with Configuration Management",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        epilog="""
Examples:
  %(prog)s --preset development               # Use development preset
  %(prog)s --epochs 100 --batch-size 64     # Custom training parameters
  %(prog)s --hpo-trials 50                   # Hyperparameter optimization
  %(prog)s --show-config                     # Display current configuration
  %(prog)s --compare-models                  # Compare model architectures
        """
    )
    
    # Training configuration group
    training_group = parser.add_argument_group('Training Parameters')
    training_group.add_argument(
        "--epochs",
        type=int,
        default=training_config.get('epochs'),
        help=f"Maximum number of training epochs (config: {training_config.get('epochs', DEFAULT_EPOCHS)})"
    )
    training_group.add_argument(
        "--batch-size",
        type=int,
        default=training_config.get('batch_size'),
        help=f"Training batch size (config: {training_config.get('batch_size', DEFAULT_BATCH_SIZE)})"
    )
    training_group.add_argument(
        "--lr",
        type=float,
        default=training_config.get('learning_rate'),
        help=f"Learning rate (config: {training_config.get('learning_rate', LEARNING_RATE)})"
    )
    training_group.add_argument(
        "--patience",
        type=int,
        default=training_config.get('patience'),
        help=f"Early stopping patience in epochs (config: {training_config.get('patience', EARLY_STOPPING_PATIENCE)})"
    )
    training_group.add_argument(
        "--weight-decay",
        type=float,
        default=training_config.get('weight_decay'),
        help=f"Weight decay for optimizer (config: {training_config.get('weight_decay', WEIGHT_DECAY)})"
    )
    training_group.add_argument(
        "--grad-clip",
        type=float,
        default=training_config.get('gradient_clip'),
        help=f"Gradient clipping value (config: {training_config.get('gradient_clip', GRADIENT_CLIP)})"
    )
    training_group.add_argument(
        "--mixed-precision",
        action="store_true",
        default=training_config.get('mixed_precision', MIXED_PRECISION),
        help="Enable mixed precision training"
    )
    
    # Model configuration group
    model_group = parser.add_argument_group('Model Parameters')
    model_group.add_argument(
        "--model-type",
        choices=['SimpleAutoencoder', 'EnhancedAutoencoder', 'AutoencoderEnsemble'],
        default=model_config.get('model_type', 'EnhancedAutoencoder'),
        help="Type of model architecture to use"
    )
    model_group.add_argument(
        "--features",
        type=int,
        default=data_config.get('features'),
        help=f"Number of input features (config: {data_config.get('features', FEATURES)})"
    )
    model_group.add_argument(
        "--encoding-dim",
        type=int,
        default=model_config.get('encoding_dim'),
        help=f"Encoder hidden dimension (config: {model_config.get('encoding_dim', DEFAULT_ENCODING_DIM)})"
    )
    model_group.add_argument(
        "--num-models",
        type=int,
        default=model_config.get('num_models', NUM_MODELS),
        help=f"Number of models in ensemble (config: {model_config.get('num_models', NUM_MODELS)})"
    )
    model_group.add_argument(
        "--hidden-dims",
        type=int,
        nargs='+',
        default=model_config.get('hidden_dims'),
        help=f"Hidden layer dimensions (config: {model_config.get('hidden_dims', HIDDEN_LAYER_SIZES)})"
    )
    model_group.add_argument(
        "--dropout-rates",
        type=float,
        nargs='+',
        default=model_config.get('dropout_rates'),
        help=f"Dropout rates for each layer (config: {model_config.get('dropout_rates', DROPOUT_RATES)})"
    )
    model_group.add_argument(
        "--activation",
        choices=['relu', 'leaky_relu', 'gelu', 'elu', 'swish'],
        default=model_config.get('activation', ACTIVATION),
        help=f"Activation function (config: {model_config.get('activation', ACTIVATION)})"
    )
    model_group.add_argument(
        "--normalization",
        choices=[None, 'batch', 'layer', 'instance'],
        default=model_config.get('normalization'),
        help=f"Normalization type (config: {model_config.get('normalization', NORMALIZATION)})"
    )
    
    # Data configuration group
    data_group = parser.add_argument_group('Data Parameters')
    data_group.add_argument(
        "--use-real-data",
        action="store_true",
        default=data_config.get('use_real_data', False),
        help="Use preprocessed data instead of synthetic"
    )
    data_group.add_argument(
        "--normal-samples",
        type=int,
        default=data_config.get('normal_samples'),
        help=f"Normal training samples for synthetic data (config: {data_config.get('normal_samples', NORMAL_SAMPLES)})"
    )
    data_group.add_argument(
        "--attack-samples",
        type=int,
        default=data_config.get('attack_samples'),
        help=f"Anomalous test samples for synthetic data (config: {data_config.get('attack_samples', ATTACK_SAMPLES)})"
    )
    data_group.add_argument(
        "--validation-split",
        type=float,
        default=data_config.get('validation_split', 0.2),
        help="Fraction of data to use for validation"
    )
    data_group.add_argument(
        "--data-path",
        type=Path,
        default=data_config.get('data_path', DEFAULT_MODEL_DIR / "preprocessed_dataset.csv"),
        help="Path to preprocessed data file"
    )
    data_group.add_argument(
        "--artifacts-path",
        type=Path,
        default=data_config.get('artifacts_path', DEFAULT_MODEL_DIR / "preprocessing_artifacts.pkl"),
        help="Path to preprocessing artifacts file"
    )
    
    # Security configuration group
    security_group = parser.add_argument_group('Security Parameters')
    security_group.add_argument(
        "--percentile",
        type=int,
        default=security_config.get('percentile'),
        help=f"Percentile for anomaly threshold (config: {security_config.get('percentile', DEFAULT_PERCENTILE)})"
    )
    security_group.add_argument(
        "--anomaly-threshold-strategy",
        choices=['percentile', 'iqr', 'zscore', 'isolation_forest'],
        default=security_config.get('anomaly_threshold_strategy', 'percentile'),
        help="Strategy for calculating anomaly threshold"
    )
    
    # System configuration group
    system_group = parser.add_argument_group('System Parameters')
    system_group.add_argument(
        "--model-dir",
        type=Path,
        default=Path(system_config.get('model_dir', DEFAULT_MODEL_DIR)),
        help="Directory to save model artifacts"
    )
    system_group.add_argument(
        "--tb-dir",
        type=Path,
        default=Path(monitoring_config.get('tensorboard_dir', TB_DIR)),
        help="TensorBoard logging directory"
    )
    system_group.add_argument(
        "--log-dir",
        type=Path,
        default=Path(system_config.get('log_dir', LOG_DIR)),
        help="Logging directory"
    )
    system_group.add_argument(
        "--config-dir",
        type=Path,
        default=Path(system_config.get('config_dir', CONFIG_DIR)),
        help="Configuration directory"
    )
    system_group.add_argument(
        "--num-workers",
        type=int,
        default=training_config.get('num_workers', min(4, os.cpu_count() or 1)),
        help="Number of workers for data loading"
    )
    system_group.add_argument(
        "--export-onnx",
        action="store_true",
        default=system_config.get('export_onnx', False),
        help="Export model to ONNX format"
    )
    system_group.add_argument(
        "--non-interactive",
        action="store_true",
        default=system_config.get('non_interactive', False),
        help="Disable all interactive prompts"
    )
    system_group.add_argument(
        "--debug",
        action="store_true",
        default=system_config.get('debug', False),
        help="Enable debug logging"
    )
    
    # Hyperparameter optimization group
    hpo_group = parser.add_argument_group('Hyperparameter Optimization')
    hpo_group.add_argument(
        "--hpo",
        action="store_true",
        default=hpo_config.get('enabled', False),
        help="Enable hyperparameter optimization"
    )
    hpo_group.add_argument(
        "--hpo-trials",
        type=int,
        default=hpo_config.get('n_trials', 50),
        help=f"Number of hyperparameter optimization trials (config: {hpo_config.get('n_trials', 50)})"
    )
    hpo_group.add_argument(
        "--hpo-timeout",
        type=int,
        default=hpo_config.get('timeout_seconds', 3600),
        help="Timeout for hyperparameter optimization in seconds (0 for no timeout)"
    )
    hpo_group.add_argument(
        "--hpo-sampler",
        choices=['TPE', 'Random', 'CmaEs'],
        default=hpo_config.get('sampler', {}).get('type', 'TPE'),
        help="Sampler type for hyperparameter optimization"
    )
    hpo_group.add_argument(
        "--hpo-pruner",
        choices=['Median', 'Hyperband', 'None'],
        default=hpo_config.get('pruner', {}).get('type', 'Median'),
        help="Pruner type for hyperparameter optimization"
    )
    
    # Configuration management group
    config_group = parser.add_argument_group('Configuration Management')
    config_group.add_argument(
        "--preset",
        choices=list(PRESET_CONFIGS.keys()),
        help=f"Use a preset configuration. Available: {list(PRESET_CONFIGS.keys())}"
    )
    config_group.add_argument(
        "--show-config",
        action="store_true",
        help="Display current configuration and exit"
    )
    config_group.add_argument(
        "--validate-config",
        action="store_true",
        help="Validate current configuration and exit"
    )
    config_group.add_argument(
        "--save-config",
        type=str,
        metavar="NAME",
        help="Save current configuration with given name"
    )
    config_group.add_argument(
        "--load-config",
        type=str,
        metavar="NAME",
        help="Load saved configuration by name"
    )
    config_group.add_argument(
        "--list-configs",
        action="store_true",
        help="List all available configurations"
    )
    config_group.add_argument(
        "--reset-config",
        action="store_true",
        help="Reset configuration to defaults"
    )
    config_group.add_argument(
        "--compare-models",
        action="store_true",
        help="Compare available model architectures"
    )
    config_group.add_argument(
        "--benchmark",
        action="store_true",
        help="Run performance benchmark"
    )
    
    # Monitoring configuration group
    monitoring_group = parser.add_argument_group('Monitoring Parameters')
    monitoring_group.add_argument(
        "--disable-tensorboard",
        action="store_true",
        help="Disable TensorBoard logging"
    )
    monitoring_group.add_argument(
        "--log-frequency",
        type=int,
        default=monitoring_config.get('log_frequency', 1),
        help="Frequency of progress logging (epochs)"
    )
    monitoring_group.add_argument(
        "--checkpoint-frequency",
        type=int,
        default=monitoring_config.get('checkpoint_frequency', 10),
        help="Frequency of model checkpointing (epochs)"
    )
    monitoring_group.add_argument(
        "--metrics-frequency",
        type=int,
        default=monitoring_config.get('metrics_frequency', 10),
        help="Frequency of detailed metrics logging (epochs)"
    )
    
    # Parse arguments
    args = parser.parse_args()
    
    # Handle configuration commands first (these exit after completion)
    if args.show_config:
        current_config = get_current_config()
        #current_config = config  # Use the config we have
        print("Current Configuration:")
        print("=" * 60)
        print(json.dumps(current_config, indent=2, default=str))
        return
    
    if args.validate_config:
        try:
            validate_config(config)
            logger.info("[INFO] Configuration is valid")
        except ValueError as e:
            logger.error(f"[ERROR] Configuration validation failed: {e}")
            sys.exit(1)
        return
    
    if args.list_configs:
        print("Available Configurations:")
        print("=" * 40)
        print("\nPreset Configurations:")
        for name, preset in PRESET_CONFIGS.items():
            print(f"  [+] {name}: {preset.get('description', 'No description')}")
        
        # List saved configurations if any
        saved_configs = list_saved_configs()
        if saved_configs:
            print("\nSaved Configurations:")
            for name in saved_configs:
                print(f"  [+] {name}")
        return
    
    if args.reset_config:
        if args.non_interactive or prompt_user("Reset configuration to defaults?", default=False):
            reset_config()
            logger.info("[INFO] Configuration reset to defaults")
        return
    
    if args.compare_models:
        display_model_comparison()
        return
    
    if args.benchmark:
        run_performance_benchmark(args)
        return
    
    # Handle configuration loading
    if args.load_config:
        try:
            #config = load_saved_config(args.load_config)
            config = load_config(args.load_config)
            update_global_config(config)
            logger.info(f"[INFO] Loaded configuration: {args.load_config}")
        except FileNotFoundError:
            logger.error(f"[ERROR] Configuration '{args.load_config}' not found")
            sys.exit(1)
    
    # Apply preset configuration if specified
    if args.preset:
        if args.preset not in PRESET_CONFIGS:
            logger.error(f"[ERROR] Invalid preset: {args.preset}")
            logger.info(f"Available presets: {list(PRESET_CONFIGS.keys())}")
            sys.exit(1)
        
        preset_config = PRESET_CONFIGS[args.preset].copy()
        current_config = get_current_config()
        merged_config = deep_update(current_config, preset_config)
        update_global_config(merged_config)
        logger.info(f"[INFO] Applied preset configuration: {args.preset}")
    
    # Configure logging level early
    if args.debug:
        logger.setLevel(logging.DEBUG)
        logging.getLogger("torch").setLevel(logging.DEBUG)
        logging.getLogger("optuna").setLevel(logging.DEBUG)
        logger.debug("Debug logging enabled")
    else:
        logging.getLogger("optuna").setLevel(logging.WARNING)
    
    # Update configuration with command line arguments
    current_config = get_current_config()
    
    # Map command line args to configuration structure
    arg_config_mapping = {
        # Training parameters
        'epochs': ('training', 'epochs'),
        'batch_size': ('training', 'batch_size'),
        'lr': ('training', 'learning_rate'),
        'patience': ('training', 'patience'),
        'weight_decay': ('training', 'weight_decay'),
        'grad_clip': ('training', 'gradient_clip'),
        'mixed_precision': ('training', 'mixed_precision'),
        'num_workers': ('training', 'num_workers'),
        
        # Model parameters
        'model_type': ('model', 'model_type'),
        'encoding_dim': ('model', 'encoding_dim'),
        'num_models': ('model', 'num_models'),
        'hidden_dims': ('model', 'hidden_dims'),
        'dropout_rates': ('model', 'dropout_rates'),
        'activation': ('model', 'activation'),
        'normalization': ('model', 'normalization'),
        
        # Data parameters
        'features': ('data', 'features'),
        'normal_samples': ('data', 'normal_samples'),
        'attack_samples': ('data', 'attack_samples'),
        'use_real_data': ('data', 'use_real_data'),
        'validation_split': ('data', 'validation_split'),
        'data_path': ('data', 'data_path'),
        'artifacts_path': ('data', 'artifacts_path'),
        
        # Security parameters
        'percentile': ('security', 'percentile'),
        'anomaly_threshold_strategy': ('security', 'anomaly_threshold_strategy'),
        
        # System parameters
        'model_dir': ('system', 'model_dir'),
        'tb_dir': ('system', 'tensorboard_dir'),
        'log_dir': ('system', 'log_dir'),
        'config_dir': ('system', 'config_dir'),
        'export_onnx': ('system', 'export_onnx'),
        'non_interactive': ('system', 'non_interactive'),
        'debug': ('system', 'debug'),
        
        # HPO parameters
        'hpo_trials': ('hyperparameter_optimization', 'n_trials'),
        'hpo_timeout': ('hyperparameter_optimization', 'timeout_seconds'),
        'hpo_sampler': ('hyperparameter_optimization', 'sampler', 'type'),
        'hpo_pruner': ('hyperparameter_optimization', 'pruner', 'type'),
        
        # Monitoring parameters
        'log_frequency': ('monitoring', 'log_frequency'),
        'checkpoint_frequency': ('monitoring', 'checkpoint_frequency'),
        'metrics_frequency': ('monitoring', 'metrics_frequency'),
    }
    
    # Update configuration with non-None command line arguments
    for arg_name, config_path in arg_config_mapping.items():
        arg_value = getattr(args, arg_name, None)
        if arg_value is not None:
            # Navigate to the correct nested dictionary
            target = current_config
            for key in config_path[:-1]:
                target = target.setdefault(key, {})
            target[config_path[-1]] = arg_value
    
    # Handle special flags
    if args.disable_tensorboard:
        current_config.setdefault('monitoring', {})['tensorboard_logging'] = False
    
    # Update global configuration
    update_global_config(current_config)
    
    # Save configuration if requested
    if args.save_config:
        try:
            #save_named_config(args.save_config, current_config)
            save_config(args.save_config, current_config)
            logger.info(f"[INFO] Configuration saved as: {args.save_config}")
        except Exception as e:
            logger.error(f"[ERROR] Failed to save configuration: {e}")
    
    # Validate final configuration
    try:
        validate_config(current_config)
    except ValueError as e:
        logger.error(f"[ERROR] Configuration validation failed: {e}")
        if not args.non_interactive and prompt_user("Continue with invalid configuration?", default=False):
            logger.warning("Proceeding with potentially invalid configuration")
        else:
            sys.exit(1)
    
    # Log system information
    logger.info("=" * 80)
    logger.info("SYSTEM INITIALIZATION")
    logger.info("=" * 80)
    logger.info(f"Python version: {sys.version}")
    logger.info(f"PyTorch version: {torch.__version__}")
    logger.info(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logger.info(f"CUDA version: {torch.version.cuda}")
        logger.info(f"GPU count: {torch.cuda.device_count()}")
    logger.info(f"Working directory: {os.getcwd()}")
    logger.info(f"Configuration preset: {args.preset or 'custom'}")
    
    # Run hyperparameter optimization if requested
    if args.hpo or args.hpo_trials > 0:
        logger.info("=" * 80)
        logger.info("HYPERPARAMETER OPTIMIZATION")
        logger.info("=" * 80)
        
        try:
            hpo_results = setup_hyperparameter_optimization(args, current_config)
            
            # Update args with best parameters for final training
            if hpo_results and 'best_config' in hpo_results:
                best_config = hpo_results['best_config']
                merged_config = deep_update(current_config, best_config)
                update_global_config(merged_config)
                
                logger.info("[INFO] Hyperparameter optimization completed")
                logger.info(f"Best objective value: {hpo_results['best_value']:.5f}")
                logger.info("Best parameters:")
                for key, value in hpo_results['best_params'].items():
                    logger.info(f"  {key}: {value}")
                
                # Ask if user wants to train final model
                if not args.non_interactive:
                    if not prompt_user("Train final model with best parameters?", default=True):
                        logger.info("Hyperparameter optimization completed. Exiting.")
                        return
                
                logger.info("Training final model with optimized parameters...")
            else:
                logger.warning("Hyperparameter optimization failed to produce results")
                return
                
        except Exception as e:
            logger.error(f"Hyperparameter optimization failed: {e}")
            if args.debug:
                logger.exception("HPO error details:")
            return
    
    # Ensure all required directories exist
    try:
        args.model_dir.mkdir(parents=True, exist_ok=True)
        args.log_dir.mkdir(parents=True, exist_ok=True)
        args.tb_dir.mkdir(parents=True, exist_ok=True)
        args.config_dir.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.error(f"Failed to create directories: {e}")
        sys.exit(1)
    
    # Log final training configuration
    logger.info("=" * 80)
    logger.info("FINAL CONFIGURATION")
    logger.info("=" * 80)
    
    final_config = get_current_config()
    config_summary = {
        'training': {
            'epochs': final_config.get('training', {}).get('epochs', DEFAULT_EPOCHS),
            'batch_size': final_config.get('training', {}).get('batch_size', DEFAULT_BATCH_SIZE),
            'learning_rate': final_config.get('training', {}).get('learning_rate', LEARNING_RATE),
            'patience': final_config.get('training', {}).get('patience', EARLY_STOPPING_PATIENCE),
            'mixed_precision': final_config.get('training', {}).get('mixed_precision', MIXED_PRECISION)
        },
        'model': {
            'type': final_config.get('model', {}).get('model_type', 'EnhancedAutoencoder'),
            'encoding_dim': final_config.get('model', {}).get('encoding_dim', DEFAULT_ENCODING_DIM),
            'features': final_config.get('data', {}).get('features', FEATURES)
        },
        'data': {
            'use_real_data': final_config.get('data', {}).get('use_real_data', False),
            'normal_samples': final_config.get('data', {}).get('normal_samples', NORMAL_SAMPLES),
            'attack_samples': final_config.get('data', {}).get('attack_samples', ATTACK_SAMPLES)
        },
        'system': {
            'model_dir': str(args.model_dir),
            'export_onnx': final_config.get('system', {}).get('export_onnx', False),
            'debug': final_config.get('system', {}).get('debug', False)
        }
    }
    
    for section, params in config_summary.items():
        logger.info(f"{section.upper()}:")
        for key, value in params.items():
            logger.info(f"  {key}: {value}")
    
    # Save final configuration to model directory
    final_config_path = args.model_dir / "run_configuration.json"
    try:
        with open(final_config_path, 'w') as f:
            json.dump(final_config, f, indent=2, default=str)
        logger.info(f"[INFO] Configuration saved to: {final_config_path}")
    except Exception as e:
        logger.warning(f"Could not save run configuration: {e}")
    
    # Run training
    logger.info("=" * 80)
    logger.info("STARTING TRAINING")
    logger.info("=" * 80)
    
    try:
        training_results = train_model(args)
        
        # Log training completion
        logger.info("=" * 80)
        logger.info("TRAINING COMPLETED SUCCESSFULLY")
        logger.info("=" * 80)
        
        # Display key results
        if training_results:
            metrics = training_results.get('evaluation', {})
            logger.info("Key Results:")
            logger.info(f"  Best validation loss: {training_results.get('training', {}).get('best_val_loss', 'N/A'):.4f}")
            logger.info(f"  Test loss: {metrics.get('test_loss', 'N/A'):.4f}")
            logger.info(f"  Anomaly detection rate: {metrics.get('anomaly_detection_rate', 'N/A'):.2%}")
            logger.info(f"  Model parameters: {training_results.get('model', {}).get('total_parameters', 'N/A'):,}")
            
            # Save training summary
            summary_path = args.model_dir / "training_complete.json"
            try:
                with open(summary_path, 'w') as f:
                    json.dump({
                        'status': 'completed',
                        'timestamp': datetime.now().isoformat(),
                        'configuration': final_config,
                        'results': training_results
                    }, f, indent=2, default=str)
            except Exception as e:
                logger.warning(f"Could not save training summary: {e}")
        
        logger.info("=" * 80)
        
    except KeyboardInterrupt:
        logger.info("Training interrupted by user")
        # Save interruption info
        interruption_path = args.model_dir / "training_interrupted.json"
        try:
            with open(interruption_path, 'w') as f:
                json.dump({
                    'status': 'interrupted',
                    'timestamp': datetime.now().isoformat(),
                    'configuration': final_config
                }, f, indent=2, default=str)
        except:
            pass
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"Training failed: {str(e)}")
        if args.debug:
            logger.exception("Training error details:")
        
        # Save error info
        error_path = args.model_dir / "training_failed.json"
        try:
            with open(error_path, 'w') as f:
                json.dump({
                    'status': 'failed',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat(),
                    'configuration': final_config,
                    'traceback': traceback.format_exc()
                }, f, indent=2, default=str)
        except:
            pass
        
        sys.exit(1)
        
    finally:
        # Cleanup
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass

if __name__ == "__main__":
    # Configure warnings and logging before anything else
    warnings.filterwarnings("ignore", category=UserWarning)
    warnings.filterwarnings("ignore", category=FutureWarning)
    
    # Ensure required directories exist
    try:
        LOG_DIR.mkdir(parents=True, exist_ok=True)
        DEFAULT_MODEL_DIR.mkdir(parents=True, exist_ok=True)
        CONFIG_DIR.mkdir(parents=True, exist_ok=True)
        TB_DIR.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        print(f"Failed to create required directories: {e}")
        sys.exit(1)
    
    try:
        main(logger)
    except KeyboardInterrupt:
        logger.info("Application interrupted by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}", exc_info=True)
        sys.exit(1)